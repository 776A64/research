1152,"以下是一个github上的jax下的一个issue, 标题是(BUG: jax.Array allows attribute setting)， 内容是 (The new `jax.Array` implementation allows arbitrary attributes to be added to array objects. This was not the case for `DeviceArray`: ```python  test.py import jax.numpy as jnp x = jnp.arange(4) x.custom_attr = 5 print(x.custom_attr) ``` ``` $ JAX_ARRAY=1 python test.py 5 $ JAX_ARRAY=0 python test.py  Traceback (most recent call last):   File ""tmp.py"", line 3, in      x.custom_attr = 5 AttributeError: 'DeviceArray' object has no attribute 'custom_attr' ``` Why is this a problem? Users who depend on this new behavior will end up with JAX code that doesn't transform correctly. For example: ```python def f(x):   x.custom_attr = 4   return x print(f(jnp.arange(4)).custom_attr)  4 print(jit(f)(jnp.arange(4)).custom_attr)  AttributeError: 'DynamicJaxprTracer' object has no attribute 'custom_attr' ``` We should discourage this kind of bug by disallowing such attributes entirely, as in the previous `DeviceArray` implementation.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,BUG: jax.Array allows attribute setting,"The new `jax.Array` implementation allows arbitrary attributes to be added to array objects. This was not the case for `DeviceArray`: ```python  test.py import jax.numpy as jnp x = jnp.arange(4) x.custom_attr = 5 print(x.custom_attr) ``` ``` $ JAX_ARRAY=1 python test.py 5 $ JAX_ARRAY=0 python test.py  Traceback (most recent call last):   File ""tmp.py"", line 3, in      x.custom_attr = 5 AttributeError: 'DeviceArray' object has no attribute 'custom_attr' ``` Why is this a problem? Users who depend on this new behavior will end up with JAX code that doesn't transform correctly. For example: ```python def f(x):   x.custom_attr = 4   return x print(f(jnp.arange(4)).custom_attr)  4 print(jit(f)(jnp.arange(4)).custom_attr)  AttributeError: 'DynamicJaxprTracer' object has no attribute 'custom_attr' ``` We should discourage this kind of bug by disallowing such attributes entirely, as in the previous `DeviceArray` implementation.",2022-12-29T19:47:02Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/13826,"I looked into this and there are some internal projects that currently need to set some attributes on Array. So without some pretty ugly hacks, it won't be possible to fix this just yet. But it should be possible in about 12 months when the other projects remove their monkey patching.","Cool, thanks for looking into this!"
1286,"以下是一个github上的jax下的一个issue, 标题是(JIT fails on GPU on certain computations (conv + stop_gradient + grad()))， 内容是 ( Description Seeing an issue where trying to jit() run the function results in the following error: ```XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/hlo/ir/hlo_computation.cc:962) ShapeUtil::Compatible(old_instruction>shape(), new_instruction>shape()) f32[1,5,3,12] vs f32[1,5,7,12]``` I've narrowed it down to a combination of:  Two Conv2D:s on the same underlying array  Recombining them in some fashion (e.g. addition) and blocking gradient flow through one of these two paths with stop_gradient  Doing a linear projection on the result  Taking grad() of all of the above.  Running on GPU (succeeds on CPU). See notebook where the nonjit version succeeds, but jit() of the same function fails to execute with an internal error.  https://colab.research.google.com/drive/1QybGmVwN90UIg7TXgqmwsS9QSizGrH7  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? GPU  Additional system info Colab GPU kernel  NVIDIA GPU info Thu Dec 29 15:45:31 2022        ++ )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JIT fails on GPU on certain computations (conv + stop_gradient + grad())," Description Seeing an issue where trying to jit() run the function results in the following error: ```XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/hlo/ir/hlo_computation.cc:962) ShapeUtil::Compatible(old_instruction>shape(), new_instruction>shape()) f32[1,5,3,12] vs f32[1,5,7,12]``` I've narrowed it down to a combination of:  Two Conv2D:s on the same underlying array  Recombining them in some fashion (e.g. addition) and blocking gradient flow through one of these two paths with stop_gradient  Doing a linear projection on the result  Taking grad() of all of the above.  Running on GPU (succeeds on CPU). See notebook where the nonjit version succeeds, but jit() of the same function fails to execute with an internal error.  https://colab.research.google.com/drive/1QybGmVwN90UIg7TXgqmwsS9QSizGrH7  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? GPU  Additional system info Colab GPU kernel  NVIDIA GPU info Thu Dec 29 15:45:31 2022        ++ ",2022-12-29T15:46:12Z,bug NVIDIA GPU,closed,0,2,https://github.com/jax-ml/jax/issues/13824, Can you take a look?,"Fixed. Until the fix is released, however, consider not baking your weight constants into the graph to avoid this issue. The problem is that the buggy XLA pass is trying to optimize based on the zeros in your conv weights and it is getting the optimization wrong."
1282,"以下是一个github上的jax下的一个issue, 标题是(JIT fails on GPU on certain computations (conv + stop_gradient + grad()))， 内容是 ( Description Seeing an issue where trying to jit() run the function results in the following error: ```XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/hlo/ir/hlo_computation.cc:962) ShapeUtil::Compatible(old_instruction>shape(), new_instruction>shape()) f32[1,5,3,12] vs f32[1,5,7,12]``` I've narrowed it down to a combination of:  Two Conv2D:s on the same underlying array  Recombining them in some fashion (e.g. addition) and blocking gradient flow through one of these packs with stop_gradient  Doing a linear projection on the result  Taking grad() of all of the above.  Running on GPU (succeeds on CPU). See notebook where the nonjit version succeeds, but jit() of the same function fails to execute with an internal error.  https://colab.research.google.com/drive/1QybGmVwN90UIg7TXgqmwsS9QSizGrH7  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? GPU  Additional system info Colab GPU kernel  NVIDIA GPU info Thu Dec 29 15:45:31 2022        ++ )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JIT fails on GPU on certain computations (conv + stop_gradient + grad())," Description Seeing an issue where trying to jit() run the function results in the following error: ```XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/hlo/ir/hlo_computation.cc:962) ShapeUtil::Compatible(old_instruction>shape(), new_instruction>shape()) f32[1,5,3,12] vs f32[1,5,7,12]``` I've narrowed it down to a combination of:  Two Conv2D:s on the same underlying array  Recombining them in some fashion (e.g. addition) and blocking gradient flow through one of these packs with stop_gradient  Doing a linear projection on the result  Taking grad() of all of the above.  Running on GPU (succeeds on CPU). See notebook where the nonjit version succeeds, but jit() of the same function fails to execute with an internal error.  https://colab.research.google.com/drive/1QybGmVwN90UIg7TXgqmwsS9QSizGrH7  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? GPU  Additional system info Colab GPU kernel  NVIDIA GPU info Thu Dec 29 15:45:31 2022        ++ ",2022-12-29T15:46:02Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/13823,duplicate of 13824
1536,"以下是一个github上的jax下的一个issue, 标题是(Different behaviour with savemat for jax.Array)， 内容是 ( Description Hi! In previous versions of jax (specifically, versions less than 0.4), it was possible to use a `jax.DeviceArray` as a substitute for a numpy array in certain circumstances. For example, the following code was previously functional: ```python >>> from jax import numpy as jnp >>> from scipy.io import savemat >>> a = jnp.ones((4,)) >>> savemat(""test.mat"", {""a"":a}) ``` This would create a `.mat` file with a variable a that contained a length4 array with all ones. However, since the output of jax functions is now a `jax.Array`, the implicit conversion to a numpy variable no longer works, and the variable is not saved correctly. For example, if Itry to load the previously created file in MATLAB, it will show an empty structure: ```matlab >> load test.mat >> a a =    struct with no fields. ``` I understand that this is not directly a problem with jax, but I believe it could potentially affect the compatibility of jax with other libraries. As jax becomes more popular for scientific computing, this could be an issue for many users. I was wondering if it would be worth mentioning this potential problem in the jax.Array migration guide  What jax/jaxlib version are you using? jax v0.4.1  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Different behaviour with savemat for jax.Array," Description Hi! In previous versions of jax (specifically, versions less than 0.4), it was possible to use a `jax.DeviceArray` as a substitute for a numpy array in certain circumstances. For example, the following code was previously functional: ```python >>> from jax import numpy as jnp >>> from scipy.io import savemat >>> a = jnp.ones((4,)) >>> savemat(""test.mat"", {""a"":a}) ``` This would create a `.mat` file with a variable a that contained a length4 array with all ones. However, since the output of jax functions is now a `jax.Array`, the implicit conversion to a numpy variable no longer works, and the variable is not saved correctly. For example, if Itry to load the previously created file in MATLAB, it will show an empty structure: ```matlab >> load test.mat >> a a =    struct with no fields. ``` I understand that this is not directly a problem with jax, but I believe it could potentially affect the compatibility of jax with other libraries. As jax becomes more popular for scientific computing, this could be an issue for many users. I was wondering if it would be worth mentioning this potential problem in the jax.Array migration guide  What jax/jaxlib version are you using? jax v0.4.1  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_",2022-12-28T20:50:56Z,bug,closed,0,9,https://github.com/jax-ml/jax/issues/13815,"Interesting, thanks for the report. I was looking into it and found that using `format='4'` rather than `format='5'` works correctly with JAX arrays: ```python from jax import numpy as jnp from scipy.io import savemat, loadmat a = jnp.ones((4,)) savemat(""test.mat"", {""a"": a}) print(loadmat('test.mat')['a'])  [[None]] savemat(""test.mat"", {""a"": a}, format='4') print(loadmat('test.mat')['a'])  [[1. 1. 1. 1.]] ``` I wonder why these treat their inputs differently?","It looks like format 4 explicitly converts inputs to arrays: https://github.com/scipy/scipy/blob/706fafea3d67d471fcac7a0308a8a6e6766b82ed/scipy/io/matlab/_mio4.pyL512 While format 5 does not, it converts its input with this function: https://github.com/scipy/scipy/blob/706fafea3d67d471fcac7a0308a8a6e6766b82ed/scipy/io/matlab/_mio5.pyL450 I can confirm that it treats `Array` and `DeviceArray` differently: ```python  check_writeable.py import jax.numpy as jnp from scipy.io.matlab._mio5 import to_writeable x = jnp.arange(5) print(type(x)) print(to_writeable(x)) ``` ``` $ JAX_ARRAY=1 python check_writeable.py    $ JAX_ARRAY=0 python check_writeable.py   [0 1 2 3 4] ``` Still not sure what the difference might be... I'll digin a bit more.","Ah, the difference is that `jax.Array` has a `__dict__` attribute while `DeviceArray` does not: ```python $ JAX_ARRAY=0 python c ""import jax.numpy as jnp; print(hasattr(jnp.arange(4), '__dict__'))"" False $ JAX_ARRAY=1 python c ""import jax.numpy as jnp; print(hasattr(jnp.arange(4), '__dict__'))"" True ``` Because of that, this line in `to_writeable` treats the two inputs differently.   do you know why `jax.Array` defines a `__dict__` method? Is it an intentional addition to the new array API?","OK, chatted with Yash offline and it looks like it's not trivial to remove the `__dict__` attribute from `jax.Array`. This scipy implementation is very strange, because essentially *any* normal userdefined type will have a `__dict__` attribute. Here's another example of an object that fails with `loadmat` in the same way as `jax.Array`: ```python import numpy as np from jax import numpy as jnp from scipy.io import savemat, loadmat class MyArray:     def __init__(self, arr):         self._arr = arr     def __array__(self):         return self._arr a = MyArray(np.arange(5.))  Object is recognized by numpy as an array: print(np.asarray(a))  [0. 1. 2. 3. 4.]  Object works with savemat format='4' savemat(""test.mat"", {""a"": a}, format='4') print(loadmat('test.mat')['a'])  [[0. 1. 2. 3. 4.]]  Object fails with savemat default (format='5') savemat(""test.mat"", {""a"": a}) print(loadmat('test.mat')['a'])  [[None]] ``` I'd probably consider this a bug in the `scipy.io.savemat` code: it seems very strange in arrayoriented code that the presence of a `__dict__` attribute (which every normal class has by default!) supersedes the `__array__` method (which is explicitly defined by people who want their objects to convert to arrays). That said, this has been scipy's behavior since 2009, so I suspect the developers will be hesitant to change this. I think your best bet here is probably to make sure you cast your inputs to numpy array before passing them to `savemat`, as this is what scipy seems to expect. i.e.: ```python savemat(""test.mat"", {""a"": np.asarray(a)}) ```","Here's an issue from 2015 reporting this same failure in the case of `xarray.DataArray`: https://github.com/scipy/scipy/issues/5371 Since  weighedin there, he might have ideas about the best way forward here. Do you think it's worth raising this issue with scipy?",This definitely seems like a scipy bug. At the very least it should be raising an error instead of silently failing on arraylike inputs. I would encourage submitting a fix upstream.,"Cool, thanks for the input .  since this is your bug/issue, are you interested in surfacing this upstream at the scipy repo? If you're not interested, let me know and I'd be happy to take over. Thanks!","Thank you very much for the super quick and indepth analysis! Regarding upstreaming this issue to scipy, I'm on leave (away from a computer) until the middle of January, so if you're happy to do it before then, , please feel free to do so!","Scipy PR proposal is here: https://github.com/scipy/scipy/pull/17676 Also, I think we should fix this on the JAX side for other reasons. I opened CC(BUG: jax.Array allows attribute setting) to track"
676,"以下是一个github上的jax下的一个issue, 标题是(validate shape & dtype in ShapeDtypeStruct)， 内容是 (Followup to CC(Fixed `ShapeDtypeStruct((), None)` giving float64 under 32bit mode) There was maybe some disagreement about this in CC(Fixed `ShapeDtypeStruct((), None)` giving float64 under 32bit mode), but my opinion remains that passing `dtype=None` in `ShapeDtypeStruct` should be an error, rather than us trying to guess what the user might have intended. I'll be sure to run a global test on this to find out if anyone is depending on this behavior.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,validate shape & dtype in ShapeDtypeStruct,"Followup to CC(Fixed `ShapeDtypeStruct((), None)` giving float64 under 32bit mode) There was maybe some disagreement about this in CC(Fixed `ShapeDtypeStruct((), None)` giving float64 under 32bit mode), but my opinion remains that passing `dtype=None` in `ShapeDtypeStruct` should be an error, rather than us trying to guess what the user might have intended. I'll be sure to run a global test on this to find out if anyone is depending on this behavior.",2022-12-28T20:06:16Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/13814,"Global presubmit showed several uses where shape entries are noninteger (either `DimPoly` or `None`). For simplicity, I dropped the check that shape entries are integers.","> [...] several uses where shape entries are noninteger (either `DimPoly` or `None`) The `DimPoly` ones should all be internal, right? Maybe we can distinguish an internal construction from a public one, and have more restrictive checks in the latter case?"
305,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Move pjit test into the right TestCase class)， 内容是 (This test uses tfxla.call_module directly, belongs to XlaCallModuleTest.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Move pjit test into the right TestCase class,"This test uses tfxla.call_module directly, belongs to XlaCallModuleTest.",2022-12-27T13:29:14Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13802
4792,"以下是一个github上的jax下的一个issue, 标题是(Problem with jax when installing multinerf)， 内容是 ( Description I am trying to install the multinerf framework on my ubuntu 20.04 machine but it seems that there is a problem with JAX. When I try to confirm that all unit tests pass I get the following error: ``` (multinerf) mypc:~/user/multinerf$ ./scripts/run_all_unit_tests.sh .  Ran 1 test in 2.074s OK ....  Ran 4 tests in 13.416s OK ...............................Mean Error = 0.0803162083029747, Tolerance = 0.1 .Mean Error = 0.08638705313205719, Tolerance = 0.1 ........................  Ran 56 tests in 90.169s OK .........20221227 13:00:39.973523: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2163] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.cholesky' failed: cuSolver internal error. E..PE of degree 5 has a maximum error of 2.5369226932525635e06 .PE of degree 10 has a maximum error of 6.4849853515625e05 .PE of degree 15 has a maximum error of 0.002378210425376892 .PE of degree 20 has a maximum error of 0.11622805148363113 .PE of degree 25 has a maximum error of 1.999955415725708 .PE of degree 30 has a maximum error of 1.9999704360961914 .... ====================================================================== ERROR: test_integrated_pos_enc (tests.coord_test.CoordTest) tests.coord_test.CoordTest.test_integrated_pos_enc  Traceback (most recent call last):   File ""/home/user/multinerf/tests/coord_test.py"", line 254, in test_integrated_pos_enc     samples = random.multivariate_normal(key, mean, cov, [num_samples])   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/random.py"", line 625, in multivariate_normal     return _multivariate_normal(key, mean, cov, shape, dtype, method)   type: ignore   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/api.py"", line 623, in cache_miss     out_flat = call_bind_continuation(execute(*args_flat))   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/interpreters/pxla.py"", line 2136, in __call__     out_bufs = self.xla_executable.execute_sharded_on_local_devices( jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.cholesky' failed: cuSolver internal error. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/user/multinerf/tests/coord_test.py"", line 254, in test_integrated_pos_enc     samples = random.multivariate_normal(key, mean, cov, [num_samples])   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/random.py"", line 625, in multivariate_normal     return _multivariate_normal(key, mean, cov, shape, dtype, method)   type: ignore jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.cholesky' failed: cuSolver internal error.  Ran 21 tests in 27.995s FAILED (errors=1) ......  Ran 6 tests in 5.412s OK ..  Ran 2 tests in 4.408s OK .  Ran 1 test in 0.325s OK .  Ran 1 test in 1.427s OK ....../home/user/multinerf/internal/math.py:28: RuntimeWarning: overflow encountered in cast   return fn(jnp.where(jnp.abs(x) < t, x, x % t)) /home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py:1090: RuntimeWarning: overflow encountered in cast   return _where(condition, x, y) .  Ran 7 tests in 4.480s OK ..........................................  Ran 42 tests in 37.426s OK ``` I've installed jaxlib with GPU support: `pip install U jax[cuda11_cudnn82] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`  What jax/jaxlib version are you using? jax 0.4.1 jaxlib 0.4.1+cuda11.cudnn82  Which accelerator(s) are you using? GeForce GTX 1080 Ti  Additional system info Python version 3.9, Ubuntu 20.04  NVIDIA GPU info nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052021 NVIDIA Corporation Built on Mon_Oct_11_21:27:02_PDT_2021 Cuda compilation tools, release 11.4, V11.4.152 Build cuda_11.4.r11.4/compiler.30521435_0)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Problem with jax when installing multinerf," Description I am trying to install the multinerf framework on my ubuntu 20.04 machine but it seems that there is a problem with JAX. When I try to confirm that all unit tests pass I get the following error: ``` (multinerf) mypc:~/user/multinerf$ ./scripts/run_all_unit_tests.sh .  Ran 1 test in 2.074s OK ....  Ran 4 tests in 13.416s OK ...............................Mean Error = 0.0803162083029747, Tolerance = 0.1 .Mean Error = 0.08638705313205719, Tolerance = 0.1 ........................  Ran 56 tests in 90.169s OK .........20221227 13:00:39.973523: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2163] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.cholesky' failed: cuSolver internal error. E..PE of degree 5 has a maximum error of 2.5369226932525635e06 .PE of degree 10 has a maximum error of 6.4849853515625e05 .PE of degree 15 has a maximum error of 0.002378210425376892 .PE of degree 20 has a maximum error of 0.11622805148363113 .PE of degree 25 has a maximum error of 1.999955415725708 .PE of degree 30 has a maximum error of 1.9999704360961914 .... ====================================================================== ERROR: test_integrated_pos_enc (tests.coord_test.CoordTest) tests.coord_test.CoordTest.test_integrated_pos_enc  Traceback (most recent call last):   File ""/home/user/multinerf/tests/coord_test.py"", line 254, in test_integrated_pos_enc     samples = random.multivariate_normal(key, mean, cov, [num_samples])   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/random.py"", line 625, in multivariate_normal     return _multivariate_normal(key, mean, cov, shape, dtype, method)   type: ignore   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/api.py"", line 623, in cache_miss     out_flat = call_bind_continuation(execute(*args_flat))   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/interpreters/pxla.py"", line 2136, in __call__     out_bufs = self.xla_executable.execute_sharded_on_local_devices( jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.cholesky' failed: cuSolver internal error. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/user/multinerf/tests/coord_test.py"", line 254, in test_integrated_pos_enc     samples = random.multivariate_normal(key, mean, cov, [num_samples])   File ""/home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/random.py"", line 625, in multivariate_normal     return _multivariate_normal(key, mean, cov, shape, dtype, method)   type: ignore jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.cholesky' failed: cuSolver internal error.  Ran 21 tests in 27.995s FAILED (errors=1) ......  Ran 6 tests in 5.412s OK ..  Ran 2 tests in 4.408s OK .  Ran 1 test in 0.325s OK .  Ran 1 test in 1.427s OK ....../home/user/multinerf/internal/math.py:28: RuntimeWarning: overflow encountered in cast   return fn(jnp.where(jnp.abs(x) < t, x, x % t)) /home/mypc/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py:1090: RuntimeWarning: overflow encountered in cast   return _where(condition, x, y) .  Ran 7 tests in 4.480s OK ..........................................  Ran 42 tests in 37.426s OK ``` I've installed jaxlib with GPU support: `pip install U jax[cuda11_cudnn82] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`  What jax/jaxlib version are you using? jax 0.4.1 jaxlib 0.4.1+cuda11.cudnn82  Which accelerator(s) are you using? GeForce GTX 1080 Ti  Additional system info Python version 3.9, Ubuntu 20.04  NVIDIA GPU info nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052021 NVIDIA Corporation Built on Mon_Oct_11_21:27:02_PDT_2021 Cuda compilation tools, release 11.4, V11.4.152 Build cuda_11.4.r11.4/compiler.30521435_0",2022-12-27T13:08:34Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/13801,Can you paste the output of the following command? This will make clear which jax/jaxlib versions you have installed: ``` $ python m pip list | grep jax ```,Hi  first of all: thank you for your help! I've updated the question. The versions for jax/jaxlib are: jax 0.4.1 jaxlib 0.4.1+cuda11.cudnn82,Does anyone have any idea what could be the problem here?,"I'm not sure what might be causing this. Can you try running the JAX test suite? if that passes, then perhaps `test_integrated_pos_enc` is exercising some corner case that is not covered by JAX's tests. Otherwise, maybe updating the cusolver version could help.",I have the same problem. How can I solve it? jax                          0.4.3 jaxlib                       0.4.3+cuda11.cudnn86
1070,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Enable Trace to handle dimension polynomials used as constants)， 内容是 (This change enables the use of dimension polynomials wherever constaints are used. This would arise, e.g., when tracing `lambda x: x.shape[0]` in presence of shape polymorphism. This won't be needed anymore once the jax_dynamic_shapes improves its coverage to replace shape polymorphism. The downside of this change is that it adds a code path to Trace.full_raise. An alternative would be to ask users to explicitly convert dimensions: `lambda x: core.dimension_as_value(x.shape[0])`. Both of these can be removed in the future, but the former has the advantage of being internal to JAX. An alternative internal change in `Trace.full_raise` would be ``` if hasattr(val, ""__jax_array__""): val = val.__jax_array__() ``` but I think that using `dimension_as_value` makes it clear what use case is addressed by this change.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Enable Trace to handle dimension polynomials used as constants,"This change enables the use of dimension polynomials wherever constaints are used. This would arise, e.g., when tracing `lambda x: x.shape[0]` in presence of shape polymorphism. This won't be needed anymore once the jax_dynamic_shapes improves its coverage to replace shape polymorphism. The downside of this change is that it adds a code path to Trace.full_raise. An alternative would be to ask users to explicitly convert dimensions: `lambda x: core.dimension_as_value(x.shape[0])`. Both of these can be removed in the future, but the former has the advantage of being internal to JAX. An alternative internal change in `Trace.full_raise` would be ``` if hasattr(val, ""__jax_array__""): val = val.__jax_array__() ``` but I think that using `dimension_as_value` makes it clear what use case is addressed by this change.",2022-12-25T12:31:40Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/13790,  PTAL
1723,"以下是一个github上的jax下的一个issue, 标题是(Full support for pluggable PJRT backends)， 内容是 (I've landed a first version of a dynamically loaded IREE PJRT plugin (https://github.com/ireeorg/ireesamples/pull/81). The code is in our samples repo here, along with notes on how to reproduce:   https://github.com/ireeorg/ireesamples/tree/main/pjrtplugin  It will stay there until ready to raise a real RFC in the new year. There's been a privatetoGoogle chat for getting it to this point, but I think this is a good checkpoint to bring that and the related artifacts public so we can collaborate on finishing this (since these are all OSS projects). Any objections? I've got a number of next steps on the functionality side to get this all good: * Create plugins for more backends and make them more configurable (i.e. even the CPU backend can shard devices based on NUMA nodes, etc  but this requires configuration). * Derive device tuning compile flags from querying the runtime for the actual connected devices (vs just compiler defaults). * Implement async invoke and h2d/d2h transfers (the sync APIs are harder to get wrong and easy so just did that first but obviously not a real solution). * Enable the CUDA backend and Tracy for local profiling (both of which are missing Bazel build support in IREE :( ). * Package this up so it can be pip installed (presumably that will also be implicated in plugin discovery?) It'd be great to work in tandem on getting the plugin infra itself to a level of completion so that I can run the test suite without hacking Jax internals or doing custom builds.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Full support for pluggable PJRT backends,"I've landed a first version of a dynamically loaded IREE PJRT plugin (https://github.com/ireeorg/ireesamples/pull/81). The code is in our samples repo here, along with notes on how to reproduce:   https://github.com/ireeorg/ireesamples/tree/main/pjrtplugin  It will stay there until ready to raise a real RFC in the new year. There's been a privatetoGoogle chat for getting it to this point, but I think this is a good checkpoint to bring that and the related artifacts public so we can collaborate on finishing this (since these are all OSS projects). Any objections? I've got a number of next steps on the functionality side to get this all good: * Create plugins for more backends and make them more configurable (i.e. even the CPU backend can shard devices based on NUMA nodes, etc  but this requires configuration). * Derive device tuning compile flags from querying the runtime for the actual connected devices (vs just compiler defaults). * Implement async invoke and h2d/d2h transfers (the sync APIs are harder to get wrong and easy so just did that first but obviously not a real solution). * Enable the CUDA backend and Tracy for local profiling (both of which are missing Bazel build support in IREE :( ). * Package this up so it can be pip installed (presumably that will also be implicated in plugin discovery?) It'd be great to work in tandem on getting the plugin infra itself to a level of completion so that I can run the test suite without hacking Jax internals or doing custom builds.",2022-12-22T05:25:42Z,enhancement,closed,1,4,https://github.com/jax-ml/jax/issues/13765,"Note also that I took some liberties on adding some layering with respect to platforms and types of plugins (and configuration). While most folks can't see this, I expect this will let us instantiate the plugins in the internal Google static builds of Jax, and that is different enough that I just implemented the necessary optionality (always easier to design for that than add it later).","FYI  if justification to further clean up the C API deps is needed, here is an abbreviated bloaty report of the IREE CPU plugin: ```     FILE SIZE        VM SIZE       53.9%  13.7Mi  54.7%  1.15Mi    protobuf   28.1%  7.14Mi  22.6%   486Ki    tensorflow   12.2%  3.09Mi  16.6%   357Ki    iree    4.5%  1.14Mi   3.0%  64.2Ki    absl    0.4%   114Ki   1.1%  23.2Ki    cpuinfo ``` Afaict, this is coming from the need to interop with the XlaShape proto and use a couple of utilities for constructing layouts. Although some of it may be coming from things coming in transitively from the way the C API depends on the TPU support library (which results in a bunch of static initializers, I think). Without all of that, the size would roughly be the combination of the iree and cpuinfo row. Note that the ""iree"" row includes the compiler interface and the CPU runtime but not the compiler (which is installed separately).",And for the CUDA plugin: ```     FILE SIZE        VM SIZE       55.3%  13.7Mi  57.9%  1.16Mi    protobuf   28.8%  7.14Mi  23.8%   486Ki    tensorflow   10.4%  2.58Mi  13.2%   269Ki    iree    4.6%  1.14Mi   3.1%  64.1Ki    absl ```,I think we can declare this issue fixed. We're now ourselves shipping: * XLA/CUDA GPU via a plugin (on PyPi) * Google TPU and we welcome more! (AMD/ROCM?)
2800,"以下是一个github上的jax下的一个issue, 标题是(Outside Call is not Eliminated)， 内容是 ( Description It seems that dead call of `host_callback.call` is not eliminated. Consider the piece of code below which is actually `flax` module. It calls a pyfunction `phase_boundary` which returns a 1D array of shape `2` and assigns it to class instance members with pattern matching. The issue is that these variables (`self.varw` and `self.varb`) are not used anywhere further but the execution time is faster without calling the host callback. ```python import jax import flax.linen as nn class CNN(nn.Module):     depth: int = 16     def setup(self):         self.varw, self.varb = jax.experimental.host_callback.call(             callback_func=phase_boundary,             arg=1 / self.depth,             result_shape=jax.ShapeDtypeStruct((2, ), jnp.float32))         self.head = nn.Dense(features=10)     def __call__(self xs: jax.Array) > jax.Array:          Expecting `xs` shape to be (?, 28, 28, C).         return xs.mean(axis=(1, 2)) ``` A quick use of  `jaxpr` shows that outside call produces two dangling variables `e` and `f` which are not eliminated. ```python >>> jax.make_jaxpr(model.apply)({...}, jnp.empty((5, 28, 28, 1))) { lambda ; a:f32[10] b:f32[1,10] c:f32[5,28,28,1]. let     d:f32[2] = outside_call[       arg_treedef=PyTreeDef(*)       callback=       flat_results_aval=(ShapedArray(float32[2]),)       identity=False       result_treedef=PyTreeDef(*)     ] 0.0625     e:f32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] d     _:f32[] = squeeze[dimensions=(0,)] e     f:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] d     _:f32[] = squeeze[dimensions=(0,)] f     g:f32[5,1] = reduce_sum[axes=(1, 2)] c     h:f32[5,1] = div g 784.0     i:f32[5,10] = dot_general[       dimension_numbers=(((1,), (0,)), ((), ()))       precision=None       preferred_element_type=None     ] h b     j:f32[1,10] = reshape[dimensions=None new_sizes=(1, 10)] a     k:f32[5,10] = add i j   in (k,) } ``` I am not sure whether I should submit an issue here or to `flax` (please see a related issue google/flax CC(Raise an error if stop_gradient is called on nonarrays)). Is there any straightforward workaround?  What jax/jaxlib version are you using? jax v0.4.1 jaxlib v0.4.1 flax v0.63  Which accelerator(s) are you using? GPU  Additional system info Python v3.10 Linux v5.4 CUDA v11.4 CUDNN v8.2.4.15  NVIDIA GPU info Used GPU is Nvidia Tesla V100SXM2 16GB. Packages `jax`, `jaxlib` were installed as ```shell pip3.10 install nocachedir \     f ""https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html"" \     ""jax[cuda11_cudnn82]"" ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Outside Call is not Eliminated," Description It seems that dead call of `host_callback.call` is not eliminated. Consider the piece of code below which is actually `flax` module. It calls a pyfunction `phase_boundary` which returns a 1D array of shape `2` and assigns it to class instance members with pattern matching. The issue is that these variables (`self.varw` and `self.varb`) are not used anywhere further but the execution time is faster without calling the host callback. ```python import jax import flax.linen as nn class CNN(nn.Module):     depth: int = 16     def setup(self):         self.varw, self.varb = jax.experimental.host_callback.call(             callback_func=phase_boundary,             arg=1 / self.depth,             result_shape=jax.ShapeDtypeStruct((2, ), jnp.float32))         self.head = nn.Dense(features=10)     def __call__(self xs: jax.Array) > jax.Array:          Expecting `xs` shape to be (?, 28, 28, C).         return xs.mean(axis=(1, 2)) ``` A quick use of  `jaxpr` shows that outside call produces two dangling variables `e` and `f` which are not eliminated. ```python >>> jax.make_jaxpr(model.apply)({...}, jnp.empty((5, 28, 28, 1))) { lambda ; a:f32[10] b:f32[1,10] c:f32[5,28,28,1]. let     d:f32[2] = outside_call[       arg_treedef=PyTreeDef(*)       callback=       flat_results_aval=(ShapedArray(float32[2]),)       identity=False       result_treedef=PyTreeDef(*)     ] 0.0625     e:f32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] d     _:f32[] = squeeze[dimensions=(0,)] e     f:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] d     _:f32[] = squeeze[dimensions=(0,)] f     g:f32[5,1] = reduce_sum[axes=(1, 2)] c     h:f32[5,1] = div g 784.0     i:f32[5,10] = dot_general[       dimension_numbers=(((1,), (0,)), ((), ()))       precision=None       preferred_element_type=None     ] h b     j:f32[1,10] = reshape[dimensions=None new_sizes=(1, 10)] a     k:f32[5,10] = add i j   in (k,) } ``` I am not sure whether I should submit an issue here or to `flax` (please see a related issue google/flax CC(Raise an error if stop_gradient is called on nonarrays)). Is there any straightforward workaround?  What jax/jaxlib version are you using? jax v0.4.1 jaxlib v0.4.1 flax v0.63  Which accelerator(s) are you using? GPU  Additional system info Python v3.10 Linux v5.4 CUDA v11.4 CUDNN v8.2.4.15  NVIDIA GPU info Used GPU is Nvidia Tesla V100SXM2 16GB. Packages `jax`, `jaxlib` were installed as ```shell pip3.10 install nocachedir \     f ""https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html"" \     ""jax[cuda11_cudnn82]"" ```",2022-12-22T02:01:58Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/13762,"Thanks for raising this! If I understand correctly (though  should correct any mistakes I make), we can't eliminate `host_callback.call`s, even when the outputs are unused, because the callback may be sideeffecting. But we may be in luck: we're in the process of deprecating `host_callback` in favor of new callback APIs, which in particular are more clear about when they should be used. For example, there's `jax.pure_callback` (see also here), where because the callback is promised to be functionally pure (and in particular isn't sideeffecting), we know it can be safely deadcode eliminated if its outputs aren't used. Is `phase_boundary` sideeffecting, or pure? If it's pure, then perhaps you can use `jax.pure_callback`. If it's sideeffecting, is there some other reason it would be safe to eliminate?","There is no sideeffects in the sense of IO but it calls pyfunction `scipy.integrate.quad` for adaptive integration (not sure about `/dev/urandom` in QUADPACK). I replaced `jax.experimental.host_callback.call` with `jax.pure_callback` ( thank you!). It works perfectly fine and there is no slow down whatsoever! However, I found that `pure_callback` is not eliminated as well (please, see below). ```python >>> jax.make_jaxpr(model.apply)({...}, jnp.empty((5, 28, 28, 1))) { lambda ; a:f32[10] b:f32[1,10] c:f32[5,28,28,1]. let     d:f32[2] = pure_callback[       callback=._flat_callback at 0x7f6d787a77f0>       result_avals=(ShapedArray(float32[2]),)       vectorized=False     ] 0.0625     e:f32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] d     _:f32[] = squeeze[dimensions=(0,)] e     f:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] d     _:f32[] = squeeze[dimensions=(0,)] f     g:f32[5,1] = reduce_sum[axes=(1, 2)] c     h:f32[5,1] = div g 784.0     i:f32[5,10] = dot_general[       dimension_numbers=(((1,), (0,)), ((), ()))       precision=None       preferred_element_type=None     ] h b     j:f32[1,10] = reshape[dimensions=None new_sizes=(1, 10)] a     k:f32[5,10] = add i j   in (k,) } ``` Am I right that the difference between `host_callback` and `pure_callback` is that the latter memoizes the results of the execution? How large is result cache in `pure_callback`?","`pure_callback` won't memoize results. But I think now the issue may be just in how we're inspecting the computation: `make_jaxpr` doesn't apply deadcode elimination, but XLA will. (Actually, JAX itself also does some DCE before lowering to XLA, but `make_jaxpr` doesn't show that.) I can think of three ways to verify it's being eliminated, and hence the callback is never called: 1. put a print statement in your callback; 2. inspect the optimized XLA HLO to see what's really going to be compiled and executed; 3. manually apply JAX's internal DCE pass to the result of `make_jaxpr`. I recommend doing the first. But here's how to do all three: ```python import jax def callback(x):   print('called!')   return x .jit def f(x):   _ = jax.pure_callback(callback, x, result_shape=x)   return x f(3.)   method 1, no print! print(f.lower(3.).compile().as_text())   method 2, no callback in optimized computation!  HloModule jit_f, entry_computation_layout={(f32[])>f32[]}, allow_spmd_sharding_propagation_to_output=true     ENTRY %main.2 (Arg_0.1: f32[]) > f32[] {    %Arg_0.1 = f32[] parameter(0), sharding={replicated}    ROOT %copy = f32[] copy(f32[] %Arg_0.1)  } from jax.interpreters import partial_eval as pe jaxpr = jax.make_jaxpr(f)(3.) jaxpr, _, _ = pe.dce_jaxpr_consts(jaxpr.jaxpr, [True]) print(jaxpr)   method 3, after JAX's DCE (before any XLA DCE), no callback! { lambda ; a:f32[]. let     b:f32[] = xla_call[call_jaxpr={ lambda ; c:f32[]. let  in (c,) } name=f] a   in (b,) } ``` WDYT?","If we replace `jax.pure_callback` with `hcb.call` (where we did `from jax.experimental import host_callback as hcb`) then all three methods show the call is not eliminated: ``` called! HloModule jit_f, entry_computation_layout={(f32[])>f32[]}, allow_spmd_sharding_propagation_to_output=true ENTRY %main.6 (Arg_0.1: f32[]) > f32[] {   %constant.2 = s64[] constant(94424012441744)   %afterall.3 = token[] afterall(), metadata={op_name=""jit(f)/jit(main)/create_token"" source_file=""/usr/local/google/home/mattjj/packages/jax/issue13762.py"" source_line=15}   %replicaid.4 = u32[] replicaid(), metadata={op_name=""jit(f)/jit(main)/outside_call[callback= identity=False arg_treedef=PyTreeDef(*) device_index=0 result_treedef=PyTreeDef(*) flat_results_aval=(ShapedArray(float32[]),) has_token=True]"" source_file=""/usr/local/google/home/mattjj/packages/jax/issue13762.py"" source_line=10}   %Arg_0.1 = f32[] parameter(0), sharding={replicated}   ROOT %copy = f32[] copy(f32[] %Arg_0.1)   %customcall.5 = (token[], f32[]) customcall(s64[] %constant.2, token[] %afterall.3, u32[] %replicaid.4, f32[] %copy), custom_call_target=""xla_python_cpu_callback"", operand_layout_constraints={s64[], token[], u32[], f32[]}, custom_call_has_side_effect=true, api_version=API_VERSION_STATUS_RETURNING, sharding={{maximal device=0}, {maximal device=0}}, metadata={op_name=""jit(f)/jit(main)/outside_call[callback= identity=False arg_treedef=PyTreeDef(*) device_index=0 result_treedef=PyTreeDef(*) flat_results_aval=(ShapedArray(float32[]),) has_token=True]"" source_file=""/usr/local/google/home/mattjj/packages/jax/issue13762.py"" source_line=10}, backend_config=""94424012441744"" } { lambda ; a:f32[]. let     b:f32[] = xla_call[       call_jaxpr={ lambda ; c:f32[]. let           _:f32[] = outside_call[             arg_treedef=PyTreeDef(*)             callback=             device_index=0             flat_results_aval=(ShapedArray(float32[]),)             identity=False             result_treedef=PyTreeDef(*)           ] c         in (c,) }       name=f     ] a   in (b,) } ```","IIUC this issue is resolved, since `pure_callback` is indeed removed when dead, while `host_callback` can't be removed in general. Let me know if I'm mistaken and there's still some outstanding issue here!", You are absolutely right. Pure callback works perfectly. There is no visible performance degradation despite that the call is possibly not eliminated. Thank you for your time and explanation.,That's great! (And to be clear I'm pretty sure the call is eliminated!) Good luck with your work.
9057,"以下是一个github上的jax下的一个issue, 标题是(Add IO callback)， 内容是 ( Background In JAX, we usually express computations as compositions of XLA operations but oftentimes we want to call out to Python functions. Historically, the primary mechanism has been `jax.experimental.host_callback` but in making host callback nonexperimental, we’re looking into improving it on all fronts: underlying implementation improvements (via send/recv on TPU, custom call on CPU/GPU), simpler API for users (see `jax.debug.print`), and safety/correctness (`jax.pure_callback`, `jax.debug.callback`).  We’ll discuss a missing newgeneration callback (`io_callback`) and some challenges in its implementation.  Host callback under transformation We’ve introduced two public, nonexperimental Python callback mechanisms, namely `jax.debug.callback` and `jax.pure_callback`, but they aren’t as general purpose as `jax.experimental.host_callback`. `jax.experimental.host_callback.call` takes in a callable, a set of arguments, and metadata about the return type. It then stages out the callable into HLO as a custom call (or uses infeed/outfeed on TPU). Unlike `jax.debug.callback` and `jax.pure_callback`, it has no requirements for the callable. `jax.debug.callback` doesn’t allow callbacks with return values and `jax.pure_callback` does not allow sideeffects in the callable (not enforceable, but it is assumed by JAX and XLA). Why do we have two separate callbacks that each have restrictions? Why not make a `jax.callback` that can handle any manner of callable? The answer is sideeffects. Unlike regular primitives that are staged out, an arbitrary Python callback may not have well defined transformation rules. How do you differentiate through logging to Tensorboard? host_callback defines transformation rules in a particular way and works with basically every transformation and higher order primitive, but as a consequence, it can have counterintuitive behavior when there are sideeffects present.  Let’s look at some brief examples. Although `vmap` is the main instigator, issues can crop up, in general, when transforming higherorder primitives with side effects in them. Example 1: vmap of cond ```python def f(pred):   def true_fun():     hcb.call(lambda _: print(""true""), (), result_shape=None)   def false_fun():     hcb.call(lambda _: print(""false""), (), result_shape=None)   return lax.cond(pred, false_fun, true_fun) jax.vmap(f)(jnp.array([True, True]))  ==> prints  true  false ``` Example 2: vmap of while ```python def check(x, _):   assert np.all(x  raises AssertionError! ``` Why are these weird behaviors happening? Fundamentally, it’s because the vmapofcond and vmapofwhile transformation rules are not aware of sideeffects and assume that their bodies are pure. Lowering a vmapofcond to a select and a vmapofwhile to a padded loop are both semantics preserving when the bodies are pure but not so when the bodies have sideeffects.  New callbacks We’ve designed the new callbacks with limitations or asterisks in place to deal with the aforementioned issues with transformations. `jax.debug.callback` was designed intentionally to have “undefined” semantics under transformations, i.e. we make no promises about whether these callbacks will be duplicated, dropped, or even called with invalid values. This makes it useful for debugging because it reveals information about the execution of the program. Note that this is more a problem of communication than anything else. We’ve made sure that in our documentation, we explicitly bring up the intent of `jax.debug.callback`. `jax.pure_callback` transforms similarly, but assumes the user does not have sideeffects in their callback. This gives JAX and XLA the right to DCE it, duplicate it, etc., but most importantly preserves the correctness of JAX’s current transformation rules. Basically, in both of the newgen callbacks, we circumvent the problem by either defining it away or by assuming a property of the input function.  Are the current callbacks enough? One way to approach this question would be to answer the question: can we replace all existing uses of host_callback with calls to `jax.debug.callback` and `jax.pure_callback`? First, suggesting people use `jax.debug.callback` as a replacement for `host_callback` is not ideal, since we’ve already defined it as the “undefined” callback function in many ways (it can be transformed arbitrarily). So, the question is, can all `host_callback` usage be replaced with `jax.pure_callback`? Based on conversations with users, we think the answer is no. Effectful host_callbacks are used in a variety of different ways.  Some known example usages that are not supported by `jax.pure_callback` are: * Streaming outputs to host while a JAX computation is executing * Calling out to stateful RPC servers  * Logging to Tensorboard * Progress bars It seems like we should support some sideeffects that have welldefined transformation semantics (where welldefined might mean to error!).  IO Callback So, is it as easy as creating a new `jax.io_callback` (where IO indicates an arbitrary sideeffect)? Well, yes and no. First we need to establish how it behaves under transformations. Here are some properties we’d like to have: * `jax.io_callback` should throw an error when autodifferentiated, since we have no way of differentiating through arbitrary side effects. * `jax.io_callback` should (by default) not be allowed in `vmap`, `pmap`, and `xmap`. Since they are parallel maps (`vmap` pushes parallelism down into the op, `pmap` runs ops on separate threads, and `xmap` does a combination thereof), we can’t safely call the function (there may be race conditions or reordering, for example). However, this is a common enough usecase that maybe we should provide an escape hatch. For example, if an `io_callback` is specified to be “unordered” explicitly (as in calls can be executed out of order except wrt to data dependency), they will be allowed in `vmap` and perhaps “thread safe” to be allowed in `pmap`. * `jax.io_callback` could be allowed in pjit if we run it on a single host + device and do an allgather on its inputs. * When inside some higher order primitives under transformation (e.g. vmapofwhile, vmapofcond), we must throw an error. Alternatively we could use a sequential (loopoverbatch) lowering of while and cond that avoids calling the callbacks extra times at the cost of performance. * `jax.io_callback` should be disallowed in `jax.remat/checkpoint` since it will execute the sideeffect extra times. What we see here is that `io_callback` is largely not compatible with JAX transformations because we have no general way of transforming it. Despite that, it should still cover the usecases mentioned. These limitations can be circumvented more generally, however, through the use of `jax.custom_jvp/vjp/batching/etc`, which will be explicit escape hatches for users. Alternatively, we can also offer `jax.unsafe_callback` which transforms arbitrarily, but also explicitly opts the user into undefined behavior. However, we should strive to offer as few callback options as possible, to keep things simple for users.  Challenge: datadependence in transformations We said that we want to throw errors when `io_callback`s are transformed. However, datadependence throws a wrench in our plans.  Take the following example: ```python def f(x):   jax.io_callback(lambda: print(""hi!""), result_shape=None)   return x jax.vmap(f)(jnp.arange(5)) ``` We mentioned before that this should throw an error. However, it is actually nontrivial to make this function error in current JAX. The reason is that `vmap` operates via datadependency. We only ever run the batching rules for primitives that have batched inputs. The `io_callback` has no inputs at all, so the vmap trace never even sees it. If we were to naively run this code with our aforementioned transformation semantics, we would not get an error because we never actually run the io_callback batching rule. We see a similar issue when transforming with AD. To actually throw errors here, we could change JAX’s tracing machinery. Specifically, we could make these traces “dynamic traces” that capture all primitive calls. However, this is a pretty heavy duty change and should be avoided unless completely necessary. For now we punt on this issue. It seems like there are paths to addressing it and for now we can add `io_callback` to `experimental` until we deal with these issues.  Challenge: sharding callbacks In this implementation, like `jax.debug.callback`, we **maximally shard** the callback. This means it always happens on a single device (and a single host). This limits its power in that users have no control over how the callback happens inside of a multidevice JITted function. However, it is the safest option that enables it working at all. We should explore this open design question on how to expose more flexible shardings. cc:    )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add IO callback," Background In JAX, we usually express computations as compositions of XLA operations but oftentimes we want to call out to Python functions. Historically, the primary mechanism has been `jax.experimental.host_callback` but in making host callback nonexperimental, we’re looking into improving it on all fronts: underlying implementation improvements (via send/recv on TPU, custom call on CPU/GPU), simpler API for users (see `jax.debug.print`), and safety/correctness (`jax.pure_callback`, `jax.debug.callback`).  We’ll discuss a missing newgeneration callback (`io_callback`) and some challenges in its implementation.  Host callback under transformation We’ve introduced two public, nonexperimental Python callback mechanisms, namely `jax.debug.callback` and `jax.pure_callback`, but they aren’t as general purpose as `jax.experimental.host_callback`. `jax.experimental.host_callback.call` takes in a callable, a set of arguments, and metadata about the return type. It then stages out the callable into HLO as a custom call (or uses infeed/outfeed on TPU). Unlike `jax.debug.callback` and `jax.pure_callback`, it has no requirements for the callable. `jax.debug.callback` doesn’t allow callbacks with return values and `jax.pure_callback` does not allow sideeffects in the callable (not enforceable, but it is assumed by JAX and XLA). Why do we have two separate callbacks that each have restrictions? Why not make a `jax.callback` that can handle any manner of callable? The answer is sideeffects. Unlike regular primitives that are staged out, an arbitrary Python callback may not have well defined transformation rules. How do you differentiate through logging to Tensorboard? host_callback defines transformation rules in a particular way and works with basically every transformation and higher order primitive, but as a consequence, it can have counterintuitive behavior when there are sideeffects present.  Let’s look at some brief examples. Although `vmap` is the main instigator, issues can crop up, in general, when transforming higherorder primitives with side effects in them. Example 1: vmap of cond ```python def f(pred):   def true_fun():     hcb.call(lambda _: print(""true""), (), result_shape=None)   def false_fun():     hcb.call(lambda _: print(""false""), (), result_shape=None)   return lax.cond(pred, false_fun, true_fun) jax.vmap(f)(jnp.array([True, True]))  ==> prints  true  false ``` Example 2: vmap of while ```python def check(x, _):   assert np.all(x  raises AssertionError! ``` Why are these weird behaviors happening? Fundamentally, it’s because the vmapofcond and vmapofwhile transformation rules are not aware of sideeffects and assume that their bodies are pure. Lowering a vmapofcond to a select and a vmapofwhile to a padded loop are both semantics preserving when the bodies are pure but not so when the bodies have sideeffects.  New callbacks We’ve designed the new callbacks with limitations or asterisks in place to deal with the aforementioned issues with transformations. `jax.debug.callback` was designed intentionally to have “undefined” semantics under transformations, i.e. we make no promises about whether these callbacks will be duplicated, dropped, or even called with invalid values. This makes it useful for debugging because it reveals information about the execution of the program. Note that this is more a problem of communication than anything else. We’ve made sure that in our documentation, we explicitly bring up the intent of `jax.debug.callback`. `jax.pure_callback` transforms similarly, but assumes the user does not have sideeffects in their callback. This gives JAX and XLA the right to DCE it, duplicate it, etc., but most importantly preserves the correctness of JAX’s current transformation rules. Basically, in both of the newgen callbacks, we circumvent the problem by either defining it away or by assuming a property of the input function.  Are the current callbacks enough? One way to approach this question would be to answer the question: can we replace all existing uses of host_callback with calls to `jax.debug.callback` and `jax.pure_callback`? First, suggesting people use `jax.debug.callback` as a replacement for `host_callback` is not ideal, since we’ve already defined it as the “undefined” callback function in many ways (it can be transformed arbitrarily). So, the question is, can all `host_callback` usage be replaced with `jax.pure_callback`? Based on conversations with users, we think the answer is no. Effectful host_callbacks are used in a variety of different ways.  Some known example usages that are not supported by `jax.pure_callback` are: * Streaming outputs to host while a JAX computation is executing * Calling out to stateful RPC servers  * Logging to Tensorboard * Progress bars It seems like we should support some sideeffects that have welldefined transformation semantics (where welldefined might mean to error!).  IO Callback So, is it as easy as creating a new `jax.io_callback` (where IO indicates an arbitrary sideeffect)? Well, yes and no. First we need to establish how it behaves under transformations. Here are some properties we’d like to have: * `jax.io_callback` should throw an error when autodifferentiated, since we have no way of differentiating through arbitrary side effects. * `jax.io_callback` should (by default) not be allowed in `vmap`, `pmap`, and `xmap`. Since they are parallel maps (`vmap` pushes parallelism down into the op, `pmap` runs ops on separate threads, and `xmap` does a combination thereof), we can’t safely call the function (there may be race conditions or reordering, for example). However, this is a common enough usecase that maybe we should provide an escape hatch. For example, if an `io_callback` is specified to be “unordered” explicitly (as in calls can be executed out of order except wrt to data dependency), they will be allowed in `vmap` and perhaps “thread safe” to be allowed in `pmap`. * `jax.io_callback` could be allowed in pjit if we run it on a single host + device and do an allgather on its inputs. * When inside some higher order primitives under transformation (e.g. vmapofwhile, vmapofcond), we must throw an error. Alternatively we could use a sequential (loopoverbatch) lowering of while and cond that avoids calling the callbacks extra times at the cost of performance. * `jax.io_callback` should be disallowed in `jax.remat/checkpoint` since it will execute the sideeffect extra times. What we see here is that `io_callback` is largely not compatible with JAX transformations because we have no general way of transforming it. Despite that, it should still cover the usecases mentioned. These limitations can be circumvented more generally, however, through the use of `jax.custom_jvp/vjp/batching/etc`, which will be explicit escape hatches for users. Alternatively, we can also offer `jax.unsafe_callback` which transforms arbitrarily, but also explicitly opts the user into undefined behavior. However, we should strive to offer as few callback options as possible, to keep things simple for users.  Challenge: datadependence in transformations We said that we want to throw errors when `io_callback`s are transformed. However, datadependence throws a wrench in our plans.  Take the following example: ```python def f(x):   jax.io_callback(lambda: print(""hi!""), result_shape=None)   return x jax.vmap(f)(jnp.arange(5)) ``` We mentioned before that this should throw an error. However, it is actually nontrivial to make this function error in current JAX. The reason is that `vmap` operates via datadependency. We only ever run the batching rules for primitives that have batched inputs. The `io_callback` has no inputs at all, so the vmap trace never even sees it. If we were to naively run this code with our aforementioned transformation semantics, we would not get an error because we never actually run the io_callback batching rule. We see a similar issue when transforming with AD. To actually throw errors here, we could change JAX’s tracing machinery. Specifically, we could make these traces “dynamic traces” that capture all primitive calls. However, this is a pretty heavy duty change and should be avoided unless completely necessary. For now we punt on this issue. It seems like there are paths to addressing it and for now we can add `io_callback` to `experimental` until we deal with these issues.  Challenge: sharding callbacks In this implementation, like `jax.debug.callback`, we **maximally shard** the callback. This means it always happens on a single device (and a single host). This limits its power in that users have no control over how the callback happens inside of a multidevice JITted function. However, it is the safest option that enables it working at all. We should explore this open design question on how to expose more flexible shardings. cc:    ",2022-12-21T22:08:29Z,useful read pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/13759,"> To actually throw errors here, we could change JAX’s tracing machinery. Specifically, we could make these traces “dynamic traces” that capture all primitive calls. This would break other applications that rely on the current behaviour. For example: ```python pred = assert_unbatched(pred) lax.cond(pred, ...) ``` where `assert_unbatched` is an identity op that raises a tracetime error in its batching rule. In this case, as you can see, to guard against a `cond > select` demotion. `pred` might depend on a loop counter or something that doesn't get batched even under vmap. It's not clear to me that this final example really should be an error. For what kind of zeroargument callback would this be undesirable? Regarding the proliferation of callbacks: perhaps these can be unified under `jax.callback(..., ordered=..., pure=..., effectful=..., io=..., vectorized=...)`?  Obviously with some minimal set of flags, not all of those.","> where assert_unbatched is an identity op that raises a tracetime error in its batching rule. In this case, as you can see, to guard against a cond > select demotion. pred might depend on a loop counter or something that doesn't get batched even under vmap. We could still wrap unbatched values in tracers and the batching rule can just make sure it's unbatched Wrt to a unified callback API, seems like it might be convenient.","Amazing PR message. Thank you. > To actually throw errors here, we could change JAX’s tracing machinery. Specifically, we could make these traces “dynamic traces” that capture all primitive calls. However, this is a pretty heavy duty change and should be avoided unless completely necessary. Actually, that's not necessary: I think we can just have io_callback check the current trace stack when it's bound to see if there are any disallowed (i.e. vmap) stack frames between it and the dynamic trace (i.e. the bottom of the trace stack). Here's one way we could even abstract that a little. We can experiment with that together if you want!","If I'm calling `jax.debug.callback(callback_fn)` somewhere deep inside a call tree that has a `pmap` at the top level, is there any way to see from inside `callback_fn` which device I'm currently in (e.g. to print something only for device 0)?","Thanks for the question! (Though I recommend opening new issues or discussions, not commenting on old PR threads, as that makes things more discoverable.) Just pass the output of `jax.lax.axis_index()` into the callback: ```python  just doing this on CPU for the example import os os.environ['XLA_FLAGS'] = 'xla_force_host_platform_device_count=8' from functools import partial import jax devices = jax.devices()   don't do this in the callback, it might hang (jax.pmap, axis_name='i') def f(_):   jax.debug.callback(callback, jax.lax.axis_index('i')) def callback(idx):   print(f""hello from axis index {idx}, meaning device {devices[idx]}"") f(jax.numpy.zeros(8)) ``` ``` hello from axis index 5, meaning device TFRT_CPU_5 hello from axis index 3, meaning device TFRT_CPU_3 hello from axis index 7, meaning device TFRT_CPU_7 hello from axis index 2, meaning device TFRT_CPU_2 hello from axis index 6, meaning device TFRT_CPU_6 hello from axis index 0, meaning device TFRT_CPU_0 hello from axis index 1, meaning device TFRT_CPU_1 hello from axis index 4, meaning device TFRT_CPU_4 ```","  Hey folks, sorry for picking a random issue, but I am trying to understand some of the underlying decisions/direction and this issue has a lot of relevant details in the description already. I have a few questions: 1. What is the rationale for moving to custom calls, send/recv as opposed to infeed/outfeed? 2. Is the plan to eventually remove infeed/outfeed from XLA side, given that the PjRt Plugin API doesn't support it and Jax doesn't use it? 3. In order for an external plugin (such as jaxmetal) to support callbacks, it would need to define custom lowering for the callbacks primitives, using custom call or send/recv. Or would that be centralized in jaxlib plugins? I would really appreciate a clarification on these, thank you :)"
709,"以下是一个github上的jax下的一个issue, 标题是(add remat tutorial docs)， 内容是 (We've never had a tutorial on doing autodiff checkpointing (aka rematerialization) with `jax.checkpoint` / `jax.remat`! And it's subtle enough to need an explanation. Also, this is the first real documentation for _custom AD checkpointing policies_, which can be a key ingredient in getting the highest model FLOP utilization for Transformer LLMs. We've had the feature for more than a year, but kept it secret by not documenting it :P TODO:  [x] update in light of https://github.com/google/jax/pull/13859)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,add remat tutorial docs,"We've never had a tutorial on doing autodiff checkpointing (aka rematerialization) with `jax.checkpoint` / `jax.remat`! And it's subtle enough to need an explanation. Also, this is the first real documentation for _custom AD checkpointing policies_, which can be a key ingredient in getting the highest model FLOP utilization for Transformer LLMs. We've had the feature for more than a year, but kept it secret by not documenting it :P TODO:  [x] update in light of https://github.com/google/jax/pull/13859",2022-12-21T18:43:17Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13756
929,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Refactoring of shape_poly_test.)， 内容是 (This all started because I noticed that the old self.CheckShapePolymorphism was not running the converted function and would only do the conversion in TF graph mode. Then I realized that there were multiple ways of specifying and running the tests: _make_harness, vmap harnesses, self.CheckShapePolymorphism. This PR unifies all test harnesses under a new PolyHarness class, with new documentation. There is a helper function check_shape_poly that simply wraps PolyHarness. Since the new tests exercise the jax2tf more deeply, especially in TF eager model, I have found 3 bugs. One is fixed here, in the jax2tf._assert_matching_abstract_shape. Two others are deferred (and a couple or tests are skipped here).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Refactoring of shape_poly_test.,"This all started because I noticed that the old self.CheckShapePolymorphism was not running the converted function and would only do the conversion in TF graph mode. Then I realized that there were multiple ways of specifying and running the tests: _make_harness, vmap harnesses, self.CheckShapePolymorphism. This PR unifies all test harnesses under a new PolyHarness class, with new documentation. There is a helper function check_shape_poly that simply wraps PolyHarness. Since the new tests exercise the jax2tf more deeply, especially in TF eager model, I have found 3 bugs. One is fixed here, in the jax2tf._assert_matching_abstract_shape. Two others are deferred (and a couple or tests are skipped here).",2022-12-19T07:40:49Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13707
13268,"以下是一个github上的jax下的一个issue, 标题是(Multiple subprocesses cannot see GPU)， 内容是 ( Description I am having difficulty getting Jax to share a GPU backend across subprocesses on a SLURM job. The experienced behavior is that whichever process binds with the GPU first is able to work correctly, and all other processes cannot a GPU backend. Below is a minimal example. In it I've experimented with the main process and varying amounts of subprocesses attempting to bind with the single GPU. I've tried various permutations of XLA flags for memory preallocation or visible devices. ```python import os import time import jax import launchpad as lp import tensorflow as tf from absl import app from launchpad.nodes.python.local_multi_processing import PythonProcess class DeviceTester:     def test(self):         print(os.environ)         print(""Backends: "", jax.default_backend())         print(""Devices: "", jax.devices())     def run(self):         time.sleep(5)         lp.stop() def _build_test_node():     return lp.CourierNode(DeviceTester) def main(_):      Test independent of Launchpad.     print(""\n\nLocal"")     print(os.environ)     test = DeviceTester()     test.test()      Test GPU accessibility on Launchpad.     print(""\n\nLaunchpad"")     program = lp.Program(name=""experiment"")     handles = []     with program.group(""test_cpu""):         handles.append(program.add_node(_build_test_node()))     with program.group(""test_gpu""):         handles.append(program.add_node(_build_test_node()))         handles.append(program.add_node(_build_test_node()))     lp.launch(         program,         launch_type=lp.LaunchType.LOCAL_MULTI_PROCESSING,         terminal=""current_terminal"",         local_resources={             ""test_cpu"": PythonProcess(                 env={                     ""CUDA_VISIBLE_DEVICES"": """",                     ""JAX_PLATFORM_NAME"": ""cpu"",                 }             ),             ""test_gpu"": PythonProcess(                 env={                     ""CUDA_VISIBLE_DEVICES"": ""0"",                     ""XLA_PYTHON_CLIENT_MEM_FRACTION"": "".2"",                     ""XLA_PYTHON_CLIENT_PREALLOCATE"": ""false"",                     ""JAX_PLATFORM_NAME"": ""gpu"",                 }             ),         },     )     for handle in handles:         handle.dereference().test() if __name__ == ""__main__"":      Provide access to jax_backend_target and jax_xla_backend flags.     jax.config.config_with_absl()      Binary should use CPU     jax.config.update(""jax_platform_name"", ""cpu"")     tf.config.experimental.set_visible_devices([], ""GPU"")     app.run(main) ``` ``` The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""launchpad_gpu_test.py"", line 80, in      app.run(main)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""launchpad_gpu_test.py"", line 70, in main     handle.dereference().test()   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/courier/python/client.py"", line 52, in inner_function     raise translate_status(e.status) from e pybind11_abseil.status.StatusNotOk: Python exception was raised on the server: Traceback (most recent call last):   File ""launchpad_gpu_test.py"", line 14, in test     print(""Backends: "", jax.default_backend())   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 490, in default_backend     return get_backend(None).platform   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 427, in get_backend     return _get_backend_uncached(platform)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 413, in _get_backend_uncached     platform = canonicalize_platform(platform)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 294, in canonicalize_platform     raise RuntimeError(f""Unknown backend: '{platform}' requested, but no "" RuntimeError: Unknown backend: 'gpu' requested, but no platforms that are instances of gpu are present. Platforms present are: interpreter,cpu ``` Both GPU nodes have the same env variables, which are: ``` [test_gpu/1] environ({'CONDA_SHLVL': '2', 'LD_LIBRARY_PATH': '/home/mxsmith/.conda/envs/model38/lib', 'LS_COLORS': 'no=00:di=34;01:tw=34;01:ow=34;01:fi=00:ln=00:pi=00:so=00:bd=00:cd=00:or=00:mi=00:ex=00:*.sh=31:*.exe=31:*.bat=31', 'CONDA_EXE': '/sw/pkgs/arc/python3.9anaconda/2021.11/bin/conda', 'SRUN_DEBUG': '3', 'SLURM_STEP_ID': '0', 'SLURM_STEP_GPUS': '0', 'SLURM_NODEID': '0', 'SLURM_TASK_PID': '827752', 'HTTP_PROXY': 'http://proxy.arcts.umich.edu:3128/', 'SSH_CONNECTION': '141.211.21.82 51992 141.211.192.38 22', 'SLURM_PRIO_PROCESS': '0', 'SLURM_CPU_BIND_VERBOSE': 'quiet', 'IEX_TOKEN': 'pk_6cd11420a64b4d5a856ac31281418f38', 'LANG': 'en_US.UTF8', 'SLURM_SUBMIT_DIR': '/home/mxsmith', 'HISTCONTROL': 'ignoreboth:erasedups', 'HOSTNAME': 'gl1520.arcts.umich.edu', 'OLDPWD': '/home/mxsmith', 'SLURM_STEPID': '0', 'SLURM_SRUN_COMM_HOST': '141.211.192.38', 'EDITOR': 'emacs', 'SLURM_DISTRIBUTION': 'cyclic', 'ROCR_VISIBLE_DEVICES': '0', 'CONDA_PREFIX': '/home/mxsmith/.conda/envs/model38', 'SQUEUE_FORMAT': '%.18i %.9P %40j %.8u %.2t %.10M %.20R', 'SLURM_PROCID': '0', 'SLURM_JOB_GID': '99464869', 'SLURM_CPU_BIND': 'quiet,mask_cpu:0x00000001', 'SLURMD_NODENAME': 'gl1520', 'GIT_EDITOR': 'emacs', 'SLURM_TASKS_PER_NODE': '1', 'S_COLORS': 'auto', '_CE_M': '', 'XLA_PYTHON_CLIENT_PREALLOCATE': 'false', 'TF2_BEHAVIOR': '1', 'XDG_SESSION_ID': '11093', 'SLURM_NNODES': '1', 'USER': 'mxsmith', 'SLURM_LAUNCH_NODE_IPADDR': '141.211.192.38', 'CONDA_PREFIX_1': '/sw/pkgs/arc/python3.9anaconda/2021.11', 'SLURM_STEP_TASKS_PER_NODE': '1', 'MATPLOTLIBRC': '/home/mxsmith/profile/matplotlib', 'FTP_PROXY': 'http://proxy.arcts.umich.edu:3128/', 'PWD': '/home/mxsmith/projects', 'SSH_ASKPASS': '/usr/libexec/openssh/gnomesshaskpass', 'SLURM_JOB_NODELIST': 'gl1520', 'HOME': '/home/mxsmith', 'SLURM_CLUSTER_NAME': 'greatlakes', 'CONDA_PYTHON_EXE': '/sw/pkgs/arc/python3.9anaconda/2021.11/bin/python', 'SLURM_NODELIST': 'gl1520', 'SLURM_GPUS_ON_NODE': '1', 'SSH_CLIENT': '141.211.21.82 51992 22', 'LMOD_VERSION': '8.6.14', 'SLURM_NTASKS': '1', 'TMUX': '/tmp/tmux99464869/default,827956,0', 'rsync_proxy': 'proxy.arcts.umich.edu:3128', 'SLURM_UMASK': '0002', 'https_proxy': 'http://proxy.arcts.umich.edu:3128/', 'KRB5CCNAME': 'FILE:/tmp/krb5cc_99464869_TngfOC', 'TF_CPP_MIN_LOG_LEVEL': '1', 'SLURM_JOB_CPUS_PER_NODE': '4', 'BASH_ENV': '/sw/lmod/lmod/init/bash', 'XDG_DATA_DIRS': '/home/mxsmith/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share', 'AUTOJUMP_ERROR_PATH': '/home/mxsmith/.local/share/autojump/errors.log', 'SLURM_TOPOLOGY_ADDR': 'gl1520', 'http_proxy': 'http://proxy.arcts.umich.edu:3128/', '_CE_CONDA': '', 'SLURM_WORKING_CLUSTER': 'greatlakes:glctld:6817:9472:109', 'SLURM_STEP_NODELIST': 'gl1520', 'SLURM_JOB_NAME': 'bash', 'SLURM_SRUN_COMM_PORT': '60207', 'TMPDIR': '/tmp', 'LMOD_sys': 'Linux', 'SLURM_JOBID': '45908601', 'JAX_PLATFORM_NAME': 'gpu', 'SLURM_CONF': '/var/spool/slurmd.spool/confcache/slurm.conf', 'LMOD_AVAIL_STYLE': 'grouped', 'no_proxy': 'localhost,127.0.0.1,.localdomain,.umich.edu', 'LMOD_ROOT': '/sw/lmod', 'SLURM_JOB_QOS': 'normal', 'SLURM_TOPOLOGY_ADDR_PATTERN': 'node', 'CONDA_PROMPT_MODIFIER': '(model38) ', 'SSH_TTY': '/dev/pts/80', 'NO_PROXY': 'localhost,127.0.0.1,.localdomain,.umich.edu', 'MAIL': '/var/spool/mail/mxsmith', 'HTTPS_PROXY': 'http://proxy.arcts.umich.edu:3128/', 'SLURM_CPUS_ON_NODE': '4', 'XLA_PYTHON_CLIENT_MEM_FRACTION': '.2', 'VISUAL': 'emacs', 'SLURM_JOB_NUM_NODES': '1', 'AUTOJUMP_SOURCED': '1', 'SHELL': '/bin/bash', 'TERM': 'xterm256color', 'SLURM_JOB_UID': '99464869', 'SLURM_JOB_PARTITION': 'spgpu', 'SLURM_PTY_WIN_ROW': '49', 'SLURM_CPU_BIND_LIST': '0x00000001', 'SLURM_JOB_USER': 'mxsmith', 'CUDA_VISIBLE_DEVICES': '0', 'SLURM_PTY_WIN_COL': '97', 'TMUX_PANE': '%3', 'SLURM_NPROCS': '1', 'SHLVL': '3', 'SLURM_SUBMIT_HOST': 'gllogin1.arcts.umich.edu', 'SLURM_JOB_ACCOUNT': 'wellman0', 'MANPATH': '/sw/lmod/lmod/share/man:/usr/local/share/man:/usr/share/man:/opt/ddn/ime/share/man:/opt/ddn/ime/share/man:/opt/slurm/share/man/:/opt/TurboVNC/man/:/opt/ddn/ime/share/man:/opt/ddn/ime/share/man', 'SLURM_STEP_LAUNCHER_PORT': '60207', 'MODULEPATH': '/sw/lmod/lmod/modulefiles/Core:/sw/modules/Core:/sw/modules/Collections', 'SLURM_PTY_PORT': '60206', 'SLURM_GTIDS': '0', 'LOGNAME': 'mxsmith', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/99464869/bus', 'XDG_RUNTIME_DIR': '/run/user/99464869', 'MODULEPATH_ROOT': '/sw/modules', 'LMOD_PACKAGE_PATH': '/sw/lmod', 'PATH': '/home/mxsmith/software/bin:/home/mxsmith/software/bin:/home/mxsmith/.conda/envs/model38/bin:/sw/pkgs/arc/python3.9anaconda/2021.11/condabin:/home/mxsmith/software/bin:/opt/TurboVNC/bin:/opt/slurm/bin:/opt/slurm/sbin:/sw/pkgs/arc/usertools/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/lpp/mmfs/bin:/opt/ddn/ime/bin:/home/mxsmith/anaconda/bin:/home/mxsmith/software/gambit15.1.1:/home/mxsmith/.local/bin:/home/mxsmith/bin:/opt/ddn/ime/bin:/home/mxsmith/anaconda/bin:/home/mxsmith/software/gambit15.1.1:/opt/ddn/ime/bin:/home/mxsmith/anaconda/bin:/home/mxsmith/software/gambit15.1.1:/home/mxsmith/.local/bin:/home/mxsmith/bin', 'SLURM_JOB_ID': '45908601', 'SLURM_CPU_BIND_TYPE': 'mask_cpu:', 'SLURM_STEP_NUM_TASKS': '1', 'MODULESHOME': '/sw/lmod/lmod', 'CONDA_DEFAULT_ENV': 'model38', 'LMOD_SETTARG_FULL_SUPPORT': 'no', 'HISTSIZE': '1000', 'LMOD_PKG': '/sw/lmod/lmod', 'CLUSTER_NAME': 'greatlakes', 'SLURM_STEP_NUM_NODES': '1', 'ftp_proxy': 'http://proxy.arcts.umich.edu:3128/', 'RSYNC_PROXY': 'proxy.arcts.umich.edu:3128', 'LMOD_CMD': '/sw/lmod/lmod/libexec/lmod', 'SLURM_LOCALID': '0', 'GPU_DEVICE_ORDINAL': '0', 'LESSOPEN': '/usr/bin/lesspipe.sh %s', 'LMOD_DIR': '/sw/lmod/lmod/libexec', 'BASH_FUNC_module%%': '() {  local __lmod_my_status;\n local __lmod_sh_dbg;\n if [ z ""${LMOD_SH_DBG_ON+x}"" ]; then\n case ""$"" in \n *v*x*)\n __lmod_sh_dbg=\'vx\'\n ;;\n *v*)\n __lmod_sh_dbg=\'v\'\n ;;\n *x*)\n __lmod_sh_dbg=\'x\'\n ;;\n esac;\n fi;\n if [ n ""${__lmod_sh_dbg:}"" ]; then\n set +$__lmod_sh_dbg;\n echo ""Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod\'s output"" 1>&2;\n fi;\n eval ""$($LMOD_CMD bash ""$@"")"" && eval $(${LMOD_SETTARG_CMD::} s sh);\n __lmod_my_status=$?;\n if [ n ""${__lmod_sh_dbg:}"" ]; then\n echo ""Shell debugging restarted"" 1>&2;\n set $__lmod_sh_dbg;\n fi;\n return $__lmod_my_status\n}', 'BASH_FUNC_ml%%': '() {  eval ""$($LMOD_DIR/ml_cmd ""$@"")""\n}'}) ``` ``` tensorflow==2.8.3 jax @ file:///home/conda/feedstock_root/build_artifacts/jax_1671027717961/work jaxlib==0.4.1+cuda11.cudnn86 dmlaunchpad==0.5.2p ```  Additional debugging If you further print out: ```python from jax._src.config import flags FLAGS = flags.FLAGS print(FLAGS.jax_cuda_visible_devices) from jax._src import distributed print(distributed.global_state.client) print(distributed.global_state.service) print(distributed.global_state.process_id) ``` Both have the same settings: ``` [test_gpu/1] all [test_gpu/1] None [test_gpu/1] None [test_gpu/1] 0 [test_gpu/0] all [test_gpu/0] None [test_gpu/0] None [test_gpu/0] 0 ``` As far as I can tell, all of the system's settings are the same during the handoff to XLA. If you add this to the nodes you can see that cuda backend is missing from the platform processing ```python from jax._src.lib import xla_bridge print(""Backends: "", xla_bridge.backends()) ``` ``` [test_gpu/0] Backends:  {'interpreter': , 'cpu': , 'cuda': } [test_gpu/1] Backends:  {'interpreter': , 'cpu': } ``` If you also add: ```python from jax.config import config print(config.jax_platforms) ``` They're both `None`, prompting all of the backend factories to be run. ``` [test_gpu/1] {'interpreter': (, 100), 'cpu': (functools.partial(, use_tfrt=True), 0), 'tpu_driver': (, 100), 'cuda': (functools.partial(, platform_name='cuda', visible_devices_flag='jax_cuda_visible_devices'), 200), 'rocm': (functools.partial(, platform_name='rocm', visible_devices_flag='jax_rocm_visible_devices'), 200), 'tpu': (functools.partial(, timer_secs=60.0), 300), 'plugin': (, 400)} [test_gpu/0] {'interpreter': (, 100), 'cpu': (functools.partial(, use_tfrt=True), 0), 'tpu_driver': (, 100), 'cuda': (functools.partial(, platform_name='cuda', visible_devices_flag='jax_cuda_visible_devices'), 200), 'rocm': (functools.partial(, platform_name='rocm', visible_devices_flag='jax_rocm_visible_devices'), 200), 'tpu': (functools.partial(, timer_secs=60.0), 300), 'plugin': (, 400)} ```  What jax/jaxlib version are you using? ``` jax @ file:///home/conda/feedstock_root/build_artifacts/jax_1671027717961/work  jaxlib==0.4.1+cuda11.cudnn86 ```  Which accelerator(s) are you using? GPU  Additional system info Red Hat Enterprise Linux 8.4 (Ootpa)  NVIDIA GPU info NVIDIASMI 510.73.08    Driver Version: 510.73.08    CUDA Version: 11.6     NVIDIA A40)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Multiple subprocesses cannot see GPU," Description I am having difficulty getting Jax to share a GPU backend across subprocesses on a SLURM job. The experienced behavior is that whichever process binds with the GPU first is able to work correctly, and all other processes cannot a GPU backend. Below is a minimal example. In it I've experimented with the main process and varying amounts of subprocesses attempting to bind with the single GPU. I've tried various permutations of XLA flags for memory preallocation or visible devices. ```python import os import time import jax import launchpad as lp import tensorflow as tf from absl import app from launchpad.nodes.python.local_multi_processing import PythonProcess class DeviceTester:     def test(self):         print(os.environ)         print(""Backends: "", jax.default_backend())         print(""Devices: "", jax.devices())     def run(self):         time.sleep(5)         lp.stop() def _build_test_node():     return lp.CourierNode(DeviceTester) def main(_):      Test independent of Launchpad.     print(""\n\nLocal"")     print(os.environ)     test = DeviceTester()     test.test()      Test GPU accessibility on Launchpad.     print(""\n\nLaunchpad"")     program = lp.Program(name=""experiment"")     handles = []     with program.group(""test_cpu""):         handles.append(program.add_node(_build_test_node()))     with program.group(""test_gpu""):         handles.append(program.add_node(_build_test_node()))         handles.append(program.add_node(_build_test_node()))     lp.launch(         program,         launch_type=lp.LaunchType.LOCAL_MULTI_PROCESSING,         terminal=""current_terminal"",         local_resources={             ""test_cpu"": PythonProcess(                 env={                     ""CUDA_VISIBLE_DEVICES"": """",                     ""JAX_PLATFORM_NAME"": ""cpu"",                 }             ),             ""test_gpu"": PythonProcess(                 env={                     ""CUDA_VISIBLE_DEVICES"": ""0"",                     ""XLA_PYTHON_CLIENT_MEM_FRACTION"": "".2"",                     ""XLA_PYTHON_CLIENT_PREALLOCATE"": ""false"",                     ""JAX_PLATFORM_NAME"": ""gpu"",                 }             ),         },     )     for handle in handles:         handle.dereference().test() if __name__ == ""__main__"":      Provide access to jax_backend_target and jax_xla_backend flags.     jax.config.config_with_absl()      Binary should use CPU     jax.config.update(""jax_platform_name"", ""cpu"")     tf.config.experimental.set_visible_devices([], ""GPU"")     app.run(main) ``` ``` The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""launchpad_gpu_test.py"", line 80, in      app.run(main)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""launchpad_gpu_test.py"", line 70, in main     handle.dereference().test()   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/courier/python/client.py"", line 52, in inner_function     raise translate_status(e.status) from e pybind11_abseil.status.StatusNotOk: Python exception was raised on the server: Traceback (most recent call last):   File ""launchpad_gpu_test.py"", line 14, in test     print(""Backends: "", jax.default_backend())   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 490, in default_backend     return get_backend(None).platform   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 427, in get_backend     return _get_backend_uncached(platform)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 413, in _get_backend_uncached     platform = canonicalize_platform(platform)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 294, in canonicalize_platform     raise RuntimeError(f""Unknown backend: '{platform}' requested, but no "" RuntimeError: Unknown backend: 'gpu' requested, but no platforms that are instances of gpu are present. Platforms present are: interpreter,cpu ``` Both GPU nodes have the same env variables, which are: ``` [test_gpu/1] environ({'CONDA_SHLVL': '2', 'LD_LIBRARY_PATH': '/home/mxsmith/.conda/envs/model38/lib', 'LS_COLORS': 'no=00:di=34;01:tw=34;01:ow=34;01:fi=00:ln=00:pi=00:so=00:bd=00:cd=00:or=00:mi=00:ex=00:*.sh=31:*.exe=31:*.bat=31', 'CONDA_EXE': '/sw/pkgs/arc/python3.9anaconda/2021.11/bin/conda', 'SRUN_DEBUG': '3', 'SLURM_STEP_ID': '0', 'SLURM_STEP_GPUS': '0', 'SLURM_NODEID': '0', 'SLURM_TASK_PID': '827752', 'HTTP_PROXY': 'http://proxy.arcts.umich.edu:3128/', 'SSH_CONNECTION': '141.211.21.82 51992 141.211.192.38 22', 'SLURM_PRIO_PROCESS': '0', 'SLURM_CPU_BIND_VERBOSE': 'quiet', 'IEX_TOKEN': 'pk_6cd11420a64b4d5a856ac31281418f38', 'LANG': 'en_US.UTF8', 'SLURM_SUBMIT_DIR': '/home/mxsmith', 'HISTCONTROL': 'ignoreboth:erasedups', 'HOSTNAME': 'gl1520.arcts.umich.edu', 'OLDPWD': '/home/mxsmith', 'SLURM_STEPID': '0', 'SLURM_SRUN_COMM_HOST': '141.211.192.38', 'EDITOR': 'emacs', 'SLURM_DISTRIBUTION': 'cyclic', 'ROCR_VISIBLE_DEVICES': '0', 'CONDA_PREFIX': '/home/mxsmith/.conda/envs/model38', 'SQUEUE_FORMAT': '%.18i %.9P %40j %.8u %.2t %.10M %.20R', 'SLURM_PROCID': '0', 'SLURM_JOB_GID': '99464869', 'SLURM_CPU_BIND': 'quiet,mask_cpu:0x00000001', 'SLURMD_NODENAME': 'gl1520', 'GIT_EDITOR': 'emacs', 'SLURM_TASKS_PER_NODE': '1', 'S_COLORS': 'auto', '_CE_M': '', 'XLA_PYTHON_CLIENT_PREALLOCATE': 'false', 'TF2_BEHAVIOR': '1', 'XDG_SESSION_ID': '11093', 'SLURM_NNODES': '1', 'USER': 'mxsmith', 'SLURM_LAUNCH_NODE_IPADDR': '141.211.192.38', 'CONDA_PREFIX_1': '/sw/pkgs/arc/python3.9anaconda/2021.11', 'SLURM_STEP_TASKS_PER_NODE': '1', 'MATPLOTLIBRC': '/home/mxsmith/profile/matplotlib', 'FTP_PROXY': 'http://proxy.arcts.umich.edu:3128/', 'PWD': '/home/mxsmith/projects', 'SSH_ASKPASS': '/usr/libexec/openssh/gnomesshaskpass', 'SLURM_JOB_NODELIST': 'gl1520', 'HOME': '/home/mxsmith', 'SLURM_CLUSTER_NAME': 'greatlakes', 'CONDA_PYTHON_EXE': '/sw/pkgs/arc/python3.9anaconda/2021.11/bin/python', 'SLURM_NODELIST': 'gl1520', 'SLURM_GPUS_ON_NODE': '1', 'SSH_CLIENT': '141.211.21.82 51992 22', 'LMOD_VERSION': '8.6.14', 'SLURM_NTASKS': '1', 'TMUX': '/tmp/tmux99464869/default,827956,0', 'rsync_proxy': 'proxy.arcts.umich.edu:3128', 'SLURM_UMASK': '0002', 'https_proxy': 'http://proxy.arcts.umich.edu:3128/', 'KRB5CCNAME': 'FILE:/tmp/krb5cc_99464869_TngfOC', 'TF_CPP_MIN_LOG_LEVEL': '1', 'SLURM_JOB_CPUS_PER_NODE': '4', 'BASH_ENV': '/sw/lmod/lmod/init/bash', 'XDG_DATA_DIRS': '/home/mxsmith/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share', 'AUTOJUMP_ERROR_PATH': '/home/mxsmith/.local/share/autojump/errors.log', 'SLURM_TOPOLOGY_ADDR': 'gl1520', 'http_proxy': 'http://proxy.arcts.umich.edu:3128/', '_CE_CONDA': '', 'SLURM_WORKING_CLUSTER': 'greatlakes:glctld:6817:9472:109', 'SLURM_STEP_NODELIST': 'gl1520', 'SLURM_JOB_NAME': 'bash', 'SLURM_SRUN_COMM_PORT': '60207', 'TMPDIR': '/tmp', 'LMOD_sys': 'Linux', 'SLURM_JOBID': '45908601', 'JAX_PLATFORM_NAME': 'gpu', 'SLURM_CONF': '/var/spool/slurmd.spool/confcache/slurm.conf', 'LMOD_AVAIL_STYLE': 'grouped', 'no_proxy': 'localhost,127.0.0.1,.localdomain,.umich.edu', 'LMOD_ROOT': '/sw/lmod', 'SLURM_JOB_QOS': 'normal', 'SLURM_TOPOLOGY_ADDR_PATTERN': 'node', 'CONDA_PROMPT_MODIFIER': '(model38) ', 'SSH_TTY': '/dev/pts/80', 'NO_PROXY': 'localhost,127.0.0.1,.localdomain,.umich.edu', 'MAIL': '/var/spool/mail/mxsmith', 'HTTPS_PROXY': 'http://proxy.arcts.umich.edu:3128/', 'SLURM_CPUS_ON_NODE': '4', 'XLA_PYTHON_CLIENT_MEM_FRACTION': '.2', 'VISUAL': 'emacs', 'SLURM_JOB_NUM_NODES': '1', 'AUTOJUMP_SOURCED': '1', 'SHELL': '/bin/bash', 'TERM': 'xterm256color', 'SLURM_JOB_UID': '99464869', 'SLURM_JOB_PARTITION': 'spgpu', 'SLURM_PTY_WIN_ROW': '49', 'SLURM_CPU_BIND_LIST': '0x00000001', 'SLURM_JOB_USER': 'mxsmith', 'CUDA_VISIBLE_DEVICES': '0', 'SLURM_PTY_WIN_COL': '97', 'TMUX_PANE': '%3', 'SLURM_NPROCS': '1', 'SHLVL': '3', 'SLURM_SUBMIT_HOST': 'gllogin1.arcts.umich.edu', 'SLURM_JOB_ACCOUNT': 'wellman0', 'MANPATH': '/sw/lmod/lmod/share/man:/usr/local/share/man:/usr/share/man:/opt/ddn/ime/share/man:/opt/ddn/ime/share/man:/opt/slurm/share/man/:/opt/TurboVNC/man/:/opt/ddn/ime/share/man:/opt/ddn/ime/share/man', 'SLURM_STEP_LAUNCHER_PORT': '60207', 'MODULEPATH': '/sw/lmod/lmod/modulefiles/Core:/sw/modules/Core:/sw/modules/Collections', 'SLURM_PTY_PORT': '60206', 'SLURM_GTIDS': '0', 'LOGNAME': 'mxsmith', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/99464869/bus', 'XDG_RUNTIME_DIR': '/run/user/99464869', 'MODULEPATH_ROOT': '/sw/modules', 'LMOD_PACKAGE_PATH': '/sw/lmod', 'PATH': '/home/mxsmith/software/bin:/home/mxsmith/software/bin:/home/mxsmith/.conda/envs/model38/bin:/sw/pkgs/arc/python3.9anaconda/2021.11/condabin:/home/mxsmith/software/bin:/opt/TurboVNC/bin:/opt/slurm/bin:/opt/slurm/sbin:/sw/pkgs/arc/usertools/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/lpp/mmfs/bin:/opt/ddn/ime/bin:/home/mxsmith/anaconda/bin:/home/mxsmith/software/gambit15.1.1:/home/mxsmith/.local/bin:/home/mxsmith/bin:/opt/ddn/ime/bin:/home/mxsmith/anaconda/bin:/home/mxsmith/software/gambit15.1.1:/opt/ddn/ime/bin:/home/mxsmith/anaconda/bin:/home/mxsmith/software/gambit15.1.1:/home/mxsmith/.local/bin:/home/mxsmith/bin', 'SLURM_JOB_ID': '45908601', 'SLURM_CPU_BIND_TYPE': 'mask_cpu:', 'SLURM_STEP_NUM_TASKS': '1', 'MODULESHOME': '/sw/lmod/lmod', 'CONDA_DEFAULT_ENV': 'model38', 'LMOD_SETTARG_FULL_SUPPORT': 'no', 'HISTSIZE': '1000', 'LMOD_PKG': '/sw/lmod/lmod', 'CLUSTER_NAME': 'greatlakes', 'SLURM_STEP_NUM_NODES': '1', 'ftp_proxy': 'http://proxy.arcts.umich.edu:3128/', 'RSYNC_PROXY': 'proxy.arcts.umich.edu:3128', 'LMOD_CMD': '/sw/lmod/lmod/libexec/lmod', 'SLURM_LOCALID': '0', 'GPU_DEVICE_ORDINAL': '0', 'LESSOPEN': '/usr/bin/lesspipe.sh %s', 'LMOD_DIR': '/sw/lmod/lmod/libexec', 'BASH_FUNC_module%%': '() {  local __lmod_my_status;\n local __lmod_sh_dbg;\n if [ z ""${LMOD_SH_DBG_ON+x}"" ]; then\n case ""$"" in \n *v*x*)\n __lmod_sh_dbg=\'vx\'\n ;;\n *v*)\n __lmod_sh_dbg=\'v\'\n ;;\n *x*)\n __lmod_sh_dbg=\'x\'\n ;;\n esac;\n fi;\n if [ n ""${__lmod_sh_dbg:}"" ]; then\n set +$__lmod_sh_dbg;\n echo ""Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod\'s output"" 1>&2;\n fi;\n eval ""$($LMOD_CMD bash ""$@"")"" && eval $(${LMOD_SETTARG_CMD::} s sh);\n __lmod_my_status=$?;\n if [ n ""${__lmod_sh_dbg:}"" ]; then\n echo ""Shell debugging restarted"" 1>&2;\n set $__lmod_sh_dbg;\n fi;\n return $__lmod_my_status\n}', 'BASH_FUNC_ml%%': '() {  eval ""$($LMOD_DIR/ml_cmd ""$@"")""\n}'}) ``` ``` tensorflow==2.8.3 jax @ file:///home/conda/feedstock_root/build_artifacts/jax_1671027717961/work jaxlib==0.4.1+cuda11.cudnn86 dmlaunchpad==0.5.2p ```  Additional debugging If you further print out: ```python from jax._src.config import flags FLAGS = flags.FLAGS print(FLAGS.jax_cuda_visible_devices) from jax._src import distributed print(distributed.global_state.client) print(distributed.global_state.service) print(distributed.global_state.process_id) ``` Both have the same settings: ``` [test_gpu/1] all [test_gpu/1] None [test_gpu/1] None [test_gpu/1] 0 [test_gpu/0] all [test_gpu/0] None [test_gpu/0] None [test_gpu/0] 0 ``` As far as I can tell, all of the system's settings are the same during the handoff to XLA. If you add this to the nodes you can see that cuda backend is missing from the platform processing ```python from jax._src.lib import xla_bridge print(""Backends: "", xla_bridge.backends()) ``` ``` [test_gpu/0] Backends:  {'interpreter': , 'cpu': , 'cuda': } [test_gpu/1] Backends:  {'interpreter': , 'cpu': } ``` If you also add: ```python from jax.config import config print(config.jax_platforms) ``` They're both `None`, prompting all of the backend factories to be run. ``` [test_gpu/1] {'interpreter': (, 100), 'cpu': (functools.partial(, use_tfrt=True), 0), 'tpu_driver': (, 100), 'cuda': (functools.partial(, platform_name='cuda', visible_devices_flag='jax_cuda_visible_devices'), 200), 'rocm': (functools.partial(, platform_name='rocm', visible_devices_flag='jax_rocm_visible_devices'), 200), 'tpu': (functools.partial(, timer_secs=60.0), 300), 'plugin': (, 400)} [test_gpu/0] {'interpreter': (, 100), 'cpu': (functools.partial(, use_tfrt=True), 0), 'tpu_driver': (, 100), 'cuda': (functools.partial(, platform_name='cuda', visible_devices_flag='jax_cuda_visible_devices'), 200), 'rocm': (functools.partial(, platform_name='rocm', visible_devices_flag='jax_rocm_visible_devices'), 200), 'tpu': (functools.partial(, timer_secs=60.0), 300), 'plugin': (, 400)} ```  What jax/jaxlib version are you using? ``` jax @ file:///home/conda/feedstock_root/build_artifacts/jax_1671027717961/work  jaxlib==0.4.1+cuda11.cudnn86 ```  Which accelerator(s) are you using? GPU  Additional system info Red Hat Enterprise Linux 8.4 (Ootpa)  NVIDIA GPU info NVIDIASMI 510.73.08    Driver Version: 510.73.08    CUDA Version: 11.6     NVIDIA A40",2022-12-16T19:43:06Z,bug NVIDIA GPU,closed,0,10,https://github.com/jax-ml/jax/issues/13687,My guess is that this is an issue with the SLURM config or how you call it. What is the slurm command that you use? nvidiasmi show all the GPUs visibles. It doesn't mean you have access to them from memory.,Did you see this documentation: https://jax.readthedocs.io/en/latest/multi_process.html ?,"My SLURM command was: `srun pty gres=gpu:1 cpuspergpu=4 mempercpu=10g time=001:00 /bin/bash` I did miss that document, I'll give it a pass now, cheers!","Ah, that document is about `pmap`, I'm not trying to distribute the workload in that nature. I'm trying to have a learner node have a GPU, while actor nodes do not need said GPU. ","If you request only 1 node with 1 GPUs and many CPUs, you can create a bash script that dispatch like this: ``` CUDA_VISIBLE_DIVICES= python actor.py &  As many time as needed python learner.py ``` The first line will hide the GPU from the process. This work for all software, not just JAX.","Thanks for the reply. In my example script that I provided, I'm dispatching two PythonProcess that use this environment variable setting: ```python             ""test_cpu"": PythonProcess(                 env={                     ""CUDA_VISIBLE_DEVICES"": """",                     ""JAX_PLATFORM_NAME"": ""cpu"",                 }             ),             ""test_gpu"": PythonProcess(                 env={                     ""CUDA_VISIBLE_DEVICES"": ""0"",                     ""XLA_PYTHON_CLIENT_MEM_FRACTION"": "".2"",                     ""XLA_PYTHON_CLIENT_PREALLOCATE"": ""false"",                     ""JAX_PLATFORM_NAME"": ""gpu"",                 }             ), ``` However, if I try and spawn two processes that can both see device 0, only the first process is seeing the device. Does that make sense?","| However, if I try and spawn two processes that can both see device 0, only the first process is seeing the device. Does that make sense? I think the issue isn't with JAX. It is probably related to your scheduler. I suppose if you spawn two test_gpu process and make sure they are on the same node. Can you print this in both process? `print(os.environ.get('CUDA_VISIBLE_DEVICES'))` I do not know launchpad. So I can't help much here. Why do you use that? Can you give me the full output of nvidiasmi? It is possible that the GPU is configured to be usable by only 1 process. Sometimes clusters are configured with that setup. This could explain your issue.","``` $python launchpad_gpu_test.py Local 0 Launchpad [test_cpu/0] I1220 18:34:44.112143 23450219082752 courier_utils.py:120] Binding: run [test_gpu/0] I1220 18:34:44.113588 22711702426624 courier_utils.py:120] Binding: run [test_gpu/1] I1220 18:34:44.114040 23041964385280 courier_utils.py:120] Binding: run [test_cpu/0] I1220 18:34:44.114264 23450219082752 courier_utils.py:120] Binding: test [test_gpu/0] I1220 18:34:44.114415 22711702426624 courier_utils.py:120] Binding: test [test_gpu/1] I1220 18:34:44.114561 23041964385280 courier_utils.py:120] Binding: test [test_gpu/1] 0 [test_gpu/0] 0 [test_cpu/0] ``` Works as expected. GPU nodes print ""0"", and the local driver program also prints ""0"". The restricted node does not see the device.  However, if I add `print(""Devices: "", jax.devices())` right after all of the `print(os.environ.get('CUDA_VISIBLE_DEVICES'))`, all of the processes correctly print their environment variables but all use CPUs. If I prevent the driver program from binding to the GPU, the processes outputs are: ``` ... [test_gpu/1] 0 [test_gpu/1] Devices:  [CpuDevice(id=0)] [test_gpu/0] 0 [test_gpu/0] Devices:  [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] [test_cpu/0] [test_cpu/0] Devices:  [CpuDevice(id=0)] ``` I'll submit a ticket to the supercomputer team to see if the suggested limitation is in place. ","Thanks for being so interactive Frédéric, I was surprised to see your name pop up here, but I remember you from our rare interactions back at Pavillon AndréAisenstadt in what 2016? :)","I've heard back from the supercomputer team and the devices are indeed set to ""exclusive process"" mode, which is undoubtedly the issue I'm encountering. Thanks again for all the help."
748,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Force keep_unused for native lowering when we have shape poly)， 内容是 (In presence of dimension variables in the input shapes we conservatively keep all unused inputs because otherwise we may drop the only inputs from whose shape we can infer the values of the dimension variables.  This change has very limited effect: only for native serialization when we have shape polymorphism and only if there are unused inputs. We may revisit this conservative solution, e.g., by changing the mechanism by which we compute the values of the dimension variables. See b/261971607.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Force keep_unused for native lowering when we have shape poly,"In presence of dimension variables in the input shapes we conservatively keep all unused inputs because otherwise we may drop the only inputs from whose shape we can infer the values of the dimension variables.  This change has very limited effect: only for native serialization when we have shape polymorphism and only if there are unused inputs. We may revisit this conservative solution, e.g., by changing the mechanism by which we compute the values of the dimension variables. See b/261971607.",2022-12-16T06:53:45Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13683
1774,"以下是一个github上的jax下的一个issue, 标题是(Jax determinism on GPU)， 内容是 ( Description I have trouble with determinism for Jax code on GPU but not on CPU. This is easy to reproduce from the flax examples https://github.com/google/flax/tree/main/examples . From https://github.com/google/jax/issues/565, I had the impression that setting TF_DETERMINISTIC_OPS=1 and TF_CUDNN_DETERMINISTIC=1 would make the GPU run deterministic. After 2 steps, the MNIST example give the same results on GPU or CPU: 4bda0ea4f536031c4619c394b64ede43 /tmp/mnist_cpu_1/checkpoint_1 4bda0ea4f536031c4619c394b64ede43 /tmp/mnist_cpu_2/checkpoint_1 1daae8e25344c91aea35c4836b46c798 /tmp/mnist_gpu_1/checkpoint_1 1daae8e25344c91aea35c4836b46c798 /tmp/mnist_gpu_2/checkpoint_1 This is not the case for the WMT transformer example where only the CPU run are deterministic. cbcf1e88ed7b2bd61376e3c90c5dcdfc /tmp/wmt_cpu_1/checkpoint_1 cbcf1e88ed7b2bd61376e3c90c5dcdfc /tmp/wmt_cpu_2/checkpoint_1 3dd11fb219e9f37dd90542267cadc86e /tmp/wmt_gpu_1/checkpoint_1 3067f6d66fdf7e8b52baef7056422291 /tmp/wmt_gpu_2/checkpoint_1 Is this expected? Am I missing some flags to make my GPU run deterministic? BTW I have the following setup CuDNN 8.4, tf = 2.11.0, jax = 0.3.13, flax = 0.5.3 Note: I filled the same issue for flax as well as they might want to be aware or look at it as well, see https://github.com/google/flax/issues/2700  What jax/jaxlib version are you using? 0.3.13  Which accelerator(s) are you using? GPU  Additional system info CuDNN 8.4, tf = 2.11.0, jax = 0.3.13, flax = 0.5.3  NVIDIA GPU info NVIDIASMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Jax determinism on GPU," Description I have trouble with determinism for Jax code on GPU but not on CPU. This is easy to reproduce from the flax examples https://github.com/google/flax/tree/main/examples . From https://github.com/google/jax/issues/565, I had the impression that setting TF_DETERMINISTIC_OPS=1 and TF_CUDNN_DETERMINISTIC=1 would make the GPU run deterministic. After 2 steps, the MNIST example give the same results on GPU or CPU: 4bda0ea4f536031c4619c394b64ede43 /tmp/mnist_cpu_1/checkpoint_1 4bda0ea4f536031c4619c394b64ede43 /tmp/mnist_cpu_2/checkpoint_1 1daae8e25344c91aea35c4836b46c798 /tmp/mnist_gpu_1/checkpoint_1 1daae8e25344c91aea35c4836b46c798 /tmp/mnist_gpu_2/checkpoint_1 This is not the case for the WMT transformer example where only the CPU run are deterministic. cbcf1e88ed7b2bd61376e3c90c5dcdfc /tmp/wmt_cpu_1/checkpoint_1 cbcf1e88ed7b2bd61376e3c90c5dcdfc /tmp/wmt_cpu_2/checkpoint_1 3dd11fb219e9f37dd90542267cadc86e /tmp/wmt_gpu_1/checkpoint_1 3067f6d66fdf7e8b52baef7056422291 /tmp/wmt_gpu_2/checkpoint_1 Is this expected? Am I missing some flags to make my GPU run deterministic? BTW I have the following setup CuDNN 8.4, tf = 2.11.0, jax = 0.3.13, flax = 0.5.3 Note: I filled the same issue for flax as well as they might want to be aware or look at it as well, see https://github.com/google/flax/issues/2700  What jax/jaxlib version are you using? 0.3.13  Which accelerator(s) are you using? GPU  Additional system info CuDNN 8.4, tf = 2.11.0, jax = 0.3.13, flax = 0.5.3  NVIDIA GPU info NVIDIASMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4  ",2022-12-15T18:42:36Z,documentation NVIDIA GPU,open,1,8,https://github.com/jax-ml/jax/issues/13672,Can you upgrade your jax and jaxlib version to the latest 0.4.1 version? (also upgrade your flax version?),I updated to Jax=0.4.1 and Flax=0.6.3 I still observe the same behavior: CPU runs are deterministic for flax/examples/wmt but not the GPU ones: 3acaaa3299a06957ce5d51b7c95bf13f  /tmp/wmt_cpu_1/checkpoint_1 3acaaa3299a06957ce5d51b7c95bf13f  /tmp/wmt_cpu_2/checkpoint_1 4f07b922eea4a92d6a2a1634d13191e7  /tmp/wmt_gpu_1/checkpoint_1 fa9011b54a845df5a2e54a5545625ec9  /tmp/wmt_gpu_2/checkpoint_1,Ping. Any idea how to get deterministic behavior on GPU?,"From CC(GPU determinism flag): > XLA:GPU reductions are nondeterministic, though. Changing this would be a lot of work. If you all wanted us to prioritize it, we should talk to understand the costs/benefits. I'm not aware of that having changed, maybe  can weighin?",Did you update jaxlib to 0.4.1?,"Yes. See my message of Dec 16, 2022.  Are XLA:GPU reductions nondeterministic?  Even with TF_DETERMINISTIC_OPS=1 and TF_CUDNN_DETERMINISTIC=1 ?  Where could I read about this?","Hey , sorry for the delay. I think we need `XLA_FLAGS='xla_gpu_deterministic_ops=true'`. With that flag, all XLA:GPU operations are deterministic _or they will loudly error_, and any such loud error is an XLA:GPU bug. (We actually don't need `TF_DETERMINISTIC_OPS=1` for XLA:GPU itself anymore, given the `XLA_FLAGS` option just mentioned, though `TF_DETERMINISTIC_ops=1` may still be useful for `tf.data` if you're using that, which it looks like the example is indeed using.) Thanks to  for explaining this. Can you try out `XLA_FLAGS='xla_gpu_deterministic_ops=true'` and verify that it works for you?","> Hey , sorry for the delay. >  > I think we need `XLA_FLAGS='xla_gpu_deterministic_ops=true'`. With that flag, all XLA:GPU operations are deterministic _or they will loudly error_, and any such loud error is an XLA:GPU bug. (We actually don't need `TF_DETERMINISTIC_OPS=1` for XLA:GPU itself anymore, given the `XLA_FLAGS` option just mentioned, though `TF_DETERMINISTIC_ops=1` may still be useful for `tf.data` if you're using that, which it looks like the example is indeed using.) >  > Thanks to  for explaining this. >  > Can you try out `XLA_FLAGS='xla_gpu_deterministic_ops=true'` and verify that it works for you? Having the same issue on different code, the flag indeed works for me, however it slows down the training time. Do you have any other idea to balance the tradeoff ?"
1511,"以下是一个github上的jax下的一个issue, 标题是(Grad call (with no jit) is much slower than forward model call (that has some parts of it jitted) on cpu)， 内容是 ( Description Hello,  I have a forward model that consists of many function calls and if statements for control flow running on CPU. The computationally heavy parts of the forward model are jitted but the whole model is not because of the control flow. This makes the model fast to run **0.36 sec** per forward model call instead of 1.8 sec without jitting some functions in it.   Now trying to call ```grad()```  on the forward model, takes much longer than the forward call itself.  It takes **47** seconds on the first grad call, and **6.2** seconds on subsequent grad calls. Note that I cannot use jit on grad since the forward model itself has some if statements. Where would the slowdown be coming from? It doesnt seem to depend on the size of the input and the first call is much slower than subsequent calls. Finally, even the fastest grad call is slower than the version of the forward model with no jitted function.  Thanks a lot for the help and pointers.    What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? CPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Grad call (with no jit) is much slower than forward model call (that has some parts of it jitted) on cpu," Description Hello,  I have a forward model that consists of many function calls and if statements for control flow running on CPU. The computationally heavy parts of the forward model are jitted but the whole model is not because of the control flow. This makes the model fast to run **0.36 sec** per forward model call instead of 1.8 sec without jitting some functions in it.   Now trying to call ```grad()```  on the forward model, takes much longer than the forward call itself.  It takes **47** seconds on the first grad call, and **6.2** seconds on subsequent grad calls. Note that I cannot use jit on grad since the forward model itself has some if statements. Where would the slowdown be coming from? It doesnt seem to depend on the size of the input and the first call is much slower than subsequent calls. Finally, even the fastest grad call is slower than the version of the forward model with no jitted function.  Thanks a lot for the help and pointers.    What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? CPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info _No response_",2022-12-14T23:42:34Z,question,closed,0,3,https://github.com/jax-ml/jax/issues/13658,"It is expected that an unjitted gradient of a oomputation will be slower than the computation itself. Why? It's because the gradient in general is an intrinsically more complicated operation. For example, if you have the function $f(x) = x^n$, which is a single operation, the gradient will be $df(x) = n \cdot x^{n  1} \cdot dx$, which is four operations. You can see this play out by examining the jaxpr for this function and its gradient: ```python import jax def f(x, n):   return x ** n print(jax.make_jaxpr(f)(10.0, 5.0))  { lambda ; a:f32[] b:f32[]. let c:f32[] = pow a b in (c,) } print(jax.make_jaxpr(jax.grad(f))(10.0, 5.0))  { lambda ; a:f32[] b:f32[]. let      _:f32[] = pow a b      c:f32[] = sub b 1.0      d:f32[] = pow a c      e:f32[] = mul b d      f:f32[] = mul 1.0 e    in (f,) } ``` When you jitcompile the latter, XLA can often fuse these multiple operations to improve the speed. But the `grad` of a function always requires strictly more operations than the function itself, so absent JIT it will almost always be a factor of a few slower. So where does that leave you? If the cost of an unjitted gradient is too much for your use case, the most straightforward way to do better is to JIT compile it, which in your case would require modifying your code to make the original function compatible with JIT. This is sometimes difficult, but probably not impossible. Does that make sense?","That makes sense in terms of why it takes longer but I have a few quick followup questions: 1. In terms of restructuring the code, is the recommendation to use lax.cond for controlflow or to avoid conditional statements all together? If so, is there a best practice published somewhere? 2. I tried the same model in Pytorch, and while the gradient computation is slower than the forward model there as well. It is much closer in terms of time. For example, in Torch, the forward model takes 0.8 sec and the grad call takes 1 sec. As opposed to 0.3 sec forward call in Jax and 6 sec for grad? 3. Finally, is there a reason why the first grad call takes much longer than subsequent ones? Thanks a lot for the help and explanations Best ,","> 1. In terms of restructuring the code, is the recommendation to use lax.cond for controlflow or to avoid conditional statements all together? If so, is there a best practice published somewhere? Yes, if the problem is `if` statements, you can generally replace those with `lax.cond`, `jnp.where`, masking, or another mechanism depending on the nature of the code. I don't know of any comprehensive general guide to this, but it's discussed a bit at https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmlcontrolflow > 2. I tried the same model in Pytorch, and while the gradient computation is slower than the forward model there as well. It is much closer in terms of time. For example, in Torch, the forward model takes 0.8 sec and the grad call takes 1 sec. As opposed to 0.3 sec forward call in Jax and 6 sec for grad? It's hard to guess why the timings might be different without seeing what your code is doing > 3. Finally, is there a reason why the first grad call takes much longer than subsequent ones? I suspect this is due to JIT compilation overhead (for the individual jitcompiled functions being called by grad) on the first run."
959,"以下是一个github上的jax下的一个issue, 标题是(Specifying the partitioning of the output of `jax.random` functions)， 内容是 (The `jax.Array` tutorial explains that to partition the output of `jax.random.uniform` one can put it inside of a jaxjit function that accepts another array with the desired partitioning: ```python .jit def f(key, x):   numbers = jax.random.uniform(key, x.shape)   return x + numbers key = jax.random.PRNGKey(42) x_sharding = jax.sharding.PositionalSharding(jax.devices()) x = jax.device_put(jnp.arange(24), x_sharding) f(key, x) ``` however this approach seems brittle, and requires passing around a mock array that would otherwise be unneeded. Is there a way to specify the sharding explicitly? Maybe something like a `sharding` keyword argument? ```python jax.random.uniform(key, shape, sharding=sharding) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Specifying the partitioning of the output of `jax.random` functions,"The `jax.Array` tutorial explains that to partition the output of `jax.random.uniform` one can put it inside of a jaxjit function that accepts another array with the desired partitioning: ```python .jit def f(key, x):   numbers = jax.random.uniform(key, x.shape)   return x + numbers key = jax.random.PRNGKey(42) x_sharding = jax.sharding.PositionalSharding(jax.devices()) x = jax.device_put(jnp.arange(24), x_sharding) f(key, x) ``` however this approach seems brittle, and requires passing around a mock array that would otherwise be unneeded. Is there a way to specify the sharding explicitly? Maybe something like a `sharding` keyword argument? ```python jax.random.uniform(key, shape, sharding=sharding) ```",2022-12-13T10:30:16Z,enhancement,open,0,2,https://github.com/jax-ml/jax/issues/13631, ,"This was meant only as an example. You could also constrain the output of `jax.random.uniform` directly with `jax.lax.with_sharding_constraint`, which is covered in the tutorial as well. Example: ```python sharding = ... .jit def f(key):   numbers = jax.random.uniform(key, x.shape)   return jax.lax.with_sharding_constraint(numbers, sharding) ``` As the tutorial mentions, be sure to opt in to the partitionable RNG future feature (e.g. with `jax.config.update('jax_threefry_partitionable', True)`)"
7291,"以下是一个github上的jax下的一个issue, 标题是(`JaxStackTraceBeforeTransformation` error with hyper-parameter optimization involving complex dtypes)， 内容是 ( Description I am trying to optimize hyperparameters that involve complex numbers within the model evaluation, resulting in `JaxStackTraceBeforeTransformation: TypeError: Cannot interpret 'Zero(ShapedArray(complex64[10]))' as a data type` being raised. Here is a minimal example: ```py import jax import jax.numpy as np def model_fn(x):     """"""Create complex array using input""""""     array = np.linspace(x, x, 10)     return np.abs(1j*array)  No error thrown if the `1j*` is removed .value_and_grad def inner_loss_fn(x, data):     return np.square(model_fn(x)  data).sum() .value_and_grad def loss_fn(lr, x, data):     for i in range(10):         inner_loss, grad = inner_loss_fn(x, data)         x = x  lr * grad     return inner_loss data = model_fn(0.) inner_loss, inner_grad = inner_loss_fn(1., data)  No error thrown loss, grad = loss_fn(1., 1., data)  Error thrown ``` Here is the full stacktrace ```py Traceback (most recent call last):   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 15, in      .value_and_grad   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 17, in loss_fn     for i in range(10):   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 13, in inner_loss_fn     return np.square(model_fn(x)  data).sum()   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 7, in model_fn     array = np.linspace(x, x, 10)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/numpy/ufuncs.py"", line 195, in absolute     return x if dt == np.bool_ or dtypes.issubdtype(dt, np.unsignedinteger) else lax.abs(x) jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: Cannot interpret 'Zero(ShapedArray(complex64[10]))' as a data type The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 24, in      loss, grad = loss_fn(1., 1., data)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/api.py"", line 1156, in value_and_grad_f     g = vjp_py(lax_internal._one(ans))   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/tree_util.py"", line 292, in __call__     return self.fun(*args, **kw)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/api.py"", line 2523, in _vjp_pullback_wrapper     ans = fun(*args)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/tree_util.py"", line 292, in __call__     return self.fun(*args, **kw)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/ad.py"", line 140, in unbound_vjp     arg_cts = backward_pass(jaxpr, reduce_axes, True, consts, dummy_args, cts)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/ad.py"", line 240, in backward_pass     cts_out = get_primitive_transpose(eqn.primitive)(   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/ad.py"", line 625, in call_transpose     out_flat = primitive.bind(fun, *all_args, **params)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/core.py"", line 1939, in bind     return call_bind(self, fun, *args, **params)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/core.py"", line 1955, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/core.py"", line 701, in process_call     return primitive.impl(f, *tracers, **params)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 234, in _xla_call_impl     compiled_fun = xla_callable(fun, device, backend, name, donated_invars,   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/linear_util.py"", line 309, in memoized_fun     ans = call(fun, *args)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 342, in _xla_callable_uncached     return lower_xla_callable(fun, device, backend, name, donated_invars, False,   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 313, in wrapper     return func(*args, **kwargs)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 428, in lower_xla_callable     jaxpr, out_type, consts = pe.trace_to_jaxpr_final2(   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 313, in wrapper     return func(*args, **kwargs)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/partial_eval.py"", line 2080, in trace_to_jaxpr_final2     jaxpr, out_type, consts = trace_to_subjaxpr_dynamic2(fun, main, debug_info)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/partial_eval.py"", line 2030, in trace_to_subjaxpr_dynamic2     ans = fun.call_wrapped(*in_tracers_)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/linear_util.py"", line 168, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/ad.py"", line 246, in backward_pass     cts_out = get_primitive_transpose(eqn.primitive)(   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 2017, in _conj_transpose_rule     return [conj(t)]   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 407, in conj     return conj_p.bind(x, input_dtype=_dtype(x))   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/dtypes.py"", line 445, in dtype     dt = np.result_type(x)   File """", line 180, in result_type jax._src.traceback_util.UnfilteredStackTrace: TypeError: Cannot interpret 'Zero(ShapedArray(complex64[10]))' as a data type The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 24, in      loss, grad = loss_fn(1., 1., data)   File """", line 180, in result_type TypeError: Cannot interpret 'Zero(ShapedArray(complex64[10]))' as a data type ``` I am at a loss as to how to move forward from here, so any help is greatly appreciated!  What jax/jaxlib version are you using? 0.3.23 / 0.3.22  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`JaxStackTraceBeforeTransformation` error with hyper-parameter optimization involving complex dtypes," Description I am trying to optimize hyperparameters that involve complex numbers within the model evaluation, resulting in `JaxStackTraceBeforeTransformation: TypeError: Cannot interpret 'Zero(ShapedArray(complex64[10]))' as a data type` being raised. Here is a minimal example: ```py import jax import jax.numpy as np def model_fn(x):     """"""Create complex array using input""""""     array = np.linspace(x, x, 10)     return np.abs(1j*array)  No error thrown if the `1j*` is removed .value_and_grad def inner_loss_fn(x, data):     return np.square(model_fn(x)  data).sum() .value_and_grad def loss_fn(lr, x, data):     for i in range(10):         inner_loss, grad = inner_loss_fn(x, data)         x = x  lr * grad     return inner_loss data = model_fn(0.) inner_loss, inner_grad = inner_loss_fn(1., data)  No error thrown loss, grad = loss_fn(1., 1., data)  Error thrown ``` Here is the full stacktrace ```py Traceback (most recent call last):   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 15, in      .value_and_grad   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 17, in loss_fn     for i in range(10):   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 13, in inner_loss_fn     return np.square(model_fn(x)  data).sum()   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 7, in model_fn     array = np.linspace(x, x, 10)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/numpy/ufuncs.py"", line 195, in absolute     return x if dt == np.bool_ or dtypes.issubdtype(dt, np.unsignedinteger) else lax.abs(x) jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: Cannot interpret 'Zero(ShapedArray(complex64[10]))' as a data type The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 24, in      loss, grad = loss_fn(1., 1., data)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/api.py"", line 1156, in value_and_grad_f     g = vjp_py(lax_internal._one(ans))   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/tree_util.py"", line 292, in __call__     return self.fun(*args, **kw)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/api.py"", line 2523, in _vjp_pullback_wrapper     ans = fun(*args)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/tree_util.py"", line 292, in __call__     return self.fun(*args, **kw)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/ad.py"", line 140, in unbound_vjp     arg_cts = backward_pass(jaxpr, reduce_axes, True, consts, dummy_args, cts)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/ad.py"", line 240, in backward_pass     cts_out = get_primitive_transpose(eqn.primitive)(   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/ad.py"", line 625, in call_transpose     out_flat = primitive.bind(fun, *all_args, **params)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/core.py"", line 1939, in bind     return call_bind(self, fun, *args, **params)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/core.py"", line 1955, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/core.py"", line 701, in process_call     return primitive.impl(f, *tracers, **params)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 234, in _xla_call_impl     compiled_fun = xla_callable(fun, device, backend, name, donated_invars,   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/linear_util.py"", line 309, in memoized_fun     ans = call(fun, *args)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 342, in _xla_callable_uncached     return lower_xla_callable(fun, device, backend, name, donated_invars, False,   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 313, in wrapper     return func(*args, **kwargs)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 428, in lower_xla_callable     jaxpr, out_type, consts = pe.trace_to_jaxpr_final2(   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 313, in wrapper     return func(*args, **kwargs)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/partial_eval.py"", line 2080, in trace_to_jaxpr_final2     jaxpr, out_type, consts = trace_to_subjaxpr_dynamic2(fun, main, debug_info)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/partial_eval.py"", line 2030, in trace_to_subjaxpr_dynamic2     ans = fun.call_wrapped(*in_tracers_)   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/linear_util.py"", line 168, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/interpreters/ad.py"", line 246, in backward_pass     cts_out = get_primitive_transpose(eqn.primitive)(   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 2017, in _conj_transpose_rule     return [conj(t)]   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 407, in conj     return conj_p.bind(x, input_dtype=_dtype(x))   File ""/Users/louis/mambaforge/envs/dlux/lib/python3.10/sitepackages/jax/_src/dtypes.py"", line 445, in dtype     dt = np.result_type(x)   File """", line 180, in result_type jax._src.traceback_util.UnfilteredStackTrace: TypeError: Cannot interpret 'Zero(ShapedArray(complex64[10]))' as a data type The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/louis/PhD/dLux/sandbox/Deconvolution/example.py"", line 24, in      loss, grad = loss_fn(1., 1., data)   File """", line 180, in result_type TypeError: Cannot interpret 'Zero(ShapedArray(complex64[10]))' as a data type ``` I am at a loss as to how to move forward from here, so any help is greatly appreciated!  What jax/jaxlib version are you using? 0.3.23 / 0.3.22  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_",2022-12-13T07:37:23Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/13629,Running into a similar issue for model architectures which involve manipulations of complex numbers.,"Hi   I tried to run the mentioned code in Google Colab with latest JAX version (0.4.23). The mentioned code executed without any error. Please refer to the gist. I have tried this code in mac with latest JAX version i.e., 0.4.23 and it executed without error. Please find the screenshot for reference.  Please check with the latest version and confirm if you still have the issue. Thank you.","Hey  you are correct it looks like this issue has been fixed in some previous version, thanks!"
2048,"以下是一个github上的jax下的一个issue, 标题是(How to efficiently lookup a value from a BCOO matrix?)， 内容是 ( Description What is the best way to look up a value that is stored in a BCOO sparse matrix? I tried two methods, (1) indexing directly given a known coordinate set and (2) make an another spares matrix with value of 1 in the coordinate of interest and multiply that matrix to the matrix of interest then sum to get the final value. ```python import jax import jax.numpy as jnp from jax.experimental.sparse import BCOO n_dim = 5 n_array = 1_000_000 num_element = 10 data = jax.random.randint(jax.random.PRNGKey(0), (num_element,), 0, n_array) indices = jax.random.randint(jax.random.PRNGKey(0), (num_element, n_dim), 0, n_array) m = BCOO((data, indices), shape=tuple([n_array]*n_dim)) def get_by_indexing(m, indices):     i0, i1, i2, i3, i4 = indices     return m[i0, i1, i2, i3, i4].todense() def get_by_multiplication(m, indices):     identity = jnp.array([1])     x = BCOO((identity, jnp.expand_dims(indices, 0)), shape=m.shape)     return jax.experimental.sparse.bcoo_multiply_sparse(m, x).sum() ``` Getting value through indexing ``` In   [2]: %timeit get_by_indexing(m, indices[0]) 55.3 ms ± 182 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) ``` Getting value through multiplication ``` In   [3]: %timeit get_by_multiplication(m, indices[0]) 8 ms ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) ``` Also just to be sure that both function returns the same value. ``` In   [4]: get_by_indexing(m, indices[0]) == get_by_multiplication(m, indices[0]) Out[4]: DeviceArray(True, dtype=bool) ``` It looks like that getting the value by multiplication is faster than indexing. Shouldn't usual indexing be faster?  What jax/jaxlib version are you using? jax v0.3.25, jaxlib v0.3.22  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,How to efficiently lookup a value from a BCOO matrix?," Description What is the best way to look up a value that is stored in a BCOO sparse matrix? I tried two methods, (1) indexing directly given a known coordinate set and (2) make an another spares matrix with value of 1 in the coordinate of interest and multiply that matrix to the matrix of interest then sum to get the final value. ```python import jax import jax.numpy as jnp from jax.experimental.sparse import BCOO n_dim = 5 n_array = 1_000_000 num_element = 10 data = jax.random.randint(jax.random.PRNGKey(0), (num_element,), 0, n_array) indices = jax.random.randint(jax.random.PRNGKey(0), (num_element, n_dim), 0, n_array) m = BCOO((data, indices), shape=tuple([n_array]*n_dim)) def get_by_indexing(m, indices):     i0, i1, i2, i3, i4 = indices     return m[i0, i1, i2, i3, i4].todense() def get_by_multiplication(m, indices):     identity = jnp.array([1])     x = BCOO((identity, jnp.expand_dims(indices, 0)), shape=m.shape)     return jax.experimental.sparse.bcoo_multiply_sparse(m, x).sum() ``` Getting value through indexing ``` In   [2]: %timeit get_by_indexing(m, indices[0]) 55.3 ms ± 182 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) ``` Getting value through multiplication ``` In   [3]: %timeit get_by_multiplication(m, indices[0]) 8 ms ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) ``` Also just to be sure that both function returns the same value. ``` In   [4]: get_by_indexing(m, indices[0]) == get_by_multiplication(m, indices[0]) Out[4]: DeviceArray(True, dtype=bool) ``` It looks like that getting the value by multiplication is faster than indexing. Shouldn't usual indexing be faster?  What jax/jaxlib version are you using? jax v0.3.25, jaxlib v0.3.22  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2022-12-13T04:35:39Z,needs info,closed,0,5,https://github.com/jax-ml/jax/issues/13628,"Hi  thanks for the question. In general, the answer for how to make operations faster in jax is to use `jit` compilation. This is especially true for sparse operations, which are generally implemented not in terms of a single efficient XLA op, but rather a sequence of XLA operations on the underlying dense buffers. If you jitcompile your two functions, you'll see that the timings are much faster, and that the two methods are comparable. Here are the results on a Colab CPU runtime: ```python get_by_indexing_jit = jax.jit(get_by_indexing) get_by_multiplication_jit = jax.jit(get_by_multiplication) _ = get_by_indexing_jit(m, indices[0]) _ = get_by_multiplication_jit(m, indices[0]) %timeit get_by_indexing(m, indices[0]).block_until_ready()  89.5 ms ± 2.19 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit get_by_multiplication(m, indices[0]).block_until_ready()  14 ms ± 505 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) %timeit get_by_indexing_jit(m, indices[0]).block_until_ready()  511 µs ± 17.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) %timeit get_by_multiplication_jit(m, indices[0]).block_until_ready()  504 µs ± 12.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) ``` With this in mind, I'd suggest using a straightforward indexing operation to access elements of the sparse array, rather than using a more complicated matmulbased approach.","Thanks!  I noticed this may work for sparse matrix with small numbers of `nse`. I intend to use large sparse matrix with a large `nse`. ```python BCOO(int32[1000000, 1000000, 1000000, 1000000, 1000000], nse=44896321) ``` rerunning the indexing functions return quite different results. ```python %timeit get_by_indexing(m, indices[0]).block_until_ready()  22 s ± 11.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit get_by_multiplication(m, indices[0]).block_until_ready()  5.9 s ± 1.87 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit get_by_indexing_jit(m, indices[0]).block_until_ready()  14.9 s ± 8.28 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit get_by_multiplication_jit(m, indices[0]).block_until_ready()  1.23 s ± 772 µs per loop (mean ± std. dev. of 7 runs, 1 loop each) ``` Is the speed of indexing inversely proportional to the number of `nse` in the sparse matrix?","Yes, indexing a sparse data structure involves searching through  the index buffer for the requested index, so in general as the size of the index buffer grows, the indexing operation will slow down. JAX's `BCOO` objects are particularly unsuited for large `nse` compared to other implementations, because XLA does not offer any efficient binary search primitive, and so these index searches scale linearly with the size of the buffers (i.e. `nse`) whereas a binarysearchbased approach could in theory scale as `log(nse)`.","I'd like to extend this so that I can process a batch of indices. This process works. Which allows a number of batch of indices to be processed. ```python get_index_fn = partial(util.get_by_multiplication_jit, matrix=matrix) vmap_get_index = jax.vmap(     get_index_fn,     in_axes=(0)     ) ``` If I set a batch too high it would crash due to OOM, which I solved with using `jax.lax.map` ```python def process_index(indices):     return jax.lax.map(vmap_get_index, indices) ``` Now I want to expand the process to be able to run parallel in 8 GPUs. The solution I made works but takes forever to start. I wander if this is the correct way to use pmap to divide the work to each GPU? ```python pmap_process_index = jax.pmap(     process_index,     in_axes=(0)     ) ``` Also, for input shape, I've made it so that the dimensions are (num_devices, num of sequence to map, vmap batch size, n_dim)","Sorry, I'm not sure what your question is. Perhaps it would help to add a fully reproducible example of what you're doing, along with pointing out where the problem occurs."
8410,"以下是一个github上的jax下的一个issue, 标题是(Failed to determine best cudnn convolution algorithm)， 内容是 ( Description Hello, I got the following error message running https://github.com/googleresearch/multinerf: ``` 20221212 14:17:37.451910: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:727] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.  Conv: (f32[2520,1,1287]{2,1,0}, u8[0]{0}) customcall(f32[2520,1,1297]{2,1,0}, f32[1,1,11]{2,1,0}), window={size=11}, dim_labels=bf0_oi0>bf0, custom_call_target=""__cudnn$convForward"", backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" Traceback (most recent call last):   File ""/.../tensorflow/multinerf/train.py"", line 288, in      app.run(main)   File ""/.../pub/anacondapy3/2021.05/envs/tensorflow2.9.1+py3.10/lib/python3.10/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/.../pub/anacondapy3/2021.05/envs/tensorflow2.9.1+py3.10/lib/python3.10/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/.../tensorflow/multinerf/train.py"", line 242, in main     metric = metric_harness(   File ""/.../tensorflow/multinerf/internal/image.py"", line 136, in __call__     ssim = float(self.ssim_fn(rgb_pred, rgb_gt))   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/api.py"", line 522, in cache_miss     out_flat = xla.xla_call(   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/core.py"", line 1836, in bind     return call_bind(self, fun, *args, **params)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/core.py"", line 1852, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/core.py"", line 683, in process_call     return primitive.impl(f, *tracers, **params)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 196, in _xla_call_impl     compiled_fun = xla_callable(fun, device, backend, name, donated_invars,   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/linear_util.py"", line 286, in memoized_fun     ans = call(fun, *args)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 246, in _xla_callable_uncached     keep_unused, *arg_specs).compile().unsafe_call   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 808, in compile     self._executable = XlaCompiledComputation.from_xla_computation(   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 911, in from_xla_computation     compiled = compile_or_get_cached(backend, xla_computation, options)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 872, in compile_or_get_cached     return backend_compile(backend, computation, compile_options)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/profiler.py"", line 312, in wrapper     return func(*args, **kwargs)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 818, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: Failed to determine best cudnn convolution algorithm for: %cudnnconv = (f32[2520,1,1287]{2,1,0}, u8[0]{0}) customcall(f32[2520,1,1297]{2,1,0} %bitcast.4, f32[1,1,11]{2,1,0} %bitcast.5), window={size=11}, dim_labels=bf0_oi0>bf0, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(ssim)/jit(main)/vmap(jit(convolve))/jit(_conv)/conv_general_dilated[window_strides=(1,) padding=((0, 0),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2), rhs_spec=(0, 1, 2), out_spec=(0, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(2520, 1, 1297) rhs_shape=(1, 1, 11) precision=(, ) preferred_element_type=None]"" source_file=""/.../.local/lib/python3.10/sitepackages/dm_pix/_src/metrics.py"" source_line=177}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" Original error: INTERNAL: All algorithms tried for %cudnnconv = (f32[2520,1,1287]{2,1,0}, u8[0]{0}) customcall(f32[2520,1,1297]{2,1,0} %bitcast.4, f32[1,1,11]{2,1,0} %bitcast.5), window={size=11}, dim_labels=bf0_oi0>bf0, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(ssim)/jit(main)/vmap(jit(convolve))/jit(_conv)/conv_general_dilated[window_strides=(1,) padding=((0, 0),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2), rhs_spec=(0, 1, 2), out_spec=(0, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(2520, 1, 1297) rhs_shape=(1, 1, 11) precision=(, ) preferred_element_type=None]"" source_file=""/.../.local/lib/python3.10/sitepackages/dm_pix/_src/metrics.py"" source_line=177}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" failed. Falling back to default algorithm.  Peralgorithm errors:   Profiling failure on cuDNN engine eng30{k2=2,k4=0,k5=0,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng28{k2=4,k3=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng1{k2=4,k3=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng42{k4=3,k5=1,k6=0,k7=0,k2=1}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng42{k2=2,k4=1,k5=0,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng1{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng2{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng4{}: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng28{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng30{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng34{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng42{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status' To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning. ``` Not sure what to do about it. Best, Rémi  What jax/jaxlib version are you using? 0.3.14  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.5, Linux RedHat 8.4  NVIDIA GPU info ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Failed to determine best cudnn convolution algorithm," Description Hello, I got the following error message running https://github.com/googleresearch/multinerf: ``` 20221212 14:17:37.451910: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:727] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.  Conv: (f32[2520,1,1287]{2,1,0}, u8[0]{0}) customcall(f32[2520,1,1297]{2,1,0}, f32[1,1,11]{2,1,0}), window={size=11}, dim_labels=bf0_oi0>bf0, custom_call_target=""__cudnn$convForward"", backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" Traceback (most recent call last):   File ""/.../tensorflow/multinerf/train.py"", line 288, in      app.run(main)   File ""/.../pub/anacondapy3/2021.05/envs/tensorflow2.9.1+py3.10/lib/python3.10/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/.../pub/anacondapy3/2021.05/envs/tensorflow2.9.1+py3.10/lib/python3.10/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/.../tensorflow/multinerf/train.py"", line 242, in main     metric = metric_harness(   File ""/.../tensorflow/multinerf/internal/image.py"", line 136, in __call__     ssim = float(self.ssim_fn(rgb_pred, rgb_gt))   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/api.py"", line 522, in cache_miss     out_flat = xla.xla_call(   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/core.py"", line 1836, in bind     return call_bind(self, fun, *args, **params)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/core.py"", line 1852, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/core.py"", line 683, in process_call     return primitive.impl(f, *tracers, **params)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 196, in _xla_call_impl     compiled_fun = xla_callable(fun, device, backend, name, donated_invars,   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/linear_util.py"", line 286, in memoized_fun     ans = call(fun, *args)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 246, in _xla_callable_uncached     keep_unused, *arg_specs).compile().unsafe_call   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 808, in compile     self._executable = XlaCompiledComputation.from_xla_computation(   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 911, in from_xla_computation     compiled = compile_or_get_cached(backend, xla_computation, options)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 872, in compile_or_get_cached     return backend_compile(backend, computation, compile_options)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/profiler.py"", line 312, in wrapper     return func(*args, **kwargs)   File ""/.../src/pub/anacondapy3/2021.05/tensorflow2.9.1+py3.10/jaxlibv0.3.14/jax/_src/dispatch.py"", line 818, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: Failed to determine best cudnn convolution algorithm for: %cudnnconv = (f32[2520,1,1287]{2,1,0}, u8[0]{0}) customcall(f32[2520,1,1297]{2,1,0} %bitcast.4, f32[1,1,11]{2,1,0} %bitcast.5), window={size=11}, dim_labels=bf0_oi0>bf0, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(ssim)/jit(main)/vmap(jit(convolve))/jit(_conv)/conv_general_dilated[window_strides=(1,) padding=((0, 0),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2), rhs_spec=(0, 1, 2), out_spec=(0, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(2520, 1, 1297) rhs_shape=(1, 1, 11) precision=(, ) preferred_element_type=None]"" source_file=""/.../.local/lib/python3.10/sitepackages/dm_pix/_src/metrics.py"" source_line=177}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" Original error: INTERNAL: All algorithms tried for %cudnnconv = (f32[2520,1,1287]{2,1,0}, u8[0]{0}) customcall(f32[2520,1,1297]{2,1,0} %bitcast.4, f32[1,1,11]{2,1,0} %bitcast.5), window={size=11}, dim_labels=bf0_oi0>bf0, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(ssim)/jit(main)/vmap(jit(convolve))/jit(_conv)/conv_general_dilated[window_strides=(1,) padding=((0, 0),) lhs_dilation=(1,) rhs_dilation=(1,) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2), rhs_spec=(0, 1, 2), out_spec=(0, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(2520, 1, 1297) rhs_shape=(1, 1, 11) precision=(, ) preferred_element_type=None]"" source_file=""/.../.local/lib/python3.10/sitepackages/dm_pix/_src/metrics.py"" source_line=177}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" failed. Falling back to default algorithm.  Peralgorithm errors:   Profiling failure on cuDNN engine eng30{k2=2,k4=0,k5=0,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng28{k2=4,k3=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng1{k2=4,k3=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng42{k4=3,k5=1,k6=0,k7=0,k2=1}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng42{k2=2,k4=1,k5=0,k6=0,k7=0}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng0{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng1{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng2{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng4{}: UNKNOWN: CUDNN_STATUS_INTERNAL_ERROR in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng28{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng30{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng34{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status'   Profiling failure on cuDNN engine eng42{}: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4369): 'status' To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning. ``` Not sure what to do about it. Best, Rémi  What jax/jaxlib version are you using? 0.3.14  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.5, Linux RedHat 8.4  NVIDIA GPU info ``` ++  ++++ ```",2022-12-12T14:04:38Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/13612,Can you try an experiment for me? I'm speculating you ran out of GPU memory. Try lowering the GPU memory fraction as documented here: https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html (Try perhaps 0.8?),> Can you try an experiment for me? I'm speculating you ran out of GPU memory. Seems like you're right! I tried on a node with 32GB V100 GPUs and it seems to work fine. I would not have thought about this so thanks! :) 
1009,"以下是一个github上的jax下的一个issue, 标题是(Run jaxlib in docker on Mac arm64)， 内容是 ( Description I'm trying to run jaxlib on Mac arm64 chip through docker container.  Trying to install it through pip I'm getting error `ERROR: No matching distribution found for jaxlib>=0.3.18` I've tried also to run this container using linux/amd64 image. Jaxlib gets installed well through pipl but after running the code I'm getting following error: `RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.` So what is the correct way to run jaxlib on mac arm64 hosted docker container?  What jax/jaxlib version are you using? jaxlib 0.3.25  Which accelerator(s) are you using? CPY  Additional system info Mac Apple M1 Pro  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Run jaxlib in docker on Mac arm64," Description I'm trying to run jaxlib on Mac arm64 chip through docker container.  Trying to install it through pip I'm getting error `ERROR: No matching distribution found for jaxlib>=0.3.18` I've tried also to run this container using linux/amd64 image. Jaxlib gets installed well through pipl but after running the code I'm getting following error: `RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.` So what is the correct way to run jaxlib on mac arm64 hosted docker container?  What jax/jaxlib version are you using? jaxlib 0.3.25  Which accelerator(s) are you using? CPY  Additional system info Mac Apple M1 Pro  NVIDIA GPU info _No response_",2022-12-11T18:04:37Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/13608,"I don't run hosted docker container, but on my M1, the following version upgrade seems fine ``` Python 3.10.1 (main, Mar  7 2022, 13:38:25) [Clang 13.0.0 (clang1300.0.29.30)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.__version__ '0.3.20' >>> import jaxlib >>> jaxlib.__version__ '0.3.20' >>> zhangqiaorjcmacbookpro2:~ zhangqiaorjc$ pip install upgrade pip Requirement already satisfied: pip in ./.pyenv/versions/3.10.1/lib/python3.10/sitepackages (22.3) Collecting pip   Downloading pip22.3.1py3noneany.whl (2.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 10.9 MB/s eta 0:00:00 Installing collected packages: pip   Attempting uninstall: pip     Found existing installation: pip 22.3     Uninstalling pip22.3:       Successfully uninstalled pip22.3 Successfully installed pip22.3.1 zhangqiaorjcmacbookpro2:~ zhangqiaorjc$ zhangqiaorjcmacbookpro2:~ zhangqiaorjc$ pip install upgrade ""jax[cpu]"" Requirement already satisfied: jax[cpu] in ./.pyenv/versions/3.10.1/lib/python3.10/sitepackages (0.3.20) Collecting jax[cpu]   Downloading jax0.3.25.tar.gz (1.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 9.1 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done Requirement already satisfied: numpy>=1.20 in ./.pyenv/versions/3.10.1/lib/python3.10/sitepackages (from jax[cpu]) (1.22.2) Requirement already satisfied: opt_einsum in ./.pyenv/versions/3.10.1/lib/python3.10/sitepackages (from jax[cpu]) (3.3.0) Requirement already satisfied: scipy>=1.5 in ./.pyenv/versions/3.10.1/lib/python3.10/sitepackages (from jax[cpu]) (1.8.0) Requirement already satisfied: typing_extensions in ./.pyenv/versions/3.10.1/lib/python3.10/sitepackages (from jax[cpu]) (4.1.1) Collecting jaxlib==0.3.25   Downloading jaxlib0.3.25cp310cp310macosx_11_0_arm64.whl (51.6 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.6/51.6 MB 33.2 MB/s eta 0:00:00 Building wheels for collected packages: jax   Building wheel for jax (setup.py) ... done   Created wheel for jax: filename=jax0.3.25py3noneany.whl size=1308509 sha256=94ad7e9a1e0cb168b6831b9540b7f1e07020a663e6b3044b51651cdd37550013   Stored in directory: /Users/zhangqiaorjc/Library/Caches/pip/wheels/45/65/e5/9d8dd300a2181533ee4d74780d647245740bbff982b700903a Successfully built jax Installing collected packages: jaxlib, jax   Attempting uninstall: jaxlib     Found existing installation: jaxlib 0.3.20     Uninstalling jaxlib0.3.20:       Successfully uninstalled jaxlib0.3.20   Attempting uninstall: jax     Found existing installation: jax 0.3.20     Uninstalling jax0.3.20:       Successfully uninstalled jax0.3.20 Successfully installed jax0.3.25 jaxlib0.3.25 zhangqiaorjcmacbookpro2:~ zhangqiaorjc$ python Python 3.10.1 (main, Mar  7 2022, 13:38:25) [Clang 13.0.0 (clang1300.0.29.30)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.__version__ '0.3.25' >>> import jaxlib >>> jaxlib.__version__ '0.3.25' >>> jax.device_put(1)+1 DeviceArray(2, dtype=int32, weak_type=True) ``` Can you try upgrade pip and also use the following instructions  https://github.com/google/jaxpipinstallationcpu If it still doesn't work, post the commands you used? also maybe the python version and other environment related things.","Are you using a Linux ARM64 image in docker? If that's the case, it's simply that we haven't released Linux ARM wheels yet (only Mac), see https://github.com/google/jax/issues/7097 You can fix this by building `jaxlib` from source. Eventually we are likely to release Linux aarch64 wheels, but we haven't done so at this time. I hope that helps!",The answer here solves the problem easily for me and many others (as seen in the number of upvotes),"> The answer here solves the problem easily for me and many others (as seen in the number of upvotes) If you use Linux/amd64 image on m1, indeed you can install jaxlib. But you won't be able to import JAX. As it will throw errors about the AVX instructions."," Yes, that's as expected. The x8664 (amd64) build of JAX uses AVX instructions, which Rosetta doesn't support. You either need to build an x8664 jaxlib without AVX support, or you need to use an aarch64 build of jaxlib. Both require that you build jaxlib from source at the moment."
365,"以下是一个github上的jax下的一个issue, 标题是(in jax.Array notebook, polish beginning and tweak title and some wording)， 内容是 (As discussed in chat, the start of the notebook was a bit out of order. Also, we can make the title more precise.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,"in jax.Array notebook, polish beginning and tweak title and some wording","As discussed in chat, the start of the notebook was a bit out of order. Also, we can make the title more precise.",2022-12-11T04:00:08Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/13602,I think you will need to change the link in the changelog.md too?,> I think you will need to change the link in the changelog.md too? Ah thanks I think I missed that one! EDIT: should be done now
22664,"以下是一个github上的jax下的一个issue, 标题是(.max() yields nan gradient despite individual values and gradients being nan-free)， 内容是 ( Description I have a strange bug: ```python3 x = [ 0.23814293  0.77400221  0.33243385  1.69167768 0.17876684 1.5681684 ] f(x) = [0.2637331  0.25321472] f(x).max() = 0.2637331010799241 grad(lambda x: f(x)[0])(x) = [0.06657368  0.0116883   0.03331883  0.00657176  0.16408419  0.00699502] grad(lambda x: f(x)[1])(x) = [0.04887613  0.02342647  0.07204693  0.0035732   0.16981069  0.00377417] grad(lambda x: f(x).max())(x) = [nan nan nan nan nan nan] grad(lambda x: f(x)[f(x).argmax()])(x) = [0.06657368  0.0116883   0.03331883  0.00657176  0.16408419  0.00699502] grad(lambda x: jnp.nanmax(f(x)))(x) = [0.06657368  0.0116883   0.03331883  0.00657176  0.16408419  0.00699502] ``` Letting `y = f(x)`: The gradient of `y.max()` is all `nan`s, even though the individual values and gradients are `nan`free. Replacing it with `y[y.argmax()]` or `jnp.nanmax(y)` fixes this. `make_jaxpr(f)(x)` is the following:  Jaxpr ``` { lambda a:f64[1] b:f64[1] c:f64[1] d:f64[1] e:f64[1] f:f64[1] g:f64[1] h:f64[1]     i:f64[1] j:f64[1] k:f64[1] l:f64[1] m:f64[2] n:f64[2] o:f64[2] p:f64[2] q:f64[2]     r:f64[2] s:f64[2] t:f64[2] u:f64[2] v:f64[2] w:f64[2] x:f64[2] y:f64[2] z:f64[2]     ba:f64[2] bb:f64[2] bc:f64[2] bd:f64[2] be:f64[2] bf:f64[2] bg:f64[2] bh:f64[2]     bi:f64[2] bj:f64[2] bk:f64[2] bl:f64[2] bm:f64[2] bn:f64[2] bo:f64[2] bp:f64[2]; bq:f64[6]. let     br:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=None] bq     bs:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] bq     bt:f64[1] = slice[limit_indices=(3,) start_indices=(2,) strides=None] bq     bu:f64[1] = slice[limit_indices=(4,) start_indices=(3,) strides=None] bq     bv:f64[1] = slice[limit_indices=(5,) start_indices=(4,) strides=None] bq     bw:f64[1] = slice[limit_indices=(6,) start_indices=(5,) strides=None] bq     bx:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] a     by:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] b     bz:f64[2,1] = concatenate[dimension=0] bx by     ca:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] c     cb:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] d     cc:f64[2,1] = concatenate[dimension=0] ca cb     cd:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] e     ce:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] f     cf:f64[2,1] = concatenate[dimension=0] cd ce     cg:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] g     ch:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] h     ci:f64[2,1] = concatenate[dimension=0] cg ch     cj:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] i     ck:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] j     cl:f64[2,1] = concatenate[dimension=0] cj ck     cm:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] k     cn:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] l     co:f64[2,1] = concatenate[dimension=0] cm cn     cp:f64[2,2] = xla_call[       call_jaxpr={ lambda ; cq:f64[2,1] cr:i64[]. let           cs:i64[1] = reshape[dimensions=None new_sizes=(1,)] cr           ct:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] cs           cu:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] ct           cv:f64[2,2] = concatenate[dimension=1] cq cu         in (cv,) }       name=append     ] bz 0     cw:f64[2] = reduce_max[axes=(1,)] cp     cx:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] cw     cy:f64[2,1] = stop_gradient cx     cz:f64[2,2] = sub cp cy     da:f64[2,2] = exp cz     db:f64[2] = reduce_sum[axes=(1,)] da     dc:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] db     dd:f64[2,2] = div da dc     de:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] dd     df:f64[2] = squeeze[dimensions=(1,)] de     dg:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] dd     dh:f64[2] = squeeze[dimensions=(1,)] dg     di:f64[2,2] = xla_call[       call_jaxpr={ lambda ; dj:f64[2,1] dk:i64[]. let           dl:i64[1] = reshape[dimensions=None new_sizes=(1,)] dk           dm:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] dl           dn:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] dm           do:f64[2,2] = concatenate[dimension=1] dj dn         in (do,) }       name=append     ] :f64[2] = reduce_max[axes=(1,)] di     dq:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dp     dr:f64[2,1] = stop_gradient dq     ds:f64[2,2] = sub di dr     dt:f64[2,2] = exp ds     du:f64[2] = reduce_sum[axes=(1,)] dt     dv:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] du     dw:f64[2,2] = div dt dv     dx:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] dw     dy:f64[2] = squeeze[dimensions=(1,)] dx     dz:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] dw     ea:f64[2] = squeeze[dimensions=(1,)] dz     eb:f64[2,2] = xla_call[       call_jaxpr={ lambda ; ec:f64[2,1] ed:i64[]. let           ee:i64[1] = reshape[dimensions=None new_sizes=(1,)] ed           ef:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] ee           eg:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] ef           eh:f64[2,2] = concatenate[dimension=1] ec eg         in (eh,) }       name=append     ] cf 0     ei:f64[2] = reduce_max[axes=(1,)] eb     ej:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] ei     ek:f64[2,1] = stop_gradient ej     el:f64[2,2] = sub eb ek     em:f64[2,2] = exp el     en:f64[2] = reduce_sum[axes=(1,)] em     eo:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] en     ep:f64[2,2] = div em eo     eq:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] ep     er:f64[2] = squeeze[dimensions=(1,)] eq     es:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] ep     et:f64[2] = squeeze[dimensions=(1,)] es     eu:f64[2,2] = xla_call[       call_jaxpr={ lambda ; ev:f64[2,1] ew:i64[]. let           ex:i64[1] = reshape[dimensions=None new_sizes=(1,)] ew           ey:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] ex           ez:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] ey           fa:f64[2,2] = concatenate[dimension=1] ev ez         in (fa,) }       name=append     ] ci 0     fb:f64[2] = reduce_max[axes=(1,)] eu     fc:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fb     fd:f64[2,1] = stop_gradient fc     fe:f64[2,2] = sub eu fd     ff:f64[2,2] = exp fe     fg:f64[2] = reduce_sum[axes=(1,)] ff     fh:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fg     fi:f64[2,2] = div ff fh     fj:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] fi     fk:f64[2] = squeeze[dimensions=(1,)] fj     fl:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] fi     fm:f64[2] = squeeze[dimensions=(1,)] fl     fn:f64[2,2] = xla_call[       call_jaxpr={ lambda ; fo:f64[2,1] fp:i64[]. let           fq:i64[1] = reshape[dimensions=None new_sizes=(1,)] fp           fr:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] fq           fs:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] fr           ft:f64[2,2] = concatenate[dimension=1] fo fs         in (ft,) }       name=append     ] cl 0     fu:f64[2] = reduce_max[axes=(1,)] fn     fv:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fu     fw:f64[2,1] = stop_gradient fv     fx:f64[2,2] = sub fn fw     fy:f64[2,2] = exp fx     fz:f64[2] = reduce_sum[axes=(1,)] fy     ga:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fz     gb:f64[2,2] = div fy ga     gc:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] gb     gd:f64[2] = squeeze[dimensions=(1,)] gc     ge:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] gb     gf:f64[2] = squeeze[dimensions=(1,)] ge     gg:f64[2,2] = xla_call[       call_jaxpr={ lambda ; gh:f64[2,1] gi:i64[]. let           gj:i64[1] = reshape[dimensions=None new_sizes=(1,)] gi           gk:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] gj           gl:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] gk           gm:f64[2,2] = concatenate[dimension=1] gh gl         in (gm,) }       name=append     ] co 0     gn:f64[2] = reduce_max[axes=(1,)] gg     go:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gn     gp:f64[2,1] = stop_gradient go     gq:f64[2,2] = sub gg gp     gr:f64[2,2] = exp gq     gs:f64[2] = reduce_sum[axes=(1,)] gr     gt:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gs     gu:f64[2,2] = div gr gt     gv:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] gu     gw:f64[2] = squeeze[dimensions=(1,)] gv     gx:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] gu     gy:f64[2] = squeeze[dimensions=(1,)] gx     gz:f64[2] = xla_call[       call_jaxpr={ lambda ; ha:f64[1] hb:i64[]. let           hc:i64[1] = reshape[dimensions=None new_sizes=(1,)] hb           hd:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] hc           he:f64[2] = concatenate[dimension=0] ha hd         in (he,) }       name=append     ] bu 0     hf:f64[] = reduce_max[axes=(0,)] gz     hg:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] hf     hh:f64[1] = stop_gradient hg     hi:f64[2] = sub gz hh     hj:f64[2] = exp hi     hk:f64[] = reduce_sum[axes=(0,)] hj     hl:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] hk     hm:f64[2] = div hj hl     hn:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] hm     ho:f64[] = squeeze[dimensions=(0,)] hn     hp:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] hm     hq:f64[] = squeeze[dimensions=(0,)] hp     hr:f64[2] = xla_call[       call_jaxpr={ lambda ; hs:f64[1] ht:i64[]. let           hu:i64[1] = reshape[dimensions=None new_sizes=(1,)] ht           hv:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] hu           hw:f64[2] = concatenate[dimension=0] hs hv         in (hw,) }       name=append     ] bt 0     hx:f64[] = reduce_max[axes=(0,)] hr     hy:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] hx     hz:f64[1] = stop_gradient hy     ia:f64[2] = sub hr hz     ib:f64[2] = exp ia     ic:f64[] = reduce_sum[axes=(0,)] ib     id:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] ic     ie:f64[2] = div ib id     if:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] ie     ig:f64[] = squeeze[dimensions=(0,)] if     ih:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] ie     ii:f64[] = squeeze[dimensions=(0,)] ih     ij:f64[2] = xla_call[       call_jaxpr={ lambda ; ik:f64[1] il:i64[]. let           im:i64[1] = reshape[dimensions=None new_sizes=(1,)] il           in:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] im           io:f64[2] = concatenate[dimension=0] ik in         in (io,) }       name=append     ] bw 0     ip:f64[] = reduce_max[axes=(0,)] ij     iq:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] ip     ir:f64[1] = stop_gradient iq     is:f64[2] = sub ij ir     it:f64[2] = exp is     iu:f64[] = reduce_sum[axes=(0,)] it     iv:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] iu     iw:f64[2] = div it iv     ix:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] iw     iy:f64[] = squeeze[dimensions=(0,)] ix     iz:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] iw     ja:f64[] = squeeze[dimensions=(0,)] iz     jb:f64[2] = xla_call[       call_jaxpr={ lambda ; jc:f64[1] jd:i64[]. let           je:i64[1] = reshape[dimensions=None new_sizes=(1,)] jd           jf:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] je           jg:f64[2] = concatenate[dimension=0] jc jf         in (jg,) }       name=append     ] bv 0     jh:f64[] = reduce_max[axes=(0,)] jb     ji:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] jh     jj:f64[1] = stop_gradient ji     jk:f64[2] = sub jb jj     jl:f64[2] = exp jk     jm:f64[] = reduce_sum[axes=(0,)] jl     jn:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] jm     jo:f64[2] = div jl jn     jp:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] jo     jq:f64[] = squeeze[dimensions=(0,)] jp     jr:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] jo     js:f64[] = squeeze[dimensions=(0,)] jr     jt:f64[2] = xla_call[       call_jaxpr={ lambda ; ju:f64[1] jv:i64[]. let           jw:i64[1] = reshape[dimensions=None new_sizes=(1,)] jv           jx:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] jw           jy:f64[2] = concatenate[dimension=0] ju jx         in (jy,) }       name=append     ] bs 0     jz:f64[] = reduce_max[axes=(0,)] jt     ka:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] jz     kb:f64[1] = stop_gradient ka     kc:f64[2] = sub jt kb     kd:f64[2] = exp kc     ke:f64[] = reduce_sum[axes=(0,)] kd     kf:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] ke     kg:f64[2] = div kd kf     kh:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] kg     ki:f64[] = squeeze[dimensions=(0,)] kh     kj:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] kg     kk:f64[] = squeeze[dimensions=(0,)] kj     kl:f64[2] = xla_call[       call_jaxpr={ lambda ; km:f64[1] kn:i64[]. let           ko:i64[1] = reshape[dimensions=None new_sizes=(1,)] kn           kp:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] ko           kq:f64[2] = concatenate[dimension=0] km kp         in (kq,) }       name=append     ] br 0     kr:f64[] = reduce_max[axes=(0,)] kl     ks:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] kr     kt:f64[1] = stop_gradient ks     ku:f64[2] = sub kl kt     kv:f64[2] = exp ku     kw:f64[] = reduce_sum[axes=(0,)] kv     kx:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] kw     ky:f64[2] = div kv kx     kz:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] ky     la:f64[] = squeeze[dimensions=(0,)] kz     lb:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] ky     lc:f64[] = squeeze[dimensions=(0,)] lb     ld:f64[2] = mul ho m     le:f64[2] = add 0.0 ld     lf:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] n     lg:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dy     lh:f64[2,2] = mul lg lf     li:f64[2,2] = add 0.0 lh     lj:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] o     lk:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] ea     ll:f64[2,2] = mul lk lj     lm:f64[2,2] = add li ll     ln:f64[2,2] = mul hq lm     lo:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] le     lp:f64[2,2] = add lo ln     lq:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] df     lr:f64[2,2] = mul lq lp     ls:f64[2,2] = add 0.0 lr     lt:f64[2] = mul ig p     lu:f64[2] = add 0.0 lt     lv:f64[2] = mul ii q     lw:f64[2] = add lu lv     lx:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] lw     ly:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dh     lz:f64[2,2] = mul ly lx     ma:f64[2,2] = add ls lz     mb:f64[2,2] = mul 0.5 ma     mc:f64[2,2] = add 0.0 mb     md:f64[2] = mul iy r     me:f64[2] = add 0.0 md     mf:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] s     mg:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dy     mh:f64[2,2] = mul mg mf     mi:f64[2,2] = add 0.0 mh     mj:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] t     mk:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] ea     ml:f64[2,2] = mul mk mj     mm:f64[2,2] = add mi ml     mn:f64[2,2] = mul ja mm     mo:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] me     mp:f64[2,2] = add mo mn     mq:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] df     mr:f64[2,2] = mul mq mp     ms:f64[2,2] = add 0.0 mr     mt:f64[2] = mul jq u     mu:f64[2] = add 0.0 mt     mv:f64[2] = mul js v     mw:f64[2] = add mu mv     mx:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] mw     my:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dh     mz:f64[2,2] = mul my mx     na:f64[2,2] = add ms mz     nb:f64[2,2] = mul 0.5 na     nc:f64[2,2] = add mc nb     nd:f64[2,2] = mul 0.3333333333333333 nc     ne:f64[2,2] = add 0.0 nd     nf:f64[2] = mul ki w     ng:f64[2] = add 0.0 nf     nh:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] x     ni:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fk     nj:f64[2,2] = mul ni nh     nk:f64[2,2] = add 0.0 nj     nl:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] y     nm:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fm     nn:f64[2,2] = mul nm nl     no:f64[2,2] = add nk nn     np:f64[2,2] = mul kk no     nq:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] ng     nr:f64[2,2] = add nq np     ns:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] er     nt:f64[2,2] = mul ns nr     nu:f64[2,2] = add 0.0 nt     nv:f64[2] = mul la z     nw:f64[2] = add 0.0 nv     nx:f64[2] = mul lc ba     ny:f64[2] = add nw nx     nz:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] ny     oa:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] et     ob:f64[2,2] = mul oa nz     oc:f64[2,2] = add nu ob     od:f64[2,2] = mul 0.5 oc     oe:f64[2,2] = add 0.0 od     of:f64[2] = mul iy bb     og:f64[2] = add 0.0 of     oh:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bc     oi:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fk     oj:f64[2,2] = mul oi oh     ok:f64[2,2] = add 0.0 oj     ol:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bd     om:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fm     on:f64[2,2] = mul om ol     oo:f64[2,2] = add ok on     op:f64[2,2] = mul ja oo     oq:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] og     or:f64[2,2] = add oq op     os:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] er     ot:f64[2,2] = mul os or     ou:f64[2,2] = add 0.0 ot     ov:f64[2] = mul jq be     ow:f64[2] = add 0.0 ov     ox:f64[2] = mul js bf     oy:f64[2] = add ow ox     oz:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] oy     pa:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] et     pb:f64[2,2] = mul pa oz     pc:f64[2,2] = add ou pb     pd:f64[2,2] = mul 0.5 pc     pe:f64[2,2] = add oe pd     pf:f64[2,2] = mul 0.3333333333333333 pe     pg:f64[2,2] = add ne pf     ph:f64[2] = mul ki bg     pi:f64[2] = add 0.0 ph     pj:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bh     pk:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gw     pl:f64[2,2] = mul pk pj     pm:f64[2,2] = add 0.0 pl     pn:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bi     po:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gy     pp:f64[2,2] = mul po pn     pq:f64[2,2] = add pm pp     pr:f64[2,2] = mul kk pq     ps:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] pi     pt:f64[2,2] = add ps pr     pu:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gd     pv:f64[2,2] = mul pu pt     pw:f64[2,2] = add 0.0 pv     px:f64[2] = mul la bj     py:f64[2] = add 0.0 px     pz:f64[2] = mul lc bk     qa:f64[2] = add py pz     qb:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] qa     qc:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gf     qd:f64[2,2] = mul qc qb     qe:f64[2,2] = add pw qd     qf:f64[2,2] = mul 0.5 qe     qg:f64[2,2] = add 0.0 qf     qh:f64[2] = mul ho bl     qi:f64[2] = add 0.0 qh     qj:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bm     qk:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gw     ql:f64[2,2] = mul qk qj     qm:f64[2,2] = add 0.0 ql     qn:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bn     qo:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gy     qp:f64[2,2] = mul qo qn     qq:f64[2,2] = add qm qp     qr:f64[2,2] = mul hq qq     qs:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] qi     qt:f64[2,2] = add qs qr     qu:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gd     qv:f64[2,2] = mul qu qt     qw:f64[2,2] = add 0.0 qv     qx:f64[2] = mul ig bo     qy:f64[2] = add 0.0 qx     qz:f64[2] = mul ii bp     ra:f64[2] = add qy qz     rb:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] ra     rc:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gf     rd:f64[2,2] = mul rc rb     re:f64[2,2] = add qw rd     rf:f64[2,2] = mul 0.5 re     rg:f64[2,2] = add qg rf     rh:f64[2,2] = mul 0.3333333333333333 rg     ri:f64[2,2] = add pg rh     rj:i64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     rk:f64[2,1] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(0, 1), collapsed_slice_dims=(), start_index_map=(1,))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(2, 1)       unique_indices=True     ] ri rj     rl:f64[2] = squeeze[dimensions=(1,)] rk   in (rl,) } ```   What jax/jaxlib version are you using? jax 0.3.25, jaxlib 0.3.25  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8, macOS 11.7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,.max() yields nan gradient despite individual values and gradients being nan-free," Description I have a strange bug: ```python3 x = [ 0.23814293  0.77400221  0.33243385  1.69167768 0.17876684 1.5681684 ] f(x) = [0.2637331  0.25321472] f(x).max() = 0.2637331010799241 grad(lambda x: f(x)[0])(x) = [0.06657368  0.0116883   0.03331883  0.00657176  0.16408419  0.00699502] grad(lambda x: f(x)[1])(x) = [0.04887613  0.02342647  0.07204693  0.0035732   0.16981069  0.00377417] grad(lambda x: f(x).max())(x) = [nan nan nan nan nan nan] grad(lambda x: f(x)[f(x).argmax()])(x) = [0.06657368  0.0116883   0.03331883  0.00657176  0.16408419  0.00699502] grad(lambda x: jnp.nanmax(f(x)))(x) = [0.06657368  0.0116883   0.03331883  0.00657176  0.16408419  0.00699502] ``` Letting `y = f(x)`: The gradient of `y.max()` is all `nan`s, even though the individual values and gradients are `nan`free. Replacing it with `y[y.argmax()]` or `jnp.nanmax(y)` fixes this. `make_jaxpr(f)(x)` is the following:  Jaxpr ``` { lambda a:f64[1] b:f64[1] c:f64[1] d:f64[1] e:f64[1] f:f64[1] g:f64[1] h:f64[1]     i:f64[1] j:f64[1] k:f64[1] l:f64[1] m:f64[2] n:f64[2] o:f64[2] p:f64[2] q:f64[2]     r:f64[2] s:f64[2] t:f64[2] u:f64[2] v:f64[2] w:f64[2] x:f64[2] y:f64[2] z:f64[2]     ba:f64[2] bb:f64[2] bc:f64[2] bd:f64[2] be:f64[2] bf:f64[2] bg:f64[2] bh:f64[2]     bi:f64[2] bj:f64[2] bk:f64[2] bl:f64[2] bm:f64[2] bn:f64[2] bo:f64[2] bp:f64[2]; bq:f64[6]. let     br:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=None] bq     bs:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] bq     bt:f64[1] = slice[limit_indices=(3,) start_indices=(2,) strides=None] bq     bu:f64[1] = slice[limit_indices=(4,) start_indices=(3,) strides=None] bq     bv:f64[1] = slice[limit_indices=(5,) start_indices=(4,) strides=None] bq     bw:f64[1] = slice[limit_indices=(6,) start_indices=(5,) strides=None] bq     bx:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] a     by:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] b     bz:f64[2,1] = concatenate[dimension=0] bx by     ca:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] c     cb:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] d     cc:f64[2,1] = concatenate[dimension=0] ca cb     cd:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] e     ce:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] f     cf:f64[2,1] = concatenate[dimension=0] cd ce     cg:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] g     ch:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] h     ci:f64[2,1] = concatenate[dimension=0] cg ch     cj:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] i     ck:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] j     cl:f64[2,1] = concatenate[dimension=0] cj ck     cm:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] k     cn:f64[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] l     co:f64[2,1] = concatenate[dimension=0] cm cn     cp:f64[2,2] = xla_call[       call_jaxpr={ lambda ; cq:f64[2,1] cr:i64[]. let           cs:i64[1] = reshape[dimensions=None new_sizes=(1,)] cr           ct:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] cs           cu:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] ct           cv:f64[2,2] = concatenate[dimension=1] cq cu         in (cv,) }       name=append     ] bz 0     cw:f64[2] = reduce_max[axes=(1,)] cp     cx:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] cw     cy:f64[2,1] = stop_gradient cx     cz:f64[2,2] = sub cp cy     da:f64[2,2] = exp cz     db:f64[2] = reduce_sum[axes=(1,)] da     dc:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] db     dd:f64[2,2] = div da dc     de:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] dd     df:f64[2] = squeeze[dimensions=(1,)] de     dg:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] dd     dh:f64[2] = squeeze[dimensions=(1,)] dg     di:f64[2,2] = xla_call[       call_jaxpr={ lambda ; dj:f64[2,1] dk:i64[]. let           dl:i64[1] = reshape[dimensions=None new_sizes=(1,)] dk           dm:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] dl           dn:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] dm           do:f64[2,2] = concatenate[dimension=1] dj dn         in (do,) }       name=append     ] :f64[2] = reduce_max[axes=(1,)] di     dq:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dp     dr:f64[2,1] = stop_gradient dq     ds:f64[2,2] = sub di dr     dt:f64[2,2] = exp ds     du:f64[2] = reduce_sum[axes=(1,)] dt     dv:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] du     dw:f64[2,2] = div dt dv     dx:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] dw     dy:f64[2] = squeeze[dimensions=(1,)] dx     dz:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] dw     ea:f64[2] = squeeze[dimensions=(1,)] dz     eb:f64[2,2] = xla_call[       call_jaxpr={ lambda ; ec:f64[2,1] ed:i64[]. let           ee:i64[1] = reshape[dimensions=None new_sizes=(1,)] ed           ef:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] ee           eg:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] ef           eh:f64[2,2] = concatenate[dimension=1] ec eg         in (eh,) }       name=append     ] cf 0     ei:f64[2] = reduce_max[axes=(1,)] eb     ej:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] ei     ek:f64[2,1] = stop_gradient ej     el:f64[2,2] = sub eb ek     em:f64[2,2] = exp el     en:f64[2] = reduce_sum[axes=(1,)] em     eo:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] en     ep:f64[2,2] = div em eo     eq:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] ep     er:f64[2] = squeeze[dimensions=(1,)] eq     es:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] ep     et:f64[2] = squeeze[dimensions=(1,)] es     eu:f64[2,2] = xla_call[       call_jaxpr={ lambda ; ev:f64[2,1] ew:i64[]. let           ex:i64[1] = reshape[dimensions=None new_sizes=(1,)] ew           ey:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] ex           ez:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] ey           fa:f64[2,2] = concatenate[dimension=1] ev ez         in (fa,) }       name=append     ] ci 0     fb:f64[2] = reduce_max[axes=(1,)] eu     fc:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fb     fd:f64[2,1] = stop_gradient fc     fe:f64[2,2] = sub eu fd     ff:f64[2,2] = exp fe     fg:f64[2] = reduce_sum[axes=(1,)] ff     fh:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fg     fi:f64[2,2] = div ff fh     fj:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] fi     fk:f64[2] = squeeze[dimensions=(1,)] fj     fl:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] fi     fm:f64[2] = squeeze[dimensions=(1,)] fl     fn:f64[2,2] = xla_call[       call_jaxpr={ lambda ; fo:f64[2,1] fp:i64[]. let           fq:i64[1] = reshape[dimensions=None new_sizes=(1,)] fp           fr:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] fq           fs:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] fr           ft:f64[2,2] = concatenate[dimension=1] fo fs         in (ft,) }       name=append     ] cl 0     fu:f64[2] = reduce_max[axes=(1,)] fn     fv:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fu     fw:f64[2,1] = stop_gradient fv     fx:f64[2,2] = sub fn fw     fy:f64[2,2] = exp fx     fz:f64[2] = reduce_sum[axes=(1,)] fy     ga:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fz     gb:f64[2,2] = div fy ga     gc:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] gb     gd:f64[2] = squeeze[dimensions=(1,)] gc     ge:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] gb     gf:f64[2] = squeeze[dimensions=(1,)] ge     gg:f64[2,2] = xla_call[       call_jaxpr={ lambda ; gh:f64[2,1] gi:i64[]. let           gj:i64[1] = reshape[dimensions=None new_sizes=(1,)] gi           gk:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] gj           gl:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(2, 1)] gk           gm:f64[2,2] = concatenate[dimension=1] gh gl         in (gm,) }       name=append     ] co 0     gn:f64[2] = reduce_max[axes=(1,)] gg     go:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gn     gp:f64[2,1] = stop_gradient go     gq:f64[2,2] = sub gg gp     gr:f64[2,2] = exp gq     gs:f64[2] = reduce_sum[axes=(1,)] gr     gt:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gs     gu:f64[2,2] = div gr gt     gv:f64[2,1] = slice[       limit_indices=(2, 1)       start_indices=(0, 0)       strides=(1, 1)     ] gu     gw:f64[2] = squeeze[dimensions=(1,)] gv     gx:f64[2,1] = slice[       limit_indices=(2, 2)       start_indices=(0, 1)       strides=(1, 1)     ] gu     gy:f64[2] = squeeze[dimensions=(1,)] gx     gz:f64[2] = xla_call[       call_jaxpr={ lambda ; ha:f64[1] hb:i64[]. let           hc:i64[1] = reshape[dimensions=None new_sizes=(1,)] hb           hd:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] hc           he:f64[2] = concatenate[dimension=0] ha hd         in (he,) }       name=append     ] bu 0     hf:f64[] = reduce_max[axes=(0,)] gz     hg:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] hf     hh:f64[1] = stop_gradient hg     hi:f64[2] = sub gz hh     hj:f64[2] = exp hi     hk:f64[] = reduce_sum[axes=(0,)] hj     hl:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] hk     hm:f64[2] = div hj hl     hn:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] hm     ho:f64[] = squeeze[dimensions=(0,)] hn     hp:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] hm     hq:f64[] = squeeze[dimensions=(0,)] hp     hr:f64[2] = xla_call[       call_jaxpr={ lambda ; hs:f64[1] ht:i64[]. let           hu:i64[1] = reshape[dimensions=None new_sizes=(1,)] ht           hv:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] hu           hw:f64[2] = concatenate[dimension=0] hs hv         in (hw,) }       name=append     ] bt 0     hx:f64[] = reduce_max[axes=(0,)] hr     hy:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] hx     hz:f64[1] = stop_gradient hy     ia:f64[2] = sub hr hz     ib:f64[2] = exp ia     ic:f64[] = reduce_sum[axes=(0,)] ib     id:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] ic     ie:f64[2] = div ib id     if:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] ie     ig:f64[] = squeeze[dimensions=(0,)] if     ih:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] ie     ii:f64[] = squeeze[dimensions=(0,)] ih     ij:f64[2] = xla_call[       call_jaxpr={ lambda ; ik:f64[1] il:i64[]. let           im:i64[1] = reshape[dimensions=None new_sizes=(1,)] il           in:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] im           io:f64[2] = concatenate[dimension=0] ik in         in (io,) }       name=append     ] bw 0     ip:f64[] = reduce_max[axes=(0,)] ij     iq:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] ip     ir:f64[1] = stop_gradient iq     is:f64[2] = sub ij ir     it:f64[2] = exp is     iu:f64[] = reduce_sum[axes=(0,)] it     iv:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] iu     iw:f64[2] = div it iv     ix:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] iw     iy:f64[] = squeeze[dimensions=(0,)] ix     iz:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] iw     ja:f64[] = squeeze[dimensions=(0,)] iz     jb:f64[2] = xla_call[       call_jaxpr={ lambda ; jc:f64[1] jd:i64[]. let           je:i64[1] = reshape[dimensions=None new_sizes=(1,)] jd           jf:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] je           jg:f64[2] = concatenate[dimension=0] jc jf         in (jg,) }       name=append     ] bv 0     jh:f64[] = reduce_max[axes=(0,)] jb     ji:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] jh     jj:f64[1] = stop_gradient ji     jk:f64[2] = sub jb jj     jl:f64[2] = exp jk     jm:f64[] = reduce_sum[axes=(0,)] jl     jn:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] jm     jo:f64[2] = div jl jn     jp:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] jo     jq:f64[] = squeeze[dimensions=(0,)] jp     jr:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] jo     js:f64[] = squeeze[dimensions=(0,)] jr     jt:f64[2] = xla_call[       call_jaxpr={ lambda ; ju:f64[1] jv:i64[]. let           jw:i64[1] = reshape[dimensions=None new_sizes=(1,)] jv           jx:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] jw           jy:f64[2] = concatenate[dimension=0] ju jx         in (jy,) }       name=append     ] bs 0     jz:f64[] = reduce_max[axes=(0,)] jt     ka:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] jz     kb:f64[1] = stop_gradient ka     kc:f64[2] = sub jt kb     kd:f64[2] = exp kc     ke:f64[] = reduce_sum[axes=(0,)] kd     kf:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] ke     kg:f64[2] = div kd kf     kh:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] kg     ki:f64[] = squeeze[dimensions=(0,)] kh     kj:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] kg     kk:f64[] = squeeze[dimensions=(0,)] kj     kl:f64[2] = xla_call[       call_jaxpr={ lambda ; km:f64[1] kn:i64[]. let           ko:i64[1] = reshape[dimensions=None new_sizes=(1,)] kn           kp:f64[1] = convert_element_type[new_dtype=float64 weak_type=False] ko           kq:f64[2] = concatenate[dimension=0] km kp         in (kq,) }       name=append     ] br 0     kr:f64[] = reduce_max[axes=(0,)] kl     ks:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] kr     kt:f64[1] = stop_gradient ks     ku:f64[2] = sub kl kt     kv:f64[2] = exp ku     kw:f64[] = reduce_sum[axes=(0,)] kv     kx:f64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] kw     ky:f64[2] = div kv kx     kz:f64[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] ky     la:f64[] = squeeze[dimensions=(0,)] kz     lb:f64[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] ky     lc:f64[] = squeeze[dimensions=(0,)] lb     ld:f64[2] = mul ho m     le:f64[2] = add 0.0 ld     lf:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] n     lg:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dy     lh:f64[2,2] = mul lg lf     li:f64[2,2] = add 0.0 lh     lj:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] o     lk:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] ea     ll:f64[2,2] = mul lk lj     lm:f64[2,2] = add li ll     ln:f64[2,2] = mul hq lm     lo:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] le     lp:f64[2,2] = add lo ln     lq:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] df     lr:f64[2,2] = mul lq lp     ls:f64[2,2] = add 0.0 lr     lt:f64[2] = mul ig p     lu:f64[2] = add 0.0 lt     lv:f64[2] = mul ii q     lw:f64[2] = add lu lv     lx:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] lw     ly:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dh     lz:f64[2,2] = mul ly lx     ma:f64[2,2] = add ls lz     mb:f64[2,2] = mul 0.5 ma     mc:f64[2,2] = add 0.0 mb     md:f64[2] = mul iy r     me:f64[2] = add 0.0 md     mf:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] s     mg:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dy     mh:f64[2,2] = mul mg mf     mi:f64[2,2] = add 0.0 mh     mj:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] t     mk:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] ea     ml:f64[2,2] = mul mk mj     mm:f64[2,2] = add mi ml     mn:f64[2,2] = mul ja mm     mo:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] me     mp:f64[2,2] = add mo mn     mq:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] df     mr:f64[2,2] = mul mq mp     ms:f64[2,2] = add 0.0 mr     mt:f64[2] = mul jq u     mu:f64[2] = add 0.0 mt     mv:f64[2] = mul js v     mw:f64[2] = add mu mv     mx:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] mw     my:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] dh     mz:f64[2,2] = mul my mx     na:f64[2,2] = add ms mz     nb:f64[2,2] = mul 0.5 na     nc:f64[2,2] = add mc nb     nd:f64[2,2] = mul 0.3333333333333333 nc     ne:f64[2,2] = add 0.0 nd     nf:f64[2] = mul ki w     ng:f64[2] = add 0.0 nf     nh:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] x     ni:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fk     nj:f64[2,2] = mul ni nh     nk:f64[2,2] = add 0.0 nj     nl:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] y     nm:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fm     nn:f64[2,2] = mul nm nl     no:f64[2,2] = add nk nn     np:f64[2,2] = mul kk no     nq:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] ng     nr:f64[2,2] = add nq np     ns:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] er     nt:f64[2,2] = mul ns nr     nu:f64[2,2] = add 0.0 nt     nv:f64[2] = mul la z     nw:f64[2] = add 0.0 nv     nx:f64[2] = mul lc ba     ny:f64[2] = add nw nx     nz:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] ny     oa:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] et     ob:f64[2,2] = mul oa nz     oc:f64[2,2] = add nu ob     od:f64[2,2] = mul 0.5 oc     oe:f64[2,2] = add 0.0 od     of:f64[2] = mul iy bb     og:f64[2] = add 0.0 of     oh:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bc     oi:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fk     oj:f64[2,2] = mul oi oh     ok:f64[2,2] = add 0.0 oj     ol:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bd     om:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] fm     on:f64[2,2] = mul om ol     oo:f64[2,2] = add ok on     op:f64[2,2] = mul ja oo     oq:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] og     or:f64[2,2] = add oq op     os:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] er     ot:f64[2,2] = mul os or     ou:f64[2,2] = add 0.0 ot     ov:f64[2] = mul jq be     ow:f64[2] = add 0.0 ov     ox:f64[2] = mul js bf     oy:f64[2] = add ow ox     oz:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] oy     pa:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] et     pb:f64[2,2] = mul pa oz     pc:f64[2,2] = add ou pb     pd:f64[2,2] = mul 0.5 pc     pe:f64[2,2] = add oe pd     pf:f64[2,2] = mul 0.3333333333333333 pe     pg:f64[2,2] = add ne pf     ph:f64[2] = mul ki bg     pi:f64[2] = add 0.0 ph     pj:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bh     pk:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gw     pl:f64[2,2] = mul pk pj     pm:f64[2,2] = add 0.0 pl     pn:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bi     po:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gy     pp:f64[2,2] = mul po pn     pq:f64[2,2] = add pm pp     pr:f64[2,2] = mul kk pq     ps:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] pi     pt:f64[2,2] = add ps pr     pu:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gd     pv:f64[2,2] = mul pu pt     pw:f64[2,2] = add 0.0 pv     px:f64[2] = mul la bj     py:f64[2] = add 0.0 px     pz:f64[2] = mul lc bk     qa:f64[2] = add py pz     qb:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] qa     qc:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gf     qd:f64[2,2] = mul qc qb     qe:f64[2,2] = add pw qd     qf:f64[2,2] = mul 0.5 qe     qg:f64[2,2] = add 0.0 qf     qh:f64[2] = mul ho bl     qi:f64[2] = add 0.0 qh     qj:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bm     qk:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gw     ql:f64[2,2] = mul qk qj     qm:f64[2,2] = add 0.0 ql     qn:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] bn     qo:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gy     qp:f64[2,2] = mul qo qn     qq:f64[2,2] = add qm qp     qr:f64[2,2] = mul hq qq     qs:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] qi     qt:f64[2,2] = add qs qr     qu:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gd     qv:f64[2,2] = mul qu qt     qw:f64[2,2] = add 0.0 qv     qx:f64[2] = mul ig bo     qy:f64[2] = add 0.0 qx     qz:f64[2] = mul ii bp     ra:f64[2] = add qy qz     rb:f64[1,2] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 2)] ra     rc:f64[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] gf     rd:f64[2,2] = mul rc rb     re:f64[2,2] = add qw rd     rf:f64[2,2] = mul 0.5 re     rg:f64[2,2] = add qg rf     rh:f64[2,2] = mul 0.3333333333333333 rg     ri:f64[2,2] = add pg rh     rj:i64[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     rk:f64[2,1] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(0, 1), collapsed_slice_dims=(), start_index_map=(1,))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(2, 1)       unique_indices=True     ] ri rj     rl:f64[2] = squeeze[dimensions=(1,)] rk   in (rl,) } ```   What jax/jaxlib version are you using? jax 0.3.25, jaxlib 0.3.25  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8, macOS 11.7  NVIDIA GPU info _No response_",2022-12-10T23:18:44Z,bug,open,2,6,https://github.com/jax-ml/jax/issues/13600,"I wonder if checkify nan_checker can narrow it down  https://jax.readthedocs.io/en/latest/debugging/checkify_guide.html?highlight=checkifythecheckifytransformation Also, do you have a `jnp.where` in your `f`? See https://jax.readthedocs.io/en/latest/faq.html?highlight=nangradientscontainnanwhereusingwhere"," I tried using checkify with `errors=checkify.all_checks` (is `checkify.all_errors` a typo on the linked page?) and `err.throw()`. No error was raised. To my knowledge, `f` does not use `jnp.where`. Perhaps a clue could lie in the differences between how the gradients of `_.max()` are computed and how the gradients of `_[_.argmax()]` / `jnp.nanmax(_)` are computed?","You can also use `jax_debug_nans`, though it might raise the error inside JAX's backwardpass code, which might be unfamiliar. Any chance you could share a runnable repro? (The smaller the better!)","JAX treats differentiation of `lambda x: x[x.argmax()]` differently from how it treats differentiation of `lambda x: x.max()`. The difference is in how ties are handled, i.e. nonunique maxima: ```python import jax def f1(x):   return x.max() def f2(x):   return x[x.argmax()] x = jnp.ones(2) print(jax.grad(f1)(x))   [0.5 0.5] print(jax.grad(f2)(x))   [1. 0.] ``` The gradient (in the sense of a representer vector for the Frechet derivative) is mathematically not well defined for `(x, y) \mapsto max(x, y)` where `x == y`. So we have to either error or choose some convention. One viable convention is to just pick one of the directional derivatives, e.g. corresponding to the choice of the first maximum (first according to, say, the lefttoright order in a flattened version of the input). Rather than arbitrarily choosing one of the maxima, another convention is to be symmetric under permutation of the maxima. In JAX we chose the latter. (Actually we chose it when writing the original Autograd, then stuck with it for JAX.) But that means that `lambda x: x[x.argmax()]` differentiates differently than `lambda x: x.max()`, because `argmax` is defined (in its docstring!) to choose the first maximum when there are multiple. (This pattern is common in autodiff: different programminglanguage denotations of the same mathematical function can lead to different automatic derivatives.) A consequence is that there may be functions `f` and values of `x` such that `grad(lambda x: f1(f(x))(x)` is `nan` but `grad(lambda x: f2(f(x))(x)` is not. But I'm interested to see if that's what's being constructed! Or maybe there's something else going on. In other words, I'm just explaining why `x.max()` and `x[x.argmax()]` could behave differently under autodiff. But I don't know exactly what's going on without a repro."," This function is buried deep inside a jitted part of the program and is entangled with a lot of other code. I'm having some trouble extracting a minimal example. Setting the `JAX_DEBUG_NANS=True` environment variable yields  ``` Traceback (most recent call last): [...]     return lax.scan(lambda x, _: (f(x), x), x, None, n + 1)[1]   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/lax/control_flow/loops.py"", line 275, in scan     out = scan_p.bind(*consts, *in_flat,   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/lax/control_flow/loops.py"", line 1000, in scan_bind     return core.AxisPrimitive.bind(scan_p, *args, **params)   File ""/usr/local/lib/python3.10/sitepackages/jax/core.py"", line 2444, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/usr/local/lib/python3.10/sitepackages/jax/core.py"", line 332, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/usr/local/lib/python3.10/sitepackages/jax/core.py"", line 712, in process_primitive     return primitive.impl(*tracers, **params)   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 115, in apply_primitive     return compiled_fun(*args)   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 896, in _execute_compiled     check_special(name, out_flat)   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 844, in check_special     _check_special(name, buf.dtype, buf)   File ""/usr/local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 849, in _check_special     raise FloatingPointError(f""invalid value (nan) encountered in {name}"") jax._src.traceback_util.UnfilteredStackTrace: FloatingPointError: invalid value (nan) encountered in scan ```  For some reason, the trace seems to go no ""deeper"" than the toplevel `scan` that creates the jitted part of the program. It's interesting that `jnp.nanmax` works when `jnp.max` doesn't, despite the lack of `nan` values. `nanmax` also uses the permutationsymmetric convention you mentioned. Where can one find the gradientcomputation code for these two functions, to compare them side by side? Perhaps carefully examining `max`'s gradientcomputation code could reveal some kind of edge/failure case? Regarding a reproducible example, is it possible to get some representation of `f` with concrete values for all constants, while deep inside a jit? I tried setting up some code to manually detect when a `nan` gradient appears and run `jax.debug.callback(callback_fn)` if it does. However, playing around with `f` inside that callback leads to complaints about leaks:  ``` *** jax._src.errors.UnexpectedTracerError: Encountered an unexpected tracer. A function transformed by JAX had a side effect, allowing for a reference to an intermediate value with type float64[6] wrapped in a DynamicJaxprTracer to escape the scope of the transformation. JAX transformations require that functions explicitly return their outputs, and disallow saving intermediate values to global state. The function being traced when the value leaked was  at [...] traced for scan.  The leaked intermediate value was created on line [...].   When the value was created, the final 5 stack frames (most recent last) excluding JAXinternal frames were:  [...]  To catch the leak earlier, try setting the environment variable JAX_CHECK_TRACER_LEAKS or using the `jax.checking_leaks` context manager. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.UnexpectedTracerError ```  I also tried explicitly passing the relevant functions and values to `jax.debug.callback` through the `*args`, but it then complains that the functions are not valid JAX types.","I solved the above callback issues by using something of the form ``` jax.debug.callback(functools.partial(callback_fn, *all_static_args), *all_dynamic_args) ``` However, something strange happens: When I compute and print the same gradient inside the callback, the `nan`s disappear and the values are correct (still using `jnp.max`)."
567,"以下是一个github上的jax下的一个issue, 标题是(Enable rules configurability by making rules a context.)， 内容是 (Enable rules configurability by making rules a context. Ensure all variants of batch sharding work.  Separate out oversharding, this is extremely brittle and can be solved later (all pareto optimal benchmarks are on chips <= num_heads) Trying to mark layers_parallel as copied, not new? Fixed sizing bug in kv_cache sharding schemes)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Enable rules configurability by making rules a context.,"Enable rules configurability by making rules a context. Ensure all variants of batch sharding work.  Separate out oversharding, this is extremely brittle and can be solved later (all pareto optimal benchmarks are on chips <= num_heads) Trying to mark layers_parallel as copied, not new? Fixed sizing bug in kv_cache sharding schemes",2022-12-10T21:46:49Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13598
1708,"以下是一个github上的jax下的一个issue, 标题是(Accessing an element in a DeviceArray is slow and adds significant compute time)， 内容是 ( Description Hello,  I am running into an issue where accessing elements in a jnp.array takes too long. Usually much longer than the computation itself. I have a feeling this is because of the lazy execution of jax DeviceArrays. I wonder if there is a way to speed this up. Here is an example below where accessing DeviceArray elements takes **13x** longer.  ```  x=jnp.ones((1000,1)) y=jnp.zeros((200)) def test(x,y):     out=0     for i in range(y.shape[0]):         if i==1:             out=out+x*y[i]         elif i==2:             out=out+x*y[i]*4.         else:              out=out+x*y[i]*10.     return out %timeit test(x,y).block_until_ready() 33 ms ± 2.98 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) ``` However, if I do not access y[i], it only takes 2.52 ms.  ``` x=jnp.ones((1000,1)) y=jnp.zeros((200)) def test(x,y):     out=0     for i in range(y.shape[0]):         if i==1:             out=out+x*i         elif i==2:             out=out+x*i*4.         else:              out=out+x*i*10.     return out %timeit test(x,y).block_until_ready() 2.52 ms ± 267 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) ``` Is there a workaround for this? Thanks a lot.   What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? CPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Accessing an element in a DeviceArray is slow and adds significant compute time," Description Hello,  I am running into an issue where accessing elements in a jnp.array takes too long. Usually much longer than the computation itself. I have a feeling this is because of the lazy execution of jax DeviceArrays. I wonder if there is a way to speed this up. Here is an example below where accessing DeviceArray elements takes **13x** longer.  ```  x=jnp.ones((1000,1)) y=jnp.zeros((200)) def test(x,y):     out=0     for i in range(y.shape[0]):         if i==1:             out=out+x*y[i]         elif i==2:             out=out+x*y[i]*4.         else:              out=out+x*y[i]*10.     return out %timeit test(x,y).block_until_ready() 33 ms ± 2.98 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) ``` However, if I do not access y[i], it only takes 2.52 ms.  ``` x=jnp.ones((1000,1)) y=jnp.zeros((200)) def test(x,y):     out=0     for i in range(y.shape[0]):         if i==1:             out=out+x*i         elif i==2:             out=out+x*i*4.         else:              out=out+x*i*10.     return out %timeit test(x,y).block_until_ready() 2.52 ms ± 267 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) ``` Is there a workaround for this? Thanks a lot.   What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? CPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info _No response_",2022-12-08T20:29:39Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/13572,"Hi  thanks for the question. This is working as expected: JAX is not optimized for speed of sequences of individuallydispatched operations, but rather optimized for speed of JITcompiled sequences of operations. If you modify your timings to use a jitcompiled version of your function instead, you should see that the two functions end up having similar timings: ```python jit_test = jax.jit(test) _ = jit_test(x, y)   trigger compilation %timeit jit_test(x, y).block_until_ready() ``` So, to answer your question directly: the workaround is to do your array indexing within a jitcompiled function. You can read more about what kind of performance to expect from JAX in FAQ: Is JAX Faster Than Numpy.","Also, sidenote, generally in JAX (as in numpy) for optimal performance it's best to avoid using python `for` loops over array axes, and instead use vectorized operations. I suspect you used loops here only for the sake of example, but I thought I'd add that just in case! For example, it would be better to write your first function as something like this, to compute the equivalent output while avoiding the forloop: ```python def test(x, y):   return x * (10 * y[0] + y[1] + 4 * y[2] + 10 * y[3:].sum()) ```","That makes perfect sense. Thanks a lot for your help.  Best, "
370,"以下是一个github上的jax下的一个issue, 标题是([TPU CI] Send chat notification on cancellation as well as failure.)， 内容是 (In particular, this makes it notify on timeouts (which usually indicates a test hang, but should be addressed in any case).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,[TPU CI] Send chat notification on cancellation as well as failure.,"In particular, this makes it notify on timeouts (which usually indicates a test hang, but should be addressed in any case).",2022-12-06T22:10:47Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13537
525,"以下是一个github上的jax下的一个issue, 标题是(DOC: pre-execute the quickstart notebook on GPU)， 内容是 (Fixes CC(Wrong benchmarks in the documentation) The downside here is we will no longer get doctest coverage of this notebook, but the APIs it uses are very stable so I think that shouldn't be too big a problem here. Preview: https://jax13536.org.readthedocs.build/en/13536/notebooks/quickstart.html)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,DOC: pre-execute the quickstart notebook on GPU,"Fixes CC(Wrong benchmarks in the documentation) The downside here is we will no longer get doctest coverage of this notebook, but the APIs it uses are very stable so I think that shouldn't be too big a problem here. Preview: https://jax13536.org.readthedocs.build/en/13536/notebooks/quickstart.html",2022-12-06T19:38:38Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/13536
2217,"以下是一个github上的jax下的一个issue, 标题是(`vmap`-`while`-`inplace` makes copies instead of updating in-place.)， 内容是 (As title  a silent performance bug. MWE: ```python import jax import jax.lax as lax import jax.numpy as jnp import timeit def f(init_step, init_xs):     def cond(carry):         step, xs = carry         return step < xs.size     def body(carry):         step, xs = carry         xs = xs.at[step].set(1)         return step + 1, xs     return lax.while_loop(cond, body, (init_step, init_xs)) f = jax.jit(f) vf = jax.jit(jax.vmap(f)) def run(size):     args = 0, jnp.zeros(size)     vargs = jnp.array([0]), jnp.zeros((1, size))     time = min(timeit.repeat(lambda: f(*args), number=1, repeat=2))     vtime = min(timeit.repeat(lambda: vf(*vargs), number=1, repeat=2))     print(f""size={size} time={time} vtime={vtime}"") run(10000) run(20000) run(30000)  size=10000 time=6.700679659843445e05 vtime=0.12796505074948072  size=20000 time=9.748293086886406e05 vtime=0.5133151961490512  size=30000 time=0.00012761494144797325 vtime=1.1480253851041198 ``` (Tested on CPU.) I think XLA is handling batch`while` by computing the batch'd body, and then `select`ing just those batch elements that had changes. But it has a missing patternmatching optimisation: it's not smart enough to recognise that `select(pred, xs, xs.at[step].set(new_x))` can be rewritten as `xs.at[step].set(select(pred, xs[step], new_x))` and thus that copying `xs` can be avoided. I'd be happy to contribute this as a fix against XLA if someone can offer some guidance on where to find the XLA code / where its list of patternmatching optimisations can be found. Alternatively, I've been able to fix this issue in JAX, by implementing the above patternmatch optimisation at the jaxpr level, and I'd be happy to contribute my fix as a PR. My fix isn't perfect, as without being part of a fullyfledged optimising compile it still gets defeated by stuff that first requires other pattern matches (e.g. `select(pred, xs.T.T, xs.at[step].set(new_x))`), but it should at least offer a way to work around this issue.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`vmap`-`while`-`inplace` makes copies instead of updating in-place.,"As title  a silent performance bug. MWE: ```python import jax import jax.lax as lax import jax.numpy as jnp import timeit def f(init_step, init_xs):     def cond(carry):         step, xs = carry         return step < xs.size     def body(carry):         step, xs = carry         xs = xs.at[step].set(1)         return step + 1, xs     return lax.while_loop(cond, body, (init_step, init_xs)) f = jax.jit(f) vf = jax.jit(jax.vmap(f)) def run(size):     args = 0, jnp.zeros(size)     vargs = jnp.array([0]), jnp.zeros((1, size))     time = min(timeit.repeat(lambda: f(*args), number=1, repeat=2))     vtime = min(timeit.repeat(lambda: vf(*vargs), number=1, repeat=2))     print(f""size={size} time={time} vtime={vtime}"") run(10000) run(20000) run(30000)  size=10000 time=6.700679659843445e05 vtime=0.12796505074948072  size=20000 time=9.748293086886406e05 vtime=0.5133151961490512  size=30000 time=0.00012761494144797325 vtime=1.1480253851041198 ``` (Tested on CPU.) I think XLA is handling batch`while` by computing the batch'd body, and then `select`ing just those batch elements that had changes. But it has a missing patternmatching optimisation: it's not smart enough to recognise that `select(pred, xs, xs.at[step].set(new_x))` can be rewritten as `xs.at[step].set(select(pred, xs[step], new_x))` and thus that copying `xs` can be avoided. I'd be happy to contribute this as a fix against XLA if someone can offer some guidance on where to find the XLA code / where its list of patternmatching optimisations can be found. Alternatively, I've been able to fix this issue in JAX, by implementing the above patternmatch optimisation at the jaxpr level, and I'd be happy to contribute my fix as a PR. My fix isn't perfect, as without being part of a fullyfledged optimising compile it still gets defeated by stuff that first requires other pattern matches (e.g. `select(pred, xs.T.T, xs.at[step].set(new_x))`), but it should at least offer a way to work around this issue.",2022-12-05T22:14:20Z,performance XLA,closed,0,1,https://github.com/jax-ml/jax/issues/13522,"Closing as this has been fixed in jaxlib 0.4.2, with this commit: https://github.com/tensorflow/tensorflow/commit/02f2d2415252594d51f5ecf34f6fab61fceee9b5 In principle it should still be possible to recreate an analogous version of the same issue, by using a `scatter` instead of a `dynamic_update_slice`. That turns out to be harder to fix: `scatter` has some other optimisations that can get in the way of the analogous fix. As such I'm punting on that for now, since the `dynamic_update_slice` case is the most important / frequent."
5611,"以下是一个github上的jax下的一个issue, 标题是(Failed to create cublas handle)， 内容是 ( Description This code appears correct and working: https://colab.research.google.com/drive/1b3XnflgL1yttHA5cOFKb3uSHPcHU64Hv?usp=share_link But on HPVictus laptop with Intel core and RTX 3050 GPU, it gives this error message: ``` 20221203 10:29:49.497205: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error 20221203 10:29:49.497835: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deeplearning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine. 20221203 10:29:49.498309: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream>parent()>GetBlasGemmAlgorithms(stream, &algorithms)  *** Begin stack trace *** 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	PyEval_EvalCode 	_PyRun_SimpleFileObject 	_PyRun_AnyFileObject 	Py_RunMain 	Py_BytesMain 	__libc_start_main 	_start *** End stack trace *** Traceback (most recent call last):   File ""/home/reza/jjj3.py"", line 17, in      yy = (xx(3,4,5))   File ""/home/reza/jjj3.py"", line 15, in xx     return (A,B, jax.numpy.matmul(A,B))   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/api.py"", line 622, in cache_miss     execute = dispatch._xla_call_impl_lazy(fun_, *tracers, **params)   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 236, in _xla_call_impl_lazy     return xla_callable(fun, device, backend, name, donated_invars, keep_unused,   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/linear_util.py"", line 303, in memoized_fun     ans = call(fun, *args)   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 360, in _xla_callable_uncached     keep_unused, *arg_specs).compile().unsafe_call   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 996, in compile     self._executable = XlaCompiledComputation.from_xla_computation(   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1194, in from_xla_computation     compiled = compile_or_get_cached(backend, xla_computation, options,   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1077, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1012, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream>parent()>GetBlasGemmAlgorithms(stream, &algorithms) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/reza/jjj3.py"", line 17, in      yy = (xx(3,4,5))   File ""/home/reza/jjj3.py"", line 15, in xx     return (A,B, jax.numpy.matmul(A,B)) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream>parent()>GetBlasGemmAlgorithms(stream, &algorithms)  ``` If you replace random matrices with ""ones"" it works. Even the randomization seems to work. But when you randomize, the matmul fails. I have installed jax with  ``` pip install upgrade pip pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` and CUDA 11.8 with the deb install from Nvidia. CUDA matmul sample code works: can multiply ~3000x3000 matrices very easily. I tried to explicitly say ""import jax.numpy as jnp"" and still got an error. What's the problem?! Do I basically have to compile jax for my machine?   What jax/jaxlib version are you using? jax 0.3.25 jaxlib 0.3.25  Which accelerator(s) are you using? RTX 3050 CUDA  Additional system info Python 3.10.6 Ubuntu 22.04 latest updates + CUDA 11.8  NVIDIA GPU info ``` reza:~$ nvidiasmi  Sat Dec  3 10:19:39 2022        ++  ++ reza:~$  ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,Failed to create cublas handle," Description This code appears correct and working: https://colab.research.google.com/drive/1b3XnflgL1yttHA5cOFKb3uSHPcHU64Hv?usp=share_link But on HPVictus laptop with Intel core and RTX 3050 GPU, it gives this error message: ``` 20221203 10:29:49.497205: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublas error 20221203 10:29:49.497835: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:221] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deeplearning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine. 20221203 10:29:49.498309: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream>parent()>GetBlasGemmAlgorithms(stream, &algorithms)  *** Begin stack trace *** 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	PyEval_EvalCode 	_PyRun_SimpleFileObject 	_PyRun_AnyFileObject 	Py_RunMain 	Py_BytesMain 	__libc_start_main 	_start *** End stack trace *** Traceback (most recent call last):   File ""/home/reza/jjj3.py"", line 17, in      yy = (xx(3,4,5))   File ""/home/reza/jjj3.py"", line 15, in xx     return (A,B, jax.numpy.matmul(A,B))   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/api.py"", line 622, in cache_miss     execute = dispatch._xla_call_impl_lazy(fun_, *tracers, **params)   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 236, in _xla_call_impl_lazy     return xla_callable(fun, device, backend, name, donated_invars, keep_unused,   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/linear_util.py"", line 303, in memoized_fun     ans = call(fun, *args)   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 360, in _xla_callable_uncached     keep_unused, *arg_specs).compile().unsafe_call   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 996, in compile     self._executable = XlaCompiledComputation.from_xla_computation(   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1194, in from_xla_computation     compiled = compile_or_get_cached(backend, xla_computation, options,   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1077, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/reza/.local/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1012, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream>parent()>GetBlasGemmAlgorithms(stream, &algorithms) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/reza/jjj3.py"", line 17, in      yy = (xx(3,4,5))   File ""/home/reza/jjj3.py"", line 15, in xx     return (A,B, jax.numpy.matmul(A,B)) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream>parent()>GetBlasGemmAlgorithms(stream, &algorithms)  ``` If you replace random matrices with ""ones"" it works. Even the randomization seems to work. But when you randomize, the matmul fails. I have installed jax with  ``` pip install upgrade pip pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` and CUDA 11.8 with the deb install from Nvidia. CUDA matmul sample code works: can multiply ~3000x3000 matrices very easily. I tried to explicitly say ""import jax.numpy as jnp"" and still got an error. What's the problem?! Do I basically have to compile jax for my machine?   What jax/jaxlib version are you using? jax 0.3.25 jaxlib 0.3.25  Which accelerator(s) are you using? RTX 3050 CUDA  Additional system info Python 3.10.6 Ubuntu 22.04 latest updates + CUDA 11.8  NVIDIA GPU info ``` reza:~$ nvidiasmi  Sat Dec  3 10:19:39 2022        ++  ++ reza:~$  ```",2022-12-03T18:34:12Z,bug NVIDIA GPU,open,0,9,https://github.com/jax-ml/jax/issues/13504,Tested this on rtx 3090 with jaxlib=0.3.22 and cuda 11.7. It worked,Please take a look at: https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html Does lowering the memory preallocation fraction help?,"I have this same `jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gemm_algorithm_picker.cc:327) stream>parent()>GetBlasGemmAlgorithms(stream, &algorithms)` error. Am using an 80G NVidia A100, `cuda==11.8`. Am also using `jax==0.3.25`"," I fixed the issue on my side. First I downgraded to `jax==0.3.22` following , this didn't solve the error but rather changed the error to be `jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support`. Googling this lead me to the actual fix which was to set `gpu_options.allow_growth = True`. Full code: `import tensorflow as tf print(""executing TF bug workaround"") config = tf.compat.v1.ConfigProto(gpu_options =                        tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8) ) config.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config) tf.compat.v1.keras.backend.set_session(session)` which needs to be executed at the start of your program. This is a common TF bug workaround",Jax and TF should host a masterclass in bad error reporting,I had this same error and the workaround above made the error go away,"   Thanks for trying, but it doesn't work. The memory environment variables (setting them in my bash) aren't helping. This sort of thing should be in the Jax config (configurable from Python, while reporting errors if it doesn't work). It's meaningless to input them into my bash. I don't know what's the problem.  I'm getting ""ModuleNotFoundError: No module named 'tensorflow'"". You can fix the Tensorflow memory usage issues if you have it, but I'm not going to install it just to fix a Jax problem (I don't really believe this is the fix for Jax). Sorry for the delay. This was (still is) taking up too much time. I had to switch Jax to CPU temporarily and get work done. (Incidentally, switching to CPU is easy through Jax config options from within Python. There is no reason why memory config should be located elsewhere.) So I am still waiting for an answer to this question. I need Jax to work on the GPU. Can anyone please help? ","CORRECTION:  This environment variable works for me now. Problem solved! export XLA_PYTHON_CLIENT_MEM_FRACTION=0.7 I don't know what happened yesterday. I tried several times (both environment variables, I think, which is a redundant combination) and it wasn't working. This is after a reboot. Thank you so much. I really appreciate that.   It's still much better to put this into jax.config, and lower the default usage. An error like ""jax out of memory, see memory options"" is so much nicer and cleaner than ""Cublas out of memory"" _because_ CUDA installation instructions are messy and complicated — even though at the end it's just 5 simple lines — once you figure it out. So, after CUDA install failing on me multiple times, I see a CUBLAS error as potentially another install failure. And I'm still trying to figure out this ""secure boot"" thing. There is really no shortage of ingenuity for making things difficult on users and making perfectly good hardware look trashy and broken just because people couldn't get their install scripts working well. So, this sort of thing is really important because it saves the user's TIME, and makes things pleasant, efficient, and working. I still don't know why (jax) memory preallocation is necessary for performance, and why a more dynamically managed system can't be the default. I could imagine some ""clever"" memory allocators based on preallocation, perhaps. But I don't know why the benefits of this (as default) would outweigh the costs? Thank you. ",export XLA_PYTHON_CLIENT_MEM_FRACTION=0.7 worked for me on Nvidia mobile 3060 RTX.
348,"以下是一个github上的jax下的一个issue, 标题是(Implement .on_device_size_in_bytes() on jax.Array.)， 内容是 (Implement .on_device_size_in_bytes() on jax.Array. This is an array present in DeviceArray that is missing from Array.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Implement .on_device_size_in_bytes() on jax.Array.,Implement .on_device_size_in_bytes() on jax.Array. This is an array present in DeviceArray that is missing from Array.,2022-12-02T20:25:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13497
1975,"以下是一个github上的jax下的一个issue, 标题是(Reporting Bug: Cloud TPU init failed)， 内容是 ( Description UserWarning: cloud_tpu_init failed: KeyError('\n\n  \n  \n  Error 404 (Not Found)!!1\n  \n    *{margin:0;padding:0}html,code{font:15px/22px arial,sansserif}html{background:fff;color: CC(jax.random.randint range must be valid);padding:15px}body{margin:7% auto 0;maxwidth:390px;minheight:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px norepeat;paddingright:205px}p{margin:11px 0 22px;overflow:hidden}ins{color: CC(Gradient of `np.exp` sometimes causes invalid values);textdecoration:none}a img{border:0} screen and (maxwidth:772px){body{background:none;margintop:0;maxwidth:none;paddingright:0}}logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) norepeat;marginleft:5px} only screen and (minresolution:192dpi){logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) norepeat 0% 0%/100% 100%;mozborderimage:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}} only screen and (webkitmindevicepixelratio:2){logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) norepeat;webkitbackgroundsize:100% 100%}}logo{display:inlineblock;height:54px;width:150px}\n  \n  \n  404. That’s an error.\n  The requested URL /computeMetadata/v1/instance/attributes/acceleratortype was not found on this server.  That’s all we know.\n')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report ""  What jax/jaxlib version are you using? 3.25  Which accelerator(s) are you using? TPU v2  Additional system info Python 3.7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Reporting Bug: Cloud TPU init failed," Description UserWarning: cloud_tpu_init failed: KeyError('\n\n  \n  \n  Error 404 (Not Found)!!1\n  \n    *{margin:0;padding:0}html,code{font:15px/22px arial,sansserif}html{background:fff;color: CC(jax.random.randint range must be valid);padding:15px}body{margin:7% auto 0;maxwidth:390px;minheight:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px norepeat;paddingright:205px}p{margin:11px 0 22px;overflow:hidden}ins{color: CC(Gradient of `np.exp` sometimes causes invalid values);textdecoration:none}a img{border:0} screen and (maxwidth:772px){body{background:none;margintop:0;maxwidth:none;paddingright:0}}logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) norepeat;marginleft:5px} only screen and (minresolution:192dpi){logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) norepeat 0% 0%/100% 100%;mozborderimage:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}} only screen and (webkitmindevicepixelratio:2){logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) norepeat;webkitbackgroundsize:100% 100%}}logo{display:inlineblock;height:54px;width:150px}\n  \n  \n  404. That’s an error.\n  The requested URL /computeMetadata/v1/instance/attributes/acceleratortype was not found on this server.  That’s all we know.\n')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report ""  What jax/jaxlib version are you using? 3.25  Which accelerator(s) are you using? TPU v2  Additional system info Python 3.7  NVIDIA GPU info _No response_",2022-12-02T18:59:37Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/13492,"Can you share the command you used to create the TPU VM? Also are you still getting this error? (Asking to see if it's a transient error, or if you continue to get it every time)",After struggling with it for a while I found this resource (https://github.com/ayaka14732/tpustarter CC(Performance tests)startaserverontpuvm) which allowed me to develop JAX on cloud TPU without any issues. Linking for others who may also run into similar issues. 
334,"以下是一个github上的jax下的一个issue, 标题是(JAX github actions: Update Python versions in test matrix for better coverage)， 内容是 (JAX github actions: Update Python versions in test matrix for better coverage)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX github actions: Update Python versions in test matrix for better coverage,JAX github actions: Update Python versions in test matrix for better coverage,2022-11-30T20:26:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13457
3504,"以下是一个github上的jax下的一个issue, 标题是(sparse.linalg.solvers do not work with inhomogeneous PyTrees)， 内容是 ( Description See this MWE: it attempts to solve with `jax.scipy.sparse.linalg.cg` a linear system where the 'vector space' is non homogeneous (as in, contains both real and complex leaves). The problem is still mathematically well defined, so in principle `cg` should not error.  ```python import jax import jax.numpy as jnp fun=lambda x:x y = {'a': jnp.ones(3), 'b': jnp.ones(3, dtype=jnp.complex64)} jax.scipy.sparse.linalg.cg(fun, y) ``` this errors with the following stack trace: ```python TypeError                                 Traceback (most recent call last) Input In [11], in () > 1 jax.scipy.sparse.linalg.cg(fun, y) File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/scipy/sparse/linalg.py:288, in cg(A, b, x0, tol, atol, maxiter, M)     235 def cg(A, b, x0=None, *, tol=1e5, atol=0.0, maxiter=None, M=None):     236   """"""Use Conjugate Gradient iteration to solve ``Ax = b``.     237     238   The numerics of JAX's ``cg`` should exact match SciPy's ``cg`` (up to    (...)     286   jax.lax.custom_linear_solve     287   """""" > 288   return _isolve(_cg_solve,     289                  A=A, b=b, x0=x0, tol=tol, atol=atol,     290                  maxiter=maxiter, M=M, check_symmetric=True) File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/scipy/sparse/linalg.py:228, in _isolve(_isolve_solve, A, b, x0, tol, atol, maxiter, M, check_symmetric)     225   return not issubclass(x.dtype.type, np.complexfloating)     226 symmetric = all(map(real_valued, tree_leaves(b))) \     227   if check_symmetric else False > 228 x = lax.custom_linear_solve(     229     A, b, solve=isolve_solve, transpose_solve=isolve_solve,     230     symmetric=symmetric)     231 info = None     232 return x, info     [... skipping hidden 9 frame] File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/scipy/sparse/linalg.py:136, in _cg_solve(A, b, x0, maxiter, tol, atol, M)     133 gamma0 = _vdot_real_tree(r0, z0).astype(dtype)     134 initial_value = (x0, r0, gamma0, p0, 0) > 136 x_final, *_ = lax.while_loop(cond_fun, body_fun, initial_value)     138 return x_final     [... skipping hidden 2 frame] File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/lax/control_flow/common.py:108, in _check_tree_and_avals(what, tree1, avals1, tree2, avals2)     105 if not all(map(core.typematch, avals1, avals2)):     106   diff = tree_map(_show_diff, tree_unflatten(tree1, avals1),     107                   tree_unflatten(tree2, avals2)) > 108   raise TypeError(f""{what} must have identical types, got\n{diff}."") TypeError: body_fun output and input must have identical types, got ({'a': 'DIFFERENT ShapedArray(complex64[3]) vs. ShapedArray(float32[3])', 'b': 'ShapedArray(complex64[3])'}, {'a': 'DIFFERENT ShapedArray(complex64[3]) vs. ShapedArray(float32[3])', 'b': 'ShapedArray(complex64[3])'}, 'ShapedArray(complex64[])', {'a': 'DIFFERENT ShapedArray(complex64[3]) vs. ShapedArray(float32[3])', 'b': 'ShapedArray(complex64[3])'}, 'ShapedArray(int32[], weak_type=True)'). ```  What jax/jaxlib version are you using? 0.3.24  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,sparse.linalg.solvers do not work with inhomogeneous PyTrees," Description See this MWE: it attempts to solve with `jax.scipy.sparse.linalg.cg` a linear system where the 'vector space' is non homogeneous (as in, contains both real and complex leaves). The problem is still mathematically well defined, so in principle `cg` should not error.  ```python import jax import jax.numpy as jnp fun=lambda x:x y = {'a': jnp.ones(3), 'b': jnp.ones(3, dtype=jnp.complex64)} jax.scipy.sparse.linalg.cg(fun, y) ``` this errors with the following stack trace: ```python TypeError                                 Traceback (most recent call last) Input In [11], in () > 1 jax.scipy.sparse.linalg.cg(fun, y) File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/scipy/sparse/linalg.py:288, in cg(A, b, x0, tol, atol, maxiter, M)     235 def cg(A, b, x0=None, *, tol=1e5, atol=0.0, maxiter=None, M=None):     236   """"""Use Conjugate Gradient iteration to solve ``Ax = b``.     237     238   The numerics of JAX's ``cg`` should exact match SciPy's ``cg`` (up to    (...)     286   jax.lax.custom_linear_solve     287   """""" > 288   return _isolve(_cg_solve,     289                  A=A, b=b, x0=x0, tol=tol, atol=atol,     290                  maxiter=maxiter, M=M, check_symmetric=True) File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/scipy/sparse/linalg.py:228, in _isolve(_isolve_solve, A, b, x0, tol, atol, maxiter, M, check_symmetric)     225   return not issubclass(x.dtype.type, np.complexfloating)     226 symmetric = all(map(real_valued, tree_leaves(b))) \     227   if check_symmetric else False > 228 x = lax.custom_linear_solve(     229     A, b, solve=isolve_solve, transpose_solve=isolve_solve,     230     symmetric=symmetric)     231 info = None     232 return x, info     [... skipping hidden 9 frame] File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/scipy/sparse/linalg.py:136, in _cg_solve(A, b, x0, maxiter, tol, atol, M)     133 gamma0 = _vdot_real_tree(r0, z0).astype(dtype)     134 initial_value = (x0, r0, gamma0, p0, 0) > 136 x_final, *_ = lax.while_loop(cond_fun, body_fun, initial_value)     138 return x_final     [... skipping hidden 2 frame] File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/lax/control_flow/common.py:108, in _check_tree_and_avals(what, tree1, avals1, tree2, avals2)     105 if not all(map(core.typematch, avals1, avals2)):     106   diff = tree_map(_show_diff, tree_unflatten(tree1, avals1),     107                   tree_unflatten(tree2, avals2)) > 108   raise TypeError(f""{what} must have identical types, got\n{diff}."") TypeError: body_fun output and input must have identical types, got ({'a': 'DIFFERENT ShapedArray(complex64[3]) vs. ShapedArray(float32[3])', 'b': 'ShapedArray(complex64[3])'}, {'a': 'DIFFERENT ShapedArray(complex64[3]) vs. ShapedArray(float32[3])', 'b': 'ShapedArray(complex64[3])'}, 'ShapedArray(complex64[])', {'a': 'DIFFERENT ShapedArray(complex64[3]) vs. ShapedArray(float32[3])', 'b': 'ShapedArray(complex64[3])'}, 'ShapedArray(int32[], weak_type=True)'). ```  What jax/jaxlib version are you using? 0.3.24  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2022-11-30T12:34:28Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/13452,The problem also arises with `jax.scipy.sparse.linalg.gmres` and `jax.scipy.sparse.linalg.bicgstab` with similar stack traces.
3474,"以下是一个github上的jax下的一个issue, 标题是(An execution of JAX code with GPU as the default device affects subsequent executions of the same code with CPU as the default device)， 内容是 ( Description This issue is basically same as my post in discussions (https://github.com/google/jax/discussions/12945). The issue is that, only when JIT is enabled, an execution of some JAX code with GPU as the default device affects subsequent executions of the same code with CPU as the default device. Here is how to reproduce: ```python import jax import jax.example_libraries.optimizers import jax.numpy as jnp import numpy as np def loss_fn(params, data):     t_train, n = data.shape     mu = params[""mu""]     def transition(carry, t):         h_prev, u_prev = carry         h_t = 0.1 * (u_prev ** 2) + h_prev         scale = jnp.sqrt(h_t)         y_t = data[t]         u_t = y_t  mu         return (h_t, u_t), (mu, scale, y_t)     h_0 = jnp.ones(n) * 0.01     u_0 = jnp.ones(n)     carry_init = (h_0, u_0)     _, (mu, scale, y_t) = jax.lax.scan(transition, carry_init, jnp.arange(1, t_train))     normalize_term = jnp.log(jnp.sqrt(2 * jnp.pi) * scale)     value_scaled = (y_t  mu) / scale     return (0.5 * value_scaled ** 2 + normalize_term).sum() def f():     np.random.seed(0)     optimizer = jax.example_libraries.optimizers.adam(1.0)     data = np.random.normal(loc=0, scale=0.01, size=(20, 1))     data_jnp = jnp.array(data)     params = {""mu"": jnp.ones(data_jnp.shape[1])}     opt_state = optimizer0     .jit     def step(i, opt_state):         params = optimizer2         loss_value, grads = jax.value_and_grad(loss_fn)(params, data_jnp)         opt_state = optimizer1         return opt_state, loss_value     for i in range(1000):         opt_state, loss_value = step(i, opt_state)          if i % 100 == 0:              print(f""step {i}, loss: {loss_value}"")     params = optimizer2     return params with jax.default_device(jax.devices(""cpu"")[0]):     y1 = f() with jax.default_device(jax.devices(""cpu"")[0]):     y2 = f() with jax.default_device(jax.devices(""gpu"")[0]):     y3 = f() with jax.default_device(jax.devices(""cpu"")[0]):     y4 = f() print(f""{y1=}"") print(f""{y2=}"") print(f""{y3=}"") print(f""{y4=}"") ``` Output (without JAX_DISABLE_JIT=1): ``` y1={'mu': DeviceArray([0.05775195], dtype=float32)} y2={'mu': DeviceArray([0.05775195], dtype=float32)} y3={'mu': DeviceArray([0.045999], dtype=float32)} y4={'mu': DeviceArray([0.03762827], dtype=float32)} ``` output (with JAX_DISABLE_JIT=1): ``` y1={'mu': DeviceArray([0.00767463], dtype=float32)} y2={'mu': DeviceArray([0.00767463], dtype=float32)} y3={'mu': DeviceArray([0.04210557], dtype=float32)} y4={'mu': DeviceArray([0.00767463], dtype=float32)} ``` Note that `y1`, `y2`, and `y4` are computed in CPU while `y3` is in GPU. It is understandable that `y3` is different from `y1` as numerical errors may be device dependent. However, it looks strange that `y4` is different from `y1` if JIT is enabled as both of them are CPUexecuted. I am not sure this is a bug or the expected behavior of JIT. Feel free to close if it is the expected behavior.  What jax/jaxlib version are you using? jax==0.3.21 jaxlib==0.3.20+cuda11.cudnn8  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.8.10, Linux  NVIDIA GPU info ``` > nvidiasmi Wed Nov 30 17:14:22 2022 ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,An execution of JAX code with GPU as the default device affects subsequent executions of the same code with CPU as the default device," Description This issue is basically same as my post in discussions (https://github.com/google/jax/discussions/12945). The issue is that, only when JIT is enabled, an execution of some JAX code with GPU as the default device affects subsequent executions of the same code with CPU as the default device. Here is how to reproduce: ```python import jax import jax.example_libraries.optimizers import jax.numpy as jnp import numpy as np def loss_fn(params, data):     t_train, n = data.shape     mu = params[""mu""]     def transition(carry, t):         h_prev, u_prev = carry         h_t = 0.1 * (u_prev ** 2) + h_prev         scale = jnp.sqrt(h_t)         y_t = data[t]         u_t = y_t  mu         return (h_t, u_t), (mu, scale, y_t)     h_0 = jnp.ones(n) * 0.01     u_0 = jnp.ones(n)     carry_init = (h_0, u_0)     _, (mu, scale, y_t) = jax.lax.scan(transition, carry_init, jnp.arange(1, t_train))     normalize_term = jnp.log(jnp.sqrt(2 * jnp.pi) * scale)     value_scaled = (y_t  mu) / scale     return (0.5 * value_scaled ** 2 + normalize_term).sum() def f():     np.random.seed(0)     optimizer = jax.example_libraries.optimizers.adam(1.0)     data = np.random.normal(loc=0, scale=0.01, size=(20, 1))     data_jnp = jnp.array(data)     params = {""mu"": jnp.ones(data_jnp.shape[1])}     opt_state = optimizer0     .jit     def step(i, opt_state):         params = optimizer2         loss_value, grads = jax.value_and_grad(loss_fn)(params, data_jnp)         opt_state = optimizer1         return opt_state, loss_value     for i in range(1000):         opt_state, loss_value = step(i, opt_state)          if i % 100 == 0:              print(f""step {i}, loss: {loss_value}"")     params = optimizer2     return params with jax.default_device(jax.devices(""cpu"")[0]):     y1 = f() with jax.default_device(jax.devices(""cpu"")[0]):     y2 = f() with jax.default_device(jax.devices(""gpu"")[0]):     y3 = f() with jax.default_device(jax.devices(""cpu"")[0]):     y4 = f() print(f""{y1=}"") print(f""{y2=}"") print(f""{y3=}"") print(f""{y4=}"") ``` Output (without JAX_DISABLE_JIT=1): ``` y1={'mu': DeviceArray([0.05775195], dtype=float32)} y2={'mu': DeviceArray([0.05775195], dtype=float32)} y3={'mu': DeviceArray([0.045999], dtype=float32)} y4={'mu': DeviceArray([0.03762827], dtype=float32)} ``` output (with JAX_DISABLE_JIT=1): ``` y1={'mu': DeviceArray([0.00767463], dtype=float32)} y2={'mu': DeviceArray([0.00767463], dtype=float32)} y3={'mu': DeviceArray([0.04210557], dtype=float32)} y4={'mu': DeviceArray([0.00767463], dtype=float32)} ``` Note that `y1`, `y2`, and `y4` are computed in CPU while `y3` is in GPU. It is understandable that `y3` is different from `y1` as numerical errors may be device dependent. However, it looks strange that `y4` is different from `y1` if JIT is enabled as both of them are CPUexecuted. I am not sure this is a bug or the expected behavior of JIT. Feel free to close if it is the expected behavior.  What jax/jaxlib version are you using? jax==0.3.21 jaxlib==0.3.20+cuda11.cudnn8  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.8.10, Linux  NVIDIA GPU info ``` > nvidiasmi Wed Nov 30 17:14:22 2022 ++  ++ ```",2022-11-30T08:16:32Z,bug NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/13450,I confirmed that the issue persists with Python v3.9.12 and ``` jax==0.4.8 jaxlib==0.4.7+cuda11.cudnn86 ``` . Any information on this would be appreciated.,"Hi , I tried to reproduce this issue with latest version of JAX(0.5.0) and verified this on a cloud VM with V100 8 GPUs. The issue seems to be resolved with and without `JAX_DISABLE_JIT`. Please refer to the screenshot attached below. !Image Thanks!", I also confirmed that in my environment with jax==0.5.0 that the issue seems resolved. Thanks for letting me know!
16565,"以下是一个github上的jax下的一个issue, 标题是(OOM with only ~47MB memory allocated / requested on GPU)， 内容是 ( Description I tried to run multinerf, but no matter how small the batch size was, OOM would arise. ``` 20221129 20:30:37.803154: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2153] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 49592744 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    2.72MiB               constant allocation:     2.8KiB         maybe_live_out allocation:   386.5KiB      preallocated temp allocation:   47.29MiB   preallocated temp fragmentation:    3.50MiB (7.40%)                  total allocation:   50.40MiB               total fragmentation:    3.86MiB (7.66%) Peak buffers:         Buffer 1:                 Size: 4.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/select_n"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=457                 XLA Label: fusion                 Shape: f32[4096,256]                 ==========================         Buffer 2:                 Size: 4.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/select_n"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=457                 XLA Label: fusion                 Shape: f32[4096,256]                 ==========================         Buffer 3:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_6))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 4:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_5))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 5:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_4))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 6:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_3))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 7:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_2))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 8:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_1))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 9:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_0))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 10:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_7))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 11:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_6))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 12:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_5))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 13:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_4))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 14:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_3))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 15:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_2))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ========================== Traceback (most recent call last):   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/runpy.py"", line 197, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/media/gccrcv/Data/Opensources/multinerf/train.py"", line 288, in      app.run(main)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/media/gccrcv/Data/Opensources/multinerf/train.py"", line 229, in main     rendering = models.render_image(   File ""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"", line 673, in render_image     chunk_renderings, _ = render_fn(rng, chunk_rays) ValueError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 49592744 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    2.72MiB               constant allocation:     2.8KiB         maybe_live_out allocation:   386.5KiB      preallocated temp allocation:   47.29MiB   preallocated temp fragmentation:    3.50MiB (7.40%)                  total allocation:   50.40MiB               total fragmentation:    3.86MiB (7.66%) Peak buffers:         Buffer 1:                 Size: 4.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/select_n"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=457                 XLA Label: fusion                 Shape: f32[4096,256]                 ==========================         Buffer 2:                 Size: 4.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/select_n"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=457                 XLA Label: fusion                 Shape: f32[4096,256]                 ==========================         Buffer 3:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_6))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 4:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_5))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 5:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_4))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 6:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_3))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 7:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_2))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 8:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_1))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 9:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_0))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 10:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_7))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 11:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_6))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 12:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_5))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 13:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_4))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 14:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_3))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 15:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_2))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ========================== ```  What jax/jaxlib version are you using? jax v0.3.24/0.3.25 jaxlib v0.3.24/0.3.25, flax v0.6.1/2  Which accelerator(s) are you using? GPU  Additional system info Python 3.9, Ubuntu 20.04.5  NVIDIA GPU info ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,OOM with only ~47MB memory allocated / requested on GPU," Description I tried to run multinerf, but no matter how small the batch size was, OOM would arise. ``` 20221129 20:30:37.803154: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2153] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 49592744 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    2.72MiB               constant allocation:     2.8KiB         maybe_live_out allocation:   386.5KiB      preallocated temp allocation:   47.29MiB   preallocated temp fragmentation:    3.50MiB (7.40%)                  total allocation:   50.40MiB               total fragmentation:    3.86MiB (7.66%) Peak buffers:         Buffer 1:                 Size: 4.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/select_n"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=457                 XLA Label: fusion                 Shape: f32[4096,256]                 ==========================         Buffer 2:                 Size: 4.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/select_n"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=457                 XLA Label: fusion                 Shape: f32[4096,256]                 ==========================         Buffer 3:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_6))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 4:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_5))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 5:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_4))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 6:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_3))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 7:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_2))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 8:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_1))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 9:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_0))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 10:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_7))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 11:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_6))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 12:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_5))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 13:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_4))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 14:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_3))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 15:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_2))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ========================== Traceback (most recent call last):   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/runpy.py"", line 197, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/media/gccrcv/Data/Opensources/multinerf/train.py"", line 288, in      app.run(main)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/media/gccrcv/Data/Opensources/multinerf/train.py"", line 229, in main     rendering = models.render_image(   File ""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"", line 673, in render_image     chunk_renderings, _ = render_fn(rng, chunk_rays) ValueError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 49592744 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    2.72MiB               constant allocation:     2.8KiB         maybe_live_out allocation:   386.5KiB      preallocated temp allocation:   47.29MiB   preallocated temp fragmentation:    3.50MiB (7.40%)                  total allocation:   50.40MiB               total fragmentation:    3.86MiB (7.66%) Peak buffers:         Buffer 1:                 Size: 4.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/select_n"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=457                 XLA Label: fusion                 Shape: f32[4096,256]                 ==========================         Buffer 2:                 Size: 4.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/select_n"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=457                 XLA Label: fusion                 Shape: f32[4096,256]                 ==========================         Buffer 3:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_6))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 4:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_5))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 5:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_4))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 6:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_3))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 7:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_2))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 8:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_1))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 9:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_0))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 10:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_7))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 11:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_6))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 12:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_5))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 13:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_4))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 14:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_3))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ==========================         Buffer 15:                 Size: 2.00MiB                 Operator: op_name=""pmap(render_eval_fn)/jit(main)/Model/NerfMLP_0/vmap(jvp(Dense_2))/dot_general[dimension_numbers=(((1,), (0,)), ((), ())) precision=None preferred_element_type=None]"" source_file=""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=196                 XLA Label: customcall                 Shape: f32[2048,256]                 ========================== ```  What jax/jaxlib version are you using? jax v0.3.24/0.3.25 jaxlib v0.3.24/0.3.25, flax v0.6.1/2  Which accelerator(s) are you using? GPU  Additional system info Python 3.9, Ubuntu 20.04.5  NVIDIA GPU info ``` ++  ++++ ```",2022-11-29T12:38:45Z,bug NVIDIA GPU,open,1,1,https://github.com/jax-ml/jax/issues/13428,"Got different output after reboot. ``` 20221129 21:04:39.275246: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2153] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 6291456 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    6.00MiB               constant allocation:         0B         maybe_live_out allocation:    6.00MiB      preallocated temp allocation:         0B                  total allocation:   12.00MiB               total fragmentation:         0B (0.00%) Peak buffers:         Buffer 1:                 Size: 6.00MiB                 Operator: op_name=""jit(concatenate)/jit(main)/concatenate[dimension=0]"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=689                 XLA Label: concatenate                 Shape: f32[4096,128,3]                 ==========================         Buffer 2:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 3:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 4:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 5:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 6:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 7:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 8:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 9:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 10:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 11:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 12:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 13:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 14:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 15:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ========================== Traceback (most recent call last):   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/runpy.py"", line 197, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/media/gccrcv/Data/Opensources/multinerf/train.py"", line 288, in      app.run(main)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/media/gccrcv/Data/Opensources/multinerf/train.py"", line 229, in main     rendering = models.render_image(   File ""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"", line 689, in render_image     jax.tree_util.tree_map(lambda *args: jnp.concatenate(args), *chunks))   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 207, in tree_map     return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 207, in      return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))   File ""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"", line 689, in      jax.tree_util.tree_map(lambda *args: jnp.concatenate(args), *chunks))   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1791, in concatenate     arrays_out = [lax.concatenate(arrays_out[i:i+k], axis)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1791, in      arrays_out = [lax.concatenate(arrays_out[i:i+k], axis)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 648, in concatenate     return concatenate_p.bind(*operands, dimension=dimension)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/core.py"", line 329, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/core.py"", line 332, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/core.py"", line 712, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 115, in apply_primitive     return compiled_fun(*args)   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 200, in      return lambda *args, **kw: compiled(*args, **kw)[0]   File ""/home/gccrcv/anaconda3/envs/multinerf/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 895, in _execute_compiled     out_flat = compiled.execute(in_flat) jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 6291456 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    6.00MiB               constant allocation:         0B         maybe_live_out allocation:    6.00MiB      preallocated temp allocation:         0B                  total allocation:   12.00MiB               total fragmentation:         0B (0.00%) Peak buffers:         Buffer 1:                 Size: 6.00MiB                 Operator: op_name=""jit(concatenate)/jit(main)/concatenate[dimension=0]"" source_file=""/media/gccrcv/Data/Opensources/multinerf/internal/models.py"" source_line=689                 XLA Label: concatenate                 Shape: f32[4096,128,3]                 ==========================         Buffer 2:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 3:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 4:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 5:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 6:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 7:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 8:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 9:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 10:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 11:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 12:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 13:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 14:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ==========================         Buffer 15:                 Size: 384.0KiB                 Entry Parameter Subshape: f32[256,128,3]                 ========================== ```"
3290,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] An alternative support for shape polymorphism for native lowering)， 内容是 ([jax2tf] An alternative support for shape polymorphism for native serialization. jax2tf already supports many cases of shape polymorphism, e.g., those where the shapes of all intermediates can be expressed as polynomials in the dimension variables appearing in input shapes. We would like to achieve the same coverage while using StableHLO as the lowering format, rather than tf.Graph. For native serialization we will support two forms of lowering:   * one is using the growing support in JAX for dynamic shapes, of which shape polymorphism is a special case. This implementation is enabled with the `jax_dynamic_shapes` flag. At the moment, the JAX dynamic shapes support is still incomplete and over 500 of the 1000 jax2tf shape polymorphism tests are failing.   * a new one (added) here in which we form a `Jaxpr` using abstract values that express dimension sizes as dimension polynomials (as for the standard jax2tf) which we then lower to StableHLO. This implementation is enabled when `jax_dynamic_shapes` is off. With this implementation only 80 jax2tf tests fail, but they seem to fail in StableHLO shape inference. The key contribution here is to enable lowering a Jaxpr that contains dimension polynomials in some of the shapes. Many lowering rules already have some partial support for Jaxprs where the shapes contain `Var`s. To the extent possible, we try to write lowering rules that should cover both cases of dynamic shapes: `Var` or polynomials in shapes. The lowering convention is that at top level we collect the sorted list of dimension variable names occurring in the input shapes, and we store it in `ModuleContext.dim_vars`. All IR functions will take `N` additional prefix arguments of int32 type containing the values of the `N` dimension variables. When generating the body of the IR function the current values of the dimension variables will be a list of `ir.Value` stored in `LoweringRuleContext.dim_var_values`. Note that the Jaxprs are not changed to have extra `Var`s for the dimension variable values. An alternative implementation could work by transforming the Jaxpr to replace dimension polynomials into Vars. The key code pattern used in the lowering rules is::         if not core.is_constant_shape(shape):    Handles both Var, and polynomials            shape = mlir.eval_dynamic_shape(ctx, shape)            return mhlo.DynamicXxxOp(..., shape)         else:            return mhlo.XxxOp(..., shape) with `mlir.eval_dynamic_shape` handling both cases of `Var` and polynomials::         def eval_dynamic_shape(ctx, shape):            if config.jax_dynamic_shapes:                Using Var               return ... subst using ctx.axis_size_env ...            else:                Using polynomials               return ... subst using ctx.module_context.dim_vars and ctx.dim_var_values In order to support the above some lowering functions need to take a LoweringContext parameter, e.g., `mlir.full_like_aval`. I expect that the changes here will improve the jax_dynamic_shapes coverage as well.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] An alternative support for shape polymorphism for native lowering,"[jax2tf] An alternative support for shape polymorphism for native serialization. jax2tf already supports many cases of shape polymorphism, e.g., those where the shapes of all intermediates can be expressed as polynomials in the dimension variables appearing in input shapes. We would like to achieve the same coverage while using StableHLO as the lowering format, rather than tf.Graph. For native serialization we will support two forms of lowering:   * one is using the growing support in JAX for dynamic shapes, of which shape polymorphism is a special case. This implementation is enabled with the `jax_dynamic_shapes` flag. At the moment, the JAX dynamic shapes support is still incomplete and over 500 of the 1000 jax2tf shape polymorphism tests are failing.   * a new one (added) here in which we form a `Jaxpr` using abstract values that express dimension sizes as dimension polynomials (as for the standard jax2tf) which we then lower to StableHLO. This implementation is enabled when `jax_dynamic_shapes` is off. With this implementation only 80 jax2tf tests fail, but they seem to fail in StableHLO shape inference. The key contribution here is to enable lowering a Jaxpr that contains dimension polynomials in some of the shapes. Many lowering rules already have some partial support for Jaxprs where the shapes contain `Var`s. To the extent possible, we try to write lowering rules that should cover both cases of dynamic shapes: `Var` or polynomials in shapes. The lowering convention is that at top level we collect the sorted list of dimension variable names occurring in the input shapes, and we store it in `ModuleContext.dim_vars`. All IR functions will take `N` additional prefix arguments of int32 type containing the values of the `N` dimension variables. When generating the body of the IR function the current values of the dimension variables will be a list of `ir.Value` stored in `LoweringRuleContext.dim_var_values`. Note that the Jaxprs are not changed to have extra `Var`s for the dimension variable values. An alternative implementation could work by transforming the Jaxpr to replace dimension polynomials into Vars. The key code pattern used in the lowering rules is::         if not core.is_constant_shape(shape):    Handles both Var, and polynomials            shape = mlir.eval_dynamic_shape(ctx, shape)            return mhlo.DynamicXxxOp(..., shape)         else:            return mhlo.XxxOp(..., shape) with `mlir.eval_dynamic_shape` handling both cases of `Var` and polynomials::         def eval_dynamic_shape(ctx, shape):            if config.jax_dynamic_shapes:                Using Var               return ... subst using ctx.axis_size_env ...            else:                Using polynomials               return ... subst using ctx.module_context.dim_vars and ctx.dim_var_values In order to support the above some lowering functions need to take a LoweringContext parameter, e.g., `mlir.full_like_aval`. I expect that the changes here will improve the jax_dynamic_shapes coverage as well.",2022-11-29T08:18:16Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13427
1100,"以下是一个github上的jax下的一个issue, 标题是(Scanning a jitted function throws error in newer JAX versions)， 内容是 ( Description Hi, First of all thank you for this nice and performant library! I recently set up a new GPU with Jax version==0.3.25, jaxlib=0.3.25+cuda11.cudnn82, though this also happens with jax==0.3.16 and jaxlib=0.3.15+cuda11.cudnn82. When employing jax.lax.scan(jitted_function, state, xs), where jitted_function is a jitted function, the following error is thrown !Bildschirmfoto 20221128 um 13 20 32 Do you have a fix or a workaround for this? (Not jitting the function would work, but at the cost of not being jitted).  Thank you! Best Sebastien  What jax/jaxlib version are you using? jax==0.3.25 and jaxlib==0.3.25+cuda11.cudnn82, or jax==0.3.16 and jaxlib==0.3.15+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Ubuntu 20.04.5 LTS  NVIDIA GPU info !Bildschirmfoto 20221128 um 13 38 32 Nvidia, A100 80GB)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Scanning a jitted function throws error in newer JAX versions," Description Hi, First of all thank you for this nice and performant library! I recently set up a new GPU with Jax version==0.3.25, jaxlib=0.3.25+cuda11.cudnn82, though this also happens with jax==0.3.16 and jaxlib=0.3.15+cuda11.cudnn82. When employing jax.lax.scan(jitted_function, state, xs), where jitted_function is a jitted function, the following error is thrown !Bildschirmfoto 20221128 um 13 20 32 Do you have a fix or a workaround for this? (Not jitting the function would work, but at the cost of not being jitted).  Thank you! Best Sebastien  What jax/jaxlib version are you using? jax==0.3.25 and jaxlib==0.3.25+cuda11.cudnn82, or jax==0.3.16 and jaxlib==0.3.15+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Ubuntu 20.04.5 LTS  NVIDIA GPU info !Bildschirmfoto 20221128 um 13 38 32 Nvidia, A100 80GB",2022-11-28T12:48:29Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/13419,Thanks for the report  could you edit your question to add a minimal reproducible example that demonstrates the issue? It will take the guesswork out as we try to address the problem. Thanks!,"Thanks for the fast reply  I just tried to set up a minimum working example, however the example does not throw an error and I wasn't able to reproduce the error I get within a larger JaxMD code base.","Does passing a normal function to jax.lax.scan(function, state, xs) automatically jit the function? If that's the case, removing jit from the passed function seems like the easiest solution. Best Sebastien","Yes, `scan(f, ...)` will always JITcompile the input function `f`, unless it is run within a `jax.disable_jit()` context, in which case execution is done via a normal Python forloop rather than a `scan` primitive.","But passing a jitcompiled function to `scan` should be fine, so I would consider it a bug if this leads to an error. If you can supply a code snippet that reproduces this bug, then we can look into it."
1915,"以下是一个github上的jax下的一个issue, 标题是(cloud_tpu_init failed)， 内容是 ( Description here's what i ran (but why should anyone using google colab service which offers google tpu hardware need to separately install google jax special software versions to get the paid service to work?): ```  JAX TPU setup for Colab  run this then restart the runtime if output says to! !pip install upgrade jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html !pip install upgrade jaxlib  AFTER restart, check TPU status using this  should get 8 devices!  https://github.com/googlecolab/colabtools/issues/3009   ""It seems it is necessary to both install the latest versions of JAX manually and supply the TPU driver explicitly:"" import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu('tpu_driver_20221122')  ""explicitly"" eh... ok just random pick some recent date...  jax.local_devices() on the ""jax.local_devices()"" command this is the error: AttributeError: module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' /usr/local/lib/python3.7/distpackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report ""  .. derp derp buncha nonsense .. RuntimeError: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' (set JAX_PLATFORMS='' to automatically choose an available backend) ```  What jax/jaxlib version are you using? the one in colab pro, and also whatever happens with ""pip install upgrade""  Which accelerator(s) are you using? TPU in colab pro  Additional system info google colab pro with google tpu and google jax  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,cloud_tpu_init failed," Description here's what i ran (but why should anyone using google colab service which offers google tpu hardware need to separately install google jax special software versions to get the paid service to work?): ```  JAX TPU setup for Colab  run this then restart the runtime if output says to! !pip install upgrade jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html !pip install upgrade jaxlib  AFTER restart, check TPU status using this  should get 8 devices!  https://github.com/googlecolab/colabtools/issues/3009   ""It seems it is necessary to both install the latest versions of JAX manually and supply the TPU driver explicitly:"" import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu('tpu_driver_20221122')  ""explicitly"" eh... ok just random pick some recent date...  jax.local_devices() on the ""jax.local_devices()"" command this is the error: AttributeError: module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' /usr/local/lib/python3.7/distpackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report ""  .. derp derp buncha nonsense .. RuntimeError: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' (set JAX_PLATFORMS='' to automatically choose an available backend) ```  What jax/jaxlib version are you using? the one in colab pro, and also whatever happens with ""pip install upgrade""  Which accelerator(s) are you using? TPU in colab pro  Additional system info google colab pro with google tpu and google jax  NVIDIA GPU info _No response_",2022-11-26T01:59:01Z,bug,open,0,8,https://github.com/jax-ml/jax/issues/13409,"Hi, thanks for the question! Note that the instructions in https://github.com/googlecolab/colabtools/issues/3009 are for working around a previous bug that has been fixed in current jax/jaxlib/libtpu versions. You shouldn't need any special instructions now beyond connecting to a Colab TPU runtime and running `jax.tools.colab_tpu.setup_tpu()` (if you've already changed package installations in your runtime, choose *Runtime>Disconnect and Delete Runtime* and then reconnect to get a new runtime with default settings & packages). Let me know if that doesn't solve your issue, or if there are other considerations at play.","i try it and happen this on colab jax version: 0.3.25, error, ```python import jax jax.tools.colab_tpu.setup_tpu() ``` ``` /usr/local/lib/python3.8/distpackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report ""  AttributeError                            Traceback (most recent call last) [](https://localhost:8080/) in        1 import jax > 2 jax.tools.colab_tpu.setup_tpu() AttributeError: module 'jax' has no attribute 'tools' ```","The `jax.tools` submodule is not imported by default into the JAX namespace. Try this instead: ```python import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() ``` See https://github.com/google/jaxpipinstallationcolabtpu, where this import is recommended.","something is amiss and i have trouble chasing it, because it's sporadic in behavior and seems to affect different things... 1. i'm able to get notebooks running in the TPU type, using the imports and setups pasted below. they are copypasta from various searches on this issue; this particular combination gives me the least grief (but ideally users shouldn't need *any* of it, other than the import right? shouldn't google software and google hardware work outofbox with the google service that has a literal dropdown selector for TPU?).  2. HOWEVER, sometimes after running the script below, there's the error reported in the linked issue, which says something like ""didn't connect to device in time"". only solution to that is to restart the notebook runtime. it was when i was trying to work with this behavior that i found the threads describing how to get nightly drivers or run various wizardry to get the TPUs recognized, and from that madness i ultimately got the issue i posted here about. reverting from that madness, i still have issues described here. i.e., the ""this is a bug"" error happened when trying to work around other errors and bad behaviors  so i can avoid it, but i can't avoid these...  3. very often, during compile and run, compile takes a really long time and will often hang completely, requiring another runtime restart. this can happen with something as stupidly simple as splitting a PRNG, or as large as a complex CV model and its params replicated over all TPUs and nearing their max RAM. it can hang immediately on calling the cell, or after an hour of happilyfast training loops. or training is successful and things are fine. or i stop training, adjust something, and restart, and more often than not, it will hang during compile that time. and for youknowwhatsandgiggles, a compiled function will randomly decide it's time to recompile for a few minutes (and maybe hang), seemingly only to charge more ""compute units""...  4. when that hang behavior happens, the readout viewable in Runtime > View Runtime Logs will print this **every 10 seconds** for as long as you let it hang: **""W external/org_tensorflow/tensorflow/compiler/xla/python/tpu_driver/client/tpu_client.cc:617] TPU Execute is taking a long time. This might be due to a deadlock between multiple TPU cores or a very slow program.""** 5. i find the above message odd because i'm using JAX and Flax, not Tensorflow, but perhaps amidst the millions of wrappers one of the libraries is importing from TF? and if the 2 reasons it gives are the only ones, then it seems it's ""deadlock between multiple TPU cores"" because when things are running, everything is fine and it's not ""a very slow program""  here's what i'm using to get things started: ```python import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() import jax import os os.environ[""USE_FLAX""] = ""1""  os.environ[""XLA_USE_BF16""] = ""1"" num_devices = jax.device_count() device_type = jax.devices()[0].device_kind ```",I am facing same error on TPU node. There is any way to setup tpu on TPU node? ``` import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() ``` This is working on TPU node also?,"`import jax.tools.colab_tpu; jax.tools.colab_tpu.setup_tpu()` should be all you need to get running on a TPU node on Colab. However, it's very possible there are other bugs once you start running things, as it looks like  is running into. , if you can provide an example notebook(s) demonstrating these problems I can try to take a look. If possible, I also recommend trying Kaggle Notebooks (https://www.kaggle.com/code, click on ""New Notebook"" near the top). You have to create an account and log in to get accelerator support. Once you do that, there's a new ""TPU VM v38"" accelerator option. This gives you a TPU notebook environment similar to Colab, but using the newer TPU VM architecture. This should be a less buggy, more performant, and overall better experience than the older TPU Node architecture (see this blog post for a brief overview of the difference). I'd be interested to hear your feedback if you give it a shot.","so yesterday i got a bunch more errors and troubles trying to use a previouslyworking notebook with TPU. i suspect some change in backend stuff in Colab... the version of Jax that is preinstalled in Colab seems a bit old  version 0.3.25 was what i got when i started a new session, but the latest pypl version is 0.4.1, so i gave that a go. i also noticed that the default value in the setup_tpu function was a driver dated in November. the default with the 0.4.1 version was something in December, but it also didn't work  both versions of Jax and driver defaults gave the same ""timed out while waiting for dependency"" error and ""no TPU devices found"". only with 0.4.1 *and* setting the driver version to ""tpu_driver_nightly"" worked.  here's what finally worked, yesterday anyway: ```python !pip install U jax import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu(tpu_driver_version='tpu_driver_nightly') ```","In general the driver version should be from the same day that the jaxlib version was released (https://jax.readthedocs.io/en/latest/changelog.html). The `tpu_driver_nightly` version is automatically updated every night, so I recommend pinning to today's day (which I believe should be the same bits as the nightly, until tomorrow at least) instead of relying on the nightly version. I would expect all the default versions shipped with Colab to ""just work"" without having to update anything though. It sounds like maybe we should update the default versions if you find newer versions working better."
32835,"以下是一个github上的jax下的一个issue, 标题是(Severe performance hit from `reraise_with_filtered_traceback`)， 内容是 ( Description MRE below, profiler shows this method occupying the bulk of time in compute pipelines involving forloops, making Jax way slower than Numpy on CPU and GPU. I've not tried making this ""ideal for Jax"" besides putting `.jit` over `fn` (which didn't help)  doing so in context would require significant effort in rewriting lots of code in supporting Jax as a backend alongside PyTorch etc. Purely vectorized operations are _sometimes_ faster than Numpy; in below code, replacing the loop with `2**16` > `2**22` makes CPU a little faster, but GPU is still slower.  MRE ``` numpy 0.009038390000005165 jaxcpu 0.09192942900001072 jaxgpu 0.12671968900000138 ``` code ```python  * coding: utf8 * import numpy as np import jax.numpy as jnp import jax from timeit import default_timer as dtime def timeit(fn, n_iters=100):     t0 = dtime()     for _ in range(n_iters):         fn()     return (dtime()  t0) / n_iters def fn(x, fft):     for _ in range(100):         fft(x.reshape(16, 1).mean(axis=0)) for lib_name in ('numpy', 'jaxcpu', 'jaxgpu'):     if lib_name == 'numpy':         fft = np.fft.fft         x = np.random.randn(2**16).astype('float32')     else:         fft = jnp.fft.fft         x = jax.random.normal(jax.random.PRNGKey(0), jnp.atleast_1d(2**16)                               ).astype('float32')         if lib_name == 'jaxcpu':             x = jax.device_put(x, device=jax.devices(""cpu"")[0])         else:             x = jax.device_put(x, device=jax.devices(""gpu"")[0])     t_avg = timeit(lambda: fn(x, fft))     print(lib_name, t_avg) ```   Environment info Reproduced on Colab so not really relevant, still collapsed below. info Installed via `pip install ""jax[cpu]===0.3.24"" f https://whls.blob.core.windows.net/unstable/index.html usedeprecated legacyresolver`. Tried building from source, but issues. conda list ```  packages in environment at D:\Anaconda\envs\jaxenv:   Name                    Version                   Build  Channel abslpy                   1.3.0                    pypi_0    pypi alabaster                 0.7.12                     py_0    condaforge appdirs                   1.4.4                    pypi_0    pypi arrow                     1.2.3              pyhd8ed1ab_0    condaforge astroid                   2.12.13         py310h5588dad_0    condaforge atomicwrites              1.4.1              pyhd8ed1ab_0    condaforge attrs                     22.1.0             pyh71513ae_1    condaforge autopep8                  1.6.0              pyhd8ed1ab_1    condaforge babel                     2.11.0             pyhd8ed1ab_0    condaforge backcall                  0.2.0              pyh9f0ad1d_0    condaforge backports                 1.1                pyhd3eb1b0_0 backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    condaforge bcrypt                    3.2.2           py310h8d17308_1    condaforge beautifulsoup4            4.11.1             pyha770c72_0    condaforge binaryornot               0.4.4                      py_1    condaforge black                     22.10.0         py310h5588dad_2    condaforge bleach                    5.0.1              pyhd8ed1ab_0    condaforge brotlipy                  0.7.0           py310h8d17308_1005    condaforge bzip2                     1.0.8                h8ffe710_4    condaforge cacertificates           2022.10.11           haa95532_0 certifi                   2022.9.24          pyhd8ed1ab_0    condaforge cffi                      1.15.1          py310h628cb3f_2    condaforge chardet                   5.0.0           py310h5588dad_1    condaforge charsetnormalizer        2.1.1              pyhd8ed1ab_0    condaforge click                     8.1.3           py310h5588dad_1    condaforge cloudpickle               2.2.0              pyhd8ed1ab_0    condaforge colorama                  0.4.6              pyhd8ed1ab_0    condaforge comm                      0.1.0              pyhd8ed1ab_0    condaforge configparser              5.3.0                    pypi_0    pypi cookiecutter              2.1.1              pyh6c4a22f_0    condaforge cryptography              38.0.3          py310h52f42fa_0    condaforge cudatoolkit               11.3.1              h280eb24_10    condaforge debugpy                   1.6.3           py310h00ffb61_1    condaforge decorator                 5.1.1              pyhd8ed1ab_0    condaforge defusedxml                0.7.1              pyhd8ed1ab_0    condaforge diffmatchpatch          20200713           pyh9f0ad1d_0    condaforge dill                      0.3.6              pyhd8ed1ab_1    condaforge docstringtomarkdown     0.10               pyhd8ed1ab_0    condaforge docutils                  0.19            py310h5588dad_1    condaforge entrypoints               0.4                pyhd8ed1ab_0    condaforge etils                     0.9.0                    pypi_0    pypi exceptiongroup            1.0.4              pyhd8ed1ab_0    condaforge flake8                    5.0.4              pyhd8ed1ab_0    condaforge flatbuffers               2.0.7                    pypi_0    pypi gettext                   0.21.1               h5728263_0    condaforge glib                      2.74.1               h12be248_1    condaforge glibtools                2.74.1               h12be248_1    condaforge gstpluginsbase          1.21.2               h001b923_0    condaforge gstreamer                 1.21.2               h6b5321d_0    condaforge icu                       70.1                 h0e60522_0    condaforge idna                      3.4                pyhd8ed1ab_0    condaforge imagesize                 1.4.1              pyhd8ed1ab_0    condaforge importlibmetadata        5.0.0              pyha770c72_1    condaforge importlib_metadata        5.0.0                hd8ed1ab_1    condaforge importlib_resources       5.10.0             pyhd8ed1ab_0    condaforge inflection                0.5.1              pyh9f0ad1d_0    condaforge iniconfig                 1.1.1              pyh9f0ad1d_0    condaforge intelopenmp              2022.1.0          h57928b3_3787    condaforge intervaltree              3.1.0              pyhd3eb1b0_0 ipykernel                 6.18.0             pyh025b116_0    condaforge ipython                   7.33.0          py310h5588dad_0    condaforge ipython_genutils          0.2.0                      py_1    condaforge isort                     5.10.1             pyhd8ed1ab_0    condaforge jaraco.classes            3.2.3              pyhd8ed1ab_0    condaforge jedi                      0.18.2             pyhd8ed1ab_0    condaforge jellyfish                 0.9.0           py310h8d17308_2    condaforge jinja2                    3.1.2              pyhd8ed1ab_1    condaforge jinja2time               0.2.0              pyhd8ed1ab_3    condaforge jpeg                      9e                   h8ffe710_2    condaforge jsonschema                4.17.0             pyhd8ed1ab_0    condaforge jupyter_client            7.4.7              pyhd8ed1ab_0    condaforge jupyter_core              5.0.0           py310h5588dad_0    condaforge jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    condaforge keyring                   23.11.0         py310h5588dad_0    condaforge krb5                      1.19.3               h1176d77_0    condaforge kymatio                   0.4.0.dev0               pypi_0    pypi lazyobjectproxy         1.8.0           py310h8d17308_0    condaforge libblas                   3.9.0              16_win64_mkl    condaforge libcblas                  3.9.0              16_win64_mkl    condaforge libclang                  15.0.5          default_h77d9078_0    condaforge libclang13                15.0.5          default_h77d9078_0    condaforge libffi                    3.4.2                h8ffe710_5    condaforge libglib                   2.74.1               he8f3873_1    condaforge libiconv                  1.17                 h8ffe710_0    condaforge liblapack                 3.9.0              16_win64_mkl    condaforge libogg                    1.3.5                h2bbff1b_1 libpng                    1.6.39               h19919ed_0    condaforge libsodium                 1.0.18               h62dcd97_1    condaforge libspatialindex           1.9.3                h39d44d4_4    condaforge libsqlite                 3.40.0               hcfcfb64_0    condaforge libvorbis                 1.3.7                ha925a31_0    condaforge libzlib                   1.2.13               hcfcfb64_4    condaforge m2w64gcclibgfortran     5.3.0                         6    condaforge m2w64gcclibs            5.3.0                         7    condaforge m2w64gcclibscore       5.3.0                         7    condaforge m2w64gmp                 6.1.0                         2    condaforge m2w64libwinpthreadgit   5.0.0.4634.697f757               2    condaforge markupsafe                2.1.1           py310h8d17308_2    condaforge matplotlibinline         0.1.6              pyhd8ed1ab_0    condaforge mccabe                    0.7.0              pyhd8ed1ab_0    condaforge mistune                   2.0.4              pyhd8ed1ab_0    condaforge mkl                       2022.1.0           h6a75c08_874    condaforge moreitertools            9.0.0              pyhd8ed1ab_0    condaforge msys2condaepoch         20160418                      1    condaforge mypy_extensions           0.4.3           py310h5588dad_6    condaforge nbclient                  0.7.0              pyhd8ed1ab_0    condaforge nbconvert                 7.2.5              pyhd8ed1ab_0    condaforge nbconvertcore            7.2.5              pyhd8ed1ab_0    condaforge nbconvertpandoc          7.2.5              pyhd8ed1ab_0    condaforge nbformat                  5.7.0              pyhd8ed1ab_0    condaforge nestasyncio              1.5.6              pyhd8ed1ab_0    condaforge numpy                     1.23.5          py310h4a8f9c9_0    condaforge numpydoc                  1.5.0              pyhd8ed1ab_0    condaforge openssl                   1.1.1s               hcfcfb64_0    condaforge opteinsum                3.3.0                    pypi_0    pypi packaging                 21.3               pyhd8ed1ab_0    condaforge pandoc                    2.19.2               h57928b3_1    condaforge pandocfilters             1.5.0              pyhd8ed1ab_0    condaforge paramiko                  2.12.0             pyhd8ed1ab_0    condaforge parso                     0.8.3              pyhd8ed1ab_0    condaforge pathspec                  0.10.2             pyhd8ed1ab_0    condaforge pcre2                     10.40                h17e33f8_0    condaforge pexpect                   4.8.0              pyh9f0ad1d_2    condaforge pickleshare               0.7.5                   py_1003    condaforge pip                       22.3.1             pyhd8ed1ab_0    condaforge pkgutilresolvename      1.3.10             pyhd8ed1ab_0    condaforge platformdirs              2.5.2              pyhd8ed1ab_1    condaforge pluggy                    1.0.0           py310h5588dad_4    condaforge ply                       3.11                       py_1    condaforge prompttoolkit            3.0.33             pyha770c72_0    condaforge psutil                    5.9.4           py310h8d17308_0    condaforge ptyprocess                0.7.0              pyhd3deb0d_0    condaforge pycodestyle               2.9.1              pyhd8ed1ab_0    condaforge pycparser                 2.21               pyhd8ed1ab_0    condaforge pydocstyle                6.1.1              pyhd8ed1ab_0    condaforge pyflakes                  2.5.0              pyhd8ed1ab_0    condaforge pygments                  2.13.0             pyhd8ed1ab_0    condaforge pylint                    2.15.6             pyhd8ed1ab_0    condaforge pylintvenv               2.3.0              pyhd8ed1ab_0    condaforge pylsspyder               0.4.0              pyhd8ed1ab_0    condaforge pynacl                    1.5.0           py310h635b8f1_2    condaforge pyopenssl                 22.1.0             pyhd8ed1ab_0    condaforge pyparsing                 3.0.9              pyhd8ed1ab_0    condaforge pyqt                      5.15.7          py310h1fd54f2_2    condaforge pyqt5sip                 12.11.0         py310h00ffb61_2    condaforge pyqtwebengine             5.15.7          py310h1fd54f2_2    condaforge pyrsistent                0.19.2          py310h8d17308_0    condaforge pysocks                   1.7.1           py310h5588dad_5    condaforge pytest                    7.2.0           py310h5588dad_1    condaforge python                    3.10.8          h0269646_0_cpython    condaforge pythondateutil           2.8.2              pyhd8ed1ab_0    condaforge pythonfastjsonschema     2.16.2             pyhd8ed1ab_0    condaforge pythonlspblack          1.2.1              pyhd8ed1ab_0    condaforge pythonlspjsonrpc        1.0.0              pyhd8ed1ab_0    condaforge pythonlspserver         1.6.0                hd8ed1ab_0    condaforge pythonlspserverbase    1.6.0              pyhd8ed1ab_0    condaforge pythonslugify            7.0.0              pyhd8ed1ab_0    condaforge python_abi                3.10                    3_cp310    condaforge pytoolconfig              1.2.2              pyhd8ed1ab_0    condaforge pytz                      2022.6             pyhd8ed1ab_0    condaforge pywin32                   304             py310h00ffb61_2    condaforge pywin32ctypes            0.2.0           py310h5588dad_1006    condaforge pyyaml                    6.0             py310h8d17308_5    condaforge pyzmq                     24.0.1          py310hcd737a0_1    condaforge qdarkstyle                3.0.3              pyhd8ed1ab_0    condaforge qstylizer                 0.2.2              pyhd8ed1ab_0    condaforge qtmain                   5.15.6               h9c3277a_2    condaforge qtwebengine              5.15.9               hb9a9bb5_4 qtawesome                 1.2.1              pyhd8ed1ab_0    condaforge qtconsole                 5.4.0              pyhd8ed1ab_0    condaforge qtconsolebase            5.4.0              pyha770c72_0    condaforge qtpy                      2.3.0              pyhd8ed1ab_0    condaforge requests                  2.28.1             pyhd8ed1ab_1    condaforge rope                      1.5.0              pyhd8ed1ab_0    condaforge rtree                     1.0.1           py310h1cbd46b_1    condaforge scipy                     1.9.3           py310h578b7cb_2    condaforge setuptools                65.5.1             pyhd8ed1ab_0    condaforge sip                       6.7.5           py310h00ffb61_0    condaforge six                       1.16.0             pyh6c4a22f_0    condaforge snowballstemmer           2.2.0              pyhd8ed1ab_0    condaforge sortedcontainers          2.4.0              pyhd8ed1ab_0    condaforge soupsieve                 2.3.2.post1        pyhd8ed1ab_0    condaforge sphinx                    5.3.0              pyhd8ed1ab_0    condaforge sphinxcontribapplehelp   1.0.2                      py_0    condaforge sphinxcontribdevhelp     1.0.2                      py_0    condaforge sphinxcontribhtmlhelp    2.0.0              pyhd8ed1ab_0    condaforge sphinxcontribjsmath      1.0.1                      py_0    condaforge sphinxcontribqthelp      1.0.3                      py_0    condaforge sphinxcontribserializinghtml 1.1.5              pyhd8ed1ab_2    condaforge spyder                    5.4.0           py310h5588dad_0    condaforge spyderkernels            2.4.0           py310h5588dad_0    condaforge tbb                       2021.7.0             h91493d7_0    condaforge textunidecode            1.3                        py_0    condaforge textdistance              4.5.0              pyhd8ed1ab_0    condaforge threemerge               0.1.1              pyh9f0ad1d_0    condaforge tinycss2                  1.2.1              pyhd8ed1ab_0    condaforge tk                        8.6.12               h8ffe710_0    condaforge toml                      0.10.2             pyhd8ed1ab_0    condaforge tomli                     2.0.1              pyhd8ed1ab_0    condaforge tomlkit                   0.11.6             pyha770c72_0    condaforge tornado                   6.2             py310h8d17308_1    condaforge traitlets                 5.5.0              pyhd8ed1ab_0    condaforge typing                    3.10.0.0           pyhd8ed1ab_0    condaforge typingextensions         4.4.0                hd8ed1ab_0    condaforge typing_extensions         4.4.0              pyha770c72_0    condaforge tzdata                    2022f                h191b570_0    condaforge ucrt                      10.0.22621.0         h57928b3_0    condaforge ujson                     5.5.0           py310h00ffb61_1    condaforge unidecode                 1.3.6              pyhd8ed1ab_0    condaforge urllib3                   1.26.12         py310haa95532_0 vc                        14.3                 h3d8a991_9    condaforge vs2015_runtime            14.32.31332          h1d6e394_9    condaforge watchdog                  2.1.9           py310h5588dad_1    condaforge wavespin                  0.1.2                    pypi_0    pypi wcwidth                   0.2.5              pyh9f0ad1d_2    condaforge webencodings              0.5.1                      py_1    condaforge whatthepatch              1.0.3              pyhd8ed1ab_0    condaforge wheel                     0.38.4             pyhd8ed1ab_0    condaforge win_inet_pton             1.1.0           py310h5588dad_5    condaforge wrapt                     1.14.1          py310h8d17308_1    condaforge xz                        5.2.6                h8d14728_0    condaforge yaml                      0.2.5                h8ffe710_2    condaforge yapf                      0.32.0             pyhd8ed1ab_0    condaforge zeromq                    4.3.4                h0e60522_1    condaforge zipp                      3.10.0             pyhd8ed1ab_0    condaforge zstd                      1.5.2                h7755175_4    condaforge ```  conda info ```      active environment : jaxenv     active env location : D:\Anaconda\envs\jaxenv             shell level : 2        user config file : C:\Users\OverL\.condarc  populated config files : C:\Users\OverL\.condarc           conda version : 4.10.3     condabuild version : 3.18.11          python version : 3.8.3.final.0        virtual packages : __cuda=11.4=0                           __win=0=0                           __archspec=1=x86_64        base environment : D:\Anaconda  (writable)       conda av data dir : D:\Anaconda\etc\conda   conda av metadata url : None            channel URLs : https://repo.anaconda.com/pkgs/main/win64                           https://repo.anaconda.com/pkgs/main/noarch                           https://repo.anaconda.com/pkgs/r/win64                           https://repo.anaconda.com/pkgs/r/noarch                           https://repo.anaconda.com/pkgs/msys2/win64                           https://repo.anaconda.com/pkgs/msys2/noarch           package cache : D:\Anaconda\pkgs                           C:\Users\OverL\.conda\pkgs                           C:\Users\OverL\AppData\Local\conda\conda\pkgs        envs directories : D:\Anaconda\envs                           C:\Users\OverL\.conda\envs                           C:\Users\OverL\AppData\Local\conda\conda\envs                platform : win64              useragent : conda/4.10.3 requests/2.24.0 CPython/3.8.3 Windows/10 Windows/10.0.19041           administrator : True              netrc file : C:\Users\OverL/.netrc            offline mode : False ```  pip freeze ``` abslpy==1.3.0 alabaster==0.7.12 appdirs==1.4.4 arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work astroid @ file:///D:/bld/astroid_1668904413310/work atomicwrites @ file:///home/conda/feedstock_root/build_artifacts/atomicwrites_1657325823582/work attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1659291887007/work autopep8 @ file:///home/conda/feedstock_root/build_artifacts/autopep8_1635267974115/work Babel @ file:///home/conda/feedstock_root/build_artifacts/babel_1667688356751/work backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work backports.functoolslrucache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1618230623929/work bcrypt @ file:///D:/bld/bcrypt_1666831293551/work beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1649463573192/work binaryornot==0.4.4 black @ file:///D:/bld/blackrecipe_1666900162923/work bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1656355450470/work brotlipy @ file:///D:/bld/brotlipy_1666764804378/work certifi==2022.9.24 cffi @ file:///D:/bld/cffi_1666754925774/work chardet @ file:///D:/bld/chardet_1666817639647/work charsetnormalizer @ file:///home/conda/feedstock_root/build_artifacts/charsetnormalizer_1661170624537/work click @ file:///D:/bld/click_1666770360789/work cloudpickle @ file:///home/conda/feedstock_root/build_artifacts/cloudpickle_1662587369221/work colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1668713578881/work configparser==5.3.0 contourpy @ file:///D:/bld/contourpy_1667248078730/work cookiecutter @ file:///home/conda/feedstock_root/build_artifacts/cookiecutter_1654122127219/work cryptography @ file:///D:/bld/cryptography_1667422960373/work cycler @ file:///home/conda/feedstock_root/build_artifacts/cycler_1635519461629/work debugpy @ file:///D:/bld/debugpy_1666826400464/work decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work diffmatchpatch @ file:///home/conda/feedstock_root/build_artifacts/diffmatchpatch_1594679019945/work dill @ file:///home/conda/feedstock_root/build_artifacts/dill_1666603105584/work docstringtomarkdown @ file:///home/conda/feedstock_root/build_artifacts/docstringtomarkdown_1637247638475/work docutils @ file:///D:/bld/docutils_1666755081246/work entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work etils==0.9.0 exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1668523704481/work fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/pythonfastjsonschema_1663619548554/work/dist flake8 @ file:///home/conda/feedstock_root/build_artifacts/flake8_1659645013175/work flatbuffers==2.0.7 fonttools @ file:///D:/bld/fonttools_1666827169023/work idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work imagesize @ file:///home/conda/feedstock_root/build_artifacts/imagesize_1656939531508/work importlibmetadata @ file:///home/conda/feedstock_root/build_artifacts/importlibmetadata_1666781969417/work importlibresources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1665204935269/work inflection @ file:///home/conda/feedstock_root/build_artifacts/inflection_1598089801258/work iniconfig @ file:///home/conda/feedstock_root/build_artifacts/iniconfig_1603384189793/work intervaltree @ file:///Users/ktietz/demo/mc3/condabld/intervaltree_1630511889664/work ipykernel @ file:///D:/bld/ipykernel_1669056763652/work ipython @ file:///D:/bld/ipython_1651240745149/work ipythongenutils==0.2.0 isort @ file:///home/conda/feedstock_root/build_artifacts/isort_1636447814597/work jaraco.classes @ file:///home/conda/feedstock_root/build_artifacts/jaraco.classes_1667024629799/work jax==0.3.24 jaxlib==0.3.24 jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1669134318875/work jellyfish @ file:///D:/bld/jellyfish_1666945709644/work Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work jinja2time @ file:///home/conda/feedstock_root/build_artifacts/jinja2time_1646750632133/work jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschemameta_1667361745641/work jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1668623095912/work jupyter_core @ file:///D:/bld/jupyter_core_1668030965393/work jupyterlabpygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work keyring @ file:///D:/bld/keyring_1667696866921/work kiwisolver @ file:///D:/bld/kiwisolver_1666805768319/work kymatio @ git+https://github.com/kymatio/kymatio lazyobjectproxy @ file:///D:/bld/lazyobjectproxy_1666812092877/work MarkupSafe @ file:///D:/bld/markupsafe_1666770331304/work matplotlib @ file:///D:/bld/matplotlibsuite_1667505061247/work matplotlibinline @ file:///home/conda/feedstock_root/build_artifacts/matplotlibinline_1660814786464/work mccabe @ file:///home/conda/feedstock_root/build_artifacts/mccabe_1643049622439/work mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1657892024508/work moreitertools @ file:///home/conda/feedstock_root/build_artifacts/moreitertools_1666110321141/work munkres==1.1.4 mypyextensions @ file:///D:/bld/mypy_extensions_1666795127376/work nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1665125402713/work nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvertmeta_1668442474361/work nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1665426034066/work nestasyncio @ file:///home/conda/feedstock_root/build_artifacts/nestasyncio_1664684991461/work numpy @ file:///D:/bld/numpy_1668919338279/work numpydoc @ file:///home/conda/feedstock_root/build_artifacts/numpydoc_1665273484262/work opteinsum==3.3.0 packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1637239678211/work pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work paramiko @ file:///home/conda/feedstock_root/build_artifacts/paramiko_1667639932385/work parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work pathspec @ file:///home/conda/feedstock_root/build_artifacts/pathspec_1668325009666/work pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1602535608087/work pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work Pillow @ file:///D:/bld/pillow_1666920753896/work pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutilresolvename_1633981968097/work platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1657729053205/work pluggy @ file:///D:/bld/pluggy_1666787589498/work ply==3.11 prompttoolkit @ file:///home/conda/feedstock_root/build_artifacts/prompttoolkit_1669057097528/work psutil @ file:///D:/bld/psutil_1667886056621/work ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess0.7.0py2.py3noneany.whl pycodestyle @ file:///home/conda/feedstock_root/build_artifacts/pycodestyle_1659638152915/work pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work pydocstyle @ file:///home/conda/feedstock_root/build_artifacts/pydocstyle_1621377123289/work pyflakes @ file:///home/conda/feedstock_root/build_artifacts/pyflakes_1659210156976/work Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1660666458521/work pylint @ file:///home/conda/feedstock_root/build_artifacts/pylint_1668871495697/work pylintvenv @ file:///home/conda/feedstock_root/build_artifacts/pylintvenv_1656263679357/work pylsspyder @ file:///home/conda/feedstock_root/build_artifacts/pylsspyder_1619747398504/work PyNaCl @ file:///D:/bld/pynacl_1666862759359/work pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1665350324128/work pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work PyQt5==5.15.7 PyQt5sip @ file:///D:/bld/pyqtsplit_1666829883856/work/pyqt_sip PyQtWebEngine==5.15.4 pyrsistent @ file:///D:/bld/pyrsistent_1667498839113/work PySocks @ file:///D:/bld/pysocks_1648857426124/work pytest==7.2.0 pythondateutil @ file:///home/conda/feedstock_root/build_artifacts/pythondateutil_1626286286081/work pythonlspblack @ file:///home/conda/feedstock_root/build_artifacts/pythonlspblack_1649795569828/work pythonlspjsonrpc @ file:///home/conda/feedstock_root/build_artifacts/pythonlspjsonrpc_1618530352985/work pythonlspserver @ file:///home/conda/feedstock_root/build_artifacts/pythonlspservermeta_1667491645580/work pythonslugify @ file:///home/conda/feedstock_root/build_artifacts/pythonslugify_1668871579734/work pytoolconfig @ file:///home/conda/feedstock_root/build_artifacts/pytoolconfig_1659322367429/work pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1667391478166/work pywin32==304 pywin32ctypes @ file:///D:/bld/pywin32ctypes_1666760898350/work PyYAML @ file:///D:/bld/pyyaml_1666772550103/work pyzmq @ file:///D:/bld/pyzmq_1666828590571/work QDarkStyle @ file:///home/conda/feedstock_root/build_artifacts/qdarkstyle_1638323533455/work qstylizer @ file:///home/conda/feedstock_root/build_artifacts/qstylizer_1662244505808/work/dist/qstylizer0.2.2py2.py3noneany.whl QtAwesome @ file:///home/conda/feedstock_root/build_artifacts/qtawesome_1666638781418/work qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsolebase_1667404144336/work QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1667873092748/work requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1661872987712/work rope @ file:///home/conda/feedstock_root/build_artifacts/rope_1669133390676/work Rtree @ file:///D:/bld/rtree_1666813330222/work scipy==1.9.3 sip @ file:///D:/bld/sip_1669040101788/work six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work snowballstemmer @ file:///home/conda/feedstock_root/build_artifacts/snowballstemmer_1637143057757/work sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work Sphinx @ file:///home/conda/feedstock_root/build_artifacts/sphinx_1665915552897/work sphinxcontribapplehelp==1.0.2 sphinxcontribdevhelp==1.0.2 sphinxcontribhtmlhelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontribhtmlhelp_1621704829796/work sphinxcontribjsmath==1.0.1 sphinxcontribqthelp==1.0.3 sphinxcontribserializinghtml @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontribserializinghtml_1649380998999/work spyder @ file:///D:/bld/spyder_1667851050917/work spyderkernels @ file:///D:/bld/spyderkernels_1667489764576/work textunidecode==1.3 textdistance @ file:///home/conda/feedstock_root/build_artifacts/textdistance_1663527496115/work threemerge @ file:///home/conda/feedstock_root/build_artifacts/threemerge_1595515817927/work tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work toml @ file:///home/conda/feedstock_root/build_artifacts/toml_1604308577558/work tomli @ file:///home/conda/feedstock_root/build_artifacts/tomli_1644342247877/work tomlkit @ file:///home/conda/feedstock_root/build_artifacts/tomlkit_1666864188602/work tornado @ file:///D:/bld/tornado_1666788744359/work traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1666115969632/work typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1665144421445/work ujson @ file:///D:/bld/ujson_1666837388708/work unicodedata2 @ file:///D:/bld/unicodedata2_1667239969015/work Unidecode @ file:///home/conda/feedstock_root/build_artifacts/unidecode_1664588179651/work urllib3 @ file:///C:/b/abs_a8_3vfznn_/croot/urllib3_1666298943664/work watchdog @ file:///D:/bld/watchdog_1666806636407/work e git+https://github.com/gptanon/wttest.gitegg=WaveSpin wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1600965781394/work webencodings==0.5.1 whatthepatch @ file:///home/conda/feedstock_root/build_artifacts/whatthepatch_1668379856745/work wininetpton @ file:///D:/bld/win_inet_pton_1666755058235/work wrapt @ file:///D:/bld/wrapt_1666806148286/work yapf @ file:///home/conda/feedstock_root/build_artifacts/yapf_1641487982943/work zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1666647772197/work ```   What jax/jaxlib version are you using? jax 0.3.24, jaxlib 0.3.24  Which accelerator(s) are you using? GTX 1070  Additional system info Windows 10, Python 3.10.8, CUDA 11.3, cuDNN 8.2.0  NVIDIA GPU info info ``` Thu Nov 24 09:31:07 2022 ++  ++ ```  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Severe performance hit from `reraise_with_filtered_traceback`," Description MRE below, profiler shows this method occupying the bulk of time in compute pipelines involving forloops, making Jax way slower than Numpy on CPU and GPU. I've not tried making this ""ideal for Jax"" besides putting `.jit` over `fn` (which didn't help)  doing so in context would require significant effort in rewriting lots of code in supporting Jax as a backend alongside PyTorch etc. Purely vectorized operations are _sometimes_ faster than Numpy; in below code, replacing the loop with `2**16` > `2**22` makes CPU a little faster, but GPU is still slower.  MRE ``` numpy 0.009038390000005165 jaxcpu 0.09192942900001072 jaxgpu 0.12671968900000138 ``` code ```python  * coding: utf8 * import numpy as np import jax.numpy as jnp import jax from timeit import default_timer as dtime def timeit(fn, n_iters=100):     t0 = dtime()     for _ in range(n_iters):         fn()     return (dtime()  t0) / n_iters def fn(x, fft):     for _ in range(100):         fft(x.reshape(16, 1).mean(axis=0)) for lib_name in ('numpy', 'jaxcpu', 'jaxgpu'):     if lib_name == 'numpy':         fft = np.fft.fft         x = np.random.randn(2**16).astype('float32')     else:         fft = jnp.fft.fft         x = jax.random.normal(jax.random.PRNGKey(0), jnp.atleast_1d(2**16)                               ).astype('float32')         if lib_name == 'jaxcpu':             x = jax.device_put(x, device=jax.devices(""cpu"")[0])         else:             x = jax.device_put(x, device=jax.devices(""gpu"")[0])     t_avg = timeit(lambda: fn(x, fft))     print(lib_name, t_avg) ```   Environment info Reproduced on Colab so not really relevant, still collapsed below. info Installed via `pip install ""jax[cpu]===0.3.24"" f https://whls.blob.core.windows.net/unstable/index.html usedeprecated legacyresolver`. Tried building from source, but issues. conda list ```  packages in environment at D:\Anaconda\envs\jaxenv:   Name                    Version                   Build  Channel abslpy                   1.3.0                    pypi_0    pypi alabaster                 0.7.12                     py_0    condaforge appdirs                   1.4.4                    pypi_0    pypi arrow                     1.2.3              pyhd8ed1ab_0    condaforge astroid                   2.12.13         py310h5588dad_0    condaforge atomicwrites              1.4.1              pyhd8ed1ab_0    condaforge attrs                     22.1.0             pyh71513ae_1    condaforge autopep8                  1.6.0              pyhd8ed1ab_1    condaforge babel                     2.11.0             pyhd8ed1ab_0    condaforge backcall                  0.2.0              pyh9f0ad1d_0    condaforge backports                 1.1                pyhd3eb1b0_0 backports.functools_lru_cache 1.6.4              pyhd8ed1ab_0    condaforge bcrypt                    3.2.2           py310h8d17308_1    condaforge beautifulsoup4            4.11.1             pyha770c72_0    condaforge binaryornot               0.4.4                      py_1    condaforge black                     22.10.0         py310h5588dad_2    condaforge bleach                    5.0.1              pyhd8ed1ab_0    condaforge brotlipy                  0.7.0           py310h8d17308_1005    condaforge bzip2                     1.0.8                h8ffe710_4    condaforge cacertificates           2022.10.11           haa95532_0 certifi                   2022.9.24          pyhd8ed1ab_0    condaforge cffi                      1.15.1          py310h628cb3f_2    condaforge chardet                   5.0.0           py310h5588dad_1    condaforge charsetnormalizer        2.1.1              pyhd8ed1ab_0    condaforge click                     8.1.3           py310h5588dad_1    condaforge cloudpickle               2.2.0              pyhd8ed1ab_0    condaforge colorama                  0.4.6              pyhd8ed1ab_0    condaforge comm                      0.1.0              pyhd8ed1ab_0    condaforge configparser              5.3.0                    pypi_0    pypi cookiecutter              2.1.1              pyh6c4a22f_0    condaforge cryptography              38.0.3          py310h52f42fa_0    condaforge cudatoolkit               11.3.1              h280eb24_10    condaforge debugpy                   1.6.3           py310h00ffb61_1    condaforge decorator                 5.1.1              pyhd8ed1ab_0    condaforge defusedxml                0.7.1              pyhd8ed1ab_0    condaforge diffmatchpatch          20200713           pyh9f0ad1d_0    condaforge dill                      0.3.6              pyhd8ed1ab_1    condaforge docstringtomarkdown     0.10               pyhd8ed1ab_0    condaforge docutils                  0.19            py310h5588dad_1    condaforge entrypoints               0.4                pyhd8ed1ab_0    condaforge etils                     0.9.0                    pypi_0    pypi exceptiongroup            1.0.4              pyhd8ed1ab_0    condaforge flake8                    5.0.4              pyhd8ed1ab_0    condaforge flatbuffers               2.0.7                    pypi_0    pypi gettext                   0.21.1               h5728263_0    condaforge glib                      2.74.1               h12be248_1    condaforge glibtools                2.74.1               h12be248_1    condaforge gstpluginsbase          1.21.2               h001b923_0    condaforge gstreamer                 1.21.2               h6b5321d_0    condaforge icu                       70.1                 h0e60522_0    condaforge idna                      3.4                pyhd8ed1ab_0    condaforge imagesize                 1.4.1              pyhd8ed1ab_0    condaforge importlibmetadata        5.0.0              pyha770c72_1    condaforge importlib_metadata        5.0.0                hd8ed1ab_1    condaforge importlib_resources       5.10.0             pyhd8ed1ab_0    condaforge inflection                0.5.1              pyh9f0ad1d_0    condaforge iniconfig                 1.1.1              pyh9f0ad1d_0    condaforge intelopenmp              2022.1.0          h57928b3_3787    condaforge intervaltree              3.1.0              pyhd3eb1b0_0 ipykernel                 6.18.0             pyh025b116_0    condaforge ipython                   7.33.0          py310h5588dad_0    condaforge ipython_genutils          0.2.0                      py_1    condaforge isort                     5.10.1             pyhd8ed1ab_0    condaforge jaraco.classes            3.2.3              pyhd8ed1ab_0    condaforge jedi                      0.18.2             pyhd8ed1ab_0    condaforge jellyfish                 0.9.0           py310h8d17308_2    condaforge jinja2                    3.1.2              pyhd8ed1ab_1    condaforge jinja2time               0.2.0              pyhd8ed1ab_3    condaforge jpeg                      9e                   h8ffe710_2    condaforge jsonschema                4.17.0             pyhd8ed1ab_0    condaforge jupyter_client            7.4.7              pyhd8ed1ab_0    condaforge jupyter_core              5.0.0           py310h5588dad_0    condaforge jupyterlab_pygments       0.2.2              pyhd8ed1ab_0    condaforge keyring                   23.11.0         py310h5588dad_0    condaforge krb5                      1.19.3               h1176d77_0    condaforge kymatio                   0.4.0.dev0               pypi_0    pypi lazyobjectproxy         1.8.0           py310h8d17308_0    condaforge libblas                   3.9.0              16_win64_mkl    condaforge libcblas                  3.9.0              16_win64_mkl    condaforge libclang                  15.0.5          default_h77d9078_0    condaforge libclang13                15.0.5          default_h77d9078_0    condaforge libffi                    3.4.2                h8ffe710_5    condaforge libglib                   2.74.1               he8f3873_1    condaforge libiconv                  1.17                 h8ffe710_0    condaforge liblapack                 3.9.0              16_win64_mkl    condaforge libogg                    1.3.5                h2bbff1b_1 libpng                    1.6.39               h19919ed_0    condaforge libsodium                 1.0.18               h62dcd97_1    condaforge libspatialindex           1.9.3                h39d44d4_4    condaforge libsqlite                 3.40.0               hcfcfb64_0    condaforge libvorbis                 1.3.7                ha925a31_0    condaforge libzlib                   1.2.13               hcfcfb64_4    condaforge m2w64gcclibgfortran     5.3.0                         6    condaforge m2w64gcclibs            5.3.0                         7    condaforge m2w64gcclibscore       5.3.0                         7    condaforge m2w64gmp                 6.1.0                         2    condaforge m2w64libwinpthreadgit   5.0.0.4634.697f757               2    condaforge markupsafe                2.1.1           py310h8d17308_2    condaforge matplotlibinline         0.1.6              pyhd8ed1ab_0    condaforge mccabe                    0.7.0              pyhd8ed1ab_0    condaforge mistune                   2.0.4              pyhd8ed1ab_0    condaforge mkl                       2022.1.0           h6a75c08_874    condaforge moreitertools            9.0.0              pyhd8ed1ab_0    condaforge msys2condaepoch         20160418                      1    condaforge mypy_extensions           0.4.3           py310h5588dad_6    condaforge nbclient                  0.7.0              pyhd8ed1ab_0    condaforge nbconvert                 7.2.5              pyhd8ed1ab_0    condaforge nbconvertcore            7.2.5              pyhd8ed1ab_0    condaforge nbconvertpandoc          7.2.5              pyhd8ed1ab_0    condaforge nbformat                  5.7.0              pyhd8ed1ab_0    condaforge nestasyncio              1.5.6              pyhd8ed1ab_0    condaforge numpy                     1.23.5          py310h4a8f9c9_0    condaforge numpydoc                  1.5.0              pyhd8ed1ab_0    condaforge openssl                   1.1.1s               hcfcfb64_0    condaforge opteinsum                3.3.0                    pypi_0    pypi packaging                 21.3               pyhd8ed1ab_0    condaforge pandoc                    2.19.2               h57928b3_1    condaforge pandocfilters             1.5.0              pyhd8ed1ab_0    condaforge paramiko                  2.12.0             pyhd8ed1ab_0    condaforge parso                     0.8.3              pyhd8ed1ab_0    condaforge pathspec                  0.10.2             pyhd8ed1ab_0    condaforge pcre2                     10.40                h17e33f8_0    condaforge pexpect                   4.8.0              pyh9f0ad1d_2    condaforge pickleshare               0.7.5                   py_1003    condaforge pip                       22.3.1             pyhd8ed1ab_0    condaforge pkgutilresolvename      1.3.10             pyhd8ed1ab_0    condaforge platformdirs              2.5.2              pyhd8ed1ab_1    condaforge pluggy                    1.0.0           py310h5588dad_4    condaforge ply                       3.11                       py_1    condaforge prompttoolkit            3.0.33             pyha770c72_0    condaforge psutil                    5.9.4           py310h8d17308_0    condaforge ptyprocess                0.7.0              pyhd3deb0d_0    condaforge pycodestyle               2.9.1              pyhd8ed1ab_0    condaforge pycparser                 2.21               pyhd8ed1ab_0    condaforge pydocstyle                6.1.1              pyhd8ed1ab_0    condaforge pyflakes                  2.5.0              pyhd8ed1ab_0    condaforge pygments                  2.13.0             pyhd8ed1ab_0    condaforge pylint                    2.15.6             pyhd8ed1ab_0    condaforge pylintvenv               2.3.0              pyhd8ed1ab_0    condaforge pylsspyder               0.4.0              pyhd8ed1ab_0    condaforge pynacl                    1.5.0           py310h635b8f1_2    condaforge pyopenssl                 22.1.0             pyhd8ed1ab_0    condaforge pyparsing                 3.0.9              pyhd8ed1ab_0    condaforge pyqt                      5.15.7          py310h1fd54f2_2    condaforge pyqt5sip                 12.11.0         py310h00ffb61_2    condaforge pyqtwebengine             5.15.7          py310h1fd54f2_2    condaforge pyrsistent                0.19.2          py310h8d17308_0    condaforge pysocks                   1.7.1           py310h5588dad_5    condaforge pytest                    7.2.0           py310h5588dad_1    condaforge python                    3.10.8          h0269646_0_cpython    condaforge pythondateutil           2.8.2              pyhd8ed1ab_0    condaforge pythonfastjsonschema     2.16.2             pyhd8ed1ab_0    condaforge pythonlspblack          1.2.1              pyhd8ed1ab_0    condaforge pythonlspjsonrpc        1.0.0              pyhd8ed1ab_0    condaforge pythonlspserver         1.6.0                hd8ed1ab_0    condaforge pythonlspserverbase    1.6.0              pyhd8ed1ab_0    condaforge pythonslugify            7.0.0              pyhd8ed1ab_0    condaforge python_abi                3.10                    3_cp310    condaforge pytoolconfig              1.2.2              pyhd8ed1ab_0    condaforge pytz                      2022.6             pyhd8ed1ab_0    condaforge pywin32                   304             py310h00ffb61_2    condaforge pywin32ctypes            0.2.0           py310h5588dad_1006    condaforge pyyaml                    6.0             py310h8d17308_5    condaforge pyzmq                     24.0.1          py310hcd737a0_1    condaforge qdarkstyle                3.0.3              pyhd8ed1ab_0    condaforge qstylizer                 0.2.2              pyhd8ed1ab_0    condaforge qtmain                   5.15.6               h9c3277a_2    condaforge qtwebengine              5.15.9               hb9a9bb5_4 qtawesome                 1.2.1              pyhd8ed1ab_0    condaforge qtconsole                 5.4.0              pyhd8ed1ab_0    condaforge qtconsolebase            5.4.0              pyha770c72_0    condaforge qtpy                      2.3.0              pyhd8ed1ab_0    condaforge requests                  2.28.1             pyhd8ed1ab_1    condaforge rope                      1.5.0              pyhd8ed1ab_0    condaforge rtree                     1.0.1           py310h1cbd46b_1    condaforge scipy                     1.9.3           py310h578b7cb_2    condaforge setuptools                65.5.1             pyhd8ed1ab_0    condaforge sip                       6.7.5           py310h00ffb61_0    condaforge six                       1.16.0             pyh6c4a22f_0    condaforge snowballstemmer           2.2.0              pyhd8ed1ab_0    condaforge sortedcontainers          2.4.0              pyhd8ed1ab_0    condaforge soupsieve                 2.3.2.post1        pyhd8ed1ab_0    condaforge sphinx                    5.3.0              pyhd8ed1ab_0    condaforge sphinxcontribapplehelp   1.0.2                      py_0    condaforge sphinxcontribdevhelp     1.0.2                      py_0    condaforge sphinxcontribhtmlhelp    2.0.0              pyhd8ed1ab_0    condaforge sphinxcontribjsmath      1.0.1                      py_0    condaforge sphinxcontribqthelp      1.0.3                      py_0    condaforge sphinxcontribserializinghtml 1.1.5              pyhd8ed1ab_2    condaforge spyder                    5.4.0           py310h5588dad_0    condaforge spyderkernels            2.4.0           py310h5588dad_0    condaforge tbb                       2021.7.0             h91493d7_0    condaforge textunidecode            1.3                        py_0    condaforge textdistance              4.5.0              pyhd8ed1ab_0    condaforge threemerge               0.1.1              pyh9f0ad1d_0    condaforge tinycss2                  1.2.1              pyhd8ed1ab_0    condaforge tk                        8.6.12               h8ffe710_0    condaforge toml                      0.10.2             pyhd8ed1ab_0    condaforge tomli                     2.0.1              pyhd8ed1ab_0    condaforge tomlkit                   0.11.6             pyha770c72_0    condaforge tornado                   6.2             py310h8d17308_1    condaforge traitlets                 5.5.0              pyhd8ed1ab_0    condaforge typing                    3.10.0.0           pyhd8ed1ab_0    condaforge typingextensions         4.4.0                hd8ed1ab_0    condaforge typing_extensions         4.4.0              pyha770c72_0    condaforge tzdata                    2022f                h191b570_0    condaforge ucrt                      10.0.22621.0         h57928b3_0    condaforge ujson                     5.5.0           py310h00ffb61_1    condaforge unidecode                 1.3.6              pyhd8ed1ab_0    condaforge urllib3                   1.26.12         py310haa95532_0 vc                        14.3                 h3d8a991_9    condaforge vs2015_runtime            14.32.31332          h1d6e394_9    condaforge watchdog                  2.1.9           py310h5588dad_1    condaforge wavespin                  0.1.2                    pypi_0    pypi wcwidth                   0.2.5              pyh9f0ad1d_2    condaforge webencodings              0.5.1                      py_1    condaforge whatthepatch              1.0.3              pyhd8ed1ab_0    condaforge wheel                     0.38.4             pyhd8ed1ab_0    condaforge win_inet_pton             1.1.0           py310h5588dad_5    condaforge wrapt                     1.14.1          py310h8d17308_1    condaforge xz                        5.2.6                h8d14728_0    condaforge yaml                      0.2.5                h8ffe710_2    condaforge yapf                      0.32.0             pyhd8ed1ab_0    condaforge zeromq                    4.3.4                h0e60522_1    condaforge zipp                      3.10.0             pyhd8ed1ab_0    condaforge zstd                      1.5.2                h7755175_4    condaforge ```  conda info ```      active environment : jaxenv     active env location : D:\Anaconda\envs\jaxenv             shell level : 2        user config file : C:\Users\OverL\.condarc  populated config files : C:\Users\OverL\.condarc           conda version : 4.10.3     condabuild version : 3.18.11          python version : 3.8.3.final.0        virtual packages : __cuda=11.4=0                           __win=0=0                           __archspec=1=x86_64        base environment : D:\Anaconda  (writable)       conda av data dir : D:\Anaconda\etc\conda   conda av metadata url : None            channel URLs : https://repo.anaconda.com/pkgs/main/win64                           https://repo.anaconda.com/pkgs/main/noarch                           https://repo.anaconda.com/pkgs/r/win64                           https://repo.anaconda.com/pkgs/r/noarch                           https://repo.anaconda.com/pkgs/msys2/win64                           https://repo.anaconda.com/pkgs/msys2/noarch           package cache : D:\Anaconda\pkgs                           C:\Users\OverL\.conda\pkgs                           C:\Users\OverL\AppData\Local\conda\conda\pkgs        envs directories : D:\Anaconda\envs                           C:\Users\OverL\.conda\envs                           C:\Users\OverL\AppData\Local\conda\conda\envs                platform : win64              useragent : conda/4.10.3 requests/2.24.0 CPython/3.8.3 Windows/10 Windows/10.0.19041           administrator : True              netrc file : C:\Users\OverL/.netrc            offline mode : False ```  pip freeze ``` abslpy==1.3.0 alabaster==0.7.12 appdirs==1.4.4 arrow @ file:///home/conda/feedstock_root/build_artifacts/arrow_1662382474514/work astroid @ file:///D:/bld/astroid_1668904413310/work atomicwrites @ file:///home/conda/feedstock_root/build_artifacts/atomicwrites_1657325823582/work attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1659291887007/work autopep8 @ file:///home/conda/feedstock_root/build_artifacts/autopep8_1635267974115/work Babel @ file:///home/conda/feedstock_root/build_artifacts/babel_1667688356751/work backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work backports.functoolslrucache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1618230623929/work bcrypt @ file:///D:/bld/bcrypt_1666831293551/work beautifulsoup4 @ file:///home/conda/feedstock_root/build_artifacts/beautifulsoup4_1649463573192/work binaryornot==0.4.4 black @ file:///D:/bld/blackrecipe_1666900162923/work bleach @ file:///home/conda/feedstock_root/build_artifacts/bleach_1656355450470/work brotlipy @ file:///D:/bld/brotlipy_1666764804378/work certifi==2022.9.24 cffi @ file:///D:/bld/cffi_1666754925774/work chardet @ file:///D:/bld/chardet_1666817639647/work charsetnormalizer @ file:///home/conda/feedstock_root/build_artifacts/charsetnormalizer_1661170624537/work click @ file:///D:/bld/click_1666770360789/work cloudpickle @ file:///home/conda/feedstock_root/build_artifacts/cloudpickle_1662587369221/work colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work comm @ file:///home/conda/feedstock_root/build_artifacts/comm_1668713578881/work configparser==5.3.0 contourpy @ file:///D:/bld/contourpy_1667248078730/work cookiecutter @ file:///home/conda/feedstock_root/build_artifacts/cookiecutter_1654122127219/work cryptography @ file:///D:/bld/cryptography_1667422960373/work cycler @ file:///home/conda/feedstock_root/build_artifacts/cycler_1635519461629/work debugpy @ file:///D:/bld/debugpy_1666826400464/work decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work defusedxml @ file:///home/conda/feedstock_root/build_artifacts/defusedxml_1615232257335/work diffmatchpatch @ file:///home/conda/feedstock_root/build_artifacts/diffmatchpatch_1594679019945/work dill @ file:///home/conda/feedstock_root/build_artifacts/dill_1666603105584/work docstringtomarkdown @ file:///home/conda/feedstock_root/build_artifacts/docstringtomarkdown_1637247638475/work docutils @ file:///D:/bld/docutils_1666755081246/work entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work etils==0.9.0 exceptiongroup @ file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1668523704481/work fastjsonschema @ file:///home/conda/feedstock_root/build_artifacts/pythonfastjsonschema_1663619548554/work/dist flake8 @ file:///home/conda/feedstock_root/build_artifacts/flake8_1659645013175/work flatbuffers==2.0.7 fonttools @ file:///D:/bld/fonttools_1666827169023/work idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work imagesize @ file:///home/conda/feedstock_root/build_artifacts/imagesize_1656939531508/work importlibmetadata @ file:///home/conda/feedstock_root/build_artifacts/importlibmetadata_1666781969417/work importlibresources @ file:///home/conda/feedstock_root/build_artifacts/importlib_resources_1665204935269/work inflection @ file:///home/conda/feedstock_root/build_artifacts/inflection_1598089801258/work iniconfig @ file:///home/conda/feedstock_root/build_artifacts/iniconfig_1603384189793/work intervaltree @ file:///Users/ktietz/demo/mc3/condabld/intervaltree_1630511889664/work ipykernel @ file:///D:/bld/ipykernel_1669056763652/work ipython @ file:///D:/bld/ipython_1651240745149/work ipythongenutils==0.2.0 isort @ file:///home/conda/feedstock_root/build_artifacts/isort_1636447814597/work jaraco.classes @ file:///home/conda/feedstock_root/build_artifacts/jaraco.classes_1667024629799/work jax==0.3.24 jaxlib==0.3.24 jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1669134318875/work jellyfish @ file:///D:/bld/jellyfish_1666945709644/work Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work jinja2time @ file:///home/conda/feedstock_root/build_artifacts/jinja2time_1646750632133/work jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschemameta_1667361745641/work jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1668623095912/work jupyter_core @ file:///D:/bld/jupyter_core_1668030965393/work jupyterlabpygments @ file:///home/conda/feedstock_root/build_artifacts/jupyterlab_pygments_1649936611996/work keyring @ file:///D:/bld/keyring_1667696866921/work kiwisolver @ file:///D:/bld/kiwisolver_1666805768319/work kymatio @ git+https://github.com/kymatio/kymatio lazyobjectproxy @ file:///D:/bld/lazyobjectproxy_1666812092877/work MarkupSafe @ file:///D:/bld/markupsafe_1666770331304/work matplotlib @ file:///D:/bld/matplotlibsuite_1667505061247/work matplotlibinline @ file:///home/conda/feedstock_root/build_artifacts/matplotlibinline_1660814786464/work mccabe @ file:///home/conda/feedstock_root/build_artifacts/mccabe_1643049622439/work mistune @ file:///home/conda/feedstock_root/build_artifacts/mistune_1657892024508/work moreitertools @ file:///home/conda/feedstock_root/build_artifacts/moreitertools_1666110321141/work munkres==1.1.4 mypyextensions @ file:///D:/bld/mypy_extensions_1666795127376/work nbclient @ file:///home/conda/feedstock_root/build_artifacts/nbclient_1665125402713/work nbconvert @ file:///home/conda/feedstock_root/build_artifacts/nbconvertmeta_1668442474361/work nbformat @ file:///home/conda/feedstock_root/build_artifacts/nbformat_1665426034066/work nestasyncio @ file:///home/conda/feedstock_root/build_artifacts/nestasyncio_1664684991461/work numpy @ file:///D:/bld/numpy_1668919338279/work numpydoc @ file:///home/conda/feedstock_root/build_artifacts/numpydoc_1665273484262/work opteinsum==3.3.0 packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1637239678211/work pandocfilters @ file:///home/conda/feedstock_root/build_artifacts/pandocfilters_1631603243851/work paramiko @ file:///home/conda/feedstock_root/build_artifacts/paramiko_1667639932385/work parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work pathspec @ file:///home/conda/feedstock_root/build_artifacts/pathspec_1668325009666/work pexpect @ file:///home/conda/feedstock_root/build_artifacts/pexpect_1602535608087/work pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work Pillow @ file:///D:/bld/pillow_1666920753896/work pkgutil_resolve_name @ file:///home/conda/feedstock_root/build_artifacts/pkgutilresolvename_1633981968097/work platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1657729053205/work pluggy @ file:///D:/bld/pluggy_1666787589498/work ply==3.11 prompttoolkit @ file:///home/conda/feedstock_root/build_artifacts/prompttoolkit_1669057097528/work psutil @ file:///D:/bld/psutil_1667886056621/work ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess0.7.0py2.py3noneany.whl pycodestyle @ file:///home/conda/feedstock_root/build_artifacts/pycodestyle_1659638152915/work pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work pydocstyle @ file:///home/conda/feedstock_root/build_artifacts/pydocstyle_1621377123289/work pyflakes @ file:///home/conda/feedstock_root/build_artifacts/pyflakes_1659210156976/work Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1660666458521/work pylint @ file:///home/conda/feedstock_root/build_artifacts/pylint_1668871495697/work pylintvenv @ file:///home/conda/feedstock_root/build_artifacts/pylintvenv_1656263679357/work pylsspyder @ file:///home/conda/feedstock_root/build_artifacts/pylsspyder_1619747398504/work PyNaCl @ file:///D:/bld/pynacl_1666862759359/work pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1665350324128/work pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work PyQt5==5.15.7 PyQt5sip @ file:///D:/bld/pyqtsplit_1666829883856/work/pyqt_sip PyQtWebEngine==5.15.4 pyrsistent @ file:///D:/bld/pyrsistent_1667498839113/work PySocks @ file:///D:/bld/pysocks_1648857426124/work pytest==7.2.0 pythondateutil @ file:///home/conda/feedstock_root/build_artifacts/pythondateutil_1626286286081/work pythonlspblack @ file:///home/conda/feedstock_root/build_artifacts/pythonlspblack_1649795569828/work pythonlspjsonrpc @ file:///home/conda/feedstock_root/build_artifacts/pythonlspjsonrpc_1618530352985/work pythonlspserver @ file:///home/conda/feedstock_root/build_artifacts/pythonlspservermeta_1667491645580/work pythonslugify @ file:///home/conda/feedstock_root/build_artifacts/pythonslugify_1668871579734/work pytoolconfig @ file:///home/conda/feedstock_root/build_artifacts/pytoolconfig_1659322367429/work pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1667391478166/work pywin32==304 pywin32ctypes @ file:///D:/bld/pywin32ctypes_1666760898350/work PyYAML @ file:///D:/bld/pyyaml_1666772550103/work pyzmq @ file:///D:/bld/pyzmq_1666828590571/work QDarkStyle @ file:///home/conda/feedstock_root/build_artifacts/qdarkstyle_1638323533455/work qstylizer @ file:///home/conda/feedstock_root/build_artifacts/qstylizer_1662244505808/work/dist/qstylizer0.2.2py2.py3noneany.whl QtAwesome @ file:///home/conda/feedstock_root/build_artifacts/qtawesome_1666638781418/work qtconsole @ file:///home/conda/feedstock_root/build_artifacts/qtconsolebase_1667404144336/work QtPy @ file:///home/conda/feedstock_root/build_artifacts/qtpy_1667873092748/work requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1661872987712/work rope @ file:///home/conda/feedstock_root/build_artifacts/rope_1669133390676/work Rtree @ file:///D:/bld/rtree_1666813330222/work scipy==1.9.3 sip @ file:///D:/bld/sip_1669040101788/work six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work snowballstemmer @ file:///home/conda/feedstock_root/build_artifacts/snowballstemmer_1637143057757/work sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work soupsieve @ file:///home/conda/feedstock_root/build_artifacts/soupsieve_1658207591808/work Sphinx @ file:///home/conda/feedstock_root/build_artifacts/sphinx_1665915552897/work sphinxcontribapplehelp==1.0.2 sphinxcontribdevhelp==1.0.2 sphinxcontribhtmlhelp @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontribhtmlhelp_1621704829796/work sphinxcontribjsmath==1.0.1 sphinxcontribqthelp==1.0.3 sphinxcontribserializinghtml @ file:///home/conda/feedstock_root/build_artifacts/sphinxcontribserializinghtml_1649380998999/work spyder @ file:///D:/bld/spyder_1667851050917/work spyderkernels @ file:///D:/bld/spyderkernels_1667489764576/work textunidecode==1.3 textdistance @ file:///home/conda/feedstock_root/build_artifacts/textdistance_1663527496115/work threemerge @ file:///home/conda/feedstock_root/build_artifacts/threemerge_1595515817927/work tinycss2 @ file:///home/conda/feedstock_root/build_artifacts/tinycss2_1666100256010/work toml @ file:///home/conda/feedstock_root/build_artifacts/toml_1604308577558/work tomli @ file:///home/conda/feedstock_root/build_artifacts/tomli_1644342247877/work tomlkit @ file:///home/conda/feedstock_root/build_artifacts/tomlkit_1666864188602/work tornado @ file:///D:/bld/tornado_1666788744359/work traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1666115969632/work typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1665144421445/work ujson @ file:///D:/bld/ujson_1666837388708/work unicodedata2 @ file:///D:/bld/unicodedata2_1667239969015/work Unidecode @ file:///home/conda/feedstock_root/build_artifacts/unidecode_1664588179651/work urllib3 @ file:///C:/b/abs_a8_3vfznn_/croot/urllib3_1666298943664/work watchdog @ file:///D:/bld/watchdog_1666806636407/work e git+https://github.com/gptanon/wttest.gitegg=WaveSpin wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1600965781394/work webencodings==0.5.1 whatthepatch @ file:///home/conda/feedstock_root/build_artifacts/whatthepatch_1668379856745/work wininetpton @ file:///D:/bld/win_inet_pton_1666755058235/work wrapt @ file:///D:/bld/wrapt_1666806148286/work yapf @ file:///home/conda/feedstock_root/build_artifacts/yapf_1641487982943/work zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1666647772197/work ```   What jax/jaxlib version are you using? jax 0.3.24, jaxlib 0.3.24  Which accelerator(s) are you using? GTX 1070  Additional system info Windows 10, Python 3.10.8, CUDA 11.3, cuDNN 8.2.0  NVIDIA GPU info info ``` Thu Nov 24 09:31:07 2022 ++  ++ ```  ",2022-11-25T06:46:37Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/13407,Google Colab: ``` jax v0.3.25 numpy 0.007686582600000094 jaxcpu 0.07994909519000004 jaxgpu 0.10680905182999993 ```,Would it be possible and easy enough to share some of the profiler output that you mention?,"On Spyder, sure, spyder_jax.Result. Some highlights:  "
334,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Move a couple of tests that ended by mistake under XlaCallModuleTest)， 内容是 ([jax2tf] Move a couple of tests that ended by mistake under XlaCallModuleTest)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Move a couple of tests that ended by mistake under XlaCallModuleTest,[jax2tf] Move a couple of tests that ended by mistake under XlaCallModuleTest,2022-11-24T19:10:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13405
1205,"以下是一个github上的jax下的一个issue, 标题是(libc++abi Error while following TPU-VM Jax Tutorial)， 内容是 ( Description I've been running into an error while trying to follow the official Jax Tutorial https://cloud.google.com/tpu/docs/runcalculationjax.  Steps to reproduce: I created a VM with ``` gcloud compute tpus tpuvm tpuname zone uscentral1f acceleratortype v28 version tpuvmtf2.10.0 metadata enableoslogin=TRUE ``` Next, install jax with the command ``` pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` I then ran ``` python3 ``` And then ``` import jax jax.device_count() ``` At this point I receive the following error: ``` libc++abi: terminating due to uncaught exception of type s_13615: yamlcpp: error at line 20, column 12: end of the map not found Aborted (core dumped) ``` Any suggestions would be appreciated!  What jax/jaxlib version are you using? jax v0.3.25, jaxlib v0.3.25  Which accelerator(s) are you using? TPU v28  Additional system info Using tpuvmtf2.10.0, python3.8.10  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,libc++abi Error while following TPU-VM Jax Tutorial," Description I've been running into an error while trying to follow the official Jax Tutorial https://cloud.google.com/tpu/docs/runcalculationjax.  Steps to reproduce: I created a VM with ``` gcloud compute tpus tpuvm tpuname zone uscentral1f acceleratortype v28 version tpuvmtf2.10.0 metadata enableoslogin=TRUE ``` Next, install jax with the command ``` pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` I then ran ``` python3 ``` And then ``` import jax jax.device_count() ``` At this point I receive the following error: ``` libc++abi: terminating due to uncaught exception of type s_13615: yamlcpp: error at line 20, column 12: end of the map not found Aborted (core dumped) ``` Any suggestions would be appreciated!  What jax/jaxlib version are you using? jax v0.3.25, jaxlib v0.3.25  Which accelerator(s) are you using? TPU v28  Additional system info Using tpuvmtf2.10.0, python3.8.10  NVIDIA GPU info _No response_",2022-11-24T16:15:06Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/13402,"I'm not able to repro this. Can you try your instructions again and verify you're still getting the issue? (Thanks for the clear instructions btw!) Also FYI, if you're only planning to access the TPU via jax, you should use `version tpuvmbase` (you can still pip install tensorflow for CPUonly TF access). But maybe you're planning use both jax and TF (note they can't access the TPU at the same time).","Thanks, looks like everything is working now!"
539,"以下是一个github上的jax下的一个issue, 标题是(Provide cuda+aarch64 downloads at https://storage.googleapis.com/jax-releases/jax_cuda_releases.html)， 内容是 (I am not asking for a new feature in the APIs but for a new release type: CUDA+Linux ARM64. At the moment there are only x86_64 wheels at https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html. I'd like to request for `aarch64` ones too! Thank you!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Provide cuda+aarch64 downloads at https://storage.googleapis.com/jax-releases/jax_cuda_releases.html,I am not asking for a new feature in the APIs but for a new release type: CUDA+Linux ARM64. At the moment there are only x86_64 wheels at https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html. I'd like to request for `aarch64` ones too! Thank you!,2022-11-24T14:13:02Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/13397,"Hi  thanks for the request. This looks like a duplicate of https://github.com/google/jax/issues/7097, you can find some information there."
396,"以下是一个github上的jax下的一个issue, 标题是(Enable faster test-runners for PR/push CI runs.)， 内容是 (I encouraged our org to enable the faster github test runner machines.  We should use them to speed up tests  and perhaps to increase the number of generated test cases.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Enable faster test-runners for PR/push CI runs.,I encouraged our org to enable the faster github test runner machines.  We should use them to speed up tests  and perhaps to increase the number of generated test cases.,2022-11-23T20:04:27Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/13382,"Previously, average of ~20min for GH CI tests. 16core result:  7m 33s 64core result: ~ 6m 52s (some runners took a while to spin up, could be faster, actual testing takes half the time of 16core.) I'll set it to 16core for now, but we could boost to 64core if we want to pump up generated test number."
13934,"以下是一个github上的jax下的一个issue, 标题是(KeyError: dtype([('float0', 'V')]) raised when passing PRNG keys to odeint function)， 内容是 ( Description I am trying to pass `PRNGKey`s to a function, which is integrated by `odeint`. Here is a simplified example reproducing the problem: ```python from functools import partial import jax.numpy as jnp from jax.tree_util import tree_map from jax.experimental.ode import odeint import jax W = jnp.eye(10, dtype=jnp.float64) b = jnp.ones((10,), dtype=jnp.float64) x = jnp.linspace(jnp.zeros((10,)), 5 * jnp.ones((10,)), 10) t = jnp.linspace(0, 10, 10) def sample_W(W, prng_key):     return jax.random.normal(prng_key, (10,)) + W def f(params, x, t, prng_key):     W, b = params     W_s = sample_W(W, prng_key)     return x @ W_s + b def loss(params, prng_key, x, t):     x_hat = odeint(partial(f, params), x[0], t, prng_key)     return jnp.mean((x_hat  x[1])**2) .jit def update(params, prng_key, x, t, eta=1e3):     grads = jax.grad(loss)(params, prng_key, x, t)     return tree_map(lambda p,g: p  eta * g,  params, grads) prng_key = jax.random.PRNGKey(0) params = (W, b) for i in range(100):     (prng_key,) = jax.random.split(prng_key, 1)     params = update(params, prng_key, x, t, 1e3) ``` Which raises the following exception:    KeyError: dtype([('float0', 'V')]) ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) File ~/GP/env/causaldev/lib/python3.8/runpy.py:194, in _run_module_as_main(***failed resolving arguments***)     193     sys.argv[0] = mod_spec.origin > 194 return _run_code(code, main_globals, None,     195                  ""__main__"", mod_spec) File ~/GP/env/causaldev/lib/python3.8/runpy.py:87, in _run_code(***failed resolving arguments***)      80 run_globals.update(__name__ = mod_name,      81                    __file__ = fname,      82                    __cached__ = cached,    (...)      85                    __package__ = pkg_name,      86                    __spec__ = mod_spec) > 87 exec(code, run_globals)      88 return run_globals File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel_launcher.py:17      15 from ipykernel import kernelapp as app > 17 app.launch_new_instance() File ~/GP/env/causaldev/lib/python3.8/sitepackages/traitlets/config/application.py:976, in Application.launch_instance(***failed resolving arguments***)     975 app.initialize(argv) > 976 app.start() File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelapp.py:712, in IPKernelApp.start(***failed resolving arguments***)     711 try: > 712     self.io_loop.start()     713 except KeyboardInterrupt: File ~/GP/env/causaldev/lib/python3.8/sitepackages/tornado/platform/asyncio.py:215, in BaseAsyncIOLoop.start(***failed resolving arguments***)     214     asyncio.set_event_loop(self.asyncio_loop) > 215     self.asyncio_loop.run_forever()     216 finally: File ~/GP/env/causaldev/lib/python3.8/asyncio/base_events.py:570, in BaseEventLoop.run_forever(***failed resolving arguments***)     569 while True: > 570     self._run_once()     571     if self._stopping: File ~/GP/env/causaldev/lib/python3.8/asyncio/base_events.py:1859, in BaseEventLoop._run_once(***failed resolving arguments***)    1858     else: > 1859         handle._run()    1860 handle = None File ~/GP/env/causaldev/lib/python3.8/asyncio/events.py:81, in Handle._run(***failed resolving arguments***)      80 try: > 81     self._context.run(self._callback, *self._args)      82 except (SystemExit, KeyboardInterrupt): File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelbase.py:510, in Kernel.dispatch_queue(***failed resolving arguments***)     509 try: > 510     await self.process_one()     511 except Exception: File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelbase.py:499, in Kernel.process_one(***failed resolving arguments***)     498         return None > 499 await dispatch(*args) File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelbase.py:406, in Kernel.dispatch_shell(***failed resolving arguments***)     405     if inspect.isawaitable(result): > 406         await result     407 except Exception: File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelbase.py:730, in Kernel.execute_request(***failed resolving arguments***)     729 if inspect.isawaitable(reply_content): > 730     reply_content = await reply_content     732  Flush output before sending the reply. File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/ipkernel.py:383, in IPythonKernel.do_execute(***failed resolving arguments***)     382 if with_cell_id: > 383     res = shell.run_cell(     384         code,     385         store_history=store_history,     386         silent=silent,     387         cell_id=cell_id,     388     )     389 else: File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/zmqshell.py:528, in ZMQInteractiveShell.run_cell(***failed resolving arguments***)     527 self._last_traceback = None > 528 return super().run_cell(*args, **kwargs) File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:2885, in InteractiveShell.run_cell(***failed resolving arguments***)    2884 try: > 2885     result = self._run_cell(    2886         raw_cell, store_history, silent, shell_futures, cell_id    2887     )    2888 finally: File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:2940, in InteractiveShell._run_cell(***failed resolving arguments***)    2939 try: > 2940     return runner(coro)    2941 except BaseException as e: File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/async_helpers.py:129, in _pseudo_sync_runner(***failed resolving arguments***)     128 try: > 129     coro.send(None)     130 except StopIteration as exc: File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:3139, in InteractiveShell.run_cell_async(***failed resolving arguments***)    3137 interactivity = ""none"" if silent else self.ast_node_interactivity > 3139 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3140        interactivity=interactivity, compiler=compiler, result=result)    3142 self.last_execution_succeeded = not has_raised File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:3318, in InteractiveShell.run_ast_nodes(***failed resolving arguments***)    3317     asy = compare(code) > 3318 if await self.run_code(code, result, async_=asy):    3319     return True File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:3378, in InteractiveShell.run_code(***failed resolving arguments***)    3377     else: > 3378         exec(code_obj, self.user_global_ns, self.user_ns)    3379 finally:    3380      Reset our crash handler in place Cell In [1], line 35      34 (prng_key,) = jax.random.split(prng_key, 1) > 35 params = update(params, prng_key, x, t, 1e3) Cell In [1], line 28, in update(***failed resolving arguments***)      26 .jit      27 def update(params, prng_key, x, t, eta=1e3): > 28     grads = jax.grad(loss)(params, prng_key, x, t)      29     return tree_map(lambda p,g: p  eta * g,  params, grads) Cell In [1], line 23, in loss(***failed resolving arguments***)      22 def loss(params, prng_key, x, t): > 23     x_hat = odeint(partial(f, params), x[0], t, prng_key)      24     return jnp.mean((x_hat  x[1])**2) File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:179, in odeint(***failed resolving arguments***)     178 converted, consts = custom_derivatives.closure_convert(func, y0, t[0], *args) > 179 return _odeint_wrapper(converted, rtol, atol, mxstep, hmax, y0, t, *args, *consts) File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:185, in _odeint_wrapper(***failed resolving arguments***)     184 func = ravel_first_arg(func, unravel) > 185 out = _odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args)     186 return jax.vmap(unravel)(out) JaxStackTraceBeforeTransformation: KeyError: dtype([('float0', 'V')]) The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: KeyError                                  Traceback (most recent call last) Cell In [1], line 35      33 for i in range(100):      34     (prng_key,) = jax.random.split(prng_key, 1) > 35     params = update(params, prng_key, x, t, 1e3)     [... skipping hidden 11 frame] Cell In [1], line 28, in update(params, prng_key, x, t, eta)      26 .jit      27 def update(params, prng_key, x, t, eta=1e3): > 28     grads = jax.grad(loss)(params, prng_key, x, t)      29     return tree_map(lambda p,g: p  eta * g,  params, grads)     [... skipping hidden 19 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:259, in _odeint_rev(func, rtol, atol, mxstep, hmax, res, g)     256   return (y_bar, t0_bar, args_bar), t_bar     258 init_carry = (g[1], 0., tree_map(jnp.zeros_like, args)) > 259 (y_bar, t0_bar, args_bar), rev_ts_bar = lax.scan(     260     scan_fun, init_carry, jnp.arange(len(ts)  1, 0, 1))     261 ts_bar = jnp.concatenate([jnp.array([t0_bar]), rev_ts_bar[::1]])     262 return (y_bar, ts_bar, *args_bar)     [... skipping hidden 9 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:249, in _odeint_rev..scan_fun(carry, i)     247 t0_bar = t0_bar  t_bar     248  Run augmented system backwards to previous observation > 249 _, y_bar, t0_bar, args_bar = odeint(     250     aug_dynamics, (ys[i], y_bar, t0_bar, args_bar),     251     jnp.array([ts[i], ts[i  1]]),     252     *args, rtol=rtol, atol=atol, mxstep=mxstep, hmax=hmax)     253 y_bar, t0_bar, args_bar = tree_map(op.itemgetter(1), (y_bar, t0_bar, args_bar))     254  Add gradient from current output File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:179, in odeint(func, y0, t, rtol, atol, mxstep, hmax, *args)     176   raise TypeError(f""t must be an array of floats, but got {t}."")     178 converted, consts = custom_derivatives.closure_convert(func, y0, t[0], *args) > 179 return _odeint_wrapper(converted, rtol, atol, mxstep, hmax, y0, t, *args, *consts)     [... skipping hidden 5 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:185, in _odeint_wrapper(func, rtol, atol, mxstep, hmax, y0, ts, *args)     183 y0, unravel = ravel_pytree(y0)     184 func = ravel_first_arg(func, unravel) > 185 out = _odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args)     186 return jax.vmap(unravel)(out)     [... skipping hidden 6 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:216, in _odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args)     213   y_target = jnp.polyval(interp_coeff, relative_output_time.astype(interp_coeff.dtype))     214   return carry, y_target > 216 f0 = func_(y0, ts[0])     217 dt = jnp.clip(initial_step_size(func_, ts[0], y0, 4, rtol, atol, f0), a_min=0., a_max=hmax)     218 interp_coeff = jnp.array([y0] * 5) File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:190, in _odeint..(y, t)     188 (jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3, 4))     189 def _odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args): > 190   func_ = lambda y, t: func(y, t, *args)     192   def scan_fun(carry, target_t):     194     def cond_fun(state):     [... skipping hidden 1 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:54, in ravel_first_arg_(unravel, y_flat, *args)      52 y = unravel(y_flat)      53 ans = yield (y,) + args, {} > 54 ans_flat, _ = ravel_pytree(ans)      55 yield ans_flat File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/_src/flatten_util.py:49, in ravel_pytree(pytree)      30 """"""Ravel (flatten) a pytree of arrays down to a 1D array.      31       32 Args:    (...)      46       47 """"""      48 leaves, treedef = tree_flatten(pytree) > 49 flat, unravel_list = _ravel_list(leaves)      50 unravel_pytree = lambda flat: tree_unflatten(treedef, unravel_list(flat))      51 return flat, unravel_pytree File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/_src/flatten_util.py:56, in _ravel_list(lst)      54 if not lst: return jnp.array([], jnp.float32), lambda _: []      55 from_dtypes = [dtypes.dtype(l) for l in lst] > 56 to_dtype = dtypes.result_type(*from_dtypes)      57 sizes, shapes = unzip2((jnp.size(x), jnp.shape(x)) for x in lst)      58 indices = np.cumsum(sizes)     [... skipping hidden 3 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/_src/dtypes.py:394, in (.0)     392 N = set(nodes)     393 UB = _lattice_upper_bounds[jax_numpy_dtype_promotion] > 394 CUB = set.intersection(*(UB[n] for n in N))     395 LUB = (CUB & N) or {c for c in CUB if CUB.issubset(UB[c])}     396 if len(LUB) == 1: KeyError: dtype([('float0', 'V')]) ```  The issue in the provided example is actually solved by the following modification: ```python def loss(params, prng_key, x, t):     x_hat = odeint(lambda x, t: f(params, x, t, prng_key), x[0], t)     return jnp.mean((x_hat  x[1])**2) ``` However, my application is more complicated than this, where the above solution is nonapplicable. If understand why the problem is raised and why it is solved by the above modification, then I think I can address this issue in my program.  What jax/jaxlib version are you using? jax==0.3.25,  jaxlib==0.3.25+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_  Update on 23/11/2022 It seems impossible to pass a PRNG to `odeint`'s `*args`, so I have changed the approach of my program. Now I sample from probability distributions and pass the random samples (floats) to `*args` instead of sampling inside the `odeint`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"KeyError: dtype([('float0', 'V')]) raised when passing PRNG keys to odeint function"," Description I am trying to pass `PRNGKey`s to a function, which is integrated by `odeint`. Here is a simplified example reproducing the problem: ```python from functools import partial import jax.numpy as jnp from jax.tree_util import tree_map from jax.experimental.ode import odeint import jax W = jnp.eye(10, dtype=jnp.float64) b = jnp.ones((10,), dtype=jnp.float64) x = jnp.linspace(jnp.zeros((10,)), 5 * jnp.ones((10,)), 10) t = jnp.linspace(0, 10, 10) def sample_W(W, prng_key):     return jax.random.normal(prng_key, (10,)) + W def f(params, x, t, prng_key):     W, b = params     W_s = sample_W(W, prng_key)     return x @ W_s + b def loss(params, prng_key, x, t):     x_hat = odeint(partial(f, params), x[0], t, prng_key)     return jnp.mean((x_hat  x[1])**2) .jit def update(params, prng_key, x, t, eta=1e3):     grads = jax.grad(loss)(params, prng_key, x, t)     return tree_map(lambda p,g: p  eta * g,  params, grads) prng_key = jax.random.PRNGKey(0) params = (W, b) for i in range(100):     (prng_key,) = jax.random.split(prng_key, 1)     params = update(params, prng_key, x, t, 1e3) ``` Which raises the following exception:    KeyError: dtype([('float0', 'V')]) ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) File ~/GP/env/causaldev/lib/python3.8/runpy.py:194, in _run_module_as_main(***failed resolving arguments***)     193     sys.argv[0] = mod_spec.origin > 194 return _run_code(code, main_globals, None,     195                  ""__main__"", mod_spec) File ~/GP/env/causaldev/lib/python3.8/runpy.py:87, in _run_code(***failed resolving arguments***)      80 run_globals.update(__name__ = mod_name,      81                    __file__ = fname,      82                    __cached__ = cached,    (...)      85                    __package__ = pkg_name,      86                    __spec__ = mod_spec) > 87 exec(code, run_globals)      88 return run_globals File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel_launcher.py:17      15 from ipykernel import kernelapp as app > 17 app.launch_new_instance() File ~/GP/env/causaldev/lib/python3.8/sitepackages/traitlets/config/application.py:976, in Application.launch_instance(***failed resolving arguments***)     975 app.initialize(argv) > 976 app.start() File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelapp.py:712, in IPKernelApp.start(***failed resolving arguments***)     711 try: > 712     self.io_loop.start()     713 except KeyboardInterrupt: File ~/GP/env/causaldev/lib/python3.8/sitepackages/tornado/platform/asyncio.py:215, in BaseAsyncIOLoop.start(***failed resolving arguments***)     214     asyncio.set_event_loop(self.asyncio_loop) > 215     self.asyncio_loop.run_forever()     216 finally: File ~/GP/env/causaldev/lib/python3.8/asyncio/base_events.py:570, in BaseEventLoop.run_forever(***failed resolving arguments***)     569 while True: > 570     self._run_once()     571     if self._stopping: File ~/GP/env/causaldev/lib/python3.8/asyncio/base_events.py:1859, in BaseEventLoop._run_once(***failed resolving arguments***)    1858     else: > 1859         handle._run()    1860 handle = None File ~/GP/env/causaldev/lib/python3.8/asyncio/events.py:81, in Handle._run(***failed resolving arguments***)      80 try: > 81     self._context.run(self._callback, *self._args)      82 except (SystemExit, KeyboardInterrupt): File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelbase.py:510, in Kernel.dispatch_queue(***failed resolving arguments***)     509 try: > 510     await self.process_one()     511 except Exception: File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelbase.py:499, in Kernel.process_one(***failed resolving arguments***)     498         return None > 499 await dispatch(*args) File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelbase.py:406, in Kernel.dispatch_shell(***failed resolving arguments***)     405     if inspect.isawaitable(result): > 406         await result     407 except Exception: File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/kernelbase.py:730, in Kernel.execute_request(***failed resolving arguments***)     729 if inspect.isawaitable(reply_content): > 730     reply_content = await reply_content     732  Flush output before sending the reply. File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/ipkernel.py:383, in IPythonKernel.do_execute(***failed resolving arguments***)     382 if with_cell_id: > 383     res = shell.run_cell(     384         code,     385         store_history=store_history,     386         silent=silent,     387         cell_id=cell_id,     388     )     389 else: File ~/GP/env/causaldev/lib/python3.8/sitepackages/ipykernel/zmqshell.py:528, in ZMQInteractiveShell.run_cell(***failed resolving arguments***)     527 self._last_traceback = None > 528 return super().run_cell(*args, **kwargs) File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:2885, in InteractiveShell.run_cell(***failed resolving arguments***)    2884 try: > 2885     result = self._run_cell(    2886         raw_cell, store_history, silent, shell_futures, cell_id    2887     )    2888 finally: File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:2940, in InteractiveShell._run_cell(***failed resolving arguments***)    2939 try: > 2940     return runner(coro)    2941 except BaseException as e: File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/async_helpers.py:129, in _pseudo_sync_runner(***failed resolving arguments***)     128 try: > 129     coro.send(None)     130 except StopIteration as exc: File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:3139, in InteractiveShell.run_cell_async(***failed resolving arguments***)    3137 interactivity = ""none"" if silent else self.ast_node_interactivity > 3139 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3140        interactivity=interactivity, compiler=compiler, result=result)    3142 self.last_execution_succeeded = not has_raised File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:3318, in InteractiveShell.run_ast_nodes(***failed resolving arguments***)    3317     asy = compare(code) > 3318 if await self.run_code(code, result, async_=asy):    3319     return True File ~/GP/env/causaldev/lib/python3.8/sitepackages/IPython/core/interactiveshell.py:3378, in InteractiveShell.run_code(***failed resolving arguments***)    3377     else: > 3378         exec(code_obj, self.user_global_ns, self.user_ns)    3379 finally:    3380      Reset our crash handler in place Cell In [1], line 35      34 (prng_key,) = jax.random.split(prng_key, 1) > 35 params = update(params, prng_key, x, t, 1e3) Cell In [1], line 28, in update(***failed resolving arguments***)      26 .jit      27 def update(params, prng_key, x, t, eta=1e3): > 28     grads = jax.grad(loss)(params, prng_key, x, t)      29     return tree_map(lambda p,g: p  eta * g,  params, grads) Cell In [1], line 23, in loss(***failed resolving arguments***)      22 def loss(params, prng_key, x, t): > 23     x_hat = odeint(partial(f, params), x[0], t, prng_key)      24     return jnp.mean((x_hat  x[1])**2) File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:179, in odeint(***failed resolving arguments***)     178 converted, consts = custom_derivatives.closure_convert(func, y0, t[0], *args) > 179 return _odeint_wrapper(converted, rtol, atol, mxstep, hmax, y0, t, *args, *consts) File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:185, in _odeint_wrapper(***failed resolving arguments***)     184 func = ravel_first_arg(func, unravel) > 185 out = _odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args)     186 return jax.vmap(unravel)(out) JaxStackTraceBeforeTransformation: KeyError: dtype([('float0', 'V')]) The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: KeyError                                  Traceback (most recent call last) Cell In [1], line 35      33 for i in range(100):      34     (prng_key,) = jax.random.split(prng_key, 1) > 35     params = update(params, prng_key, x, t, 1e3)     [... skipping hidden 11 frame] Cell In [1], line 28, in update(params, prng_key, x, t, eta)      26 .jit      27 def update(params, prng_key, x, t, eta=1e3): > 28     grads = jax.grad(loss)(params, prng_key, x, t)      29     return tree_map(lambda p,g: p  eta * g,  params, grads)     [... skipping hidden 19 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:259, in _odeint_rev(func, rtol, atol, mxstep, hmax, res, g)     256   return (y_bar, t0_bar, args_bar), t_bar     258 init_carry = (g[1], 0., tree_map(jnp.zeros_like, args)) > 259 (y_bar, t0_bar, args_bar), rev_ts_bar = lax.scan(     260     scan_fun, init_carry, jnp.arange(len(ts)  1, 0, 1))     261 ts_bar = jnp.concatenate([jnp.array([t0_bar]), rev_ts_bar[::1]])     262 return (y_bar, ts_bar, *args_bar)     [... skipping hidden 9 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:249, in _odeint_rev..scan_fun(carry, i)     247 t0_bar = t0_bar  t_bar     248  Run augmented system backwards to previous observation > 249 _, y_bar, t0_bar, args_bar = odeint(     250     aug_dynamics, (ys[i], y_bar, t0_bar, args_bar),     251     jnp.array([ts[i], ts[i  1]]),     252     *args, rtol=rtol, atol=atol, mxstep=mxstep, hmax=hmax)     253 y_bar, t0_bar, args_bar = tree_map(op.itemgetter(1), (y_bar, t0_bar, args_bar))     254  Add gradient from current output File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:179, in odeint(func, y0, t, rtol, atol, mxstep, hmax, *args)     176   raise TypeError(f""t must be an array of floats, but got {t}."")     178 converted, consts = custom_derivatives.closure_convert(func, y0, t[0], *args) > 179 return _odeint_wrapper(converted, rtol, atol, mxstep, hmax, y0, t, *args, *consts)     [... skipping hidden 5 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:185, in _odeint_wrapper(func, rtol, atol, mxstep, hmax, y0, ts, *args)     183 y0, unravel = ravel_pytree(y0)     184 func = ravel_first_arg(func, unravel) > 185 out = _odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args)     186 return jax.vmap(unravel)(out)     [... skipping hidden 6 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:216, in _odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args)     213   y_target = jnp.polyval(interp_coeff, relative_output_time.astype(interp_coeff.dtype))     214   return carry, y_target > 216 f0 = func_(y0, ts[0])     217 dt = jnp.clip(initial_step_size(func_, ts[0], y0, 4, rtol, atol, f0), a_min=0., a_max=hmax)     218 interp_coeff = jnp.array([y0] * 5) File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:190, in _odeint..(y, t)     188 (jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3, 4))     189 def _odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args): > 190   func_ = lambda y, t: func(y, t, *args)     192   def scan_fun(carry, target_t):     194     def cond_fun(state):     [... skipping hidden 1 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/experimental/ode.py:54, in ravel_first_arg_(unravel, y_flat, *args)      52 y = unravel(y_flat)      53 ans = yield (y,) + args, {} > 54 ans_flat, _ = ravel_pytree(ans)      55 yield ans_flat File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/_src/flatten_util.py:49, in ravel_pytree(pytree)      30 """"""Ravel (flatten) a pytree of arrays down to a 1D array.      31       32 Args:    (...)      46       47 """"""      48 leaves, treedef = tree_flatten(pytree) > 49 flat, unravel_list = _ravel_list(leaves)      50 unravel_pytree = lambda flat: tree_unflatten(treedef, unravel_list(flat))      51 return flat, unravel_pytree File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/_src/flatten_util.py:56, in _ravel_list(lst)      54 if not lst: return jnp.array([], jnp.float32), lambda _: []      55 from_dtypes = [dtypes.dtype(l) for l in lst] > 56 to_dtype = dtypes.result_type(*from_dtypes)      57 sizes, shapes = unzip2((jnp.size(x), jnp.shape(x)) for x in lst)      58 indices = np.cumsum(sizes)     [... skipping hidden 3 frame] File ~/GP/env/causaldev/lib/python3.8/sitepackages/jax/_src/dtypes.py:394, in (.0)     392 N = set(nodes)     393 UB = _lattice_upper_bounds[jax_numpy_dtype_promotion] > 394 CUB = set.intersection(*(UB[n] for n in N))     395 LUB = (CUB & N) or {c for c in CUB if CUB.issubset(UB[c])}     396 if len(LUB) == 1: KeyError: dtype([('float0', 'V')]) ```  The issue in the provided example is actually solved by the following modification: ```python def loss(params, prng_key, x, t):     x_hat = odeint(lambda x, t: f(params, x, t, prng_key), x[0], t)     return jnp.mean((x_hat  x[1])**2) ``` However, my application is more complicated than this, where the above solution is nonapplicable. If understand why the problem is raised and why it is solved by the above modification, then I think I can address this issue in my program.  What jax/jaxlib version are you using? jax==0.3.25,  jaxlib==0.3.25+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_  Update on 23/11/2022 It seems impossible to pass a PRNG to `odeint`'s `*args`, so I have changed the approach of my program. Now I sample from probability distributions and pass the random samples (floats) to `*args` instead of sampling inside the `odeint`.",2022-11-23T02:43:46Z,bug P2 (eventual),open,0,3,https://github.com/jax-ml/jax/issues/13369,Why do you need to pass the key to odeint? Is the dynamics function stochastic? If you're looking for ways to integrate SDEs then you might want to take a look at Diffrax. I think the best fix for this issue would be to add a better error message. Otherwise it's probably intended that it doesn't work.,"> Why do you need to pass the key to odeint? Is the dynamics function stochastic? If you're looking for ways to integrate SDEs then you might want to take a look at Diffrax. >  > I think the best fix for this issue would be to add a better error message. Otherwise it's probably intended that it doesn't work. It is still an ODE problem in my actual program, but with L0regularisation for sparsity over the dynamics. The idea appeared recently here and implemented in PyTorch here. It is inspired by this work, where it is originally applied to feedforward networks. I have managed to overcome this and avoid passing PRNGkeys as I mentioned at the end: > It seems impossible to pass a PRNG to odeint's *args, so I have changed the approach of my program. Now I sample from probability distributions and pass the random samples (floats) to *args instead of sampling inside the odeint. Thank you!","Yeah, it would be good to improve this error, especially if`ode` graduates from `experimental`. Glad you found a workaround, and thanks for filing!"
1456,"以下是一个github上的jax下的一个issue, 标题是(odeint keeps throwing unexpected keyword argument 'args' TypeError:)， 内容是 ( Description While trying to use odeint I noticed that the documentation states that ""*args: tuple of additional arguments for `func`, which must be arrays scalars, or (nested) standard Python containers (tuples, lists, dicts, namedtuples, i.e. pytrees) of those types"" but if I follow above instruction from the documentation I get the abovementioned TypeError: ```python def pend(y, t, b, c):     theta, omega = y[0], y[1]     dydt = jnp.array([omega, b*omega  c*jnp.sin(theta)])     return dydt b = 0.25 c = 5.0 t = np.linspace(0, 10, 101) y0 = np.array([np.pi  0.1, 0.0]) sol = odeint(pend, y0, t, args=(b, c)) ``` The above code throws TypeError: odeint() got an unexpected keyword argument 'args' But if the additional parameters a passed as separate arguments the errors goes away, see the code below; ```python def pend(y, t, b, c):     theta, omega = y[0], y[1]     dydt = jnp.array([omega, b*omega  c*jnp.sin(theta)])     return dydt b = 0.25 c = 5.0 t = np.linspace(0, 10, 101) y0 = np.array([np.pi  0.1, 0.0]) sol = odeint(pend, y0, t, b, c) ```  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? CPU  Additional system info: The above code was run on google colab)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,odeint keeps throwing unexpected keyword argument 'args' TypeError:," Description While trying to use odeint I noticed that the documentation states that ""*args: tuple of additional arguments for `func`, which must be arrays scalars, or (nested) standard Python containers (tuples, lists, dicts, namedtuples, i.e. pytrees) of those types"" but if I follow above instruction from the documentation I get the abovementioned TypeError: ```python def pend(y, t, b, c):     theta, omega = y[0], y[1]     dydt = jnp.array([omega, b*omega  c*jnp.sin(theta)])     return dydt b = 0.25 c = 5.0 t = np.linspace(0, 10, 101) y0 = np.array([np.pi  0.1, 0.0]) sol = odeint(pend, y0, t, args=(b, c)) ``` The above code throws TypeError: odeint() got an unexpected keyword argument 'args' But if the additional parameters a passed as separate arguments the errors goes away, see the code below; ```python def pend(y, t, b, c):     theta, omega = y[0], y[1]     dydt = jnp.array([omega, b*omega  c*jnp.sin(theta)])     return dydt b = 0.25 c = 5.0 t = np.linspace(0, 10, 101) y0 = np.array([np.pi  0.1, 0.0]) sol = odeint(pend, y0, t, b, c) ```  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? CPU  Additional system info: The above code was run on google colab",2022-11-22T22:53:37Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/13362," I don't believe this is an issue with jax, but rather related to the workings of `*args` in python. To highlight this, consider the following function.  ``` def f(t, *args):   print(type(args))   for arg in args:     print(arg) ```  The following won't work: ``` f(1., args=(""Hi"", 2.0, ""Hello"")) TypeError: f() got an unexpected keyword argument 'args' ``` But, this will ``` f(1., 'Hi', 2.0, 'Hello')  Hi 2.0 Hello ```",I figured that might be the issue. I guess I am used to the scipy.integrate.odeint. I think this might be one of those gotcha things for someone used to scipy
8091,"以下是一个github上的jax下的一个issue, 标题是(Significant Overhead with nn.scan)， 内容是 ( Description When using NVIDIA GPUs, we are noticing significant performance overheads when we turn on nn.scan. The step times increase by ~15% when nn.scan is used. I am attaching a simple test to reproduce this phenomenon. Is this significant slowdown with nn.scan expected? We observed this problem when running GPT transformer models. These models have 2492 layers. The reproducer code below contains a part of one such layer.  main.py: ``` from __future__ import annotations import argparse import dataclasses import time import attention from typing import Any, Optional, Sequence, Tuple, Union, List import functools import jax from jax import numpy as jnp from jax.ad_checkpoint import checkpoint_name import copy import dataclasses import itertools from absl import logging from functools import partial def test_mha(model_dims = 4096, num_heads = 32, scan = True, num_layers = 3):   if scan:     block_p = attention.MultiheadAttentionRepeated(num_layers, model_dims, num_heads)   else:     block_p = attention.MultiLayerMultiheadAttention(num_layers, model_dims, num_heads)   return block_p def train_step(model, params, inputs, grad):     out, f_vjp = jax.vjp(functools.partial(model.apply), params, inputs)     out = f_vjp(grad)     return out def main():   parser = argparse.ArgumentParser(description='PAX Transformer Unit Test')   parser.add_argument(""batch_size"", dest=""batch_size"", type=int, default=2)   parser.add_argument(""seq_len"", dest=""seq_len"", type=int, default=2048)   parser.add_argument(""num_attn_heads"", dest=""num_attn_heads"", type=int, default=8)   parser.add_argument(""head_dim"", dest=""head_dim"", type=int, default=128)   parser.add_argument(""model_dims"", dest=""model_dims"", type=int, default=4096)   parser.add_argument(""profile"", action=""store_true"")   parser.add_argument(""scan"", dest=""scan"", action=""store_true"")   parser.add_argument(""num_layers"", dest=""num_layers"", type=int, default = 3)   args = parser.parse_args()   print(args)   dtype = jnp.float32   key = jax.random.PRNGKey(0)   key1, key2 = jax.random.split(key, 2)   inputs = jax.random.uniform(key2, (args.batch_size, args.seq_len, args.model_dims), dtype=dtype)   grads = jax.random.uniform(key2, (args.batch_size, args.seq_len, args.model_dims), dtype=jnp.float32)   model = test_mha(model_dims = args.model_dims, num_heads = args.num_attn_heads, scan = args.scan, num_layers = args.num_layers)   jitted_train_step = jax.jit(functools.partial(train_step), static_argnums=[0])   prng_key = jax.random.PRNGKey(seed=123)   prng_key, init_key = jax.random.split(prng_key)   params = model.init(     prng_key,     inputs,   )   if args.profile:     import nvtx, ctypes     libcudart = ctypes.cdll.LoadLibrary('libcudart.so')     for i in range(100):         if i == 9:             libcudart.cudaProfilerStart()         with nvtx.annotate(message=f""step_{i}""):             outputs = jitted_train_step(model, params, inputs, grads)     libcudart.cudaProfilerStop()   else:     for i in range(100):           start = time.time()       outputs = jitted_train_step(model, params, inputs, grads)       end = time.time()       print(f""step {i} time: {(end  start)*1000}"") main() ``` attention.py ``` import jax import jax.numpy as jnp from jax import random import os import numpy as np import math import json from functools import partial import flax from flax import linen as nn from flax.training import train_state, checkpoints import optax def scaled_dot_product(q, k, v, mask=None):     d_k = q.shape[1]     attn_logits = logits = jnp.einsum('BTNH,BSNH>BNTS', q, k)     attn_logits = attn_logits / math.sqrt(d_k)     if mask is not None:         attn_logits = jnp.where(mask == 0, 9e15, attn_logits)     attention = nn.softmax(attn_logits, axis=1)     values = jnp.einsum('BNTS,BSNH>BTNH', attention, v)     return values, attention class MultiheadAttention(nn.Module):     embed_dim : int   Output dimension     num_heads : int   Number of parallel heads (h)     def setup(self):         self.qkv_proj = nn.Dense(3*self.embed_dim,                                  kernel_init=nn.initializers.xavier_uniform(),   Weights with Xavier uniform init                                  bias_init=nn.initializers.zeros   Bias init with zeros                                 )         self.o_proj = nn.Dense(self.embed_dim,                                kernel_init=nn.initializers.xavier_uniform(),                                bias_init=nn.initializers.zeros)     .compact     def __call__(self, x, mask=None):          Determine value outputs         qkv = self.qkv_proj(x)         qkv = qkv.reshape(x.shape[0], x.shape[1], self.num_heads, 1)         q, k, v = jnp.array_split(qkv, 3, axis=1)         values, attention = scaled_dot_product(q, k, v, mask=mask)         values = values.reshape(x.shape[0], x.shape[1], self.embed_dim)         o = self.o_proj(values)         return o, None class MultiLayerMultiheadAttention(nn.Module):     num_layers: int     embed_dim : int   Output dimension     num_heads : int   Number of parallel heads (h)     def setup(self):         self.layers = [MultiheadAttention(self.embed_dim, self.num_heads) for i in range(self.num_layers)]     def __call__(self, x):         for i, lyr in enumerate(self.layers):             x, _ = lyr(x)         return x class MultiheadAttentionRepeated(nn.Module):     x_times : int   Number of times to Repeat MHA     embed_dim : int   Output dimension     num_heads : int   Number of parallel heads (h)     .compact     def __call__(self, x):         block_repeated = nn.scan(             MultiheadAttention,             variable_broadcast=""params"",             split_rngs={""params"": False},             length=self.x_times,             in_axes=0,             out_axes = 0         )         return block_repeated(self.embed_dim, self.num_heads)(x, None)[0] if __name__=='__main__':     main_rng = random.PRNGKey(42)     main_rng, x_rng = random.split(main_rng)     x = random.normal(x_rng, (8, 2048, 4096))      Create attention     mh_attn = MultiLayerMultiheadAttention(num_layers = 3, embed_dim=4096, num_heads=32)      Initialize parameters of attention with random key and inputs     main_rng, init_rng = random.split(main_rng)     params = mh_attn.init(init_rng, x)['params']      Apply attention with parameters on the inputs     out = mh_attn.apply({'params': params}, x)     print('Out', out.shape)     mh_attn_r = MultiheadAttentionRepeated(3, 4096, 32)     main_rng, init_rng = random.split(main_rng)     params = mh_attn_r.init(init_rng, x)['params']      Apply attention with parameters on the inputs     out = mh_attn_r.apply({'params': params}, x)     print('Out Repeated', out[0].shape) ``` requirements.txt ``` pyglove==0.1.0a0 protobuf==3.15 abslpy==0.12.0 tensorflow===2.8.2 tensorflowtext==2.8.2 numpy==1.21.1 flax==0.5.3 seqionightly==0.0.8.dev20220809 tensorstore==0.1.22 fiddleconfig==0.2.0 optax==0.1.3 optaxshampoo==0.0.5 jaxbitemperedloss==0.0.2 einops==0.4.1 jsonlines==3.1.0 pysimdjson zstandard==0.18.0 dllogger+https://github.com/NVIDIA/dlloggeregg=dllogger ``` Observed output: `python main.py`: ``` Namespace(batch_size=2, head_dim=128, model_dims=4096, num_attn_heads=8, num_layers=3, profile=False, scan=False, seq_len=2048) step 0 time: 64.68343734741211 step 1 time: 64.04900550842285 step 2 time: 64.49079513549805 step 3 time: 64.07880783081055 step 4 time: 63.95602226257324 ... ``` `python main.py scan`: ``` Namespace(batch_size=2, head_dim=128, model_dims=4096, num_attn_heads=8, num_layers=3, profile=False, scan=True, seq_len=2048) step 0 time: 74.74923133850098 step 1 time: 73.94671440124512 step 2 time: 74.51796531677246 step 3 time: 74.53227043151855 step 4 time: 74.3551254272461 ... ```  What jax/jaxlib version are you using? jax v0.3.24, jaxlib v0.3.24  Which accelerator(s) are you using? GPU  Additional system info Python v3.8.10  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Significant Overhead with nn.scan," Description When using NVIDIA GPUs, we are noticing significant performance overheads when we turn on nn.scan. The step times increase by ~15% when nn.scan is used. I am attaching a simple test to reproduce this phenomenon. Is this significant slowdown with nn.scan expected? We observed this problem when running GPT transformer models. These models have 2492 layers. The reproducer code below contains a part of one such layer.  main.py: ``` from __future__ import annotations import argparse import dataclasses import time import attention from typing import Any, Optional, Sequence, Tuple, Union, List import functools import jax from jax import numpy as jnp from jax.ad_checkpoint import checkpoint_name import copy import dataclasses import itertools from absl import logging from functools import partial def test_mha(model_dims = 4096, num_heads = 32, scan = True, num_layers = 3):   if scan:     block_p = attention.MultiheadAttentionRepeated(num_layers, model_dims, num_heads)   else:     block_p = attention.MultiLayerMultiheadAttention(num_layers, model_dims, num_heads)   return block_p def train_step(model, params, inputs, grad):     out, f_vjp = jax.vjp(functools.partial(model.apply), params, inputs)     out = f_vjp(grad)     return out def main():   parser = argparse.ArgumentParser(description='PAX Transformer Unit Test')   parser.add_argument(""batch_size"", dest=""batch_size"", type=int, default=2)   parser.add_argument(""seq_len"", dest=""seq_len"", type=int, default=2048)   parser.add_argument(""num_attn_heads"", dest=""num_attn_heads"", type=int, default=8)   parser.add_argument(""head_dim"", dest=""head_dim"", type=int, default=128)   parser.add_argument(""model_dims"", dest=""model_dims"", type=int, default=4096)   parser.add_argument(""profile"", action=""store_true"")   parser.add_argument(""scan"", dest=""scan"", action=""store_true"")   parser.add_argument(""num_layers"", dest=""num_layers"", type=int, default = 3)   args = parser.parse_args()   print(args)   dtype = jnp.float32   key = jax.random.PRNGKey(0)   key1, key2 = jax.random.split(key, 2)   inputs = jax.random.uniform(key2, (args.batch_size, args.seq_len, args.model_dims), dtype=dtype)   grads = jax.random.uniform(key2, (args.batch_size, args.seq_len, args.model_dims), dtype=jnp.float32)   model = test_mha(model_dims = args.model_dims, num_heads = args.num_attn_heads, scan = args.scan, num_layers = args.num_layers)   jitted_train_step = jax.jit(functools.partial(train_step), static_argnums=[0])   prng_key = jax.random.PRNGKey(seed=123)   prng_key, init_key = jax.random.split(prng_key)   params = model.init(     prng_key,     inputs,   )   if args.profile:     import nvtx, ctypes     libcudart = ctypes.cdll.LoadLibrary('libcudart.so')     for i in range(100):         if i == 9:             libcudart.cudaProfilerStart()         with nvtx.annotate(message=f""step_{i}""):             outputs = jitted_train_step(model, params, inputs, grads)     libcudart.cudaProfilerStop()   else:     for i in range(100):           start = time.time()       outputs = jitted_train_step(model, params, inputs, grads)       end = time.time()       print(f""step {i} time: {(end  start)*1000}"") main() ``` attention.py ``` import jax import jax.numpy as jnp from jax import random import os import numpy as np import math import json from functools import partial import flax from flax import linen as nn from flax.training import train_state, checkpoints import optax def scaled_dot_product(q, k, v, mask=None):     d_k = q.shape[1]     attn_logits = logits = jnp.einsum('BTNH,BSNH>BNTS', q, k)     attn_logits = attn_logits / math.sqrt(d_k)     if mask is not None:         attn_logits = jnp.where(mask == 0, 9e15, attn_logits)     attention = nn.softmax(attn_logits, axis=1)     values = jnp.einsum('BNTS,BSNH>BTNH', attention, v)     return values, attention class MultiheadAttention(nn.Module):     embed_dim : int   Output dimension     num_heads : int   Number of parallel heads (h)     def setup(self):         self.qkv_proj = nn.Dense(3*self.embed_dim,                                  kernel_init=nn.initializers.xavier_uniform(),   Weights with Xavier uniform init                                  bias_init=nn.initializers.zeros   Bias init with zeros                                 )         self.o_proj = nn.Dense(self.embed_dim,                                kernel_init=nn.initializers.xavier_uniform(),                                bias_init=nn.initializers.zeros)     .compact     def __call__(self, x, mask=None):          Determine value outputs         qkv = self.qkv_proj(x)         qkv = qkv.reshape(x.shape[0], x.shape[1], self.num_heads, 1)         q, k, v = jnp.array_split(qkv, 3, axis=1)         values, attention = scaled_dot_product(q, k, v, mask=mask)         values = values.reshape(x.shape[0], x.shape[1], self.embed_dim)         o = self.o_proj(values)         return o, None class MultiLayerMultiheadAttention(nn.Module):     num_layers: int     embed_dim : int   Output dimension     num_heads : int   Number of parallel heads (h)     def setup(self):         self.layers = [MultiheadAttention(self.embed_dim, self.num_heads) for i in range(self.num_layers)]     def __call__(self, x):         for i, lyr in enumerate(self.layers):             x, _ = lyr(x)         return x class MultiheadAttentionRepeated(nn.Module):     x_times : int   Number of times to Repeat MHA     embed_dim : int   Output dimension     num_heads : int   Number of parallel heads (h)     .compact     def __call__(self, x):         block_repeated = nn.scan(             MultiheadAttention,             variable_broadcast=""params"",             split_rngs={""params"": False},             length=self.x_times,             in_axes=0,             out_axes = 0         )         return block_repeated(self.embed_dim, self.num_heads)(x, None)[0] if __name__=='__main__':     main_rng = random.PRNGKey(42)     main_rng, x_rng = random.split(main_rng)     x = random.normal(x_rng, (8, 2048, 4096))      Create attention     mh_attn = MultiLayerMultiheadAttention(num_layers = 3, embed_dim=4096, num_heads=32)      Initialize parameters of attention with random key and inputs     main_rng, init_rng = random.split(main_rng)     params = mh_attn.init(init_rng, x)['params']      Apply attention with parameters on the inputs     out = mh_attn.apply({'params': params}, x)     print('Out', out.shape)     mh_attn_r = MultiheadAttentionRepeated(3, 4096, 32)     main_rng, init_rng = random.split(main_rng)     params = mh_attn_r.init(init_rng, x)['params']      Apply attention with parameters on the inputs     out = mh_attn_r.apply({'params': params}, x)     print('Out Repeated', out[0].shape) ``` requirements.txt ``` pyglove==0.1.0a0 protobuf==3.15 abslpy==0.12.0 tensorflow===2.8.2 tensorflowtext==2.8.2 numpy==1.21.1 flax==0.5.3 seqionightly==0.0.8.dev20220809 tensorstore==0.1.22 fiddleconfig==0.2.0 optax==0.1.3 optaxshampoo==0.0.5 jaxbitemperedloss==0.0.2 einops==0.4.1 jsonlines==3.1.0 pysimdjson zstandard==0.18.0 dllogger+https://github.com/NVIDIA/dlloggeregg=dllogger ``` Observed output: `python main.py`: ``` Namespace(batch_size=2, head_dim=128, model_dims=4096, num_attn_heads=8, num_layers=3, profile=False, scan=False, seq_len=2048) step 0 time: 64.68343734741211 step 1 time: 64.04900550842285 step 2 time: 64.49079513549805 step 3 time: 64.07880783081055 step 4 time: 63.95602226257324 ... ``` `python main.py scan`: ``` Namespace(batch_size=2, head_dim=128, model_dims=4096, num_attn_heads=8, num_layers=3, profile=False, scan=True, seq_len=2048) step 0 time: 74.74923133850098 step 1 time: 73.94671440124512 step 2 time: 74.51796531677246 step 3 time: 74.53227043151855 step 4 time: 74.3551254272461 ... ```  What jax/jaxlib version are you using? jax v0.3.24, jaxlib v0.3.24  Which accelerator(s) are you using? GPU  Additional system info Python v3.8.10  NVIDIA GPU info ``` ++  ++ ```",2022-11-22T16:28:46Z,bug XLA NVIDIA GPU,open,0,8,https://github.com/jax-ml/jax/issues/13356,Our observation is that nn.scan introduces while loops in forward pass (and backward pass) which introduces extra pointwise (fusion) kernels for slice and update_slice operations. This adds about  ~14 ms overhead in forward pass. Attaching HLO logs for  scantrue and scanfalse. module_0066.jit__unnamed_wrapped_function_.21.sm_8.6_gpu_after_optimizations.txt module_0085.jit__unnamed_wrapped_function_.36.sm_8.6_gpu_after_optimizations.txt Better fusion of these kernels could improve performance with scan. ,"Another factor observed to contribute to the slow down using scan is a large amount of D2D copies before loop is run. These D2Ds are making copies of the same broadcasted constant and being fed into dynamicupdateslice ops in the loop body. These D2Ds accumulated together could slow down an iteration by 5% to 20% depending on batch size and number of layers. Attaching the HLO for one of the cases. module_0280.pjit_train_step.sm_8.6_gpu_after_optimizations.txt An example copy of this sort is ""copy.525"" in the above graph. One possible fix to reduce the slowdown is to replace copies of the same broadcasted constant with memset.", You already talked about those copies. Any idea why they are needed and how we could get rid of them?,"This is tracked internally, but it's not trivial to fix.", This is related to the issue we have been seeing in PAXML.,"This issue is also observable when following the tutorial at https://uvadlcnotebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/single_gpu_transformer.html . With batch size 1 and the following config: ``` model_config = ConfigDict(     dict(         hidden_size=1024,         dropout_rate=0.1,         mlp_expansion=3,         num_layers=12,         head_dim=128,         causal_mask=True,         max_seq_len=data_config.seq_len,         vocab_size=data_config.vocab_size,         num_outputs=data_config.vocab_size,         dtype=jnp.bfloat16,         softmax_dtype=jnp.float32,         scan_layers=True,         remat=(""MLP"", ""Attn""),     ) ) ``` on CPU, training for 60 iterations, I get 1.1 iterations/sec. If I just change `scan_layers` to `False`, I get 2.2 iterations/sec, twice as fast (but it takes longer to start). Passing `unroll=True` to the nn.scan gets it to 1.5 iterations/sec. The relevant code: ```         if self.config.scan_layers:             block = block_fn(name=""block"")             x, _ = nn.scan(                 lambda module, carry, _: (module(carry), None),                 variable_axes={""params"": 0},                 split_rngs={""params"": True, ""dropout"": True},                 length=self.config.num_layers,             )(block, x, ())         else:             for l_idx in range(self.config.num_layers):                 x = block_fn(name=f""block_{l_idx}"")(x) ```","Overall it's indeed known that it's better to always unroll loops in XLA for performance (yes we know it's not perfect). Extra copies are mentioned above, but I think most of those were fixed. Another issue is that whileloop introduces forcesynchronization at every loop iteration, which makes further optimizations difficult.","> Another issue is that whileloop introduces forcesynchronization at every loop iteration, which makes further optimizations difficult. The loop here has a fixed trip count, so that issue don't apply in the latest example. There is some work being done to continue to optimize this. Some of that works is still under some XLA flags documented here: https://github.com/NVIDIA/JAXToolbox/blob/main/rosetta/docs/GPU_performance.md You can try those 2 XLA flags and hopefully, they will help you for loops. xla_gpu_enable_custom_fusions=true xla_gpu_enable_address_computation_fusion=true"
310,"以下是一个github上的jax下的一个issue, 标题是(Transition default use of XlaCallModule to StableHLO (version 2).)， 内容是 (Transition default use of XlaCallModule to StableHLO (version 2).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Transition default use of XlaCallModule to StableHLO (version 2).,Transition default use of XlaCallModule to StableHLO (version 2).,2022-11-22T09:43:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13349
2152,"以下是一个github上的jax下的一个issue, 标题是(Numerical error in jax.vjp computation)， 内容是 ( Description Hi,  I am trying to produce the vectorJacobian products of a linear function but the numerical error can not be fully eliminated.  The following code can reproduce the numerical error: ```python import numpy as np import jax import jax.numpy as jnp import torch np.random.seed(0) w = np.random.randn(5, 3).astype(np.float32) x = np.random.randn(3, 2).astype(np.float32)  JAX vjp results w_jax = jnp.array(w) x_jax = jnp.array(x) def f(z):     return jnp.matmul(w_jax, z)   a linear function w.r.t. x, whose Jacobian should be w value, f_vjp = jax.vjp(f, x_jax) vjp_by_jax = f_vjp(value)[0] vjp_manual = jnp.matmul(w_jax.T, value) print(jnp.mean(jnp.abs(vjp_by_jax  vjp_manual)))   Should be zero  Pytorch vjp results w_torch = torch.from_numpy(w) x_torch = torch.from_numpy(x) def g(z):     return torch.matmul(w_torch, z)   a linear function w.r.t. x, whose Jacobian should be w value = g(x_torch) vjp_by_torch = torch.autograd.functional.vjp(g, x_torch, value)[1] vjp_manual = torch.matmul(w_torch.T, value) print(torch.mean(torch.abs(vjp_by_torch  vjp_manual)))   Should be zero ``` The code prints the difference between the VJP from JAX and the VJP computed manually. The expected output is zero. Running the code produces the following results, where a severe numerical error is encountered: > 0.0032820504 > tensor(0.) A recent issue CC(Numerical error in Jacobian computation) suggests that the error is caused by internal float16 conversion and `NVIDIA_TF32_OVERRIDE=0` can avoid it. But with `NVIDIA_TF32_OVERRIDE=0` the above code still produce a (though much smaller) numerical error: >1.5894572e07 >tensor(0.) As a reference, an equivalent implementation in Pytorch can precisely produce zero.  What jax/jaxlib version are you using? jax==0.3.24, jaxlib==0.3.24+cuda11.cudnn805  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10; Ubuntu 20.04.3 LTS  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Numerical error in jax.vjp computation," Description Hi,  I am trying to produce the vectorJacobian products of a linear function but the numerical error can not be fully eliminated.  The following code can reproduce the numerical error: ```python import numpy as np import jax import jax.numpy as jnp import torch np.random.seed(0) w = np.random.randn(5, 3).astype(np.float32) x = np.random.randn(3, 2).astype(np.float32)  JAX vjp results w_jax = jnp.array(w) x_jax = jnp.array(x) def f(z):     return jnp.matmul(w_jax, z)   a linear function w.r.t. x, whose Jacobian should be w value, f_vjp = jax.vjp(f, x_jax) vjp_by_jax = f_vjp(value)[0] vjp_manual = jnp.matmul(w_jax.T, value) print(jnp.mean(jnp.abs(vjp_by_jax  vjp_manual)))   Should be zero  Pytorch vjp results w_torch = torch.from_numpy(w) x_torch = torch.from_numpy(x) def g(z):     return torch.matmul(w_torch, z)   a linear function w.r.t. x, whose Jacobian should be w value = g(x_torch) vjp_by_torch = torch.autograd.functional.vjp(g, x_torch, value)[1] vjp_manual = torch.matmul(w_torch.T, value) print(torch.mean(torch.abs(vjp_by_torch  vjp_manual)))   Should be zero ``` The code prints the difference between the VJP from JAX and the VJP computed manually. The expected output is zero. Running the code produces the following results, where a severe numerical error is encountered: > 0.0032820504 > tensor(0.) A recent issue CC(Numerical error in Jacobian computation) suggests that the error is caused by internal float16 conversion and `NVIDIA_TF32_OVERRIDE=0` can avoid it. But with `NVIDIA_TF32_OVERRIDE=0` the above code still produce a (though much smaller) numerical error: >1.5894572e07 >tensor(0.) As a reference, an equivalent implementation in Pytorch can precisely produce zero.  What jax/jaxlib version are you using? jax==0.3.24, jaxlib==0.3.24+cuda11.cudnn805  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10; Ubuntu 20.04.3 LTS  NVIDIA GPU info ``` ++  ++ ```",2022-11-22T08:19:05Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/13348,"1e7 is pretty much at the limit of precision for `float32`, so I would treat that as working as intended. If you want to find out what JAX is really doing in the VJP, you can wrap it in `jax.make_jaxpr` and print the result. If you inspect your case, you'll see that the VJP gets expanded as `jnp.tensordot(value, w_jax, axes=([0], [0])).T`. That is, a matrix multiply with a lhs transpose fused in, followed by another transposition. I suspect that this equivalent, but slightly different reformulation might cause e.g. cuBLAS to use different kernels and that would explain the numerical differences. When I tried your example on CPU it gave exactly the same results. I'm closing this, since I don't think this is a bug, but feel free to reopen if you feel otherwise!"
1010,"以下是一个github上的jax下的一个issue, 标题是(What should I expect from compilation_cache?)， 内容是 ( Description I've been trying out the experimental `compilation_cache` on TPUs, which is great! Unfortunately there isn't documentation for this yet.  I'm using it on v38 TPUVM, with haiku & pjit, running on an IPython notebook. While I find some mild speedup for initializing params (38>29.5s), the inference function I have (which is pjitted and involves a `scan` over `apply`s) takes 27s to compile (same for each rerun) w/ cache on, whereas it took ~25s without the cache.  I know this isn't a minimal example, but it would still be great to have a high level understanding of how it works / why I might not see benefits.  What jax/jaxlib version are you using? 0.3.16  Which accelerator(s) are you using? v38 TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,What should I expect from compilation_cache?," Description I've been trying out the experimental `compilation_cache` on TPUs, which is great! Unfortunately there isn't documentation for this yet.  I'm using it on v38 TPUVM, with haiku & pjit, running on an IPython notebook. While I find some mild speedup for initializing params (38>29.5s), the inference function I have (which is pjitted and involves a `scan` over `apply`s) takes 27s to compile (same for each rerun) w/ cache on, whereas it took ~25s without the cache.  I know this isn't a minimal example, but it would still be great to have a high level understanding of how it works / why I might not see benefits.  What jax/jaxlib version are you using? 0.3.16  Which accelerator(s) are you using? v38 TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2022-11-19T09:49:09Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/13325,"There is a builtin inmemory compilation cache for pjit, while`compilation_cache` in jax/experimental is a persistent compilation cache. If you get recompilation for inmemory pjit cache, using the persistent compilation cache isn't going to help you because you will still get cache miss. The way inmemory compilation cache works is that if you do `pf = pjit(f); y = pf(x); y = pf(x)`, the second `pf(x)` would not recompile even without using `compilation_cache`, because the pjit cache sees the the argument `x` shape and dtype are the same so you get cache hit. Based on what you are describing "" takes 27s to compile (same for each rerun)"", you are probably getting compilation miss in inmemory cache and seeing recompilation for every invocation of `pf(x)`. You can confirm this by setting `JAX_LOG_COMPILES=1` `JAX_LOG_COMPILES=1 python your_program.py` If you see recompilation logs, then you need to figure out whether the arguments you pass to your pjitted function have different shapes or dtypes on each invocation. This could happen for flax programs if the `vars` you pass to `apply` changes shapes and have additional fields for each loop. Say you have ``` for i in range(num_steps):   y, vars = m.apply(vars, inputs, mutable=True) ``` You want `print(jax.tree_map(lambda x: x.shape, vars))` after each iteration, and see the vars shape stay the same. You could do the same checks for `inputs`."," Thanks for the response! Sorry I wasn't clear — when I said ""for each rerun"", I meant between restarts of the entire program, not between calls during the same run. It definitely doesn't recompile between calls! ", could perhaps help with persistent `compilation_cache`,"First I would confirm that the cache is actually on and getting cache hits. You can use jax's log output to confirm this. You should first see something like: `WARNING:jax.experimental.compilation_cache.compilation_cache:Initialized persistent compilation cache at /tmp/jax_cache` This confirms the cache is on. Add the following near the top of your code to turn on more logging: ```python import logging logging.getLogger(""jax"").setLevel(logging.INFO) ``` Then you should see cache misses e.g.: `INFO:jax.experimental.compilation_cache.compilation_cache:Writing jit__lambda_ to persistent compilation cache with key 7dcb1f130a290f26c1a2b138e3e6ba52396f4543e2b559e5066587e41009ec84.` And/or cache hits e.g.: `INFO:jax._src.dispatch:Persistent compilation cache hit for 'jit__lambda_'` Assuming you are indeed getting cache hits and it's still slow, it may be taking a long time to trace your functions (although I'd be surprised tracing is so slow compared to compilation). Bumping the log level down to `logging.DEBUG` should give you finergrained timings including trace time, e.g.: `DEBUG:jax._src.dispatch:Finished tracing + transforming jit() in 0.0015163421630859375 sec` (You'll get a lot of other output too, you may wanna redirect to a file so you can grep) Try the above and we can go from there? This is also a good nudge for writing some documentation, I've been meaning to do that :) This is helping me think through what we should include, thanks for reporting!"," Thanks for the detailed response! It is indeed the case that it's a cache miss (it's recompiling between program runs). The function in question shows up as `pjit(apply_fn)` in the logs: First run: log_gen_k10.txt Second run (with additional logs from unrelated setup in the beginning, sorry) log_gen_k10_cached.txt For context, the fn is returned from smth like this: ``` def get_fn(settings):     def fn(x):        return hk.scan using x (with hk.next_rng_key)     hk_fn = pjit(hk.transform(fn).apply)     def wrapper(x):         return hk_fn(x)     return wrapper ``` And in my notebook I'm using  ``` gen_fn = module.get_fn(settings) {use gen_fn} ``` Any insight into how the cache is hashing this, so that the same code is producing a different key?","Thanks for including the debug logging! I see the following: log_gen_k10.txt: ``` 04:39:44,693 absl DEBUG Finished tracing + transforming pjit(apply_fn) in 0.6419768333435059 sec 04:39:50,728 absl DEBUG get_cache_key hash of serialized computation: dc1948059c0f27435f699953035fc8d87729bbaa3ccbcae43ba37ae1895180aa ``` log_gen_k10_cached.txt: ``` 04:41:54,562 absl DEBUG Finished tracing + transforming pjit(apply_fn) in 0.642808198928833 sec 04:42:00,259 absl DEBUG get_cache_key hash of serialized computation: e5d711c8ae9d6259236a7815d88995ef362f300831ddfa72f4ccfe063f4b6693 ``` Note the different hashes of the serialized computations. This means that somehow you're generating different HLO computations across processes (i.e. different XLA programs generated by jax). I'm guessing this is due to a known issue where HLO programs will have slightly different function/module names across processes, due to how XLA uniquifies names (e.g. `program.1` vs `program.2`). The best way to verify this is to dump the HLO. You can do this by setting the env var `XLA_FLAGS=xla_dump_to=/some/path` (the path can either be a local path on the TPU VM, or a GCS bucket path `gs://...`). You should then find the `before_optimizations` file associated with the `pjit(apply_fn)` (might have a slightly different filename). Do this twice across processes and compare, and/or share the HLO here and I can take a look. Unfortunately, there isn't really a fix for this at the moment. This can happen if different processes do slightly different things before getting to the cached function, so if you keep running it, you might eventually start getting cache hits. I've been meaning to add an option to make the cache try to strip out ""unimportant"" information like names from the HLO to actually fix this. It's also possible something else is happening. Getting the HLO is the only way to confirm for sure."
944,"以下是一个github上的jax下的一个issue, 标题是(TPU not found on VM)， 内容是 ( Description Hello I'm running a TPU v38 VM on Google. On the VM I installed jax with `pip install ""jax[tpu]==0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html`. Unfortunately, I'm getting the message `No GPU/TPU found, falling back to CPU.` when issuing `jax.device_count()`. The same holds for `pip install jax==0.2.12`. Only when I'm using `pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html` (newest jax version), it works. But I need jax version 0.2.12 or 0.2.16. How can I get it running with these versions?  What jax/jaxlib version are you using? jax 0.2.16  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,TPU not found on VM," Description Hello I'm running a TPU v38 VM on Google. On the VM I installed jax with `pip install ""jax[tpu]==0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html`. Unfortunately, I'm getting the message `No GPU/TPU found, falling back to CPU.` when issuing `jax.device_count()`. The same holds for `pip install jax==0.2.12`. Only when I'm using `pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html` (newest jax version), it works. But I need jax version 0.2.12 or 0.2.16. How can I get it running with these versions?  What jax/jaxlib version are you using? jax 0.2.16  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2022-11-19T00:41:53Z,bug,open,3,12,https://github.com/jax-ml/jax/issues/13321, do you know?,"Can you say more about why you need 0.2.12 or 0.2.16? These are very old versions. I'll dig into why 0.2.16 doesn't work, but it would definitely be simpler to use a newer version :)","That didn't take as long as I expected :) For 0.2.16, you can workaround by setting the env var `TPU_LIBRARY_PATH=/home/skyewm/.local/lib/python3.8/sitepackages/libtpu/libtpu.so`. (You may have to adjust that path depending on where libtpunightly was installed; `locate libtpu.so` may be helpful) The underlying problem is that this version of jax still expected libtpu.so to be automatically installed in the VM image (https://github.com/google/jax/blob/jaxv0.2.16/jax/_src/cloud_tpu_init.pyL104), which the TPU VM base image no longer does.","Hey , does this work for you? If so I'll close the issue.",Thank you so much for your help. Highly appreciated! I will test it this week and let you know.,"Thanks for letting us know, no rush!","I tested it now. I created a new TPU VM and installed jax using `pip install ""jax[tpu]==0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html`. Afterwards, I set `echo TPU_LIBRARY_PATH=/home/myUsername/.local/lib/python3.8/sitepackages/libtpu/libtpu.so`. When I was running now `python3; import jax; jax.device_count()`, I got the following response: ``` D1205 22:55:15.241566995   11179 ev_posix.cc:173]            Using polling engine: epollex D1205 22:55:15.241638035   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""grpclb"" D1205 22:55:15.241648712   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""priority_experimental"" D1205 22:55:15.241653540   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""weighted_target_experimental"" D1205 22:55:15.241656624   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""pick_first"" D1205 22:55:15.241659451   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""round_robin"" D1205 22:55:15.241664217   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""ring_hash_experimental"" D1205 22:55:15.241674608   11179 dns_resolver_ares.cc:497]   Using ares dns resolver D1205 22:55:15.241704644   11179 certificate_provider_registry.cc:33] registering certificate provider factory for ""file_watcher"" D1205 22:55:15.241714378   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""cds_experimental"" D1205 22:55:15.241719498   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_impl_experimental"" D1205 22:55:15.241726682   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_resolver_experimental"" D1205 22:55:15.241732858   11179 lb_policy_registry.cc:42]   registering LB policy factory for ""xds_cluster_manager_experimental"" I1205 22:55:15.241853753   11179 server_builder.cc:349]      Synchronous server. Num CQs: 1, Min pollers: 1, Max Pollers: 2, CQ timeout (msec): 10000 I1205 22:55:15.241936900   11179 socket_utils_common_posix.cc:353] TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter I1205 22:55:15.278600008   11881 subchannel.cc:1065]         New connected subchannel at 0x47f3080 for subchannel 0x492a000 8 ``` So I think this looks good. But when I then exited python and issued again `python3; import jax;`, I got the error: ``` WARNING: Logging before InitGoogle() is written to STDERR I1205 22:59:35.682652   15381 tpu_initializer_helper.cc:66] libtpu.so already in used by another process. Not attempting to load libtpu.so in this process. ``` Edit: When I run `rm rf /tmp/libtpu_lockfile /tmp/tpu_logs`, the error is gone but then I get the `WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` when running `jax.device_count()`","Can you try rerunning and sharing `/tmp/tpu_logs/tpu_driver.INFO`? That should give more information why the TPU runtime isn't able to come up. (BTW, we've improved TPU runtime error reporting substantially since  0.2.16)","Also, are you able to share why you're unable to upgrade jax? The real fix here may be addressing that issue :)","Please see below the content of tpu_driver.INFO. I'm not able to upgrade jax because I want to use jax for finetuning GPTJ using the following tutorial: https://github.com/kingoflolz/meshtransformerjax/blob/master/howto_finetune.md With newer Jax versions this does not work. ``` Log file created at: 2022/12/09 15:40:15 Running on machine: t1vnee970b5aw0 Binary: Built on Jun 10 2021 11:50:32 (1623351002) Binary: Built at cloudtpusruntimereleasetool.prod.google.com:/google/src/cloud/buildrabbitusername/buildrabbitclient/g3 Binary: Built for gcc4.X.Ycrosstoolv18llvmgrtev4k8 Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg I1209 15:40:15.567064   13256 b295d63588a.cc:758] Linux version 5.13.01027gcp (builddamd64062) (gcc (Ubuntu 9.4.01ubuntu1~20.04.1) 9.4.0, GNU ld (GNU Binutils for Ubuntu) 2.34) CC(Fix the bug in classifier example, batching_test and README)~20.04.1Ubuntu SMP Thu May 26 10:53:08 UTC 2022 I1209 15:40:15.567366   13256 b295d63588a.cc:825] Process id 13256 I1209 15:40:15.567378   13256 b295d63588a.cc:830] Current working directory /home/myUsername I1209 15:40:15.567379   13256 b295d63588a.cc:832] Current timezone is UTC (currently UTC +00:00) I1209 15:40:15.567382   13256 b295d63588a.cc:836] Built on Jun 10 2021 11:50:32 (1623351002) I1209 15:40:15.567382   13256 b295d63588a.cc:837]  at cloudtpusruntimereleasetool.prod.google.com:/google/src/cloud/buildrabbitusername/buildrabbitclient/g3 I1209 15:40:15.567383   13256 b295d63588a.cc:838]  as //learning/45eac/tfrc/executor:_libtpu.so I1209 15:40:15.567384   13256 b295d63588a.cc:839]  for gcc4.X.Ycrosstoolv18llvmgrtev4k8 I1209 15:40:15.567385   13256 b295d63588a.cc:842]  from changelist 378699432 with baseline 378699432 in a mint client based on __ar56t/g3 I1209 15:40:15.567386   13256 b295d63588a.cc:846] Build label: libtpu_runtime_20210610_RC00 I1209 15:40:15.567387   13256 b295d63588a.cc:848] Build tool: Bazel, release r4rca2021.06.046 (mainline ) I1209 15:40:15.567388   13256 b295d63588a.cc:849] Build target: I1209 15:40:15.567389   13256 b295d63588a.cc:861] Command line arguments: I1209 15:40:15.567390   13256 b295d63588a.cc:863] argv[0]: './tpu_driver' I1209 15:40:15.567393   13256 b295d63588a.cc:863] argv[1]: 'minloglevel=0' I1209 15:40:15.567394   13256 b295d63588a.cc:863] argv[2]: 'stderrthreshold=3' I1209 15:40:15.567395   13256 b295d63588a.cc:863] argv[3]: 'v=0' I1209 15:40:15.567396   13256 b295d63588a.cc:863] argv[4]: 'vmodule=' I1209 15:40:15.567397   13256 b295d63588a.cc:863] argv[5]: 'log_dir=/tmp/tpu_logs' I1209 15:40:15.567398   13256 b295d63588a.cc:863] argv[6]: 'max_log_size=1024' I1209 15:40:15.567654   13256 builtin.cc:16] 7edfa70aa11b3ffd6f: /memfile/routing_cache_files I1209 15:40:15.567666   13256 builtin.cc:16] 7edfa70aa11b3ffd6f: /memfile/tpu_chip_config_memfile_default I1209 15:40:15.567670   13256 builtin.cc:16] 7edfa70aa11b3ffd6f: /memfile/tpu_chip_config_memfile_inference I1209 15:40:15.567675   13256 builtin.cc:16] 7edfa70aa11b3ffd6f: /memfile/tpu_chip_parts_memfile I1209 15:40:15.567717   13256 coredump_hook.cc:666] Remote crash gathering hook installed. I1209 15:40:15.568024   13256 prodhostname_userspace_monitor_impl.cc:188] Not running under a Borglet, disabling ProdHostname userspace monitoring. W1209 15:40:15.568059   13256 tf_tpu_flags.cc:51] Configuring 2a886c8 Platform flags with tensorflow flags. The original flag values are ignored. W1209 15:40:15.568136   13256 tf_tpu_flags.cc:78] 2a886c8_chips_per_host_bounds overridden to: {x = 2, y = 2, z = 1} W1209 15:40:15.568150   13256 tf_tpu_flags.cc:85] 2a886c8_wrap overridden to: {x = false, y = false, z = false} W1209 15:40:15.568155   13256 tf_tpu_flags.cc:94] 2a886c8_host_bounds overridden to: {x = 1, y = 1, z = 1} W1209 15:40:15.568161   13256 tf_tpu_flags.cc:98] 2a886c8_missing_chip_count overridden to: 0 I1209 15:40:15.568209   13256 logger.cc:274] Enabling threaded logging for severity WARNING I1209 15:40:23.491515   13256 device_util.cc:61] Found 4 6bf72d463e chips. I1209 15:40:23.491567   13256 tpu_version_flag.cc:50] Using autodetected TPU version 6bf72d463e I1209 15:40:23.492314   13256 device_util.cc:61] Found 4 6bf72d463e chips. I1209 15:40:23.492992   13256 device_util.cc:61] Found 4 6bf72d463e chips. I1209 15:40:23.493635   13256 device_util.cc:61] Found 4 6bf72d463e chips. I1209 15:40:23.493641   13256 flags_util.cc:215] Using default chip configuration. I1209 15:40:23.493926   13256 flags_util.cc:330] Picked unused port 56020 as a555f10594 port. I1209 15:40:23.494705   13256 device_util.cc:61] Found 4 6bf72d463e chips. I1209 15:40:23.494713   13256 2a886c8_platform.cc:402] Initializing 2a886c8Platform hardware implementation. I1209 15:40:23.495307   13256 device_util.cc:61] Found 4 6bf72d463e chips. I1209 15:40:23.495957   13256 device_util.cc:61] Found 4 6bf72d463e chips. W1209 15:40:23.499049   13256 device_scanner.cc:210] failures while refreshing ba16c7433 device info from files: FAILED_PRECONDITION: Failed to read file [type.googleapis.com/util.ErrorSpacePayload='util::PosixErrorSpace::Bad file descriptor'] FAILED_PRECONDITION: Failed to read file [type.googleapis.com/util.ErrorSpacePayload='util::PosixErrorSpace::Bad file descriptor'] FAILED_PRECONDITION: Failed to read file [type.googleapis.com/util.ErrorSpacePayload='util::PosixErrorSpace::Bad file descriptor'] ```","Hey ! For what it's worth, I was able to train GPTJ with this code using jax 0.2.18 or 0.2.20. You'd have to resolve a few dependency issues (I used poetry), but it works ","Hey sorry, missed this earlier. Unfortunately it's not clear to me from the log what the issue is. This looks ok (despite the FAILED_PRECONDITION lines, those are usually spurious), but like it's just getting stuck somewhere. I suggest trying to use a newer jax version like . You can also try using a different code base that supports newer jax versions. https://github.com/salesforce/jaxformer could be a reasonable alternative."
1565,"以下是一个github上的jax下的一个issue, 标题是(Installing with minimum-jaxlib fails for Python 3.11)， 内容是 ( Description Trying to install jax v0.3.25 on Python 3.11 via pip with the `minimumjaxlib` extra fails to resolve a jaxlib version: ``` pip install ""jax[minimumjaxlib]==0.3.25"" ``` gives output ``` ERROR: Could not find a version that satisfies the requirement jaxlib==0.3.22; extra == ""minimumjaxlib"" (from jax[minimumjaxlib]) (from versions: 0.3.24, 0.3.25) ERROR: No matching distribution found for jaxlib==0.3.22; extra == ""minimumjaxlib"" ``` . The problem appears that only jaxlib versions >= 0.3.24 are available on PyPI for Python 3.11.  Possible fixes:  update minimumjaxlib to 0.3.24  make jaxlib 0.3.22 available for python 3.11  instead of freezing any particular jaxlib version via `minimumjaxlib`, maybe an extra `compatiblejaxlib` that defines a version range of known good jaxlib versions and is updated with every jax release would be a better option. For jax v0.3.25 it would now be `jaxlib >= 0.3.22, <=0.3.25` for example. That would give a bit more freedom to which particular version to resolve from the package index, while still restricting it to one that is known to work with that particular jax version.  What jax/jaxlib version are you using? jax v0.3.25, jaxlib v0.3.22  Which accelerator(s) are you using? CPU/any  Additional system info Python 3.11, Ubuntu Linux, Anaconda  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Installing with minimum-jaxlib fails for Python 3.11," Description Trying to install jax v0.3.25 on Python 3.11 via pip with the `minimumjaxlib` extra fails to resolve a jaxlib version: ``` pip install ""jax[minimumjaxlib]==0.3.25"" ``` gives output ``` ERROR: Could not find a version that satisfies the requirement jaxlib==0.3.22; extra == ""minimumjaxlib"" (from jax[minimumjaxlib]) (from versions: 0.3.24, 0.3.25) ERROR: No matching distribution found for jaxlib==0.3.22; extra == ""minimumjaxlib"" ``` . The problem appears that only jaxlib versions >= 0.3.24 are available on PyPI for Python 3.11.  Possible fixes:  update minimumjaxlib to 0.3.24  make jaxlib 0.3.22 available for python 3.11  instead of freezing any particular jaxlib version via `minimumjaxlib`, maybe an extra `compatiblejaxlib` that defines a version range of known good jaxlib versions and is updated with every jax release would be a better option. For jax v0.3.25 it would now be `jaxlib >= 0.3.22, <=0.3.25` for example. That would give a bit more freedom to which particular version to resolve from the package index, while still restricting it to one that is known to work with that particular jax version.  What jax/jaxlib version are you using? jax v0.3.25, jaxlib v0.3.22  Which accelerator(s) are you using? CPU/any  Additional system info Python 3.11, Ubuntu Linux, Anaconda  NVIDIA GPU info _No response_",2022-11-18T11:15:11Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/13309,    for viz,"Metaquestion: why are you installing the minimum jaxlib at all? This particular problem will resolve itself when we bump the minimum, which we are technically free to do at any time, but we typically try to not do it right after a new release just in case the new release is problematic.",I mostly need this in automated tests that verify that my project still works with older jax versions. I cannot use the `cpu` extra/target for that as it would just install the newest (and likely incompatible) jaxlib version.,"You should be able to use the `cpu` extra along with a version constraint, e.g., ``` pip install ""jax[cpu]==0.3.24"" ``` should install jax 0.3.24 along with whatever `jaxlib` that version of `jax` pinned. Is there a particular reason you care about the jaxlib version and not just the jax version?","It seems I have missed the change in jax version v 0.2.27 that freezes the jaxlib version. I set up my tests before then and there had the problem that jax would always pull the latest jaxlib version available, which tended to cause incompatibilities for older versions of jax. So it seems that now I can switch to using the `cpu` extra then.. Remaining questions is then, why is there still a `minimumjaxlib` extra?","Yes, we made a number of changes to versioning that prompted the 0.3 bump in the first place. Notably, the versions of `jax` and `jaxlib` are now tied. `minimumjaxlib` is an extra mostly for our own CI testing: we want to make sure that the current `jax` works with the oldest `jaxlib` for which it declares support."
362,"以下是一个github上的jax下的一个issue, 标题是([sparse] avoid re-indexing for linear unary ops)， 内容是 (Also avoid dropping explicit zeros when sparsifying unary operations. Fixes CC([sparse] Difference between BCOO matrices removes zeros))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] avoid re-indexing for linear unary ops,Also avoid dropping explicit zeros when sparsifying unary operations. Fixes CC([sparse] Difference between BCOO matrices removes zeros),2022-11-17T18:14:45Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13300
2207,"以下是一个github上的jax下的一个issue, 标题是(`AssertionError` in `jax.custom_transpose` if linearized)， 内容是 (This issue is about the unreleased version of `custom_transpose` implemented as part of CC(custom transposition). Feel free to close this issue if `jax.custom_transpose` is not intended to solve this just yet. `jax.custom_transpose` yields an `AssertionError` if linearized via `jax.linearize`. Note, the VJP, JVP, and the transpose of the VJP can be computed without a problem. The following minimal reproducible example showcases the issue. ```python from functools import partial import jax import numpy as np class _custom_transpose:     def __init__(self, out_types, fun):         from jax.custom_transpose import custom_transpose         self.out_types = out_types         self.fun = custom_transpose(fun)     def __getattr__(self, name):         return getattr(self.fun, name)     def __call__(self, *args):         return self.fun(self.out_types, *args) def _mul(residual_args, a, *, c):     print(""_mul"")     b, = residual_args     c = np.array(c)   needs to be known at tracetime     return a * b * c def _mul_T(residual_args, out, *, c):     print(""_mul_T"")     b, = residual_args     c = np.array(c)   needs to be known at tracetime     return out * b * c def mul(a, b, c):     print(a.shape)     m = partial(_mul, c=c)     m_T = partial(_mul_T, c=c)     out_types = jax.tree_map(         lambda x: jax.core.get_aval(x).at_least_vspace(), m((b, ), a)     )     inp_types = jax.tree_map(         lambda x: jax.core.get_aval(x).at_least_vspace(), a     )     m = _custom_transpose(out_types, m)     m_T = _custom_transpose(inp_types, m_T)     m.def_transpose(m_T)     m_T.def_transpose(m)     return m((b, ), a) a, b, c = np.arange(12, dtype=float), 10, np.array([2., 4.]).reshape(2, 1) o = np.zeros(mul(a, b, c).shape) m = partial(mul, b=b, c=c) m_T = jax.linear_transpose(m, a) m_T(o)   works _, m_T = jax.vjp(m, a) m_T(o)   works m_TT = jax.linear_transpose(m_T, o) m_TT((a, ))   works (with different signature) _, m2 = jax.linearize(m, a) m2(a)   fails ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`AssertionError` in `jax.custom_transpose` if linearized,"This issue is about the unreleased version of `custom_transpose` implemented as part of CC(custom transposition). Feel free to close this issue if `jax.custom_transpose` is not intended to solve this just yet. `jax.custom_transpose` yields an `AssertionError` if linearized via `jax.linearize`. Note, the VJP, JVP, and the transpose of the VJP can be computed without a problem. The following minimal reproducible example showcases the issue. ```python from functools import partial import jax import numpy as np class _custom_transpose:     def __init__(self, out_types, fun):         from jax.custom_transpose import custom_transpose         self.out_types = out_types         self.fun = custom_transpose(fun)     def __getattr__(self, name):         return getattr(self.fun, name)     def __call__(self, *args):         return self.fun(self.out_types, *args) def _mul(residual_args, a, *, c):     print(""_mul"")     b, = residual_args     c = np.array(c)   needs to be known at tracetime     return a * b * c def _mul_T(residual_args, out, *, c):     print(""_mul_T"")     b, = residual_args     c = np.array(c)   needs to be known at tracetime     return out * b * c def mul(a, b, c):     print(a.shape)     m = partial(_mul, c=c)     m_T = partial(_mul_T, c=c)     out_types = jax.tree_map(         lambda x: jax.core.get_aval(x).at_least_vspace(), m((b, ), a)     )     inp_types = jax.tree_map(         lambda x: jax.core.get_aval(x).at_least_vspace(), a     )     m = _custom_transpose(out_types, m)     m_T = _custom_transpose(inp_types, m_T)     m.def_transpose(m_T)     m_T.def_transpose(m)     return m((b, ), a) a, b, c = np.arange(12, dtype=float), 10, np.array([2., 4.]).reshape(2, 1) o = np.zeros(mul(a, b, c).shape) m = partial(mul, b=b, c=c) m_T = jax.linear_transpose(m, a) m_T(o)   works _, m_T = jax.vjp(m, a) m_T(o)   works m_TT = jax.linear_transpose(m_T, o) m_TT((a, ))   works (with different signature) _, m2 = jax.linearize(m, a) m2(a)   fails ```",2022-11-17T17:13:06Z,enhancement,open,0,2,https://github.com/jax-ml/jax/issues/13298," Can you take a quick look at this? P.S. sorry for tagging you directly. As author of CC(custom transposition), I imagine you might know best where there error is. This particular error is currently the only big road blocker for a project of mine in which I would like to enable custom dirivates of arbitrary order (with transpositions) for a model.","Thanks for the mention! I managed to miss this when you filed it, and I'm likely the one we ought to assign this to."
1490,"以下是一个github上的jax下的一个issue, 标题是(Documentation on GPU backend and host communication)， 内容是 ( Description I encountered the  behavior that the GPU of my code heavily employing a lot of jax operations is around 3x slower than running the code CPUonly. I know that this problem has been reported before but I want to ask in this report if you could provide a general guideline how to debug and fix such problems.  More specifically, I can see a lot of HostDevice Memcpy in the jax.profiler output of my code but I cannot really assign that calls to parts of my code since they are surrounded of a lot of internal calls...  In https://github.com/google/jax/issues/9259issuecomment1018008102 was noted that XLA's loops needs CPU calls for controlling the loop. Is this true for all loops (XLA while, scan, fori)? If not, is there somewhere a reference for that so I could debug my code (which is heavy on loop usage)? I would be happy if people could point me out how to improve my code. If the important points could be spelled out here, I would offer to help to add some docs about this CPUGPU usage in jax. Thanks in advance  What jax/jaxlib version are you using? jax0.3.25 jaxlib0.3.25+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.9, Debian Bullseye  NVIDIA GPU info NVIDIA Geforce RTX 3090 ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Documentation on GPU backend and host communication," Description I encountered the  behavior that the GPU of my code heavily employing a lot of jax operations is around 3x slower than running the code CPUonly. I know that this problem has been reported before but I want to ask in this report if you could provide a general guideline how to debug and fix such problems.  More specifically, I can see a lot of HostDevice Memcpy in the jax.profiler output of my code but I cannot really assign that calls to parts of my code since they are surrounded of a lot of internal calls...  In https://github.com/google/jax/issues/9259issuecomment1018008102 was noted that XLA's loops needs CPU calls for controlling the loop. Is this true for all loops (XLA while, scan, fori)? If not, is there somewhere a reference for that so I could debug my code (which is heavy on loop usage)? I would be happy if people could point me out how to improve my code. If the important points could be spelled out here, I would offer to help to add some docs about this CPUGPU usage in jax. Thanks in advance  What jax/jaxlib version are you using? jax0.3.25 jaxlib0.3.25+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.9, Debian Bullseye  NVIDIA GPU info NVIDIA Geforce RTX 3090 ``` ++  ++++ ```",2022-11-17T16:17:38Z,bug NVIDIA GPU,open,0,3,https://github.com/jax-ml/jax/issues/13297,"loop is a big topic that I don't master in JAX/XLA. One quick information, if you have a for loop with a fixed number of iteration, this should be faster. XLA will skip some communication. But even in that case, there is still copies that aren't optimized. This needs works at the XLA level. Nothing that can be done easily outside (except having a fixed iteration count)","I tried to use `scan` with a maximal number of iterations but then there is some call to `jax.lax.cond` inside it which I expect to still cause the communication, am I wrong there? If `cond` inside `scan` is preferable above `while_loop`, I can try this...",I meant to have a static constant number of iteration. max iteration doesn't do that.
1732,"以下是一个github上的jax下的一个issue, 标题是(support consts in custom batching rules)， 内容是 (To save memory in a setup where `jax.checkpoint` was insufficient, I implemented a custom transposition via `jax.custom_derivatives.linear_call`. This worked just fine and yielded the desired savings. However, when `vmap`ing, I hit a road blocker. Unfortunately, `jax.custom_derivative.linear_call` does not implement a batching rule and thus naively applying `vmap` does not work even if the arguments to `linear_call` do support batching. Furthermore, in my case it is not possible to implement the batching manually via `jax.custom_batching.custom_vmap` because `custom_vmap` does not work with constants in jaxpr. I think the best approach would be to allow both `linear_call` to be batched and `custom_vmap` to work with constants. Either of the two would unblock me :) Below is a minimal reproducible example: ```python from functools import partial import jax from jax.custom_batching import sequential_vmap from jax.custom_derivatives import linear_call import numpy as np def _mul(residual_args, a, *, c):     b, = residual_args     c = np.array(c)   needs to be known at tracetime     return a * b * c def _mul_T(residual_args, out, *, c):     b, = residual_args     c = np.array(c)   needs to be known at tracetime     return out * b * c def mul(a, b, c):     print(a.shape)     return linear_call(partial(_mul, c=c), partial(_mul_T, c=c), (b, ), a) a, b, c = np.arange(12, dtype=float), 10, np.array([2., 4.]).reshape(2, 1) m = partial(mul, b=b, c=c) jax.vmap(sequential_vmap(m), in_axes=(0, ))(a) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,support consts in custom batching rules,"To save memory in a setup where `jax.checkpoint` was insufficient, I implemented a custom transposition via `jax.custom_derivatives.linear_call`. This worked just fine and yielded the desired savings. However, when `vmap`ing, I hit a road blocker. Unfortunately, `jax.custom_derivative.linear_call` does not implement a batching rule and thus naively applying `vmap` does not work even if the arguments to `linear_call` do support batching. Furthermore, in my case it is not possible to implement the batching manually via `jax.custom_batching.custom_vmap` because `custom_vmap` does not work with constants in jaxpr. I think the best approach would be to allow both `linear_call` to be batched and `custom_vmap` to work with constants. Either of the two would unblock me :) Below is a minimal reproducible example: ```python from functools import partial import jax from jax.custom_batching import sequential_vmap from jax.custom_derivatives import linear_call import numpy as np def _mul(residual_args, a, *, c):     b, = residual_args     c = np.array(c)   needs to be known at tracetime     return a * b * c def _mul_T(residual_args, out, *, c):     b, = residual_args     c = np.array(c)   needs to be known at tracetime     return out * b * c def mul(a, b, c):     print(a.shape)     return linear_call(partial(_mul, c=c), partial(_mul_T, c=c), (b, ), a) a, b, c = np.arange(12, dtype=float), 10, np.array([2., 4.]).reshape(2, 1) m = partial(mul, b=b, c=c) jax.vmap(sequential_vmap(m), in_axes=(0, ))(a) ```",2022-11-16T21:48:29Z,enhancement,closed,0,5,https://github.com/jax-ml/jax/issues/13283,"Currently, this can not be circumvented by using `jax.custom_transpose` since it is missing a batching rule too.",The current plan is to push ahead with `custom_vmap` and perhaps do away with `linear_call`. Tracking at CC(custom batching (vmap)).,"Ok, so in this case it would be great if `custom_vmap` would work with constants. Should I file a separate issue for that and close this one? Furthermore, to me CC(`AssertionError` in `jax.custom_transpose` if linearized) becomes relevant then because there are some things that `linear_call` can do which `custom_transpose` can't (yet).","I'll repurpose this issue. We'll certainly need to make sure consts are supported before we consider custom batching complete. It doesn't hurt to have an open issue reminding us to do this, with your particular example registered. Thanks for filing!",Awesome! Thank you very much for the resolving this issue so quickly!
352,"以下是一个github上的jax下的一个issue, 标题是([sparse] refactor with sparse.test_util)， 内容是 (This should keep test coverage more or less constant, but I think the sparse test utilities make for a nicer expression of the tests.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[sparse] refactor with sparse.test_util,"This should keep test coverage more or less constant, but I think the sparse test utilities make for a nicer expression of the tests.",2022-11-16T19:48:44Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13280
333,"以下是一个github上的jax下的一个issue, 标题是(Include which jaxlib/libtpu version failed (latest or nightly) in TPU CI chat notification)， 内容是 (Test run: https://github.com/google/jax/actions/runs/3482341477)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,Include which jaxlib/libtpu version failed (latest or nightly) in TPU CI chat notification,Test run: https://github.com/google/jax/actions/runs/3482341477,2022-11-16T19:09:22Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13278
1533,"以下是一个github上的jax下的一个issue, 标题是(`roll` incorrect if jit-compiled on GPU with 64-bit mode)， 内容是 ( Description For arrays of length `2^n` (tested 4, 8, 16, and 64), `roll` gives different results before and after applying `jit`. This behavior appears only on the GPU in 64bit mode and depends on the CUDA/cuDNN version. I can't tell if this is a problem with JAX, some underlying library, or possibly a faulty installation. In 64bit mode: ```python from jax.config import config config.update(""jax_enable_x64"", True) import jax import jax.numpy as jnp def roll(arr, index):     return jnp.roll(arr, index) ```  `roll` gives different results before and after `jit`: ```python >>> roll(l, 1) DeviceArray([1, 2, 3, 4, 5, 6, 7, 0], dtype=int64) >>> jax.jit(roll)(l, 1) DeviceArray([7, 0, 1, 2, 3, 4, 5, 6], dtype=int64) ``` I found this behavior with two different machines (see below) and with `cuDNN 8.4.1.50`,  `CUDA 11.7.0`, and multiple versions of JAX. The problem disappeared when using `cuDNN 8.2.1.32` and`CUDA 11.3.1`. I could also not reproduce this on colab with `jax v0.3.25`, `jaxlib v0.3.25`  on the Tesla T4 GPU (`CUDA 11.2`).  What jax/jaxlib version are you using? Both jax v0.3.25, jaxlib v0.3.25 and jax v0.3.14, jaxlib v0.3.14  Which accelerator(s) are you using? GPU, 64bit mode  Additional system info Linux, python 3.10.4  NVIDIA GPU info Reproduced this on ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`roll` incorrect if jit-compiled on GPU with 64-bit mode," Description For arrays of length `2^n` (tested 4, 8, 16, and 64), `roll` gives different results before and after applying `jit`. This behavior appears only on the GPU in 64bit mode and depends on the CUDA/cuDNN version. I can't tell if this is a problem with JAX, some underlying library, or possibly a faulty installation. In 64bit mode: ```python from jax.config import config config.update(""jax_enable_x64"", True) import jax import jax.numpy as jnp def roll(arr, index):     return jnp.roll(arr, index) ```  `roll` gives different results before and after `jit`: ```python >>> roll(l, 1) DeviceArray([1, 2, 3, 4, 5, 6, 7, 0], dtype=int64) >>> jax.jit(roll)(l, 1) DeviceArray([7, 0, 1, 2, 3, 4, 5, 6], dtype=int64) ``` I found this behavior with two different machines (see below) and with `cuDNN 8.4.1.50`,  `CUDA 11.7.0`, and multiple versions of JAX. The problem disappeared when using `cuDNN 8.2.1.32` and`CUDA 11.3.1`. I could also not reproduce this on colab with `jax v0.3.25`, `jaxlib v0.3.25`  on the Tesla T4 GPU (`CUDA 11.2`).  What jax/jaxlib version are you using? Both jax v0.3.25, jaxlib v0.3.25 and jax v0.3.14, jaxlib v0.3.14  Which accelerator(s) are you using? GPU, 64bit mode  Additional system info Linux, python 3.10.4  NVIDIA GPU info Reproduced this on ``` ++  ++++ ```",2022-11-16T07:46:14Z,bug NVIDIA GPU,closed,0,6,https://github.com/jax-ml/jax/issues/13271,"Smaller repro: ``` import jax, jax.lax as lax jax.config.update(""jax_enable_x64"", True) def f(x):   return lax.rem(x, 8) print(f(1)) print(jax.jit(f)(1)) ``` which prints ``` 1 1 ``` when it should print the same thing both times. XLA ends up optimizing this to: ``` def f(a):   x = a   y = lax.select(x < 0, x, x)   z = lax.bitwise_and(y, 0x7)   return lax.select(x < 0, z, z) ``` and that repro works also.","This appears to be a miscompilation of some sort in `ptxas` which is provided by NVIDIA. As a workaround, you should downgrade your CUDA installation to a copy without the bug. In both cases, JAX generates the following PTX, which looks correct: ``` // // Generated by LLVM NVPTX BackEnd // .version 7.0 .target sm_80 .address_size 64         // .globl       fusion .visible .entry fusion(         .param .u64 fusion_param_0,         .param .u64 fusion_param_1 ) .reqntid 1, 1, 1 {         .reg .pred      %p;         .reg .b64       %rd;         ld.param.u64    %rd1, [fusion_param_0];         ld.param.u64    %rd2, [fusion_param_1];         cvta.to.global.u64      %rd3, %rd2;         cvta.to.global.u64      %rd4, %rd1;         ld.global.nc.u64        %rd5, [%rd4];         neg.s64         %rd6, %rd5;         setp.lt.s64     %p1, %rd6, 0;         abs.s64         %rd7, %rd5;         and.b64         %rd8, %rd7, 7;         neg.s64         %rd9, %rd8;         selp.b64        %rd10, %rd9, %rd8, %p1;         st.global.u64   [%rd3], %rd10;         ret; } ``` With `ptxas` from CUDA 11.8 we get a wrong output, but with `ptxas` from CUDA 11.6 we get a correct output.",  for viz,Filed NVIDIA partners bug 3872915,"Hi   Looks like this issue has been resolved in later versions of JAX. I executed the mentioned code on colab (GPU T4) with cuda 12.3 and cuDNN 8.9.7 and JAX versions 0.4.23 and 0.4.25. The `roll` produces the same output with and without `jit`. I also verified with cuda 11.8 and cuDNN 8.9.6 and JAX versions 0.4.23 and 0.4.25, the output is same for both with and without jitcompilation. ```python from jax import config config.update(""jax_enable_x64"", True) import jax import jax.numpy as jnp def roll(arr, index):     return jnp.roll(arr, index) ``` Without `JIT`Compilation: ```python l = jnp.arange(8) print(l) roll(l, 1) ``` Output: ``` [0 1 2 3 4 5 6 7] Array([1, 2, 3, 4, 5, 6, 7, 0], dtype=int64) ``` With `JIT`Compilation: ```python jax.jit(roll)(l, 1) ``` Output: ``` Array([1, 2, 3, 4, 5, 6, 7, 0], dtype=int64) ``` Kindly find the gist for reference. Thank you","Hi   Please feel free to close the issue, if it is resolved. Thank you."
1314,"以下是一个github上的jax下的一个issue, 标题是(Getting started with a TPU)， 内容是 ( Description I am trying to use a TPU VM. The first time I run the VM everything works great. However after stopping / starting the VM instance it reports it cannot find any TPU's anymore. ``` chrislaptop ~ $ python3 Python 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax /home/chris/.local/lib/python3.8/sitepackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: ConnectionError(MaxRetryError(""HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/agentworkernumber (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 2] Name or service not known'))""))  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report "" ```  What jax/jaxlib version are you using? jax[tpu] 0.3.24, jaxlib 0.3.24  Which accelerator(s) are you using? TPU  Additional system info TPU VM 28  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Getting started with a TPU," Description I am trying to use a TPU VM. The first time I run the VM everything works great. However after stopping / starting the VM instance it reports it cannot find any TPU's anymore. ``` chrislaptop ~ $ python3 Python 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax /home/chris/.local/lib/python3.8/sitepackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: ConnectionError(MaxRetryError(""HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/agentworkernumber (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 2] Name or service not known'))""))  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report "" ```  What jax/jaxlib version are you using? jax[tpu] 0.3.24, jaxlib 0.3.24  Which accelerator(s) are you using? TPU  Additional system info TPU VM 28  NVIDIA GPU info _No response_",2022-11-15T17:12:38Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/13257
238,"以下是一个github上的jax下的一个issue, 标题是(Send message to internal chat room on Cloud TPU CI failure)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,Send message to internal chat room on Cloud TPU CI failure,,2022-11-14T19:45:55Z,kokoro:force-run pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/13236,I'll create the `BUILD_CHAT_WEBHOOK` once this PR is approved. Sample message: !image  ,I might wanna do the issue creation too! But I think I'm personally more likely to see the chats and it's easier for now.
2490,"以下是一个github上的jax下的一个issue, 标题是(Batching rules not implemented for host_callback.call)， 内容是 ( Description I'm trying to vmap a function that solves linear programs (related: CC(Add scipy.optimize.linprog)) using cvxopt.solvers.lp wrapped inside a host_callback.call. Consider the following example: ```python3 from cvxopt import matrix, solvers from numpy import asarray from jax import numpy as jnp, jit, ShapeDtypeStruct, vmap, lax from jax.experimental.host_callback import call solvers.options['glpk'] = {'msg_lev': 'GLP_MSG_OFF'} def lp_helper(c, G, h, A=None, b=None):     result = solvers.lp(         matrix(asarray(c, dtype='double')),         matrix(asarray(G, dtype='double')),         matrix(asarray(h, dtype='double')),         None if A is None else matrix(asarray(A, dtype='double')),         None if b is None else matrix(asarray(b, dtype='double')),         solver='glpk',     )     if result['status'] == 'optimal':         return {             'x': jnp.array(result['x'], dtype=float)[:, 0],             'z': jnp.array(result['z'], dtype=float)[:, 0],         }     else:         breakpoint() def lp(*args):     m, n = args[1].shape     return call(         lambda args: lp_helper(*args),         args,         result_shape={             'x': ShapeDtypeStruct([n], float),             'z': ShapeDtypeStruct([m], float),         },     ) def main():     lp_jit = jit(lp)     G = jnp.array([         [1, 0],         [0, 1],         [2, 3],     ])     h = jnp.array([0, 0, 1])     cs = jnp.array([         [1, 0],         [0, 1],         [1, 1],     ])     xs = jnp.stack(list(map(lambda c: lp_jit(c, G, h)['x'], cs)))     print(xs)     xs = lax.map(lambda c: lp_jit(c, G, h)['x'], cs)     print(xs)     try:         xs = vmap(lambda c: lp_jit(c, G, h)['x'])(cs)         print(xs)     except NotImplementedError as e:         print(e) if __name__ == '__main__':     main() ``` It outputs ``` [[0.5        0.        ]  [0.         0.33333334]  [0.         0.        ]] [[0.5        0.        ]  [0.         0.33333334]  [0.         0.        ]] batching rules are implemented only for id_tap, not for call. ``` As you can see, vmap raises an error.  What jax/jaxlib version are you using? jax 0.3.24, jaxlib 0.3.24  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.7, macOS 11.7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Batching rules not implemented for host_callback.call," Description I'm trying to vmap a function that solves linear programs (related: CC(Add scipy.optimize.linprog)) using cvxopt.solvers.lp wrapped inside a host_callback.call. Consider the following example: ```python3 from cvxopt import matrix, solvers from numpy import asarray from jax import numpy as jnp, jit, ShapeDtypeStruct, vmap, lax from jax.experimental.host_callback import call solvers.options['glpk'] = {'msg_lev': 'GLP_MSG_OFF'} def lp_helper(c, G, h, A=None, b=None):     result = solvers.lp(         matrix(asarray(c, dtype='double')),         matrix(asarray(G, dtype='double')),         matrix(asarray(h, dtype='double')),         None if A is None else matrix(asarray(A, dtype='double')),         None if b is None else matrix(asarray(b, dtype='double')),         solver='glpk',     )     if result['status'] == 'optimal':         return {             'x': jnp.array(result['x'], dtype=float)[:, 0],             'z': jnp.array(result['z'], dtype=float)[:, 0],         }     else:         breakpoint() def lp(*args):     m, n = args[1].shape     return call(         lambda args: lp_helper(*args),         args,         result_shape={             'x': ShapeDtypeStruct([n], float),             'z': ShapeDtypeStruct([m], float),         },     ) def main():     lp_jit = jit(lp)     G = jnp.array([         [1, 0],         [0, 1],         [2, 3],     ])     h = jnp.array([0, 0, 1])     cs = jnp.array([         [1, 0],         [0, 1],         [1, 1],     ])     xs = jnp.stack(list(map(lambda c: lp_jit(c, G, h)['x'], cs)))     print(xs)     xs = lax.map(lambda c: lp_jit(c, G, h)['x'], cs)     print(xs)     try:         xs = vmap(lambda c: lp_jit(c, G, h)['x'])(cs)         print(xs)     except NotImplementedError as e:         print(e) if __name__ == '__main__':     main() ``` It outputs ``` [[0.5        0.        ]  [0.         0.33333334]  [0.         0.        ]] [[0.5        0.        ]  [0.         0.33333334]  [0.         0.        ]] batching rules are implemented only for id_tap, not for call. ``` As you can see, vmap raises an error.  What jax/jaxlib version are you using? jax 0.3.24, jaxlib 0.3.24  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.7, macOS 11.7  NVIDIA GPU info _No response_",2022-11-12T21:23:32Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/13221,"Try using `jax.pure_callback` instead. In particular this takes a `vectorized` argument, which you can use to toggle how batching should work for the callback.","kidger Thanks. Any idea why this throws a `ValueError` when `jax_enable_x64` is disabled?  ```python3 from cvxopt import matrix, solvers from numpy import array from jax import numpy as jnp, jit, ShapeDtypeStruct, vmap, lax, pure_callback from jax.config import config solvers.options['glpk'] = {'msg_lev': 'GLP_MSG_OFF'} def lp_helper(c, G, h, A=None, b=None):     result = solvers.lp(         matrix(c.astype(float)),         matrix(G.astype(float)),         matrix(h.astype(float)),         None if A is None else matrix(A.astype(float)),         None if b is None else matrix(b.astype(float)),         solver='glpk',     )     if result['status'] == 'optimal':         return {             'x': array(result['x'])[:, 0],             'z': array(result['z'])[:, 0],         }     else:         breakpoint() def lp(*args):     m, n = args[1].shape     return pure_callback(         lp_helper,         {             'x': ShapeDtypeStruct([n], float),             'z': ShapeDtypeStruct([m], float),         },         *args,     ) def main():      config.update('jax_enable_x64', True)     G = jnp.array([         [1, 0],         [0, 1],         [2, 3],     ])     h = jnp.array([0, 0, 1])     cs = jnp.array([         [1, 0],         [0, 1],         [1, 1],     ])     try:         lp(cs[0], G, h)     except ValueError as e:         print(e)          Cannot return 64bit values when `jax_enable_x64` is disabled         exit()     lp_jit = jit(lp)     xs = jnp.stack(list(map(lambda c: lp_jit(c, G, h)['x'], cs)))     print(xs)     xs = lax.map(lambda c: lp_jit(c, G, h)['x'], cs)     print(xs)     xs = vmap(lambda c: lp_jit(c, G, h)['x'])(cs)     print(xs) if __name__ == '__main__':     main() ```  Also, is there a reason why pure_callback converts inputs/outputs to/from numpy arrays instead of leaving them as jax arrays, and letting the user do the conversion if necessary?","Just looking at the code: probably the fact that you're using the Python builtin `float`. I think this can get canonicalised to 32 or 64 differently, depending on whether it's JAX or numpy, whether x64 is enabled or not, whether there's an 'r' in the month... Be explicit about the precision and it should probably be fine. `pure_callback` does the conversion because JAX is only reentrant on the CPU (I believe). That is, the contents of a callback must execute on the CPU, which is of course where numpy arrays are anyway.",I agree w/ Patrick. `pure_callback` seems like the appropriate solution here. `host_callback.call` doesn't support batching because it isn't generally safe to batch callbacks that might have arbitrary sideeffects in them., Thanks. Perhaps a note about this could be added to the doc page?,Hi   `host_callback` has been `deprecated with JAX version 0.4.26` and `removed with JAX version 0.4.35`. The alternatives for `host_callback` are discussed in CC(Deprecate jax.experimental.host_callback in favor of JAX external callbacks). Thank you.,"Thanks,  !"
616,"以下是一个github上的jax下的一个issue, 标题是(Support for TFLite FloorMod)， 内容是 (I'm creating JAX functions and convert them to TFLite models using `tf.lite.TFLiteConverter.experimental_from_jax`. When trying to convert a simple `i = (i + 1) % buff_size` operation, I'm getting the following error: > 'tf.Mod' op is neither a custom op nor a flex op TFLite does support `floor_mod`, which seems to be ok for my case. So I tried `i = jnp.fmod(i + 1, buff_size)` but got the exact same error.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Support for TFLite FloorMod,"I'm creating JAX functions and convert them to TFLite models using `tf.lite.TFLiteConverter.experimental_from_jax`. When trying to convert a simple `i = (i + 1) % buff_size` operation, I'm getting the following error: > 'tf.Mod' op is neither a custom op nor a flex op TFLite does support `floor_mod`, which seems to be ok for my case. So I tried `i = jnp.fmod(i + 1, buff_size)` but got the exact same error.",2022-11-11T15:14:09Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/13207,"Can you try an alternative conversion using jax2tf followed by the TFLite converter, as described here?","On a coincidence, I switched laptops and installed updated TensorFlow & JAX versions, seems like `i = (i + 1) % buff_size` expressions are correctly being converted to `FloorMod` ops. Thank you for your help."
1887,"以下是一个github上的jax下的一个issue, 标题是(pmap on ""different types"" of GPU)， 内容是 (Is there a method to perform pmap on different types of GPUs in a single host?  Our server have A100s and A800s, (In fact they are just different in memoryIO ability. The structures are the same). And I tried to perform pmap on our server: ``` python In [1]: import jax    ...: import jax.numpy as jnp    ...: def pca(x, y):    ...:     mean = (y[..., None]*x).mean()    ...:     centered = x  mean    ...:     cov = centered.T @ jnp.diag(y) @ centered    ...:    ...:     s, vecs = jnp.linalg.eigh(cov)    ...:     vecs = vecs.T    ...:     return s, vecs    ...:    ...: def cond_pca(x, y):    ...:     s, axes = pca(x, y)    ...:     def fn(_):    ...:         s, axes = pca(x, y)    ...:         return s, axes    ...:     s, axes = jax.lax.cond(s.sum()  in       18     return axes      19 > 20 jax.pmap(jax.vmap(lambda x: cond_pca(x, jnp.arange(5))))(jnp.ones((6, 4, 5, 3)))     [... skipping hidden 3 frame] ~/jax/jax/interpreters/pxla.py in __call__(self, *args)    2069       out_bufs = self._call_with_tokens(input_bufs)    2070     else: > 2071       out_bufs = self.xla_executable.execute_sharded_on_local_devices(    2072           input_bufs)    2073     if dispatch.needs_check_special(): XlaRuntimeError: INVALID_ARGUMENT: executable is built for device CUDA:0 of type ""NVIDIA A800 80GB PCIe""; cannot run it on device CUDA:3 of type ""NVIDIA A100 80GB PCIe"": while running replica 3 and partition 0 of a replicated computation (other replicas may have failed as well). ``` It seems the JAX regard NVIDIA A100 80GB PCIe and NVIDIA A800 80GB PCIe different types of devices. and the pmap can not be performed. Is there a simple way to resolve this? Thanks a ton.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"pmap on ""different types"" of GPU","Is there a method to perform pmap on different types of GPUs in a single host?  Our server have A100s and A800s, (In fact they are just different in memoryIO ability. The structures are the same). And I tried to perform pmap on our server: ``` python In [1]: import jax    ...: import jax.numpy as jnp    ...: def pca(x, y):    ...:     mean = (y[..., None]*x).mean()    ...:     centered = x  mean    ...:     cov = centered.T @ jnp.diag(y) @ centered    ...:    ...:     s, vecs = jnp.linalg.eigh(cov)    ...:     vecs = vecs.T    ...:     return s, vecs    ...:    ...: def cond_pca(x, y):    ...:     s, axes = pca(x, y)    ...:     def fn(_):    ...:         s, axes = pca(x, y)    ...:         return s, axes    ...:     s, axes = jax.lax.cond(s.sum()  in       18     return axes      19 > 20 jax.pmap(jax.vmap(lambda x: cond_pca(x, jnp.arange(5))))(jnp.ones((6, 4, 5, 3)))     [... skipping hidden 3 frame] ~/jax/jax/interpreters/pxla.py in __call__(self, *args)    2069       out_bufs = self._call_with_tokens(input_bufs)    2070     else: > 2071       out_bufs = self.xla_executable.execute_sharded_on_local_devices(    2072           input_bufs)    2073     if dispatch.needs_check_special(): XlaRuntimeError: INVALID_ARGUMENT: executable is built for device CUDA:0 of type ""NVIDIA A800 80GB PCIe""; cannot run it on device CUDA:3 of type ""NVIDIA A100 80GB PCIe"": while running replica 3 and partition 0 of a replicated computation (other replicas may have failed as well). ``` It seems the JAX regard NVIDIA A100 80GB PCIe and NVIDIA A800 80GB PCIe different types of devices. and the pmap can not be performed. Is there a simple way to resolve this? Thanks a ton.",2022-11-11T07:45:06Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/13203,"We don't support `pmap` across different kinds of GPUs. You can run computations on different GPUs simply by placing them  there, but `pmap` requires the same kind of GPU. Make sense?"
1602,"以下是一个github上的jax下的一个issue, 标题是(Cannot use tokens in custom calls with layout)， 内容是 ( Description At mpi4jax we are using tokens passed to custom calls to prevent reordering of stateful operations. We recently noticed that we are getting wrong results when transposes are not manifested on GPU (mpi4jax/mpi4jax CC(add np.append and np.polyval)), so we tried switching to `CustomCallWithLayout`. Now to the main problem: when passing tokens to `CustomCallWithLayout`, XLA fails when trying to determine the layout of the token. (We are getting the same issue when using the new `jaxlib.mhlo_helpers.custom_call` instead.) Example: ```python def my_dummy_op_encode_cpu(ctx, x, token):      ...     return custom_call(       b""my_dummy_op"",       [dtype, token.type],       [x, token],       backend_config=None,       operand_layouts=[layout, ()],       result_layouts=[layout, ()],       has_side_effect=True) ``` This crashes the interpreter with: ```  20221109 19:13:37.960054: F external/org_tensorflow/tensorflow/compiler/xla/shape.h:162] Check failed: IsArray() element_type: TOKEN ``` MWE on Colab, thanks to . If this is indeed a bug in XLA, is there a simple workaround? Explicitly making a copy would be acceptable as a temporary fix, but `jnp.ascontiguousarray` is not available.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot use tokens in custom calls with layout," Description At mpi4jax we are using tokens passed to custom calls to prevent reordering of stateful operations. We recently noticed that we are getting wrong results when transposes are not manifested on GPU (mpi4jax/mpi4jax CC(add np.append and np.polyval)), so we tried switching to `CustomCallWithLayout`. Now to the main problem: when passing tokens to `CustomCallWithLayout`, XLA fails when trying to determine the layout of the token. (We are getting the same issue when using the new `jaxlib.mhlo_helpers.custom_call` instead.) Example: ```python def my_dummy_op_encode_cpu(ctx, x, token):      ...     return custom_call(       b""my_dummy_op"",       [dtype, token.type],       [x, token],       backend_config=None,       operand_layouts=[layout, ()],       result_layouts=[layout, ()],       has_side_effect=True) ``` This crashes the interpreter with: ```  20221109 19:13:37.960054: F external/org_tensorflow/tensorflow/compiler/xla/shape.h:162] Check failed: IsArray() element_type: TOKEN ``` MWE on Colab, thanks to . If this is indeed a bug in XLA, is there a simple workaround? Explicitly making a copy would be acceptable as a temporary fix, but `jnp.ascontiguousarray` is not available.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2022-11-10T12:03:44Z,bug NVIDIA GPU,closed,1,1,https://github.com/jax-ml/jax/issues/13187,I ran into this bug a few days ago as well! It is fixed at HEAD in XLA: https://github.com/tensorflow/tensorflow/commit/c84f2805db56e9739c90619ea6bd6926cd20aba7. A new jaxlib release should not have this issue.
526,"以下是一个github上的jax下的一个issue, 标题是([sparse] fix shape bug in bcoo_transpose)， 内容是 (This bug only existed when calling `bcoo_transpose` directly, and we didn't have test coverage for that. Using `mat.T` or `sparsify(lax.transpose)` computed the shape via different logic and so it did not have this bug. Discovered in the course of CC([sparse] add bcoo_gather & support for sparse indexing))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[sparse] fix shape bug in bcoo_transpose,"This bug only existed when calling `bcoo_transpose` directly, and we didn't have test coverage for that. Using `mat.T` or `sparsify(lax.transpose)` computed the shape via different logic and so it did not have this bug. Discovered in the course of CC([sparse] add bcoo_gather & support for sparse indexing)",2022-11-09T20:05:32Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13175
270,"以下是一个github上的jax下的一个issue, 标题是(Remove flaky Array defragmentation test check)， 内容是 (Remove flaky Array defragmentation test check)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Remove flaky Array defragmentation test check,Remove flaky Array defragmentation test check,2022-11-09T03:10:10Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13164
288,"以下是一个github上的jax下的一个issue, 标题是([jax] Fix manual defragment method to work with Arrays)， 内容是 ([jax] Fix manual defragment method to work with Arrays)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax] Fix manual defragment method to work with Arrays,[jax] Fix manual defragment method to work with Arrays,2022-11-08T19:47:09Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13154
1163,"以下是一个github上的jax下的一个issue, 标题是(Jax on GPU and jit is 11x slower than Pytorch on GPU)， 内容是 ( Description I migrated from Pytorch to Jax but I am noticing 11x slowdown on Jax. To test more generally, I used a simple function that sums the first three powers of a matrix ``` def fn(x):     return x+x*x+x*x*x x=np.random.randn(10000,10000).astype(dtype='float32') jax_fn=jit(fn) x=jnp.array(x) %timeit  n5 jax_fn(x).block_until_ready() ``` Jax takes 5.48 ms. This is running on GPU [by checking print(device_put(1, jax.devices()[0]).device_buffer.device()) ] While same code on Pytorch on the same GPU runs in 459 microseconds which is 11x faster.  Im wondering where the slowdown is coming from and if there are any ways to speed it up? Thanks a lot for your help  What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? GPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Jax on GPU and jit is 11x slower than Pytorch on GPU," Description I migrated from Pytorch to Jax but I am noticing 11x slowdown on Jax. To test more generally, I used a simple function that sums the first three powers of a matrix ``` def fn(x):     return x+x*x+x*x*x x=np.random.randn(10000,10000).astype(dtype='float32') jax_fn=jit(fn) x=jnp.array(x) %timeit  n5 jax_fn(x).block_until_ready() ``` Jax takes 5.48 ms. This is running on GPU [by checking print(device_put(1, jax.devices()[0]).device_buffer.device()) ] While same code on Pytorch on the same GPU runs in 459 microseconds which is 11x faster.  Im wondering where the slowdown is coming from and if there are any ways to speed it up? Thanks a lot for your help  What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? GPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info ``` ++  ++ ```",2022-11-08T03:57:22Z,performance needs info NVIDIA GPU,closed,0,11,https://github.com/jax-ml/jax/issues/13151,"I think your benchmark also measures compilation time of `jax_fn`, which happens in the first iteration of `timeit` (that's where `JIT` comes from, as far as I am aware  it gets compiled as late as on the first invocation with arguments). Meanwhile in the JAX docs (FAQ section), there's this snippet which splits off compilation specifically from the benchmark: ```py %time x_jax = jax.device_put(x_np)   measure JAX device transfer time f_jit = jax.jit(f) %time f_jit(x_jax).block_until_ready()   measure JAX compilation time %timeit f_jit(x_jax).block_until_ready()   measure JAX runtime ``` So by restructuring your benchmark in this way, you should be able to obtain significantly better runtimes. ","Hi ,  Thanks for your response. I followed your suggestion but it did not change the timings significantly. Here are the outputs below. It is still way slower than Pytorch (5.41 ms for Jax vs 0.459 ms for Pytorch). Is there a way you can replicate this simple example and report your timings? I'm wondering if there is something inherent about jax that is slower than PyTorch or is it something in my version or installation? ``` %time x_jax = jax.device_put(x,device=gpus[0])   measure JAX device transfer time CPU times: user 47.9 ms, sys: 293 ms, total: 341 ms Wall time: 623 ms f_jit = jax.jit(fn) %time f_jit(x_jax).block_until_ready()   measure JAX compilation time CPU times: user 36.6 ms, sys: 6.71 ms, total: 43.3 ms Wall time: 207 ms DeviceArray([[ 0.73889023, 0.00520617, 1.6567433 , ...,  0.25744492,  1.8352127 , 0.19825274],              [ 0.12012546, 0.6986189 , 0.21019334, ..., 0.35663012, 3.3269477 ,  2.5003362 ],              [ 0.72691584, 0.22199397, 23.771     , ..., 2.9344034 ,  2.3571334 , 0.25715792],              ...,              [ 0.20999207,  0.14257683,  0.6876698 , ...,  0.23829335,  8.207063  ,  3.1074247 ],              [0.10749066, 0.7747447 ,  6.4594707 , ...,  1.9226366 , 1.1958256 , 0.11689798],              [ 0.07622809, 9.640307  ,  0.6005621 , ...,  0.65201366, 0.70378876, 0.78431636]], dtype=float32) %timeit f_jit(x_jax).block_until_ready()   measure JAX runtime 5.41 ms ± 110 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) ```","Can you share your Pytorch benchmark code? Interestingly, printing the resulting jaxpr here, ```py >>> x=np.random.randn(100,100).astype(dtype='float32') >>> x = jnp.array(x) >>> jax.make_jaxpr(fn)(x) { lambda ; a:f32[100,100]. let     b:f32[100,100] = mul a a     c:f32[100,100] = add a b     d:f32[100,100] = mul a a     e:f32[100,100] = mul d a     f:f32[100,100] = add c e   in (f,) } ``` reveals that the same calculation (`x*x`) is run twice. I don't know if the compiler is smart enough to substitute the result of the first computation for `d` here (I can imagine it is, but in my opinion, it should be `d:f32[100,100] = mul b a`, and then `e = add c d`). If it's not, you can still work around it by binding the result of `x*x` to an intermediate value, although that's a bit verbose. (I would still think JAX should be able to do this on its own, though, so maybe this is a more interesting case than just raw compute performance.)","A note on repeated computations: jaxprs don't contain any of this logic (they're just intermediate representations of the computations you write out in the Python code); all deduplication is done at the compiler level. You can confirm this by printing the compiled HLO: ```python print(jax.jit(fn).lower(x).compile().as_text()) ``` ``` HloModule jit_fn, entry_computation_layout={(f32[10]{0})>f32[10]{0}} %fused_computation (param_0.2: f32[10]) > f32[10] {   %param_0.2 = f32[10]{0} parameter(0)   %multiply.1 = f32[10]{0} multiply(f32[10]{0} %param_0.2, f32[10]{0} %param_0.2), metadata={op_name=""jit(fn)/jit(main)/mul"" source_file="""" source_line=5}   %add.1 = f32[10]{0} add(f32[10]{0} %param_0.2, f32[10]{0} %multiply.1), metadata={op_name=""jit(fn)/jit(main)/add"" source_file="""" source_line=5}   %multiply.0 = f32[10]{0} multiply(f32[10]{0} %multiply.1, f32[10]{0} %param_0.2), metadata={op_name=""jit(fn)/jit(main)/mul"" source_file="""" source_line=5}   ROOT %add.0 = f32[10]{0} add(f32[10]{0} %add.1, f32[10]{0} %multiply.0), metadata={op_name=""jit(fn)/jit(main)/add"" source_file="""" source_line=5} } ENTRY %main.7 (Arg_0.1: f32[10]) > f32[10] {   %Arg_0.1 = f32[10]{0} parameter(0)   ROOT %fusion = f32[10]{0} fusion(f32[10]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit(fn)/jit(main)/add"" source_file="""" source_line=5} } ``` The output is a bit hard to reed, but you can see here that multiply(param_0, param_0) is only computed once in the compiled version.","Yeah absolutely. Here is the Pytorch code  .  ``` import torch import numpy as np def fn(x):     return x+x*x+x*x*x x=np.random.randn(10000,10000).astype(dtype='float32') x_torch=torch.tensor(x,device='cuda') %timeit  fn(x_torch) ``` An interesting thing I am noticing is that the performance is slightly more comparable as array sizes are decreased. For example for 1000x1000 (instead of 10,000x10,000), Pytorch takes 97 us while Jax takes 393 us which is 4x slower instead of 11x slower.  Your insight about x*x is very interesting, for bigger and more complicated models, any advice on how to avoid duplicate computations? Thanks a lot for the help and fast responses. ","To provide more context, the model that I am trying to port from Pytorch to Jax runs in 2 seconds on pytorch and 16 seconds on jax.  ","I cannot reproduce this on Apple Silicon CPU, at least: ```py ➜ cat bench.py import jax import jax.numpy as jnp import numpy as np import torch import timeit def fn(x):     return x+x*x+x*x*x y=np.random.randn(1000, 1000).astype(dtype='float32') y_torch=torch.tensor(y) y_jax=jnp.array(y) jax_fn = jax.jit(fn) jax_fn(y_jax).block_until_ready() t = timeit.timeit(lambda: jax_fn(y_jax).block_until_ready(), number=1000) print(f""JAX: {t * 1000} usec"") tt = timeit.timeit(lambda: fn(y_torch), number=1000) print(f""Pytorch: {tt * 1000} usec"") ``` Prints: ``` ~ via 🐍 v3.10.8 (jax) ➜ python bench.py JAX: 161.2808329955442 usec Pytorch: 1106.6547080044984 usec ``` I tried this with `torch.tensor(..., device=""cpu"")` as well, without a change. So this could be a GPU problem? Can you post the results you obtain with this script? Then we have a frame of reference over which we can compare the numbers.","You are absolutely right! Running this code on cpu gives much better performance for jax.  ``` %timeit n1000 fn(x_torch)   for pytorch 3.02 ms ± 89.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) %timeit n1000 f_jit(x_jax).block_until_ready()   measure JAX runtime 242 µs ± 18.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) ``` On cpu Jax is 12x FASTER!  I'm using WSL and a Quadro P5200,  CUDA Version: 11.7 but unsure about cuDNN version for the GPU tests from before. ","Just like we need `block_until_ready()` to properly profile GPUs in JAX, for PyTorch we will need `torch.cuda.synchronize()`. On a Colab T4 instance: ``` %timeit n 100 f_jit(x_jax).block_until_ready()   measure JAX runtime ``` > 3.85 ms ± 26.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) ``` x_torch = torch.tensor(x, device='cuda') def fn_torch(x):     r = x+x*x+x*x*x     torch.cuda.synchronize() with torch.no_grad():     %timeit n 10 fn_torch(x_torch) ``` > 21.3 ms ± 131 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) Hence JAX is indeed faster 😄 "," Since you said your endtoend model is slower under JAX, I suspect that what has happened is that your microbenchmark is no longer representative of your original benchmark. Can you make a more representative benchmark?","We never got any more information about the actual end to end model, and the only benchmark given was faster under JAX. There's not much more we can do here without more information. Closing."
909,"以下是一个github上的jax下的一个issue, 标题是(Has xla_force_host_platform_device_count been removed?)， 内容是 ( Description trying to use it says that it's undefined and crashes ``` In [1]: import os    ...: os.environ[""XLA_FLAGS""]=""–xla_force_host_platform_device_count=4""    ...: import jax    ...: In [2]: jax.devices() 20221107 19:17:39.159296: F external/org_tensorflow/tensorflow/compiler/xla/parse_flags_from_env.cc:226] Unknown flags in XLA_FLAGS: –xla_force_host_platform_device_count=4 Perhaps you meant to specify these on the TF_XLA_FLAGS envvar? zsh: IOT instruction (core dumped)  ipython ```  What jax/jaxlib version are you using? jax 0.3.24, jaxlib 0.3.24  Which accelerator(s) are you using? CPU/GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Has xla_force_host_platform_device_count been removed?," Description trying to use it says that it's undefined and crashes ``` In [1]: import os    ...: os.environ[""XLA_FLAGS""]=""–xla_force_host_platform_device_count=4""    ...: import jax    ...: In [2]: jax.devices() 20221107 19:17:39.159296: F external/org_tensorflow/tensorflow/compiler/xla/parse_flags_from_env.cc:226] Unknown flags in XLA_FLAGS: –xla_force_host_platform_device_count=4 Perhaps you meant to specify these on the TF_XLA_FLAGS envvar? zsh: IOT instruction (core dumped)  ipython ```  What jax/jaxlib version are you using? jax 0.3.24, jaxlib 0.3.24  Which accelerator(s) are you using? CPU/GPU  Additional system info _No response_  NVIDIA GPU info _No response_",2022-11-07T18:19:34Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/13148,"I think this is a flag that can't be set at runtime, but must be set as an environment variable. So, for example this will work: ```bash $ XLA_FLAGS=""xla_force_host_platform_device_count=4"" python c ""import jax; print(jax.devices())"" [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3)] ```","thank you, indeed that's the case... sorry for the question."
462,"以下是一个github上的jax下的一个issue, 标题是(Check for `ArrayImpl` rather than `sharding` because this code is supposed to check for concrete Array until a `shard_like` primitive exists.)， 内容是 (Check for `ArrayImpl` rather than `sharding` because this code is supposed to check for concrete Array until a `shard_like` primitive exists.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Check for `ArrayImpl` rather than `sharding` because this code is supposed to check for concrete Array until a `shard_like` primitive exists.,Check for `ArrayImpl` rather than `sharding` because this code is supposed to check for concrete Array until a `shard_like` primitive exists.,2022-11-07T15:45:17Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13143
892,"以下是一个github上的jax下的一个issue, 标题是(ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available)， 内容是 ( Description ``` ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8, num_partitions=1) ``` I'm trying to execute this notebook on Kaggle, which is an introduction to JAX. While trying to parallelize the augmentation on multiple TPU instances on line 24, I came across the abovementioned error. I would love to be able to understand why this happened, and what I can do to fix it.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info Windows 11  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available"," Description ``` ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8, num_partitions=1) ``` I'm trying to execute this notebook on Kaggle, which is an introduction to JAX. While trying to parallelize the augmentation on multiple TPU instances on line 24, I came across the abovementioned error. I would love to be able to understand why this happened, and what I can do to fix it.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info Windows 11  NVIDIA GPU info _No response_",2022-11-06T19:39:48Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/13137,What does `jax.devices()` show? My guess is that you aren't using a TPU machine (a machine with 8 GPUs would work also).
2111,"以下是一个github上的jax下的一个issue, 标题是(Cannot use grad with sparse matrix multiplication)， 内容是 ( Description I'm working on an optimization problem over a large sparse system. When the (sparse BCOO) matrix describing the system is created by addition or simply constructed with appropriate indices and values, there is no problem. However, the most natural way of composing the matrix is by multiplying other sparse matrices, which causes evaluating the gradient to fail. Below is a minimal example ``` import jax import jax.numpy as jnp from jax.experimental.sparse import BCOO from jax import grad def f1(k):  test grad with sparse matrix mult. grad_k ^2 	n = jnp.size(k) 	D = BCOO((k, jnp.tile(jnp.arange(n), jnp.array([2,1])).T), shape=(n, n)) 	G = BCOO((jnp.hstack([jnp.ones(n), 1*jnp.ones(n1)]), jnp.vstack([jnp.hstack([jnp.arange(0, n), jnp.arange(1, n)]), jnp.hstack([jnp.arange(0, n), jnp.arange(0, n1)])]).T), shape=(n,n)) 	A = D @ G 	print(A.todense()) 	b = jnp.ones(n) 	x = A @ b 	return jnp.linalg.norm(x)**2 df1 = grad(f1) df2 = grad(f2) K0 = jnp.array([5., 10, 15, 20, 25]) print(""f1(K0): "", f1(K0)) print(""f2(K0): "", f2(K0)) print(""grad_f1(K0): "", df1(K0)) print(""grad_f2(K0): "", df2(K0)) ``` In this toy example, evaluating both f1 and f2 is fine: > f1(K0):  25.0 > f2(K0):  25.0 Calculating the gradient of f1 is fine: > grad_f1(K0):  [10.  0.  0.  0.  0.] Calculating the gradient of f2 fails with the underlying issue:  > jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Transpose rule (for reversemode differentiation) for 'bcoo_spdot_general' not implemented Is there any plans to extend jax sparse support? Otherwise, is there any better idea than trying to do the actual matrix construction in using the scipy sparse implementation and then defining a custom_vjp for it?  What jax/jaxlib version are you using? v0.3.14  Which accelerator(s) are you using? CPU  Additional system info Mac M1  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot use grad with sparse matrix multiplication," Description I'm working on an optimization problem over a large sparse system. When the (sparse BCOO) matrix describing the system is created by addition or simply constructed with appropriate indices and values, there is no problem. However, the most natural way of composing the matrix is by multiplying other sparse matrices, which causes evaluating the gradient to fail. Below is a minimal example ``` import jax import jax.numpy as jnp from jax.experimental.sparse import BCOO from jax import grad def f1(k):  test grad with sparse matrix mult. grad_k ^2 	n = jnp.size(k) 	D = BCOO((k, jnp.tile(jnp.arange(n), jnp.array([2,1])).T), shape=(n, n)) 	G = BCOO((jnp.hstack([jnp.ones(n), 1*jnp.ones(n1)]), jnp.vstack([jnp.hstack([jnp.arange(0, n), jnp.arange(1, n)]), jnp.hstack([jnp.arange(0, n), jnp.arange(0, n1)])]).T), shape=(n,n)) 	A = D @ G 	print(A.todense()) 	b = jnp.ones(n) 	x = A @ b 	return jnp.linalg.norm(x)**2 df1 = grad(f1) df2 = grad(f2) K0 = jnp.array([5., 10, 15, 20, 25]) print(""f1(K0): "", f1(K0)) print(""f2(K0): "", f2(K0)) print(""grad_f1(K0): "", df1(K0)) print(""grad_f2(K0): "", df2(K0)) ``` In this toy example, evaluating both f1 and f2 is fine: > f1(K0):  25.0 > f2(K0):  25.0 Calculating the gradient of f1 is fine: > grad_f1(K0):  [10.  0.  0.  0.  0.] Calculating the gradient of f2 fails with the underlying issue:  > jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Transpose rule (for reversemode differentiation) for 'bcoo_spdot_general' not implemented Is there any plans to extend jax sparse support? Otherwise, is there any better idea than trying to do the actual matrix construction in using the scipy sparse implementation and then defining a custom_vjp for it?  What jax/jaxlib version are you using? v0.3.14  Which accelerator(s) are you using? CPU  Additional system info Mac M1  NVIDIA GPU info _No response_",2022-11-04T19:38:09Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/13118,"Thanks for the question – the transpose rule (i.e. reversemode autodiff) for sparsesparse matrix multiplication hasn't yet been implemented: https://github.com/google/jax/blob/2ce7eb5b5cd724588dfdf6b78760905579621d85/jax/experimental/sparse/bcoo.pyL1289 It's actually quite a complicated operation, which is why I haven't gotten around to implementing it yet. In the meantime, as long as you stick with forwardmode autodiff it should work: ```python df2 = jax.jacfwd(f2) print(""grad_f2(K0): "", df2(K0)) ``` ``` grad_f2(K0):  [10.  0.  0.  0.  0.] ```","Hello! Any chance of a fix for this issue? I'm facing this in a setting where it would be a real pain to pay the extra cost of forwardmode autodiff (one scalar output and several hundred inputs). Thanks! Update: I found a workaround. I only need sparsedense matrixvector products, and using the `sparsify` transform with one sparse and one dense input seems to work, so I don't think this issue blocks us.",Hi  glad you solved it. Yes sparsedense matmul has fullyimplemented autodiff. It's only reversemode sparsesparse matmul that remains unimplemented.
814,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: module 'jax.scipy' has no attribute 'optimize')， 内容是 (Hello, I am trying to use the minimize functionality described here: https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.optimize.minimize.html . However, when I try to do this, for example as follows: `m = jax.scipy.optimize.minimize(jax.jit(NLL), jnp.array([1.0,0.0]), method=""BFGS"")` I keep getting the error: AttributeError: module 'jax.scipy' has no attribute 'optimize' My python version is 3.9.   What jax/jaxlib version are you using? jax v0.3.23  Which accelerator(s) are you using? CPU  Additional system info python v3.9.0  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AttributeError: module 'jax.scipy' has no attribute 'optimize',"Hello, I am trying to use the minimize functionality described here: https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.optimize.minimize.html . However, when I try to do this, for example as follows: `m = jax.scipy.optimize.minimize(jax.jit(NLL), jnp.array([1.0,0.0]), method=""BFGS"")` I keep getting the error: AttributeError: module 'jax.scipy' has no attribute 'optimize' My python version is 3.9.   What jax/jaxlib version are you using? jax v0.3.23  Which accelerator(s) are you using? CPU  Additional system info python v3.9.0  NVIDIA GPU info _No response_",2022-11-04T13:25:03Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/13110,Resolved by doing an 'import jax.scipy.optimize' first. 
2918,"以下是一个github上的jax下的一个issue, 标题是(add a basic prototype of piles, behind jax_dynamic_shapes)， 内容是 (Basic goals: 1. **`vmap`/`scan`/etc + dynamic shapes closure:** With dynamic shapes we have to at least think about raggedness, since e.g. `vmap(jnp.arange)(jnp.array([3, 1, 4]))` must produce something like a ragged result. Less toy examples might come up in e.g. differentiating a `scan` where the body has shapes which depend on the scannedover input and thus ragged residuals are needed. 2. **Better expressiveness:** Being able to efficiently batch over e.g. sequences of different lengths, or arrays of different sizes, would be pretty handy. 3. **Let numpy stay rectangular:** Let's not try to change or extend the NumPy API, for which rectangularity is deeply ingrained. Instead, let's be minimal with what can operate on ragged data types: just one map function (plus introduction/elimination forms). 4. **Don't touch jaxprs:** Changing jaxprs is so heavyweight! All jaxpr producers and consumers need to be updated. In contrast, things like pytrees and transforms (like vmap) have so much more leverage. Let's keep ragged data types out of jaxprs, keeping jaxprs all about rectangular shapes, and instead lower away ragged representations as early as possible. 13 here are design goals. The fourth is an implementation goal. We may ultimately support both padded and concatenated physical representations, though this PR focuses on concatenated representations. We could involve padding via bints. This PR adds some extremely basic support and tests. It's just a prototype, undocumented and only for jax devs at the moment. It also hardcodes a few special cases which we may want to relax. In particular, we: 1. Add a `PileTy` representing the type of a ""pile"" of arrays of different sizes. We can write them like `a:N => elt_ty` where `a` is a binder, `N` is an integer representing a length, and `elt_ty` is an element type, which could be another pile or could be a `ShapedArray` like `f32[k1, k2, ...]`, where in addition to being `int`s or `bint`s (or variables with those types) the `k` can also be indexing expressions of the form `arr.a` for a pilebound variable `a` and an array `arr`. 2. Add a `Pile` data type for ""toplevel"" ragged values, which is just a `PileTy` paired with an array representing the actual data (e.g. concatenated). 3. Generalize `vmap` internals so that instead of batch dims being an `Optional[int]`, there is a third arm called `ConcatAxis` which represents arrays of (potentially) different sizes along that axis being concatenated together (so `ConcatAxis` stores segment lengths along with it). That includes generalizing some `vmap` rules (though only `reduce_sum` and `dot_general`, while a lot of elementwise primitives Just Work).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"add a basic prototype of piles, behind jax_dynamic_shapes","Basic goals: 1. **`vmap`/`scan`/etc + dynamic shapes closure:** With dynamic shapes we have to at least think about raggedness, since e.g. `vmap(jnp.arange)(jnp.array([3, 1, 4]))` must produce something like a ragged result. Less toy examples might come up in e.g. differentiating a `scan` where the body has shapes which depend on the scannedover input and thus ragged residuals are needed. 2. **Better expressiveness:** Being able to efficiently batch over e.g. sequences of different lengths, or arrays of different sizes, would be pretty handy. 3. **Let numpy stay rectangular:** Let's not try to change or extend the NumPy API, for which rectangularity is deeply ingrained. Instead, let's be minimal with what can operate on ragged data types: just one map function (plus introduction/elimination forms). 4. **Don't touch jaxprs:** Changing jaxprs is so heavyweight! All jaxpr producers and consumers need to be updated. In contrast, things like pytrees and transforms (like vmap) have so much more leverage. Let's keep ragged data types out of jaxprs, keeping jaxprs all about rectangular shapes, and instead lower away ragged representations as early as possible. 13 here are design goals. The fourth is an implementation goal. We may ultimately support both padded and concatenated physical representations, though this PR focuses on concatenated representations. We could involve padding via bints. This PR adds some extremely basic support and tests. It's just a prototype, undocumented and only for jax devs at the moment. It also hardcodes a few special cases which we may want to relax. In particular, we: 1. Add a `PileTy` representing the type of a ""pile"" of arrays of different sizes. We can write them like `a:N => elt_ty` where `a` is a binder, `N` is an integer representing a length, and `elt_ty` is an element type, which could be another pile or could be a `ShapedArray` like `f32[k1, k2, ...]`, where in addition to being `int`s or `bint`s (or variables with those types) the `k` can also be indexing expressions of the form `arr.a` for a pilebound variable `a` and an array `arr`. 2. Add a `Pile` data type for ""toplevel"" ragged values, which is just a `PileTy` paired with an array representing the actual data (e.g. concatenated). 3. Generalize `vmap` internals so that instead of batch dims being an `Optional[int]`, there is a third arm called `ConcatAxis` which represents arrays of (potentially) different sizes along that axis being concatenated together (so `ConcatAxis` stores segment lengths along with it). That includes generalizing some `vmap` rules (though only `reduce_sum` and `dot_general`, while a lot of elementwise primitives Just Work).",2022-11-04T06:17:39Z,pull ready,closed,2,0,https://github.com/jax-ml/jax/issues/13108
2241,"以下是一个github上的jax下的一个issue, 标题是(Wrong output for flax.linen.Conv + jax.pure_callback in jit-ed function on GPU)， 内容是 ( Description Not quite sure if this is a Flax or a JAX issue. I filed the same bug also here; see comments therein. On multiGPU (I'm using 8 GPUs), the `linen.Conv` layer gives wrong outputs in a jited function where the parameters are selected within a `jax.pure_callback` function. The error does not replicate on CPU, and it does not appear without `jax.pure_callback`. I paste here a minimal code snippet reproducing the issue. ``` from typing import Any import jax.numpy as jnp from flax import linen as nn from jax import random, pure_callback, jit class Model(nn.Module):     output_dim: int = 10     dtype: Any = jnp.float32     .compact     def __call__(self, x):         x = nn.Conv(             features=6,             kernel_size=(5, 5),             strides=(1, 1),             padding=""valid"",             dtype=self.dtype,         )(x)         x = nn.Dense(features=self.output_dim, dtype=self.dtype)(x)         return x if __name__ == ""__main__"":     model = Model(10)     dict_params = [model.init(random.PRNGKey(0), jnp.zeros((1, 28, 28, 1))),                    model.init(random.PRNGKey(1), jnp.zeros((1, 28, 28, 1)))]     def fun():         params = pure_callback(lambda i: dict_params[i], dict_params[0], 0)         return model.apply(params, jnp.ones((1024, 28, 28, 1)))     print(jit(fun)().mean(), jit(fun)().mean())      0.13688925 0.13688925     print(fun().mean(), fun().mean())      0.079618014 0.079618014 ``` The correct output is `0.079618014`. However, when the function is jitted, this gives the wrong output `0.13688925`. I've got no idea why this is happening, but this is currently hindering my development. Could you please look into it? Thank you very much!  What jax/jaxlib version are you using? jax = ""^0.3.23"" jaxlib = {url = ""https://storage.googleapis.com/jaxreleases/cuda11/jaxlib0.3.18+cuda11.cudnn805cp39cp39manylinux2014_x86_64.whl""}  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Wrong output for flax.linen.Conv + jax.pure_callback in jit-ed function on GPU," Description Not quite sure if this is a Flax or a JAX issue. I filed the same bug also here; see comments therein. On multiGPU (I'm using 8 GPUs), the `linen.Conv` layer gives wrong outputs in a jited function where the parameters are selected within a `jax.pure_callback` function. The error does not replicate on CPU, and it does not appear without `jax.pure_callback`. I paste here a minimal code snippet reproducing the issue. ``` from typing import Any import jax.numpy as jnp from flax import linen as nn from jax import random, pure_callback, jit class Model(nn.Module):     output_dim: int = 10     dtype: Any = jnp.float32     .compact     def __call__(self, x):         x = nn.Conv(             features=6,             kernel_size=(5, 5),             strides=(1, 1),             padding=""valid"",             dtype=self.dtype,         )(x)         x = nn.Dense(features=self.output_dim, dtype=self.dtype)(x)         return x if __name__ == ""__main__"":     model = Model(10)     dict_params = [model.init(random.PRNGKey(0), jnp.zeros((1, 28, 28, 1))),                    model.init(random.PRNGKey(1), jnp.zeros((1, 28, 28, 1)))]     def fun():         params = pure_callback(lambda i: dict_params[i], dict_params[0], 0)         return model.apply(params, jnp.ones((1024, 28, 28, 1)))     print(jit(fun)().mean(), jit(fun)().mean())      0.13688925 0.13688925     print(fun().mean(), fun().mean())      0.079618014 0.079618014 ``` The correct output is `0.079618014`. However, when the function is jitted, this gives the wrong output `0.13688925`. I've got no idea why this is happening, but this is currently hindering my development. Could you please look into it? Thank you very much!  What jax/jaxlib version are you using? jax = ""^0.3.23"" jaxlib = {url = ""https://storage.googleapis.com/jaxreleases/cuda11/jaxlib0.3.18+cuda11.cudnn805cp39cp39manylinux2014_x86_64.whl""}  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_",2022-11-03T21:04:21Z,bug NVIDIA GPU,closed,0,6,https://github.com/jax-ml/jax/issues/13098,Thanks for raising this. Sorry for the numerics bug. I wonder if https://github.com/google/jax/pull/13084 will fix this.," could you try working from github HEAD, and patching that PR? Actually you may be able to do `pip install upgrade git+https://github.com/google/jax.git` to install jax from that branch.","Late to the party, I had made a pure jax repro: ```python import jax import jax.numpy as jnp from jax import random, pure_callback, jit, lax keys = random.split(random.PRNGKey(1), 2) dict_params = [{'c_w': random.normal(keys[0], (5, 5, 1, 6))},                {'c_w': random.normal(keys[1], (5, 5, 1, 6))}] dn = lax.conv_dimension_numbers((1024, 28, 28, 1),                                 (5, 5, 1, 6),                                 ('NHWC', 'HWIO', 'NHWC')) def test_fn(params, x):   y = lax.conv_general_dilated(x,                                params['c_w'],                                 window_strides=(1,1),                                padding='valid',                                lhs_dilation=(1,1),                                rhs_dilation=(1,1),                                dimension_numbers=dn,                                feature_group_count=1,                                batch_group_count=1)   return y def test_case():      causes numerical difference when jitted:     params = pure_callback(lambda i: dict_params[i], dict_params[0], 0)      no numerical difference when jitted:      params = dict_params[0]     return test_fn(params, jnp.ones((1024, 28, 28, 1))) print(jnp.mean(jnp.abs(jit(test_case)()  test_case())))   6.0820947  variant: explicitly passed input leads to a tracer error at pure_callback: def test_case_2(dict_params):     params = pure_callback(lambda i: dict_params[i], dict_params[0], 0)     return test_fn(params, jnp.ones((1024, 28, 28, 1))) print(jnp.mean(jnp.abs(jit(test_case_2)(dict_params)  test_case_2(dict_params))))    > TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object ``` The latter error confused me  aren't we supposed to be able to trace through a pure_callback?"," I think the error makes sense, you are passing a tracer into the lambda via a capture, it should be an input to `pure_callback` instead: ```python def test_case_2(dict_params):     params = pure_callback(lambda ds, i: ds[i], dict_params[0], dict_params, 0)     return test_fn(params, jnp.ones((1024, 28, 28, 1))) ``` I ran it and it works.  Complete Code ```python import jax import jax.numpy as jnp from jax import random, pure_callback, jit, lax keys = random.split(random.PRNGKey(1), 2) dict_params = [{'c_w': random.normal(keys[0], (5, 5, 1, 6))},                {'c_w': random.normal(keys[1], (5, 5, 1, 6))}] dn = lax.conv_dimension_numbers((1024, 28, 28, 1),                                 (5, 5, 1, 6),                                 ('NHWC', 'HWIO', 'NHWC')) def test_fn(params, x):   y = lax.conv_general_dilated(x,                                params['c_w'],                                 window_strides=(1,1),                                padding='valid',                                lhs_dilation=(1,1),                                rhs_dilation=(1,1),                                dimension_numbers=dn,                                feature_group_count=1,                                batch_group_count=1)   return y def test_case():      causes numerical difference when jitted:     params = pure_callback(lambda i: dict_params[i], dict_params[0], 0)      no numerical difference when jitted:      params = dict_params[0]     return test_fn(params, jnp.ones((1024, 28, 28, 1))) print(jnp.mean(jnp.abs(jit(test_case)()  test_case())))   6.0820947  variant: explicitly passed input leads to a tracer error at pure_callback: def test_case_2(dict_params):     params = pure_callback(lambda ds, i: ds[i], dict_params[0], dict_params, 0)     return test_fn(params, jnp.ones((1024, 28, 28, 1))) print(jnp.mean(jnp.abs(jit(test_case_2)(dict_params)  test_case_2(dict_params))))   ``` ",The other version of Jax appears to fix the issue: ip install upgrade git+https://github.com/google/jax.git https://colab.research.google.com/drive/1k9qynITvJLgHgjKWam8yfLLOwXh5mEm1?usp=sharing,This was fixed by the new version. Closing the issue.
4730,"以下是一个github上的jax下的一个issue, 标题是(Fix issue in `check_tree`)， 内容是 (I think this is an untested issue in jax, which arises if you use sparse solvers with `has_aux=True`. See https://github.com/google/jax/issues/13092 for more info . I am adding here the following MWE that shows that without this PR it is impossible to implement solvers using `lax.custom_linear_solve` that use `has_aux=True`. The MWE simply redefines `_isolve` and `cg` to return a dumb auxiliary structure. ```python  from jax._src.scipy.sparse.linalg import (_identity, _normalize_matvec, _shapes, _cg_solve) from functools import partial import numpy as np import jax.numpy as jnp from jax import device_put from jax import lax from jax.tree_util import (tree_leaves, tree_map, tree_structure) from jax._src.util import safe_map as map def _isolve(_isolve_solve, A, b, x0=None, *, tol=1e5, atol=0.0,             maxiter=None, M=None, check_symmetric=False, has_aux=False):   if x0 is None:     x0 = tree_map(jnp.zeros_like, b)   b, x0 = device_put((b, x0))   if maxiter is None:     size = sum(bi.size for bi in tree_leaves(b))     maxiter = 10 * size   copied from scipy   if M is None:     M = _identity   A = _normalize_matvec(A)   M = _normalize_matvec(M)   if tree_structure(x0) != tree_structure(b):     raise ValueError(         'x0 and b must have matching tree structure: '         f'{tree_structure(x0)} vs {tree_structure(b)}')   if _shapes(x0) != _shapes(b):     raise ValueError(         'arrays in x0 and b must have matching shapes: '         f'{_shapes(x0)} vs {_shapes(b)}')   isolve_solve = partial(       _isolve_solve, x0=x0, tol=tol, atol=atol, maxiter=maxiter, M=M)    realvalued positivedefinite linear operators are symmetric   def real_valued(x):     return not issubclass(x.dtype.type, np.complexfloating)   symmetric = all(map(real_valued, tree_leaves(b))) \     if check_symmetric else False   return lax.custom_linear_solve(       A, b, solve=isolve_solve, transpose_solve=isolve_solve,       symmetric=symmetric, has_aux=has_aux) def _cg_solve_with_aux(*args, **kwargs):   x_final = _cg_solve(*args, **kwargs)    additional info output structure   info = {'error': 0.03, 'niter': 1}   return x_final, info def cg_with_aux(A, b, x0=None, *, tol=1e5, atol=0.0, maxiter=None, M=None):   return _isolve(_cg_solve_with_aux,                  A=A, b=b, x0=x0, tol=tol, atol=atol,                  maxiter=maxiter, M=M, check_symmetric=True, has_aux=True) import jax fun = lambda x: jax.tree_map(lambda y: y*y, x) pt = {'a': 1, 'b':jnp.ones(3)+1} cg_with_aux(fun, pt) ``` on master it gives the following error: ```python  TypeError                                 Traceback (most recent call last) Input In [1], in ()      66 fun = lambda x: jax.tree_map(lambda y: y*y, x)      67 pt = {'a': 1, 'b':jnp.ones(3)+1} > 68 cg_with_aux(fun, pt) Input In [1], in cg_with_aux(A, b, x0, tol, atol, maxiter, M)      58 def cg_with_aux(A, b, x0=None, *, tol=1e5, atol=0.0, maxiter=None, M=None): > 59   return _isolve(_cg_solve_with_aux,      60                  A=A, b=b, x0=x0, tol=tol, atol=atol,      61                  maxiter=maxiter, M=M, check_symmetric=True, has_aux=True) Input In [1], in _isolve(_isolve_solve, A, b, x0, tol, atol, maxiter, M, check_symmetric, has_aux)      44   return not issubclass(x.dtype.type, np.complexfloating)      45 symmetric = all(map(real_valued, tree_leaves(b))) \      46   if check_symmetric else False > 47 return lax.custom_linear_solve(      48     A, b, solve=isolve_solve, transpose_solve=isolve_solve,      49     symmetric=symmetric, has_aux=has_aux)     [... skipping hidden 2 frame] File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/lax/control_flow/common.py:125, in _check_tree(func_name, expected_name, actual_tree, expected_tree, has_aux)     119     raise ValueError(     120       f""{func_name}() produced a pytree with structure ""     121       f""{actual_tree}, but a pytree tuple with auxiliary ""     122       f""output was expected because has_aux was set to True."")     124 if actual_tree != expected_tree: > 125   raise TypeError(     126       f""{func_name}() output pytree structure must match {expected_name}, ""     127       f""got {actual_tree} and {expected_tree}."") TypeError: solve() output pytree structure must match b, got PyTreeDef(*) and PyTreeDef({'a': *, 'b': *}). ``` with the changes in this PR the snippet above runs fine: ```python Out[1]: ({'a': DeviceArray(3.5198984, dtype=float32),   'b': DeviceArray([4.0182953, 4.0182953, 4.0182953], dtype=float32)},  {'error': 0.03, 'niter': 1}) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Fix issue in `check_tree`,"I think this is an untested issue in jax, which arises if you use sparse solvers with `has_aux=True`. See https://github.com/google/jax/issues/13092 for more info . I am adding here the following MWE that shows that without this PR it is impossible to implement solvers using `lax.custom_linear_solve` that use `has_aux=True`. The MWE simply redefines `_isolve` and `cg` to return a dumb auxiliary structure. ```python  from jax._src.scipy.sparse.linalg import (_identity, _normalize_matvec, _shapes, _cg_solve) from functools import partial import numpy as np import jax.numpy as jnp from jax import device_put from jax import lax from jax.tree_util import (tree_leaves, tree_map, tree_structure) from jax._src.util import safe_map as map def _isolve(_isolve_solve, A, b, x0=None, *, tol=1e5, atol=0.0,             maxiter=None, M=None, check_symmetric=False, has_aux=False):   if x0 is None:     x0 = tree_map(jnp.zeros_like, b)   b, x0 = device_put((b, x0))   if maxiter is None:     size = sum(bi.size for bi in tree_leaves(b))     maxiter = 10 * size   copied from scipy   if M is None:     M = _identity   A = _normalize_matvec(A)   M = _normalize_matvec(M)   if tree_structure(x0) != tree_structure(b):     raise ValueError(         'x0 and b must have matching tree structure: '         f'{tree_structure(x0)} vs {tree_structure(b)}')   if _shapes(x0) != _shapes(b):     raise ValueError(         'arrays in x0 and b must have matching shapes: '         f'{_shapes(x0)} vs {_shapes(b)}')   isolve_solve = partial(       _isolve_solve, x0=x0, tol=tol, atol=atol, maxiter=maxiter, M=M)    realvalued positivedefinite linear operators are symmetric   def real_valued(x):     return not issubclass(x.dtype.type, np.complexfloating)   symmetric = all(map(real_valued, tree_leaves(b))) \     if check_symmetric else False   return lax.custom_linear_solve(       A, b, solve=isolve_solve, transpose_solve=isolve_solve,       symmetric=symmetric, has_aux=has_aux) def _cg_solve_with_aux(*args, **kwargs):   x_final = _cg_solve(*args, **kwargs)    additional info output structure   info = {'error': 0.03, 'niter': 1}   return x_final, info def cg_with_aux(A, b, x0=None, *, tol=1e5, atol=0.0, maxiter=None, M=None):   return _isolve(_cg_solve_with_aux,                  A=A, b=b, x0=x0, tol=tol, atol=atol,                  maxiter=maxiter, M=M, check_symmetric=True, has_aux=True) import jax fun = lambda x: jax.tree_map(lambda y: y*y, x) pt = {'a': 1, 'b':jnp.ones(3)+1} cg_with_aux(fun, pt) ``` on master it gives the following error: ```python  TypeError                                 Traceback (most recent call last) Input In [1], in ()      66 fun = lambda x: jax.tree_map(lambda y: y*y, x)      67 pt = {'a': 1, 'b':jnp.ones(3)+1} > 68 cg_with_aux(fun, pt) Input In [1], in cg_with_aux(A, b, x0, tol, atol, maxiter, M)      58 def cg_with_aux(A, b, x0=None, *, tol=1e5, atol=0.0, maxiter=None, M=None): > 59   return _isolve(_cg_solve_with_aux,      60                  A=A, b=b, x0=x0, tol=tol, atol=atol,      61                  maxiter=maxiter, M=M, check_symmetric=True, has_aux=True) Input In [1], in _isolve(_isolve_solve, A, b, x0, tol, atol, maxiter, M, check_symmetric, has_aux)      44   return not issubclass(x.dtype.type, np.complexfloating)      45 symmetric = all(map(real_valued, tree_leaves(b))) \      46   if check_symmetric else False > 47 return lax.custom_linear_solve(      48     A, b, solve=isolve_solve, transpose_solve=isolve_solve,      49     symmetric=symmetric, has_aux=has_aux)     [... skipping hidden 2 frame] File ~/Documents/pythonenvs/netket/python3.10.6/lib/python3.10/sitepackages/jax/_src/lax/control_flow/common.py:125, in _check_tree(func_name, expected_name, actual_tree, expected_tree, has_aux)     119     raise ValueError(     120       f""{func_name}() produced a pytree with structure ""     121       f""{actual_tree}, but a pytree tuple with auxiliary ""     122       f""output was expected because has_aux was set to True."")     124 if actual_tree != expected_tree: > 125   raise TypeError(     126       f""{func_name}() output pytree structure must match {expected_name}, ""     127       f""got {actual_tree} and {expected_tree}."") TypeError: solve() output pytree structure must match b, got PyTreeDef(*) and PyTreeDef({'a': *, 'b': *}). ``` with the changes in this PR the snippet above runs fine: ```python Out[1]: ({'a': DeviceArray(3.5198984, dtype=float32),   'b': DeviceArray([4.0182953, 4.0182953, 4.0182953], dtype=float32)},  {'error': 0.03, 'niter': 1}) ```",2022-11-03T18:18:08Z,kokoro:force-run pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/13093," sorry for pinging you, I know you don't officially support iterative solvers with custom auxiliary fields,  but as of now `jax.lax.custom_lienar_solver` is broken and prevents implementing them. This PR fixes it and at least allows users to implement them...",Thanks!  I came up with a test that breaks on master otherwise. ,Squashed and formatted
2607,"以下是一个github上的jax下的一个issue, 标题是(Assertion error when trying to take jacobian of projected gradient solution using projection_polyhedron)， 内容是 ( Description The output of compute_pg is a vector . If I just evaluate f_x below at the desired input it works but the jacobian doesnt seem to work. I am not sure what is happening here. ``` (jit, static_argnums=(0,))  def proj(self,p,C):      return projection_polyhedron(p,C,check_feasible = False) (jit, static_argnums=(0,)) def compute_pg(self,p,cx,cy,x_obs_inp,y_obs_inp,vx_obs_inp, vy_obs_inp):         p = jnp.reshape(p,(jnp.shape(p)[0],1))         b_obs = jnp.hstack((x_obs_inp+self.p_obs_x_ub,(x_obs_inpself.p_obs_x_lb),                                         y_obs_inp+self.p_obs_y_ub,(y_obs_inpself.p_obs_y_lb)))         b_obs = jnp.reshape(b_obs,(jnp.shape(self.C_obs)[0],1))         pg = ProjectedGradient(fun=self.compute_obstacle_penalty_temp,projection=self.proj,jit=True)         pg_sol = pg.run(p,hyperparams_proj=(self.A_obs,self.a_obstacle,self.C_obs,b_obs),c_x=cx,c_y=cy,vx_obs_inp=vx_obs_inp,vy_obs_inp=vy_obs_inp).params         return pg_sol  f_x = lambda cx: self.compute_pg(p[0,:],cx,best_cy.T,x_obs_inp[0,:],y_obs_inp[0,:],vx_obs_inp[0,:], vy_obs_inp[0,:]) dp_dcx = jax.jacobian(f_x)(best_cx.T) ``` This is the error I get:   ``` File ""/home/ims/ros2_ws/install/mpc_python/lib/python3.8/sitepackages/mpc_python/mpc_expert_bilevel.py"", line 592, in compute_bilevel     dp_dcx = jax.jacobian(f_x)(best_cx.T)   File ""/home/ims/.local/lib/python3.8/sitepackages/jaxopt/_src/implicit_diff.py"", line 236, in solver_fun_bwd     vjps = root_vjp(optimality_fun=optimality_fun, sol=sol,   File ""/home/ims/.local/lib/python3.8/sitepackages/jaxopt/_src/implicit_diff.py"", line 69, in root_vjp     u = solve(matvec, v)   File ""/home/ims/.local/lib/python3.8/sitepackages/jaxopt/_src/linear_solve.py"", line 193, in solve_normal_cg     Ab = rmatvec(b)   A.T b   File ""/home/ims/.local/lib/python3.8/sitepackages/jaxopt/_src/linear_solve.py"", line 145, in      return lambda y: transpose(y)[0] AssertionError ``` What I observed was that if I set implicit_diff=False in the ProjectedGradient then it works but is super slow.Kindly advice.   What jax/jaxlib version are you using? 0.3.22  Which accelerator(s) are you using? GPU  Additional system info 3.8.10,Ubuntu 20.04.4 LTS,1th Gen Intel® Core™ i911980HK @ 2.60GHz × 16,NVIDIA Corporation / NVIDIA GeForce RTX 3080 Laptop GPU/PCIe/SSE2  NVIDIA GPU info +  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Assertion error when trying to take jacobian of projected gradient solution using projection_polyhedron," Description The output of compute_pg is a vector . If I just evaluate f_x below at the desired input it works but the jacobian doesnt seem to work. I am not sure what is happening here. ``` (jit, static_argnums=(0,))  def proj(self,p,C):      return projection_polyhedron(p,C,check_feasible = False) (jit, static_argnums=(0,)) def compute_pg(self,p,cx,cy,x_obs_inp,y_obs_inp,vx_obs_inp, vy_obs_inp):         p = jnp.reshape(p,(jnp.shape(p)[0],1))         b_obs = jnp.hstack((x_obs_inp+self.p_obs_x_ub,(x_obs_inpself.p_obs_x_lb),                                         y_obs_inp+self.p_obs_y_ub,(y_obs_inpself.p_obs_y_lb)))         b_obs = jnp.reshape(b_obs,(jnp.shape(self.C_obs)[0],1))         pg = ProjectedGradient(fun=self.compute_obstacle_penalty_temp,projection=self.proj,jit=True)         pg_sol = pg.run(p,hyperparams_proj=(self.A_obs,self.a_obstacle,self.C_obs,b_obs),c_x=cx,c_y=cy,vx_obs_inp=vx_obs_inp,vy_obs_inp=vy_obs_inp).params         return pg_sol  f_x = lambda cx: self.compute_pg(p[0,:],cx,best_cy.T,x_obs_inp[0,:],y_obs_inp[0,:],vx_obs_inp[0,:], vy_obs_inp[0,:]) dp_dcx = jax.jacobian(f_x)(best_cx.T) ``` This is the error I get:   ``` File ""/home/ims/ros2_ws/install/mpc_python/lib/python3.8/sitepackages/mpc_python/mpc_expert_bilevel.py"", line 592, in compute_bilevel     dp_dcx = jax.jacobian(f_x)(best_cx.T)   File ""/home/ims/.local/lib/python3.8/sitepackages/jaxopt/_src/implicit_diff.py"", line 236, in solver_fun_bwd     vjps = root_vjp(optimality_fun=optimality_fun, sol=sol,   File ""/home/ims/.local/lib/python3.8/sitepackages/jaxopt/_src/implicit_diff.py"", line 69, in root_vjp     u = solve(matvec, v)   File ""/home/ims/.local/lib/python3.8/sitepackages/jaxopt/_src/linear_solve.py"", line 193, in solve_normal_cg     Ab = rmatvec(b)   A.T b   File ""/home/ims/.local/lib/python3.8/sitepackages/jaxopt/_src/linear_solve.py"", line 145, in      return lambda y: transpose(y)[0] AssertionError ``` What I observed was that if I set implicit_diff=False in the ProjectedGradient then it works but is super slow.Kindly advice.   What jax/jaxlib version are you using? 0.3.22  Which accelerator(s) are you using? GPU  Additional system info 3.8.10,Ubuntu 20.04.4 LTS,1th Gen Intel® Core™ i911980HK @ 2.60GHz × 16,NVIDIA Corporation / NVIDIA GeForce RTX 3080 Laptop GPU/PCIe/SSE2  NVIDIA GPU info +  ++",2022-11-03T12:44:54Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/13089,This looks like a duplicate of CC(未找到相关数据); let's discuss it there. In the future please feel free to just ask once – the issues and discussions are both monitored by the team!
2389,"以下是一个github上的jax下的一个issue, 标题是(pjit + jax.numpy.fft.fftn is not sharding properly)， 内容是 ( Description I am trying to compute the FFT of a matrix along the rows.  Since each row is independent, `pjit(..., in_axis_resources=PartitionSpec('x', None), out_axis_resources=PartitionSpec('x', None))` should work, but it doesn't. On a 4 GPUs machine, every GPU computes the entire batch of FFTs. Repro ```python import jax from jax.experimental import maps, PartitionSpec from jax.experimental.pjit import pjit from functools import partial import numpy as np if __name__ == ""__main__"":     devices = np.asarray(jax.devices())     mesh = maps.Mesh(devices, ('x',))     with maps.Mesh(mesh.devices, mesh.axis_names):          ('x', None) means the array is distributed along the first dimension but not the second         batch_ffts = pjit(partial(jax.numpy.fft.fftn, axes=[1]), in_axis_resources=PartitionSpec('x', None), out_axis_resources=PartitionSpec('x', None))          DeviceArray         x = jax.numpy.ones((4096, 4096))         for _ in range(5):              After each batch_fft, it decomes a ShardedDeviceArray              x = batch_ffts(x)          Should be a ShardedDeviceArray here         print(type(x))         x.block_until_ready() ``` Running the code with Nsight Systems lets us examine the kernels running on each GPU ``` $ CUDA_VISIBLE_DEVICES=0,1,2,3 nsys profile python3 repro.py  $ nsys stats report gputrace report.nsysrep  Start (ns)  Duration (ns)  CorrId  GrdX  GrdY  GrdZ  BlkX  BlkY  BlkZ  Reg/Trd  StcSMem (MB)  DymSMem (MB)  Bytes (MB)  Throughput (MBps)  SrcMemKd  DstMemKd           Device            Ctx  Strm         Name                                          Many more lines  6699839655         161344   13477  4096  1     1     256   1     1     48       0.000         0.017               None                     None      None      NVIDIA A100SXM480GB (2)    1    66  void vector_fft  What jax/jaxlib version are you using? jax v0.3.23, jaxlib v0.3.22+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Linux, CUDA (driver) 11.8, NVIDIA Tensorflow container nvcr.io/nvidia/tensorflow:22.09tf2py3   NVIDIA GPU info ``` $ nvidiasmi Wed Nov  2 20:28:12 2022 ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,pjit + jax.numpy.fft.fftn is not sharding properly," Description I am trying to compute the FFT of a matrix along the rows.  Since each row is independent, `pjit(..., in_axis_resources=PartitionSpec('x', None), out_axis_resources=PartitionSpec('x', None))` should work, but it doesn't. On a 4 GPUs machine, every GPU computes the entire batch of FFTs. Repro ```python import jax from jax.experimental import maps, PartitionSpec from jax.experimental.pjit import pjit from functools import partial import numpy as np if __name__ == ""__main__"":     devices = np.asarray(jax.devices())     mesh = maps.Mesh(devices, ('x',))     with maps.Mesh(mesh.devices, mesh.axis_names):          ('x', None) means the array is distributed along the first dimension but not the second         batch_ffts = pjit(partial(jax.numpy.fft.fftn, axes=[1]), in_axis_resources=PartitionSpec('x', None), out_axis_resources=PartitionSpec('x', None))          DeviceArray         x = jax.numpy.ones((4096, 4096))         for _ in range(5):              After each batch_fft, it decomes a ShardedDeviceArray              x = batch_ffts(x)          Should be a ShardedDeviceArray here         print(type(x))         x.block_until_ready() ``` Running the code with Nsight Systems lets us examine the kernels running on each GPU ``` $ CUDA_VISIBLE_DEVICES=0,1,2,3 nsys profile python3 repro.py  $ nsys stats report gputrace report.nsysrep  Start (ns)  Duration (ns)  CorrId  GrdX  GrdY  GrdZ  BlkX  BlkY  BlkZ  Reg/Trd  StcSMem (MB)  DymSMem (MB)  Bytes (MB)  Throughput (MBps)  SrcMemKd  DstMemKd           Device            Ctx  Strm         Name                                          Many more lines  6699839655         161344   13477  4096  1     1     256   1     1     48       0.000         0.017               None                     None      None      NVIDIA A100SXM480GB (2)    1    66  void vector_fft  What jax/jaxlib version are you using? jax v0.3.23, jaxlib v0.3.22+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Linux, CUDA (driver) 11.8, NVIDIA Tensorflow container nvcr.io/nvidia/tensorflow:22.09tf2py3   NVIDIA GPU info ``` $ nvidiasmi Wed Nov  2 20:28:12 2022 ++  ++ ```",2022-11-02T20:39:00Z,enhancement XLA P1 (soon) NVIDIA GPU TPU,open,1,2,https://github.com/jax-ml/jax/issues/13081,Here is the HLO dump in case that's useful. hlo.zip,Tracked internally in b/263023739
752,"以下是一个github上的jax下的一个issue, 标题是(Support for `numpy.ufunc.outer`)， 内容是 (In `numpy`, universal functions support several augmented methods, among them:  `ufunc.outer(A, B, /, **kwargs)` Apply the ufunc op to all pairs (a, b) with a in A and b in B.  `ufunc.reduce(array[, axis, dtype, out, ...])` Reduces array's dimension by one, by applying ufunc along one axis.  `ufunc.accumulate(array[, axis, dtype, out])` Accumulate the result of applying the operator to all elements. See: https://numpy.org/doc/stable/reference/ufuncs.htmlmethods. In particular, something like `add.outer(a,b)` come up surprisingly often.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Support for `numpy.ufunc.outer`,"In `numpy`, universal functions support several augmented methods, among them:  `ufunc.outer(A, B, /, **kwargs)` Apply the ufunc op to all pairs (a, b) with a in A and b in B.  `ufunc.reduce(array[, axis, dtype, out, ...])` Reduces array's dimension by one, by applying ufunc along one axis.  `ufunc.accumulate(array[, axis, dtype, out])` Accumulate the result of applying the operator to all elements. See: https://numpy.org/doc/stable/reference/ufuncs.htmlmethods. In particular, something like `add.outer(a,b)` come up surprisingly often.",2022-10-31T19:27:06Z,enhancement,closed,0,12,https://github.com/jax-ml/jax/issues/13054,"Thanks  I have a draft PR that adds this functionality here: https://github.com/google/jax/pull/9529 We haven't gone farther wilth it because it adds a lot of complexity to the codebase and it's not clear that the benefit is commensurate. In numpy, the benefit of having `outer` and other operations tied to ufuncs is that it provides a way to ensure that batched operations are lowered down to compiled loops. In JAX, we have the `vmap` transform that can do efficient batching for **any** function, not just ufuncs! For example: ```python import numpy as np import jax.numpy as jnp from jax import vmap x = np.array([3, 4, 2]) y = np.array([5, 2, 3, 6]) print(np.multiply.outer(x, y))  [[15  6  9 18]   [20  8 12 24]   [10  4  6 12]] print(vmap(vmap(jnp.multiply, (None, 0)), (0, None))(x, y))  [[15  6  9 18]   [20  8 12 24]   [10  4  6 12]] ``` In many ways, `vmap` subsumes the need to define an `outer` attribute on all individual functions, and the result is a simpler codebase with far more expressivity. What do you think?","I just ran into this as well and think it would be super neat to see `reduce` and other ufunc methods implemented in `jax`, which would ease changing over from `numpy`.  Additionally I would argue that the `numpy` syntax is more readable than wrapping functions in layers of `vmap`s. Alternatively maybe a helpful error could be added to a documentation page describing workarounds using `vmap`?",Thanks for the note  I should clean up and land my ufunc PR – I agree it would be nice to support this.,also need the functionality... ETA?,Tracked in CC(`jax.numpy` APIs should be wrapped as ufuncs by default),"I would like to add my vote to this proposal.  Numpy's `ufunc.accumulate` method is useful for recurrent equations such as when simulating a dynamic system (i.e. recursively applying a state transition function to a state vector over multiple timesteps). Simple example: ```python import numpy as np def lp_filter(x, u):     return 0.3 * x + 0.7 * u lp_filter_ufunc = np.frompyfunc(lp_filter, nin=2, nout=1) U = np.zeros(11) U[1:] = 1. Y = lp_filter_ufunc.accumulate(U) ``` I guess you could do this with a for loop in Jax. What would be really nice though, and as far as I know doesn't exist in Numpy, would be an accumulate method for general vectorized functions that takes multidimensional inputs and has a multidimensional state. I.e. doesn't broadcast over the leading dimensions specified in its signature.","We already have support for this – the general `accumulate` can be done via `jnp.frompyfunc`: ```python import jax import jax.numpy as jnp def lp_filter(x, u):     return 0.3 * x + 0.7 * u lp_filter_ufunc = jnp.frompyfunc(lp_filter, nin=2, nout=1) U = jnp.zeros(11).at[1:].set(1) Y = lp_filter_ufunc.accumulate(U) ``` Accumulate here is implemented via `scan`. For the vectorized accumulate, if I'm understanding what you have in mind, I think you can achieve that by wrapping the above function in `vmap`; for example: ```python U = jnp.arange(15).reshape(3, 5) print(jax.vmap(lp_filter_ufunc.accumulate)(U)) ``` ``` [[ 0.        0.7       1.61      2.583     3.5749  ]  [ 5.        5.7       6.61      7.583     8.5749  ]  [10.       10.7      11.61     12.582999 13.5749  ]] ```","Sorry.  I think it's because I'm using an old version of jax (0.4.14).  I got: `module 'jax.numpy' has no attribute 'frompyfunc'`. Regarding the second part, I want to simulate a system where `x` and `u` are vectors.  For example: ```python A = jnp.array([        [ 6. ,  0. ,   0. ,  5. ],        [ 0. ,  6. ,   0.5,  0. ],        [  0. , 10. ,   0. ,  0. ],        [  1. ,   0. ,   0. ,  0. ] ]) B = jnp.array([        [10.,  0.],        [ 0.,  1.],        [ 0.,  0.],        [ 0.,  0.] ]) def f(x, u):     return A.dot(x) + B.dot(u) f_vec = jnp.vectorize(f, signature='(n),(m)>(n)')   or something like this U = jnp.zeros((11, 2)).at[1:, :].set(1) X = f_vec.accumulate(U)   does not exist yet ``` ``` AttributeError: 'function' object has no attribute 'accumulate' ``` [I think what you showed above is how to simulate multiple identical systems in parallel.]","Actually, it can be done quite easily using Python's accumulate: ```python from itertools import accumulate x0 = jnp.zeros(4) U = jnp.zeros((10, 2)).at[1:, :].set(1) X = jnp.array(list(accumulate(U, f, initial=x0))) ``` Maybe I should just use this.  This will be differentiable right?","Ah, it sounds like you're asking for the JAX equivalent of this numpy feature request: https://github.com/numpy/numpy/issues/14020. I don't know of an easy API for this in JAX (you could always write a `scan` manually if you wish). Regarding the `itertools` approach: it will work and be compatible with autodiff, but will become very slow as the size of the array becomes large, because this kind of pythonside iteration is unrolled in JAX's compute graph.","Yes exactly.  I didn't know about that GUFunc proposal. That is awesome, I wish they would implement it. I suspect you are right about the size of the computation graph in my application.  It may quickly become a limiting factor (I was planning to simulate for 60 time steps and x(k) might be length 20 or so). There are better autodiff tools for simulating and optimizing dynamic systems (e.g. CasADi), which take account of the sparsity of the A, B matrices. The client of this project is familiar with Jax so I was trying to stay within their ecosystem. Thanks for the feedback and sorry this was a bit off the topic of this thread.","Since you mentioned it, I checked out `scan` and it seems ideal for this. You can even include a mapping from states to outputs: ```python import jax import jax.numpy as jnp A = jnp.array([        [ 6. ,  0. ,   0. ,  5. ],        [ 0. ,  6. ,   0.5,  0. ],        [  0. , 10. ,   0. ,  0. ],        [  1. ,   0. ,   0. ,  0. ] ]) B = jnp.array([        [10.,  0.],        [ 0.,  1.],        [ 0.,  0.],        [ 0.,  0.] ]) C = jnp.array([        [0. , 0. , 0.5,  0.5],        [ 1. , 0. , 1. ,  0.5] ]) def f(x, u):     return A.dot(x) + B.dot(u) def g(x, u):     return C.dot(x) def update(x, u):     x = f(x, u)     y = g(x, u)     return x, y x0 = jnp.zeros(4) U = jnp.zeros((10, 2)).at[1:, :].set(1) xN, Y = jax.lax.scan(update, x0, U) ```"
1345,"以下是一个github上的jax下的一个issue, 标题是(Host callback attempts to perform computation on GPU)， 内容是 ( Description I am trying to run a computation including `jax.numpy.linalg.eig` on a GPU. Since `eig` is not implemented with GPU backend, I am trying to use the experimental `host_callback` module to force the calculation to take place on the CPU. Unfortunately, it seems like an attempt is still made to run this on the GPU, which fails. This code snippet reproduces the error, for example in colab with a GPU runtime. ``` def _eig_host(matrix: jnp.ndarray) > Tuple[jnp.ndarray, jnp.ndarray]:     """"""Performs an `eig` solve on the host (CPU).""""""     eigenvalues_shape = jax.ShapeDtypeStruct(matrix.shape[:1], complex)     eigenvectors_shape = jax.ShapeDtypeStruct(matrix.shape, complex)     return hcb.call(         jnp.linalg.eig,         matrix.astype(complex),         result_shape=(eigenvalues_shape, eigenvectors_shape),     ) x = jax.random.normal(jax.random.PRNGKey(0), (3, 3)) _eig_host(x)   This fails, because `eig` is not implemented for GPU. ```  What jax/jaxlib version are you using? 0.3.23  Which accelerator(s) are you using? colab GPU  Additional system info colab  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Host callback attempts to perform computation on GPU," Description I am trying to run a computation including `jax.numpy.linalg.eig` on a GPU. Since `eig` is not implemented with GPU backend, I am trying to use the experimental `host_callback` module to force the calculation to take place on the CPU. Unfortunately, it seems like an attempt is still made to run this on the GPU, which fails. This code snippet reproduces the error, for example in colab with a GPU runtime. ``` def _eig_host(matrix: jnp.ndarray) > Tuple[jnp.ndarray, jnp.ndarray]:     """"""Performs an `eig` solve on the host (CPU).""""""     eigenvalues_shape = jax.ShapeDtypeStruct(matrix.shape[:1], complex)     eigenvectors_shape = jax.ShapeDtypeStruct(matrix.shape, complex)     return hcb.call(         jnp.linalg.eig,         matrix.astype(complex),         result_shape=(eigenvalues_shape, eigenvectors_shape),     ) x = jax.random.normal(jax.random.PRNGKey(0), (3, 3)) _eig_host(x)   This fails, because `eig` is not implemented for GPU. ```  What jax/jaxlib version are you using? 0.3.23  Which accelerator(s) are you using? colab GPU  Additional system info colab  NVIDIA GPU info _No response_",2022-10-29T19:27:54Z,bug NVIDIA GPU,open,0,4,https://github.com/jax-ml/jax/issues/13046,"OK, I was actually able to work around this issue by `jit`ing the call to `eig` and specifying CPU. There don't seem to be any downsides here, so I will close this issue. ``` def _eig_host(matrix: jnp.ndarray) > Tuple[jnp.ndarray, jnp.ndarray]:     """"""Wraps numpy.linalg.eig so that it can be jited on a machine with GPUs.""""""     eigenvalues_shape = jax.ShapeDtypeStruct(matrix.shape[:1], complex)     eigenvectors_shape = jax.ShapeDtypeStruct(matrix.shape, complex)     return host_callback.call(          We force this computation to be performed on the cpu by jiting and          explicitly specifying the device.         jax.jit(jnp.linalg.eig, device=jax.devices(""cpu"")[0]),         matrix,         result_shape=[eigenvalues_shape, eigenvectors_shape],     ) ```","the callback gives you CPUbound numpy objects, so I think the ideal thing here would be to callback into `np.linalg.eig`.","Yes, this is what I originally intended, but for my version of jax and numpy, the jax implementation was much faster. So, I wanted to find a way to use the jax implementation.",Update: `jax.numpy.linalg.eig` is now supported on GPUs with JAX version 0.4.36. See  CC(Add a GPU implementation of `lax.linalg.eig`.) for more details.
3200,"以下是一个github上的jax下的一个issue, 标题是(c++ pytree code allows unhashable metadata)， 内容是 (```python import numpy as np import jax .tree_util.register_pytree_node_class class A:   x: np.ndarray   not hashable!   def __init__(self, x):     self.x = x   def tree_flatten(self):     return (), (self.x,)   metadata is not hashable!      def tree_unflatten(cls, aux_data, children):     () = children     x, = aux_data     return cls(x) a = A(np.arange(3.)) _, treedef1 = jax.tree_util.tree_flatten(a) b = A(np.arange(3.)) _, treedef2 = jax.tree_util.tree_flatten(b) print(hash(treedef1))   hash works! print(hash(treedef2))   hash works! print(treedef1 == treedef2)   this crashes ``` Here we're making a pytreedef with unhashable metadata in it. Yet hashing that pytreedef doesn't fail (!), even though equality checking later fails, in this case with: ``` ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() ``` This error arose in real life with the `jit` cache. It looked more like this: ```python a = A(np.arange(3.)) b = A(np.arange(3.)) .jit def f(x):   return x f(a) f(b)   crashes with 'The truth value of any array ...' ``` AIUI the issue was: 1. Some `flax.struct`s (with `struct.field(pytree_node=False)` usage) didn't have hashable pytree metadata because of arrays in them, which means they aren't valid pytreedefs (it seems we didn't say 'hashable' in the pytree overview doc, which is an oversight, but the Python type annotations and the docstring tell the truth). This is the heart of the issue, but we didn't catch it very early or raise a clear error message... 2. When tree_flattening, we don't check if the metadata is hashable. That would slow things down. So we allow unhashable metadata to be built into pytreedefs produced by tree_flattening. 3. When we ultimately hash these treedefs, we don't get a hashing error because we don't actually call `__hash__` on the metadata associated with custom pytree nodes in a treedef. Instead, we just hash the pointer to the metadata. I assume that's for performance, so we don't call back into Python (we wait for equality checking to call back into Python). That's legit for hashable data, but for unhashable data it means we're delaying an inevitable error: we only actually raise an error when `__eq__` is called (to resolve hash bucket collisions). That's when we ultimately call back into Python, which in turn calls `numpy.ndarray.__eq__`, which doesn't follow Python `__eq__` rules (i.e. it doesn't return a builtin bool but rather an array) and thus results in this error message when `bool` is called on the result. On the JAX side, this is really an issue of improving the error message; there's not much we can do about unhashable (and uneqable) custom Pytree node registrations. For any user wanting to fix this issue: just make the metadata hashable, e.g. by wrapping any `numpy.ndarray` instances in the metadata with a class which hashes by value (implementing `__hash__` and `__eq__`, particularly `__eq__` so it returns a Python `bool`).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,c++ pytree code allows unhashable metadata,"```python import numpy as np import jax .tree_util.register_pytree_node_class class A:   x: np.ndarray   not hashable!   def __init__(self, x):     self.x = x   def tree_flatten(self):     return (), (self.x,)   metadata is not hashable!      def tree_unflatten(cls, aux_data, children):     () = children     x, = aux_data     return cls(x) a = A(np.arange(3.)) _, treedef1 = jax.tree_util.tree_flatten(a) b = A(np.arange(3.)) _, treedef2 = jax.tree_util.tree_flatten(b) print(hash(treedef1))   hash works! print(hash(treedef2))   hash works! print(treedef1 == treedef2)   this crashes ``` Here we're making a pytreedef with unhashable metadata in it. Yet hashing that pytreedef doesn't fail (!), even though equality checking later fails, in this case with: ``` ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() ``` This error arose in real life with the `jit` cache. It looked more like this: ```python a = A(np.arange(3.)) b = A(np.arange(3.)) .jit def f(x):   return x f(a) f(b)   crashes with 'The truth value of any array ...' ``` AIUI the issue was: 1. Some `flax.struct`s (with `struct.field(pytree_node=False)` usage) didn't have hashable pytree metadata because of arrays in them, which means they aren't valid pytreedefs (it seems we didn't say 'hashable' in the pytree overview doc, which is an oversight, but the Python type annotations and the docstring tell the truth). This is the heart of the issue, but we didn't catch it very early or raise a clear error message... 2. When tree_flattening, we don't check if the metadata is hashable. That would slow things down. So we allow unhashable metadata to be built into pytreedefs produced by tree_flattening. 3. When we ultimately hash these treedefs, we don't get a hashing error because we don't actually call `__hash__` on the metadata associated with custom pytree nodes in a treedef. Instead, we just hash the pointer to the metadata. I assume that's for performance, so we don't call back into Python (we wait for equality checking to call back into Python). That's legit for hashable data, but for unhashable data it means we're delaying an inevitable error: we only actually raise an error when `__eq__` is called (to resolve hash bucket collisions). That's when we ultimately call back into Python, which in turn calls `numpy.ndarray.__eq__`, which doesn't follow Python `__eq__` rules (i.e. it doesn't return a builtin bool but rather an array) and thus results in this error message when `bool` is called on the result. On the JAX side, this is really an issue of improving the error message; there's not much we can do about unhashable (and uneqable) custom Pytree node registrations. For any user wanting to fix this issue: just make the metadata hashable, e.g. by wrapping any `numpy.ndarray` instances in the metadata with a class which hashes by value (implementing `__hash__` and `__eq__`, particularly `__eq__` so it returns a Python `bool`).",2022-10-28T03:39:37Z,better_errors,open,0,4,https://github.com/jax-ml/jax/issues/13027,"I think we could, essentially, `try: ... except ValueError: ...` around the hash table lookup and raise a better error message (e.g. mentioning custom pytree nodes with unhashable metadata are a likely culprit, and pointing to this issue). But I think we'll need that logic in multiple places: in the C++ dispatch path, in linear_util.py:cache, and perhaps in weakref_lru_cache. Maybe that's about it...","> 3. When we ultimately hash these treedefs, we don't get a hashing error because we don't actually call `__hash__` on the metadata associated with custom pytree nodes in a treedef. Instead, we just hash the pointer to the metadata. We only hash the node kind, arity and the pointer to the Python type object (not metadata): ```cpp template  H AbslHashValue(H h, const PyTreeDef::Node& n) {   h = H::combine(std::move(h), n.kind, n.arity, n.custom);   return h; } template  H AbslHashValue(H h, const PyTreeDef& t) {   h = H::combine(std::move(h), t.traversal_);   return h; } ``` while `node.node_data` is considered for `__eq__` check: ```cpp bool PyTreeDef::operator==(const PyTreeDef& other) const {   if (traversal_.size() != other.traversal_.size()) {     return false;   }   for (size_t i = 0; i   hash1 == hash2  (forced) hash1 == hash2  =?=>  obj1 == obj2   (no restriction) ```",Couldn't we just make sure to hash the treedef and test it for selfequality (even if we don't need its value: just verify it *can* be done) during the cache miss path?,"> Couldn't we just make sure to hash the treedef and test it for selfequality (even if we don't need its value: just verify it can be done) during the cache miss path? I think the true issue here is about the `__eq__` check calls `py::object.not_equal` (fails for `numpy.ndarray`), not about how to hash the metadata. ```cpp // not_equal calls PyObject_RichCompareBool(..., Py_NE) if (a.node_data && a.node_data.not_equal(b.node_data)) {   return false; } ``` An approach is to replace `PyObject_RichCompareBool` with `PyObject_RichCompare`, and check whether the returned object has a callable attribute `any`. ```cpp if (a.node_data) {   py::object ne = PyObject_RichCompare(a.node_data.ptr(), b.node_data.ptr(), Py_NE);   if (py::hasattr(ne, ""any"") && py::isinstance(ne.attr(""any"")) && ne.attr(""any"")().cast()) {     return false;   }   if (ne.cast()) {     return false;   } } ```  UPDATE: Having a mutable object as metadata (e.g., `dict` or `numpy.ndarray` in this issue) in the opaque `treedef` is not good. For example, the custom flatten function can save and expose the metadata to Python access: ```python data = [] def to_iterable(container):     children, metadata = ..., ...     data.append(metadata)     return children, metadata  Then we have a reference to the metadata in `treedef`  Unexpected behavior may raise for mutable metadata data[1]['a'] = 'b'   treedef internal has been modified! ```"
607,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Uses MHLO bytecode for XlaCallModule op.)， 内容是 ([jax2tf] Uses MHLO bytecode for XlaCallModule op. Most of the changes here have to do with the fact that it is harder to inspect the converted code, since the MHLO is not in text form. This means that some tests need to be adjusted, and we are dropping an error message when the converted code uses custom calls, since the detection was based on inspecting the text of the MHLO.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Uses MHLO bytecode for XlaCallModule op.,"[jax2tf] Uses MHLO bytecode for XlaCallModule op. Most of the changes here have to do with the fact that it is harder to inspect the converted code, since the MHLO is not in text form. This means that some tests need to be adjusted, and we are dropping an error message when the converted code uses custom calls, since the detection was based on inspecting the text of the MHLO.",2022-10-26T08:47:16Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12985
498,"以下是一个github上的jax下的一个issue, 标题是(Skip two unit tests about custom sharding on libtpu)， 内容是 (DETAILS: Due to xc.register_custom_call_partitioner is not supported on libtpu, the following two tests are skipped: tests/pjit_test.py::PJitTest::test_custom_partitioner tests/debugging_primitives_test.py::InspectShardingTest::test_inspect_sharding_is_called_in_pjit)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Skip two unit tests about custom sharding on libtpu,"DETAILS: Due to xc.register_custom_call_partitioner is not supported on libtpu, the following two tests are skipped: tests/pjit_test.py::PJitTest::test_custom_partitioner tests/debugging_primitives_test.py::InspectShardingTest::test_inspect_sharding_is_called_in_pjit",2022-10-25T19:32:10Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/12977,"Do you mind adding a `is_cloud_tpu` helper function to https://github.com/google/jax/blob/main/jax/_src/test_util.py and using that instead of copy/pasting the check? That way it's easier to reuse, update, and find tests that we need to reenable.","Add a util func is_cloud_tpu to test_utils. PTAL, thanks"
2004,"以下是一个github上的jax下的一个issue, 标题是(Unexpected NaNs in a jax port of PSIS-LOO)， 内容是 ( Description I'm trying to port the Pareto smoothed importance sampling (PSIS) algorithm to JAX, since I need to be able to differentiate through the logweights; specifically, I am interested in porting the `psislw` function. There are already two existing implementations I'm aware of  Aki Vehtari's original version, and an Arviz version. The main difficulty in porting these to JAX was that the original code involves a lot of dynamic array operations  shapes depend on values. To try and solve this, I wrote a version in which all array sizes are static and ""truncations"" are replaced with `jnp.nan`s (so that sums and means could be replaced by `jnp.nansum` and `jnp.nanmean`, etc). There were also some controlflow issues which I replaced with `jnp.where` (sometimes at the cost of precomputing both branches  that may be avoidable, I'm not sure). The current version of my jax port is here. This worked fine, and I was able to reproduce the results of both implementations on random data. However, when I try to take the `grad` of the version I wrote, I'm getting `nan` gradients; when I ran this with `config.update(""jax_debug_nans"", True)`, I got this (just the relevant part): ``` ~/projects/recastoptimizerapi/optimizer/algorithms/loo.py in psis_single(x, cutoff_ind, cutoffmin, k_min)     103      104     expxcutoff = np.exp(xcutoff) > 105     x2 = np.where(x > xcutoff, x, np.nan)     106     n2 = np.where(x > xcutoff, 1, 0).sum()     107  ```  so seems like the `nan`padding worked fine for the ""forward pass"", but doesn't play nicely with the gradient computation? Is that the expected behavior? Thanks!  What jax/jaxlib version are you using? 0.3.14  Which accelerator(s) are you using? CPU  Additional system info macOS, Python 3.9.5  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unexpected NaNs in a jax port of PSIS-LOO," Description I'm trying to port the Pareto smoothed importance sampling (PSIS) algorithm to JAX, since I need to be able to differentiate through the logweights; specifically, I am interested in porting the `psislw` function. There are already two existing implementations I'm aware of  Aki Vehtari's original version, and an Arviz version. The main difficulty in porting these to JAX was that the original code involves a lot of dynamic array operations  shapes depend on values. To try and solve this, I wrote a version in which all array sizes are static and ""truncations"" are replaced with `jnp.nan`s (so that sums and means could be replaced by `jnp.nansum` and `jnp.nanmean`, etc). There were also some controlflow issues which I replaced with `jnp.where` (sometimes at the cost of precomputing both branches  that may be avoidable, I'm not sure). The current version of my jax port is here. This worked fine, and I was able to reproduce the results of both implementations on random data. However, when I try to take the `grad` of the version I wrote, I'm getting `nan` gradients; when I ran this with `config.update(""jax_debug_nans"", True)`, I got this (just the relevant part): ``` ~/projects/recastoptimizerapi/optimizer/algorithms/loo.py in psis_single(x, cutoff_ind, cutoffmin, k_min)     103      104     expxcutoff = np.exp(xcutoff) > 105     x2 = np.where(x > xcutoff, x, np.nan)     106     n2 = np.where(x > xcutoff, 1, 0).sum()     107  ```  so seems like the `nan`padding worked fine for the ""forward pass"", but doesn't play nicely with the gradient computation? Is that the expected behavior? Thanks!  What jax/jaxlib version are you using? 0.3.14  Which accelerator(s) are you using? CPU  Additional system info macOS, Python 3.9.5  NVIDIA GPU info _No response_",2022-10-24T17:44:18Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/12952,"In short, I think this is expected behavior. Please take a look at the FAQ entry on ""Gradients contain NaN where using `where`"" for more details and suggestions: https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere","Thanks for the quick response  ! If I understand this correctly, this means that the idea of ""nanpadding"" to bypass the dynamic nature of arrays in the `psislw` function isn't going to work, right? Will the same hold for ""infpadding"", I guess?","I think Stephan's point was not that NaNpadding won't work, but that you may need (surprisingly) two `where` clauses one before and one after.","Yes, this is unfortunately a well known issue with most AD systems. The FAQ section also has links to two issues where it has been explained it detail. The short story is that nan padding is not a bad idea by itself, but you might need to replace those nans with values from the domain of a function before applying it. You might get away with not doing that for some basic stuff like `+`, but anything more complicated (`*`, `sin`, `exp`, ...) will likely break. I'll close this since it's already been discussed at length. But feel free to open up a new discussion if you need more help!"
1111,"以下是一个github上的jax下的一个issue, 标题是(`DeviceArray` operators can't easily be distinguished)， 内容是 (As far as I can tell, different operators on the `DeviceArray` class aren't distinguishable. For example,  ``` >>> import jaxlib.xla_extension >>> import jax.numpy as jnp >>> x = jnp.array(1) >>> jaxlib.xla_extension.DeviceArray.__add__ .deferring_binary_op(self, other)> ``` gives the same result as  ``` >>> jaxlib.xla_extension.DeviceArray.__mul__ .deferring_binary_op(self, other)> ``` They all point to this function https://github.com/google/jax/blob/a4e366394bec6054f6d5029eb5bfea30a8411e98/jax/_src/numpy/lax_numpy.pyL4753 . Of course the `binary_op` in this function is different for each operator, but this can't be accessed and so you can't easily see which operator is being used (without tediously trying values to figure out what the operator is).  Maybe it'd be good to have something like `deferring_binary_op.binary_op = binary_op` to facilitate introspection. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`DeviceArray` operators can't easily be distinguished,"As far as I can tell, different operators on the `DeviceArray` class aren't distinguishable. For example,  ``` >>> import jaxlib.xla_extension >>> import jax.numpy as jnp >>> x = jnp.array(1) >>> jaxlib.xla_extension.DeviceArray.__add__ .deferring_binary_op(self, other)> ``` gives the same result as  ``` >>> jaxlib.xla_extension.DeviceArray.__mul__ .deferring_binary_op(self, other)> ``` They all point to this function https://github.com/google/jax/blob/a4e366394bec6054f6d5029eb5bfea30a8411e98/jax/_src/numpy/lax_numpy.pyL4753 . Of course the `binary_op` in this function is different for each operator, but this can't be accessed and so you can't easily see which operator is being used (without tediously trying values to figure out what the operator is).  Maybe it'd be good to have something like `deferring_binary_op.binary_op = binary_op` to facilitate introspection. ",2022-10-24T11:53:20Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/12947,"Thanks for the report – this is an interesting question. Can you say more about what you're trying to accomplish, and how you would like to distinguish the operators? For example, you can currently distinguish them using operator identity – does this suit your needs? ```python import jax.numpy as jnp x = jnp.array(1) cls = type(x) method = cls.__add__ print(method is cls.__add__)  True print(method is cls.__mul__)  False ```"
835,"以下是一个github上的jax下的一个issue, 标题是([Flaxformer] Store biases that won't conflict with transformer layers of different sequence lengths. Because this affects only the auto-regressive decoding cache which has a lifetime bounded to a single decoding process (i.e. it's never persisted to disk/checkpoints), we don't need to worry about backward compatibility issues.)， 内容是 ([Flaxformer] Store biases that won't conflict with transformer layers of different sequence lengths. Because this affects only the autoregressive decoding cache which has a lifetime bounded to a single decoding process (i.e. it's never persisted to disk/checkpoints), we don't need to worry about backward compatibility issues.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,"[Flaxformer] Store biases that won't conflict with transformer layers of different sequence lengths. Because this affects only the auto-regressive decoding cache which has a lifetime bounded to a single decoding process (i.e. it's never persisted to disk/checkpoints), we don't need to worry about backward compatibility issues.","[Flaxformer] Store biases that won't conflict with transformer layers of different sequence lengths. Because this affects only the autoregressive decoding cache which has a lifetime bounded to a single decoding process (i.e. it's never persisted to disk/checkpoints), we don't need to worry about backward compatibility issues.",2022-10-22T16:42:22Z,,closed,0,1,https://github.com/jax-ml/jax/issues/12937,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
4051,"以下是一个github上的jax下的一个issue, 标题是(Failed to use JAX on Cloud TPU node (cloud_tpu_init failed))， 内容是 ( Description I'm trying to run a JAX project on Cloud TPU node architecture. But after `import jax`, i'm getting this error: ```shell /home/gaer/.local/lib/python3.7/sitepackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('\n\n  \n  \n  Error 404 (Not Found)!!1\n  \n    *{margin:0;padding:0}html,code{font:15px/22px arial,sansserif}html{background:fff;color: CC(jax.random.randint range must be valid);padding:15px}body{margin:7% auto 0;maxwidth:390px;minheight:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px norepeat;paddingright:205px}p{margin:11px 0 22px;overflow:hidden}ins{color: CC(Gradient of `np.exp` sometimes causes invalid values);textdecoration:none}a img{border:0} screen and (maxwidth:772px){body{background:none;margintop:0;maxwidth:none;paddingright:0}}logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) norepeat;marginleft:5px} only screen and (minresolution:192dpi){logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) norepeat 0% 0%/100% 100%;mozborderimage:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}} only screen and (webkitmindevicepixelratio:2){logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) norepeat;webkitbackgroundsize:100% 100%}}logo{display:inlineblock;height:54px;width:150px}\n  \n  \n  404. That’s an error.\n  The requested URL /computeMetadata/v1/instance/attributes/acceleratortype was not found on this server.  That’s all we know.\n')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report "" WARNING: Logging before InitGoogle() is written to STDERR I0000 00:00:1666349323.329415   14971 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349323.832365   14971 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349324.335298   14971 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349324.838544   14971 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349325.341522   14971 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349325.844483   14971 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Failed to fetch URL (http status: 404) No error Failed to get accelerator type with error: Failed to fetch URL (http status: 404) No errorI0000 00:00:1666349326.347823   14971 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Failed to fetch URL (http status: 404) No error ``` In another issue with a similar error (https://github.com/google/jax/issues/7444) was suggested to switch to Cloud TPU VM architecture, which indeed doesn't have that problem. But we plan to use TPU in Kubernetes cluster and GKE only supports TPU nodes, so we can't switch to Cloud TPU VM. I believe there should be a workaround for TPU nodes as well, e.g. as I understand Google Colab also uses TPU nodes, and JAX works there without any issues. So as I understand from colab_tpu.py we just need to correctly resolve gRPC host for our TPU and set it to the config.  I would really appreciate any hints and advice on how we can do this.  What jax/jaxlib version are you using? jax==0.3.23, jaxlib==0.3.22 (jax[tpu]>=0.2.16)  Which accelerator(s) are you using? Cloud TPU node, v38  Additional system info Debian 10, Python 3.7.3,   NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Failed to use JAX on Cloud TPU node (cloud_tpu_init failed)," Description I'm trying to run a JAX project on Cloud TPU node architecture. But after `import jax`, i'm getting this error: ```shell /home/gaer/.local/lib/python3.7/sitepackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('\n\n  \n  \n  Error 404 (Not Found)!!1\n  \n    *{margin:0;padding:0}html,code{font:15px/22px arial,sansserif}html{background:fff;color: CC(jax.random.randint range must be valid);padding:15px}body{margin:7% auto 0;maxwidth:390px;minheight:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px norepeat;paddingright:205px}p{margin:11px 0 22px;overflow:hidden}ins{color: CC(Gradient of `np.exp` sometimes causes invalid values);textdecoration:none}a img{border:0} screen and (maxwidth:772px){body{background:none;margintop:0;maxwidth:none;paddingright:0}}logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) norepeat;marginleft:5px} only screen and (minresolution:192dpi){logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) norepeat 0% 0%/100% 100%;mozborderimage:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}} only screen and (webkitmindevicepixelratio:2){logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) norepeat;webkitbackgroundsize:100% 100%}}logo{display:inlineblock;height:54px;width:150px}\n  \n  \n  404. That’s an error.\n  The requested URL /computeMetadata/v1/instance/attributes/acceleratortype was not found on this server.  That’s all we know.\n')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report "" WARNING: Logging before InitGoogle() is written to STDERR I0000 00:00:1666349323.329415   14971 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349323.832365   14971 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349324.335298   14971 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349324.838544   14971 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349325.341522   14971 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Failed to fetch URL (http status: 404) No error I0000 00:00:1666349325.844483   14971 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Failed to fetch URL (http status: 404) No error Failed to get accelerator type with error: Failed to fetch URL (http status: 404) No errorI0000 00:00:1666349326.347823   14971 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Failed to fetch URL (http status: 404) No error ``` In another issue with a similar error (https://github.com/google/jax/issues/7444) was suggested to switch to Cloud TPU VM architecture, which indeed doesn't have that problem. But we plan to use TPU in Kubernetes cluster and GKE only supports TPU nodes, so we can't switch to Cloud TPU VM. I believe there should be a workaround for TPU nodes as well, e.g. as I understand Google Colab also uses TPU nodes, and JAX works there without any issues. So as I understand from colab_tpu.py we just need to correctly resolve gRPC host for our TPU and set it to the config.  I would really appreciate any hints and advice on how we can do this.  What jax/jaxlib version are you using? jax==0.3.23, jaxlib==0.3.22 (jax[tpu]>=0.2.16)  Which accelerator(s) are you using? Cloud TPU node, v38  Additional system info Debian 10, Python 3.7.3,   NVIDIA GPU info _No response_",2022-10-21T11:00:20Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/12917,"I don't really recommend this workflow, because JAX works much better on TPU VMs than TPU Nodes (better performance, more features, fewer bugs), and support for JAX on TPU Nodes is besteffort at this point. I realize this doesn't leave you many options if you wanna use GKE, JAX, and TPUs without waiting for better GKE TPU support. A workaround we've used is to use GKE to orchestrate small nonTPU VMs, and manually use gcloud on the GKE nodes to create and delete TPU VMs. It's pretty hacky, and you have to be careful not to leak TPU VMs, but it works alright. Lemme know if more details on this would be helpful. If you do wanna go ahead with TPU Nodes (and to actually answer your question), I believe you can use `$KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS` to find the TPU Node address(es) you need. I'm not exactly sure what the format is, so I suggest manually looking at it, and then setting `COLAB_TPU_ADDR` using it.","FYI, I learned that TPU VM GKE integration is expected early next year. There's currently a limited integration involving manually registering TPU VMs as GKE nodes, but it's very experimental. Please email me if you'd be interested in trying this and I can try to find more info.","Thanks for the information ! I will try to make it works with TPU node first. But anyway, can you give more detail on how we can create and delete TPU VMs on GKE nodes?","We do it like this: https://github.com/GoogleCloudPlatform/mltestingaccelerators/blob/master/tests/experimental.libsonnet It's a bit hard to make sense of because it's using jsonnet to generate the json files we config GKE with (I barely know how to use GKE so I'm fuzzy on that part). It creates a TPU VM here, and also writes a script for deleting the VM here, which it somehow calls when the job finishes here. We then basically run a bash script using `gcloud ... ssh` here. Hopefully this is enough to get you started, but let me know if you have more questions.","there is also the gcp python api's to create, run scripts and delete TPU VMs from a GKE node. This setup works pretty well and is battle tested. fyi you might want to do a retry loop (both with bash or in python cases) when creating, running scripts on the tpu vm, and deleting them so sometimes it may fail :( also, using container lifecycles is probably needed such that when the container fails, it will cleanup the tpus it created https://kubernetes.io/docs/concepts/containers/containerlifecyclehooks/"
2998,"以下是一个github上的jax下的一个issue, 标题是(tests comparing to numpy from_dlpack should skip on < 1.23.0)， 内容是 ( Description ```py .skipIf(numpy_version < (1, 22, 0), ""Requires numpy 1.22 or newer"") ``` is used to compare results with `np.from_dlpack`  see https://github.com/google/jax/blob/main/tests/array_interoperability_test.pyL201 numpy added `from_dlpack` in 1.23.0. In 1.22.x this was `_from_dlpack` instead. So, when I run the tests with numpy 1.22.3, I get the error ``` E       AttributeError: module 'numpy' has no attribute 'from_dlpack' ``` Test summary: ``` FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_float32[0,4]  AttributeError: module 'nump...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_int8[]  AttributeError: module 'numpy' has...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_uint8[2,3,4]  AttributeError: module 'nump...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_float32[2,3,4]  AttributeError: module 'nu...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_uint8[4]  AttributeError: module 'numpy' h...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_float32[3,4]  AttributeError: module 'nump... FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_float64[2,3,4]  AttributeError: module 'nu...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_int32[3,4]  AttributeError: module 'numpy'...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_int32[4]  AttributeError: module 'numpy' h...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_uint8[]  AttributeError: module 'numpy' ha... ```  What jax/jaxlib version are you using? jax v0.3.23, jaxlib 0.3.22  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,tests comparing to numpy from_dlpack should skip on < 1.23.0," Description ```py .skipIf(numpy_version < (1, 22, 0), ""Requires numpy 1.22 or newer"") ``` is used to compare results with `np.from_dlpack`  see https://github.com/google/jax/blob/main/tests/array_interoperability_test.pyL201 numpy added `from_dlpack` in 1.23.0. In 1.22.x this was `_from_dlpack` instead. So, when I run the tests with numpy 1.22.3, I get the error ``` E       AttributeError: module 'numpy' has no attribute 'from_dlpack' ``` Test summary: ``` FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_float32[0,4]  AttributeError: module 'nump...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_int8[]  AttributeError: module 'numpy' has...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_uint8[2,3,4]  AttributeError: module 'nump...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_float32[2,3,4]  AttributeError: module 'nu...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_uint8[4]  AttributeError: module 'numpy' h...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_float32[3,4]  AttributeError: module 'nump... FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_float64[2,3,4]  AttributeError: module 'nu...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_int32[3,4]  AttributeError: module 'numpy'...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_int32[4]  AttributeError: module 'numpy' h...                                                                                                                      FAILED tests/array_interoperability_test.py::DLPackTest::testJaxToNumpy_uint8[]  AttributeError: module 'numpy' ha... ```  What jax/jaxlib version are you using? jax v0.3.23, jaxlib 0.3.22  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2022-10-20T20:17:53Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/12898,Thanks for the report – are you interested in sending a PR? If not I can take care of it.
1885,"以下是一个github上的jax下的一个issue, 标题是(Accidental H2D and D2H copies when forgetting @pmap)， 内容是 ( Description This issue is not about a bug but rather a sharp edge which is perhaps insufficiently discoverable. I am training my model mostly with a single GPU but wrap my code with `` just in case I wish to scale it larger for certain experiments. I start from a ""single"" model, replicate it with `jax.tree_map(partial(device_put_replicated, devices=jax.local_devices()), model)`, wrap the function responsible for making the training step with `pmap`, add a `pmean` on the loss value and voila. The trouble came when I wanted to keep an exponential moving average (EMA) of the model parameters and for whatever reason I didn't want to include it in the pmapped`make_step` function. I implement it separately as ```python .jit def ema_update(old, new):     def update_one(old, new):         return old * alpha + (1  alpha) * new     return jax.tree_map(update_one, old, new) ``` and use roughly as ```python model, ... = make_step(model, ...) some_logging_and_python_stuff() ema_model = ema_update(ema_model, model) ``` This resulted in an unexpectedly big slowdown and only after profiling and finding a bunch of memcopies, I realized that I should have used `pmap` instead of `jit`. I am surprised it even works with sharded arrays, so it didn't occur to me I could have made an error there. With this issue I am wondering if there should be some kind of warning (in code or in documentation?) about this pitfall. !Screenshot 20221020 at 13 33 58  What jax/jaxlib version are you using? jax==0.3.20, jaxlib==0.3.20+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Ubuntu 20.04 LTS  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Accidental H2D and D2H copies when forgetting @pmap," Description This issue is not about a bug but rather a sharp edge which is perhaps insufficiently discoverable. I am training my model mostly with a single GPU but wrap my code with `` just in case I wish to scale it larger for certain experiments. I start from a ""single"" model, replicate it with `jax.tree_map(partial(device_put_replicated, devices=jax.local_devices()), model)`, wrap the function responsible for making the training step with `pmap`, add a `pmean` on the loss value and voila. The trouble came when I wanted to keep an exponential moving average (EMA) of the model parameters and for whatever reason I didn't want to include it in the pmapped`make_step` function. I implement it separately as ```python .jit def ema_update(old, new):     def update_one(old, new):         return old * alpha + (1  alpha) * new     return jax.tree_map(update_one, old, new) ``` and use roughly as ```python model, ... = make_step(model, ...) some_logging_and_python_stuff() ema_model = ema_update(ema_model, model) ``` This resulted in an unexpectedly big slowdown and only after profiling and finding a bunch of memcopies, I realized that I should have used `pmap` instead of `jit`. I am surprised it even works with sharded arrays, so it didn't occur to me I could have made an error there. With this issue I am wondering if there should be some kind of warning (in code or in documentation?) about this pitfall. !Screenshot 20221020 at 13 33 58  What jax/jaxlib version are you using? jax==0.3.20, jaxlib==0.3.20+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Ubuntu 20.04 LTS  NVIDIA GPU info ++  ++",2022-10-20T11:37:13Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/12887,"Thanks for explaining this pitfall so clearly. I agree that having the `ShardedDeviceArray` output of a `pmap` computation getting automatically collected into a single `DeviceArray` can be surprising, and a performance footgun. On the other hand, in other settings it can be valuable to treat sharding as ""just layout"", which should affect performance but shouldn't affect the semantics of what values are computed or what computations are even allowed. In other words, I think it's a tradeoff, and in different situations one policy may work better than another. Luckily, there are some ways to control whether you want to allow such implicit device transfers: take a look at these Transfer guard docs. You may prefer to disallow `""device_to_device""` transfers, and maybe others as well. We're also working on improving the situation in the future: instead of having `jit` force all its inputs onto a single device, we'll instead have `jit` build a computation specifically for its arguments' distributed layouts, and then XLA will automatically parallelize the computation as it sees fit. (Unfortunately that doesn't yet work with `pmap` outputs...) Stay tuned for updates on that front! Does the transfer guard help solve the footgun? Maybe we can surface it better in the documentation, e.g. add it to the sharp bits doc (if it's not already there)? (I wonder if we should also have a blanket 'debug mode' which turns on lots of warnings, including these but lots of others...)"
671,"以下是一个github上的jax下的一个issue, 标题是(CPU-only linux wheels for jaxlib 0.3.22 are not published)， 内容是 ( Description Breaking with tradition from prior releases, the 0.3.22 release of jaxlib does not have published linux CPUonly wheels in https://storage.googleapis.com/jaxreleases. Would it be possible to get linux, CPU jaxlib wheels published in https://storage.googleapis.com/jaxreleases?  What jax/jaxlib version are you using? jaxlib 0.3.22  Which accelerator(s) are you using? CPU  Additional system info MacOS  NVIDIA GPU info n/a)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,CPU-only linux wheels for jaxlib 0.3.22 are not published," Description Breaking with tradition from prior releases, the 0.3.22 release of jaxlib does not have published linux CPUonly wheels in https://storage.googleapis.com/jaxreleases. Would it be possible to get linux, CPU jaxlib wheels published in https://storage.googleapis.com/jaxreleases?  What jax/jaxlib version are you using? jaxlib 0.3.22  Which accelerator(s) are you using? CPU  Additional system info MacOS  NVIDIA GPU info n/a",2022-10-20T02:33:45Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/12879,I can see 0.3.22 wheels here: https://storage.googleapis.com/jaxreleases/jax_releases.html ``` nocuda/jaxlib0.3.22cp310cp310manylinux2014_x86_64.whl nocuda/jaxlib0.3.22cp37cp37mmanylinux2014_x86_64.whl nocuda/jaxlib0.3.22cp38cp38manylinux2014_x86_64.whl nocuda/jaxlib0.3.22cp39cp39manylinux2014_x86_64.whl ```, Oh interesting... I'm not seeing those on https://storage.googleapis.com/jaxreleases (the XML version). Why the difference between https://storage.googleapis.com/jaxreleases/jax_releases.html and https://storage.googleapis.com/jaxreleases?,The second link is the link to the bucket which cannot be read. The first link is the actual link to the index.,> The second link is the link to the bucket which cannot be read. The first link is the actual link to the index. I'm confused... What does the XML output represent?,"The printed XML output only lists the first 1000 objects in the bucket (I'm not sure how the order is determined), and since there are more than 1000 items in the storage bucket it doesn't include everything.","ahhh gotcha, that would explain it! thanks !"
550,"以下是一个github上的jax下的一个issue, 标题是(Disable compilation from jax.lax.(cond, while_loop, fori_loop)?)， 内容是 (As I understand it, code underneath one of these `jax.lax` control flow constructs will be JIT compiled regardless of whether there is a `jax.jit` used. This is a bit annoying when I am running unit tests because our compilation times inflate the test runtime dramatically.  Any advice would be appreciated!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Disable compilation from jax.lax.(cond, while_loop, fori_loop)?","As I understand it, code underneath one of these `jax.lax` control flow constructs will be JIT compiled regardless of whether there is a `jax.jit` used. This is a bit annoying when I am running unit tests because our compilation times inflate the test runtime dramatically.  Any advice would be appreciated!",2022-10-19T22:59:16Z,enhancement,closed,0,7,https://github.com/jax-ml/jax/issues/12874,"Thanks for the suggestion! Instead of compiling these higherorder primitives, we could effectively run a Pythonlevel jaxpr interpreter. Then there wouldn't be compile times, but there would be high Python overheads. I wonder if that would be a better tradeoff, at least in some cases. Luckily I think you can experiment with that behavior now by using `jax.disable_jit`. In fact, I think you can do `nojit_cond = jax.disable_jit()(jax.lax.cond)` and similar. WDYT about trying that use of `jax.disable_jit` to see if it helps?",Here are some examples of the relevant logic: * `scan` * `cond` * `while_loop`,"It would be ideal if we didn't have to change our codebase just for test code. But looking at the examples you listed +  this, there seems to be a more noninvasive strategy: ```python3  test.py from jax.config import config config.update('jax_disable_jit', True)  import codebase  test code ... config.update('jax_disable_jit', False)  is this needed? Just put it here to be safe.. ``` Would this work?","Thanks  and . Between these two solutions, I think we're all set! I've gotten the particular unit test I was looking at down from 32s to 1.2s which is great. Thanks!!","You can also put that kind of thing in setUp / tearDown methods on your test classes or modules. Those can be more robust to e.g. exceptions being raised in tests. (That is, if a test fails with an exception, you may want to clean up any changes to the global state as it exits.)","Would setUp/tearDown be more robust to exceptions than wrapping the test in the `jax.disable_jit` context manager like this? ``` def test_f():     with jax.disable_jit():         f() ``` or, possibly better and works great because `disable_jit` is written using ``. ``` .disable_jit def test_f():     f() ```","I think that will work well too! Our config context managers attempt to be robust to exceptions by using `try`/`finally`, and putting the clean up under the `finally`."
2000,"以下是一个github上的jax下的一个issue, 标题是(Convolution is significantly slower inside a loop body (CPU))， 内容是 ( Description Consider this `conv` inside `while` (2 iterations): ```python import jax import jax.lax import jax.random import jax.numpy as jnp def test(inp, kernel):     def loop_body(i, args):         inp, kernel = args         inp = jax.lax.conv(inp, kernel, (1,), ""VALID"")         return (inp, kernel)     return jax.lax.fori_loop(0, 2, loop_body, (inp, kernel))[0] inp = jnp.full((1, 1000, 1024), 0.5) kernel = jnp.full((1000, 1000, 1), 0.5)   import time s = time.time() jax.block_until_ready(jax.jit(test)(inp, kernel)) print(time.time()  s) s = time.time() jax.block_until_ready(jax.jit(test)(inp, kernel)) print(time.time()  s) ``` It takes ~16s to run on my machine. **However**, when we remove the loop and just do: ```python def test(inp, kernel):     return jax.lax.conv(inp, kernel, (1,), ""VALID"") ``` It takes just 3ms. **Additional info** Moreover, the corresponding channellast convolution doesn't suffer from this: ```python def test(inp, kernel):     def loop_body(i, args):         inp, kernel = args         dim_nums = jax.lax.ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1))         inp = jax.lax.conv_general_dilated(inp, kernel, (1,), ""VALID"", (1,), (1,), dim_nums)         return (inp, kernel)     return jax.lax.fori_loop(0, 2, loop_body, (inp, kernel))[0] inp = jnp.full((1, 1024, 1000), 0.5) kernel = jnp.full((1, 1000, 1000), 0.5) ``` It takes 4ms for me. And: * this is also the case for convolution inside a `conditional` branch * this is also the case for convolution with 2 spacial dimensions  What jax/jaxlib version are you using? jax v0.3.23, jaxlib v0.3.22  Which accelerator(s) are you using? CPU / Intel i56200U (4) @ 2.800GHz  Additional system info Python 3.8.6, Ubuntu 22.04.1 LTS x86_64)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Convolution is significantly slower inside a loop body (CPU)," Description Consider this `conv` inside `while` (2 iterations): ```python import jax import jax.lax import jax.random import jax.numpy as jnp def test(inp, kernel):     def loop_body(i, args):         inp, kernel = args         inp = jax.lax.conv(inp, kernel, (1,), ""VALID"")         return (inp, kernel)     return jax.lax.fori_loop(0, 2, loop_body, (inp, kernel))[0] inp = jnp.full((1, 1000, 1024), 0.5) kernel = jnp.full((1000, 1000, 1), 0.5)   import time s = time.time() jax.block_until_ready(jax.jit(test)(inp, kernel)) print(time.time()  s) s = time.time() jax.block_until_ready(jax.jit(test)(inp, kernel)) print(time.time()  s) ``` It takes ~16s to run on my machine. **However**, when we remove the loop and just do: ```python def test(inp, kernel):     return jax.lax.conv(inp, kernel, (1,), ""VALID"") ``` It takes just 3ms. **Additional info** Moreover, the corresponding channellast convolution doesn't suffer from this: ```python def test(inp, kernel):     def loop_body(i, args):         inp, kernel = args         dim_nums = jax.lax.ConvDimensionNumbers(lhs_spec=(0, 2, 1), rhs_spec=(2, 1, 0), out_spec=(0, 2, 1))         inp = jax.lax.conv_general_dilated(inp, kernel, (1,), ""VALID"", (1,), (1,), dim_nums)         return (inp, kernel)     return jax.lax.fori_loop(0, 2, loop_body, (inp, kernel))[0] inp = jnp.full((1, 1024, 1000), 0.5) kernel = jnp.full((1, 1000, 1000), 0.5) ``` It takes 4ms for me. And: * this is also the case for convolution inside a `conditional` branch * this is also the case for convolution with 2 spacial dimensions  What jax/jaxlib version are you using? jax v0.3.23, jaxlib v0.3.22  Which accelerator(s) are you using? CPU / Intel i56200U (4) @ 2.800GHz  Additional system info Python 3.8.6, Ubuntu 22.04.1 LTS x86_64",2022-10-19T12:37:56Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/12868,"Upon analyzing the XLA HLO before and after optimisation, it looks like in the fast cases `convolution` is optimised into `transpose`+`dot`, but in the slow case it is not.","1. `jax.block_until_ready(jax.jit(func)(*args))` is an antipattern in JAX, since the function is compiled every timed iteration. So you are measuring compilation+execution time. It's better to execute the compiled function once to trigger compilation, and then perform timing. 2. Measuring only one iteration with explicit `time()` is also not encouraged. Instead you could use e.g. `%timeit` inside ipython/jupyter. This results in ```python func_jit = jax.jit(func) func_jit(*args)    trigger compilation %timeit jax.block_until_ready(func_jit(*args)) ``` PS: It's possible that JAX implemented compilation caching in the meantime. If yes, I am not aware of it."
273,"以下是一个github上的jax下的一个issue, 标题是([typing] annotate jax._src.numpy.index_tricks)， 内容是 (Part of CC(Tracking Issue: JAX Type Annotations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[typing] annotate jax._src.numpy.index_tricks,Part of CC(Tracking Issue: JAX Type Annotations),2022-10-18T19:27:21Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/12853
618,"以下是一个github上的jax下的一个issue, 标题是(Roll forward with fix: Remove the original python function `fun_` from C++ PjitFunction, as the destroying `fun_` may yield the thread in some cases, which causes error during deleting the python object of PjitFunction.)， 内容是 (Roll forward with fix: Remove the original python function `fun_` from C++ PjitFunction, as the destroying `fun_` may yield the thread in some cases, which causes error during deleting the python object of PjitFunction.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Roll forward with fix: Remove the original python function `fun_` from C++ PjitFunction, as the destroying `fun_` may yield the thread in some cases, which causes error during deleting the python object of PjitFunction.","Roll forward with fix: Remove the original python function `fun_` from C++ PjitFunction, as the destroying `fun_` may yield the thread in some cases, which causes error during deleting the python object of PjitFunction.",2022-10-17T18:38:41Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12840
548,"以下是一个github上的jax下的一个issue, 标题是(Improve the error message when users are trying to create SDAs and pass them into pjit/xmap when jax.Array is enabled. The error message tells them exactly what to do to fix the error.)， 内容是 (Improve the error message when users are trying to create SDAs and pass them into pjit/xmap when jax.Array is enabled. The error message tells them exactly what to do to fix the error.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve the error message when users are trying to create SDAs and pass them into pjit/xmap when jax.Array is enabled. The error message tells them exactly what to do to fix the error.,Improve the error message when users are trying to create SDAs and pass them into pjit/xmap when jax.Array is enabled. The error message tells them exactly what to do to fix the error.,2022-10-15T02:11:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12821
113098,"以下是一个github上的jax下的一个issue, 标题是(⚠️ Nightly GPU Multiprocess CI failed ⚠️)， 内容是 (Workflow Run URL Failure summary outputtestjaxlibnightly12330.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.24.dev20221016%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (154.3 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.3/154.3 MB 23.2 MB/s eta 0:00:00 Collecting numpy>=1.20   Downloading numpy1.23.4cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 114.1 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.24.dev20221016+cuda11.cudnn82 numpy1.23.4 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuildgyttjifr   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuildgyttjifr   Resolved https://github.com/google/jax to commit 4cfa01f1cf1325d6c0ccc61b61ba8f1124c938af   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.24) (1.23.4) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 16.1 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.24) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.4.0py3noneany.whl (26 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.24py3noneany.whl size=1282455 sha256=f6f6bf4b7d6d6c7ca18bd1890b9c42dcef2d8bc59387c39b97e985630566245c   Stored in directory: /tmp/pipephemwheelcacheew2xkkpn/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, jax Successfully installed jax0.3.24 opt_einsum3.3.0 typing_extensions4.4.0 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting pytest   Downloading pytest7.1.3py3noneany.whl (298 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.2/298.2 kB 32.4 MB/s eta 0:00:00 Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 22.3 MB/s eta 0:00:00 Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 27.2 MB/s eta 0:00:00 Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.3 tomli2.0.1 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.3) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum  workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot FAILEDFAILEDFAILEDFAILEDFAILEDFAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 7, local_device_ids = [7]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".564022954"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_7.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.29s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 4, local_device_ids = [4]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563866271"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_4.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.28s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 2, local_device_ids = [2]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".564053704"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_2.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.29s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 6, local_device_ids = [6]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563889471"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_6.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.28s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 5, local_device_ids = [5]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563866361"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_5.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.28s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 3, local_device_ids = [3]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563934422"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_3.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.29s (0:06:12) ============= FAILEDFAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 0, local_device_ids = [0]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563799229"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_0.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.30s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 1, local_device_ids = [1]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563927882"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_1.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.30s (0:06:12) ============= ```  Failure summary outputtestjaxlibnightly12331.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.24.dev20221016%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (154.3 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.3/154.3 MB 22.9 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Collecting numpy>=1.20   Downloading numpy1.23.4cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 111.1 MB/s eta 0:00:00 Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.24.dev20221016+cuda11.cudnn82 numpy1.23.4 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuildf_ad95v8   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuildf_ad95v8   Resolved https://github.com/google/jax to commit 4cfa01f1cf1325d6c0ccc61b61ba8f1124c938af   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.24) (1.23.4) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 13.8 kB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.24) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.4.0py3noneany.whl (26 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.24py3noneany.whl size=1282455 sha256=cfc32e604f80c40248f964df6e801b380c94cc0afb13b8fa1188d45b6ae00314   Stored in directory: /tmp/pipephemwheelcachecgwijhou/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, jax Successfully installed jax0.3.24 opt_einsum3.3.0 typing_extensions4.4.0 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting pytest   Downloading pytest7.1.3py3noneany.whl (298 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.2/298.2 kB 39.2 MB/s eta 0:00:00 Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 29.1 MB/s eta 0:00:00 Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 17.2 MB/s eta 0:00:00 Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.3 tomli2.0.1 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.3) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum  workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum  workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f5302cea280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007fc24d316280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.010772: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".010376079"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.010814: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".010376079"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.010828: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".010376079"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_12.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.55s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007fc4bc6b6280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.099949: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".099502037"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.100000: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".099502037"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.100017: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".099502037"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_10.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.64s (0:01:12) ============= FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:53.909748: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".907841846"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:53.909797: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".907841846"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:53.909810: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".907841846"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_13.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.65s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f29561ff280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.186293: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".185856583"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.186349: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".185856583"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.186366: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".185856583"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_14.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.72s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f499f47b280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.273299: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".272855077"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.273346: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".272855077"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.273360: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".272855077"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_9.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.81s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f8b8ddb8280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.361552: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".361109353"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.361597: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".361109353"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.361609: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".361109353"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_11.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.90s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f32d2412280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.449720: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".449282537"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.449778: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".449282537"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.449789: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".449282537"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_15.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.98s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007fb2aaa82280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.537896: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".537455530"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.537949: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".537455530"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.537965: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".537455530"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_8.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 73.07s (0:01:13) ============= ``` )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,⚠️ Nightly GPU Multiprocess CI failed ⚠️,"Workflow Run URL Failure summary outputtestjaxlibnightly12330.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.24.dev20221016%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (154.3 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.3/154.3 MB 23.2 MB/s eta 0:00:00 Collecting numpy>=1.20   Downloading numpy1.23.4cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 114.1 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.24.dev20221016+cuda11.cudnn82 numpy1.23.4 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuildgyttjifr   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuildgyttjifr   Resolved https://github.com/google/jax to commit 4cfa01f1cf1325d6c0ccc61b61ba8f1124c938af   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.24) (1.23.4) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 16.1 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.24) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.4.0py3noneany.whl (26 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.24py3noneany.whl size=1282455 sha256=f6f6bf4b7d6d6c7ca18bd1890b9c42dcef2d8bc59387c39b97e985630566245c   Stored in directory: /tmp/pipephemwheelcacheew2xkkpn/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, jax Successfully installed jax0.3.24 opt_einsum3.3.0 typing_extensions4.4.0 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting pytest   Downloading pytest7.1.3py3noneany.whl (298 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.2/298.2 kB 32.4 MB/s eta 0:00:00 Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 22.3 MB/s eta 0:00:00 Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 27.2 MB/s eta 0:00:00 Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.3 tomli2.0.1 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.3) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 Mon Oct 17 12:08:42 GMT 2022 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum  workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot FAILEDFAILEDFAILEDFAILEDFAILEDFAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 7, local_device_ids = [7]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".564022954"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_7.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.29s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 4, local_device_ids = [4]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563866271"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_4.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.28s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 2, local_device_ids = [2]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".564053704"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_2.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.29s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 6, local_device_ids = [6]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563889471"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_6.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.28s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 5, local_device_ids = [5]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563866361"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_5.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.28s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 3, local_device_ids = [3]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563934422"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_3.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.29s (0:06:12) ============= FAILEDFAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 0, local_device_ids = [0]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563799229"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_0.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.30s (0:06:12) ============= =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ self =      def test_pjit_gda_non_contiguous_mesh_2d_aot(self): >     jax.distributed.initialize() workspace/tests/multiprocess_gpu_test.py:483:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  workspace/jax/_src/distributed.py:160: in initialize     global_state.initialize(coordinator_address, num_processes, process_id, local_device_ids) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  self =  coordinator_address = 'computepermanentnode326:62673', num_processes = 16 process_id = 1, local_device_ids = [1]     def initialize(self,                    coordinator_address: Optional[str] = None,                    num_processes: Optional[int] = None,                    process_id: Optional[int] = None,                    local_device_ids: Optional[Union[int, Sequence[int]]] = None):       coordinator_address = (coordinator_address or                              os.environ.get('JAX_COORDINATOR_ADDRESS', None))       if isinstance(local_device_ids, int):         local_device_ids = [local_device_ids]       (coordinator_address,        num_processes,        process_id,        local_device_ids) = ClusterEnv.auto_detect_unset_distributed_params(         coordinator_address, num_processes, process_id, local_device_ids)       if coordinator_address is None:         raise ValueError('coordinator_address should be defined.')       if num_processes is None:         raise ValueError('Number of processes must be defined.')       if process_id is None:         raise ValueError('The process id of the current process must be defined.')       if local_device_ids:         visible_devices = ','.join(str(x) for x in local_device_ids)  type: ignore[unionattr]         logger.info('JAX distributed initialized with visible devices: %s', visible_devices)         config.update(""jax_cuda_visible_devices"", visible_devices)         config.update(""jax_rocm_visible_devices"", visible_devices)       self.process_id = process_id       if process_id == 0:         if self.service is not None:           raise RuntimeError('distributed.initialize should only be called once.')         logger.info('Starting JAX distributed service on %s', coordinator_address)         self.service = xla_extension.get_distributed_runtime_service(             coordinator_address, num_processes, config.jax_coordination_service)       if self.client is not None:         raise RuntimeError('distributed.initialize should only be called once.')        Set init_timeout to 5 min to leave time for all the processes to connect       self.client = xla_extension.get_distributed_runtime_client(           coordinator_address, process_id, config.jax_coordination_service,           init_timeout=300)       logger.info('Connecting to JAX distributed service on %s', coordinator_address) >     self.client.connect() E     jaxlib.xla_extension.XlaRuntimeError: DEADLINE_EXCEEDED: Barrier timed out. Barrier_id: PjRT_Client_Connect E     Additional GRPC error information from remote target unknown_target_for_coordination_leader: E     :{""created"":"".563927882"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Barrier timed out. Barrier_id: PjRT_Client_Connect"",""grpc_status"":4} [type.googleapis.com/tensorflow.CoordinationServiceError=''] workspace/jax/_src/distributed.py:80: XlaRuntimeError  generated xml file: /workspace/outputs/junit_output_1.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============ 1 failed, 6 passed, 5 deselected in 372.30s (0:06:12) ============= ```  Failure summary outputtestjaxlibnightly12331.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.24.dev20221016%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (154.3 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.3/154.3 MB 22.9 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Collecting numpy>=1.20   Downloading numpy1.23.4cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 111.1 MB/s eta 0:00:00 Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.24.dev20221016+cuda11.cudnn82 numpy1.23.4 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuildf_ad95v8   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuildf_ad95v8   Resolved https://github.com/google/jax to commit 4cfa01f1cf1325d6c0ccc61b61ba8f1124c938af   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.24) (1.23.4) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 13.8 kB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.24) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.4.0py3noneany.whl (26 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.24py3noneany.whl size=1282455 sha256=cfc32e604f80c40248f964df6e801b380c94cc0afb13b8fa1188d45b6ae00314   Stored in directory: /tmp/pipephemwheelcachecgwijhou/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, jax Successfully installed jax0.3.24 opt_einsum3.3.0 typing_extensions4.4.0 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting pytest   Downloading pytest7.1.3py3noneany.whl (298 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.2/298.2 kB 39.2 MB/s eta 0:00:00 Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 29.1 MB/s eta 0:00:00 Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 17.2 MB/s eta 0:00:00 Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.3 tomli2.0.1 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.3) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 Mon Oct 17 12:08:40 GMT 2022 [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip [notice] A new release of pip available: 22.2.2 > 22.3 [notice] To update, run: pip install upgrade pip jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 jax                     0.3.24 jaxlib                  0.3.24.dev20221016+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum collected 12 items / 5 deselected / 7 selected workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum  workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum  workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_transparent_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_eval_shape PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_multi_input_multi_output PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f5302cea280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007fc24d316280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.010772: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".010376079"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.010814: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".010376079"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.010828: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".010376079"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:12"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_12.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.55s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007fc4bc6b6280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.099949: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".099502037"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.100000: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".099502037"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.100017: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".099502037"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:10"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_10.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.64s (0:01:12) ============= FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:53.909748: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".907841846"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:53.909797: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".907841846"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:53.909810: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".907841846"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:13"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_13.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.65s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f29561ff280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.186293: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".185856583"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.186349: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".185856583"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.186366: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".185856583"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:14"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_14.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.72s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f499f47b280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.273299: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".272855077"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.273346: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".272855077"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.273360: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".272855077"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:9"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_9.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.81s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f8b8ddb8280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.361552: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".361109353"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.361597: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".361109353"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.361609: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".361109353"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:11"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_11.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.90s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007f32d2412280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.449720: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".449282537"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.449778: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".449282537"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.449789: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".449282537"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:15"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_15.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 72.98s (0:01:12) ============= PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot Fatal Python error: Aborted Current thread 0x00007fb2aaa82280 (most recent call first):   File ""/workspace/jax/_src/distributed.py"", line 80 in initialize   File ""/workspace/jax/_src/distributed.py"", line 160 in initialize   File ""/workspace/tests/multiprocess_gpu_test.py"", line 483 in test_pjit_gda_non_contiguous_mesh_2d_aot   File ""/usr/local/lib/python3.8/unittest/case.py"", line 633 in _callTestMethod   File ""/usr/local/lib/python3.8/unittest/case.py"", line 676 in run   File ""/usr/local/lib/python3.8/unittest/case.py"", line 736 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/unittest.py"", line 330 in runtest   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 166 in pytest_runtest_call   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 259 in    File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 338 in from_call   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 258 in call_runtest_hook   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 219 in call_and_report   File ""/usr/local/lib/python3.8/sitepackages/_pytest/runner.py"", line 130 in runtestprotocol   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 68 in runforked   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 65 in _child   File ""/usr/local/lib/python3.8/sitepackages/py/_process/forkedfunc.py"", line 50 in __init__   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 73 in forked_run_report   File ""/usr/local/lib/python3.8/sitepackages/pytest_forked/__init__.py"", line 51 in pytest_runtest_protocol   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 347 in pytest_runtestloop   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 322 in _main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 268 in wrap_session   File ""/usr/local/lib/python3.8/sitepackages/_pytest/main.py"", line 315 in pytest_cmdline_main   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/usr/local/lib/python3.8/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 164 in main   File ""/usr/local/lib/python3.8/sitepackages/_pytest/config/__init__.py"", line 187 in console_main   File ""/usr/local/lib/python3.8/sitepackages/pytest/__main__.py"", line 5 in    File ""/usr/local/lib/python3.8/runpy.py"", line 86 in _run_code   File ""/usr/local/lib/python3.8/runpy.py"", line 193 in _run_module_as_main FAILED =================================== FAILURES =================================== ________ SlurmMultiNodeGpuTest.test_pjit_gda_non_contiguous_mesh_2d_aot ________ :1: running the test CRASHED with signal 6  captured stderr  20221017 12:09:54.537896: E external/org_tensorflow/tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:678] Coordination agent is in ERROR: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".537455530"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.537949: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.cc:452] Coordination service agent in error status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".537455530"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError=''] 20221017 12:09:54.537965: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/distributed/client.h:75] Terminating process because the coordinator detected missing heartbeats. This most likely indicates that another task died; see the other task logs for more details. Status: INVALID_ARGUMENT: Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8 Additional GRPC error information from remote target unknown_target_for_coordination_leader: :{""created"":"".537455530"",""description"":""Error received from peer ipv4:172.16.5.69:62673"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unexpected task registered with task_name=/job:jax_worker/replica:0/task:8"",""grpc_status"":3} [type.googleapis.com/tensorflow.CoordinationServiceError='']  generated xml file: /workspace/outputs/junit_output_8.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_pjit_gda_non_contiguous_mesh_2d_aot ============= 1 failed, 6 passed, 5 deselected in 73.07s (0:01:13) ============= ``` ",2022-10-14T12:09:34Z,Nightly-CI,closed,0,3,https://github.com/jax-ml/jax/issues/12804,I saw similar errors in Kokoro GPU runs yesterday. Maybe a flake?,"Right, this run didn't pick an updated nightly. The latest manual run picked up the latest jaxlib nightly and it failed for a different reason. ",Closing as likely stale.
2419,"以下是一个github上的jax下的一个issue, 标题是(dynamic update causes constant-folding)， 内容是 ( Description I see that whenever I use `in_place` update on an array created inside `jit`, I get constantfolding that increases compilation time considerably.  This can be seen in the following example: ```python import jax import time .jit def foo():     x = jax.numpy.ones((500, 500, 500))     x = x.at[(100, 100, 100)].set(2)     return x for _ in range(2):     start = time.time()     _ = foo().block_until_ready()     print(f'Time elapsed: {time.time()start:.3}s') ``` the output looks like: ``` 20221013 11:07:46.573300: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:   dynamicupdateslice.1 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above). This isn't necessarily a bug; constantfolding is inherently a tradeoff between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time. If you'd like to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. 20221013 11:07:47.789842: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2.216622055s Constant folding an instruction is taking > 1s:   dynamicupdateslice.1 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above). This isn't necessarily a bug; constantfolding is inherently a tradeoff between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time. If you'd like to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. Time elapsed: 3.31s Time elapsed: 0.00235s ``` Is this the expected behaviour?  What jax/jaxlib version are you using? jax v0.3.21, jaxlib v0.3.20  Which accelerator(s) are you using? CPU/GPU  Additional system info python 3.10.6, ubuntu 22.04  NVIDIA GPU info ``` Thu Oct 13 11:15:24 2022        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,dynamic update causes constant-folding," Description I see that whenever I use `in_place` update on an array created inside `jit`, I get constantfolding that increases compilation time considerably.  This can be seen in the following example: ```python import jax import time .jit def foo():     x = jax.numpy.ones((500, 500, 500))     x = x.at[(100, 100, 100)].set(2)     return x for _ in range(2):     start = time.time()     _ = foo().block_until_ready()     print(f'Time elapsed: {time.time()start:.3}s') ``` the output looks like: ``` 20221013 11:07:46.573300: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:65] Constant folding an instruction is taking > 1s:   dynamicupdateslice.1 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above). This isn't necessarily a bug; constantfolding is inherently a tradeoff between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time. If you'd like to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. 20221013 11:07:47.789842: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:133] The operation took 2.216622055s Constant folding an instruction is taking > 1s:   dynamicupdateslice.1 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above). This isn't necessarily a bug; constantfolding is inherently a tradeoff between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time. If you'd like to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. Time elapsed: 3.31s Time elapsed: 0.00235s ``` Is this the expected behaviour?  What jax/jaxlib version are you using? jax v0.3.21, jaxlib v0.3.20  Which accelerator(s) are you using? CPU/GPU  Additional system info python 3.10.6, ubuntu 22.04  NVIDIA GPU info ``` Thu Oct 13 11:15:24 2022        ++  ++ ```",2022-10-13T09:15:57Z,bug XLA,closed,0,2,https://github.com/jax-ml/jax/issues/12789,Thanks for the report! We're looking at why XLA is deciding to constantfold such a large (500MB) constant at compiletime. We'll update here when we get a chance.,This issue is fixed in XLA and the fix should be in the next jaxlib release.
15434,"以下是一个github上的jax下的一个issue, 标题是(jax with iree backend gives compilation error on array update via .at[].set() (works with default cpu backend))， 内容是 ( Description Trying to use the iree backend with jax yields an error when updating slice with .at[].set() It looks like the underlying logic does not correctly assemble scatter_indices during broadcast or slice. Reproducer: ```python import jax import jax.numpy as jnp crashes later: jax.config.update('jax_platform_name', 'iree') works: jax.config.update('jax_platform_name', 'cpu') output: [  0   1 99 99 99 99   6   7] .jit def setslice(x):   myslice = slice(2,2)   return x.at[myslice].set(99) tst = jnp.arange(8) result = setslice(tst) print(result) ``` With the standard ""cpu"" backend, it yields correct result of  [  0   1 99 99 99 99   6   7] With the ""iree"" backend (cpu), compilation fails: ``` /usr/bin/python3 simple.py Traceback (most recent call last):   File ""/Users/aUser/reproducer/simple.py"", line 18, in      result = setslice(tst)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/api.py"", line 606, in cache_miss     out_flat = xla.xla_call(   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/core.py"", line 1939, in bind     return call_bind(self, fun, *args, **params)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/core.py"", line 1955, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/core.py"", line 701, in process_call     return primitive.impl(f, *tracers, **params)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 234, in _xla_call_impl     compiled_fun = xla_callable(fun, device, backend, name, donated_invars,   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/linear_util.py"", line 309, in memoized_fun     ans = call(fun, *args)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 342, in _xla_callable_uncached     return lower_xla_callable(fun, device, backend, name, donated_invars, False,   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 978, in compile     self._executable = XlaCompiledComputation.from_xla_computation(   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 1136, in from_xla_computation     compiled = compile_or_get_cached(backend, xla_computation, options,   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 1054, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/profiler.py"", line 313, in wrapper     return func(*args, **kwargs)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 994, in backend_compile     return backend.compile(built_c, compile_options=options)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/iree.py"", line 186, in compile     iree_binary = iree.compiler.compile_str(   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/core.py"", line 278, in compile_str     result = invoke_immediate(cl, immediate_input=input_bytes)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/binaries.py"", line 196, in invoke_immediate     raise CompilerToolError(process) jax._src.traceback_util.UnfilteredStackTrace: iree.compiler.tools.binaries.CompilerToolError: Error invoking IREE compiler tool ireecompile Diagnostics: /Users/aUser/reproducer/simple.py:15:1: error: 'mhlo.scatter' op expects bounds of the scatter dimensions of updates to be same as the bounds of the corresponding dimensions of scatter indices. For scatter dimension 1, updates bound is 4 , scatter_indices bound is 1.   return x.at[myslice].set(99) ^ :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` compilation failed Invoked with:  ireecompile /Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/../_mlir_libs/ireecompile  ireeinputtype=mhlo ireevmbytecodemoduleoutputformat=flatbufferbinary ireehaltargetbackends=llvmcpu ireellvmembeddedlinkerpath=/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/../_mlir_libs/ireelld mlirprintdebuginfo mlirprintopondiagnostic=false mlirpasspipelinecrashreproducer=/Users/aUser/reproducer/savedir/corereproducer.mlir Need more information? Set IREE_SAVE_TEMPS=/some/dir in your environment to save all artifacts and reproducers. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/aUser/reproducer/simple.py"", line 18, in      result = setslice(tst)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/iree.py"", line 186, in compile     iree_binary = iree.compiler.compile_str(   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/core.py"", line 278, in compile_str     result = invoke_immediate(cl, immediate_input=input_bytes)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/binaries.py"", line 196, in invoke_immediate     raise CompilerToolError(process) iree.compiler.tools.binaries.CompilerToolError: Error invoking IREE compiler tool ireecompile Diagnostics: /Users/aUser/reproducer/simple.py:15:1: error: 'mhlo.scatter' op expects bounds of the scatter dimensions of updates to be same as the bounds of the corresponding dimensions of scatter indices. For scatter dimension 1, updates bound is 4 , scatter_indices bound is 1.   return x.at[myslice].set(99) ^ :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` compilation failed Invoked with:  ireecompile /Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/../_mlir_libs/ireecompile  ireeinputtype=mhlo ireevmbytecodemoduleoutputformat=flatbufferbinary ireehaltargetbackends=llvmcpu ireellvmembeddedlinkerpath=/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/../_mlir_libs/ireelld mlirprintdebuginfo mlirprintopondiagnostic=false mlirpasspipelinecrashreproducer=/Users/aUser/reproducer/savedir/corereproducer.mlir Need more information? Set IREE_SAVE_TEMPS=/some/dir in your environment to save all artifacts and reproducers. ``` Output in reproducer: ``` ireeopt savedir/coreinput.mlir  module  {   func.func public (%arg0: tensor) > tensor {     %0 = mhlo.constant dense : tensor     %1 = ""mhlo.broadcast_in_dim""(%0) {broadcast_dimensions = dense : tensor} : (tensor) > tensor     %2 = mhlo.constant dense : tensor     %3 = ""mhlo.broadcast_in_dim""(%2) {broadcast_dimensions = dense : tensor} : (tensor) > tensor     %4 = ""mhlo.scatter""(%arg0, %1, %3) ({     ^bb0(%arg1: tensor, %arg2: tensor):       mhlo.return %arg2 : tensor     }) {indices_are_sorted = true, scatter_dimension_numbers = mhlo.scatter, unique_indices = true} : (tensor, tensor, tensor) > tensor     return %4 : tensor   } } ``` Output of corereproducer: ``` more savedir/corereproducer.mlir  loc0 = loc(unknown) module  {   func.func public (%arg0: tensor loc(unknown)) > tensor {     %0 = mhlo.constant dense : tensor loc(loc0)     %1 = ""mhlo.broadcast_in_dim""(%0) {broadcast_dimensions = dense : tensor} : (tensor) > tensor loc(loc1)     %2 = mhlo.constant dense : tensor loc(loc0)     %3 = ""mhlo.broadcast_in_dim""(%2) {broadcast_dimensions = dense : tensor} : (tensor) > tensor loc(loc2)     %4 = ""mhlo.scatter""(%arg0, %1, %3) ({     ^bb0(%arg1: tensor loc(unknown), %arg2: tensor loc(unknown)):       mhlo.return %arg2 : tensor loc(loc3)     }) {indices_are_sorted = true, scatter_dimension_numbers = mhlo.scatter, unique_indices = true} : (tensor, tensor, tensor) > tensor loc(loc3)     return %4 : tensor loc(loc0)   } loc(loc0) } loc(loc0) loc1 = loc(""jit(setslice)/jit(main)/broadcast_in_dim[shape=(1,) broadcast_dimensions=()]""(""/Users/aUser/reproducer/simple.py"":15:1)) loc2 = loc(""jit(setslice)/jit(main)/broadcast_in_dim[shape=(4,) broadcast_dimensions=()]""(""/Users/aUser/reproducer/simple.py"":15:1)) loc3 = loc(""jit(setslice)/jit(main)/scatter[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(0,), inserted_window_dims=(), scatter_dims_to_operand_dims=(0,)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.FILL_OR_DROP]""(""/Users/aUser/reproducer/simple.py"":15:1)) {   external_resources: {     mlir_reproducer: {       pipeline: ""stablehlolegalizetohlo, func.func(mhlolegalizecontrolflow,ireetoplevelscftocfg,ireemhlotomhlopreprocessing{orderconvfeatures=true},canonicalize{  maxiterations=10 regionsimplify=true topdown=true},shapetoshapelowering), convertshapetostd, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true}), inline{defaultpipeline=canonicalize{  maxiterations=10 regionsimplify=true topdown=true} maxiterations=4 }, ireeutildemotei64toi32, ireeutildemotef64tof32, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,hlolegalizeshapecomputations,ireemhlotolinalgext,ireemhlotolinalgontensors), reconcileunrealizedcasts, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true}), ireemhloverifycompilerinputlegality, ireeimportpublic, ireeimportmlprogram, ireesanitizemodulenames, ireeabiwrapentrypoints, inline{defaultpipeline=canonicalize{  maxiterations=10 regionsimplify=true topdown=true} maxiterations=4 }, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse), symboldce, ireeutildemotef64tof32, func.func(ireeflowconvert1x1filterconv2dtomatmul,ireeflowdetachelementwisefromnamedops,ireeverifyinputlegality),util.initializer(ireeflowconvert1x1filterconv2dtomatmul,ireeflowdetachelementwisefromnamedops,ireeverifyinputlegality), linalgnamedopconversion, ireeflowexpandtensorshapes, ireeutilfixedpointiterator{maxiterations=10 pipeline=func.func(ireeutilsimplifyglobalaccesses),util.initializer(ireeutilsimplifyglobalaccesses),ireeutilapplypatterns,ireeutilfoldglobals,func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse)}, ireeflowtensorpadtotensorinsertslice{skiponelinalgusecase=false}, func.func(convertelementwisetolinalg,linalgfoldunitextentdims{foldonetriploopsonly=false},ireeflowinterchangegenericops,resolveshapedtyperesultdims,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeflowfusionoftensorops{fusemultiuse=false multiusefusioniteration=2},linalgdetensorize{aggressivemode=false},canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeflowsplitreductionops,ireeflowinterchangegenericops,ireeflowdispatchlinalgontensorspass{aggressivefusion=false},ireeflowcapturedispatchdynamicdims,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse),util.initializer(convertelementwisetolinalg,linalgfoldunitextentdims{foldonetriploopsonly=false},ireeflowinterchangegenericops,resolveshapedtyperesultdims,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeflowfusionoftensorops{fusemultiuse=false multiusefusioniteration=2},linalgdetensorize{aggressivemode=false},canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeflowsplitreductionops,ireeflowinterchangegenericops,ireeflowdispatchlinalgontensorspass{aggressivefusion=false},ireeflowcapturedispatchdynamicdims,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse), ireeflowinitializeemptytensors, ireeflowoutlinedispatchregions, flow.executable(ireeutilstripdebugops),func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true}),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true}), ireeflowdeduplicateexecutables, flow.executable(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse),func.func(ireeflowcleanuptensorshapes,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse),util.initializer(ireeflowcleanuptensorshapes,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse), symboldce, ireestreamverifyinput, ireestreamoutlineconstants, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses), ireeutilapplypatterns, ireeutilfoldglobals, ireeutilfuseglobals, ireestreamconversion, ireestreamverifyloweringtotensors, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses), ireeutilapplypatterns, ireeutilfoldglobals, ireeutilfuseglobals, ireeutilcombineinitializers, func.func(ireestreamencodehosttensors),stream.executable(ireestreamencodedevicetensors),util.initializer(ireestreamencodehosttensors), ireestreammaterializebuiltins, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses), ireeutilapplypatterns, ireeuti ```  What jax/jaxlib version are you using? jax v0.3.23 jaxlib v0.3.22  Which accelerator(s) are you using? CPU  Additional system info tested with Python 3.9.6, 3.10 under Linux, macOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax with iree backend gives compilation error on array update via .at[].set() (works with default cpu backend)," Description Trying to use the iree backend with jax yields an error when updating slice with .at[].set() It looks like the underlying logic does not correctly assemble scatter_indices during broadcast or slice. Reproducer: ```python import jax import jax.numpy as jnp crashes later: jax.config.update('jax_platform_name', 'iree') works: jax.config.update('jax_platform_name', 'cpu') output: [  0   1 99 99 99 99   6   7] .jit def setslice(x):   myslice = slice(2,2)   return x.at[myslice].set(99) tst = jnp.arange(8) result = setslice(tst) print(result) ``` With the standard ""cpu"" backend, it yields correct result of  [  0   1 99 99 99 99   6   7] With the ""iree"" backend (cpu), compilation fails: ``` /usr/bin/python3 simple.py Traceback (most recent call last):   File ""/Users/aUser/reproducer/simple.py"", line 18, in      result = setslice(tst)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/api.py"", line 606, in cache_miss     out_flat = xla.xla_call(   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/core.py"", line 1939, in bind     return call_bind(self, fun, *args, **params)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/core.py"", line 1955, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/core.py"", line 701, in process_call     return primitive.impl(f, *tracers, **params)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 234, in _xla_call_impl     compiled_fun = xla_callable(fun, device, backend, name, donated_invars,   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/linear_util.py"", line 309, in memoized_fun     ans = call(fun, *args)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 342, in _xla_callable_uncached     return lower_xla_callable(fun, device, backend, name, donated_invars, False,   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 978, in compile     self._executable = XlaCompiledComputation.from_xla_computation(   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 1136, in from_xla_computation     compiled = compile_or_get_cached(backend, xla_computation, options,   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 1054, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/profiler.py"", line 313, in wrapper     return func(*args, **kwargs)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/dispatch.py"", line 994, in backend_compile     return backend.compile(built_c, compile_options=options)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/iree.py"", line 186, in compile     iree_binary = iree.compiler.compile_str(   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/core.py"", line 278, in compile_str     result = invoke_immediate(cl, immediate_input=input_bytes)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/binaries.py"", line 196, in invoke_immediate     raise CompilerToolError(process) jax._src.traceback_util.UnfilteredStackTrace: iree.compiler.tools.binaries.CompilerToolError: Error invoking IREE compiler tool ireecompile Diagnostics: /Users/aUser/reproducer/simple.py:15:1: error: 'mhlo.scatter' op expects bounds of the scatter dimensions of updates to be same as the bounds of the corresponding dimensions of scatter indices. For scatter dimension 1, updates bound is 4 , scatter_indices bound is 1.   return x.at[myslice].set(99) ^ :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` compilation failed Invoked with:  ireecompile /Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/../_mlir_libs/ireecompile  ireeinputtype=mhlo ireevmbytecodemoduleoutputformat=flatbufferbinary ireehaltargetbackends=llvmcpu ireellvmembeddedlinkerpath=/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/../_mlir_libs/ireelld mlirprintdebuginfo mlirprintopondiagnostic=false mlirpasspipelinecrashreproducer=/Users/aUser/reproducer/savedir/corereproducer.mlir Need more information? Set IREE_SAVE_TEMPS=/some/dir in your environment to save all artifacts and reproducers. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/aUser/reproducer/simple.py"", line 18, in      result = setslice(tst)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/jax/_src/iree.py"", line 186, in compile     iree_binary = iree.compiler.compile_str(   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/core.py"", line 278, in compile_str     result = invoke_immediate(cl, immediate_input=input_bytes)   File ""/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/binaries.py"", line 196, in invoke_immediate     raise CompilerToolError(process) iree.compiler.tools.binaries.CompilerToolError: Error invoking IREE compiler tool ireecompile Diagnostics: /Users/aUser/reproducer/simple.py:15:1: error: 'mhlo.scatter' op expects bounds of the scatter dimensions of updates to be same as the bounds of the corresponding dimensions of scatter indices. For scatter dimension 1, updates bound is 4 , scatter_indices bound is 1.   return x.at[myslice].set(99) ^ :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` :0: error: Failures have been detected while processing an MLIR pass pipeline :0: note: Pipeline failed while executing [`MHLOToMHLOPreprocessing` on 'func.func' operation: ]: reproducer generated at `/Users/aUser/reproducer/savedir/corereproducer.mlir` compilation failed Invoked with:  ireecompile /Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/../_mlir_libs/ireecompile  ireeinputtype=mhlo ireevmbytecodemoduleoutputformat=flatbufferbinary ireehaltargetbackends=llvmcpu ireellvmembeddedlinkerpath=/Users/aUser/Library/Python/3.9/lib/python/sitepackages/iree/compiler/tools/../_mlir_libs/ireelld mlirprintdebuginfo mlirprintopondiagnostic=false mlirpasspipelinecrashreproducer=/Users/aUser/reproducer/savedir/corereproducer.mlir Need more information? Set IREE_SAVE_TEMPS=/some/dir in your environment to save all artifacts and reproducers. ``` Output in reproducer: ``` ireeopt savedir/coreinput.mlir  module  {   func.func public (%arg0: tensor) > tensor {     %0 = mhlo.constant dense : tensor     %1 = ""mhlo.broadcast_in_dim""(%0) {broadcast_dimensions = dense : tensor} : (tensor) > tensor     %2 = mhlo.constant dense : tensor     %3 = ""mhlo.broadcast_in_dim""(%2) {broadcast_dimensions = dense : tensor} : (tensor) > tensor     %4 = ""mhlo.scatter""(%arg0, %1, %3) ({     ^bb0(%arg1: tensor, %arg2: tensor):       mhlo.return %arg2 : tensor     }) {indices_are_sorted = true, scatter_dimension_numbers = mhlo.scatter, unique_indices = true} : (tensor, tensor, tensor) > tensor     return %4 : tensor   } } ``` Output of corereproducer: ``` more savedir/corereproducer.mlir  loc0 = loc(unknown) module  {   func.func public (%arg0: tensor loc(unknown)) > tensor {     %0 = mhlo.constant dense : tensor loc(loc0)     %1 = ""mhlo.broadcast_in_dim""(%0) {broadcast_dimensions = dense : tensor} : (tensor) > tensor loc(loc1)     %2 = mhlo.constant dense : tensor loc(loc0)     %3 = ""mhlo.broadcast_in_dim""(%2) {broadcast_dimensions = dense : tensor} : (tensor) > tensor loc(loc2)     %4 = ""mhlo.scatter""(%arg0, %1, %3) ({     ^bb0(%arg1: tensor loc(unknown), %arg2: tensor loc(unknown)):       mhlo.return %arg2 : tensor loc(loc3)     }) {indices_are_sorted = true, scatter_dimension_numbers = mhlo.scatter, unique_indices = true} : (tensor, tensor, tensor) > tensor loc(loc3)     return %4 : tensor loc(loc0)   } loc(loc0) } loc(loc0) loc1 = loc(""jit(setslice)/jit(main)/broadcast_in_dim[shape=(1,) broadcast_dimensions=()]""(""/Users/aUser/reproducer/simple.py"":15:1)) loc2 = loc(""jit(setslice)/jit(main)/broadcast_in_dim[shape=(4,) broadcast_dimensions=()]""(""/Users/aUser/reproducer/simple.py"":15:1)) loc3 = loc(""jit(setslice)/jit(main)/scatter[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(0,), inserted_window_dims=(), scatter_dims_to_operand_dims=(0,)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.FILL_OR_DROP]""(""/Users/aUser/reproducer/simple.py"":15:1)) {   external_resources: {     mlir_reproducer: {       pipeline: ""stablehlolegalizetohlo, func.func(mhlolegalizecontrolflow,ireetoplevelscftocfg,ireemhlotomhlopreprocessing{orderconvfeatures=true},canonicalize{  maxiterations=10 regionsimplify=true topdown=true},shapetoshapelowering), convertshapetostd, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true}), inline{defaultpipeline=canonicalize{  maxiterations=10 regionsimplify=true topdown=true} maxiterations=4 }, ireeutildemotei64toi32, ireeutildemotef64tof32, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,hlolegalizeshapecomputations,ireemhlotolinalgext,ireemhlotolinalgontensors), reconcileunrealizedcasts, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true}), ireemhloverifycompilerinputlegality, ireeimportpublic, ireeimportmlprogram, ireesanitizemodulenames, ireeabiwrapentrypoints, inline{defaultpipeline=canonicalize{  maxiterations=10 regionsimplify=true topdown=true} maxiterations=4 }, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse), symboldce, ireeutildemotef64tof32, func.func(ireeflowconvert1x1filterconv2dtomatmul,ireeflowdetachelementwisefromnamedops,ireeverifyinputlegality),util.initializer(ireeflowconvert1x1filterconv2dtomatmul,ireeflowdetachelementwisefromnamedops,ireeverifyinputlegality), linalgnamedopconversion, ireeflowexpandtensorshapes, ireeutilfixedpointiterator{maxiterations=10 pipeline=func.func(ireeutilsimplifyglobalaccesses),util.initializer(ireeutilsimplifyglobalaccesses),ireeutilapplypatterns,ireeutilfoldglobals,func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse)}, ireeflowtensorpadtotensorinsertslice{skiponelinalgusecase=false}, func.func(convertelementwisetolinalg,linalgfoldunitextentdims{foldonetriploopsonly=false},ireeflowinterchangegenericops,resolveshapedtyperesultdims,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeflowfusionoftensorops{fusemultiuse=false multiusefusioniteration=2},linalgdetensorize{aggressivemode=false},canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeflowsplitreductionops,ireeflowinterchangegenericops,ireeflowdispatchlinalgontensorspass{aggressivefusion=false},ireeflowcapturedispatchdynamicdims,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse),util.initializer(convertelementwisetolinalg,linalgfoldunitextentdims{foldonetriploopsonly=false},ireeflowinterchangegenericops,resolveshapedtyperesultdims,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeflowfusionoftensorops{fusemultiuse=false multiusefusioniteration=2},linalgdetensorize{aggressivemode=false},canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeflowsplitreductionops,ireeflowinterchangegenericops,ireeflowdispatchlinalgontensorspass{aggressivefusion=false},ireeflowcapturedispatchdynamicdims,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse), ireeflowinitializeemptytensors, ireeflowoutlinedispatchregions, flow.executable(ireeutilstripdebugops),func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true}),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true}), ireeflowdeduplicateexecutables, flow.executable(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse),func.func(ireeflowcleanuptensorshapes,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse),util.initializer(ireeflowcleanuptensorshapes,canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse), symboldce, ireestreamverifyinput, ireestreamoutlineconstants, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses), ireeutilapplypatterns, ireeutilfoldglobals, ireeutilfuseglobals, ireestreamconversion, ireestreamverifyloweringtotensors, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses), ireeutilapplypatterns, ireeutilfoldglobals, ireeutilfuseglobals, ireeutilcombineinitializers, func.func(ireestreamencodehosttensors),stream.executable(ireestreamencodedevicetensors),util.initializer(ireestreamencodehosttensors), ireestreammaterializebuiltins, func.func(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses),util.initializer(canonicalize{  maxiterations=10 regionsimplify=true topdown=true},cse,ireeutilsimplifyglobalaccesses), ireeutilapplypatterns, ireeuti ```  What jax/jaxlib version are you using? jax v0.3.23 jaxlib v0.3.22  Which accelerator(s) are you using? CPU  Additional system info tested with Python 3.9.6, 3.10 under Linux, macOS  NVIDIA GPU info _No response_",2022-10-13T08:08:05Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/12786,"Thanks, this is an IREE bug I think. I filed it with IREE as https://github.com/ireeorg/iree/issues/10774",Seems to be fixed as of IREE candidate20221102.315. Many thanks!
1747,"以下是一个github上的jax下的一个issue, 标题是(calculates the 2-norm partial gradient as coefficient for loss )， 内容是 (I am trying to replicate the VQGAN in JAX and FLAX, it works great, but I met an issue with how to calculate the 2norm partial gradient as the coefficient for loss. The function can be seen here (https://github.com/CompVis/tamingtransformers/blob/24268930bf1dce879235a7fddd0b2355b84d7ea6/taming/modules/losses/vqperceptual.pyL63)  ```     def calculate_adaptive_weight(self, nll_loss, g_loss, last_layer=None):         if last_layer is not None:             nll_grads = torch.autograd.grad(nll_loss, last_layer, retain_graph=True)[0]             g_grads = torch.autograd.grad(g_loss, last_layer, retain_graph=True)[0]         else:             nll_grads = torch.autograd.grad(nll_loss, self.last_layer[0], retain_graph=True)[0]             g_grads = torch.autograd.grad(g_loss, self.last_layer[0], retain_graph=True)[0]         d_weight = torch.norm(nll_grads) / (torch.norm(g_grads) + 1e4)         d_weight = torch.clamp(d_weight, 0.0, 1e4).detach()         d_weight = d_weight * self.discriminator_weight         return d_weight ``` The d_weight fraction is used as the weight coefficient of the discriminator to weight the total_loss. And It calculates the 2norm ratio after deriving the parameters of the last layer of the model based on rec_loss and g_loss.  Do I need to nest the grad function inside the loss function? Is there any faster implementation? Thanks Please:  [ x] Check for duplicate requests.  [ x] Describe your goal, and if possible provide a code snippet with a motivating example.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,calculates the 2-norm partial gradient as coefficient for loss ,"I am trying to replicate the VQGAN in JAX and FLAX, it works great, but I met an issue with how to calculate the 2norm partial gradient as the coefficient for loss. The function can be seen here (https://github.com/CompVis/tamingtransformers/blob/24268930bf1dce879235a7fddd0b2355b84d7ea6/taming/modules/losses/vqperceptual.pyL63)  ```     def calculate_adaptive_weight(self, nll_loss, g_loss, last_layer=None):         if last_layer is not None:             nll_grads = torch.autograd.grad(nll_loss, last_layer, retain_graph=True)[0]             g_grads = torch.autograd.grad(g_loss, last_layer, retain_graph=True)[0]         else:             nll_grads = torch.autograd.grad(nll_loss, self.last_layer[0], retain_graph=True)[0]             g_grads = torch.autograd.grad(g_loss, self.last_layer[0], retain_graph=True)[0]         d_weight = torch.norm(nll_grads) / (torch.norm(g_grads) + 1e4)         d_weight = torch.clamp(d_weight, 0.0, 1e4).detach()         d_weight = d_weight * self.discriminator_weight         return d_weight ``` The d_weight fraction is used as the weight coefficient of the discriminator to weight the total_loss. And It calculates the 2norm ratio after deriving the parameters of the last layer of the model based on rec_loss and g_loss.  Do I need to nest the grad function inside the loss function? Is there any faster implementation? Thanks Please:  [ x] Check for duplicate requests.  [ x] Describe your goal, and if possible provide a code snippet with a motivating example.",2022-10-11T20:39:39Z,question,open,1,0,https://github.com/jax-ml/jax/issues/12755
434,"以下是一个github上的jax下的一个issue, 标题是(Fix the type annotation of return type of `device_buffer` and `device_buffers` which return `ArrayImpl` instead of DeviceArray.)， 内容是 (Fix the type annotation of return type of `device_buffer` and `device_buffers` which return `ArrayImpl` instead of DeviceArray.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix the type annotation of return type of `device_buffer` and `device_buffers` which return `ArrayImpl` instead of DeviceArray.,Fix the type annotation of return type of `device_buffer` and `device_buffers` which return `ArrayImpl` instead of DeviceArray.,2022-10-10T20:00:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12724
4518,"以下是一个github上的jax下的一个issue, 标题是(Memory leak when closing over large constants in compiled jax.lax.cond statement)， 内容是 ( Description I was running into an issue that I could condense to an elementary example. It seems to me as if closing over large constants in lax.cond creates a memory leak. I've reported a similar issue concerning lax.fori_loop in CC(Memory leak in compiled loops closing over large constants) which was solved in PR CC(Use weakref to avoid cache leaks in fori_loop.). However, my followup post in the original issue got no attention so I decided to close the original issue since the originally reported issue was fixed and to open this one. I hope that is ok. I'm aware that closing over large constant is not recommended and instead they should be passed as arguments. Nonetheless, this leaking behaviour is unexpected. Maybe it can be  similar to CC(Memory leak in compiled loops closing over large constants)   easily fixed taking weak references to true_fun and false_fun.  MRE The sideeffectfree function leak creates in ints body a large constant and later compiles a function which closes over the large constant within a lax.cond statement. The compiled function called. When leaving the function leak, all references to the large constant are destroyed, including the closure and the compiled closure and the memory should be freed. However, it is not. ```py import numpy as np import jax import psutil def print_mem_usage() > None:         print(         f""Resident memory: {psutil.Process().memory_info().rss // (1024 * 1024)} MB""     ) def print_live_buffers() > None:     lbs = jax.lib.xla_bridge.get_backend().live_buffers()     print(f""Number live buffers: {len(lbs)}"")     for idx, lb in enumerate(lbs):         print(f""\tlbs[{idx}]: shape {lb.shape} at 0x{lb.unsafe_buffer_pointer():x}"") def print_num_live_executables() > None:     les = jax.lib.xla_bridge.get_backend().live_executables()     print(f""Number live executables: {len(les)}"") def leak():      ~ 1 GB of float32s     size = 1024 * 1024 * 1024 // 4     data = jax.device_put(np.empty((size, ), dtype=np.float32) + 1)     def g():         return jax.lax.cond(             True,             lambda: data[0],             lambda: data[1],         )     jg = jax.jit(g)     _ = jg().block_until_ready()     del g, jg, data, _ def run() > None:     for iter in range(5):         print(f""iter: {iter} ================================================================"")         print_mem_usage()         _ = leak()         print_mem_usage()         print_live_buffers()         print_num_live_executables() if __name__ == ""__main__"":     run() ``` Output ``` $ python leak3.py      iter: 0 ================================================================ Resident memory: 116 MB Resident memory: 1168 MB Number live buffers: 1 	lbs[0]: shape (268435456,) at 0x293698000 Number live executables: 0 iter: 1 ================================================================ Resident memory: 1168 MB Resident memory: 2192 MB Number live buffers: 2 	lbs[0]: shape (268435456,) at 0x2d3698000 	lbs[1]: shape (268435456,) at 0x293698000 Number live executables: 0 iter: 2 ================================================================ Resident memory: 2192 MB Resident memory: 3216 MB Number live buffers: 3 	lbs[0]: shape (268435456,) at 0x313698000 	lbs[1]: shape (268435456,) at 0x2d3698000 	lbs[2]: shape (268435456,) at 0x293698000 Number live executables: 0 iter: 3 ================================================================ Resident memory: 3216 MB Resident memory: 4242 MB Number live buffers: 4 	lbs[0]: shape (268435456,) at 0x353698000 	lbs[1]: shape (268435456,) at 0x313698000 	lbs[2]: shape (268435456,) at 0x2d3698000 	lbs[3]: shape (268435456,) at 0x293698000 Number live executables: 0 iter: 4 ================================================================ Resident memory: 4242 MB Resident memory: 5266 MB Number live buffers: 5 	lbs[0]: shape (268435456,) at 0x393698000 	lbs[1]: shape (268435456,) at 0x353698000 	lbs[2]: shape (268435456,) at 0x313698000 	lbs[3]: shape (268435456,) at 0x2d3698000 	lbs[4]: shape (268435456,) at 0x293698000 Number live executables: 0 ```   What jax/jaxlib version are you using? jax v0.3.21, jaxlib v0.3.20  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.5 on a Mac with M1Pro/Ultimate (MacOS 12.6))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Memory leak when closing over large constants in compiled jax.lax.cond statement," Description I was running into an issue that I could condense to an elementary example. It seems to me as if closing over large constants in lax.cond creates a memory leak. I've reported a similar issue concerning lax.fori_loop in CC(Memory leak in compiled loops closing over large constants) which was solved in PR CC(Use weakref to avoid cache leaks in fori_loop.). However, my followup post in the original issue got no attention so I decided to close the original issue since the originally reported issue was fixed and to open this one. I hope that is ok. I'm aware that closing over large constant is not recommended and instead they should be passed as arguments. Nonetheless, this leaking behaviour is unexpected. Maybe it can be  similar to CC(Memory leak in compiled loops closing over large constants)   easily fixed taking weak references to true_fun and false_fun.  MRE The sideeffectfree function leak creates in ints body a large constant and later compiles a function which closes over the large constant within a lax.cond statement. The compiled function called. When leaving the function leak, all references to the large constant are destroyed, including the closure and the compiled closure and the memory should be freed. However, it is not. ```py import numpy as np import jax import psutil def print_mem_usage() > None:         print(         f""Resident memory: {psutil.Process().memory_info().rss // (1024 * 1024)} MB""     ) def print_live_buffers() > None:     lbs = jax.lib.xla_bridge.get_backend().live_buffers()     print(f""Number live buffers: {len(lbs)}"")     for idx, lb in enumerate(lbs):         print(f""\tlbs[{idx}]: shape {lb.shape} at 0x{lb.unsafe_buffer_pointer():x}"") def print_num_live_executables() > None:     les = jax.lib.xla_bridge.get_backend().live_executables()     print(f""Number live executables: {len(les)}"") def leak():      ~ 1 GB of float32s     size = 1024 * 1024 * 1024 // 4     data = jax.device_put(np.empty((size, ), dtype=np.float32) + 1)     def g():         return jax.lax.cond(             True,             lambda: data[0],             lambda: data[1],         )     jg = jax.jit(g)     _ = jg().block_until_ready()     del g, jg, data, _ def run() > None:     for iter in range(5):         print(f""iter: {iter} ================================================================"")         print_mem_usage()         _ = leak()         print_mem_usage()         print_live_buffers()         print_num_live_executables() if __name__ == ""__main__"":     run() ``` Output ``` $ python leak3.py      iter: 0 ================================================================ Resident memory: 116 MB Resident memory: 1168 MB Number live buffers: 1 	lbs[0]: shape (268435456,) at 0x293698000 Number live executables: 0 iter: 1 ================================================================ Resident memory: 1168 MB Resident memory: 2192 MB Number live buffers: 2 	lbs[0]: shape (268435456,) at 0x2d3698000 	lbs[1]: shape (268435456,) at 0x293698000 Number live executables: 0 iter: 2 ================================================================ Resident memory: 2192 MB Resident memory: 3216 MB Number live buffers: 3 	lbs[0]: shape (268435456,) at 0x313698000 	lbs[1]: shape (268435456,) at 0x2d3698000 	lbs[2]: shape (268435456,) at 0x293698000 Number live executables: 0 iter: 3 ================================================================ Resident memory: 3216 MB Resident memory: 4242 MB Number live buffers: 4 	lbs[0]: shape (268435456,) at 0x353698000 	lbs[1]: shape (268435456,) at 0x313698000 	lbs[2]: shape (268435456,) at 0x2d3698000 	lbs[3]: shape (268435456,) at 0x293698000 Number live executables: 0 iter: 4 ================================================================ Resident memory: 4242 MB Resident memory: 5266 MB Number live buffers: 5 	lbs[0]: shape (268435456,) at 0x393698000 	lbs[1]: shape (268435456,) at 0x353698000 	lbs[2]: shape (268435456,) at 0x313698000 	lbs[3]: shape (268435456,) at 0x2d3698000 	lbs[4]: shape (268435456,) at 0x293698000 Number live executables: 0 ```   What jax/jaxlib version are you using? jax v0.3.21, jaxlib v0.3.20  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.5 on a Mac with M1Pro/Ultimate (MacOS 12.6)",2022-10-10T16:11:44Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/12719,"October 10 2022! Oh man... If we comment out this ``, the leak goes away. And looking at the code, that cache is clearly holding strong references to the callables! We need it to be more like `weakref_lru_cache`. But the first arg is a list here..."
890,"以下是一个github上的jax下的一个issue, 标题是(Reapply: Use input-output aliasing for jaxlib GPU custom calls.)， 内容是 (Reapply: Use inputoutput aliasing for jaxlib GPU custom calls. Previously we had no way to tell XLA that inputs and outputs of GPU custom calls must alias. This now works in XLA:GPU so we can just ask XLA to enforce the aliasing we need. It turns out some users are relying on the API contract of the custom calls within serialized HLO remaining stable. For the moment, we reapply only the Python changes. The C++ code is already tolerant of both aliased and unaliased outputs, and this gets us all the benefit of saving a copy. We can break backwards compatibility on the serialized HLO after users upgrade their saved HLO to the aliased version.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Reapply: Use input-output aliasing for jaxlib GPU custom calls.,"Reapply: Use inputoutput aliasing for jaxlib GPU custom calls. Previously we had no way to tell XLA that inputs and outputs of GPU custom calls must alias. This now works in XLA:GPU so we can just ask XLA to enforce the aliasing we need. It turns out some users are relying on the API contract of the custom calls within serialized HLO remaining stable. For the moment, we reapply only the Python changes. The C++ code is already tolerant of both aliased and unaliased outputs, and this gets us all the benefit of saving a copy. We can break backwards compatibility on the serialized HLO after users upgrade their saved HLO to the aliased version.",2022-10-10T15:41:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12718
2259,"以下是一个github上的jax下的一个issue, 标题是(Unused arguments to function changes output)， 内容是 ( Description After a long time tracking down a NaN in a neural network training on a TPU, I was able to isolate a case where the output of the jitcompiled forward/backward pass of the network changes depending on whether/what extra arrays, that are never used in the actual computations, are passed into the function as additional arguments and then returned unchanged. Depending on exactly what unused arrays are passed in I have seen:  Sometimes the mean gradient will become NaN, sometimes not  Sometimes the loss will become NaN, sometimes not  Sometimes some of the intermediate values in the network will be reported to be mathematically impossible values (in particular this value will sometimes be much greater than 0), and sometimes not I can't think of an explanation for this other then some kind of memorymanagement error, so I am hoping someone here can help me figure out what is going on. I built a repo to reproduce the issue here,  it runs a simplified Transformer neural network over a batch of inputs. I would have liked to simplify things more, but so far my attempts have resulted in the NaN disappearing.  Some other notes:  I have seen this issue both with and without using `donate_argnums` (I originally saw this bug when a large number of arrays from the optimizer state were donated to the training step), although the repo does not donate anything for simplicity.  The NaN loss appears only in _very_ particular circumstances, it's not just the amount of memory allocated but the particular shapes of the arrays allocated as well  Sometime the attention weights after the softmax are reported as inf, but the inf does not seem to propagate to other values  Reproducing the issue has been pretty flaky in general. The repo should reproduce it consistently, but I believe a different kind of TPU will likely see different results  What jax/jaxlib version are you using? jax 0.3.21, jaxlib 0.3.20  Which accelerator(s) are you using? TPU v38   Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Unused arguments to function changes output," Description After a long time tracking down a NaN in a neural network training on a TPU, I was able to isolate a case where the output of the jitcompiled forward/backward pass of the network changes depending on whether/what extra arrays, that are never used in the actual computations, are passed into the function as additional arguments and then returned unchanged. Depending on exactly what unused arrays are passed in I have seen:  Sometimes the mean gradient will become NaN, sometimes not  Sometimes the loss will become NaN, sometimes not  Sometimes some of the intermediate values in the network will be reported to be mathematically impossible values (in particular this value will sometimes be much greater than 0), and sometimes not I can't think of an explanation for this other then some kind of memorymanagement error, so I am hoping someone here can help me figure out what is going on. I built a repo to reproduce the issue here,  it runs a simplified Transformer neural network over a batch of inputs. I would have liked to simplify things more, but so far my attempts have resulted in the NaN disappearing.  Some other notes:  I have seen this issue both with and without using `donate_argnums` (I originally saw this bug when a large number of arrays from the optimizer state were donated to the training step), although the repo does not donate anything for simplicity.  The NaN loss appears only in _very_ particular circumstances, it's not just the amount of memory allocated but the particular shapes of the arrays allocated as well  Sometime the attention weights after the softmax are reported as inf, but the inf does not seem to propagate to other values  Reproducing the issue has been pretty flaky in general. The repo should reproduce it consistently, but I believe a different kind of TPU will likely see different results  What jax/jaxlib version are you using? jax 0.3.21, jaxlib 0.3.20  Which accelerator(s) are you using? TPU v38   Additional system info _No response_  NVIDIA GPU info _No response_",2022-10-10T01:06:26Z,bug needs info TPU,open,0,11,https://github.com/jax-ml/jax/issues/12714,"This sounds concerning. However, those are reasonably old versions of `jax` and `jaxlib`, so the first thing I would want to know is ""is this fixed with the latest release"", namely jax 0.3.21 and jaxlib 0.3.20?","I miswrote on in the issue, actually I was tested with jax=0.3.17 and jaxlib=0.3.15 I can confirm the issue continues to exist when upgrading to jax=0.3.21 and jaxlib=0.3.20, I have updated the repo to make sure those versions are installed.","I'm trying to run your repro, but am getting access denied when trying to download the checkpoint with `gsutil m cp r gs://chriscpublic/debugcheckpoint .`. Can you check that it's a public bucket?",Should be fixed now,"I'm able to repro now, thanks for putting that together! I believe this is a compiler bug; I've reported it to the XLA:TPU team, and I'll update here as I learn more.",Internal issue for googlers' reference: b/253108046,"Any updates on this? Even some advice on how to work around or avoid tripping this issue would be helpful, currently its making training our models with jax very difficult."," is digging into your repro, but we don't have any updates yet. As a workaround, you could try manually setting `keep_unused=False` here: https://github.com/google/jax/blob/280153334bca32a4fae1f75bd6f1abe1a8f9eacd/jax/experimental/pjit.pyL1068 This will make jax prune unused arguments before compiling with XLA. You can try something like this: ``` git clone https://github.com/google/jax cd jax git checkout jaxv0.3.21 pip install e .  make local change(s) ``` That should install jax such that any changes you make in that checkout will be reflected when you import jax. If you give this a try, please let us know how it goes! We could at least make this a configurable option so you don't have to manually hack jax.",Update: Berkin is making good progress narrowing in on the bug! Another workaround you can try in the meantime: set the env var `LIBTPU_INIT_ARGS='xla_jf_rematerialization_percent_shared_memory_limit=100'`. This fixes the repro you provided. Berkin's still looking into how to fix this so you don't need any workarounds.,The env vars seems to work! I tried retraining the model I originally saw the NaN appearing in and it seems to be training smoothly. Thanks for digging into this and finding such a convenient way to fix the issue.,"Great, thanks again for the clear bug report and easy repro! I'll leave this open until we have a real fix committed."
364,"以下是一个github上的jax下的一个issue, 标题是([dynamic-shapes] implement bint arrays (opaque dtypes), add padding rules)， 内容是 (This was the last bit we needed to run a batchshapepolymorphic transformer on XLA. Coauthoredby: Sharad Vikram )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,"[dynamic-shapes] implement bint arrays (opaque dtypes), add padding rules",This was the last bit we needed to run a batchshapepolymorphic transformer on XLA. Coauthoredby: Sharad Vikram ,2022-10-08T12:23:11Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/12707,"For reference, the ""opaque dtypes"" mention in the PR title here is to say that this builds on CC(prototype unfettered element types in jaxpr arrays) and subsequent developments."
1556,"以下是一个github上的jax下的一个issue, 标题是(Multi GPU deadlock when using collective ops)， 内容是 ( Description Hi, we are facing strange behavior when using collective ops. Whenever a pmapped function includes jax.lax.all_gather or jax.lax.ppermute, the compilation of said function will stall indefinitely at 100% GPU utilization. However, the collective ops jax.lax.psum, jax.lax.pmean, jax.lax.pmin, jax.lax.pmax work fine. Setup of the server/versions of the software:      Ubuntu 20.04      Nvidia Driver Version 515.65.01      CUDA Version: 11.7      8x NVIDIA A6000      jax 0.3.21, jaxlib 0.3.20 We tried installing jax/jaxlib both using the pip installation and building it from source. In the latter case, we manually install NCCL 2.13.4 before building jax/jaxlib from source. Both approaches yield the same outcome. We got the idea to build jax/jaxlib from source after reading the threads about similar issues https://github.com/google/jax/issues/10969, https://github.com/google/jax/issues/11637. Also, using different CUDA versions did not help.  E.g., this simple code snippet will produce a deadlock. import os import jax import numpy as np os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1"" x = np.arange(2) y = jax.pmap(lambda x: jax.lax.all_gather(x, axis_name='i'), axis_name='i')(x) However, this code snippet works just fine: x = np.arange(2) y = jax.pmap(lambda x: jax.lax.pmin(x, axis_name='i'), axis_name='i')(x))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Multi GPU deadlock when using collective ops," Description Hi, we are facing strange behavior when using collective ops. Whenever a pmapped function includes jax.lax.all_gather or jax.lax.ppermute, the compilation of said function will stall indefinitely at 100% GPU utilization. However, the collective ops jax.lax.psum, jax.lax.pmean, jax.lax.pmin, jax.lax.pmax work fine. Setup of the server/versions of the software:      Ubuntu 20.04      Nvidia Driver Version 515.65.01      CUDA Version: 11.7      8x NVIDIA A6000      jax 0.3.21, jaxlib 0.3.20 We tried installing jax/jaxlib both using the pip installation and building it from source. In the latter case, we manually install NCCL 2.13.4 before building jax/jaxlib from source. Both approaches yield the same outcome. We got the idea to build jax/jaxlib from source after reading the threads about similar issues https://github.com/google/jax/issues/10969, https://github.com/google/jax/issues/11637. Also, using different CUDA versions did not help.  E.g., this simple code snippet will produce a deadlock. import os import jax import numpy as np os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1"" x = np.arange(2) y = jax.pmap(lambda x: jax.lax.all_gather(x, axis_name='i'), axis_name='i')(x) However, this code snippet works just fine: x = np.arange(2) y = jax.pmap(lambda x: jax.lax.pmin(x, axis_name='i'), axis_name='i')(x)",2022-10-07T18:27:33Z,bug NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/12701,I wasn't able to reproduce this on an 8xA100 machine (I ran the repro many times with those jax and jaxlib versions). I don't have access to any A6000 GPUs. Any more tips on reproducing this?,"It seems that the problem is related to NCCL. When we disable GPUtoGPU (P2P) communication by setting the environment variable NCCL_P2P_DISABLE=1, everything works as expected. We then upgraded the OS to Ubuntu 22.04 which (apparently) solved the issue.",This issue seems to have been fixed by the OS upgrade.
237,"以下是一个github上的jax下的一个issue, 标题是(Switch lax_numpy_indexing_test to use jtu.sample_product.)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Switch lax_numpy_indexing_test to use jtu.sample_product.,,2022-10-06T15:54:01Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/12686
260,"以下是一个github上的jax下的一个issue, 标题是(jnp.average: support tuple axis)， 内容是 (Fixes CC(jax.numpy.average can't take tuple axis))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jnp.average: support tuple axis,Fixes CC(jax.numpy.average can't take tuple axis),2022-10-05T23:08:20Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/12678
423,"以下是一个github上的jax下的一个issue, 标题是(Rename Executable to LoadedExecutable within jax.)， 内容是 (Rename Executable to LoadedExecutable within jax. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/google/jax/pull/12678 from jakevdp:averageaxistuple 32ef3ba37b31cdb51bbef6663265090bca468f4c)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Rename Executable to LoadedExecutable within jax.,Rename Executable to LoadedExecutable within jax. FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/google/jax/pull/12678 from jakevdp:averageaxistuple 32ef3ba37b31cdb51bbef6663265090bca468f4c,2022-10-05T20:48:02Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12672
667,"以下是一个github上的jax下的一个issue, 标题是(jax.jacobian: propagate function signature to transformed function)， 内容是 (Fixes this error: ```python import jax def f(a, b):   return a if b else 0.0 jax.jit(jax.grad(f), static_argnames=['b'])(1.0, False) 0.0 jax.jit(jax.jacrev(f), static_argnames=['b'])(1.0, False)  ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected... ``` Found by user in https://stackoverflow.com/questions/73941657/errorwhentryingtojitthecomputationofthejacobianinjaxvalueerrorno)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.jacobian: propagate function signature to transformed function,"Fixes this error: ```python import jax def f(a, b):   return a if b else 0.0 jax.jit(jax.grad(f), static_argnames=['b'])(1.0, False) 0.0 jax.jit(jax.jacrev(f), static_argnames=['b'])(1.0, False)  ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected... ``` Found by user in https://stackoverflow.com/questions/73941657/errorwhentryingtojitthecomputationofthejacobianinjaxvalueerrorno",2022-10-04T17:22:32Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/12653
1240,"以下是一个github上的jax下的一个issue, 标题是(JAX GPU memory usage even with CPU allocation)， 内容是 ( Description I'm studying memory allocation of JAX to make my code faster and I found GPU memory usage of JAX even though I set it to use only CPU. My little code is: ``` import jax import jax.numpy as jnp jax.config.update('jax_platform_name', 'cpu') x=jnp.zeros(10) for i in range(10000000000):     1+1 ``` The for loop is just to see whether this program is using GPU or not. After this, what I found is it always uses 253MiB of GPU: > 303028 oh        20   0   29.5g 377844 294168 R 100.0   0.1   0:08.21 python ./test.py   and [![enter image description here][1]][1] Actually the PID 299133 and 299522 are also using GPU memory with JAX set to use CPU. How can I set it not to use GPU at all?   [1]: https://i.stack.imgur.com/6JU7q.png  What jax/jaxlib version are you using? jax v0.3.13, jaxlib v0.3.10+cuda11.cudnn82  Which accelerator(s) are you using? CPU  Additional system info Python 3.8.12, Ubuntu 20.04.3 LTS  NVIDIA GPU info  NVIDIASMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6    )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JAX GPU memory usage even with CPU allocation," Description I'm studying memory allocation of JAX to make my code faster and I found GPU memory usage of JAX even though I set it to use only CPU. My little code is: ``` import jax import jax.numpy as jnp jax.config.update('jax_platform_name', 'cpu') x=jnp.zeros(10) for i in range(10000000000):     1+1 ``` The for loop is just to see whether this program is using GPU or not. After this, what I found is it always uses 253MiB of GPU: > 303028 oh        20   0   29.5g 377844 294168 R 100.0   0.1   0:08.21 python ./test.py   and [![enter image description here][1]][1] Actually the PID 299133 and 299522 are also using GPU memory with JAX set to use CPU. How can I set it not to use GPU at all?   [1]: https://i.stack.imgur.com/6JU7q.png  What jax/jaxlib version are you using? jax v0.3.13, jaxlib v0.3.10+cuda11.cudnn82  Which accelerator(s) are you using? CPU  Additional system info Python 3.8.12, Ubuntu 20.04.3 LTS  NVIDIA GPU info  NVIDIASMI 510.73.05    Driver Version: 510.73.05    CUDA Version: 11.6    ",2022-10-03T16:14:13Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/12629,"Hi wooh I tried to run the mentioned code on Colab GPU with latest JAX version (0.4.23) and I could see that it is using the CPU only and the GPU memory has not been used. You can refer in the following screenshot for memory usage.  Kindly find the gist. Could you please check if any other python code is running on GPU and if any, please close them and run this code and check whether you are getting the same issue. Thank you"
1244,"以下是一个github上的jax下的一个issue, 标题是(Program hang when donated input share memory with un-donated input)， 内容是 ( Description When a jitted function is called with a donated input and an undonated input with the same underlying memory, the program stucks forever with 0% GPU and CPU utilization. I think we should raise a warning or error in this situation. This is especially hard to find when these conflicting buffers hide deep in a pytree. BTW, this may be the underlying issue of https://github.com/google/jax/issues/10737 Minimal example to reproduce: ```python from functools import partial import jax import jax.numpy as jnp def main():     (jax.jit, donate_argnums=(0,))     def update(x, batch):         return x + batch      x takes 1GB memory     x = jnp.zeros((1 * 1024 * 1024 * 1024 // 4), dtype=jnp.float32)      A training loop     for _ in range(10):         batch = x         x = update(x, batch) if __name__ == ""__main__"":     main() ```  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.6  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Program hang when donated input share memory with un-donated input," Description When a jitted function is called with a donated input and an undonated input with the same underlying memory, the program stucks forever with 0% GPU and CPU utilization. I think we should raise a warning or error in this situation. This is especially hard to find when these conflicting buffers hide deep in a pytree. BTW, this may be the underlying issue of https://github.com/google/jax/issues/10737 Minimal example to reproduce: ```python from functools import partial import jax import jax.numpy as jnp def main():     (jax.jit, donate_argnums=(0,))     def update(x, batch):         return x + batch      x takes 1GB memory     x = jnp.zeros((1 * 1024 * 1024 * 1024 // 4), dtype=jnp.float32)      A training loop     for _ in range(10):         batch = x         x = update(x, batch) if __name__ == ""__main__"":     main() ```  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.6  ++ ```",2022-10-03T14:57:46Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/12627,"Hi  Looks like this issue is resolved in latest version of JAX (0.4.23) by raising an `XLARunTimeError`. I reproduced the mentioned code in Google Colab. Program didn't hang  instead gave the following error message. ```  XlaRuntimeError                           Traceback (most recent call last)  in ()      20       21 if __name__ == ""__main__"": > 22     main() 1 frames     [... skipping hidden 10 frame] /usr/local/lib/python3.10/distpackages/jax/_src/interpreters/pxla.py in __call__(self, *args)    1157       self._handle_token_bufs(result_token_bufs, sharded_runtime_token)    1158     else: > 1159       results = self.xla_executable.execute_sharded(input_bufs)    1160     if dispatch.needs_check_special():    1161       out_arrays = results.disassemble_into_single_device_arrays() XlaRuntimeError: INVALID_ARGUMENT: Attempt to use a buffer that was previously donated in the same call to Execute() (second use: flattened argument 1, replica 0). Toy example for this bug: `f(donate(a), a)`. ``` Kindly find the gist. Could you please check and confirm. Thank you."
1861,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Failure with lax.scan with enable_xla=False)， 内容是 ( Description  Hello! I'm getting some bad behavior when using lax.scan to compute an exponential moving average over a time axis.  Specifically, the EMA function here is causing issues Here  The model exports to TF SavedModel (with enable_xla=False), but then we get an exception when running the model: ``` google3/third_party/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      52   try:      53     ctx.ensure_initialized() > 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,      55                                         inputs, attrs, num_outputs)      56   except core._NotOkStatusException as e: InvalidArgumentError: Must have updates.shape = indices.shape[:batch_dim] + buffer_shape[num_index_dims:], got updates.shape: [1,1,1,16], indices.shape: [1,128,1,16,1], buffer_shape: [2048], num_index_dims: 1, and batch_dim: 4 	 [[{{node jax2tf_infer_fn_/scan/while/body/ScatterNd_1}}]] 	 [[PartitionedCall/jax2tf_infer_fn_/scan/while]] [Op:__inference_restored_function_body_8540] ``` which suggests some indices are being misplaced.  A couple observations:  The error occurs even when polymorphic shape is disabled.  When xla is enabled, the savedmodel works correctly.  Strangely, converting the SavedModel to TFLite seems to work out; the model runs and the outptus match the jax model numerically.  Minimal reproducing code is in my usual colab notebook of minimal reproductions Here  What jax/jaxlib version are you using? v0.3.17  Which accelerator(s) are you using? CPU, GPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Failure with lax.scan with enable_xla=False," Description  Hello! I'm getting some bad behavior when using lax.scan to compute an exponential moving average over a time axis.  Specifically, the EMA function here is causing issues Here  The model exports to TF SavedModel (with enable_xla=False), but then we get an exception when running the model: ``` google3/third_party/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      52   try:      53     ctx.ensure_initialized() > 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,      55                                         inputs, attrs, num_outputs)      56   except core._NotOkStatusException as e: InvalidArgumentError: Must have updates.shape = indices.shape[:batch_dim] + buffer_shape[num_index_dims:], got updates.shape: [1,1,1,16], indices.shape: [1,128,1,16,1], buffer_shape: [2048], num_index_dims: 1, and batch_dim: 4 	 [[{{node jax2tf_infer_fn_/scan/while/body/ScatterNd_1}}]] 	 [[PartitionedCall/jax2tf_infer_fn_/scan/while]] [Op:__inference_restored_function_body_8540] ``` which suggests some indices are being misplaced.  A couple observations:  The error occurs even when polymorphic shape is disabled.  When xla is enabled, the savedmodel works correctly.  Strangely, converting the SavedModel to TFLite seems to work out; the model runs and the outptus match the jax model numerically.  Minimal reproducing code is in my usual colab notebook of minimal reproductions Here  What jax/jaxlib version are you using? v0.3.17  Which accelerator(s) are you using? CPU, GPU  Additional system info Linux  NVIDIA GPU info _No response_",2022-10-03T10:43:38Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/12623,,Closing this since it is a duplicate of CC([jax2tf] Failure with lax.scan with enable_xla=False)
2083,"以下是一个github上的jax下的一个issue, 标题是(Build error: An error occurred during the fetch of repository 'local_config_cuda')， 内容是 ( Description I'm trying to build jaxlib but I see the following error during installation: ```console ERROR: An error occurred during the fetch of repository 'local_config_cuda':    Traceback (most recent call last):         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/gpus/cuda_configure.bzl"", line 1076, column 27, in _create_local_cuda_repository                 cuda_libs = _find_libs(repository_ctx, check_cuda_libs_script, cuda_config)         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, column 21, in _find_libs                 _check_cuda_libs(repository_ctx, check_cuda_libs_script, check_cuda_libs_params.values())         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, column 28, in _check_cuda_libs                 checked_paths = execute(repository_ctx, [python_bin, ""c"", cmd]).stdout.splitlines()         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed Expected even number of arguments ``` Any idea how to fix this? Here is the full build log: * build log  What jax/jaxlib version are you using? jaxlib v0.1.74  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.13, Linux  NVIDIA GPU info This error was found in CI, so I don't have access to run `nvidiasmi`. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Build error: An error occurred during the fetch of repository 'local_config_cuda'," Description I'm trying to build jaxlib but I see the following error during installation: ```console ERROR: An error occurred during the fetch of repository 'local_config_cuda':    Traceback (most recent call last):         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl                 _create_local_cuda_repository(repository_ctx)         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/gpus/cuda_configure.bzl"", line 1076, column 27, in _create_local_cuda_repository                 cuda_libs = _find_libs(repository_ctx, check_cuda_libs_script, cuda_config)         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, column 21, in _find_libs                 _check_cuda_libs(repository_ctx, check_cuda_libs_script, check_cuda_libs_params.values())         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, column 28, in _check_cuda_libs                 checked_paths = execute(repository_ctx, [python_bin, ""c"", cmd]).stdout.splitlines()         File ""/tmp/spack26d5tykr/c126d21af0d42f631ea076a051f19300/external/org_tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed Expected even number of arguments ``` Any idea how to fix this? Here is the full build log: * build log  What jax/jaxlib version are you using? jaxlib v0.1.74  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.13, Linux  NVIDIA GPU info This error was found in CI, so I don't have access to run `nvidiasmi`. ",2022-10-01T20:45:24Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/12614,this is probably the fix: https://github.com/tensorflow/tensorflow/commit/a76f797b9cd4b9b15bec4c503b16236a804f676f,"some more detail for anyone else who runs into this:  cuda 11.7.1 added whitespace in front of the definitions for versioning in cusolver include files, causing the error you see.  TF 2.10.0 includes a fix for this: https://github.com/tensorflow/tensorflow/commit/a76f797b9cd4b9b15bec4c503b16236a804f676f  jax 0.3.14 references a version of tensorflow that includes that commit.","Thanks for the diagnosis! It sounds like this is fixed at HEAD, so there's no action for us to take here."
262,"以下是一个github上的jax下的一个issue, 标题是(Add jax_array coverage to debug_nans_test)， 内容是 (Add jax_array coverage to debug_nans_test)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add jax_array coverage to debug_nans_test,Add jax_array coverage to debug_nans_test,2022-09-30T17:30:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12600
5697,"以下是一个github上的jax下的一个issue, 标题是(OOM with only ~300MB memory allocated / requested on GPU)， 内容是 ( Description The following code crashes with OOM, even though from the error message it doesn't seem to be using or requesting much memory: on an A40 card with ~40GB of available memory, it crashes when having allocated ~340 MB and requesting ~250 MB of memory. ```python import jax, flax from jax import numpy as np from flax import linen as nn from tqdm import trange model = nn.Sequential([nn.Dense(50)]) (jax.vmap, in_axes=(0, None)) def sample_from_prior(rng, inp):     params = model.init(rng, np.zeros((10, 50)))     return model.apply(params, inp) for i in trange(1000):     rngs = jax.random.split(jax.random.PRNGKey(23), 10000)     sample_from_prior(rngs, np.ones((4, 50)))      this won't help      if i % 400 == 0:          jax.lib.xla_bridge.get_backend().defragment() ``` A typical error log looks like this: ``` jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 250000256 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    78.1KiB               constant allocation:         0B         maybe_live_out allocation:   95.37MiB      preallocated temp allocation:  238.42MiB   preallocated temp fragmentation:         0B (0.00%)                  total allocation:  333.86MiB               total fragmentation:   47.68MiB (14.28%) Peak buffers:         Buffer 1:                 Size: 95.37MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/jit(clip)/min"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: f32[10000,50,50]                 ==========================         Buffer 2:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: customcall                 Shape: u32[10000,1250]                 ==========================         Buffer 3:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: customcall                 Shape: u32[10000,1250]                 ==========================         Buffer 4:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: u32[10000,1250]                 ==========================         Buffer 5:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: u32[10000,1250]                 ==========================         Buffer 6:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: u32[10000,1250]                 ==========================         Buffer 7:                 Size: 78.1KiB                 Entry Parameter Subshape: u32[10000,2]                 ==========================         Buffer 8:                 Size: 16B                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: (u32[10000,1250], u32[10000,1250])                 ==========================         Buffer 9:                 Size: 16B                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: customcall                 Shape: (u32[10000,1250], u32[10000,1250])                 ==========================         Buffer 10:                 Size: 4B                 Entry Parameter Subshape: s32[]                 ==========================         Buffer 11:                 Size: 4B                 Entry Parameter Subshape: s32[]                 ========================== ``` I've tried setting `XLA_PYTHON_CLIENT_MEM_FRACTION` to a smaller value, or using `XLA_PYTHON_CLIENT_ALLOCATOR=platform`, but neither helps. I should note that this seems specific to some computation performed by flax, in particular the `Sequential` wrapper: it does not happen if we remove it.  Nonetheless it would be better if we don't have to work around it, and the error looks rather strange, so I hope it could get fixed some time.  What jax/jaxlib version are you using? 0.3.20, 0.3.20+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.9, Ubuntu 20.04.2, flax 0.6.0  NVIDIA GPU info ``` ++   omitted below; code was using an unoccupied card. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,OOM with only ~300MB memory allocated / requested on GPU," Description The following code crashes with OOM, even though from the error message it doesn't seem to be using or requesting much memory: on an A40 card with ~40GB of available memory, it crashes when having allocated ~340 MB and requesting ~250 MB of memory. ```python import jax, flax from jax import numpy as np from flax import linen as nn from tqdm import trange model = nn.Sequential([nn.Dense(50)]) (jax.vmap, in_axes=(0, None)) def sample_from_prior(rng, inp):     params = model.init(rng, np.zeros((10, 50)))     return model.apply(params, inp) for i in trange(1000):     rngs = jax.random.split(jax.random.PRNGKey(23), 10000)     sample_from_prior(rngs, np.ones((4, 50)))      this won't help      if i % 400 == 0:          jax.lib.xla_bridge.get_backend().defragment() ``` A typical error log looks like this: ``` jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 250000256 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    78.1KiB               constant allocation:         0B         maybe_live_out allocation:   95.37MiB      preallocated temp allocation:  238.42MiB   preallocated temp fragmentation:         0B (0.00%)                  total allocation:  333.86MiB               total fragmentation:   47.68MiB (14.28%) Peak buffers:         Buffer 1:                 Size: 95.37MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/jit(clip)/min"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: f32[10000,50,50]                 ==========================         Buffer 2:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: customcall                 Shape: u32[10000,1250]                 ==========================         Buffer 3:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: customcall                 Shape: u32[10000,1250]                 ==========================         Buffer 4:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: u32[10000,1250]                 ==========================         Buffer 5:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: u32[10000,1250]                 ==========================         Buffer 6:                 Size: 47.68MiB                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: u32[10000,1250]                 ==========================         Buffer 7:                 Size: 78.1KiB                 Entry Parameter Subshape: u32[10000,2]                 ==========================         Buffer 8:                 Size: 16B                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: fusion                 Shape: (u32[10000,1250], u32[10000,1250])                 ==========================         Buffer 9:                 Size: 16B                 Operator: op_name=""jit(_truncated_normal)/jit(main)/threefry2x32"" source_file=""/workspace/home/ziyu/anaconda3/envs/jaxc/lib/python3.9/sitepackages/flax/core/scope.py"" source_line=775                 XLA Label: customcall                 Shape: (u32[10000,1250], u32[10000,1250])                 ==========================         Buffer 10:                 Size: 4B                 Entry Parameter Subshape: s32[]                 ==========================         Buffer 11:                 Size: 4B                 Entry Parameter Subshape: s32[]                 ========================== ``` I've tried setting `XLA_PYTHON_CLIENT_MEM_FRACTION` to a smaller value, or using `XLA_PYTHON_CLIENT_ALLOCATOR=platform`, but neither helps. I should note that this seems specific to some computation performed by flax, in particular the `Sequential` wrapper: it does not happen if we remove it.  Nonetheless it would be better if we don't have to work around it, and the error looks rather strange, so I hope it could get fixed some time.  What jax/jaxlib version are you using? 0.3.20, 0.3.20+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.9, Ubuntu 20.04.2, flax 0.6.0  NVIDIA GPU info ``` ++   omitted below; code was using an unoccupied card. ```",2022-09-29T14:17:34Z,bug needs info NVIDIA GPU,closed,0,11,https://github.com/jax-ml/jax/issues/12575,"I wasn't able to repro this. Are you able to consistently repro this error? Also, could you make sure there isn't any other process running on the GPUs?","I think so.  I can consistently reproduce this on the A40 server (CUDA 11.2), as well as another Linux server with a GTX 2080 Ti (NVIDIA drivier 510.47.03, CUDA 11.4) and the same python / package versions.  If these help: the first server is a docker instance, but the second is bare metal; my `pip freeze` result is here.  I checked `nvidiasmi` before running the program, and used `CUDA_DEVICE_ORDER=PCI_BUS_ID` to ensure I use the right card. I find the crash to happen faster if we use a larger network, e.g., `nn.Sequential([nn.Dense(100), nn.Dense(100)])` crashes at iteration 68, and the output is ``` jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1000000256 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    78.1KiB               constant allocation:         0B         maybe_live_out allocation:  381.47MiB      preallocated temp allocation:  953.67MiB   preallocated temp fragmentation:         0B (0.00%)                  total allocation:    1.30GiB               total fragmentation:  190.73MiB (14.28%)                                                                                                                                         ```","Ah I can repro now. I think this code is bound to OOM since you're creating a new `model` every iteration (do you intend to do that?)  Edit: I take it back. The `model` is not very big though. So, it could be something else. But I'm not very sure about the usecase here.  do you think you could help?","If i `print(i)` in the loop, it consistently OOM at step 146 on my machine and 333.86MiB* 146 = 51.5GiB, so it could be hitting total GPU memory limit. It would seem that somehow the GPU allocations in each loop iteration are not freed, despite trying to delete things at the end of each loop ``` client = jax.lib.xla_bridge.get_backend() for i in trange(1000):     print('step=', i)     bs = [b.shape for b in client.live_buffers()]     print('num_live_buffers=', len(bs))     print('live_buffers_shapes=', bs)     rngs = jax.random.split(jax.random.PRNGKey(23), 10000)     out = sample_from_prior(rngs, np.ones((4, 50)))     out.block_until_ready()     del out     del rngs ``` Output is ``` step= 0 num_live_buffers= 0 live_buffers_shapes= []   0%▍                      ``` I'll need to dig into nn.Sequential and see why it's causing this behavior.","If you jit the function, it works. ``` step= 999 num_live_buffers= 2 live_buffers_shapes= [(10000, 4, 50), (10000, 2)] ``` Something is holding onto JAX GPU array references, and it's not clear to me why this simple nn.Sequential implementation would lead to that. But perhaps .jit + nn.Sequential is a ok workaround while I get Flax dev to look into this some more? Full code ``` import jax, flax from jax import numpy as np from flax import linen as nn from tqdm import trange from functools import partial model = nn.Sequential([nn.Dense(50)]) model = nn.Dense(50) .jit (jax.vmap, in_axes=(0, None)) def sample_from_prior(rng, inp):     params = model.init(rng, np.zeros((10, 50)))     out = model.apply(params, inp)     del params     return out client = jax.lib.xla_bridge.get_backend() import gc for i in trange(1000):     print('step=', i)     bs = [b.shape for b in client.live_buffers()]     print('num_live_buffers=', len(bs))     print('live_buffers_shapes=', bs)     rngs = jax.random.split(jax.random.PRNGKey(23), 10000)     out = sample_from_prior(rngs, np.ones((4, 50)))     gc.collect()     out.block_until_ready()     del out     del rngs     if i == 5:         break ```","Thanks   for the comments and workaround!  I should have thought of `client.live_buffers()`, but I'm not sure why the `BufferAssignment OOM Debugging` log seems to report different things. And FWIW, I just found it's not limited to `nn.Sequential`.  This also fails if (and only if) we don't `jit`: ``` class Model(nn.Module):     .compact     def __call__(self, x):         x = nn.Dense(200)(x)         return nn.Dense(200)(x) model = Model() ```",This bug should be fixed if you update to Flax head. See fix in https://github.com/google/flax/issues/2493,"Unfortunately, while the code in the OP runs to 1000 steps for me (on an A100 w/ 40GB), following inf's thinking in this comment, a larger model like `Sequential([Dense(250), Dense(250)])` still OOMs for me around step 20 with analogous BufferAssignment stats: ```python from functools import partial import jax, flax from jax import numpy as np from flax import linen as nn from tqdm import trange model = nn.Sequential([nn.Dense(250), nn.Dense(250)]) (jax.vmap, in_axes=(0, None)) def sample_from_prior(rng, inp):     params = model.init(rng, np.zeros((10, 50)))     return model.apply(params, inp) for i in trange(1000):     print(i)     rngs = jax.random.split(jax.random.PRNGKey(23), 10000)     sample_from_prior(rngs, np.ones((4, 50))) ``` ``` 20221004 03:17:38.522221: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2130] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 6250000256 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    78.1KiB               constant allocation:         0B         maybe_live_out allocation:    2.33GiB      preallocated temp allocation:    5.82GiB   preallocated temp fragmentation:         0B (0.00%)                  total allocation:    8.15GiB               total fragmentation:    1.16GiB (14.29%) ``` ``` (base) mattjja1008:~$ nvidiasmi Tue Oct  4 03:18:22 2022 ++  ++++ ``` I think this is analogous because the stats show plenty of free memory, yet the allocation failed at runtime. (Please let me know if I'm misinterpreting these.) We suspect that the reason the code in the OP now runs is that Python's garbage collection just happens to be triggered with that code. IIUC it has heuristics to trigger based on e.g. the number of new objects it sees. But if we make the sizes bigger, like in the larger model case, the Python gc doesn't trigger before we overflow GPU memory. IIUC google/flax CC(separate out deprecated custom_transforms stuff) fixed one issue, but we still have a cyclic references problem. A workaround for now is to add `gc.collect()` statements to your code, perhaps every few iterations.","I think Flax's `Sequential` is creating reference cycles. I modified jax/core.py's `maybe_find_leaked_tracers` to look like this:  Then, running the code with `jax.config.update('jax_check_tracer_leaks', True)`, I got this: ``` y is a   140478991096608 y.parent is a  140478991097568 y.parent.layers is a  140478991096416 y.parent.layers[0] is a  140478991096608 ``` So I think the issue is a Flax reference cycle. We're talking with the friendly Flax devs to brainstorm a fix! But in the meantime, just doing some `gc.collect()` would solve things as a workaround.","We forgot to update this issue, but we believe this was fixed last week in google/flax CC(how to vectorize custom functions with vmap). I'll leave it open until one of us can verify the original code doesn't reproduce the issue.",I just checked the fix (using flax HEAD); it works on all cases.  Thanks!
326,"以下是一个github上的jax下的一个issue, 标题是(Use `Array` in `__repr__` instead of the class name which is `ArrayImpl`.)， 内容是 (Use `Array` in `__repr__` instead of the class name which is `ArrayImpl`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Use `Array` in `__repr__` instead of the class name which is `ArrayImpl`.,Use `Array` in `__repr__` instead of the class name which is `ArrayImpl`.,2022-09-28T15:20:19Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12559
1529,"以下是一个github上的jax下的一个issue, 标题是(Jax crashes on TPU in version 0.3.19)， 内容是 ( Description Hi,  I installed Jax on a TPU V38: ``` pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` However, when running Jax, I get the following error.  ```python (base) gerardodurann7177f451w0:~$ python Python 3.10.6  (main, Aug 22 2022, 20:35:26) [GCC 10.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> from jax import jit >>> def f(x): return x**2 ...  >>> f_cpu = jit(f, backend='cpu') >>> f_tpu = jit(f, backend='tpu') >>>  >>> f_cpu(2) DeviceArray(4, dtype=int32, weak_type=True) >>> f_tpu(2) tcmalloc: large alloc 378179171590144 bytes == (nil) @  0x7f78decaf680 0x7f78decd0824 0x7f76eeeaf2da 0x7f76eee60bae 0x7f76ea77687a 0x7f76ea775a05 0x7f76ea777c62 0x7f76e9a5c82e 0x7f7798183b66 0x7f77971f6541 0x7f77971e2912 0x7f77971dc61d 0x7f7795b4248c 0x7f7795b51176 0x7f7793f76ded 0x7f7793d52ac8 0x7f7793d532d3 0x7f7793d2e916 0x55fc669c83cc 0x55fc669c1738 0x55fc669d5f80 0x55fc669b9107 0x55fc669c886f 0x55fc669ba99f 0x55fc669c886f 0x55fc669b80ff 0x55fc669c886f 0x55fc669b80ff 0x55fc669c886f 0x55fc669d67f8 0x55fc669ba99f  ... more errors ```  What jax/jaxlib version are you using? jax 0.3.19 / jaxlib 0.3.15  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Jax crashes on TPU in version 0.3.19," Description Hi,  I installed Jax on a TPU V38: ``` pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` However, when running Jax, I get the following error.  ```python (base) gerardodurann7177f451w0:~$ python Python 3.10.6  (main, Aug 22 2022, 20:35:26) [GCC 10.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> from jax import jit >>> def f(x): return x**2 ...  >>> f_cpu = jit(f, backend='cpu') >>> f_tpu = jit(f, backend='tpu') >>>  >>> f_cpu(2) DeviceArray(4, dtype=int32, weak_type=True) >>> f_tpu(2) tcmalloc: large alloc 378179171590144 bytes == (nil) @  0x7f78decaf680 0x7f78decd0824 0x7f76eeeaf2da 0x7f76eee60bae 0x7f76ea77687a 0x7f76ea775a05 0x7f76ea777c62 0x7f76e9a5c82e 0x7f7798183b66 0x7f77971f6541 0x7f77971e2912 0x7f77971dc61d 0x7f7795b4248c 0x7f7795b51176 0x7f7793f76ded 0x7f7793d52ac8 0x7f7793d532d3 0x7f7793d2e916 0x55fc669c83cc 0x55fc669c1738 0x55fc669d5f80 0x55fc669b9107 0x55fc669c886f 0x55fc669ba99f 0x55fc669c886f 0x55fc669b80ff 0x55fc669c886f 0x55fc669b80ff 0x55fc669c886f 0x55fc669d67f8 0x55fc669ba99f  ... more errors ```  What jax/jaxlib version are you using? jax 0.3.19 / jaxlib 0.3.15  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2022-09-28T10:34:55Z,bug P1 (soon),closed,0,10,https://github.com/jax-ml/jax/issues/12550,"update: it seems that the problem has to do with version `0.3.19` of Jax. If I downgrade to `0.3.17` I don't get the error any longer ``` Python 3.10.6  (main, Aug 22 2022, 20:35:26) [GCC 10.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.__version__ '0.3.17' >>> jax.numpy.sqrt(2) DeviceArray(1.4142135, dtype=float32, weak_type=True) ```",The issue is that the new `jax` release pins an incompatible `libtpu` version. We messed up. Another workaround for now is to install the new `jax` but to downgrade your `libtpu` version: ``` pip install libtpunightly==0.1.dev20220723 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` I'm working on a new release with a fix.," I just tested this in a simple Python 3.8 image (see here), unfortunately this workaround does not seem to work for me.", Does that happen outside of a docker container? I think that issue is specifically related to docker.," the problem happens outside docker for me, but jax `0.3.17` fixes it. This is my repro on v232 in case it helps: ``` TPU_CHIPS_PER_PROCESS_BOUNDS=1,1,1 TPU_PROCESS_BOUNDS=1,1,1 TPU_VISIBLE_DEVICES=0,1,2,3 python c ""import jax; jax.random.PRNGKey(0)""  hangs forever or crashes ``` Running Python 3.9.12 in my case.", That doesn't sound like the same issue reported in the first post of this issue.," Sorry, I assumed it was the same because the behaviour is similar and affects the same version. I can open a new issue with any details you need, no problem :)","Hi , thanks for looking into this. I tested this with the same version of Docker (`Docker version 20.10.6, build 370c289`) and an earlier version of JAX (`0.3.13`) and it worked (I've detailed a minimum reproducible example in CC(Running JAX in Docker container using TPU fails)): I ran this inside a Python 3.8 container on the TPU VM: ``` Python 3.8.14 (default, Sep 13 2022, 15:03:48) [GCC 10.2.1 20210110] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.__version__ '0.3.13' >>> jax.local_devices() [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] >>> key = jax.random.PRNGKey(0) >>> key DeviceArray([0, 0], dtype=uint32) ```",This should be fixed by jax/jaxlib v0.3.20 which we just released. Hope that helps!,"That solved it for me, thank you so much ! 🚀 "
2127,"以下是一个github上的jax下的一个issue, 标题是(CUDA kernel crash upon convolution)， 内容是 ( Description Computing gradient of convolution crashes with certain input shapes. Code: ```python import jax import jax.numpy def my_convolution(weight, input):     y = jax.lax.conv_general_dilated(         input, weight, window_strides=(1,1), lhs_dilation=(2,2), padding=((1,1),(1,1)),         dimension_numbers = ('NHWC', 'HWIO', 'NHWC')     )     return jax.numpy.mean(y) my_grad = jax.grad(my_convolution, argnums=0) my_weight = jax.numpy.ones((2, 2, 256, 64)) my_input = jax.numpy.ones((1, 15, 10, 256)) print(my_grad(my_weight, my_input)) ``` It crashes with CUDA_ERROR_ILLEGAL_ADDRESS error: ``` jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: Failed to determine best cudnn convolution algorithm for: %cudnnconv = (f32[2,2,256,64]{1,0,2,3}, u8[0]{0}) customcall(f32[1,30,20,64]{2,1,0,3} %reverse, f32[1,29,19,256]{2,1,0,3} %pad), window={size=29x19 rhs_reversal=1x1}, dim_labels=f01b_i01o>01fb, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(conv_general_dilated)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(2, 2) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(3, 0, 1, 2), rhs_spec=(3, 0, 1, 2), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 15, 10, 256) rhs_shape=(1, 30, 20, 64) precision=None preferred_element_type=None]"" source_file=""/home/as/repos/vaejax/jax_crash.py"" source_line=5}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" Original error: INTERNAL: Failed to launch CUDA kernel: redzone_checker with block dimensions: 1024x1x1 and grid dimensions: 8192x1x1: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered ```  What jax/jaxlib version are you using? jax 0.3.19, jaxlib 0.3.15+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,CUDA kernel crash upon convolution," Description Computing gradient of convolution crashes with certain input shapes. Code: ```python import jax import jax.numpy def my_convolution(weight, input):     y = jax.lax.conv_general_dilated(         input, weight, window_strides=(1,1), lhs_dilation=(2,2), padding=((1,1),(1,1)),         dimension_numbers = ('NHWC', 'HWIO', 'NHWC')     )     return jax.numpy.mean(y) my_grad = jax.grad(my_convolution, argnums=0) my_weight = jax.numpy.ones((2, 2, 256, 64)) my_input = jax.numpy.ones((1, 15, 10, 256)) print(my_grad(my_weight, my_input)) ``` It crashes with CUDA_ERROR_ILLEGAL_ADDRESS error: ``` jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: Failed to determine best cudnn convolution algorithm for: %cudnnconv = (f32[2,2,256,64]{1,0,2,3}, u8[0]{0}) customcall(f32[1,30,20,64]{2,1,0,3} %reverse, f32[1,29,19,256]{2,1,0,3} %pad), window={size=29x19 rhs_reversal=1x1}, dim_labels=f01b_i01o>01fb, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(conv_general_dilated)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(2, 2) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(3, 0, 1, 2), rhs_spec=(3, 0, 1, 2), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 15, 10, 256) rhs_shape=(1, 30, 20, 64) precision=None preferred_element_type=None]"" source_file=""/home/as/repos/vaejax/jax_crash.py"" source_line=5}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" Original error: INTERNAL: Failed to launch CUDA kernel: redzone_checker with block dimensions: 1024x1x1 and grid dimensions: 8192x1x1: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered ```  What jax/jaxlib version are you using? jax 0.3.19, jaxlib 0.3.15+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` ++  ++++ ```",2022-09-28T09:43:46Z,bug needs info NVIDIA GPU,closed,0,5,https://github.com/jax-ml/jax/issues/12549,Can you share what cudnn version you have installed? Can you try updating it to the newest release if it isn't already? In addition the GPU model name was truncated in the `nvidiasmi` output. Which GPU is it?,"I have cudnn 8.2.1. Upgrading to 8.4 didn't work, there seems to be no wheels for this cudnn version. The GPU is NVIDIA GeForce RTX 2080 Ti.","`jaxlib` wheels should be compatible with all newer CuDNN versions than they were built with. 8.4 should work. In addition, we just released `jax`/`jaxlib` 0.3.20. Try those?","Ah, I thought jaxlib must match cudnn version exactly. Upgrading to cudnn 8.4 solved the problem.",That means it was likely a CuDNN bug but it has been fixed with newer releases. Phew! Happy this worked.
3683,"以下是一个github上的jax下的一个issue, 标题是(Running JAX in Docker container using TPU fails)， 内容是 ( Description Hey, I'm a big fan of JAX and have run into an issue recently: when I run simple JAX commands inside a Docker container (whose host machine is a GCP TPU VM with type v28), I get a strange segmentation error. **Minimum reproducible example** I did the following steps: 1. Create a TPU VM (on GCP) ``` gcloud alpha compute tpus tpuvm create NAMEOFTPU zone uscentral1f \         acceleratortype v28 \         version v2alpha ``` 2. Connect to the TPU VM via SSH 3. Pull a simple Python image and run the container: ``` sudo docker run privileged it rm python:3.8 bash ``` 4. Install the latest version of JAX: ``` pip install jax[tpu]==0.3.19 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` 5. Open Python inside the container and run some commands: ``` import jax jax.local_devices()   This works. key = jax.random.PRNGKey(0)   This fails. ``` The line `jax.local_devices()` returns the following: ``` [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] ``` However, the line `key = jax.random.PRNGKey(0)` fails with the following error: ``` https://symbolize.stripped_domain/r/?trace=7f3c11794032,7f3d33acfd5f,7f3653fff173,7f365357c33f,cccccccccccccccb&map=ca08008df67fa564c14ead76d3f2385a:7f36436130007f365371ec00 *** SIGSEGV (), see gl__________41s15 received by PID 32 (TID 32) on cpu 18; stack trace: *** PC: @     0x7f3c11794032  (unknown)  (unknown)     @     0x7f3653481ab4       1120  (unknown)     @     0x7f3d33acfd60  908665504  (unknown)     @     0x7f3653fff174  (unknown)  (unknown)     @     0x7f365357c340  (unknown)  (unknown)     @ 0xcccccccccccccccc  (unknown)  (unknown) https://symbolize.stripped_domain/r/?trace=7f3c11794032,7f3653481ab3,7f3d33acfd5f,7f3653fff173,7f365357c33f,cccccccccccccccb&map=ca08008df67fa564c14ead76d3f2385a:7f36436130007f365371ec00 E0928 09:12:27.193924      32 coredump_hook.cc:395] RAW: Remote crash data gathering hook invoked. E0928 09:12:27.193967      32 client.cc:243] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec. E0928 09:12:27.193972      32 coredump_hook.cc:502] RAW: Sending fingerprint to remote end. E0928 09:12:27.193979      32 coredump_socket.cc:120] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket E0928 09:12:27.193996      32 coredump_hook.cc:506] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running? E0928 09:12:27.194004      32 coredump_hook.cc:577] RAW: Dumping core locally. E0928 09:12:33.311853      32 process_state.cc:774] RAW: Raising signal 11 with default behavior Segmentation fault (core dumped) ``` I have tested that the above works with JAX version 0.3.13.  Any ideas on how to fix this? Thanks so much!  What jax/jaxlib version are you using? jax v0.3.19, jaxlib v0.3.15, libtpunightly 0.1.dev20220926  Which accelerator(s) are you using? TPU  Additional system info Python 3.8, Linux OS (of TPU)  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Running JAX in Docker container using TPU fails," Description Hey, I'm a big fan of JAX and have run into an issue recently: when I run simple JAX commands inside a Docker container (whose host machine is a GCP TPU VM with type v28), I get a strange segmentation error. **Minimum reproducible example** I did the following steps: 1. Create a TPU VM (on GCP) ``` gcloud alpha compute tpus tpuvm create NAMEOFTPU zone uscentral1f \         acceleratortype v28 \         version v2alpha ``` 2. Connect to the TPU VM via SSH 3. Pull a simple Python image and run the container: ``` sudo docker run privileged it rm python:3.8 bash ``` 4. Install the latest version of JAX: ``` pip install jax[tpu]==0.3.19 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` 5. Open Python inside the container and run some commands: ``` import jax jax.local_devices()   This works. key = jax.random.PRNGKey(0)   This fails. ``` The line `jax.local_devices()` returns the following: ``` [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] ``` However, the line `key = jax.random.PRNGKey(0)` fails with the following error: ``` https://symbolize.stripped_domain/r/?trace=7f3c11794032,7f3d33acfd5f,7f3653fff173,7f365357c33f,cccccccccccccccb&map=ca08008df67fa564c14ead76d3f2385a:7f36436130007f365371ec00 *** SIGSEGV (), see gl__________41s15 received by PID 32 (TID 32) on cpu 18; stack trace: *** PC: @     0x7f3c11794032  (unknown)  (unknown)     @     0x7f3653481ab4       1120  (unknown)     @     0x7f3d33acfd60  908665504  (unknown)     @     0x7f3653fff174  (unknown)  (unknown)     @     0x7f365357c340  (unknown)  (unknown)     @ 0xcccccccccccccccc  (unknown)  (unknown) https://symbolize.stripped_domain/r/?trace=7f3c11794032,7f3653481ab3,7f3d33acfd5f,7f3653fff173,7f365357c33f,cccccccccccccccb&map=ca08008df67fa564c14ead76d3f2385a:7f36436130007f365371ec00 E0928 09:12:27.193924      32 coredump_hook.cc:395] RAW: Remote crash data gathering hook invoked. E0928 09:12:27.193967      32 client.cc:243] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec. E0928 09:12:27.193972      32 coredump_hook.cc:502] RAW: Sending fingerprint to remote end. E0928 09:12:27.193979      32 coredump_socket.cc:120] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket E0928 09:12:27.193996      32 coredump_hook.cc:506] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running? E0928 09:12:27.194004      32 coredump_hook.cc:577] RAW: Dumping core locally. E0928 09:12:33.311853      32 process_state.cc:774] RAW: Raising signal 11 with default behavior Segmentation fault (core dumped) ``` I have tested that the above works with JAX version 0.3.13.  Any ideas on how to fix this? Thanks so much!  What jax/jaxlib version are you using? jax v0.3.19, jaxlib v0.3.15, libtpunightly 0.1.dev20220926  Which accelerator(s) are you using? TPU  Additional system info Python 3.8, Linux OS (of TPU)  NVIDIA GPU info _No response_",2022-09-28T09:28:52Z,bug,closed,2,7,https://github.com/jax-ml/jax/issues/12548,"Hey , Try downgrading to version `0.3.17` of Jax. I encountered the same problem (See CC(Jax crashes on TPU in version 0.3.19)). Running ``` pip install ""jax[tpu]==0.3.17"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` fixed the problem for me.","Hey ! Thanks for the reply. When I downgrade to version `0.3.17` of JAX, I get the following error when calling `jax.local_devices()`: ``` Python 3.8.14 (default, Sep 13 2022, 15:03:48) [GCC 10.2.1 20210110] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.local_devices() F0928 10:54:11.694598      34 enforce_kernel_ipv6_support.cc:72] Check failed: loopback6_ok  !loopback4_ok This machine (50c345e24205) does not have an IPv6 loopback (::1) interface.  This configuration is obsolete.  You may temporarily set enforce_kernel_ipv6_support=false to bypass this check, but code elsewhere in g3      may fail as result.  See gl_________________30/topics/localhost for details. E0928 10:54:15.930859      34 process_state.cc:774] RAW: Raising signal 6 with default behavior Aborted (core dumped) ``` I also get this error when running `jax.random.PRNGKey(0)`. Any ideas? ","Hey . Can I ask which version of Jaxlib you have? ``` (base) gerardodurann225a77cew0:~$ ipython Python 3.10.6  (main, Aug 22 2022, 20:35:26) [GCC 10.4.0] Type 'copyright', 'credits' or 'license' for more information IPython 8.5.0  An enhanced Interactive Python. Type '?' for help. In [1]: import jax In [2]: jax.local_devices() Out[2]:  [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),  TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),  TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),  TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),  TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),  TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),  TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),  TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] In [3]: import jaxlib In [4]: jax.__version__ Out[4]: '0.3.17' In [5]: jaxlib.__version__ Out[5]: '0.3.15' In [6]: jax.random.PRNGKey(0) Out[6]: DeviceArray([0, 0], dtype=uint32) ``` I also installed python 3.8.13 and ran the code above, but that doesn't seem an issue either.","> Hey . Can I ask which version of Jaxlib you have? >  > ``` > (base) gerardodurann225a77cew0:~$ ipython > Python 3.10.6  (main, Aug 22 2022, 20:35:26) [GCC 10.4.0] > Type 'copyright', 'credits' or 'license' for more information > IPython 8.5.0  An enhanced Interactive Python. Type '?' for help. >  > In [1]: import jax >  > In [2]: jax.local_devices() > Out[2]:  > [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), >  TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), >  TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), >  TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), >  TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), >  TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), >  TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), >  TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] >  > In [3]: import jaxlib >  > In [4]: jax.__version__ > Out[4]: '0.3.17' >  > In [5]: jaxlib.__version__ > Out[5]: '0.3.15' >  > In [6]: jax.random.PRNGKey(0) > Out[6]: DeviceArray([0, 0], dtype=uint32) > ``` >  > I also installed python 3.8.13 and ran the code above, but that doesn't seem an issue either. Did you run this inside a Docker container?",Hi . I didn't. I created a TPU VM and ssh directly into it.  Do check this answer where they mention another solution to this problem.,"Hey . I tried this too in the simple Python 3.8 container, unfortunately with no luck: `pip list` returns: ``` Package             Version   abslpy             1.2.0 certifi             2022.9.24 charsetnormalizer  2.1.1 etils               0.8.0 idna                3.4 importlibresources 5.9.0 jax                 0.3.19 jaxlib              0.3.15 libtpunightly      0.1.dev20220723 numpy               1.23.3 opteinsum          3.3.0 pip                 22.0.4 requests            2.28.1 scipy               1.9.1 setuptools          57.5.0 typing_extensions   4.3.0 urllib3             1.26.12 wheel               0.37.1 zipp                3.8.1 ``` Next, I get a similar error: ``` Python 3.8.14 (default, Sep 13 2022, 15:03:48) [GCC 10.2.1 20210110] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.local_devices() F0928 14:54:46.233109      36 enforce_kernel_ipv6_support.cc:72] Check failed: loopback6_ok  !loopback4_ok This machine (a60669130e01) does not have an IPv6 loopback (::1) interface.  This configuration is obsolete.  You may temporarily set enforce_kernel_ipv6_support=false to bypass this check, but code elsewhere in g3      may fail as result.  See gl_________________30/topics/localhost for details. E0928 14:54:50.457875      36 process_state.cc:774] RAW: Raising signal 6 with default behavior Aborted (core dumped) ```",Updating to `jax[tpu]==0.3.20` solved it for me  thanks ! ⚡ 
336,"以下是一个github上的jax下的一个issue, 标题是([typing] setup: bundle *.pyi files with distribution)， 内容是 (Fixes CC(Array typing/IDE errors since version 0.3.18); part of CC(Tracking Issue: JAX Type Annotations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[typing] setup: bundle *.pyi files with distribution,Fixes CC(Array typing/IDE errors since version 0.3.18); part of CC(Tracking Issue: JAX Type Annotations),2022-09-27T19:56:09Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/12538
4109,"以下是一个github上的jax下的一个issue, 标题是(Failed to initialize CUDA backend on multi-process distributed environments)， 内容是 ( Description The issue is happening when trying to initialize a multiprocess multiGPU environment with Slurm (but I think the problem is external to that).  Take the following simple script ```python  import jax import logging logging.getLogger().setLevel(logging.DEBUG) jax.distributed.initialize() if jax.process_index() == 0:   print(jax.devices())   print(jax.device_count())     total number of accelerator devices in the cluster   print(jax.local_device_count())     number of accelerator devices attached to this host ``` and executed with  `srun gres=gpu:2 ntasks=2 nodes=1  python main.py` and it return  ``` INFO:absl:JAX distributed initialized with visible devices: 0 INFO:absl:JAX distributed initialized with visible devices: 1 INFO:absl:Starting JAX distributed service on ainode17:4192 INFO:absl:Connecting to JAX distributed service on ainode17:4192 INFO:absl:Connecting to JAX distributed service on ainode17:4192 DEBUG:absl:Initializing backend 'interpreter' DEBUG:absl:Initializing backend 'interpreter' DEBUG:absl:Backend 'interpreter' initialized DEBUG:absl:Initializing backend 'cpu' DEBUG:absl:Backend 'cpu' initialized DEBUG:absl:Initializing backend 'tpu_driver' INFO:absl:Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker:  DEBUG:absl:Initializing backend 'cuda' DEBUG:absl:Backend 'interpreter' initialized DEBUG:absl:Initializing backend 'cpu' DEBUG:absl:Backend 'cpu' initialized DEBUG:absl:Initializing backend 'tpu_driver' INFO:absl:Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker:  DEBUG:absl:Initializing backend 'cuda' 20220927 19:23:48.425044: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/pjrt/gpu_device.cc:345) local_device>device_ordinal() == local_topology.devices_size()  *** Begin stack trace *** 	PyCFunction_Call 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	_PyFunction_Vectorcall 	_PyObject_FastCallDict 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	_PyFunction_Vectorcall 	PyObject_Call 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	PyEval_EvalCode 	PyRun_SimpleFileExFlags 	Py_RunMain 	Py_BytesMain 	__libc_start_main 	_start *** End stack trace *** INFO:absl:Unable to initialize backend 'cuda': INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/pjrt/gpu_device.cc:345) local_device>device_ordinal() == local_topology.devices_size()  DEBUG:absl:Initializing backend 'rocm' INFO:absl:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: ""rocm"". Available platform names are: CUDA Interpreter Host DEBUG:absl:Initializing backend 'tpu' INFO:absl:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` Recently (in 0.3.18) there has been an update to the interface for clusters (Slurm and TPUpods), but it doesn't look like it's due to that (i.e. manually setting `coordinator_address`, `num_processes` and `process_id` in `distributed.initialize(...)` has the same effect).  Am I doing something wrong?   What jax/jaxlib version are you using? jax==0.3.18, jaxlib==0.3.15+cuda11.cudnn82  Which accelerator(s) are you using? GPUs  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Failed to initialize CUDA backend on multi-process distributed environments," Description The issue is happening when trying to initialize a multiprocess multiGPU environment with Slurm (but I think the problem is external to that).  Take the following simple script ```python  import jax import logging logging.getLogger().setLevel(logging.DEBUG) jax.distributed.initialize() if jax.process_index() == 0:   print(jax.devices())   print(jax.device_count())     total number of accelerator devices in the cluster   print(jax.local_device_count())     number of accelerator devices attached to this host ``` and executed with  `srun gres=gpu:2 ntasks=2 nodes=1  python main.py` and it return  ``` INFO:absl:JAX distributed initialized with visible devices: 0 INFO:absl:JAX distributed initialized with visible devices: 1 INFO:absl:Starting JAX distributed service on ainode17:4192 INFO:absl:Connecting to JAX distributed service on ainode17:4192 INFO:absl:Connecting to JAX distributed service on ainode17:4192 DEBUG:absl:Initializing backend 'interpreter' DEBUG:absl:Initializing backend 'interpreter' DEBUG:absl:Backend 'interpreter' initialized DEBUG:absl:Initializing backend 'cpu' DEBUG:absl:Backend 'cpu' initialized DEBUG:absl:Initializing backend 'tpu_driver' INFO:absl:Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker:  DEBUG:absl:Initializing backend 'cuda' DEBUG:absl:Backend 'interpreter' initialized DEBUG:absl:Initializing backend 'cpu' DEBUG:absl:Backend 'cpu' initialized DEBUG:absl:Initializing backend 'tpu_driver' INFO:absl:Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker:  DEBUG:absl:Initializing backend 'cuda' 20220927 19:23:48.425044: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/pjrt/gpu_device.cc:345) local_device>device_ordinal() == local_topology.devices_size()  *** Begin stack trace *** 	PyCFunction_Call 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	_PyFunction_Vectorcall 	_PyObject_FastCallDict 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	_PyFunction_Vectorcall 	PyObject_Call 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	PyEval_EvalCode 	PyRun_SimpleFileExFlags 	Py_RunMain 	Py_BytesMain 	__libc_start_main 	_start *** End stack trace *** INFO:absl:Unable to initialize backend 'cuda': INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/pjrt/gpu_device.cc:345) local_device>device_ordinal() == local_topology.devices_size()  DEBUG:absl:Initializing backend 'rocm' INFO:absl:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: ""rocm"". Available platform names are: CUDA Interpreter Host DEBUG:absl:Initializing backend 'tpu' INFO:absl:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` Recently (in 0.3.18) there has been an update to the interface for clusters (Slurm and TPUpods), but it doesn't look like it's due to that (i.e. manually setting `coordinator_address`, `num_processes` and `process_id` in `distributed.initialize(...)` has the same effect).  Am I doing something wrong?   What jax/jaxlib version are you using? jax==0.3.18, jaxlib==0.3.15+cuda11.cudnn82  Which accelerator(s) are you using? GPUs  Additional system info _No response_  NVIDIA GPU info _No response_",2022-09-27T17:33:37Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/12533,"The current `jaxlib` release (0.3.15) is missing support for using subsets of the CUDA devices, which the SLURM support needs. We need either to make a new `jaxlib` release (which we are working on already) or you can build `jaxlib` from source for the moment. We're working on it!",Thanks for the info. Is there a timeline for the next release of jaxlib?,Should be fixed IIUC. ,"This should be fixed with `jax` and `jaxlib` 0.3.20, which we just released. Please try it out!","Thanks for the update. I was compiling the new version, but I guess tomorrow I’ll try now with the precompiled. "
276,"以下是一个github上的jax下的一个issue, 标题是(Rename the concrete class `Array` to `ArrayImpl`)， 内容是 (Rename the concrete class `Array` to `ArrayImpl`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Rename the concrete class `Array` to `ArrayImpl`,Rename the concrete class `Array` to `ArrayImpl`,2022-09-26T19:51:31Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12518
386,"以下是一个github上的jax下的一个issue, 标题是(Add `make_array_from_single_device_arrays` to prepare to rename of the concrete `Array` to `ArrayImpl`.)， 内容是 (Add `make_array_from_single_device_arrays` to prepare to rename of the concrete `Array` to `ArrayImpl`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add `make_array_from_single_device_arrays` to prepare to rename of the concrete `Array` to `ArrayImpl`.,Add `make_array_from_single_device_arrays` to prepare to rename of the concrete `Array` to `ArrayImpl`.,2022-09-26T15:40:38Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12512
633,"以下是一个github上的jax下的一个issue, 标题是(Adds 'preferred_element_type' argument to einsum - which is then forwarded to the dot_general calls in the same way as precision.)， 内容是 (Adds 'preferred_element_type' argument to einsum  which is then forwarded to the dot_general calls in the same way as precision. This came up when working with quantised models, where explicit control of the accumulation type is important  and it is cleaner to code with einsum rather than specifying dot general dimensions.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Adds 'preferred_element_type' argument to einsum - which is then forwarded to the dot_general calls in the same way as precision.,"Adds 'preferred_element_type' argument to einsum  which is then forwarded to the dot_general calls in the same way as precision. This came up when working with quantised models, where explicit control of the accumulation type is important  and it is cleaner to code with einsum rather than specifying dot general dimensions.",2022-09-26T05:28:38Z,,closed,0,2,https://github.com/jax-ml/jax/issues/12508,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Closing Copybara created PR due to inactivity
1990,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Failure with lax.scan with enable_xla=False)， 内容是 ( Description Hello! I'm getting some bad behavior when using lax.scan to compute an exponential moving average over a time axis. Specifically, the EMA function here is causing issues: https://github.com/googleresearch/chirp/blob/main/chirp/audio_utils.pyL37 The model exports to TF SavedModel (with enable_xla=False), but then we get an exception when running the model: ``` google3/third_party/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      52   try:      53     ctx.ensure_initialized() > 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,      55                                         inputs, attrs, num_outputs)      56   except core._NotOkStatusException as e: InvalidArgumentError: Must have updates.shape = indices.shape[:batch_dim] + buffer_shape[num_index_dims:], got updates.shape: [1,1,1,16], indices.shape: [1,128,1,16,1], buffer_shape: [2048], num_index_dims: 1, and batch_dim: 4 	 [[{{node jax2tf_infer_fn_/scan/while/body/ScatterNd_1}}]] 	 [[PartitionedCall/jax2tf_infer_fn_/scan/while]] [Op:__inference_restored_function_body_8540]  ```  which suggests some indices are being misplaced.  A couple observations: * The error occurs even when polymorphic shape is disabled. * When xla is enabled, the savedmodel works correctly. * Strangely, converting the SavedModel to TFLite seems to work out; the model runs and the outptus match the jax model numerically. Minimal reproducing code is in my usual colab notebook of minimal reproductions:  https://colab.corp.google.com/drive/1FX99EPcaX1mAVnpnpUQwkkR0WZ_6o3hscrollTo=280PugPMoiQH    What jax/jaxlib version are you using? v0.3.17  Which accelerator(s) are you using? CPU, GPU  Additional System Info Linux)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Failure with lax.scan with enable_xla=False," Description Hello! I'm getting some bad behavior when using lax.scan to compute an exponential moving average over a time axis. Specifically, the EMA function here is causing issues: https://github.com/googleresearch/chirp/blob/main/chirp/audio_utils.pyL37 The model exports to TF SavedModel (with enable_xla=False), but then we get an exception when running the model: ``` google3/third_party/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)      52   try:      53     ctx.ensure_initialized() > 54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,      55                                         inputs, attrs, num_outputs)      56   except core._NotOkStatusException as e: InvalidArgumentError: Must have updates.shape = indices.shape[:batch_dim] + buffer_shape[num_index_dims:], got updates.shape: [1,1,1,16], indices.shape: [1,128,1,16,1], buffer_shape: [2048], num_index_dims: 1, and batch_dim: 4 	 [[{{node jax2tf_infer_fn_/scan/while/body/ScatterNd_1}}]] 	 [[PartitionedCall/jax2tf_infer_fn_/scan/while]] [Op:__inference_restored_function_body_8540]  ```  which suggests some indices are being misplaced.  A couple observations: * The error occurs even when polymorphic shape is disabled. * When xla is enabled, the savedmodel works correctly. * Strangely, converting the SavedModel to TFLite seems to work out; the model runs and the outptus match the jax model numerically. Minimal reproducing code is in my usual colab notebook of minimal reproductions:  https://colab.corp.google.com/drive/1FX99EPcaX1mAVnpnpUQwkkR0WZ_6o3hscrollTo=280PugPMoiQH    What jax/jaxlib version are you using? v0.3.17  Which accelerator(s) are you using? CPU, GPU  Additional System Info Linux",2022-09-25T08:16:39Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/12504
39172,"以下是一个github上的jax下的一个issue, 标题是(⚠️ Nightly GPU Multiprocess CI failed ⚠️)， 内容是 (Workflow Run URL Failure summary output10410.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.18.dev20220923%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (161.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.0/161.0 MB 21.5 MB/s eta 0:00:00 Collecting numpy>=1.20   Downloading numpy1.23.3cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 118.5 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.2.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.18.dev20220923+cuda11.cudnn82 numpy1.23.3 Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuildbmjlygut   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuildbmjlygut   Resolved https://github.com/google/jax to commit 7c85ca38f45e3ae0d8a80b2d33cf908de9c6564f   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.2.0) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.23.3) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 23.5 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.8.0py3noneany.whl (127 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.2/127.2 kB 38.1 MB/s eta 0:00:00 Requirement already satisfied: zipp in /usr/local/lib/python3.8/sitepackages (from etils[epath]>jax==0.3.18) (3.8.1) Collecting importlib_resources   Downloading importlib_resources5.9.0py3noneany.whl (33 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.18py3noneany.whl size=1256221 sha256=13501af342be3d8d3aa15cd2d01a84d8b4d54a3cc735b6559acb8489dae51a01   Stored in directory: /tmp/pipephemwheelcacheoyn9yxw9/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, importlib_resources, etils, jax Successfully installed etils0.8.0 importlib_resources5.9.0 jax0.3.18 opt_einsum3.3.0 typing_extensions4.3.0 Collecting pytest   Downloading pytest7.1.3py3noneany.whl (298 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.2/298.2 kB 38.7 MB/s eta 0:00:00 Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 24.3 MB/s eta 0:00:00 Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 35.8 MB/s eta 0:00:00 Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.3 tomli2.0.1 Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.3) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_7.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_1.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.54s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_3.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_4.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.54s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_2.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_6.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.56s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_0.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.57s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_5.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.58s =============================== ```  Failure summary output10411.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.18.dev20220923%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (161.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.0/161.0 MB 17.4 MB/s eta 0:00:00 Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.2.0) Collecting numpy>=1.20   Downloading numpy1.23.3cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 118.1 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.18.dev20220923+cuda11.cudnn82 numpy1.23.3 Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuild5182cvp7   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuild5182cvp7   Resolved https://github.com/google/jax to commit 7c85ca38f45e3ae0d8a80b2d33cf908de9c6564f   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.2.0) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.23.3) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 17.8 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.8.0py3noneany.whl (127 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.2/127.2 kB 32.6 MB/s eta 0:00:00 Requirement already satisfied: zipp in /usr/local/lib/python3.8/sitepackages (from etils[epath]>jax==0.3.18) (3.8.1) Collecting importlib_resources   Downloading importlib_resources5.9.0py3noneany.whl (33 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.18py3noneany.whl size=1256221 sha256=f30e3cd0c54350dca5b4d0a18de7a2eb2cd3cd1a69f76b966815b0fe9b8536ec   Stored in directory: /tmp/pipephemwheelcachey43d32uk/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, importlib_resources, etils, jax Successfully installed etils0.8.0 importlib_resources5.9.0 jax0.3.18 opt_einsum3.3.0 typing_extensions4.3.0 Collecting pytest   Downloading pytest7.1.3py3noneany.whl (298 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.2/298.2 kB 24.7 MB/s eta 0:00:00 Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 20.9 MB/s eta 0:00:00 Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 32.1 MB/s eta 0:00:00 Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.3 tomli2.0.1 Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.3) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_9.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.54s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_12.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.54s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_11.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_13.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_10.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_15.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.56s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_8.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.57s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_14.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.58s =============================== ``` )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,⚠️ Nightly GPU Multiprocess CI failed ⚠️,"Workflow Run URL Failure summary output10410.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.18.dev20220923%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (161.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.0/161.0 MB 21.5 MB/s eta 0:00:00 Collecting numpy>=1.20   Downloading numpy1.23.3cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 118.5 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.2.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.18.dev20220923+cuda11.cudnn82 numpy1.23.3 Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuildbmjlygut   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuildbmjlygut   Resolved https://github.com/google/jax to commit 7c85ca38f45e3ae0d8a80b2d33cf908de9c6564f   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.2.0) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.23.3) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 23.5 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.8.0py3noneany.whl (127 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.2/127.2 kB 38.1 MB/s eta 0:00:00 Requirement already satisfied: zipp in /usr/local/lib/python3.8/sitepackages (from etils[epath]>jax==0.3.18) (3.8.1) Collecting importlib_resources   Downloading importlib_resources5.9.0py3noneany.whl (33 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.18py3noneany.whl size=1256221 sha256=13501af342be3d8d3aa15cd2d01a84d8b4d54a3cc735b6559acb8489dae51a01   Stored in directory: /tmp/pipephemwheelcacheoyn9yxw9/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, importlib_resources, etils, jax Successfully installed etils0.8.0 importlib_resources5.9.0 jax0.3.18 opt_einsum3.3.0 typing_extensions4.3.0 Collecting pytest   Downloading pytest7.1.3py3noneany.whl (298 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.2/298.2 kB 38.7 MB/s eta 0:00:00 Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 24.3 MB/s eta 0:00:00 Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 35.8 MB/s eta 0:00:00 Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.3 tomli2.0.1 Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.3) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_7.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_1.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.54s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_3.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_4.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.54s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_2.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_6.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.56s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_0.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.57s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_5.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.58s =============================== ```  Failure summary output10411.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.18.dev20220923%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (161.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 161.0/161.0 MB 17.4 MB/s eta 0:00:00 Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.2.0) Collecting numpy>=1.20   Downloading numpy1.23.3cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 118.1 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.18.dev20220923+cuda11.cudnn82 numpy1.23.3 Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuild5182cvp7   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuild5182cvp7   Resolved https://github.com/google/jax to commit 7c85ca38f45e3ae0d8a80b2d33cf908de9c6564f   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.2.0) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.23.3) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 17.8 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.18) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.8.0py3noneany.whl (127 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.2/127.2 kB 32.6 MB/s eta 0:00:00 Requirement already satisfied: zipp in /usr/local/lib/python3.8/sitepackages (from etils[epath]>jax==0.3.18) (3.8.1) Collecting importlib_resources   Downloading importlib_resources5.9.0py3noneany.whl (33 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.18py3noneany.whl size=1256221 sha256=f30e3cd0c54350dca5b4d0a18de7a2eb2cd3cd1a69f76b966815b0fe9b8536ec   Stored in directory: /tmp/pipephemwheelcachey43d32uk/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, importlib_resources, etils, jax Successfully installed etils0.8.0 importlib_resources5.9.0 jax0.3.18 opt_einsum3.3.0 typing_extensions4.3.0 Collecting pytest   Downloading pytest7.1.3py3noneany.whl (298 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.2/298.2 kB 24.7 MB/s eta 0:00:00 Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 20.9 MB/s eta 0:00:00 Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 32.1 MB/s eta 0:00:00 Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.3 tomli2.0.1 Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.3) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 Sat Sep 24 12:06:36 GMT 2022 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 jax                     0.3.18 jaxlib                  0.3.18.dev20220923+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.3, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_9.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.54s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_12.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.54s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_11.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_13.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_10.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.55s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_15.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.56s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_8.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.57s =============================== collected 0 items / 1 error ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ ImportError while importing test module '/workspace/tests/multiprocess_gpu_test.py'. Hint: make sure your test modules/packages have valid Python names. Traceback: usr/local/lib/python3.8/importlib/__init__.py:127: in import_module     return _bootstrap._gcd_import(name[level:], package, level) workspace/tests/multiprocess_gpu_test.py:26: in      import jax workspace/jax/__init__.py:35: in      from jax import config as _config_module workspace/jax/config.py:17: in      from jax._src.config import config workspace/jax/_src/config.py:29: in      from jax._src import lib workspace/jax/_src/lib/__init__.py:100: in      import jaxlib.xla_client as xla_client usr/local/lib/python3.8/sitepackages/jaxlib/xla_client.py:25: in      from . import xla_extension as _xla E   ImportError: /usr/local/lib/python3.8/sitepackages/jaxlib/xla_extension.so: undefined symbol: _ZN15stream_executor4cuda9GetBlasLtEPNS_6StreamE  generated xml file: /workspace/outputs/junit_output_14.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py =============================== 1 error in 0.58s =============================== ``` ",2022-09-24T12:07:44Z,Nightly-CI,closed,0,1,https://github.com/jax-ml/jax/issues/12499,`jaxlib` wasn't built with the XLA head that fixed this issue. CI passes here: https://github.com/google/jax/actions/runs/3122080023
587,"以下是一个github上的jax下的一个issue, 标题是(Modify CorrCoef test to not rely on floating poing representation of 1/3)， 内容是 (Modify CorrCoef test to not rely on floating poing representation of 1/3 The operation computed an average while using the dimension of size 3. This is then changed to multiplying by 1/3 with compilers, but 1/3 cannot be represented perfectly. That made this test case rely on a very precise result from an unrepresentable calculation.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Modify CorrCoef test to not rely on floating poing representation of 1/3,"Modify CorrCoef test to not rely on floating poing representation of 1/3 The operation computed an average while using the dimension of size 3. This is then changed to multiplying by 1/3 with compilers, but 1/3 cannot be represented perfectly. That made this test case rely on a very precise result from an unrepresentable calculation.",2022-09-22T12:25:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12463
18831,"以下是一个github上的jax下的一个issue, 标题是(JAX is not finding cuDNN even though it is on LD_LIBRARY_PATH)， 内容是 ( Description I have a project which is relatively simple at the moment, yet whenever a convolution is called the following error is raised: ``` jaxlib.xla_extension.XlaRuntimeError: UNIMPLEMENTED: DNN library is not found. ``` I tried to create a minimal reproducible example, but the error message is different for some reason. Here is the example: ``` from jax.lax import conv import jax.random as jrd rng = jrd.PRNGKey(0) x = jrd.truncated_normal(rng, 1, 1, (1, 1, 28, 28)) kernel = jrd.truncated_normal(rng, 1, 1, (2, 1, 3, 3)) y = conv(x, kernel, window_strides=(1, 1), padding='SAME') ``` Here is the full output when this script is run: ``` 20220922 13:34:04.032810: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:727] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.  Conv: (f32[1,2,28,28]{3,2,1,0}, u8[0]{0}) customcall(f32[1,1,28,28]{3,2,1,0}, f32[2,1,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" 20220922 13:34:04.112949: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:930] Failed to determine best cudnn convolution algorithm for: %cudnnconv = (f32[1,2,28,28]{3,2,1,0}, u8[0]{0}) customcall(f32[1,1,28,28]{3,2,1,0} %Arg_0.1, f32[2,1,3,3]{3,2,1,0} %Arg_1.2), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(conv_general_dilated)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2, 3), rhs_spec=(0, 1, 2, 3), out_spec=(0, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 1, 28, 28) rhs_shape=(2, 1, 3, 3) precision=None preferred_element_type=None]"" source_file=""/home/medusa/Projects/repros/jaxconv.py"" source_line=5}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" Original error: INTERNAL: All algorithms tried for %cudnnconv = (f32[1,2,28,28]{3,2,1,0}, u8[0]{0}) customcall(f32[1,1,28,28]{3,2,1,0} %Arg_0.1, f32[2,1,3,3]{3,2,1,0} %Arg_1.2), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(conv_general_dilated)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2, 3), rhs_spec=(0, 1, 2, 3), out_spec=(0, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 1, 28, 28) rhs_shape=(2, 1, 3, 3) precision=None preferred_element_type=None]"" source_file=""/home/medusa/Projects/repros/jaxconv.py"" source_line=5}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" failed. Falling back to default algorithm.  Peralgorithm errors:   Profiling failure on cuDNN engine 1TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 1: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 0TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 0: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 2TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 2: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 4TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 4: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 6TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 6: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 5TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 5: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 7TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 7: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 1TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 1: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 0TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 0: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 2TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 2: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 4TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 4: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 6TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 6: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 5TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 5: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 7TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 7: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())' As a result, convolution performance may be suboptimal. 20220922 13:34:04.118887: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2130] Execution of replica 0 failed: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())' Traceback (most recent call last):   File ""/home/medusa/Projects/repros/jaxconv.py"", line 7, in      y = conv(x, kernel, window_strides=(1, 1), padding='SAME')   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/lax/convolution.py"", line 194, in conv     return conv_general_dilated(lhs, rhs, window_strides, padding,   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/lax/convolution.py"", line 158, in conv_general_dilated     return conv_general_dilated_p.bind(   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/core.py"", line 325, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/core.py"", line 328, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/core.py"", line 686, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 113, in apply_primitive     return compiled_fun(*args)   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 198, in      return lambda *args, **kw: compiled(*args, **kw)[0]   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 837, in _execute_compiled     out_flat = compiled.execute(in_flat) jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())' ``` It's a really strange and verbose error for a convolution that should be exceedingly simple. This occurs both with and without `export XLA_FLAGS=""xla_gpu_strict_conv_algorithm_picker=false""` commented in my bashrc. Based on my research of other github issues, the basic cause of this should be that cuDNN is just not installed. However, I followed the instructions for installing it from nvidia, and it is indeed present: ``` $ echo $LD_LIBRARY_PATH :/usr/local/cuda11.6/include:/usr/local/cuda11.6/lib64:/usr/local/cuda11.6/include:/usr/local/cuda11.6/lib64 ``` Indeed, inside `/usr/local/cuda11.6/lib64` is `libcudnn.so` and `libcudnn.so.8`, as well as a lot of other shared object files for cuDNN. It seems either there is a problem with what JAX is looking for or I am missing some important file. I'm guessing its the latter, so any list of files I should double check for would be extremely helpful.  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? GPU  Additional System Info Python 3.10.4 / Ubuntu 21.10)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JAX is not finding cuDNN even though it is on LD_LIBRARY_PATH," Description I have a project which is relatively simple at the moment, yet whenever a convolution is called the following error is raised: ``` jaxlib.xla_extension.XlaRuntimeError: UNIMPLEMENTED: DNN library is not found. ``` I tried to create a minimal reproducible example, but the error message is different for some reason. Here is the example: ``` from jax.lax import conv import jax.random as jrd rng = jrd.PRNGKey(0) x = jrd.truncated_normal(rng, 1, 1, (1, 1, 28, 28)) kernel = jrd.truncated_normal(rng, 1, 1, (2, 1, 3, 3)) y = conv(x, kernel, window_strides=(1, 1), padding='SAME') ``` Here is the full output when this script is run: ``` 20220922 13:34:04.032810: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:727] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.  Conv: (f32[1,2,28,28]{3,2,1,0}, u8[0]{0}) customcall(f32[1,1,28,28]{3,2,1,0}, f32[2,1,3,3]{3,2,1,0}), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" 20220922 13:34:04.112949: W external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:930] Failed to determine best cudnn convolution algorithm for: %cudnnconv = (f32[1,2,28,28]{3,2,1,0}, u8[0]{0}) customcall(f32[1,1,28,28]{3,2,1,0} %Arg_0.1, f32[2,1,3,3]{3,2,1,0} %Arg_1.2), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(conv_general_dilated)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2, 3), rhs_spec=(0, 1, 2, 3), out_spec=(0, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 1, 28, 28) rhs_shape=(2, 1, 3, 3) precision=None preferred_element_type=None]"" source_file=""/home/medusa/Projects/repros/jaxconv.py"" source_line=5}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" Original error: INTERNAL: All algorithms tried for %cudnnconv = (f32[1,2,28,28]{3,2,1,0}, u8[0]{0}) customcall(f32[1,1,28,28]{3,2,1,0} %Arg_0.1, f32[2,1,3,3]{3,2,1,0} %Arg_1.2), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convForward"", metadata={op_name=""jit(conv_general_dilated)/jit(main)/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 1, 2, 3), rhs_spec=(0, 1, 2, 3), out_spec=(0, 1, 2, 3)) feature_group_count=1 batch_group_count=1 lhs_shape=(1, 1, 28, 28) rhs_shape=(2, 1, 3, 3) precision=None preferred_element_type=None]"" source_file=""/home/medusa/Projects/repros/jaxconv.py"" source_line=5}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" failed. Falling back to default algorithm.  Peralgorithm errors:   Profiling failure on cuDNN engine 1TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 1: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 0TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 0: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 2TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 2: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 4TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 4: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 6TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 6: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 5TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 5: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 7TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 7: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 1TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 1: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 0TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 0: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 2TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 2: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 4TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 4: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 6TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 6: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 5TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 5: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 7TC: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())'   Profiling failure on cuDNN engine 7: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())' As a result, convolution performance may be suboptimal. 20220922 13:34:04.118887: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2130] Execution of replica 0 failed: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())' Traceback (most recent call last):   File ""/home/medusa/Projects/repros/jaxconv.py"", line 7, in      y = conv(x, kernel, window_strides=(1, 1), padding='SAME')   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/lax/convolution.py"", line 194, in conv     return conv_general_dilated(lhs, rhs, window_strides, padding,   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/lax/convolution.py"", line 158, in conv_general_dilated     return conv_general_dilated_p.bind(   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/core.py"", line 325, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/core.py"", line 328, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/core.py"", line 686, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 113, in apply_primitive     return compiled_fun(*args)   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 198, in      return lambda *args, **kw: compiled(*args, **kw)[0]   File ""/home/medusa/anaconda3/envs/critrep/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 837, in _execute_compiled     out_flat = compiled.execute(in_flat) jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: CUDNN_STATUS_EXECUTION_FAILED in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_dnn.cc(4023): 'cudnnConvolutionForward( cudnn.handle(), alpha, input_nd_.handle(), input_data.opaque(), filter_.handle(), filter_data.opaque(), conv_.handle(), ToConvForwardAlgo(algo), scratch_memory.opaque(), scratch_memory.size(), beta, output_nd_.handle(), output_data.opaque())' ``` It's a really strange and verbose error for a convolution that should be exceedingly simple. This occurs both with and without `export XLA_FLAGS=""xla_gpu_strict_conv_algorithm_picker=false""` commented in my bashrc. Based on my research of other github issues, the basic cause of this should be that cuDNN is just not installed. However, I followed the instructions for installing it from nvidia, and it is indeed present: ``` $ echo $LD_LIBRARY_PATH :/usr/local/cuda11.6/include:/usr/local/cuda11.6/lib64:/usr/local/cuda11.6/include:/usr/local/cuda11.6/lib64 ``` Indeed, inside `/usr/local/cuda11.6/lib64` is `libcudnn.so` and `libcudnn.so.8`, as well as a lot of other shared object files for cuDNN. It seems either there is a problem with what JAX is looking for or I am missing some important file. I'm guessing its the latter, so any list of files I should double check for would be extremely helpful.  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? GPU  Additional System Info Python 3.10.4 / Ubuntu 21.10",2022-09-22T11:46:56Z,bug NVIDIA GPU,open,0,5,https://github.com/jax-ml/jax/issues/12461,"What GPU do you have, and what is the output of `nvidiasmi`?","GeForce RTX 2060 ``` $ nvidiasmi Fri Sep 23 15:54:49 2022        ++  ++ ``` By the way, no such error occurs with regular matrix multiplication, and the following script distinctively increases GPU utilization: ``` import jax.numpy as jnp import jax.random as jrd rng = jrd.PRNGKey(0) x = jrd.truncated_normal(rng, 1, 1, (100000,)) kernel = jrd.truncated_normal(rng, 1, 1, (1000, 100000)) y = jnp.matmul(kernel, x) ```","Try an experiment for me: try disabling preallocation, lowering the memory fraction, or using the platform allocator as described here: https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html Does that help?","  Phenomenal! Disabling preallocation solves the issue! I was aware that these flags were available if too much memory was being preallocated with Xorg (etc) active in the background, but I would have never made the connection with being unable to load cuDNN. What is the root cause here / how did you know to try that?","The underlying problem is that CuDNN will fail to initialize if JAX grabs too much GPU memory.  There are two things we can do to improve matters here: a) improve the error message to suggest this as a possible cause, and b) change the JAX allocator design a bit. (b) is longer term work but (a) we should be able to do sooner."
3222,"以下是一个github上的jax下的一个issue, 标题是(Error on large TPU Pods)， 内容是 ( Description Hi, I'm not sure if this is a jax bug, or an issue on the TPU side. Steps to replicate my issue: 1) Create a v3128 TPU instance 2) Install `jax`: `pip install ""jax[tpu]"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html` 3) Run `script.py` (for all instances) ```  script.py import jax print(jax.device_count()) ``` I get the following error **only on the first machine (....w0)**. The other machines just stall with the `TPU backend initializatoin is taking ....` warning. ``` $ python3 script.py /home/wilson/.local/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py:184: UserWarning: TPU backend initialization is taking more than 60.0 seconds. Did you run your code on all TPU hosts? See https://jax.readthedocs.io/en/latest/multi_process.html for more information.   warnings.warn( https://symbolize.stripped_domain/r/?trace=7facba4a8de2,7fadfda3d08f,7facba4ad373,7facb845d7c2,7facbd753668,7facbd74f0fd,7fadfd9df608&map=068bf80b76f830987166dd8847d0248f:7faca82980007facbdbcade0  *** SIGSEGV (@(nil)), see gl__________41s15 received by PID 11743 (TID 12350) on cpu 17; stack trace: *** PC: @     0x7facba4a8de2  (unknown)  (unknown)     @     0x7facbd838294        976  (unknown)     @     0x7fadfda3d090    1120528  (unknown)     @     0x7facba4ad374         32  (unknown)     @     0x7facb845d7c3         48  (unknown)     @     0x7facbd753669        112  (unknown)     @     0x7facbd74f0fe        240  (unknown)     @     0x7fadfd9df609  (unknown)  start_thread https://symbolize.stripped_domain/r/?trace=7facba4a8de2,7facbd838293,7fadfda3d08f,7facba4ad373,7facb845d7c2,7facbd753668,7facbd74f0fd,7fadfd9df608&map=068bf80b76f830987166dd8847d0248f:7faca82980007facbdbcade0  E0922 01:12:51.498153   12350 coredump_hook.cc:370] RAW: Remote crash data gathering hook invoked. E0922 01:12:51.498165   12350 coredump_hook.cc:416] RAW: Skipping coredump since rlimit was 0 at process start. E0922 01:12:51.498174   12350 client.cc:242] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec. E0922 01:12:51.498181   12350 coredump_hook.cc:477] RAW: Sending fingerprint to remote end. E0922 01:12:51.498186   12350 coredump_socket.cc:118] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket E0922 01:12:51.498196   12350 coredump_hook.cc:481] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running? E0922 01:12:51.498202   12350 coredump_hook.cc:555] RAW: Discarding core. E0922 01:12:51.645117   12350 process_state.cc:774] RAW: Raising signal 11 with default behavior Segmentation fault (core dumped) ``` My `jax` versions across all instances are ``` jax==0.3.17 jaxlib==0.3.15 libtpunightly==0.1.dev20220723 ``` A few notes:  This worked fine on and before 9/20  I get a similar error on `jax==0.3.13`, so I don't think it's version dependent (hence I'm not sure if this is a jax bug)  **This only happens on a larger instance. This does not happen on v332 and v364.**)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Error on large TPU Pods," Description Hi, I'm not sure if this is a jax bug, or an issue on the TPU side. Steps to replicate my issue: 1) Create a v3128 TPU instance 2) Install `jax`: `pip install ""jax[tpu]"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html` 3) Run `script.py` (for all instances) ```  script.py import jax print(jax.device_count()) ``` I get the following error **only on the first machine (....w0)**. The other machines just stall with the `TPU backend initializatoin is taking ....` warning. ``` $ python3 script.py /home/wilson/.local/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py:184: UserWarning: TPU backend initialization is taking more than 60.0 seconds. Did you run your code on all TPU hosts? See https://jax.readthedocs.io/en/latest/multi_process.html for more information.   warnings.warn( https://symbolize.stripped_domain/r/?trace=7facba4a8de2,7fadfda3d08f,7facba4ad373,7facb845d7c2,7facbd753668,7facbd74f0fd,7fadfd9df608&map=068bf80b76f830987166dd8847d0248f:7faca82980007facbdbcade0  *** SIGSEGV (@(nil)), see gl__________41s15 received by PID 11743 (TID 12350) on cpu 17; stack trace: *** PC: @     0x7facba4a8de2  (unknown)  (unknown)     @     0x7facbd838294        976  (unknown)     @     0x7fadfda3d090    1120528  (unknown)     @     0x7facba4ad374         32  (unknown)     @     0x7facb845d7c3         48  (unknown)     @     0x7facbd753669        112  (unknown)     @     0x7facbd74f0fe        240  (unknown)     @     0x7fadfd9df609  (unknown)  start_thread https://symbolize.stripped_domain/r/?trace=7facba4a8de2,7facbd838293,7fadfda3d08f,7facba4ad373,7facb845d7c2,7facbd753668,7facbd74f0fd,7fadfd9df608&map=068bf80b76f830987166dd8847d0248f:7faca82980007facbdbcade0  E0922 01:12:51.498153   12350 coredump_hook.cc:370] RAW: Remote crash data gathering hook invoked. E0922 01:12:51.498165   12350 coredump_hook.cc:416] RAW: Skipping coredump since rlimit was 0 at process start. E0922 01:12:51.498174   12350 client.cc:242] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec. E0922 01:12:51.498181   12350 coredump_hook.cc:477] RAW: Sending fingerprint to remote end. E0922 01:12:51.498186   12350 coredump_socket.cc:118] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket E0922 01:12:51.498196   12350 coredump_hook.cc:481] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running? E0922 01:12:51.498202   12350 coredump_hook.cc:555] RAW: Discarding core. E0922 01:12:51.645117   12350 process_state.cc:774] RAW: Raising signal 11 with default behavior Segmentation fault (core dumped) ``` My `jax` versions across all instances are ``` jax==0.3.17 jaxlib==0.3.15 libtpunightly==0.1.dev20220723 ``` A few notes:  This worked fine on and before 9/20  I get a similar error on `jax==0.3.13`, so I don't think it's version dependent (hence I'm not sure if this is a jax bug)  **This only happens on a larger instance. This does not happen on v332 and v364.**",2022-09-22T01:14:50Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/12456,"Just to check, you're using `version tpuvmbase` when you're creating the v3128? Can you share `/tmp/tpu_logs/tpu_driver.INFO` from the failing worker after you get that segfault? The same file from other workers may also be useful if it's easy to grab, but the failing one is likely the most useful.","Thanks for getting back! It seems like the issue has fixed itself somehow. I don't think it was random yesterday either since every v3128 I allocated had the same issue, but seems to work fine on the first time today. I'll close the issue for now unless it happens against sometime in the future",I'm having the same issue. The worker is stalled at the warning. Any ideas on why this could be happening?," could you file a new issue giving more information (e.g. TPU type, what you're running, what you get on each host, anything else that could be useful)? Please tag me in it too.",Also make sure you're invoking your Python script on each TPU node if you're not already.,"In my log file, there is an error message. ``` FAILED_PRECONDITION: Failed to read file [type.googleapis.com/util.ErrorSpacePayload='util::PosixErrorSpace::Bad file descriptor'] ``` How to resolve it?",I found that the TPU Pod has different way to execute the code..
697,"以下是一个github上的jax下的一个issue, 标题是(Add initial implementation of jax.Array base class)， 内容是 (Part of CC(Tracking Issue: JAX Type Annotations) The idea here is that `jax.Array` will eventually be defined in C++ and used as a true base class of both `core.Tracer` and `jax.experimental.array.Array`, which will be renamed to `ArrayImpl` or something similar before being moved to `jax._src.array`. ~Still need to add some tests similar to the ones in https://github.com/google/jax/pull/12351 to ensure that this implementation has the desired properties.~ *Done*)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add initial implementation of jax.Array base class,"Part of CC(Tracking Issue: JAX Type Annotations) The idea here is that `jax.Array` will eventually be defined in C++ and used as a true base class of both `core.Tracer` and `jax.experimental.array.Array`, which will be renamed to `ArrayImpl` or something similar before being moved to `jax._src.array`. ~Still need to add some tests similar to the ones in https://github.com/google/jax/pull/12351 to ensure that this implementation has the desired properties.~ *Done*",2022-09-19T19:50:01Z,pull ready,closed,3,5,https://github.com/jax-ml/jax/issues/12421,I had been making some effort to land independent parts of this separately in order to make the change less extensive (e.g. https://github.com/google/jax/pull/12370). Do you think that's worth doing here? I worry a change like this is big enough that it might cause unexpected issues (e.g. does having `Tracer` inherit from a class without `__slots__` cause performance degredation?) So thought it would be worth landing in small bits. What do you think?,"> does having Tracer inherit from a class without __slots__ cause performance degredation?)  I think the best way to know this is to write a quick benchmark and see if it regresses. The one good thing I like about this CL is that if it does break something, we can roll it back as 1 CL rather than rolling back parts of it. So I think it's fine to keep this as is. If you are worried about perf, https://github.com/google/jax/blob/main/benchmarks/api_benchmark.py has been really handy and you can run this locally too!","I'm more worried about the things I haven't thought of... and changes like CC([typing] overloaded type declaration for variadic lax.sort) are truly independent – they're correct whether or not we land the rest of this, so there wouldn't have to be rolled back together. There are a few other similar changes in here (e.g. the changes to `lax_numpy.py`)",Ohh I see. Yeah splitting up SGTM.,(Just wanted to say a huge THANK YOU for this Jake  it's something that a lot of us at DeepMind have been wanting for a long time!)
2539,"以下是一个github上的jax下的一个issue, 标题是(Running pytest locally -- against AttributeError: num_generated_cases)， 内容是 ( Description I'm trying to run pytest locally, but the test session fails due to ` AttributeError: num_generated_cases` The command: either `$ pytest` or  `$ JAX_NUM_GENERATED_CASES=1 pytest` ``` ... ERROR tests/state_test.py  AttributeError: num_generated_cases ERROR tests/stax_test.py  AttributeError: num_generated_cases ERROR tests/svd_test.py  AttributeError: num_generated_cases ERROR tests/x64_context_test.py  AttributeError: num_generated_cases ERROR tests/xmap_test.py  AttributeError: num_generated_cases ERROR tests/third_party/scipy/line_search_test.py  AttributeError: num_generated_cases !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 49 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!! ========================================= 49 errors in 6.24s ======================================== ``` Failing tests' stacktraces are: ```python ___________ ERROR collecting tests/third_party/scipy/line_search_test.py ___________ tests/third_party/scipy/line_search_test.py:14: in      class TestLineSearch(jtu.JaxTestCase): tests/third_party/scipy/line_search_test.py:64: in TestLineSearch     .named_parameters(jtu.cases_from_list( jax/_src/test_util.py:661: in cases_from_list     k = min(n, FLAGS.num_generated_cases) jax/_src/config.py:453: in __getattr__     return self._getter(name) jax/_src/config.py:97: in read     return self._read(name) jax/_src/config.py:101: in _read     return getattr(self.absl_flags.FLAGS, name) .../lib/python3.10/sitepackages/absl/flags/_flagvalues.py:471: in __getattr__     raise AttributeError(name) E   AttributeError: num_generated_cases ``` But `FLAGS.num_generated_cases` is defined in `jax/_src/test_util.py`: ```python flags.DEFINE_integer(                                 'num_generated_cases',                              int(os.getenv('JAX_NUM_GENERATED_CASES', '10')),    help='Number of generated cases to test')         ``` Running individual tests, i.e. `pytest  tests/third_party/scipy/line_search_test.py` works fine. During collecting the test suites, ABSL flags seem to not have been registered properly (maybe at the import time).  What jax/jaxlib version are you using? latest master (0.3.17+)  Which accelerator(s) are you using? N/A  Additional System Info macOS  Python: 3.10.5 (miniconda3)  pytest 7.1.3  abslpy 1.2.0)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Running pytest locally -- against AttributeError: num_generated_cases," Description I'm trying to run pytest locally, but the test session fails due to ` AttributeError: num_generated_cases` The command: either `$ pytest` or  `$ JAX_NUM_GENERATED_CASES=1 pytest` ``` ... ERROR tests/state_test.py  AttributeError: num_generated_cases ERROR tests/stax_test.py  AttributeError: num_generated_cases ERROR tests/svd_test.py  AttributeError: num_generated_cases ERROR tests/x64_context_test.py  AttributeError: num_generated_cases ERROR tests/xmap_test.py  AttributeError: num_generated_cases ERROR tests/third_party/scipy/line_search_test.py  AttributeError: num_generated_cases !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 49 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!! ========================================= 49 errors in 6.24s ======================================== ``` Failing tests' stacktraces are: ```python ___________ ERROR collecting tests/third_party/scipy/line_search_test.py ___________ tests/third_party/scipy/line_search_test.py:14: in      class TestLineSearch(jtu.JaxTestCase): tests/third_party/scipy/line_search_test.py:64: in TestLineSearch     .named_parameters(jtu.cases_from_list( jax/_src/test_util.py:661: in cases_from_list     k = min(n, FLAGS.num_generated_cases) jax/_src/config.py:453: in __getattr__     return self._getter(name) jax/_src/config.py:97: in read     return self._read(name) jax/_src/config.py:101: in _read     return getattr(self.absl_flags.FLAGS, name) .../lib/python3.10/sitepackages/absl/flags/_flagvalues.py:471: in __getattr__     raise AttributeError(name) E   AttributeError: num_generated_cases ``` But `FLAGS.num_generated_cases` is defined in `jax/_src/test_util.py`: ```python flags.DEFINE_integer(                                 'num_generated_cases',                              int(os.getenv('JAX_NUM_GENERATED_CASES', '10')),    help='Number of generated cases to test')         ``` Running individual tests, i.e. `pytest  tests/third_party/scipy/line_search_test.py` works fine. During collecting the test suites, ABSL flags seem to not have been registered properly (maybe at the import time).  What jax/jaxlib version are you using? latest master (0.3.17+)  Which accelerator(s) are you using? N/A  Additional System Info macOS  Python: 3.10.5 (miniconda3)  pytest 7.1.3  abslpy 1.2.0",2022-09-19T01:45:09Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/12411,"`absl`'s flag mechanism has always been a big pain for me because it has a side effect on the modulelevel, e.g., use of global variables as state. I wonder if absl flag is reset somewhere while collecting/executing the module, or absl flag parsing is skipped due to `already_configured_with_absl`. It looks like once `parse_flags_with_absl` is called, all subsequent `FLAGS.define_XXX` are ignored.","Given the above observation, a workaround I find is to add following lines to `conftest.py`, which is executed before collecting pytest suites: ```python  Ensure JAX test flags are registered before collecting test suites.  see https://github.com/google/jax/issues/12411  pylint: disablenext=unusedimport import jax._src.test_util ``` which eliminates all the AttributeErrors during test collection.",I suspect the issue is we always run `pytest tests` not `pytest`. But it seems like a reasonable thing to want to do.
804,"以下是一个github上的jax下的一个issue, 标题是(scipy.optimize not found)， 内容是 ( Description When trying to get the optimize, it is not found. Same behavior on local installation and Google Collab. ``` In [9]: import jax In [10]: jax.__version__ Out[10]: '0.3.17' In [12]: jax.scipy.optimize.minimize  AttributeError                            Traceback (most recent call last) Input In [12], in () > 1 jax.scipy.optimize.minimize AttributeError: module 'jax.scipy' has no attribute 'optimize' ``` Am I missing something? Best regards  What jax/jaxlib version are you using? jax v0.3.17 and jaxlib v0.3.15  Which accelerator(s) are you using? CPU/GPU  Additional System Info Linux)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,scipy.optimize not found," Description When trying to get the optimize, it is not found. Same behavior on local installation and Google Collab. ``` In [9]: import jax In [10]: jax.__version__ Out[10]: '0.3.17' In [12]: jax.scipy.optimize.minimize  AttributeError                            Traceback (most recent call last) Input In [12], in () > 1 jax.scipy.optimize.minimize AttributeError: module 'jax.scipy' has no attribute 'optimize' ``` Am I missing something? Best regards  What jax/jaxlib version are you using? jax v0.3.17 and jaxlib v0.3.15  Which accelerator(s) are you using? CPU/GPU  Additional System Info Linux",2022-09-16T17:24:52Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/12390,"Thanks for the question! I think this is working as intended: just write `import jax.scipy.optimize` first. The `scipy` name isn't bound in the toplevel `jax` package/namespace (i.e. not imported in the `jax/__init__.py` file), so instead you just import the module directly. ``` In [1]: import jax.scipy.optimize In [2]: jax.scipy.optimize.minimize Out[2]:  jax._src.scipy.optimize.minimize.OptimizeResults> ``` This organization actually mirrors scipy's too: ``` In [1]: import scipy In [2]: scipy.optimize  AttributeError                            Traceback (most recent call last)  in  > 1 scipy.optimize AttributeError: module 'scipy' has no attribute 'optimize' In [3]: import scipy.optimize In [4]: scipy.optimize.minimize Out[4]: In [4]: ```"
912,"以下是一个github上的jax下的一个issue, 标题是(jax.numpy.average can't take tuple axis)， 内容是 ( Description Tuple axis is documented: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.average.htmljax.numpy.average And supported in numpy: ```python import numpy as np np.average(np.arange(0, 4), axis=(0,), weights=np.arange(1, 5))   2.0 ``` But will result an error ```python from jax import numpy as jnp jnp.average(jnp.arange(0, 4), axis=(0,), weights=jnp.arange(1, 5)) ``` ``` TypeError: 'tuple' object cannot be interpreted as an integer ``` Possibly related: CC(jax.numpy.flip() unable to accept tuple inputs to axis keyword argument)  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Linux)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jax.numpy.average can't take tuple axis," Description Tuple axis is documented: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.average.htmljax.numpy.average And supported in numpy: ```python import numpy as np np.average(np.arange(0, 4), axis=(0,), weights=np.arange(1, 5))   2.0 ``` But will result an error ```python from jax import numpy as jnp jnp.average(jnp.arange(0, 4), axis=(0,), weights=jnp.arange(1, 5)) ``` ``` TypeError: 'tuple' object cannot be interpreted as an integer ``` Possibly related: CC(jax.numpy.flip() unable to accept tuple inputs to axis keyword argument)  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Linux",2022-09-16T10:37:40Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/12388, are you up for taking this one?
1777,"以下是一个github上的jax下的一个issue, 标题是(Problem mixing checkify.check and jax.lax.switch)， 内容是 ( Description I'm trying to do a switch over functions that have a checkify check inside. It goes somewhat like this: ```python def make_branch(i):   def branch():     result = jnp.full((1,), i)     checkify.check(jnp.sum(result) > 0, 'Failed!')     return result   return branch index = jnp.full((), 0) result = jax.lax.switch(index, [make_branch(i) for i in range(3)]) ``` This, unfortunately, fails with `Cannot abstractly evaluate a checkify.check which was not functionalized`, presumably because `switch` compiles its branches. I've tried transforming branches with `checkify`: ```python def make_branch(i):   .checkify   def branch():     result = jnp.full((1,), i)     checkify.check(jnp.sum(result) > 0, 'Failed!')     return result   return branch index = jnp.full((), 0) err, result = jax.lax.switch(index, [make_branch(i) for i in range(3)]) checkify.check_error(err) ``` This approach, unfortunately, fails with another error: ``` branch 0 and 1 outputs must have same type structure, got PyTreeDef((CustomNode(Error[((4, 'Failed! (check failed at :6 (branch))'),)], [*, *, *]), *)) and PyTreeDef((CustomNode(Error[((5, 'Failed! (check failed at :6 (branch))'),)], [*, *, *]), *)). ``` For some reason type signatures for branches are different where error is envolved. That seems like an unexpected behavior that prohibits using checks with conditional constructs. Is there a way around it?  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? GPU  Additional System Info Google Colab)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Problem mixing checkify.check and jax.lax.switch," Description I'm trying to do a switch over functions that have a checkify check inside. It goes somewhat like this: ```python def make_branch(i):   def branch():     result = jnp.full((1,), i)     checkify.check(jnp.sum(result) > 0, 'Failed!')     return result   return branch index = jnp.full((), 0) result = jax.lax.switch(index, [make_branch(i) for i in range(3)]) ``` This, unfortunately, fails with `Cannot abstractly evaluate a checkify.check which was not functionalized`, presumably because `switch` compiles its branches. I've tried transforming branches with `checkify`: ```python def make_branch(i):   .checkify   def branch():     result = jnp.full((1,), i)     checkify.check(jnp.sum(result) > 0, 'Failed!')     return result   return branch index = jnp.full((), 0) err, result = jax.lax.switch(index, [make_branch(i) for i in range(3)]) checkify.check_error(err) ``` This approach, unfortunately, fails with another error: ``` branch 0 and 1 outputs must have same type structure, got PyTreeDef((CustomNode(Error[((4, 'Failed! (check failed at :6 (branch))'),)], [*, *, *]), *)) and PyTreeDef((CustomNode(Error[((5, 'Failed! (check failed at :6 (branch))'),)], [*, *, *]), *)). ``` For some reason type signatures for branches are different where error is envolved. That seems like an unexpected behavior that prohibits using checks with conditional constructs. Is there a way around it?  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? GPU  Additional System Info Google Colab",2022-09-16T00:28:31Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/12385,"hm, I think this means we need a `switch_error_check` which will do this functionalizing/merging for us. Thanks for reporting! (and for bearing with us while we make checkify feature complete!)","aha, actually the issue is that the `lax.switch` doesn't have any inputs: if you add an operand, the check is succesfully eliminated (so you don't get the first functionalization error). ``` def make_branch(i):                                                                                                                                         def branch(x):                                                                                                                                              checkify.check(jnp.sum(x) > 0, 'Failed!')                                                                                                                 return x                                                                                                                                                return branch                                                                                                                                           def f(x):                                                                                                                                                   return lax.switch(0, [make_branch(i) for i in range(3)], x)                                                                                             checked_f = checkify.checkify(f)                                                                                                                          err, _ = checked_f(1.)                                                                                                                                   ``` This still needs to be fixed, but might unblock you for the moment?"
610,"以下是一个github上的jax下的一个issue, 标题是(Remove device_buffer and device_buffer properties from Array)， 内容是 (Currently `Array` has `device_buffer` and `device_buffers` properties. This is because of backwards compatiblity reasons as DA and SDA has those properties respectively and we want the transition to Array as simple as possible with minimal user code changes. Those properties will be removed when an Array specific public API is provided to access the underlying buffer.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Remove device_buffer and device_buffer properties from Array,Currently `Array` has `device_buffer` and `device_buffers` properties. This is because of backwards compatiblity reasons as DA and SDA has those properties respectively and we want the transition to Array as simple as possible with minimal user code changes. Those properties will be removed when an Array specific public API is provided to access the underlying buffer.,2022-09-15T19:35:05Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12380
1738,"以下是一个github上的jax下的一个issue, 标题是(Allow pjit to close over values sharded on multiple devices)， 内容是 (Consider the following code fragment: ```py x = load_checkpoint() (pjit, in_axis_resources=in_sharding, out_axis_resources=out_sharding): def foo(a)   return a + x y = load_another_checkpoint() z = foo(y) ``` The call to `foo(y)` fails if the value of `x` is sharded over multiple hosts. One scenario when supporting this would be very useful is if one need to close over a large static model in order to adapt to the interface required by an existing library they use. E.g. assume one wants to use the `XTraner` class below, which is a part of a library they don't have control over. I.e. they can't modify or extend `XTrainer` or `loss_fn`. ```py  class XTrainer:   loss_fn: Callable[[Prediction, BatchType], LossValue]   (pjit, ...)   def train_step(self, input_batch: BatchType, train_state: TrainState) > TrainState:     """"""Performs a single gradient descent step on the model represented with `train_state`.        Runs a prediction in the model and computes the value of the loss by calling `loss_fn`.     """"""     ... ``` There are cases in which one would want to exercise a static pretrained model inside the implementation of `loss_fn`. Some that come to mind are: 1. One wants to train from human feedback as in InstructGPT. 2. One wants to distill the trained model to be close to a target, pretrained model. Unfortunately, if the static model needs to be sharded over multiple hosts, this is currently not supported and AFAIK, one can't use `XTrainer` without modification in this case.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,Allow pjit to close over values sharded on multiple devices,"Consider the following code fragment: ```py x = load_checkpoint() (pjit, in_axis_resources=in_sharding, out_axis_resources=out_sharding): def foo(a)   return a + x y = load_another_checkpoint() z = foo(y) ``` The call to `foo(y)` fails if the value of `x` is sharded over multiple hosts. One scenario when supporting this would be very useful is if one need to close over a large static model in order to adapt to the interface required by an existing library they use. E.g. assume one wants to use the `XTraner` class below, which is a part of a library they don't have control over. I.e. they can't modify or extend `XTrainer` or `loss_fn`. ```py  class XTrainer:   loss_fn: Callable[[Prediction, BatchType], LossValue]   (pjit, ...)   def train_step(self, input_batch: BatchType, train_state: TrainState) > TrainState:     """"""Performs a single gradient descent step on the model represented with `train_state`.        Runs a prediction in the model and computes the value of the loss by calling `loss_fn`.     """"""     ... ``` There are cases in which one would want to exercise a static pretrained model inside the implementation of `loss_fn`. Some that come to mind are: 1. One wants to train from human feedback as in InstructGPT. 2. One wants to distill the trained model to be close to a target, pretrained model. Unfortunately, if the static model needs to be sharded over multiple hosts, this is currently not supported and AFAIK, one can't use `XTrainer` without modification in this case.",2022-09-14T08:10:42Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/12354,"If I'm understanding correctly, this can actually be done with `jax.tree_util.Partial` https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.Partial.html","Yes, please use what  suggested or pass the sharded Array as an argument. Feel free to reopen if both techniques don't work."
2533,"以下是一个github上的jax下的一个issue, 标题是(Unexpected CPU memory allocation when running on GPU with torch)， 内容是 ( Description The first call to `jax.random.split` allocates ~3GB of RAM and keeps it until the process finishes.  the issue occurs when running the script with GPU support **and** with `torch` being imported  the issue does not occur when I force jax to use CPU **or** do not import `torch` Minimal example: ``` import os os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8' import jax import torch   if torch is not imported, no memleak is observed from memory_profiler import profile  if using CPU, no memleak is observed  jax.config.update('jax_platform_name', 'cpu')  def memleak():     key = jax.random.PRNGKey(220)      MEMLEAK: next line increases RAM consumption     key_1, key_2 = jax.random.split(key, 2) memleak() ``` which prints: ``` Line     Mem usage    Increment  Occurrences   Line Contents =============================================================     14    388.3 MiB    388.3 MiB           1        15                                         def memleak():     16    713.6 MiB    325.2 MiB           1       key = jax.random.PRNGKey(220)     17                                              MEMLEAK: next line increases RAM consumption     18   3754.8 MiB   3041.3 MiB           1       key_1, key_2 = jax.random.split(key, 2) ```  Dockerfile to replicate the env ``` FROM nvidia/cuda:11.4.3develubuntu18.04 ARG DIR=/root/jaxmemleak RUN mkdir $DIR WORKDIR $DIR RUN aptget update && aptget install y wget ARG CONDA=miniconda.sh RUN wget https://repo.anaconda.com/miniconda/Miniconda3py39_4.10.3Linuxx86_64.sh O $CONDA \     && bash $CONDA b p ~/opt/miniconda \     && rm $CONDA RUN eval ""$(~/opt/miniconda/bin/conda shell.bash hook)"" \     && conda init \     && conda create y n jax python=3.9 \     && conda activate jax \     && echo ""conda activate jax"" >> ~/.bashrc\     && conda install y pytorch==1.11.0 cudatoolkit=11.3 c pytorch \     && pip install ""jax[cuda11_cudnn82]==0.3.13"" \             f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html \         memory_profiler ENTRYPOINT [""/bin/bash""] ```  (run container with GPU forwarding: `gpus=all`)  What jax/jaxlib version are you using? jax v0.3.13, jaxlib v0.3.10+cuda11.cudnn82 (system wide cuda v11.4)  Which accelerator(s) are you using? GPU  Additional System Info Python 3.9.12, Ubuntu)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Unexpected CPU memory allocation when running on GPU with torch," Description The first call to `jax.random.split` allocates ~3GB of RAM and keeps it until the process finishes.  the issue occurs when running the script with GPU support **and** with `torch` being imported  the issue does not occur when I force jax to use CPU **or** do not import `torch` Minimal example: ``` import os os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.8' import jax import torch   if torch is not imported, no memleak is observed from memory_profiler import profile  if using CPU, no memleak is observed  jax.config.update('jax_platform_name', 'cpu')  def memleak():     key = jax.random.PRNGKey(220)      MEMLEAK: next line increases RAM consumption     key_1, key_2 = jax.random.split(key, 2) memleak() ``` which prints: ``` Line     Mem usage    Increment  Occurrences   Line Contents =============================================================     14    388.3 MiB    388.3 MiB           1        15                                         def memleak():     16    713.6 MiB    325.2 MiB           1       key = jax.random.PRNGKey(220)     17                                              MEMLEAK: next line increases RAM consumption     18   3754.8 MiB   3041.3 MiB           1       key_1, key_2 = jax.random.split(key, 2) ```  Dockerfile to replicate the env ``` FROM nvidia/cuda:11.4.3develubuntu18.04 ARG DIR=/root/jaxmemleak RUN mkdir $DIR WORKDIR $DIR RUN aptget update && aptget install y wget ARG CONDA=miniconda.sh RUN wget https://repo.anaconda.com/miniconda/Miniconda3py39_4.10.3Linuxx86_64.sh O $CONDA \     && bash $CONDA b p ~/opt/miniconda \     && rm $CONDA RUN eval ""$(~/opt/miniconda/bin/conda shell.bash hook)"" \     && conda init \     && conda create y n jax python=3.9 \     && conda activate jax \     && echo ""conda activate jax"" >> ~/.bashrc\     && conda install y pytorch==1.11.0 cudatoolkit=11.3 c pytorch \     && pip install ""jax[cuda11_cudnn82]==0.3.13"" \             f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html \         memory_profiler ENTRYPOINT [""/bin/bash""] ```  (run container with GPU forwarding: `gpus=all`)  What jax/jaxlib version are you using? jax v0.3.13, jaxlib v0.3.10+cuda11.cudnn82 (system wide cuda v11.4)  Which accelerator(s) are you using? GPU  Additional System Info Python 3.9.12, Ubuntu",2022-09-13T15:48:51Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/12340,That's working as intended. See https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html I'm guessing 3GB is 80% of your available GPU memory?,"> I'm guessing 3GB is 80% of your available GPU memory? No. The issue is with host memory, not GPU memory. For example, if I set `XLA_PYTHON_CLIENT_MEM_FRACTION = 0.2`, I get the same result. `sleep(*)` at the end + `htop` confirm that the memory profiler is sane.", are you up for taking a look at this?,"`jax.random.split` contains a custom call to `cuda_threefry2x32`. Cuda will initialize all linked kernels every time you dispatch a kernel (each of which require ram for loading). This means that cuda is picking up and loading all of the linked kernels. If we insert any pytorch calls before the jax.random.split call, the memory increase will be assigned there instead."
3270,"以下是一个github上的jax下的一个issue, 标题是(float0 should support addition, subtraction, and scalar multiplication)， 内容是 ( Description I have some modules whose parameters include discrete values (e.g. indices). After turning on `allow_int=True` in jax.grad to handle these, I get the following error: ``` TypeError: Called multiply with a float0 array. float0s do not support any operations by design because they are not compatible with nontrivial vector spaces. No implicit dtype conversion is done. You can use np.zeros_like(arr, dtype=np.float) to cast a float0 array to a regular zeros array.  If you didn't expect to get a float0 you might have accidentally taken a gradient with respect to an integer argument. ``` The problem comes from multiplication of the gradients of the discrete parameters (which happen to belong to the trivial vector space) with the learning rate. It is very inconvenient for float0 to not work like the other float types in this situation. It is also mathematically incorrect for it not to. The trivial vector space contains only a zero vector. It is perfectly legitimate to scale this zero vector by any scalar quantity: Doing so just happens to yield the zero vector again. Same goes for addition/subtraction: Adding/subtracting two zero vectors just yields the zero vector again. Furthermore, it would be convenient if one could add a float0 to a discrete value (simply yielding the latter), so that optimizers' parameter updates of the form `param_new = param_old + lr * grad` can work without modification. Mathematically, this is justified by the fact that the trivial vector space is a subspace of every vector space, and the trivial module (which it is equivalent to) is a submoduleSubmodules_and_homomorphisms) of every module. In short, a float0 should act like an absorber/annihilator under multiplication and an identity under addition. Example: ```python3 import jax def f(i):     return jax.numpy.array([.8, .9])[i].sum() fp = jax.grad(f, allow_int=True) i = jax.numpy.array([0, 0, 1]) fp_i = fp(i) print(fp_i.dtype) print(fp_i.shape) print() try:     print(fp_i * .3) except Exception as e:     print(e) print() try:     print(fp_i + fp_i) except Exception as e:     print(e) print() try:     print(i + fp_i) except Exception as e:     print(e) ``` Output: ``` [('float0', 'V')] (3,) ufunc 'multiply' did not contain a loop with signature matching types (dtype([('float0', 'V')]), dtype('float64')) > None ufunc 'add' did not contain a loop with signature matching types (dtype([('float0', 'V')]), dtype([('float0', 'V')])) > None Called add with a float0 array. float0s do not support any operations by design because they are not compatible with nontrivial vector spaces. No implicit dtype conversion is done. You can use np.zeros_like(arr, dtype=np.float) to cast a float0 array to a regular zeros array.  If you didn't expect to get a float0 you might have accidentally taken a gradient with respect to an integer argument. ```  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Python 3.10.5, macOS 11.6.8)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"float0 should support addition, subtraction, and scalar multiplication"," Description I have some modules whose parameters include discrete values (e.g. indices). After turning on `allow_int=True` in jax.grad to handle these, I get the following error: ``` TypeError: Called multiply with a float0 array. float0s do not support any operations by design because they are not compatible with nontrivial vector spaces. No implicit dtype conversion is done. You can use np.zeros_like(arr, dtype=np.float) to cast a float0 array to a regular zeros array.  If you didn't expect to get a float0 you might have accidentally taken a gradient with respect to an integer argument. ``` The problem comes from multiplication of the gradients of the discrete parameters (which happen to belong to the trivial vector space) with the learning rate. It is very inconvenient for float0 to not work like the other float types in this situation. It is also mathematically incorrect for it not to. The trivial vector space contains only a zero vector. It is perfectly legitimate to scale this zero vector by any scalar quantity: Doing so just happens to yield the zero vector again. Same goes for addition/subtraction: Adding/subtracting two zero vectors just yields the zero vector again. Furthermore, it would be convenient if one could add a float0 to a discrete value (simply yielding the latter), so that optimizers' parameter updates of the form `param_new = param_old + lr * grad` can work without modification. Mathematically, this is justified by the fact that the trivial vector space is a subspace of every vector space, and the trivial module (which it is equivalent to) is a submoduleSubmodules_and_homomorphisms) of every module. In short, a float0 should act like an absorber/annihilator under multiplication and an identity under addition. Example: ```python3 import jax def f(i):     return jax.numpy.array([.8, .9])[i].sum() fp = jax.grad(f, allow_int=True) i = jax.numpy.array([0, 0, 1]) fp_i = fp(i) print(fp_i.dtype) print(fp_i.shape) print() try:     print(fp_i * .3) except Exception as e:     print(e) print() try:     print(fp_i + fp_i) except Exception as e:     print(e) print() try:     print(i + fp_i) except Exception as e:     print(e) ``` Output: ``` [('float0', 'V')] (3,) ufunc 'multiply' did not contain a loop with signature matching types (dtype([('float0', 'V')]), dtype('float64')) > None ufunc 'add' did not contain a loop with signature matching types (dtype([('float0', 'V')]), dtype([('float0', 'V')])) > None Called add with a float0 array. float0s do not support any operations by design because they are not compatible with nontrivial vector spaces. No implicit dtype conversion is done. You can use np.zeros_like(arr, dtype=np.float) to cast a float0 array to a regular zeros array.  If you didn't expect to get a float0 you might have accidentally taken a gradient with respect to an integer argument. ```  What jax/jaxlib version are you using? jax v0.3.17, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Python 3.10.5, macOS 11.6.8",2022-09-13T04:31:10Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/12339,Thanks for the question! I'm assigning  because he knows some of the context of discussions around these kinds of ideas.,More concisely: Let `zero` be a value of float0 type and `any` be a value of any type. Then the following (and perhaps more) ought to hold: * `zero + any == any` * `any + zero == any` * `zero * any == zero` * `any * zero == zero` * `zero == zero` * `zero  any == zero + any == any` * `any  zero == any + zero == any` * `abs(zero) == zero`
766,"以下是一个github上的jax下的一个issue, 标题是(`jnp.squeeze` will fail unexpectedly with `jit` compilation)， 内容是 (`jnp.squeeze` will fail unexpectedly with `jit` compilation but direct invocation will succeed. For example, ```py import jax def fn(inp):     axis = jax.numpy.array(1, dtype=jax.numpy.int32)     return jax.numpy.squeeze(inp, axis) mykey = jax.random.PRNGKey(0) inp = jax.random.uniform(mykey, [3, 1], jax.numpy.float32, minval=0, maxval=1) print(fn(inp))  [0.9653214  0.31468165 0.63302994] jax.jit(fn)(inp)  TypeError: iteration over a 0d array ``` I think the input is valid since the dim`1` has shape 1.  Version jax 0.3.17)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jnp.squeeze` will fail unexpectedly with `jit` compilation,"`jnp.squeeze` will fail unexpectedly with `jit` compilation but direct invocation will succeed. For example, ```py import jax def fn(inp):     axis = jax.numpy.array(1, dtype=jax.numpy.int32)     return jax.numpy.squeeze(inp, axis) mykey = jax.random.PRNGKey(0) inp = jax.random.uniform(mykey, [3, 1], jax.numpy.float32, minval=0, maxval=1) print(fn(inp))  [0.9653214  0.31468165 0.63302994] jax.jit(fn)(inp)  TypeError: iteration over a 0d array ``` I think the input is valid since the dim`1` has shape 1.  Version jax 0.3.17",2022-09-12T18:40:36Z,better_errors,closed,0,2,https://github.com/jax-ml/jax/issues/12331,"BTW, `jnp.expand_dims` has a similar issue ``` import jax def fn(inp):     axis = jax.numpy.array(0, dtype=jax.numpy.int64)     return jax.numpy.expand_dims(inp, axis) inp = jax.numpy.array(128, dtype=jax.numpy.float32) print(fn(inp))  [128.] jax.jit(fn)(inp)  TypeError: iteration over a 0d array ```","The error is misleading here, but I believe the issue is that in both cases the `axis` argument must be static, and when you pass a jax array to the function within `jit` it is no longer static. We should update the `_ensure_index_tuple` helper function to return a more suitable error in this case."
1700,"以下是一个github上的jax下的一个issue, 标题是(Possible bug in `jax.lax.index_take`)， 内容是 (The `jax.lax.index_take` will fail when `axes` is a list, for example, ```py import jax def fn(src):     idxs = jax.numpy.array([[2, 2], [31, 0]], dtype=jax.numpy.int32)     axes = [0, 1]     return jax.lax.index_take(src, idxs, axes) genkey = jax.random.PRNGKey(90376501) src = jax.random.randint(genkey, [3, 4, 5], 32, 32, jax.numpy.int32) fn(src) ```  ``` File ~/torch1.12.1/lib/python3.9/sitepackages/jax/_src/lax/slicing.py:281, in gather(operand, start_indices, dimension_numbers, slice_sizes, unique_indices, indices_are_sorted, mode, fill_value)     279 else:     280   fill_value = None > 281 return gather_p.bind( ...     220   return f(*args, **kwargs)     221 else: > 222   return cached(config._trace_context(), *args, **kwargs) TypeError: unhashable type: 'list' ``` However, the document said `axes` is a sequence of integer. Thus, I think it should succeed with integer list.  Plus, its `jit` version can succeed with the same input ```py import jax def fn(src):     idxs = jax.numpy.array([[2, 2], [31, 0]], dtype=jax.numpy.int32)     axes = [0, 1]     return jax.lax.index_take(src, idxs, axes) genkey = jax.random.PRNGKey(90376501) src = jax.random.randint(genkey, [3, 4, 5], 32, 32, jax.numpy.int32) print(jax.jit(fn)(src)) ``` ``` [[17 28   3  16 31]  [ 14 28 21 18  17]] ``` but I think the indices here is not valid, `31` is obviously out of range, and jax just uses `31 % 4` as the index to take the elements. Maybe it needs a index check.  jax version: 0.3.17)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Possible bug in `jax.lax.index_take`,"The `jax.lax.index_take` will fail when `axes` is a list, for example, ```py import jax def fn(src):     idxs = jax.numpy.array([[2, 2], [31, 0]], dtype=jax.numpy.int32)     axes = [0, 1]     return jax.lax.index_take(src, idxs, axes) genkey = jax.random.PRNGKey(90376501) src = jax.random.randint(genkey, [3, 4, 5], 32, 32, jax.numpy.int32) fn(src) ```  ``` File ~/torch1.12.1/lib/python3.9/sitepackages/jax/_src/lax/slicing.py:281, in gather(operand, start_indices, dimension_numbers, slice_sizes, unique_indices, indices_are_sorted, mode, fill_value)     279 else:     280   fill_value = None > 281 return gather_p.bind( ...     220   return f(*args, **kwargs)     221 else: > 222   return cached(config._trace_context(), *args, **kwargs) TypeError: unhashable type: 'list' ``` However, the document said `axes` is a sequence of integer. Thus, I think it should succeed with integer list.  Plus, its `jit` version can succeed with the same input ```py import jax def fn(src):     idxs = jax.numpy.array([[2, 2], [31, 0]], dtype=jax.numpy.int32)     axes = [0, 1]     return jax.lax.index_take(src, idxs, axes) genkey = jax.random.PRNGKey(90376501) src = jax.random.randint(genkey, [3, 4, 5], 32, 32, jax.numpy.int32) print(jax.jit(fn)(src)) ``` ``` [[17 28   3  16 31]  [ 14 28 21 18  17]] ``` but I think the indices here is not valid, `31` is obviously out of range, and jax just uses `31 % 4` as the index to take the elements. Maybe it needs a index check.  jax version: 0.3.17",2022-09-10T18:35:48Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/12321,Thanks  we should be able to fix this.
653,"以下是一个github上的jax下的一个issue, 标题是(Minimally support `device` argument on `jit` in the `jax.Array` path)， 内容是 (Minimally support `device` argument on `jit` in the `jax.Array` path This means that only a single device is allowed to flow through this path. This is a compromise i.e. it will support the existing codepaths but won't support sharded arrays to go through this path and encourage users to use other well supported techniques like using device_put explicitly instead of relying on `jit` to do that for you.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Minimally support `device` argument on `jit` in the `jax.Array` path,Minimally support `device` argument on `jit` in the `jax.Array` path This means that only a single device is allowed to flow through this path. This is a compromise i.e. it will support the existing codepaths but won't support sharded arrays to go through this path and encourage users to use other well supported techniques like using device_put explicitly instead of relying on `jit` to do that for you.,2022-09-09T22:49:57Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12316
2735,"以下是一个github上的jax下的一个issue, 标题是(jax[cuda] installation replaces current jax version with old jax-0.2.22 version)， 内容是 ( Description When I run ```pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html``` on my laptop, the latest version of jax, currently jax0.3.17, gets replaced by jax0.2.22. I don't think this was happening before. Here is a representative output of the command above: ``` Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda]   Using cached jax0.3.17.tar.gz (1.1 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.16.tar.gz (1.0 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.15.tar.gz (1.0 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.14.tar.gz (990 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.13.tar.gz (951 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.12.tar.gz (947 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.11.tar.gz (947 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.10.tar.gz (939 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.9.tar.gz (937 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.8.tar.gz (935 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.7.tar.gz (944 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.6.tar.gz (936 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.5.tar.gz (946 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.4.tar.gz (924 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.3.tar.gz (924 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.2.tar.gz (926 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.1.tar.gz (912 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.0.tar.gz (896 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.28.tar.gz (887 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.27.tar.gz (873 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.26.tar.gz (850 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.25.tar.gz (786 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.24.tar.gz (786 kB)   Preparing metadata (setup.py) ... done WARNING: jax 0.2.22 does not provide the extra 'cuda' ```  What jax/jaxlib version are you using? jax0.3.17  Which accelerator(s) are you using? GPU  Additional System Info MAC)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jax[cuda] installation replaces current jax version with old jax-0.2.22 version," Description When I run ```pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html``` on my laptop, the latest version of jax, currently jax0.3.17, gets replaced by jax0.2.22. I don't think this was happening before. Here is a representative output of the command above: ``` Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda]   Using cached jax0.3.17.tar.gz (1.1 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.16.tar.gz (1.0 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.15.tar.gz (1.0 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.14.tar.gz (990 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.13.tar.gz (951 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.12.tar.gz (947 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.11.tar.gz (947 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.10.tar.gz (939 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.9.tar.gz (937 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.8.tar.gz (935 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.7.tar.gz (944 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.6.tar.gz (936 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.5.tar.gz (946 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.4.tar.gz (924 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.3.tar.gz (924 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.2.tar.gz (926 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.1.tar.gz (912 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.0.tar.gz (896 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.28.tar.gz (887 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.27.tar.gz (873 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.26.tar.gz (850 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.25.tar.gz (786 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.24.tar.gz (786 kB)   Preparing metadata (setup.py) ... done WARNING: jax 0.2.22 does not provide the extra 'cuda' ```  What jax/jaxlib version are you using? jax0.3.17  Which accelerator(s) are you using? GPU  Additional System Info MAC",2022-09-09T12:47:14Z,bug,closed,0,15,https://github.com/jax-ml/jax/issues/12307,~The URL should be `https://storage.googleapis.com/jaxreleases/jax_releases.html`~ ~ Why is `jax_cuda_releases.html` even still present? Wouldn't it be better to delete it to prevent any problems like this?~,Actually I'm wrong! You had the right URL the first time. I think the index was in a broken state. Try now?  Is it possible the index file doesn't get updated atomically?, still getting the same output!,"My guess is that's related to some sort of caching of the index and it will fix itself soon. If you open it up in a web browser, do you see: ``` cuda11/jaxlib0.3.15+cuda11.cudnn805cp310nonemanylinux2014_x86_64.whl cuda11/jaxlib0.3.15+cuda11.cudnn805cp37nonemanylinux2014_x86_64.whl cuda11/jaxlib0.3.15+cuda11.cudnn805cp38nonemanylinux2014_x86_64.whl cuda11/jaxlib0.3.15+cuda11.cudnn805cp39nonemanylinux2014_x86_64.whl cuda11/jaxlib0.3.15+cuda11.cudnn82cp310nonemanylinux2014_x86_64.whl cuda11/jaxlib0.3.15+cuda11.cudnn82cp37nonemanylinux2014_x86_64.whl cuda11/jaxlib0.3.15+cuda11.cudnn82cp38nonemanylinux2014_x86_64.whl cuda11/jaxlib0.3.15+cuda11.cudnn82cp39nonemanylinux2014_x86_64.whl ``` in the list? You can always download the necessary wheel manually.","Yes, they are all there.","Oh! Wait! You are on a Mac. We don't support CUDA on Mac. So there's no matching wheel found. You should install the CPU wheels on Mac (i.e., just `pip install jaxlib`).","I'm seeing the same issue on Ubuntu as well. Most recently is the behavior below on a new Jetson Orin Dev Kit (so it is Linux for Tortuga) ``` pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Defaulting to user installation because normal sitepackages is not writeable Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Requirement already satisfied: jax[cuda] in ./.local/lib/python3.8/sitepackages (0.3.23) Requirement already satisfied: etils[epath] in ./.local/lib/python3.8/sitepackages (from jax[cuda]) (0.8.0) Requirement already satisfied: abslpy in ./.local/lib/python3.8/sitepackages (from jax[cuda]) (1.3.0) Requirement already satisfied: scipy>=1.5 in ./.local/lib/python3.8/sitepackages (from jax[cuda]) (1.9.3) Requirement already satisfied: numpy>=1.20 in ./.local/lib/python3.8/sitepackages (from jax[cuda]) (1.23.4) Requirement already satisfied: opteinsum in ./.local/lib/python3.8/sitepackages (from jax[cuda]) (3.3.0) Requirement already satisfied: typingextensions in ./.local/lib/python3.8/sitepackages (from jax[cuda]) (4.4.0) Collecting jax[cuda]   Using cached jax0.3.23py3noneany.whl   Using cached jax0.3.22.tar.gz (1.1 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.21.tar.gz (1.1 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.20.tar.gz (1.1 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.19.tar.gz (1.1 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.17.tar.gz (1.1 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.16.tar.gz (1.0 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.15.tar.gz (1.0 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.14.tar.gz (990 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.13.tar.gz (951 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.12.tar.gz (947 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.11.tar.gz (947 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.10.tar.gz (939 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.9.tar.gz (937 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.8.tar.gz (935 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.7.tar.gz (944 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.6.tar.gz (936 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.5.tar.gz (946 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.4.tar.gz (924 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.3.tar.gz (924 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.2.tar.gz (926 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.1.tar.gz (912 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.3.0.tar.gz (896 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.28.tar.gz (887 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.27.tar.gz (873 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.26.tar.gz (850 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.25.tar.gz (786 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.24.tar.gz (786 kB)   Preparing metadata (setup.py) ... done   Using cached jax0.2.22py3noneany.whl WARNING: jax 0.2.22 does not provide the extra 'cuda' Installing collected packages: jax   Attempting uninstall: jax     Found existing installation: jax 0.3.23     Uninstalling jax0.3.23:       Successfully uninstalled jax0.3.23 Successfully installed jax0.2.22 ``` But if I instead provide my versions of cuda (11.4) and cuddnn (8.3.2). I get the expected behavior: ``` pip install upgrade ""jax[cuda114_cudnn832]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Defaulting to user installation because normal sitepackages is not writeable Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Requirement already satisfied: jax[cuda114_cudnn832] in ./.local/lib/python3.8/sitepackages (0.2.22) Collecting jax[cuda114_cudnn832]   Using cached jax0.3.23py3noneany.whl WARNING: jax 0.3.23 does not provide the extra 'cuda114_cudnn832' Requirement already satisfied: scipy>=1.5 in ./.local/lib/python3.8/sitepackages (from jax[cuda114_cudnn832]) (1.9.3) Requirement already satisfied: typingextensions in ./.local/lib/python3.8/sitepackages (from jax[cuda114_cudnn832]) (4.4.0) Requirement already satisfied: abslpy in ./.local/lib/python3.8/sitepackages (from jax[cuda114_cudnn832]) (1.3.0) Requirement already satisfied: opteinsum in ./.local/lib/python3.8/sitepackages (from jax[cuda114_cudnn832]) (3.3.0) Requirement already satisfied: numpy>=1.20 in ./.local/lib/python3.8/sitepackages (from jax[cuda114_cudnn832]) (1.23.4) Requirement already satisfied: etils[epath] in ./.local/lib/python3.8/sitepackages (from jax[cuda114_cudnn832]) (0.8.0) Requirement already satisfied: importlib_resources in ./.local/lib/python3.8/sitepackages (from etils[epath]>jax[cuda114_cudnn832]) (5.10.0) Requirement already satisfied: zipp in ./.local/lib/python3.8/sitepackages (from etils[epath]>jax[cuda114_cudnn832]) (3.10.0) Installing collected packages: jax   Attempting uninstall: jax     Found existing installation: jax 0.2.22     Uninstalling jax0.2.22:       Successfully uninstalled jax0.2.22 Successfully installed jax0.3.23 ``` If I just just `pip install upgrade jax` I get the same result ``` pip install upgrade jax Defaulting to user installation because normal sitepackages is not writeable Requirement already satisfied: jax in ./.local/lib/python3.8/sitepackages (0.2.22) Collecting jax   Using cached jax0.3.23py3noneany.whl Requirement already satisfied: abslpy in ./.local/lib/python3.8/sitepackages (from jax) (1.3.0) Requirement already satisfied: opteinsum in ./.local/lib/python3.8/sitepackages (from jax) (3.3.0) Requirement already satisfied: etils[epath] in ./.local/lib/python3.8/sitepackages (from jax) (0.8.0) Requirement already satisfied: typingextensions in ./.local/lib/python3.8/sitepackages (from jax) (4.4.0) Requirement already satisfied: scipy>=1.5 in ./.local/lib/python3.8/sitepackages (from jax) (1.9.3) Requirement already satisfied: numpy>=1.20 in ./.local/lib/python3.8/sitepackages (from jax) (1.23.4) Requirement already satisfied: zipp in ./.local/lib/python3.8/sitepackages (from etils[epath]>jax) (3.10.0) Requirement already satisfied: importlib_resources in ./.local/lib/python3.8/sitepackages (from etils[epath]>jax) (5.10.0) Installing collected packages: jax   Attempting uninstall: jax     Found existing installation: jax 0.2.22     Uninstalling jax0.2.22:       Successfully uninstalled jax0.2.22 Successfully installed jax0.3.23 ``` Is this just that `pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` grabs the version of jax with the least restrictive requirements? It seems in either case the extra I provide isn't recognized, but with ""[cuda]"" it defaults to an old version, and with ""[cuda114_cudnn832]"" or no extra at all it defaults to the newest install.",  What version of Python are you using?,"Also, what happens if you run this? ``` $ pip install jaxlib==0.3.22+cuda11.cudnn82 f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```",Thanks for replying so fast! I'm using python 3.8.10. (And pip is 22.3) Running that command gives me: ``` pip install jaxlib==0.3.22+cuda11.cudnn82 f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Defaulting to user installation because normal sitepackages is not writeable Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ERROR: Could not find a version that satisfies the requirement jaxlib==0.3.22+cuda11.cudnn82 (from versions: none) ERROR: No matching distribution found for jaxlib==0.3.22+cuda11.cudnn82 ```,That wheel is in the index and the command works on Colab. So there must be some quirk in your own system. A couple guesses:  perhaps you have a firewall that's blocking the jax releases URL?  perhaps your flavor of linux is not compatible with `manylinux2010` wheels?  perhaps your system is not compatible with `x86_64` wheels?,"Thank you, that might be the issue. When I try to just pip install jax, it does install jax 0.3.23, but not jaxlib along side it. I looks like from CC(Provide AArch64 (ARM) Linux jaxlib wheels) jax isn't compatible with ARM CPUs? The Nvidia Jetson Orin uses an Arm Cortex CPU. I'm guessing that is why above it says you don't support mac?  Maybe this should be a separate issue or mentioned on CC(Provide AArch64 (ARM) Linux jaxlib wheels), but I didn't see that jax wheels are only available of x86_64 architectures until digging into the issues, is that something that makes sense to add to the installation guide? ","Yes, if the steps here didn't help with your problem, we should add a warning there about the issue you ran into. I'm not sure about the status of jaxlib wheel releases with ARM CPUs, but it sound like that may be the culprit.","Thank you for your help with this, I think that is the main issue. I found CC(Doesn't compile on Nvidia Jetson Nano) about compiling jax for an older version of the Jetson series, so I'll try following that. The steps you linked were what I was following when I ran into this, I think adding a warning about ARM CPUs would fix the confusion. I can try submitting a pull request on that if you would like.","Yes, a PR would be much appreciated. Thanks!"
372,"以下是一个github上的jax下的一个issue, 标题是([Rollback] Add a github presubmit build which runs with jax.Array flag enabled for OSS coverage.)， 内容是 ([Rollback] Add a github presubmit build which runs with jax.Array flag enabled for OSS coverage.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[Rollback] Add a github presubmit build which runs with jax.Array flag enabled for OSS coverage.,[Rollback] Add a github presubmit build which runs with jax.Array flag enabled for OSS coverage.,2022-09-09T05:06:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12304
581,"以下是一个github上的jax下的一个issue, 标题是(Minimally support the `backend` argument on `jit` in the jax.Array path.)， 内容是 (Minimally support the `backend` argument on `jit` in the jax.Array path. This means that only a single device is allowed to flow through this path. This is a compromise i.e. it will support the existing codepaths but won't support sharded arrays to go through this path and encourage users to use other well supported techniques.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Minimally support the `backend` argument on `jit` in the jax.Array path.,Minimally support the `backend` argument on `jit` in the jax.Array path. This means that only a single device is allowed to flow through this path. This is a compromise i.e. it will support the existing codepaths but won't support sharded arrays to go through this path and encourage users to use other well supported techniques.,2022-09-09T03:16:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12303
350,"以下是一个github上的jax下的一个issue, 标题是(Add a github presubmit build which runs with jax.Array flag enabled for OSS coverage.)， 内容是 (Add a github presubmit build which runs with jax.Array flag enabled for OSS coverage.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add a github presubmit build which runs with jax.Array flag enabled for OSS coverage.,Add a github presubmit build which runs with jax.Array flag enabled for OSS coverage.,2022-09-08T21:49:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12299
485,"以下是一个github上的jax下的一个issue, 标题是(- Wraps calls to lax.xeinsum and _einsum in a named call with their 'spec', the string specifying the computation. Makes xprof traces more interpretable.)， 内容是 ( Wraps calls to lax.xeinsum and _einsum in a named call with their 'spec', the string specifying the computation. Makes xprof traces more interpretable.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"- Wraps calls to lax.xeinsum and _einsum in a named call with their 'spec', the string specifying the computation. Makes xprof traces more interpretable."," Wraps calls to lax.xeinsum and _einsum in a named call with their 'spec', the string specifying the computation. Makes xprof traces more interpretable.",2022-09-08T12:43:52Z,,closed,0,1,https://github.com/jax-ml/jax/issues/12276,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
350,"以下是一个github上的jax下的一个issue, 标题是(Implemented a squared ReLU activation function.)， 内容是 (This is a primitive operation used in https://arxiv.org/abs/2109.08668 to enable more efficient Transformer model training.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Implemented a squared ReLU activation function.,This is a primitive operation used in https://arxiv.org/abs/2109.08668 to enable more efficient Transformer model training.,2022-09-08T11:55:09Z,,closed,0,1,https://github.com/jax-ml/jax/issues/12274,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
980,"以下是一个github上的jax下的一个issue, 标题是(jax.Array: support duck-typed isinstance checks)， 内容是 (Why? As part of the type promotion discussion in JAX ( CC(JEP: Type Annotations) & CC(Tracking Issue: JAX Type Annotations)), kidger and I have been discussing what API jax should support for both annotations and `isinstance` checks; we think the best approach would be to eventually make `jax.Array` unify all these purposes. This is a simple enhancement to `jax.Array` that allows it to be used in `isinstance` checks in traced functions, similar to how `isinstance(obj, jnp.ndarray)` works currently. , I'm curious whether you see any issues with adding a mechanism like this to the `Array` class? Do you think this idea of unifying type checking and instance checks around the eventual `jax.Array` object has merit? Do you have any hesitations here?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.Array: support duck-typed isinstance checks,"Why? As part of the type promotion discussion in JAX ( CC(JEP: Type Annotations) & CC(Tracking Issue: JAX Type Annotations)), kidger and I have been discussing what API jax should support for both annotations and `isinstance` checks; we think the best approach would be to eventually make `jax.Array` unify all these purposes. This is a simple enhancement to `jax.Array` that allows it to be used in `isinstance` checks in traced functions, similar to how `isinstance(obj, jnp.ndarray)` works currently. , I'm curious whether you see any issues with adding a mechanism like this to the `Array` class? Do you think this idea of unifying type checking and instance checks around the eventual `jax.Array` object has merit? Do you have any hesitations here?",2022-09-07T16:08:24Z,,closed,0,5,https://github.com/jax-ml/jax/issues/12256,Array is in the process of being lowering to C++. How does this affect that? (given that you are adding a metaclass to Array). Is it possible to keep the behavior of Array similar to DA in this case because DA is also in C++ and it doesn't have a metaclass on it right?,"`DeviceArray` does not behave this way, but we want `Array` to behave this way. If we lower `Array` to C++, we should add an equivalent metaclass in the C++ definition, so that this new test still passes. Does that sound reasonable? If not, it changes the plan we're landing on in https://github.com/google/jax/pull/11859 so it would be good to know now.",> we should add an equivalent metaclass in the C++ definition. Does that sound reasonable?  or  Is it okay if we add the metaclass to C++? I don't know if that is possible. I am fine with it existing in Python though.,I've found a couple examples of overriding metaclasses / `__instancecheck__` within pybind11; one example is in torch: https://github.com/pytorch/pytorch/blob/31ef8ddb8c4467f5b8698ef1eb9bb8bab7056855/torch/csrc/tensor/python_tensor.cppL149L177,Replaced by https://github.com/google/jax/pull/12300
1544,"以下是一个github上的jax下的一个issue, 标题是(Pickling a JAX array does not preserve its original device)， 内容是 ( Description  What I hope to achieve The JAX arrays are created in a subprocess, evoked by `multiprocessing.map`, on CPU. I hope that when the arrays are sent to the main process, they are still on the same device (i.e. CPU). However, after they are sent to the main process, they are placed on the default device (i.e. GPU) instead. ```python import jax import jax.numpy as np import multiprocessing def f():     jax.config.update('jax_platforms', 'cpu')     a = np.zeros((2,))     print(a.device())   TFRT_CPU_0     return a def main():     ctx = multiprocessing.get_context('spawn')     with ctx.Pool(1) as p:         a = p.starmap(f, ((),))[0]     print(a.device())   gpu:0, expected TFRT_CPU_0 if __name__ == '__main__':     main() ```  This also applies to the underlying pickle module ```python import jax import jax.numpy as np import pickle jax.config.update('jax_platforms', 'cpu') a = np.zeros((2,)) print(a.device())   TFRT_CPU_0 with open('1.dat', 'wb') as f:     pickle.dump(a, f) ``` ```python import pickle with open('1.dat', 'rb') as f:     a = pickle.load(f) print(a.device())   gpu:0, expected TFRT_CPU_0 ```  What jax/jaxlib version are you using? jax 0.3.17, jaxlib 0.3.15+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional System Info Python 3.10.6, Arch Linux x86_64)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Pickling a JAX array does not preserve its original device," Description  What I hope to achieve The JAX arrays are created in a subprocess, evoked by `multiprocessing.map`, on CPU. I hope that when the arrays are sent to the main process, they are still on the same device (i.e. CPU). However, after they are sent to the main process, they are placed on the default device (i.e. GPU) instead. ```python import jax import jax.numpy as np import multiprocessing def f():     jax.config.update('jax_platforms', 'cpu')     a = np.zeros((2,))     print(a.device())   TFRT_CPU_0     return a def main():     ctx = multiprocessing.get_context('spawn')     with ctx.Pool(1) as p:         a = p.starmap(f, ((),))[0]     print(a.device())   gpu:0, expected TFRT_CPU_0 if __name__ == '__main__':     main() ```  This also applies to the underlying pickle module ```python import jax import jax.numpy as np import pickle jax.config.update('jax_platforms', 'cpu') a = np.zeros((2,)) print(a.device())   TFRT_CPU_0 with open('1.dat', 'wb') as f:     pickle.dump(a, f) ``` ```python import pickle with open('1.dat', 'rb') as f:     a = pickle.load(f) print(a.device())   gpu:0, expected TFRT_CPU_0 ```  What jax/jaxlib version are you using? jax 0.3.17, jaxlib 0.3.15+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional System Info Python 3.10.6, Arch Linux x86_64",2022-09-07T13:21:14Z,bug,open,0,13,https://github.com/jax-ml/jax/issues/12253,"This is expected behavior; see the description from the CHANGELOG entry when `pickle` support was added: https://github.com/google/jax/blob/main/CHANGELOG.mdjax0314june272022 The issue is that pickling and unpickling need not happen in the same environment, and so attempting to preserve the original device can be problematic; see https://github.com/google/jax/pull/10659 for further discussion, and let us know if you have any suggestions.", Thank you! I agree that attempting to preserve the original device can be problematic. My suggestion is that we should give priority to implementing a `with` block to specify the target device. For example: in PyTorch we can do: ```python with torch.cuda.device(2):   specify default device    ...   create array on the default device ``` I remember that this issue was discussed in https://github.com/google/jax/issues/8879,"https://github.com/google/jax/pull/9118 may have added this context manager already, but I'm not sure. Checking with  ...","Yes, I believe doing something like `with jax.default_device(jax.devices(""cpu"")[0])` should work. Please let me know if it doesn't, I haven't tried it with pickle.","Unpickling of an array uses `jax.device_put` with no device argument, so I believe the default default device context manager should do the right thing.","To put a fine point on it, the analogue of code like this: ```python with torch.cuda.device(2):   specify default device    ...   create array on the default device ``` Is something like this: ```python devices = jax.devices() with jax.default_device(devices[2]):   ...  create or load (e.g. unpickle) array on default device ``` I think this issue is resolved, but please reopen if not!"," Thank you for the reply, but this issue is not solved. `jax.default_device` works when creating the array, but does not work when loading the array from pickle: ```python import jax import jax.numpy as np import multiprocessing def f():     jax.config.update('jax_platforms', 'cpu')     a = np.zeros((2,))     print(a.device())   TFRT_CPU_0     return a def main():     device_cpu = jax.devices('cpu')[0]     with jax.default_device(device_cpu):         a = np.zeros((2,))         print(a.device())   TFRT_CPU_0         ctx = multiprocessing.get_context('spawn')         with ctx.Pool(1) as p:             a = p.starmap(f, ((),))[0]         print(a.device())   gpu:0, expected TFRT_CPU_0 if __name__ == '__main__':     main() ```","I found the cause of the problem > Unpickling of an array uses jax.device_put with no device argument, so I believe the default default device context manager should do the right thing. But actually this does not work as expected: ```python import jax import numpy as onp device_cpu = jax.devices('cpu')[0] with jax.default_device(device_cpu):     a = onp.array([1., 2.])     b = jax.device_put(a)     print(b.device())   gpu:0, not TFRT_CPU_0 ```", Please reopen this,Reopening and assigning to  – it looks like `device_put` does not respect the default device context. Is this intended?,I think that is *not* intended. ( thanks for following up and for the pings. Sorry I didn't notice your replies until now!),"Sorry for prematurely closing this. I really should've added a test, and then I would've seen that it doesn't work! On CPU at least, it looks like `xla_extension.Client.buffer_from_pyval(x, None)` doesn't respect the default device...","Are there any updates on this? I'm wondering if I may be running into a related problem.. **TL;DR** recently updated jax, and code that used to run fineusing multiprocessing (forkserver) + jax (with default device / all arrays placed on CPU)is now crashing due to memory overload on GPU. Monitoring the GPU, I can see that it starts by allocating 10Mb per process, which over time balloons to 100400+, and ultimately crashes. Thinking this might be caused by pickle (underlying multiprocessing) not playing well with device placement EDIT: confirmed that arrays are being passed on CPU device, but are on GPU once inside a pickled function. fixed by moving the `jax.default_device` statement inside rather than outside the picked function. I wonder if this rises to the level of a ""Sharp Bit"" ? (and/or, ideally would be tallied somewhere on a list of best practices for interfacing with multiprocessing)"
325,"以下是一个github上的jax下的一个issue, 标题是(Support deserializing from non-chunked storage such as tensorstore.array.)， 内容是 (Support deserializing from nonchunked storage such as tensorstore.array.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Support deserializing from non-chunked storage such as tensorstore.array.,Support deserializing from nonchunked storage such as tensorstore.array.,2022-09-06T18:40:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12237
623,"以下是一个github上的jax下的一个issue, 标题是(Multiplying Nan by False give 0. instead of NaN)， 内容是 ( Description Multiplying a NaN by False in jax gives a 0. See example below: ```python nan = jnp.array(float('nan'), dtype=jnp.float32) print(nan) print(nan * jnp.array(0., dtype=jnp.float32)) print(nan * jnp.array(False, dtype=jnp.bool_)) ``` Output: ``` nan nan 0.0 ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional System Info Linux)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Multiplying Nan by False give 0. instead of NaN," Description Multiplying a NaN by False in jax gives a 0. See example below: ```python nan = jnp.array(float('nan'), dtype=jnp.float32) print(nan) print(nan * jnp.array(0., dtype=jnp.float32)) print(nan * jnp.array(False, dtype=jnp.bool_)) ``` Output: ``` nan nan 0.0 ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional System Info Linux",2022-09-06T14:24:19Z,bug XLA,open,1,5,https://github.com/jax-ml/jax/issues/12233,"Update: if you replace float32 by bfloat16 in the above code, the multiplication by False gives NaN as expected.",Thanks for the report! This is really peculiar... I'm looking into it now.,"This only occurs within JIT, so I suspect this is coming from some XLA simplification where it replaces multiplication by zero with zero: ```python import jax.numpy as jnp import jax def f(x, y):   This is basically what `jnp.mul` does internally:   y = jax.lax.convert_element_type(y, float)   return jax.lax.mul(x, y) print(jax.make_jaxpr(f)(jnp.nan, False))  { lambda ; a:f32[] b:bool[]. let      c:f32[] = convert_element_type[new_dtype=float32 weak_type=False] b      d:f32[] = mul a c    in (d,) } print(f(jnp.nan, False))  nan print(jax.jit(f)(jnp.nan, False))  0.0 ``` However, the thing I don't understand is that if you replace `False` with `0` or `0.0`, the result correctly returns `nan`.","Here's the difference between how XLA treats integer vs boolean zero. In the case of multiplication with a constant boolean, XLA replaces the mutiplication by a `select` statement, which is correct for every finite floating point value, but will return the wrong value for `nan` and `inf`. ```python print(jax.jit(f).lower(jnp.nan, False).compile().as_text()) ``` ``` HloModule jit_f.3, entry_computation_layout={(f32[],pred[])>f32[]} ENTRY %main.5 (Arg_0.1: f32[], Arg_1.2: pred[]) > f32[] {   %Arg_1.2 = pred[] parameter(1)   %Arg_0.1 = f32[] parameter(0)   %constant.1 = f32[] constant(0)   ROOT %select = f32[] select(pred[] %Arg_1.2, f32[] %Arg_0.1, f32[] %constant.1), metadata={op_name=""jit(f)/jit(main)/mul"" source_file=""tmp.py"" source_line=6} } ``` For integer or float input, the compiled code actually uses multiplication, and so it returns the expected result: ```python print(jax.jit(f).lower(jnp.nan, 0).compile().as_text()) ``` ``` HloModule jit_f.4, entry_computation_layout={(f32[],s32[])>f32[]} %fused_computation (param_0: f32[], param_1.1: s32[]) > f32[] {   %param_0 = f32[] parameter(0)   %param_1.1 = s32[] parameter(1)   %convert.0 = f32[] convert(s32[] %param_1.1), metadata={op_name=""jit(f)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]"" source_file=""tmp.py"" source_line=5}   ROOT %multiply.0 = f32[] multiply(f32[] %param_0, f32[] %convert.0), metadata={op_name=""jit(f)/jit(main)/mul"" source_file=""tmp.py"" source_line=6} } ENTRY %main.5 (Arg_0.1: f32[], Arg_1.2: s32[]) > f32[] {   %Arg_0.1 = f32[] parameter(0)   %Arg_1.2 = s32[] parameter(1)   ROOT %fusion = f32[] fusion(f32[] %Arg_0.1, s32[] %Arg_1.2), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit(f)/jit(main)/mul"" source_file=""tmp.py"" source_line=6} } ```",Internal tracking: b/245348010
505,"以下是一个github上的jax下的一个issue, 标题是(Introduce class PyArray that contains the data members of python Array.)， 内容是 (Introduce class PyArray that contains the data members of python Array. A few key methods is implemented in C++ while the rest are still implmemented in python and added to the class later. A class decorator, , is added to add python methods to xc.Array.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Introduce class PyArray that contains the data members of python Array.,"Introduce class PyArray that contains the data members of python Array. A few key methods is implemented in C++ while the rest are still implmemented in python and added to the class later. A class decorator, , is added to add python methods to xc.Array.",2022-09-02T23:05:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12220
855,"以下是一个github上的jax下的一个issue, 标题是(Dce and execute trivial)， 内容是 (To achieve `jit == pjit` nirvana, we must add trivial jaxpr support to the pxla path, e.g. `lower_sharding_computation` and dispatch. So we added an `_execute_trivial` to pxla.py and the machinery/bookkeeping needed to call it. We also took the opportunity to make some longimprovements to the trivial dispatch path, namely using `pe.dce_jaxpr` to avoid building trivial XLA computations in more cases. That meant we were applying DCE much more broadly than before, and that turned up a couple DCE bugs (e.g. one in pmap's DCE rule, and one to do with infeed/outfeed needing to be considered effectful according to the current tests in infeed_test.py).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Dce and execute trivial,"To achieve `jit == pjit` nirvana, we must add trivial jaxpr support to the pxla path, e.g. `lower_sharding_computation` and dispatch. So we added an `_execute_trivial` to pxla.py and the machinery/bookkeeping needed to call it. We also took the opportunity to make some longimprovements to the trivial dispatch path, namely using `pe.dce_jaxpr` to avoid building trivial XLA computations in more cases. That meant we were applying DCE much more broadly than before, and that turned up a couple DCE bugs (e.g. one in pmap's DCE rule, and one to do with infeed/outfeed needing to be considered effectful according to the current tests in infeed_test.py).",2022-09-02T18:59:40Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/12217
2953,"以下是一个github上的jax下的一个issue, 标题是(slice-based indexing is slow for repeated indexing)， 内容是 (This issue is the result of some pairdebugging with . In the course of trying to reland CC(generate lax.slice instead of lax.gather for more indexing cases), I found that it was leading to some test timeouts due to replacing `lax.gather` with `lax.slice`. This issue actually can be reproduced in the current release of JAX; given the current implementation, we can define two functions, one of which takes the `gather` path and one of which takes the `slice` path: ```python import jax.numpy as jnp import jax def index_via_gather(x):   return x[(0,)] def index_via_slice(x):   return x[0] x = jnp.arange(100) print(jax.make_jaxpr(index_via_gather)(x))  { lambda ; a:i32[100]. let      b:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0      c:i32[] = gather[        dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))        fill_value=None        indices_are_sorted=True        mode=GatherScatterMode.PROMISE_IN_BOUNDS        slice_sizes=(1,)        unique_indices=True      ] a b    in (c,) } print(jax.make_jaxpr(index_via_slice)(x))  { lambda ; a:i32[100]. let      b:i32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] a      c:i32[] = squeeze[dimensions=(0,)] b    in (c,) } ``` The latter slice behavior was added in CC(generate lax.slice instead of lax.gather for x[] or x[:]) because it can be more efficient. Unfortunately, there is a catch: when indexing is done repeatedly on different static indices, the `slice` approach has a cache miss on each new index, making it much slower. The test that led to the rollback of CC(generate lax.slice instead of lax.gather for more indexing cases) exposed this issue, which we can reproduce for the current implementation like this: ```python def f_gather(x):   return jnp.array([x[(i,)] for i in range(len(x))]) def f_slice(x):   return jnp.array([x[i] for i in range(len(x))]) x = jnp.arange(500) %timeit n 1 r 1 f_gather(x).block_until_ready()  611 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) %timeit n 1 r 1 f_slice(x).block_until_ready()  6.78 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) ``` Repeated indexing via static slice is *very* slow comparted to repeated indexing via gather. We could instead consider using `lax.dynamic_slice`, which has comparable performance to `gather` for this repeated operation: ```python def f_dynamic_slice(x):   return jnp.array([jax.lax.dynamic_index_in_dim(x, i, 0) for i in range(len(x))]) %timeit n 1 r 1 f_dynamic_slice(x).block_until_ready()  420 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) ``` Any thoughts? cc/ , who contributed CC(generate lax.slice instead of lax.gather for x[] or x[:]))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,slice-based indexing is slow for repeated indexing,"This issue is the result of some pairdebugging with . In the course of trying to reland CC(generate lax.slice instead of lax.gather for more indexing cases), I found that it was leading to some test timeouts due to replacing `lax.gather` with `lax.slice`. This issue actually can be reproduced in the current release of JAX; given the current implementation, we can define two functions, one of which takes the `gather` path and one of which takes the `slice` path: ```python import jax.numpy as jnp import jax def index_via_gather(x):   return x[(0,)] def index_via_slice(x):   return x[0] x = jnp.arange(100) print(jax.make_jaxpr(index_via_gather)(x))  { lambda ; a:i32[100]. let      b:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0      c:i32[] = gather[        dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))        fill_value=None        indices_are_sorted=True        mode=GatherScatterMode.PROMISE_IN_BOUNDS        slice_sizes=(1,)        unique_indices=True      ] a b    in (c,) } print(jax.make_jaxpr(index_via_slice)(x))  { lambda ; a:i32[100]. let      b:i32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] a      c:i32[] = squeeze[dimensions=(0,)] b    in (c,) } ``` The latter slice behavior was added in CC(generate lax.slice instead of lax.gather for x[] or x[:]) because it can be more efficient. Unfortunately, there is a catch: when indexing is done repeatedly on different static indices, the `slice` approach has a cache miss on each new index, making it much slower. The test that led to the rollback of CC(generate lax.slice instead of lax.gather for more indexing cases) exposed this issue, which we can reproduce for the current implementation like this: ```python def f_gather(x):   return jnp.array([x[(i,)] for i in range(len(x))]) def f_slice(x):   return jnp.array([x[i] for i in range(len(x))]) x = jnp.arange(500) %timeit n 1 r 1 f_gather(x).block_until_ready()  611 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) %timeit n 1 r 1 f_slice(x).block_until_ready()  6.78 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) ``` Repeated indexing via static slice is *very* slow comparted to repeated indexing via gather. We could instead consider using `lax.dynamic_slice`, which has comparable performance to `gather` for this repeated operation: ```python def f_dynamic_slice(x):   return jnp.array([jax.lax.dynamic_index_in_dim(x, i, 0) for i in range(len(x))]) %timeit n 1 r 1 f_dynamic_slice(x).block_until_ready()  420 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) ``` Any thoughts? cc/ , who contributed CC(generate lax.slice instead of lax.gather for x[] or x[:])",2022-09-01T16:20:44Z,bug performance,closed,2,1,https://github.com/jax-ml/jax/issues/12198,Chatted offline with ; we agreed that switching from `slice` to `dynamic_slice` is probably the best option here.
1330,"以下是一个github上的jax下的一个issue, 标题是(jax.jnp gives NaN values)， 内容是 ( Description I'm trying to calculate average evaluation loss of ~39K values, but it seems that the summation operation produces NaN value. Here is the snippet that causes this: eval_metrics = [] ``` for i, batch_idx in enumerate(tqdm(eval_batch_idx, desc=""Evaluating ..."", position=2)): 	 Model forward     metrics = pad_shard_unpad(p_eval_step, static_return=True)(         state.params, model_inputs.data, min_device_batch=per_device_eval_batch_size     )      jax.debug.print('Loss_2: {}', metrics['loss'])      jax.debug.print('normalizer: {}', metrics['normalizer'])     eval_metrics.append(metrics) eval_metrics = get_metrics(eval_metrics) eval_metrics = jax.tree_map(jnp.sum, eval_metrics) eval_metrics = jax.tree_map(lambda x: x / eval_normalizer, eval_metrics) ``` `get_metrics(eval_metrics)` returns arrrays of values (39K values), and `jax.tree_map(jnp.sum, eval_metrics)` sums these values (the root of the problem), producing NaN value for the arrays whose summation overflows.   What jax/jaxlib version are you using? jax 0.3.16  Which accelerator(s) are you using? GPU  Additional System Info Linux Ubuntu 18.04)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.jnp gives NaN values," Description I'm trying to calculate average evaluation loss of ~39K values, but it seems that the summation operation produces NaN value. Here is the snippet that causes this: eval_metrics = [] ``` for i, batch_idx in enumerate(tqdm(eval_batch_idx, desc=""Evaluating ..."", position=2)): 	 Model forward     metrics = pad_shard_unpad(p_eval_step, static_return=True)(         state.params, model_inputs.data, min_device_batch=per_device_eval_batch_size     )      jax.debug.print('Loss_2: {}', metrics['loss'])      jax.debug.print('normalizer: {}', metrics['normalizer'])     eval_metrics.append(metrics) eval_metrics = get_metrics(eval_metrics) eval_metrics = jax.tree_map(jnp.sum, eval_metrics) eval_metrics = jax.tree_map(lambda x: x / eval_normalizer, eval_metrics) ``` `get_metrics(eval_metrics)` returns arrrays of values (39K values), and `jax.tree_map(jnp.sum, eval_metrics)` sums these values (the root of the problem), producing NaN value for the arrays whose summation overflows.   What jax/jaxlib version are you using? jax 0.3.16  Which accelerator(s) are you using? GPU  Additional System Info Linux Ubuntu 18.04",2022-08-31T19:11:52Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/12185,"It's difficult to understand what's happening without more information. Can you put together a complete minimal example? (i.e. including data and imports of the functions you're using?) Note: I'm not asking you to share your data, but you should be able to generate similar data using `np.random.rand` or similar that leads to the same result. For example, it wouild be nice to know if there are NaN values among the arrays passed to `jax.tree_map`.","Hi   The posted snippet is actually a part of the Flax on pertaining BART language model with Huggingface. I essentially try to use that code on my custom dataset, whose validation instances are about ~39K, so the returning jnp array has 39K values that should be normalized to display.  Actually I did a further debugging using `jax.debug.print()`. Here is the losses within the returned list:  eval_metrics[""loss""]= `[22339.582 22161.816 22221.883 ... 22303.713 23481.447       nan]` So it seems that the NaN is caused by a specific instance, that makes the who summation return NaN.  I debugged the program to find this instance, and noticed that one row of the input_ids within the eval_step function is essentially all zero, that makes the returned logits NaN. Here are the input_ids as well as the computed logits: ``` input_ids [[50264   179  4783 ...     1     1     1]  [ 9226   118 50264 ...     1     1     1]  [  700 50264  1794 ...     1     1     1]  [    0     0     0 ...     0     0     0]]   logits [[[ 8.235256    2.0338707   1.2944565  ...  7.842334    8.178107     8.643117  ]   [ 5.334233    3.0293756   0.13147672 ... 10.615038   10.16274     9.98302   ]   [ 6.7125335   2.661169     0.44605067 ... 10.435076   10.56175     8.552453  ]   ...   [ 4.102307    1.8319362    3.401112   ...  9.376341    9.284803     6.1490345 ]   [ 7.457472    3.5980809    1.4830123  ... 11.469392   11.51775    10.788591  ]   [ 5.4814677   2.5836527    1.274877   ... 11.137861   11.14509    11.144929  ]]  [[ 8.236677    2.0341225   1.2935951  ...  7.842687    8.178189     8.642431  ]   [ 6.3445244   2.9560397   0.50635713 ... 10.960117   11.018831    10.380886  ]   [ 4.027665    1.2919675    1.4371854  ...  7.0959826   6.739063     6.0127587 ]   ...   [ 4.9435964   2.7261846    0.84278244 ... 13.782334   14.252498    13.15108   ]   [ 2.3252742   3.1369848   0.84928894 ... 13.953517   14.834746    14.477739  ]   [ 4.905314    4.5334086   0.95402426 ... 11.740114   12.343864    11.699981  ]]  [[ 8.233315    2.03387     1.2937447  ...  7.843014    8.178542     8.64241   ]   [ 8.00472     2.0967667   0.24173795 ... 10.587084   10.671728     9.482687  ]   [ 3.4650757   2.165937    0.75387245 ... 11.202366   11.33529    10.740507  ]   ...   [ 4.254065    2.3328125    0.44079754 ... 11.385041   11.237292     9.75622   ]   [ 6.5205693   1.8131874   0.44322762 ... 13.063074   13.735182    13.360048  ]   [ 4.8410645   2.7622364   0.9499095  ... 14.075225   14.371651    14.25375   ]]  [[         nan          nan          nan ...          nan          nan             nan]   [         nan          nan          nan ...          nan          nan             nan]   [         nan          nan          nan ...          nan          nan             nan]   ...   [         nan          nan          nan ...          nan          nan             nan]   [         nan          nan          nan ...          nan          nan             nan]   [         nan          nan          nan ...          nan          nan             nan]]] ``` Moreover, I checked the input values before calling the `eval_step` function, and here are those related to the above example: { ``` 'input_ids': array([[  463,   118,   119, ...,     1,     1,     1],        [15010, 50264,   560, ...,     1,     1,     1],        [  118,   298,  3252, ...,     1,     1,     1],        ...,        [10669, 12465, 50264, ...,     1,     1,     1],        [25800,   627, 50264, ...,     1,     1,     1],        [ 8877, 50264,  6025, ...,     1,     1,     1]]),  'labels': array([[  605,   995,  1250, ...,  1794,   560, 26628],        [    4,  100,  4297, ...,  9902, 18028,   354],        [  844,     6,  1409, ..., 17137,    43,     6],        ...,        [17137,  2463,   281, ..., 24638, 41404, 29676],        [  118,  8494, 36024, ..., 30035, 35507,  4783],        [ 4484,   560,  1610, ...,  7443, 11970,  4297]]),  'decoder_input_ids': array([[    2,   605,   995, ...,  1990,  1794,   560],        [    2,     4,     1, ...,  4783,  9902, 18028],        [    2,   844,     6, ...,   428, 17137,    43],        ...,        [    2, 17137,  2463, ...,  3654, 24638, 41404],        [    2,   118,  8494, ...,  7333, 30035, 35507],        [    2,  4484,   560, ...,  1459,  7443, 11970]]),  'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],        [1, 1, 1, ..., 0, 0, 0],        [1, 1, 1, ..., 0, 0, 0],        ...,        [1, 1, 1, ..., 0, 0, 0],        [1, 1, 1, ..., 0, 0, 0],        [1, 1, 1, ..., 0, 0, 0]]),  'decoder_attention_mask': array([[1, 1, 1, ..., 1, 1, 1],        [1, 1, 0, ..., 1, 1, 1],        [1, 1, 1, ..., 1, 1, 1],        ...,        [1, 1, 1, ..., 1, 1, 1],        [1, 1, 1, ..., 1, 1, 1],        [1, 1, 1, ..., 1, 1, 1]]) } ``` I'm stuck at this stage; wondering how the Jax could modify the input values. Since there's no row whose all values is 0, but in the Jax function, we do see that there's such a row. I should add that since I'm training and evaluating in dualGPU setting, the effective batch_size is 8, and each GPU process half (i.e., 4) of these instances. ","UPDATE: Through debugging the posted code, I could finally find the root of the problem! It seems that JAX pads the given input with 0's given the batch_size in the experiments. However, the BART model treats 0 as a special token. The error comes from the inconsistency between the JAX padding value (0) and BART's padding value (1) as the input_ids. As a workaround for it, I made my inputs all fit in batch_size. For instance, if the last batch includes 7 instances (like the one above) which is less than 8, I just drop that sample. Given my evaluation set size, I don't have any problem with dropping the last batch of data, but for other experiments (where we want to keep and evaluate all instances within the evaluation set), I believe we need to change padding value of the JAX and make it consistent with the padding value that the core model (Here, BART) takes in.  The source snippet uses `flax.jax_utils.pad_shard_unpad(...)` to call the flax utilities, but I'm not sure how we can change the padding value from 0 to 1 in this function. Probably, we need to call more specific functions as instructed here.",Thanks for the update – it sounds like JAX is behaving as expected. Perhaps it's worth opening a flax issue?," YW. I see. Yes, I think this looks more like a thing that flax should handle. I'm closing the repo accordingly. "
1341,"以下是一个github上的jax下的一个issue, 标题是(cannot import name 'stax' from 'jax.experimental' (in Jupyter notebook))， 内容是 ( Description I have been trying to run some example code from a repository implementing normalizing flows in Jax (https://github.com/ChrisWaites/jaxflows). Trying to import 'stax' from 'jax.experimental' seems to fail with the following error: ```bash  ImportError                               Traceback (most recent call last) /tmp/ipykernel_66047/1298984118.py in        3        4 from jax import grad, jit, random > 5 from jax.experimental import stax, optimizers ImportError: cannot import name 'stax' from 'jax.experimental' (/home/user/.pyenv/versions/3.7.11/lib/python3.7/sitepackages/jax/experimental/__init__.py) ``` This happens during the import statement and can simply be reproduced with ```python from jax.experimental import stax ``` The full code in which this bug occurred can be found here. Any ideas on how to fix this? Is it simply the wrong version of jax (v0.3.16) or jaxlib (v0.3.15)? Thanks in advance!  What jax/jaxlib version are you using? jax v0.3.16, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Python 3.7.11, Linux Manjaro)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,cannot import name 'stax' from 'jax.experimental' (in Jupyter notebook)," Description I have been trying to run some example code from a repository implementing normalizing flows in Jax (https://github.com/ChrisWaites/jaxflows). Trying to import 'stax' from 'jax.experimental' seems to fail with the following error: ```bash  ImportError                               Traceback (most recent call last) /tmp/ipykernel_66047/1298984118.py in        3        4 from jax import grad, jit, random > 5 from jax.experimental import stax, optimizers ImportError: cannot import name 'stax' from 'jax.experimental' (/home/user/.pyenv/versions/3.7.11/lib/python3.7/sitepackages/jax/experimental/__init__.py) ``` This happens during the import statement and can simply be reproduced with ```python from jax.experimental import stax ``` The full code in which this bug occurred can be found here. Any ideas on how to fix this? Is it simply the wrong version of jax (v0.3.16) or jaxlib (v0.3.15)? Thanks in advance!  What jax/jaxlib version are you using? jax v0.3.16, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Python 3.7.11, Linux Manjaro",2022-08-30T10:41:49Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/12160,`jax.experimental.xtax` was deprecated in JAX version 0.2.25 and removed in JAX version 0.3.16. Instead you can use ```python from jax.example_libraries import stax ```
4907,"以下是一个github上的jax下的一个issue, 标题是(Gradient leakage through masked convolutions)， 内容是 ( Description **Short summary**: When applying a mask to a convolution kernel, the gradients are unexpectedly nonzero for masked input elements.  **Minimal example in Colab**: ![Open In Collab](https://colab.research.google.com/drive/1pC4_eIrDW2AV7UO8TJFwnsZnx1OA2AA4?usp=sharing) The bug/unexpected behavior is related to the situation where we want to mask certain elements in a convolutional filter. For example, for autoregressive image modeling, one would not want to look at 'future' pixels in order to allow for an efficient training via teacher forcing. However, when investigating the gradients through a masked convolution, it turns out that the gradient for masked input elements is nonzero, despite the kernel value being zero for these pixels.  Example Consider an image of size 5x5 on which we apply a 3x3 kernel. We mask out the last row of the kernel, hence only taking into account the pixels above and on the same row of a reference pixel. We then apply this masked convolution to the input image, and determine the gradients of the input image through this operation. For clarity, we use the sum of the center pixel features as 'loss', which corresponds to visualizing the receptive field of the center pixel. ```python import jax import jax.numpy as jnp from jax import lax from jax import random  Setting up input, kernel, mask feat_dim = 128 rng = random.PRNGKey(42) rng, inp_rng, kernel_rng = random.split(rng, 3) inp = random.normal(inp_rng, (1, 5, 5, feat_dim)) kernel = 1/jnp.sqrt(9*feat_dim) * random.normal(kernel_rng, (3, 3, feat_dim, feat_dim))  Masking last row of 3x3 filter: [[1, 1, 1], [1, 1, 1], [0, 0, 0]] mask = jnp.concatenate([jnp.ones((2, 3, feat_dim, feat_dim)),                         jnp.zeros((1, 3, feat_dim, feat_dim))], axis=0)  Applying convolution with mask applied to kernel def apply_masked_conv(input_tensor, kernel_tensor):   kernel_tensor *= mask   y = lax.conv_general_dilated(           input_tensor,           kernel_tensor,           (1, 1),           ""SAME"",           dimension_numbers=('NHWC', 'HWOI', 'NHWC')       )   return y  Example gradient function with respect to the center pixel at position 2,2 grad_fn = jax.grad(lambda input_tensor: apply_masked_conv(input_tensor, kernel)[:,2,2,:].sum()) grads = grad_fn(inp)  Printing absolute gradient per pixel, averaged over input channels print(jnp.abs(grads).mean(axis=1)) ``` Because of the masking, one would expect that only the pixels `inp[:,1:3,1:4]` have nonzero gradients. However, the gradients are as follows: ``` [[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]   [0.0000000e+00 2.4213897e01 2.4514729e01 2.7408689e01 0.0000000e+00]   [0.0000000e+00 2.9646739e01 2.5383139e01 2.4915963e01 0.0000000e+00]   [0.0000000e+00 6.6611392e08 1.0057556e07 1.1386874e07 0.0000000e+00]   [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]] ``` The elements `[6.6611392e08 1.0057556e07 1.1386874e07]` are the gradients for the pixels that are masked out by the kernel, and are nonzero despite the kernel value being zero for them. Note that this occurs on a GPU, but the elements are zeros if the code is run on CPU, or a very small channel size is used (e.g. 4). Additionally, the gradients of the kernels are correctly zero for the masked elements.  Relevance Masked convolutions are used in autoregressive convolutional models such as PixelCNN. When implemented with masked convolutions, one sees that the receptive field of a pixel spans across the whole image. A full implementation example with the unexpected behavior can be found here. This leads to gradients through masked features when stacking multiple masked layers, and introduced a gradient bias to the optimization of the kernels through masked inputs. A simple example, where the optimization goes wrong because of this, can be found here. In practice, the leakage is not directly noticeable, since the leaked gradients are usually a few magnitudes smaller than the nonmasked elements. A workaround, also for efficiency, is to use smaller kernels in the first place (e.g. [2x3] instead of masked [3x3]). But considering that masked convolutions are supported in, e.g., flax, I think it would be good to point out this gradient leakage. Please let me know in case this bug/behavior has already been discussed in detail somewhere in the JAX documentation, a search through it and the current github issues didn't show any results.  What jax/jaxlib version are you using? jax v0.3.14, jaxlib v0.3.14  Which accelerator(s) are you using? GPU (behavior as expected on CPU)  Additional System Info Tested on Colab and locally (Ubuntu, Python 3.9, NVIDIA GTX1080Ti))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Gradient leakage through masked convolutions," Description **Short summary**: When applying a mask to a convolution kernel, the gradients are unexpectedly nonzero for masked input elements.  **Minimal example in Colab**: ![Open In Collab](https://colab.research.google.com/drive/1pC4_eIrDW2AV7UO8TJFwnsZnx1OA2AA4?usp=sharing) The bug/unexpected behavior is related to the situation where we want to mask certain elements in a convolutional filter. For example, for autoregressive image modeling, one would not want to look at 'future' pixels in order to allow for an efficient training via teacher forcing. However, when investigating the gradients through a masked convolution, it turns out that the gradient for masked input elements is nonzero, despite the kernel value being zero for these pixels.  Example Consider an image of size 5x5 on which we apply a 3x3 kernel. We mask out the last row of the kernel, hence only taking into account the pixels above and on the same row of a reference pixel. We then apply this masked convolution to the input image, and determine the gradients of the input image through this operation. For clarity, we use the sum of the center pixel features as 'loss', which corresponds to visualizing the receptive field of the center pixel. ```python import jax import jax.numpy as jnp from jax import lax from jax import random  Setting up input, kernel, mask feat_dim = 128 rng = random.PRNGKey(42) rng, inp_rng, kernel_rng = random.split(rng, 3) inp = random.normal(inp_rng, (1, 5, 5, feat_dim)) kernel = 1/jnp.sqrt(9*feat_dim) * random.normal(kernel_rng, (3, 3, feat_dim, feat_dim))  Masking last row of 3x3 filter: [[1, 1, 1], [1, 1, 1], [0, 0, 0]] mask = jnp.concatenate([jnp.ones((2, 3, feat_dim, feat_dim)),                         jnp.zeros((1, 3, feat_dim, feat_dim))], axis=0)  Applying convolution with mask applied to kernel def apply_masked_conv(input_tensor, kernel_tensor):   kernel_tensor *= mask   y = lax.conv_general_dilated(           input_tensor,           kernel_tensor,           (1, 1),           ""SAME"",           dimension_numbers=('NHWC', 'HWOI', 'NHWC')       )   return y  Example gradient function with respect to the center pixel at position 2,2 grad_fn = jax.grad(lambda input_tensor: apply_masked_conv(input_tensor, kernel)[:,2,2,:].sum()) grads = grad_fn(inp)  Printing absolute gradient per pixel, averaged over input channels print(jnp.abs(grads).mean(axis=1)) ``` Because of the masking, one would expect that only the pixels `inp[:,1:3,1:4]` have nonzero gradients. However, the gradients are as follows: ``` [[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]   [0.0000000e+00 2.4213897e01 2.4514729e01 2.7408689e01 0.0000000e+00]   [0.0000000e+00 2.9646739e01 2.5383139e01 2.4915963e01 0.0000000e+00]   [0.0000000e+00 6.6611392e08 1.0057556e07 1.1386874e07 0.0000000e+00]   [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]] ``` The elements `[6.6611392e08 1.0057556e07 1.1386874e07]` are the gradients for the pixels that are masked out by the kernel, and are nonzero despite the kernel value being zero for them. Note that this occurs on a GPU, but the elements are zeros if the code is run on CPU, or a very small channel size is used (e.g. 4). Additionally, the gradients of the kernels are correctly zero for the masked elements.  Relevance Masked convolutions are used in autoregressive convolutional models such as PixelCNN. When implemented with masked convolutions, one sees that the receptive field of a pixel spans across the whole image. A full implementation example with the unexpected behavior can be found here. This leads to gradients through masked features when stacking multiple masked layers, and introduced a gradient bias to the optimization of the kernels through masked inputs. A simple example, where the optimization goes wrong because of this, can be found here. In practice, the leakage is not directly noticeable, since the leaked gradients are usually a few magnitudes smaller than the nonmasked elements. A workaround, also for efficiency, is to use smaller kernels in the first place (e.g. [2x3] instead of masked [3x3]). But considering that masked convolutions are supported in, e.g., flax, I think it would be good to point out this gradient leakage. Please let me know in case this bug/behavior has already been discussed in detail somewhere in the JAX documentation, a search through it and the current github issues didn't show any results.  What jax/jaxlib version are you using? jax v0.3.14, jaxlib v0.3.14  Which accelerator(s) are you using? GPU (behavior as expected on CPU)  Additional System Info Tested on Colab and locally (Ubuntu, Python 3.9, NVIDIA GTX1080Ti)",2022-08-30T10:22:17Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/12159,"Thanks for the report, this is an interesting bug. Strangely enough it looks like jitcompiling `grad_fn` is sufficient to fix the issue: ```python grads = jax.jit(grad_fn)(inp) print(jnp.abs(grads).mean(axis=1))  [[[0.         0.         0.         0.         0.        ]    [0.         0.24213897 0.24514729 0.2740869  0.        ]    [0.         0.29646742 0.2538314  0.24915966 0.        ]    [0.         0.         0.         0.         0.        ]    [0.         0.         0.         0.         0.        ]]] ```","Hi , thanks for looking at it and interesting that jitcompiling fixes it for this single layer! Interestingly, however, the optimization through two layers still showed issues under jitcompiling the update step and the same holds for the PixelCNN.  Under further investigation, it seems that the output of the convolution is already affected by this. For example, consider the input image which has 0s on the first three rows, and 1s on the last two rows for all channels. Applying a filter with the mask above should output zero features for the center pixel, since we convolve a filter with mask `[[1 1 1], [1 1 1], [0 0 0]]` with an input `[[0 0 0], [0 0 0], [1 1 1]]` for this single pixel. In other words, we only multiply 0s with 1s. However, the output is again nonzero for these pixels: ```python inp = jnp.concatenate([jnp.zeros((1, 3, 5, feat_dim)),                        jnp.ones((1, 2, 5, feat_dim))], axis=1) out = jax.jit(apply_masked_conv)(inp, kernel) print(jnp.abs(out).mean(axis=1)) ``` Output on a GPU (Colab): ``` [[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]   [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]   [1.4640318e07 1.2042801e07 1.2042801e07 1.2042801e07 1.2423152e07]   [3.6361670e01 4.2430118e01 4.2430118e01 4.2430118e01 3.6237997e01]   [5.5979115e01 6.9566023e01 6.9566023e01 6.9566023e01 5.3750622e01]]] ``` On a CPU, the output is, as expected, zero for the center row: ``` [[[0.         0.         0.         0.         0.        ]   [0.         0.         0.         0.         0.        ]   [0.         0.         0.         0.         0.        ]   [0.3636167  0.42430118 0.42430118 0.42430118 0.36237997]   [0.55979115 0.69566035 0.69566035 0.69566035 0.5375063 ]]] ``` Interestingly, the upper two rows are all zeros even on a GPU, which suggests that the behavior only occurs when the input is nonzero.","In a related discussion, it was suggested that the reason for this difference might be the convolution kernel chosen by XLA. "
2226,"以下是一个github上的jax下的一个issue, 标题是(Float16 behavior affect by jax_enable_x64)， 内容是 ( Description When the flag for float 64 enabling is ON, the behavior of float16 operations changes  unexpectedly, in at least one case.  Here is a function that reproduces: ```python def multi_uniform_slow(_keys, shape, dtype):     out_uniform = []     for _k in _keys:         _uniform = random.uniform(_k, shape=shape, dtype=dtype)         out_uniform.append(_uniform)     return numpy.stack(out_uniform) ``` That is a slower version of something like this, vmap'ing the random.normal function: ```python (jit, static_argnums=(1,2)) def multi_uniform(keys, shape, dtype):     l = jit(vmap(lambda x : random.uniform(x, shape=shape, dtype=dtype)))     return l(keys) ``` I discovered this bug because I wrote a unit test for the ""real"" function above (`multi_uniform`) that looks like this: ```python .mark.parametrize(""n_keys"", [10]) .mark.parametrize(""dtype"", [numpy.float64,numpy.float32,numpy.float16, numpy.bfloat16]) def test_multi_uniform(n_keys, seed, dtype):     key = random.PRNGKey(int(seed))     shape = ()     def multi_uniform_slow(_keys, shape, dtype):         out_uniform = []         for _k in _keys:             _uniform = random.uniform(_k, shape=shape, dtype=dtype)             out_uniform.append(_uniform)             print(_uniform)         return numpy.stack(out_uniform)     multikeys = random.split(key, n_keys)     slow_uniform = multi_uniform_slow(multikeys, shape, dtype)     fast_uniform = multi_uniform(multikeys, shape, dtype)     assert (slow_uniform == fast_uniform).all() ``` This unit test passes for fp16 when fp64 enabling is OFF.  It fails for fp16 when fp64 enabling is ON.  The uniform function returns all 0s when used in the ""slow"" version in the case that fp64 is ON, though notably they JIT'd function returns sensible (but unchecked by unit testing) values. Expected behavior: fp64 flags shouldn't affect fp16 operations.  What jax/jaxlib version are you using? jax 0.3.15, jaxlib 0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Mac OSX)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Float16 behavior affect by jax_enable_x64," Description When the flag for float 64 enabling is ON, the behavior of float16 operations changes  unexpectedly, in at least one case.  Here is a function that reproduces: ```python def multi_uniform_slow(_keys, shape, dtype):     out_uniform = []     for _k in _keys:         _uniform = random.uniform(_k, shape=shape, dtype=dtype)         out_uniform.append(_uniform)     return numpy.stack(out_uniform) ``` That is a slower version of something like this, vmap'ing the random.normal function: ```python (jit, static_argnums=(1,2)) def multi_uniform(keys, shape, dtype):     l = jit(vmap(lambda x : random.uniform(x, shape=shape, dtype=dtype)))     return l(keys) ``` I discovered this bug because I wrote a unit test for the ""real"" function above (`multi_uniform`) that looks like this: ```python .mark.parametrize(""n_keys"", [10]) .mark.parametrize(""dtype"", [numpy.float64,numpy.float32,numpy.float16, numpy.bfloat16]) def test_multi_uniform(n_keys, seed, dtype):     key = random.PRNGKey(int(seed))     shape = ()     def multi_uniform_slow(_keys, shape, dtype):         out_uniform = []         for _k in _keys:             _uniform = random.uniform(_k, shape=shape, dtype=dtype)             out_uniform.append(_uniform)             print(_uniform)         return numpy.stack(out_uniform)     multikeys = random.split(key, n_keys)     slow_uniform = multi_uniform_slow(multikeys, shape, dtype)     fast_uniform = multi_uniform(multikeys, shape, dtype)     assert (slow_uniform == fast_uniform).all() ``` This unit test passes for fp16 when fp64 enabling is OFF.  It fails for fp16 when fp64 enabling is ON.  The uniform function returns all 0s when used in the ""slow"" version in the case that fp64 is ON, though notably they JIT'd function returns sensible (but unchecked by unit testing) values. Expected behavior: fp64 flags shouldn't affect fp16 operations.  What jax/jaxlib version are you using? jax 0.3.15, jaxlib 0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Mac OSX",2022-08-29T15:50:04Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/12145,"Oh, one other point of data: the `random.normal` function is NOT affected by this.  I have not tested any other random generators.","What is `numpy` in this code snippet? Could you provide a complete example, including imports?","Yes, sorry, that's a pretty important piece of information I left out.... `numpy` is `jax.numpy` exclusively here. Here's a standalone reproducer that fails on my mac for fp16 only.  If I remove line 23: ```python from jax.config import config; config.update(""jax_enable_x64"", True"") ```  it succeeds instead of fails. https://gist.github.com/coreyjadams/c6bf63ab52154988fd343f20bec4563c I went ahead and tested on nvidia A100 and the failure is not present on GPU; perhaps isolated to CPU.  On GPU, jax and jaxlib were both 0.3.15","Thanks for the info. For future reference, it would be more helpful if you could edit your initial post to add a complete example rather than linking to an external example.","Here's a reproduction that narrowsin on the core issue here: ```python import jax key = jax.random.PRNGKey(0) for dtype in ['float16', 'float32']:   print(""dtype ="", dtype)   with jax.experimental.enable_x64(True):     print(""  x64=True: "", jax.random.uniform(key, (), dtype))   with jax.experimental.enable_x64(False):     print(""  x64=False:"", jax.random.uniform(key, (), dtype)) ``` ``` dtype = float16   x64=True:  0.0   x64=False: 0.004883 dtype = float32   x64=True:  0.41845703   x64=False: 0.41845703 ``` Assigning to  because he's been working on PRNG stuff lately.","Hi   Looks like this issue has been resolved. I executed the minimal reproducer provided by  on colab CPU with JAX version 0.4.23. It resulted the expected (same) output with and without the `x64` enabled for `float16` also. ```python import jax print(f""JAX versioin: {jax.__version__}"") key = jax.random.PRNGKey(0) for dtype in ['float16', 'float32']:   print(""dtype ="", dtype)   with jax.experimental.enable_x64(True):     print(""  x64=True: "", jax.random.uniform(key, (), dtype))   with jax.experimental.enable_x64(False):     print(""  x64=False:"", jax.random.uniform(key, (), dtype)) ``` Output: ``` JAX versioin: 0.4.23 dtype = float16   x64=True:  0.004883   x64=False: 0.004883 dtype = float32   x64=True:  0.41845703   x64=False: 0.41845703 ``` Attaching the gist for reference. Also verified its behavior on mac with JAX version 0.4.25 and it produced the same result for `float16` with both `x64 = True` and `x64=False`. Please find the screenshot on mac.  Could you please verify with the latest JAX versions and let us know if the issue still persists. Thank you",Thanks for following up!,"Thanks for following up, looks like it is working for me too!"
38600,"以下是一个github上的jax下的一个issue, 标题是(⚠️ Nightly GPU Multiprocess CI failed ⚠️)， 内容是 (Workflow Run URL Failure summary output8680.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.17.dev20220830%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (156.6 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156.6/156.6 MB 18.4 MB/s eta 0:00:00 Collecting numpy>=1.20   Downloading numpy1.23.2cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 120.8 MB/s eta 0:00:00 Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.2.0) Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.17.dev20220830+cuda11.cudnn82 numpy1.23.2 Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuild9vfmtrck   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuild9vfmtrck   Resolved https://github.com/google/jax to commit da24b99d308141cb8c477a10ad9ac6239e0643f3   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.2.0) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.23.2) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 20.5 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.7.1py3noneany.whl (124 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.9/124.9 kB 32.9 MB/s eta 0:00:00 Requirement already satisfied: zipp in /usr/local/lib/python3.8/sitepackages (from etils[epath]>jax==0.3.17) (3.8.1) Collecting importlib_resources   Downloading importlib_resources5.9.0py3noneany.whl (33 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.17py3noneany.whl size=1221924 sha256=65a8f6481e05c416762042dcee349fe865bac3efe4bb90e69cbecfbfebc4b0da   Stored in directory: /tmp/pipephemwheelcachems4piq56/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, importlib_resources, etils, jax Successfully installed etils0.7.1 importlib_resources5.9.0 jax0.3.17 opt_einsum3.3.0 typing_extensions4.3.0 Collecting pytest   Downloading pytest7.1.2py3noneany.whl (297 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 kB 41.0 MB/s eta 0:00:00 Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 24.3 MB/s eta 0:00:00 Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 36.6 MB/s eta 0:00:00 Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.2 tomli2.0.1 Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.2) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 PASSEDPASSEDPASSEDPASSEDPASSEDPASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1 PASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_4.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.34s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_5.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.53s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_0.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.53s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_7.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.75s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_1.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.93s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(out, f'{num_gpus_per_task},{num_gpus},[{num_gpus}.]') E       AssertionError:  E        1,1,[1.] E       ?   ^  ^ E       + 1,4,[4.] E       ?   ^  ^ workspace/tests/multiprocess_gpu_test.py:149: AssertionError  generated xml file: /workspace/outputs/junit_output_2.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.94s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_3.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.95s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_6.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 180.45s (0:03:00) ==================== ```  Failure summary output8681.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.17.dev20220830%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (156.6 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156.6/156.6 MB 21.9 MB/s eta 0:00:00 Collecting numpy>=1.20   Downloading numpy1.23.2cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 125.3 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.2.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.17.dev20220830+cuda11.cudnn82 numpy1.23.2 Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuildgwtjmbml   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuildgwtjmbml   Resolved https://github.com/google/jax to commit da24b99d308141cb8c477a10ad9ac6239e0643f3   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.2.0) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.23.2) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 17.1 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.7.1py3noneany.whl (124 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.9/124.9 kB 9.3 MB/s eta 0:00:00 Collecting importlib_resources   Downloading importlib_resources5.9.0py3noneany.whl (33 kB) Requirement already satisfied: zipp in /usr/local/lib/python3.8/sitepackages (from etils[epath]>jax==0.3.17) (3.8.1) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.17py3noneany.whl size=1221924 sha256=d79afb7943638ef5f1547f86b1f2c90c1e2c0f4c19789e69061829f1f3ce26ce   Stored in directory: /tmp/pipephemwheelcachelv_a9h0k/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, importlib_resources, etils, jax Successfully installed etils0.7.1 importlib_resources5.9.0 jax0.3.17 opt_einsum3.3.0 typing_extensions4.3.0 Collecting pytest   Downloading pytest7.1.2py3noneany.whl (297 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 kB 40.1 MB/s eta 0:00:00 Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 35.2 MB/s eta 0:00:00 Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 14.9 MB/s eta 0:00:00 Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.2 tomli2.0.1 Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.2) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 PASSEDPASSEDPASSEDPASSEDPASSEDPASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1 PASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_12.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.12s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_15.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.22s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_10.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.42s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_14.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.41s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_9.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.73s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_8.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.74s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_11.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.74s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(out, f'{num_gpus_per_task},{num_gpus},[{num_gpus}.]') E       AssertionError:  E        1,1,[1.] E       ?   ^  ^ E       + 1,4,[4.] E       ?   ^  ^ workspace/tests/multiprocess_gpu_test.py:149: AssertionError  generated xml file: /workspace/outputs/junit_output_13.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 180.26s (0:03:00) ==================== ``` )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,⚠️ Nightly GPU Multiprocess CI failed ⚠️,"Workflow Run URL Failure summary output8680.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.17.dev20220830%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (156.6 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156.6/156.6 MB 18.4 MB/s eta 0:00:00 Collecting numpy>=1.20   Downloading numpy1.23.2cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 120.8 MB/s eta 0:00:00 Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.2.0) Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.17.dev20220830+cuda11.cudnn82 numpy1.23.2 Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuild9vfmtrck   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuild9vfmtrck   Resolved https://github.com/google/jax to commit da24b99d308141cb8c477a10ad9ac6239e0643f3   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.2.0) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.23.2) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 20.5 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.7.1py3noneany.whl (124 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.9/124.9 kB 32.9 MB/s eta 0:00:00 Requirement already satisfied: zipp in /usr/local/lib/python3.8/sitepackages (from etils[epath]>jax==0.3.17) (3.8.1) Collecting importlib_resources   Downloading importlib_resources5.9.0py3noneany.whl (33 kB) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.17py3noneany.whl size=1221924 sha256=65a8f6481e05c416762042dcee349fe865bac3efe4bb90e69cbecfbfebc4b0da   Stored in directory: /tmp/pipephemwheelcachems4piq56/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, importlib_resources, etils, jax Successfully installed etils0.7.1 importlib_resources5.9.0 jax0.3.17 opt_einsum3.3.0 typing_extensions4.3.0 Collecting pytest   Downloading pytest7.1.2py3noneany.whl (297 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 kB 41.0 MB/s eta 0:00:00 Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 24.3 MB/s eta 0:00:00 Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 36.6 MB/s eta 0:00:00 Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.2 tomli2.0.1 Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.2) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 PASSEDPASSEDPASSEDPASSEDPASSEDPASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1 PASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_4.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.34s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_5.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.53s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_0.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.53s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_7.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.75s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_1.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.93s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(out, f'{num_gpus_per_task},{num_gpus},[{num_gpus}.]') E       AssertionError:  E        1,1,[1.] E       ?   ^  ^ E       + 1,4,[4.] E       ?   ^  ^ workspace/tests/multiprocess_gpu_test.py:149: AssertionError  generated xml file: /workspace/outputs/junit_output_2.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.94s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_3.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.95s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_6.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 180.45s (0:03:00) ==================== ```  Failure summary output8681.txt ``` pyxis: imported docker image: nvcr.io/nvidian/jax_t5x:cuda11.4cudnn8.2ubuntu20.04manylinux2014multipython Looking in links: https://storage.googleapis.com/jaxreleases/jaxlib_nightly_cuda_releases.html Collecting jaxlib   Downloading https://storage.googleapis.com/jaxreleases/nightly/cuda114/jaxlib0.3.17.dev20220830%2Bcuda11.cudnn82cp38cp38manylinux2014_x86_64.whl (156.6 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156.6/156.6 MB 21.9 MB/s eta 0:00:00 Collecting numpy>=1.20   Downloading numpy1.23.2cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 125.3 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.9.0) Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jaxlib) (1.2.0) Installing collected packages: numpy, jaxlib   Attempting uninstall: numpy     Found existing installation: numpy 1.19.0     Uninstalling numpy1.19.0:       Successfully uninstalled numpy1.19.0 Successfully installed jaxlib0.3.17.dev20220830+cuda11.cudnn82 numpy1.23.2 Collecting git+https://github.com/google/jax   Cloning https://github.com/google/jax to /tmp/pipreqbuildgwtjmbml   Running command git clone filter=blob:none quiet https://github.com/google/jax /tmp/pipreqbuildgwtjmbml   Resolved https://github.com/google/jax to commit da24b99d308141cb8c477a10ad9ac6239e0643f3   Preparing metadata (setup.py): started   Preparing metadata (setup.py): finished with status 'done' Requirement already satisfied: abslpy in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.2.0) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.23.2) Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 17.1 MB/s eta 0:00:00 Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/sitepackages (from jax==0.3.17) (1.9.0) Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.7.1py3noneany.whl (124 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.9/124.9 kB 9.3 MB/s eta 0:00:00 Collecting importlib_resources   Downloading importlib_resources5.9.0py3noneany.whl (33 kB) Requirement already satisfied: zipp in /usr/local/lib/python3.8/sitepackages (from etils[epath]>jax==0.3.17) (3.8.1) Building wheels for collected packages: jax   Building wheel for jax (setup.py): started   Building wheel for jax (setup.py): finished with status 'done'   Created wheel for jax: filename=jax0.3.17py3noneany.whl size=1221924 sha256=d79afb7943638ef5f1547f86b1f2c90c1e2c0f4c19789e69061829f1f3ce26ce   Stored in directory: /tmp/pipephemwheelcachelv_a9h0k/wheels/69/d2/e4/503a58b7967c1c679f121f0d4a17856479e7e926d913c101e1 Successfully built jax Installing collected packages: typing_extensions, opt_einsum, importlib_resources, etils, jax Successfully installed etils0.7.1 importlib_resources5.9.0 jax0.3.17 opt_einsum3.3.0 typing_extensions4.3.0 Collecting pytest   Downloading pytest7.1.2py3noneany.whl (297 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.0/297.0 kB 40.1 MB/s eta 0:00:00 Collecting iniconfig   Downloading iniconfig1.1.1py2.py3noneany.whl (5.0 kB) Collecting pluggy=0.12   Downloading pluggy1.0.0py2.py3noneany.whl (13 kB) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest) (21.3) Collecting py>=1.8.2   Downloading py1.11.0py2.py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.7/98.7 kB 35.2 MB/s eta 0:00:00 Collecting tomli>=1.0.0   Downloading tomli2.0.1py3noneany.whl (12 kB) Collecting attrs>=19.2.0   Downloading attrs22.1.0py2.py3noneany.whl (58 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 kB 14.9 MB/s eta 0:00:00 Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest) (3.0.9) Installing collected packages: iniconfig, tomli, py, pluggy, attrs, pytest Successfully installed attrs22.1.0 iniconfig1.1.1 pluggy1.0.0 py1.11.0 pytest7.1.2 tomli2.0.1 Collecting pytestforked   Downloading pytest_forked1.4.0py3noneany.whl (4.9 kB) Requirement already satisfied: pytest>=3.10 in /usr/local/lib/python3.8/sitepackages (from pytestforked) (7.1.2) Requirement already satisfied: py in /usr/local/lib/python3.8/sitepackages (from pytestforked) (1.11.0) Requirement already satisfied: packaging in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (21.3) Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (2.0.1) Requirement already satisfied: pluggy=0.12 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.0.0) Requirement already satisfied: iniconfig in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (1.1.1) Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/sitepackages (from pytest>=3.10>pytestforked) (22.1.0) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/sitepackages (from packaging>pytest>=3.10>pytestforked) (3.0.9) Installing collected packages: pytestforked Successfully installed pytestforked1.4.0 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 Wed Aug 31 12:07:59 GMT 2022 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 jax                     0.3.17 jaxlib                  0.3.17.dev20220830+cuda11.cudnn82 ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... ============================= test session starts ============================== platform linux  Python 3.8.2, pytest7.1.2, pluggy1.0.0  /usr/local/bin/python3.8 cachedir: .pytest_cache rootdir: /workspace, configfile: pytest.ini plugins: forked1.4.0 collecting ... collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 collected 6 items workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown0 PASSEDPASSEDPASSEDPASSEDPASSEDPASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown1 PASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2  workspace/tests/multiprocess_gpu_test.py::DistributedTest::testConcurrentInitializeAndShutdown2 PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSEDPASSEDPASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices PASSEDPASSED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices  workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_gpu_distributed_initialize PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED workspace/tests/multiprocess_gpu_test.py::SlurmMultiNodeGpuTest::test_gpu_multi_node_initialize_and_psum PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_12.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.12s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_15.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.22s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_10.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.42s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_14.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.41s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_9.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.73s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 1 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_8.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.74s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(proc.returncode, 0) E       AssertionError: 6 != 0 workspace/tests/multiprocess_gpu_test.py:148: AssertionError  generated xml file: /workspace/outputs/junit_output_11.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 179.74s (0:02:59) ==================== PASSED =================================== FAILURES =================================== ________ MultiProcessGpuTest.test_distributed_jax_cuda_visible_devices _________ self =      .skipIf(xla_extension_version        self.assertEqual(out, f'{num_gpus_per_task},{num_gpus},[{num_gpus}.]') E       AssertionError:  E        1,1,[1.] E       ?   ^  ^ E       + 1,4,[4.] E       ?   ^  ^ workspace/tests/multiprocess_gpu_test.py:149: AssertionError  generated xml file: /workspace/outputs/junit_output_13.xml  =========================== short test summary info ============================ FAILED workspace/tests/multiprocess_gpu_test.py::MultiProcessGpuTest::test_distributed_jax_cuda_visible_devices =================== 1 failed, 5 passed in 180.26s (0:03:00) ==================== ``` ",2022-08-28T12:12:07Z,P1 (soon) NVIDIA GPU Nightly-CI,closed,0,2,https://github.com/jax-ml/jax/issues/12137,"It looks like process 2 failed with the following error and so process 1 timed out. I don't have a repro of this yet ``` ==================================== ERRORS ==================================== _______________ ERROR collecting tests/multiprocess_gpu_test.py ________________ workspace/tests/multiprocess_gpu_test.py:29: in      from jax._src import test_util as jtu :991: in _find_and_load     ??? :975: in _find_and_load_unlocked     ??? :671: in _load_unlocked     ??? usr/local/lib/python3.8/sitepackages/_pytest/assertion/rewrite.py:163: in exec_module     _write_pyc(state, co, source_stat, pyc) usr/local/lib/python3.8/sitepackages/_pytest/assertion/rewrite.py:350: in _write_pyc     fp.close() E   OSError: [Errno 116] Stale file handle  generated xml file: /workspace/outputs/junit_output_1.xml  =========================== short test summary info ============================ ERROR workspace/tests/multiprocess_gpu_test.py  OSError: [Errno 116] Stale f... =============================== 1 error in 1.34s =============================== ```","The original issue is that there's a potential race condition while editing report (*.xml) files when multiple tests are called in separate processes. When some test fails, it might result in cascading failure (due to race condition while adding to report files) of the actual multinode tests. Now the other test fails because it launches nested subprocesses within slurm's processes which is unexpected (and unwanted) behavior. The workaround is to call only multinode tests in the multinode CI (for GPUs). Fixed by https://github.com/google/jax/pull/12172"
5449,"以下是一个github上的jax下的一个issue, 标题是(ImportError: cannot import name 'pytree' from 'jaxlib')， 内容是 ( Description Hi I'm trying to run this code on my ubuntu18.04,but It turns out ``` Traceback (most recent call last):   File ""ilqr_jax_MPC.py"", line 1, in      from jax import jit, jacfwd, jacrev, hessian, lax   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/__init__.py"", line 19, in      from jax.api import *   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/api.py"", line 37, in      from . import core   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 28, in      from . import dtypes   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/dtypes.py"", line 31, in      from .lib import xla_client   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/lib/__init__.py"", line 55, in      from jaxlib import pytree ImportError: cannot import name 'pytree' from 'jaxlib' (/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jaxlib/__init__.py) ``` Actually I have run this code successfully on jaxlib 0.1.47,but It turns out ti be very slow to complete one loop which needs more than 2 hours, and it has a warning that :Can not found GPU, fall back to CPU. So I want to use GPU to accelerate the code, while I have tried many versions but all failed. For my GPU is NVIDIA 3060ti, some old version of jaxlib and CUDA are not support for the gpu, when I try the jaxlib 0.1.52 I have the same erro that "" cannot import name 'pytree' from 'jaxlib' "". while using newer jaxlib and jax it also turns out some other erros like ``` pygame 2.1.2 (SDL 2.0.16, Python 3.8.13) Hello from the pygame community. https://www.pygame.org/contribute.html Traceback (most recent call last): File ""ilqr_jax_MPC.py"", line 164, in jac_l, hes_l, jac_l_final, hes_l_final, jac_f = derivative_init() File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback return fun(*args, **kwargs) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/src/api.py"", line 527, in cache_miss out_flat = xla.xla_call( File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 1937, in bind return call_bind(self, fun, *args, **params) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 1953, in call_bind outs = top_trace.process_call(primitive, fun, tracers, params) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 687, in process_call return primitive.impl(f, *tracers, **params) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 208, in _xla_call_impl compiled_fun = xla_callable(fun, device, backend, name, donated_invars, File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/linear_util.py"", line 295, in memoized_fun ans = call(fun, *args) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 257, in _xla_callable_uncached return lower_xla_callable(fun, device, backend, name, donated_invars, False, File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 294, in wrapper return func(*args, **kwargs) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 302, in lower_xla_callable jaxpr, out_type, consts = pe.trace_to_jaxpr_final2( File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 294, in wrapper return func(*args, **kwargs) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 2188, in trace_to_jaxpr_final2 jaxpr, out_type, consts = trace_to_subjaxpr_dynamic2(fun, main, debug_info) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 2139, in trace_to_subjaxpr_dynamic2 out_tracers = map(trace.full_raise, ans) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/util.py"", line 47, in safe_map return list(map(f, *args)) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 415, in full_raise return self.pure(val) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 1761, in new_const aval = raise_to_shaped(get_aval(c), weak_type=dtypes.is_weakly_typed(c)) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 1163, in get_aval return concrete_aval(x) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 1155, in concrete_aval raise TypeError(f""Value {repr(x)} with type {type(x)} is not a valid JAX "" jax._src.traceback_util.UnfilteredStackTrace: TypeError: Value > with type  is not a valid JAX type The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified. ``` I really don't know how to solve them. Sorry if I have some misunderstanding because I just start using python and still a lack of experience. Looking forward to your reply.  What jax/jaxlib version are you using? jax0.1.61 jaxlib 0.1.60+CUDA111  Which accelerator(s) are you using? GPU  Additional System Info py38 ubuntu18.04 CUDA11.1 cuDNN8.0.5 NVIDIA 3060ti)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,ImportError: cannot import name 'pytree' from 'jaxlib'," Description Hi I'm trying to run this code on my ubuntu18.04,but It turns out ``` Traceback (most recent call last):   File ""ilqr_jax_MPC.py"", line 1, in      from jax import jit, jacfwd, jacrev, hessian, lax   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/__init__.py"", line 19, in      from jax.api import *   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/api.py"", line 37, in      from . import core   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 28, in      from . import dtypes   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/dtypes.py"", line 31, in      from .lib import xla_client   File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/lib/__init__.py"", line 55, in      from jaxlib import pytree ImportError: cannot import name 'pytree' from 'jaxlib' (/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jaxlib/__init__.py) ``` Actually I have run this code successfully on jaxlib 0.1.47,but It turns out ti be very slow to complete one loop which needs more than 2 hours, and it has a warning that :Can not found GPU, fall back to CPU. So I want to use GPU to accelerate the code, while I have tried many versions but all failed. For my GPU is NVIDIA 3060ti, some old version of jaxlib and CUDA are not support for the gpu, when I try the jaxlib 0.1.52 I have the same erro that "" cannot import name 'pytree' from 'jaxlib' "". while using newer jaxlib and jax it also turns out some other erros like ``` pygame 2.1.2 (SDL 2.0.16, Python 3.8.13) Hello from the pygame community. https://www.pygame.org/contribute.html Traceback (most recent call last): File ""ilqr_jax_MPC.py"", line 164, in jac_l, hes_l, jac_l_final, hes_l_final, jac_f = derivative_init() File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback return fun(*args, **kwargs) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/src/api.py"", line 527, in cache_miss out_flat = xla.xla_call( File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 1937, in bind return call_bind(self, fun, *args, **params) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 1953, in call_bind outs = top_trace.process_call(primitive, fun, tracers, params) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 687, in process_call return primitive.impl(f, *tracers, **params) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 208, in _xla_call_impl compiled_fun = xla_callable(fun, device, backend, name, donated_invars, File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/linear_util.py"", line 295, in memoized_fun ans = call(fun, *args) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 257, in _xla_callable_uncached return lower_xla_callable(fun, device, backend, name, donated_invars, False, File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 294, in wrapper return func(*args, **kwargs) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 302, in lower_xla_callable jaxpr, out_type, consts = pe.trace_to_jaxpr_final2( File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 294, in wrapper return func(*args, **kwargs) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 2188, in trace_to_jaxpr_final2 jaxpr, out_type, consts = trace_to_subjaxpr_dynamic2(fun, main, debug_info) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 2139, in trace_to_subjaxpr_dynamic2 out_tracers = map(trace.full_raise, ans) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/_src/util.py"", line 47, in safe_map return list(map(f, *args)) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 415, in full_raise return self.pure(val) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 1761, in new_const aval = raise_to_shaped(get_aval(c), weak_type=dtypes.is_weakly_typed(c)) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 1163, in get_aval return concrete_aval(x) File ""/home/hj/anaconda3/envs/carla/lib/python3.8/sitepackages/jax/core.py"", line 1155, in concrete_aval raise TypeError(f""Value {repr(x)} with type {type(x)} is not a valid JAX "" jax._src.traceback_util.UnfilteredStackTrace: TypeError: Value > with type  is not a valid JAX type The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified. ``` I really don't know how to solve them. Sorry if I have some misunderstanding because I just start using python and still a lack of experience. Looking forward to your reply.  What jax/jaxlib version are you using? jax0.1.61 jaxlib 0.1.60+CUDA111  Which accelerator(s) are you using? GPU  Additional System Info py38 ubuntu18.04 CUDA11.1 cuDNN8.0.5 NVIDIA 3060ti",2022-08-28T05:05:43Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/12136,This looks like a duplicate of https://github.com/google/jax/discussions/12135
3152,"以下是一个github上的jax下的一个issue, 标题是((`jax2tf`) Compilation of `Pytree`-returning functions via `tf.function` throws `TypeError`)， 内容是 ( Description I have some code which uses `jax2tf.convert` and `tf.function`  reading the examples for `jax2tf`, it seemed like returning `Pytree` satisfying containers was okay. Indeed, examine the following: ```python import jax from jax.experimental import jax2tf import tensorflow as tf import genjax .gen def model(key):     x = genjax.trace(""x"", genjax.Normal)(key, ())     return x def __inner(key, args):     key, tr = genjax.simulate(model)(key, args)     return key, tr key = jax.random.PRNGKey(314159) f_tf = jax2tf.convert(__inner) key, tr = f_tf(key, ()) print(tr) fn = tf.function(f_tf, autograph=False, jit_compile=True) key, tr = fn(key, ()) ``` For the first call to `f_tf`  this works correctly and is fine, the `tr` (which satisfies `Pytree`) is returned without an issue. However, when I `tf.function(...)` the computation  it raises a `TypeError` concerning the `tr` type: ```python TypeError: To be compatible with `tf.function` Python functions must return zero or more Tensors or ExtensionTypes ... found return value of type JAXTrace ``` `JAXTrace` being the `Pytree` satisfying dataclass which works correctly when I first call `f_tf`  Full stacktrace ``` WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 20220827 21:30:52.800908: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW 20220827 21:30:52.801035: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 510.73.5 does not match DSO version 515.48.7  cannot find working devices in this configuration JAXTrace(   gen_fn=,   args=(),   retval=(,),   choices=JAXChoiceMap(     tree={       'x':       DistributionTrace(         gen_fn=_Normal(),         args=(),         value=,         score=       )     }   ),   score= ) Traceback (most recent call last):   File ""/home/mccoy/research/genjax/examples/genjax2wasm/genjax2wasm.py"", line 35, in      key, tr = fn(key, ())   File ""/home/mccoy/.cache/pypoetry/virtualenvs/genjax2wasmf2UzH0Xspy3.10/lib/python3.10/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/mccoy/.cache/pypoetry/virtualenvs/genjax2wasmf2UzH0Xspy3.10/lib/python3.10/sitepackages/tensorflow/python/framework/func_graph.py"", line 1097, in convert     raise TypeError( TypeError: To be compatible with tf.function, Python functions must return zero or more Tensors or ExtensionTypes or None values; in compilation of .converted_fun at 0x7fd64d905000>, found return value of type JAXTrace, which is not a Tensor or ExtensionType. ```  What jax/jaxlib version are you using? jax v0.3.16, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Python 3.10, Linux)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,(`jax2tf`) Compilation of `Pytree`-returning functions via `tf.function` throws `TypeError`," Description I have some code which uses `jax2tf.convert` and `tf.function`  reading the examples for `jax2tf`, it seemed like returning `Pytree` satisfying containers was okay. Indeed, examine the following: ```python import jax from jax.experimental import jax2tf import tensorflow as tf import genjax .gen def model(key):     x = genjax.trace(""x"", genjax.Normal)(key, ())     return x def __inner(key, args):     key, tr = genjax.simulate(model)(key, args)     return key, tr key = jax.random.PRNGKey(314159) f_tf = jax2tf.convert(__inner) key, tr = f_tf(key, ()) print(tr) fn = tf.function(f_tf, autograph=False, jit_compile=True) key, tr = fn(key, ()) ``` For the first call to `f_tf`  this works correctly and is fine, the `tr` (which satisfies `Pytree`) is returned without an issue. However, when I `tf.function(...)` the computation  it raises a `TypeError` concerning the `tr` type: ```python TypeError: To be compatible with `tf.function` Python functions must return zero or more Tensors or ExtensionTypes ... found return value of type JAXTrace ``` `JAXTrace` being the `Pytree` satisfying dataclass which works correctly when I first call `f_tf`  Full stacktrace ``` WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 20220827 21:30:52.800908: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW 20220827 21:30:52.801035: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 510.73.5 does not match DSO version 515.48.7  cannot find working devices in this configuration JAXTrace(   gen_fn=,   args=(),   retval=(,),   choices=JAXChoiceMap(     tree={       'x':       DistributionTrace(         gen_fn=_Normal(),         args=(),         value=,         score=       )     }   ),   score= ) Traceback (most recent call last):   File ""/home/mccoy/research/genjax/examples/genjax2wasm/genjax2wasm.py"", line 35, in      key, tr = fn(key, ())   File ""/home/mccoy/.cache/pypoetry/virtualenvs/genjax2wasmf2UzH0Xspy3.10/lib/python3.10/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/home/mccoy/.cache/pypoetry/virtualenvs/genjax2wasmf2UzH0Xspy3.10/lib/python3.10/sitepackages/tensorflow/python/framework/func_graph.py"", line 1097, in convert     raise TypeError( TypeError: To be compatible with tf.function, Python functions must return zero or more Tensors or ExtensionTypes or None values; in compilation of .converted_fun at 0x7fd64d905000>, found return value of type JAXTrace, which is not a Tensor or ExtensionType. ```  What jax/jaxlib version are you using? jax v0.3.16, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Python 3.10, Linux",2022-08-28T01:26:07Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/12133
1402,"以下是一个github上的jax下的一个issue, 标题是(generate lax.slice instead of lax.gather for more indexing cases)， 内容是 (Followup to CC(generate lax.slice instead of lax.gather for x[] or x[:]), supporting more cases. Still might need some additional test coverage... e.g. ```python In [1]: import jax In [2]: import jax.numpy as jnp In [3]: x = jnp.arange(24).reshape(2, 3, 4) In [4]: jax.make_jaxpr(lambda x: x[0, 1, 1:3])(x) { lambda ; a:i32[2,3,4]. let     b:i32[1,1,2] = slice[       limit_indices=(1, 2, 3)       start_indices=(0, 1, 1)       strides=(1, 1, 1)     ] a     c:i32[2] = squeeze[dimensions=(0, 1)] b   in (c,) } ``` previously the output was: ```python { lambda ; a:i32[2,3,4]. let     b:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     c:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 1     d:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 1     e:i32[3] = concatenate[dimension=0] b c d     f:i32[2] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(0, 1), start_index_map=(0, 1, 2))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(1, 1, 2)       unique_indices=True     ] a e   in (f,) } ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,generate lax.slice instead of lax.gather for more indexing cases,"Followup to CC(generate lax.slice instead of lax.gather for x[] or x[:]), supporting more cases. Still might need some additional test coverage... e.g. ```python In [1]: import jax In [2]: import jax.numpy as jnp In [3]: x = jnp.arange(24).reshape(2, 3, 4) In [4]: jax.make_jaxpr(lambda x: x[0, 1, 1:3])(x) { lambda ; a:i32[2,3,4]. let     b:i32[1,1,2] = slice[       limit_indices=(1, 2, 3)       start_indices=(0, 1, 1)       strides=(1, 1, 1)     ] a     c:i32[2] = squeeze[dimensions=(0, 1)] b   in (c,) } ``` previously the output was: ```python { lambda ; a:i32[2,3,4]. let     b:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     c:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 1     d:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 1     e:i32[3] = concatenate[dimension=0] b c d     f:i32[2] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(0, 1), start_index_map=(0, 1, 2))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(1, 1, 2)       unique_indices=True     ] a e   in (f,) } ```",2022-08-24T22:28:34Z,pull ready,closed,1,1,https://github.com/jax-ml/jax/issues/12091,Need to rebase on CC(pmap_test: disable DeviceBufferToArray when jax_array=True) to fix failing tests
393,"以下是一个github上的jax下的一个issue, 标题是([XLA:GPU] Allow simplifying lowering-precision-conversions by default)， 内容是 ([XLA:GPU] Allow simplifying loweringprecisionconversions by default This might lead to the output having higher precision than specified by HLO.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[XLA:GPU] Allow simplifying lowering-precision-conversions by default,[XLA:GPU] Allow simplifying loweringprecisionconversions by default This might lead to the output having higher precision than specified by HLO.,2022-08-23T12:34:13Z,,closed,0,0,https://github.com/jax-ml/jax/issues/12052
4022,"以下是一个github上的jax下的一个issue, 标题是(BUG: `custom_vjp` omits `jax.ensure_compile_time_eval()`)， 内容是 ( Description Hi everyone, I am writing a custom VJP rule and it is raising an error that I do not understand. Let me explain: 1. I have a function  `_f` that just calls `_f_test` 2. `_f_test` computes `x**s` 3. `x` is an input and `s` is the sum of `[0,...,N]`, `N` being the number of steps between `t0` and `t1` with step of size `step_size` 4. `s` is computed with the help of `jax.lax.cond` and `jax.lax.while_loop` The code to compute `s` looks like this. The most important take away here is that we use `jax.lax.cond` and `jax.lax.while_loop`. ```python import jax import jax.numpy as jnp from functools import partial from collections import namedtuple from jax import lax def _clip_to_end(tnext, t1):          adapted from the Diffrax library 	if tnext.dtype is jnp.dtype(""float64""): 		tol = 1e10 	else: 		tol = 1e5 		clip = tnext > t1  tol 	return jnp.where(clip, t1, tnext) def _get_number_steps(t0, t1, step_size):          adapted from the Diffrax library 	"""""" 	A function that computes the number of steps between t0 and t1 	with ah given step size.  	"""""" 	carry = namedtuple(""carry"", ""t t1 step_size nb_step"") 	def _cond_fn(C): 		t, t1, step_size, nb_step = C 	return t < t1 	def _body_fn(C): 		_t, t1, step_size, nb_step = C 	return carry(_clip_to_end(_t + step_size, t1), t1, step_size, nb_step + 1) 	t0, t1 = lax.cond(t0 <= t1, lambda _: (t0, t1), lambda _: (t1, t0), None) 	c = carry(t0, t1, step_size, 1) 	last_t = lax.while_loop(_cond_fn, _body_fn, c) 	return last_t.nb_step ``` Then the function we are interested in looks like this: ```python def _f_test(x, t0,t1,step_size): 	with jax.ensure_compile_time_eval(): 		N = _get_number_steps( 		t0, t1, step_size 		)  	l = jnp.arange(N) 	s = jnp.sum(l) 	return x**s (jax.jit, static_argnums = (1,2,3)) def _f(x,t0,t1,step_size): 	jax.debug.print(""t0 = {x}, t1 = {y}, step = {z}"", x=t0, y=t1, z=step_size) 	return _f_test(x,t0,t1,step_size) print(_f(2., 0. , 0.1, 0.01)) ``` And this works like a charm, no problem here !  The problem arises if we derive a `custom_vjp` rule: ```python .custom_vjp def _f_test(x, t0,t1,step_size):     with jax.ensure_compile_time_eval():   Ensure that the computation is done, even though it is compiled.         N = _get_number_steps(             t0, t1, step_size         )   Static value is needed for linspace when we jit the function.     l = jnp.arange(N)  ConcretizationTypeError here     s = jnp.sum(l)     return x**s def _f_test_fwd(x, t0,t1,step_size):     return _f_test(x, t0, t1, step_size), (x, t0, t1, step_size) def _f_test_bwd(residual, g):     x, t0, t1, step_size = residual     with jax.ensure_compile_time_eval():   Ensure that the computation is done, even though it is compiled.         N = _get_number_steps(             t0, t1, step_size         )   Static value is needed for linspace when we jit the function.     l = jnp.arange(N)     s = jnp.sum(l)     return g*s*x**(s1), None, None, None _f_test.defvjp(_f_test_fwd, _f_test_bwd) (jax.jit, static_argnums = (1,2,3)) def _f(x,t0,t1,step_size):     jax.debug.print(""t0 = {x}, t1 = {y}, step = {z}"", x=t0, y=t1, z=step_size)     return _f_test(x,t0,t1,step_size) print(_f(2., 0. , 0.1, 0.01)) ``` This will return a `ConcretizationTypeError` because `N` is not a static value anymore. It feels like `custom_vjp` is not taking into account `jax.ensure_compile_time_eval()`.  Does anyone know why this is the case ? Is this a bug or an intended behavior ? If so does anyone have a clue on how to achieve what I am trying to do ? Thank you for any input !  PS: This was originally a discussion CC(未找到相关数据)  but I believe it is a bug, hence the opened issue.  What jax/jaxlib version are you using? jax v0.3.16, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Mac, python 3.9.12)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,BUG: `custom_vjp` omits `jax.ensure_compile_time_eval()`," Description Hi everyone, I am writing a custom VJP rule and it is raising an error that I do not understand. Let me explain: 1. I have a function  `_f` that just calls `_f_test` 2. `_f_test` computes `x**s` 3. `x` is an input and `s` is the sum of `[0,...,N]`, `N` being the number of steps between `t0` and `t1` with step of size `step_size` 4. `s` is computed with the help of `jax.lax.cond` and `jax.lax.while_loop` The code to compute `s` looks like this. The most important take away here is that we use `jax.lax.cond` and `jax.lax.while_loop`. ```python import jax import jax.numpy as jnp from functools import partial from collections import namedtuple from jax import lax def _clip_to_end(tnext, t1):          adapted from the Diffrax library 	if tnext.dtype is jnp.dtype(""float64""): 		tol = 1e10 	else: 		tol = 1e5 		clip = tnext > t1  tol 	return jnp.where(clip, t1, tnext) def _get_number_steps(t0, t1, step_size):          adapted from the Diffrax library 	"""""" 	A function that computes the number of steps between t0 and t1 	with ah given step size.  	"""""" 	carry = namedtuple(""carry"", ""t t1 step_size nb_step"") 	def _cond_fn(C): 		t, t1, step_size, nb_step = C 	return t < t1 	def _body_fn(C): 		_t, t1, step_size, nb_step = C 	return carry(_clip_to_end(_t + step_size, t1), t1, step_size, nb_step + 1) 	t0, t1 = lax.cond(t0 <= t1, lambda _: (t0, t1), lambda _: (t1, t0), None) 	c = carry(t0, t1, step_size, 1) 	last_t = lax.while_loop(_cond_fn, _body_fn, c) 	return last_t.nb_step ``` Then the function we are interested in looks like this: ```python def _f_test(x, t0,t1,step_size): 	with jax.ensure_compile_time_eval(): 		N = _get_number_steps( 		t0, t1, step_size 		)  	l = jnp.arange(N) 	s = jnp.sum(l) 	return x**s (jax.jit, static_argnums = (1,2,3)) def _f(x,t0,t1,step_size): 	jax.debug.print(""t0 = {x}, t1 = {y}, step = {z}"", x=t0, y=t1, z=step_size) 	return _f_test(x,t0,t1,step_size) print(_f(2., 0. , 0.1, 0.01)) ``` And this works like a charm, no problem here !  The problem arises if we derive a `custom_vjp` rule: ```python .custom_vjp def _f_test(x, t0,t1,step_size):     with jax.ensure_compile_time_eval():   Ensure that the computation is done, even though it is compiled.         N = _get_number_steps(             t0, t1, step_size         )   Static value is needed for linspace when we jit the function.     l = jnp.arange(N)  ConcretizationTypeError here     s = jnp.sum(l)     return x**s def _f_test_fwd(x, t0,t1,step_size):     return _f_test(x, t0, t1, step_size), (x, t0, t1, step_size) def _f_test_bwd(residual, g):     x, t0, t1, step_size = residual     with jax.ensure_compile_time_eval():   Ensure that the computation is done, even though it is compiled.         N = _get_number_steps(             t0, t1, step_size         )   Static value is needed for linspace when we jit the function.     l = jnp.arange(N)     s = jnp.sum(l)     return g*s*x**(s1), None, None, None _f_test.defvjp(_f_test_fwd, _f_test_bwd) (jax.jit, static_argnums = (1,2,3)) def _f(x,t0,t1,step_size):     jax.debug.print(""t0 = {x}, t1 = {y}, step = {z}"", x=t0, y=t1, z=step_size)     return _f_test(x,t0,t1,step_size) print(_f(2., 0. , 0.1, 0.01)) ``` This will return a `ConcretizationTypeError` because `N` is not a static value anymore. It feels like `custom_vjp` is not taking into account `jax.ensure_compile_time_eval()`.  Does anyone know why this is the case ? Is this a bug or an intended behavior ? If so does anyone have a clue on how to achieve what I am trying to do ? Thank you for any input !  PS: This was originally a discussion CC(未找到相关数据)  but I believe it is a bug, hence the opened issue.  What jax/jaxlib version are you using? jax v0.3.16, jaxlib v0.3.15  Which accelerator(s) are you using? CPU  Additional System Info Mac, python 3.9.12",2022-08-22T22:21:13Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/12047,"Digging into this a bit more, it does not seem that there is a problem between `custom_vjp`and `jax.ensure_compile_time_eval()` but that `custom_vjp` will trace all its inputs. I think this can be seen in the following example: ```python .custom_vjp def _f_test(x, N):      l = jnp.arange(N)     s = jnp.sum(l)     return x**s def _f_test_fwd(x, N):     jax.debug.print(""Entering fwd"")     return _f_test(x, N), (x,  N) def _f_test_bwd(residual, g):     x,  N = residual     l = jnp.arange(N)     s = jnp.sum(l)     return g*s*x**(s1), N _f_test.defvjp(_f_test_fwd, _f_test_bwd) (jax.jit, static_argnums = (1,2,3)) def _f(x,t0,t1,step_size):     with jax.ensure_compile_time_eval():           N = _get_number_steps(             t0, t1, step_size         )   Static value is needed for linspace when we jit the function.     jax.debug.print(""t0 = {x}, t1 = {y}, step = {z}, N = {t}"", x=t0, y=t1, z=step_size, t=N)     return _f_test(x,N) print(_f(2., 0. , 0.1, 0.01)) ``` Before `_f_test`, `N` is a static value but after entering `_f_test`, which is wrapped by `.custom_vjp` it becomes a traced array.","I think you need to make `N` a static argument to the custom VJP.  `nondiff_argnums` is the way to specify static arguments for custom VJP.  Your code works with this subsitution: ```python (jax.custom_vjp, nondiff_argnums=(1,)) ``` > `N` is a static value but after entering [a] custom_vjp it becomes a traced array. The staticvsdynamic question is an aspect of the decorated functions—it's not bound to the argument in some way.","Thank you ! I thought `nondiff_arguments` was unrelated to the static/traced characteristic. Thank you also for the second comment, it was not obvious to me, from the documentation, that the staticvsdynamic question is an aspect of the decorated function and not the argument. Everything makes much more sense now ! "
1751,"以下是一个github上的jax下的一个issue, 标题是(jax.dispatch decorator - for improved readability)， 内容是 (I would like to request a `jax.dispatch` decorator that can be used to transform a function into a function supporting single/multiple dispatch on its typed arguments. The main advantage of such a decorator is    a) improved code readability   b) less boilerplate code   c) no runtime overhead, since dispatch happens at JITcompile time  Consider e.g. the following scenario ```python from typing import NamedTuple, Union class Batch_1D(NamedTuple):     x: jnp.ndarray class Batch_2D(NamedTuple):     x: jnp.ndarray def _flatten_batch_dim(batch: Batch_2D) > Batch_1D:     x = batch.x.reshape((1,)+batch.x.shape[2:])     return Batch_1D(x) .jit def flatten_batch_dim(batch: Union[Batch_1D, Batch_2D]) > Batch_1D:     if isinstance(batch, Batch_2D):         batch = _flatten_batch_dim(batch)      return batch  ``` Now, the type of my data determines the function behavior. E.g.  ```python batch = Batch_1D(x = jnp.ones((16,32,5))) print(flatten_batch_dim(batch).x.shape) ``` ``` (16,32,5) ``` whereas ```python batch = Batch_2D(x = jnp.ones((16,32,5))) print(flatten_batch_dim(batch).x.shape) ``` ``` (512,5) ``` This could be more beautifully achieved using dispatch. E.g. ```python from typing import NamedTuple class Batch_1D(NamedTuple):     x: jnp.ndarray class Batch_2D(NamedTuple):     x: jnp.ndarray .dispatch  def flatten_batch_dim(batch: Batch_2D) > Batch_1D:     x = batch.x.reshape((1,)+batch.x.shape[2:])     return Batch_1D(x) .dispatch def flatten_batch_dim(batch: Batch_1D) > Batch_1D:     return batch  ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.dispatch decorator - for improved readability,"I would like to request a `jax.dispatch` decorator that can be used to transform a function into a function supporting single/multiple dispatch on its typed arguments. The main advantage of such a decorator is    a) improved code readability   b) less boilerplate code   c) no runtime overhead, since dispatch happens at JITcompile time  Consider e.g. the following scenario ```python from typing import NamedTuple, Union class Batch_1D(NamedTuple):     x: jnp.ndarray class Batch_2D(NamedTuple):     x: jnp.ndarray def _flatten_batch_dim(batch: Batch_2D) > Batch_1D:     x = batch.x.reshape((1,)+batch.x.shape[2:])     return Batch_1D(x) .jit def flatten_batch_dim(batch: Union[Batch_1D, Batch_2D]) > Batch_1D:     if isinstance(batch, Batch_2D):         batch = _flatten_batch_dim(batch)      return batch  ``` Now, the type of my data determines the function behavior. E.g.  ```python batch = Batch_1D(x = jnp.ones((16,32,5))) print(flatten_batch_dim(batch).x.shape) ``` ``` (16,32,5) ``` whereas ```python batch = Batch_2D(x = jnp.ones((16,32,5))) print(flatten_batch_dim(batch).x.shape) ``` ``` (512,5) ``` This could be more beautifully achieved using dispatch. E.g. ```python from typing import NamedTuple class Batch_1D(NamedTuple):     x: jnp.ndarray class Batch_2D(NamedTuple):     x: jnp.ndarray .dispatch  def flatten_batch_dim(batch: Batch_2D) > Batch_1D:     x = batch.x.reshape((1,)+batch.x.shape[2:])     return Batch_1D(x) .dispatch def flatten_batch_dim(batch: Batch_1D) > Batch_1D:     return batch  ```",2022-08-20T10:08:14Z,enhancement,closed,0,8,https://github.com/jax-ml/jax/issues/12031,"If I understand correctly, I think the mechanism you have in mind is already provided by the builtin `functools.singledispatch` decorator. Here's an example: ```python import jax.numpy as jnp from typing import Any, NamedTuple from functools import singledispatch class Batch_1D(NamedTuple):     x: jnp.ndarray class Batch_2D(NamedTuple):     x: jnp.ndarray  def flatten_batch_dim(batch: Any) > Batch_1D:     raise NotImplementedError(f""flatten_batch_dim for type {type(batch)}"") .register def _(batch: Batch_2D) > Batch_1D:     x = batch.x.reshape((1,)+batch.x.shape[2:])     return Batch_1D(x) .register def _(batch: Batch_1D) > Batch_1D:     return batch  batch = Batch_2D(x = jnp.ones((16,32,5))) print(flatten_batch_dim(batch).x.shape)  (512, 5) batch = Batch_1D(x = jnp.ones((16,32,5))) print(flatten_batch_dim(batch).x.shape)  (16, 32, 5) ```","Ah poor example on my part. I was more hoping for a jitable version of e.g. `fastcorelike dispatch`, see https://fastcore.fast.ai/dispatch.htmltypedispatchdecorator It's quite powerful but unfortunately this implementation does not like jit ;) ","Have you tried plum? I've not tested to see how it works under `jax.jit`. But I think the argument to be made here is that function dispatching is really a Python thing, not a JAX thing. (In passing  there's been some offline discussion about trying to get plum working with jaxtyping, which would be pretty neat, and may solve your use case.)",Can confirm that plum (including multiple dispatch) works indeed perfectly under jit. Very cool!,"Unfortunately, both `plum` and `functools` can not handle singledispatch (or multiple dispatch) when using a `vmap`transformation. Example:  ```python .singledispatch def f(x):     raise NotImplementedError() .register  def _(x: jnp.ndarray):     print(""Arrayoperation"") .vmap  def g(x: jnp.ndarray):     return f(x) g(jnp.ones((10,1))) >>> NotImplementedError ``` Edit: *fixed typo","The reason this doesn't work is likely because `jnp.ndarray` is not actually in the class hierarchy of either arrays or tracers; you'd probably have to annotate with `jax.DeviceArray` and `jax.core.Tracer` directly if you want this dispatch method to work. We're exploring changing that in CC(JEP: Type Annotations), but regardless of the outcome there I'm still of the opinion that a multiple dispatch decorator is not a good fit to include in JAX.","according to jake this should work ```python .singledispatch def f(x):     raise NotImplementedError() .register(jnp.ndarray) .register(jax.core.Tracer) def _(x):     print(""Arrayoperation"") .vmap  def g(x: jnp.ndarray):     return f(x) g(jnp.ones((10,1)))  Arrayoperation ``` Also If you want multiple dispatch using only `functools` , try using the following code snippet https://github.com/ASEM000/PyTreeClass/blob/main/pytreeclass/_src/dispatch.py It seems that python `functools` implementation is a bit faster ```python  Tested on mac m1 CPU from multipledispatch import dispatch as dispatch_md from plum import dispatch as dispatch_plum from functools import singledispatch as dispatch_std (int) def f_md(x):    return x  def f_plum(x: int):    return x def f_native(x):     return x  def f_std(x): ... .register(int) def _(x):     return x f_md(1); f_plum(1);   Run once to populate cache. %timeit f_native(1)  39.6 ns ± 0.629 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each) %timeit f_md(1)  281 ns ± 2.08 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) %timeit f_plum(1)  337 ns ± 5.64 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) %timeit f_std(1)  267 ns ± 1.97 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each) ```","Yes i guess this does work. It is just a very unpretty amount of decorators which kind of defeats its purpose (improved readability) in the first place.  Either way, thank you all. I will close this."
2221,"以下是一个github上的jax下的一个issue, 标题是(BUG: pjit results in incorrect/incompatible shapes with distributed setup)， 内容是 ( Description Note: I can only reproduce this when I'm using more than 1 machine (namely, the v3256 cloud TPU VM I'm getting to play with) I was trying to implement ZeRO3 (along the lines hinted in https://github.com/googleresearch/t5x/blob/main/docs/usage/partitioning.mddataparallelwithparametergather) , when I ran into a bug/limitation in pjit. When using pjit, if I try to elementwisemultiply two arrays together that have ""conflicting"" PartitionSpecs I get a shape error that I don't get when not using pjit. (They conflict in that one is sharded along the first axis and the other along the second using the same axis of the mesh) ```python import jax import jax.numpy as jnp from jax.experimental.pjit import pjit from jax.interpreters.pxla import Mesh, PartitionSpec import numpy as np devices = jax.devices() mesh = Mesh(np.array(devices).reshape((1, 4)), [""data"", ""model""]) with mesh:     spec = PartitionSpec(""data"")     array = jnp.zeros(512, dtype=jnp.bfloat16)     def foo(weight, data):         return weight.reshape(1, 1) * data      ok     out1 = foo(array, jnp.zeros((256, 512)))     print(out1.shape)      error     pjit_foo_novmap = pjit(foo, in_axis_resources=(spec, PartitionSpec(""data"", None)), out_axis_resources=PartitionSpec(""data"", None))     out = pjit_foo_novmap(array, jnp.zeros((256, 512))) ``` The error is `TypeError: mul got incompatible shapes for broadcasting: (1, 16384), (8192, 512).` Using with_sharding_constraint to force weight to be replicated doesn't fix it. i.e. I still get the same error if I do: ```python  def foo(weight, data):         return with_sharding_constraint(weight, PartitionSpec(None)).reshape(1, 1) * data ``` 16384 is 32 * 512 and 8192 is 32 * 256, and 32 is the number of worker VMs  What jax/jaxlib version are you using? jax 0.3.16 (main from a week ago or so)/ jaxlib 0.3.14  Which accelerator(s) are you using? TPU (must be more than one VM)  Additional System Info Cloud TPU VM v3256, Python 3.10.5,)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,BUG: pjit results in incorrect/incompatible shapes with distributed setup," Description Note: I can only reproduce this when I'm using more than 1 machine (namely, the v3256 cloud TPU VM I'm getting to play with) I was trying to implement ZeRO3 (along the lines hinted in https://github.com/googleresearch/t5x/blob/main/docs/usage/partitioning.mddataparallelwithparametergather) , when I ran into a bug/limitation in pjit. When using pjit, if I try to elementwisemultiply two arrays together that have ""conflicting"" PartitionSpecs I get a shape error that I don't get when not using pjit. (They conflict in that one is sharded along the first axis and the other along the second using the same axis of the mesh) ```python import jax import jax.numpy as jnp from jax.experimental.pjit import pjit from jax.interpreters.pxla import Mesh, PartitionSpec import numpy as np devices = jax.devices() mesh = Mesh(np.array(devices).reshape((1, 4)), [""data"", ""model""]) with mesh:     spec = PartitionSpec(""data"")     array = jnp.zeros(512, dtype=jnp.bfloat16)     def foo(weight, data):         return weight.reshape(1, 1) * data      ok     out1 = foo(array, jnp.zeros((256, 512)))     print(out1.shape)      error     pjit_foo_novmap = pjit(foo, in_axis_resources=(spec, PartitionSpec(""data"", None)), out_axis_resources=PartitionSpec(""data"", None))     out = pjit_foo_novmap(array, jnp.zeros((256, 512))) ``` The error is `TypeError: mul got incompatible shapes for broadcasting: (1, 16384), (8192, 512).` Using with_sharding_constraint to force weight to be replicated doesn't fix it. i.e. I still get the same error if I do: ```python  def foo(weight, data):         return with_sharding_constraint(weight, PartitionSpec(None)).reshape(1, 1) * data ``` 16384 is 32 * 512 and 8192 is 32 * 256, and 32 is the number of worker VMs  What jax/jaxlib version are you using? jax 0.3.16 (main from a week ago or so)/ jaxlib 0.3.14  Which accelerator(s) are you using? TPU (must be more than one VM)  Additional System Info Cloud TPU VM v3256, Python 3.10.5,",2022-08-20T00:12:08Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/12024,"It seems to work for me locally with 4 devices:  ``` In [2]: import jax    ...: import jax.numpy as jnp    ...: from jax.experimental.pjit import pjit    ...: from jax.interpreters.pxla import Mesh, PartitionSpec    ...: import numpy as np    ...:    ...: devices = jax.devices()    ...: mesh = Mesh(np.array(devices).reshape((1, 4)), [""data"", ""model""])    ...:    ...: with mesh:    ...:     spec = PartitionSpec(""data"")    ...:     array = jnp.zeros(512, dtype=jnp.bfloat16)    ...:    ...:     def foo(weight, data):    ...:         return weight.reshape(1, 1) * data    ...:    ...:      ok    ...:     out1 = foo(array, jnp.zeros((256, 512)))    ...:     print(out1.shape)    ...:    ...:      error    ...:     pjit_foo_novmap = pjit(foo, in_axis_resources=(spec, PartitionSpec(""data"", None)), out_axis_resources=PartitionSpec(""data"", None))    ...:     out = pjit_foo_novmap(array, jnp.zeros((256, 512)))    ...:     print(out.shape)    ...: (256, 512) (256, 512) ``` ``` In [6]: jaxlib.__version__ Out[6]: '0.3.15' In [7]: jax.__version__ Out[7]: '0.3.16' ```","It has to be more than one node, not just multiple devices On Mon, Aug 22, 2022, 7:50 AM Yash Katariya ***@***.***> wrote: > It seems to work for me locally with 4 devices: > > In [2]: import jax >    ...: import jax.numpy as jnp >    ...: from jax.experimental.pjit import pjit >    ...: from jax.interpreters.pxla import Mesh, PartitionSpec >    ...: import numpy as np >    ...: >    ...: devices = jax.devices() >    ...: mesh = Mesh(np.array(devices).reshape((1, 4)), [""data"", ""model""]) >    ...: >    ...: with mesh: >    ...:     spec = PartitionSpec(""data"") >    ...:     array = jnp.zeros(512, dtype=jnp.bfloat16) >    ...: >    ...:     def foo(weight, data): >    ...:         return weight.reshape(1, 1) * data >    ...: >    ...:      ok >    ...:     out1 = foo(array, jnp.zeros((256, 512))) >    ...:     print(out1.shape) >    ...: >    ...:      error >    ...:     pjit_foo_novmap = pjit(foo, in_axis_resources=(spec, PartitionSpec(""data"", None)), out_axis_resources=PartitionSpec(""data"", None)) >    ...:     out = pjit_foo_novmap(array, jnp.zeros((256, 512))) >    ...:     print(out.shape) >    ...: > (256, 512) > (256, 512) > > In [6]: jaxlib.__version__ > Out[6]: '0.3.15' > > In [7]: jax.__version__ > Out[7]: '0.3.16' > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Got it! I'll try to reproduce on 4 processes and see if I can hit this error. If I still don't hit that then I can try on 256 devices.,I am hitting this on 4 processes too. ,"I think the problem is that `arr` and the `data` value is hostlocal and pjit internally converts it to a global value which is where the mismatch is coming from. Since you are using pjit (without GDA) on multiple processes, you need to pass hostlocal shaped inputs to pjit. Can you try doing that and see if it works?","ah, right ok. Switching to pjit initialization of the arrays fixed it. Thank you! I've been bitten by this before with xmap but didn't realize I was doing it again!  I keep assuming that if I pass in arrays created identically on different nodes they get treated as though they were equal/replicated, and not concatenated. I seem to have missed the `note` in the pjit docs! Sorry about that.","On a general note, we are fixing this by making pjit accept only global arrays. So there is no local aval to global aval conversion. GDA takes a step towards this but `jax.Array` will be a better improvement. ",Gotcha. That sounds great!
568,"以下是一个github上的jax下的一个issue, 标题是(BUG: module 'jaxlib.pocketfft' has no attribute 'pocketfft')， 内容是 ( Description Hi Jax developers! Tried 2 GPTJ colab playbooks, neither worked. At some point, whatever I do, I get: ```python AttributeError: module 'jaxlib.pocketfft' has no attribute 'pocketfft' ```  What jax/jaxlib version are you using? jax==0.2.12  Which accelerator(s) are you using? TPU  Additional System Info Google Colab)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,BUG: module 'jaxlib.pocketfft' has no attribute 'pocketfft'," Description Hi Jax developers! Tried 2 GPTJ colab playbooks, neither worked. At some point, whatever I do, I get: ```python AttributeError: module 'jaxlib.pocketfft' has no attribute 'pocketfft' ```  What jax/jaxlib version are you using? jax==0.2.12  Which accelerator(s) are you using? TPU  Additional System Info Google Colab",2022-08-18T13:08:50Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/11980,What `jaxlib` version do you have installed? (Also note `jax` 0.2.12 is quite old.),Closing because I'm pretty confident this will not reproduce with the current `jax` and `jaxlib` releases.,"jaxlib 0.3.10 , jax 0.3.23 , now I have the same issue, and I need your help,please!Thanks!"," You are using incompatible versions of `jax` and `jaxlib`. Update your jaxlib release. In fact I'm not quite sure how you saw this error because if you try the versions you have you should get the following error when importing `jax`: ``` RuntimeError: jaxlib is version 0.3.10, but this version of jax requires version >= 0.3.15. ```", I am getting this error with `jaxlib` `0.4.3` and see this error: ``` ImportError: cannot import name 'pocketfft' from 'jaxlib' (./miniconda3/envs/courseranlp/lib/python3.9/sitepackages/jaxlib/__init__.py ```
3723,"以下是一个github上的jax下的一个issue, 标题是(BUG: Cannot build jaxlib on CentOS)， 内容是 ( Description I am trying to follow these instructions to build jaxlib with CUDA support on a Slurm computing cluster. When I run `python3 build/build.py enable_cuda`, I get the following error: ``` Downloading bazel from: https://github.com/bazelbuild/bazel/releases/download/5.1.1/bazel5.1.1linuxx86_64 Traceback (most recent call last):   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 1348, in do_open     h.request(req.get_method(), req.selector, req.data, headers,   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1282, in request     self._send_request(method, url, body, headers, encode_chunked)   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1328, in _send_request     self.endheaders(body, encode_chunked=encode_chunked)   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1277, in endheaders     self._send_output(message_body, encode_chunked=encode_chunked)   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1037, in _send_output     self.send(msg)   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 975, in send     self.connect()   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1454, in connect     self.sock = self._context.wrap_socket(self.sock,   File "".../anaconda3/envs/py10/lib/python3.10/ssl.py"", line 512, in wrap_socket     return self.sslsocket_class._create(   File "".../anaconda3/envs/py10/lib/python3.10/ssl.py"", line 1070, in _create     self.do_handshake()   File "".../anaconda3/envs/py10/lib/python3.10/ssl.py"", line 1341, in do_handshake     self._sslobj.do_handshake() ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997) During handling of the above exception, another exception occurred: Traceback (most recent call last):   File "".../jax/build/build.py"", line 555, in      main()   File "".../jax/build/build.py"", line 473, in main     bazel_path, bazel_version = get_bazel_path(args.bazel_path)   File "".../jax/build/build.py"", line 195, in get_bazel_path     for path in filter(None, get_bazel_paths(bazel_path_flag)):   File "".../jax/build/build.py"", line 183, in get_bazel_paths     yield download_and_verify_bazel()   File "".../jax/build/build.py"", line 150, in download_and_verify_bazel     tmp_path, _ = urlretrieve(uri, None,   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 241, in urlretrieve     with contextlib.closing(urlopen(url, data)) as fp:   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 216, in urlopen     return opener.open(url, data, timeout)   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 519, in open     response = self._open(req, data)   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 536, in _open     result = self._call_chain(self.handle_open, protocol, protocol +   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 496, in _call_chain     result = func(*args)   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 1391, in https_open     return self.do_open(http.client.HTTPSConnection, req,   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 1351, in do_open     raise URLError(err) urllib.error.URLError:  ```  What jax/jaxlib version are you using? jaxlib v0.3.15  Which accelerator(s) are you using? GPU  Additional System Info Python 3.10.4, CentOS 7 (Core))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,BUG: Cannot build jaxlib on CentOS," Description I am trying to follow these instructions to build jaxlib with CUDA support on a Slurm computing cluster. When I run `python3 build/build.py enable_cuda`, I get the following error: ``` Downloading bazel from: https://github.com/bazelbuild/bazel/releases/download/5.1.1/bazel5.1.1linuxx86_64 Traceback (most recent call last):   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 1348, in do_open     h.request(req.get_method(), req.selector, req.data, headers,   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1282, in request     self._send_request(method, url, body, headers, encode_chunked)   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1328, in _send_request     self.endheaders(body, encode_chunked=encode_chunked)   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1277, in endheaders     self._send_output(message_body, encode_chunked=encode_chunked)   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1037, in _send_output     self.send(msg)   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 975, in send     self.connect()   File "".../anaconda3/envs/py10/lib/python3.10/http/client.py"", line 1454, in connect     self.sock = self._context.wrap_socket(self.sock,   File "".../anaconda3/envs/py10/lib/python3.10/ssl.py"", line 512, in wrap_socket     return self.sslsocket_class._create(   File "".../anaconda3/envs/py10/lib/python3.10/ssl.py"", line 1070, in _create     self.do_handshake()   File "".../anaconda3/envs/py10/lib/python3.10/ssl.py"", line 1341, in do_handshake     self._sslobj.do_handshake() ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997) During handling of the above exception, another exception occurred: Traceback (most recent call last):   File "".../jax/build/build.py"", line 555, in      main()   File "".../jax/build/build.py"", line 473, in main     bazel_path, bazel_version = get_bazel_path(args.bazel_path)   File "".../jax/build/build.py"", line 195, in get_bazel_path     for path in filter(None, get_bazel_paths(bazel_path_flag)):   File "".../jax/build/build.py"", line 183, in get_bazel_paths     yield download_and_verify_bazel()   File "".../jax/build/build.py"", line 150, in download_and_verify_bazel     tmp_path, _ = urlretrieve(uri, None,   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 241, in urlretrieve     with contextlib.closing(urlopen(url, data)) as fp:   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 216, in urlopen     return opener.open(url, data, timeout)   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 519, in open     response = self._open(req, data)   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 536, in _open     result = self._call_chain(self.handle_open, protocol, protocol +   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 496, in _call_chain     result = func(*args)   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 1391, in https_open     return self.do_open(http.client.HTTPSConnection, req,   File "".../anaconda3/envs/py10/lib/python3.10/urllib/request.py"", line 1351, in do_open     raise URLError(err) urllib.error.URLError:  ```  What jax/jaxlib version are you using? jaxlib v0.3.15  Which accelerator(s) are you using? GPU  Additional System Info Python 3.10.4, CentOS 7 (Core)",2022-08-17T23:38:21Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/11970,Possibly related issue: CC(Cannot build jaxlib on Windows through proxy server),It sounds to me like something is broken with SSL certificate chain in your `python` implementation. This isn't really JAX specific: this means that `python` is unable to open an https connection to `github.com`. I suspect you need to fix something about your Python installation or the SSL key store that it is using. (Or just use a prebuilt `jaxlib`.) It's possible if you download `bazel` 5.1.1 and put it in your `PATH` you may get past this step. I'm not sure there's any action we can take here.
1202,"以下是一个github上的jax下的一个issue, 标题是(BUG: build warning of download tensorflow runtime package)， 内容是 ( Description Tried build jaxlib in WSL2 on Windows 11 and got below warnings. Seems the files cannot be downloaded with below URL, but the files are available in the original URL like ""https://github.com/tensorflow/runtime/archive/e3da90c2cb1bd594a90ffb06b80f6d8df47c4e11.tar.gz"". Are these build warnings critical for a successful build? ```console WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/e3da90c2cb1bd594a90ffb06b80f6d8df47c4e11.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/2c3ca3b684bb2b188d977d47548e79dc559fb8ad.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found ```  What jax/jaxlib version are you using? Build from latest source  Which accelerator(s) are you using? CPU  Additional System Info Windows 11, WSL2.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,BUG: build warning of download tensorflow runtime package," Description Tried build jaxlib in WSL2 on Windows 11 and got below warnings. Seems the files cannot be downloaded with below URL, but the files are available in the original URL like ""https://github.com/tensorflow/runtime/archive/e3da90c2cb1bd594a90ffb06b80f6d8df47c4e11.tar.gz"". Are these build warnings critical for a successful build? ```console WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/e3da90c2cb1bd594a90ffb06b80f6d8df47c4e11.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/2c3ca3b684bb2b188d977d47548e79dc559fb8ad.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found ```  What jax/jaxlib version are you using? Build from latest source  Which accelerator(s) are you using? CPU  Additional System Info Windows 11, WSL2.",2022-08-17T08:48:06Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/11959,These are benign warnings. They happen because we haven't added that particular file to a mirror repository.
7234,"以下是一个github上的jax下的一个issue, 标题是(introduce key-element-type arrays and overhaul the Python PRNG key array)， 内容是 (Big step for CC(RNGs: key types and custom implementations). Before this change, the Python PRNG key array was a pytree type wrapping a `uint32` array. This was a stopgap that misbehaved under `vmap`, `scan`, and even `jax.tree_map`. For a while, we thought we might rely on something like the typeclass mechanisms in development (e.g. `vmappable`) to move away from a pytree. We're now taking a different approach: introducing key element types into our IR and other internal machinery. During staging, we map userfacing PRNG key arrays to abstract arrays such element type. Check it out: ```python >>> import jax >>> def f(seed): ...   key = jax.random.PRNGKey(seed) ...   key, _ = jax.random.split(key, 2) ...   return jax.random.uniform(key, (3, 7)) ...  >>> print(jax.make_jaxpr(f)(54)) { lambda ; a:i32[]. let     b:key[] = random_seed[impl=fry] a     c:key[2] = random_split[count=2] b     d:key[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] c     e:key[] = squeeze[dimensions=(0,)] d     f:key[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] c     _:key[] = squeeze[dimensions=(0,)] f     g:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] 0.0     h:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] 1.0     i:u32[3,7] = random_bits[bit_width=32 shape=(3, 7)] e     j:u32[3,7] = shift_right_logical i 9     k:u32[3,7] = or j 1065353216     l:f32[3,7] = bitcast_convert_type[new_dtype=float32] k     m:f32[3,7] = sub l 1.0     n:f32[1,1] = sub h g     o:f32[3,7] = mul m n     p:f32[3,7] = add o g     q:f32[3,7] = max g p   in (q,) } ``` This leans heavily on our recentlyintroduced extended element type capabilities from CC(prototype unfettered element types in jaxpr arrays) (and CC(defer to custom eltype for `slice` lowering rule) and CC(defer to custom eltype for `gather` lowering rule)). As a consequence, `vmap`, `scan`, etc. now work. Another consequence—because the logic for this was needed either way—is that now seeding, random bit generation, and `jax.random.{split,fold_in}` are all batchpolymorphic (and `fold_in` in particular does argument broadcasting): ```python >>> import jax.numpy as jnp >>> def f(seed): ...   keys = jax.random.split(jax.random.PRNGKey(seed), 4) ...   msgs = jnp.arange(4, dtype=jnp.uint32) ...   a = jax.random.fold_in(keys, msgs) ...   b = jax.random.fold_in(keys[None, :], msgs[:, None]) ...   c = jax.random.fold_in(keys[:, None], msgs[None, :]) ...   d = jax.random.split(keys) ...   return a, b, c, d ... >>> print(jax.make_jaxpr(f)(54)) { lambda ; a:i32[]. let     b:key[] = random_seed[impl=fry] a     c:key[4] = random_split[count=4] b     d:u32[4] = iota[dimension=0 dtype=uint32 shape=(4,)]      e:key[4] = random_fold_in c d     f:key[1,4] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 4)] c     g:u32[4,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(4, 1)] d     h:key[4,4] = random_fold_in f g     i:key[4,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(4, 1)] c     j:u32[1,4] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 4)] d     k:key[4,4] = random_fold_in i j     l:key[4,2] = random_split[count=2] c   in (e, h, k, l) } ``` We should do more argument checking as a followup if we want to allow this kind of usage. A sample of changes made to introduce keyelementtype arrays: * Introduce a new element type (`prng.KeyTy`), with the requisite IR type mapping and device result handlers, as well as lowering rules for dtypepolymorphic primitive operations. * Introduce primitives for basic RNG operations: `random_seed`, `random_bits`, `random_split`, `random_fold_in`. These primitives essentially delegate to the underlying PRNG implementation (directly so in their impl rules, and by translating their stagedout form in lowering rules). * Also introduce `random_wrap` and `random_unwrap` for ""unsafe"" conversion from/to the base `uint32` array. We need this backwards compatibility, and it's useful for tests. * Introduce some `vmap`based helpers to adapt PRNG impls (which define basic `random_bits`, `split`, etc. on scalars) to the above batchpolymorphic primitives. Most of the primitives are vectorized, but `random_fold_in` is a broadcasting binary op. * Update the `gamma` primitive rules to account for keyelementtype abstract arrays (nice simplification here). * Give PRNG implementation short string names (""tags"") for IR prettyprinting. * Update `lax.stop_gradient` to handle opaque dtypes (currently anything from the custom/extended set). * Fix up loop MLIR lowering, which assumed that shaped arrays of all dtypes have the same physical shape. * Add new tests (exercising staging, jaxprs, lowerings, ...) A sample of changes made to rework Pythonlevel PRNG key arrays: * Mimic `isinstance(x, KeyArray)` checks on abstract key arrays and tracers that carry them. * Patch (only a subset of) standard device array attributes onto PRNG key arrays. * Implement various conversion handlers (sharding, constantcreation, `device_put`). * Accept PRNG key arrays as input to `lax_numpy.transpose`. * Update tests and rename some internals. A sample of extra changes along the way: * Disallow AD on keytyped arrays in the main API. * Hoist `random_bits`'s namedshapehandling logic, which used to only take place in the threefry PRNG's `random_bits` implementation, up to the new `random_bits` traceable, so that we apply it consistently across PRNG implementations. By overhauling random stuff onto the new element type extension machinery, we're also now indirectly testing the latter more heavily. Parts of this change are really about that—further general development on internal handling of element types. Most notably, I had to teach `jax2tf` about such extensions. This involved: * Introducing a `physical_avals` view as a method on an element typedefinition. This is analogous to the existing mapping to MHLO types (`aval_to_ir_types`), but where the output is an aval with a grounded element type (and hence a direct correspondence to TF and to lowerings). * Teaching jax2tf to understand the logical vs physical aval distinction, and to trace with logical avals but work with physical ones as needed. This required fixing up various `tf_impl` rules and changing various helpers. * Writing `tf_impl` rules for `random_{seed,split,fold_in,bits}` primitives, and teaching the jax2tf test harness to unwrap keyarraytyped outputs into their physical `uint32` form when doing output comparison tests. Finally, this change leaves some unwanted `lax` and `jax.numpy` operations superficially available on key arrays during tracing/staging (e.g. under `jit`), though not outside of it. We ultimately want to disallow these and raise useful errors, and I'm leaving that for followup work. For now, applying such operations under `jit` may result in downstream errors in the middleend instead. Everything here is still guarded by `config.jax_enable_custom_prng`, whose default setting hasn't changed (it is off).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,introduce key-element-type arrays and overhaul the Python PRNG key array,"Big step for CC(RNGs: key types and custom implementations). Before this change, the Python PRNG key array was a pytree type wrapping a `uint32` array. This was a stopgap that misbehaved under `vmap`, `scan`, and even `jax.tree_map`. For a while, we thought we might rely on something like the typeclass mechanisms in development (e.g. `vmappable`) to move away from a pytree. We're now taking a different approach: introducing key element types into our IR and other internal machinery. During staging, we map userfacing PRNG key arrays to abstract arrays such element type. Check it out: ```python >>> import jax >>> def f(seed): ...   key = jax.random.PRNGKey(seed) ...   key, _ = jax.random.split(key, 2) ...   return jax.random.uniform(key, (3, 7)) ...  >>> print(jax.make_jaxpr(f)(54)) { lambda ; a:i32[]. let     b:key[] = random_seed[impl=fry] a     c:key[2] = random_split[count=2] b     d:key[1] = slice[limit_indices=(1,) start_indices=(0,) strides=(1,)] c     e:key[] = squeeze[dimensions=(0,)] d     f:key[1] = slice[limit_indices=(2,) start_indices=(1,) strides=(1,)] c     _:key[] = squeeze[dimensions=(0,)] f     g:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] 0.0     h:f32[1,1] = broadcast_in_dim[broadcast_dimensions=() shape=(1, 1)] 1.0     i:u32[3,7] = random_bits[bit_width=32 shape=(3, 7)] e     j:u32[3,7] = shift_right_logical i 9     k:u32[3,7] = or j 1065353216     l:f32[3,7] = bitcast_convert_type[new_dtype=float32] k     m:f32[3,7] = sub l 1.0     n:f32[1,1] = sub h g     o:f32[3,7] = mul m n     p:f32[3,7] = add o g     q:f32[3,7] = max g p   in (q,) } ``` This leans heavily on our recentlyintroduced extended element type capabilities from CC(prototype unfettered element types in jaxpr arrays) (and CC(defer to custom eltype for `slice` lowering rule) and CC(defer to custom eltype for `gather` lowering rule)). As a consequence, `vmap`, `scan`, etc. now work. Another consequence—because the logic for this was needed either way—is that now seeding, random bit generation, and `jax.random.{split,fold_in}` are all batchpolymorphic (and `fold_in` in particular does argument broadcasting): ```python >>> import jax.numpy as jnp >>> def f(seed): ...   keys = jax.random.split(jax.random.PRNGKey(seed), 4) ...   msgs = jnp.arange(4, dtype=jnp.uint32) ...   a = jax.random.fold_in(keys, msgs) ...   b = jax.random.fold_in(keys[None, :], msgs[:, None]) ...   c = jax.random.fold_in(keys[:, None], msgs[None, :]) ...   d = jax.random.split(keys) ...   return a, b, c, d ... >>> print(jax.make_jaxpr(f)(54)) { lambda ; a:i32[]. let     b:key[] = random_seed[impl=fry] a     c:key[4] = random_split[count=4] b     d:u32[4] = iota[dimension=0 dtype=uint32 shape=(4,)]      e:key[4] = random_fold_in c d     f:key[1,4] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 4)] c     g:u32[4,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(4, 1)] d     h:key[4,4] = random_fold_in f g     i:key[4,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(4, 1)] c     j:u32[1,4] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 4)] d     k:key[4,4] = random_fold_in i j     l:key[4,2] = random_split[count=2] c   in (e, h, k, l) } ``` We should do more argument checking as a followup if we want to allow this kind of usage. A sample of changes made to introduce keyelementtype arrays: * Introduce a new element type (`prng.KeyTy`), with the requisite IR type mapping and device result handlers, as well as lowering rules for dtypepolymorphic primitive operations. * Introduce primitives for basic RNG operations: `random_seed`, `random_bits`, `random_split`, `random_fold_in`. These primitives essentially delegate to the underlying PRNG implementation (directly so in their impl rules, and by translating their stagedout form in lowering rules). * Also introduce `random_wrap` and `random_unwrap` for ""unsafe"" conversion from/to the base `uint32` array. We need this backwards compatibility, and it's useful for tests. * Introduce some `vmap`based helpers to adapt PRNG impls (which define basic `random_bits`, `split`, etc. on scalars) to the above batchpolymorphic primitives. Most of the primitives are vectorized, but `random_fold_in` is a broadcasting binary op. * Update the `gamma` primitive rules to account for keyelementtype abstract arrays (nice simplification here). * Give PRNG implementation short string names (""tags"") for IR prettyprinting. * Update `lax.stop_gradient` to handle opaque dtypes (currently anything from the custom/extended set). * Fix up loop MLIR lowering, which assumed that shaped arrays of all dtypes have the same physical shape. * Add new tests (exercising staging, jaxprs, lowerings, ...) A sample of changes made to rework Pythonlevel PRNG key arrays: * Mimic `isinstance(x, KeyArray)` checks on abstract key arrays and tracers that carry them. * Patch (only a subset of) standard device array attributes onto PRNG key arrays. * Implement various conversion handlers (sharding, constantcreation, `device_put`). * Accept PRNG key arrays as input to `lax_numpy.transpose`. * Update tests and rename some internals. A sample of extra changes along the way: * Disallow AD on keytyped arrays in the main API. * Hoist `random_bits`'s namedshapehandling logic, which used to only take place in the threefry PRNG's `random_bits` implementation, up to the new `random_bits` traceable, so that we apply it consistently across PRNG implementations. By overhauling random stuff onto the new element type extension machinery, we're also now indirectly testing the latter more heavily. Parts of this change are really about that—further general development on internal handling of element types. Most notably, I had to teach `jax2tf` about such extensions. This involved: * Introducing a `physical_avals` view as a method on an element typedefinition. This is analogous to the existing mapping to MHLO types (`aval_to_ir_types`), but where the output is an aval with a grounded element type (and hence a direct correspondence to TF and to lowerings). * Teaching jax2tf to understand the logical vs physical aval distinction, and to trace with logical avals but work with physical ones as needed. This required fixing up various `tf_impl` rules and changing various helpers. * Writing `tf_impl` rules for `random_{seed,split,fold_in,bits}` primitives, and teaching the jax2tf test harness to unwrap keyarraytyped outputs into their physical `uint32` form when doing output comparison tests. Finally, this change leaves some unwanted `lax` and `jax.numpy` operations superficially available on key arrays during tracing/staging (e.g. under `jit`), though not outside of it. We ultimately want to disallow these and raise useful errors, and I'm leaving that for followup work. For now, applying such operations under `jit` may result in downstream errors in the middleend instead. Everything here is still guarded by `config.jax_enable_custom_prng`, whose default setting hasn't changed (it is off).",2022-08-16T22:57:55Z,pull ready,closed,2,1,https://github.com/jax-ml/jax/issues/11952,", note the jax2tf changes"
282,"以下是一个github上的jax下的一个issue, 标题是(ROCm dlpack support)， 内容是 (Addresses test failure related to DLPack support in array_interoperability_tests.py)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,ROCm dlpack support,Addresses test failure related to DLPack support in array_interoperability_tests.py,2022-08-16T19:36:20Z,pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/11947,FYI: Tensorflow PR that enables DLPack for ROCm was merged this morning https://github.com/tensorflow/tensorflow/pull/57640,  Ping!,"  Any feedback on this? Can we merge it? Also, please note ROCm support for DLPack is in Tensorflow Eager and XLA, so that prereq is met."
5060,"以下是一个github上的jax下的一个issue, 标题是(BUG: JIT session error: Cannot allocate memory)， 内容是 ( Description Hi, I am running reinforcement learning with jax, flax and jraph. Often, after a few hundreds of episodes, JAX report:  ``` JIT session error: Cannot allocate memory Fatal Python error: Segmentation fault Thread 0x00007fce7effd700 (most recent call first):   File ""/usr/lib/python3.9/threading.py"", line 316 in wait   File ""/usr/lib/python3.9/threading.py"", line 581 in wait   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/tqdm/_monitor.py"", line 60 in run   File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner   File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap Current thread 0x00007fdde7b2e740 (most recent call first):   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 717 in _execute_compiled   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 167 in    File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 101 in apply_primitive   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/core.py"", line 680 in process_primitive   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/core.py"", line 330 in bind_with_trace   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/core.py"", line 327 in bind   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 808 in reshape   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 760 in _reshape   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 740 in reshape   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jraph/_src/utils.py"", line 1025 in _expand_trailing_dimensions   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jraph/_src/utils.py"", line 1031 in    File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 196 in    File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 196 in tree_map   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jraph/_src/utils.py"", line 1097 in zero_out_padding   File ""/root/tensorrewriter/python/graph_rewriter/agents/encoder/gnn.py"", line 214 in __call__   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 651 in _call_wrapped_method   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 352 in wrapped_module_method   File ""/usr/lib/python3.9/contextlib.py"", line 79 in inner   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/transforms.py"", line 1235 in wrapped_fn   File ""/root/tensorrewriter/python/graph_rewriter/agents/models.py"", line 356 in __call__   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 651 in _call_wrapped_method   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 352 in wrapped_module_method   File ""/usr/lib/python3.9/contextlib.py"", line 79 in inner   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/transforms.py"", line 1235 in wrapped_fn   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 1535 in scope_fn   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/core/scope.py"", line 831 in wrapper   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 1159 in apply   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 162 in reraise_with_filtered_traceback   File ""/root/tensorrewriter/python/graph_rewriter/agents/gat_hierarchical_agent.py"", line 145 in act   File ""/root/tensorrewriter/python/graph_rewriter/examples/run_gat_hierarchical_rl.py"", line 206 in main   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/absl/app.py"", line 258 in _run_main   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/absl/app.py"", line 312 in run   File ""/root/tensorrewriter/python/graph_rewriter/examples/run_gat_hierarchical_rl.py"", line 365 in  Segmentation fault (core dumped) ``` The strange thing is I am not using any JIT function, and I am only using the cpu version of JAX. However I do use GPU to run CuDNN. I try googling, but no one seems to run into the same problem as me. My machine has 252 GB of RAM, and the program shouldn't need more than ~50 GB of RAM.   What jax/jaxlib version are you using? jax==0.3.14, jaxlib==0.3.14, jraph==0.0.5.dev0, flax==0.5.2  Which accelerator(s) are you using? CPU, and install CPU version of JAX  Additional System Info Ubuntu 20.04)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,BUG: JIT session error: Cannot allocate memory," Description Hi, I am running reinforcement learning with jax, flax and jraph. Often, after a few hundreds of episodes, JAX report:  ``` JIT session error: Cannot allocate memory Fatal Python error: Segmentation fault Thread 0x00007fce7effd700 (most recent call first):   File ""/usr/lib/python3.9/threading.py"", line 316 in wait   File ""/usr/lib/python3.9/threading.py"", line 581 in wait   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/tqdm/_monitor.py"", line 60 in run   File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner   File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap Current thread 0x00007fdde7b2e740 (most recent call first):   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 717 in _execute_compiled   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 167 in    File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 101 in apply_primitive   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/core.py"", line 680 in process_primitive   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/core.py"", line 330 in bind_with_trace   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/core.py"", line 327 in bind   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 808 in reshape   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 760 in _reshape   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 740 in reshape   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jraph/_src/utils.py"", line 1025 in _expand_trailing_dimensions   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jraph/_src/utils.py"", line 1031 in    File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 196 in    File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 196 in tree_map   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jraph/_src/utils.py"", line 1097 in zero_out_padding   File ""/root/tensorrewriter/python/graph_rewriter/agents/encoder/gnn.py"", line 214 in __call__   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 651 in _call_wrapped_method   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 352 in wrapped_module_method   File ""/usr/lib/python3.9/contextlib.py"", line 79 in inner   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/transforms.py"", line 1235 in wrapped_fn   File ""/root/tensorrewriter/python/graph_rewriter/agents/models.py"", line 356 in __call__   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 651 in _call_wrapped_method   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 352 in wrapped_module_method   File ""/usr/lib/python3.9/contextlib.py"", line 79 in inner   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/transforms.py"", line 1235 in wrapped_fn   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 1535 in scope_fn   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/core/scope.py"", line 831 in wrapper   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/flax/linen/module.py"", line 1159 in apply   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 162 in reraise_with_filtered_traceback   File ""/root/tensorrewriter/python/graph_rewriter/agents/gat_hierarchical_agent.py"", line 145 in act   File ""/root/tensorrewriter/python/graph_rewriter/examples/run_gat_hierarchical_rl.py"", line 206 in main   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/absl/app.py"", line 258 in _run_main   File ""/root/tensorrewriter/virtual_env/lib/python3.9/sitepackages/absl/app.py"", line 312 in run   File ""/root/tensorrewriter/python/graph_rewriter/examples/run_gat_hierarchical_rl.py"", line 365 in  Segmentation fault (core dumped) ``` The strange thing is I am not using any JIT function, and I am only using the cpu version of JAX. However I do use GPU to run CuDNN. I try googling, but no one seems to run into the same problem as me. My machine has 252 GB of RAM, and the program shouldn't need more than ~50 GB of RAM.   What jax/jaxlib version are you using? jax==0.3.14, jaxlib==0.3.14, jraph==0.0.5.dev0, flax==0.5.2  Which accelerator(s) are you using? CPU, and install CPU version of JAX  Additional System Info Ubuntu 20.04",2022-08-15T21:31:54Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/11923,How did you estimate that your program should only need 50GB of RAM? In general arraybased computing using Python as a frontend generally requires a factor of a few in memory overhead depending on what the program is doing. My guess from your description here is that you are actually running out of memory.,"In RL, every update round I clear the data buffer and reset the environment for a fresh start again. I was watching ""htop"" for a few update round and saw that the peak memory usage is actually never large than 10 GB. But you are right, probably I need to properly monitor the memory usage throughout training episodes. On the other hand, I am not using any JIT function, and yet it reports a JIT session error, is this OK?","There are a number of JAX API functions that are JITcompiled by default, for example `matmul`: https://github.com/google/jax/blob/b75969c5a15cb412b685f5cbb007f5e25894e80a/jax/_src/numpy/lax_numpy.pyL2773L2774 So having a JIT error when you're not using JIT on your own functions is not surprising.",I see. Thanks for letting me know!,"I am now thinking if I call `matmul` a lot of time with variable shapes of a, b. JAX will probably compile and cache lots of `matmul` kernels in memory? Is this a source of memory overhead?  "," yes that's right. There's a cache limit in terms of the number of entries, but it doesn't actually factor in the cache entry sizes, so the cache size can still blow up. CC(Add mechanism to clear full compilation cache) tracks the ability to clear all jit caches. It seems we haven't yet landed an actually good API to do this, but there's some partial info in that thread. If you periodically clear those caches, or clear them then retry on memory errors, does it help mitigate your issue?","hi , yes clearing cache does help! I use the clear_backends() function in CC(Add mechanism to clear full compilation cache) periodically and Outofmemory does not happen again. Thanks!"
406,"以下是一个github上的jax下的一个issue, 标题是(Use --no-implicit-optional for type checking)， 内容是 (This makes type checking PEP 484 compliant (as of 2018). mypy will change its defaults soon. See: https://github.com/python/mypy/issues/9091 https://github.com/python/mypy/pull/13401)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Use --no-implicit-optional for type checking,This makes type checking PEP 484 compliant (as of 2018). mypy will change its defaults soon. See: https://github.com/python/mypy/issues/9091 https://github.com/python/mypy/pull/13401,2022-08-13T02:07:45Z,kokoro:force-run pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/11899
786,"以下是一个github上的jax下的一个issue, 标题是(BUG: can create 64-bit arrays using dlpack when x64 is disabled)， 内容是 ( Description Repro: ```python import tensorflow as tf import jax import jax.dlpack assert not jax.config.jax_enable_x64 x = jax.dlpack.from_dlpack(tf.experimental.dlpack.to_dlpack(tf.ones(5, tf.int64))) assert x.dtype == jax.numpy.int64 ``` This can cause loud and confusing errors down the line when our MHLO expects 32bit numbers but gets 64bit. For example, with our illegal 64bit `x` array, try doing `x[0]`.  What jax/jaxlib version are you using? HEAD/0.3.15  Which accelerator(s) are you using? CPU  Additional System Info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,BUG: can create 64-bit arrays using dlpack when x64 is disabled," Description Repro: ```python import tensorflow as tf import jax import jax.dlpack assert not jax.config.jax_enable_x64 x = jax.dlpack.from_dlpack(tf.experimental.dlpack.to_dlpack(tf.ones(5, tf.int64))) assert x.dtype == jax.numpy.int64 ``` This can cause loud and confusing errors down the line when our MHLO expects 32bit numbers but gets 64bit. For example, with our illegal 64bit `x` array, try doing `x[0]`.  What jax/jaxlib version are you using? HEAD/0.3.15  Which accelerator(s) are you using? CPU  Additional System Info _No response_",2022-08-13T00:00:37Z,bug P1 (soon),closed,0,4,https://github.com/jax-ml/jax/issues/11895,"> This can cause loud and confusing errors down the line Arguably a separate issue to file regarding this: if we do somehow end up with unexpected 64bit device (or abstract) arrays, we ideally catch that and err early on, ahead of lowering in particular.","Reviving this issue. Hi, I had experienced the same exact problem and the error I got was: ```python XlaRuntimeError: UNKNOWN: :0: error: type of return operand 0 ('tensor') doesn't match function result type ('tensor') in function  :0: note: see current operation: ""func.return""(%0) : (tensor) > () :0: note: in bytecode version 0 produced by: MLIR16.0.0git ``` Which wasn't very intuitive to figure out. Furthermore, it seems that some tensorflow_datasets may use int64 to describe labels (like in tfds.load('mnist')) and if used together with dlpack it will result in this issue. Isn´t there a way to protect users from this, like a more intuitive RuntimeError, Warning? For context I will leave here the colab that I used to replicate this issue.","  – So long as the x64 flag exists and means what it means, how should we behave on `jax.dlpack.from_dlpack` given a 64bitdtyped array, when x64 mode is off? Some options: 1. Error 2. Convert to 32bit silently 3. Convert to 32bit and `warnings.warn` I'll send a PR for what we think is best.","converting silently is consistent with how the x64 flag works in similar situations, e.g. `jnp.asarray(numpy_arr)`"
579,"以下是一个github上的jax下的一个issue, 标题是(JAX API to evict programs from live executables)， 内容是 (JAX API to evict programs from live executables This defines a scoped object similar to TransferGuard to evict programs of live executables when exiting the scope. This is intended to help with freeing up memory and avoiding memory fragmentation. Usage: from jax.experimental import program_eviction as pe with pe.loaded_program_eviction_scope():   ...)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX API to evict programs from live executables,JAX API to evict programs from live executables This defines a scoped object similar to TransferGuard to evict programs of live executables when exiting the scope. This is intended to help with freeing up memory and avoiding memory fragmentation. Usage: from jax.experimental import program_eviction as pe with pe.loaded_program_eviction_scope():   ...,2022-08-12T21:55:08Z,,closed,1,2,https://github.com/jax-ml/jax/issues/11893,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Closing Copybara created PR due to inactivity
2279,"以下是一个github上的jax下的一个issue, 标题是(BUG: jax2tf (enable_xla=False): Incorrect results for reduce_window, lax.add and padding=""SAME"")， 内容是 ( Description In jax2tf (`enable_xla=False`) we use `tf.nn.avg_pool` to implement `lax.reduce_window_sum_p`. This is because TF does not have a ""sum pooling"" layer. The idea is that we take the average over a window and then multiply with the window size. Suppose the following input ``` [[1, 2]  [3, 4]] ``` And suppose further that `window_size=(2, 2)`, `strides=(1,1)`, and `padding=""VALID""`. * `lax.reduce_window(lax.sum)` returns `10`. * `tf.nn.avg_pool` returns `10/4 = 2.5.` Then `2.5*prod((2, 2)) = 2.5*4 = 10`. This seems to work fine, but unfortunately it breaks when using `SAME` padding with `window_size != (1, 1, ..)`. The problem is that `tf.nn.avg_pool` returns the average ignoring the padding tokens, so multiplying with `np.prod(window)` will give too high outputs.  Example Failure Here is an example: Suppose the same input and `window_size` as before, but now with `SAME` padding. This means the input will first be padded as follows: ``` [[1, 2, 0]  [3, 4, 0]  [0, 0, 0]] ``` Now `lax.reduce_window(lax.sum)` returns: ``` [[10, 6]  [ 7, 4]] ``` But `tf.nn.avg_pool` returns: ``` [[2.5, 3]  [3.5, 4]] ``` As we can see, the averaging happens only over the nonpadding tokens! The average `3` is computed by doing `(2+4)/2`, rather than `(2+4)/4`. As a result, when we now multiply this output with `np.prod(window_size)`, we get: ``` [[10, 12]  [14, 16]] ``` This is wrong!  Unfortunately TF does not allow us to choose whether to include or exclude padding tokens. This is set in the source code but is not exposed to the user.  Solution One way to resolve this is to manually pad the input when we are doing SAME padding.  In our example, this would mean the following: ``` x_pad = tf.pad(x, tf.constant([[0, 0], [0, 1], [0, 1], [0, 0]])) tf.nn.avg_pool(x_pad, [2, 2], [1, 1], ""VALID"") ``` This then gives the correct output. One thing to consider is that `lax.pads_to_padtype` has a different padding algorithm than TF, so we should probably implement it manually (explained here).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"BUG: jax2tf (enable_xla=False): Incorrect results for reduce_window, lax.add and padding=""SAME"""," Description In jax2tf (`enable_xla=False`) we use `tf.nn.avg_pool` to implement `lax.reduce_window_sum_p`. This is because TF does not have a ""sum pooling"" layer. The idea is that we take the average over a window and then multiply with the window size. Suppose the following input ``` [[1, 2]  [3, 4]] ``` And suppose further that `window_size=(2, 2)`, `strides=(1,1)`, and `padding=""VALID""`. * `lax.reduce_window(lax.sum)` returns `10`. * `tf.nn.avg_pool` returns `10/4 = 2.5.` Then `2.5*prod((2, 2)) = 2.5*4 = 10`. This seems to work fine, but unfortunately it breaks when using `SAME` padding with `window_size != (1, 1, ..)`. The problem is that `tf.nn.avg_pool` returns the average ignoring the padding tokens, so multiplying with `np.prod(window)` will give too high outputs.  Example Failure Here is an example: Suppose the same input and `window_size` as before, but now with `SAME` padding. This means the input will first be padded as follows: ``` [[1, 2, 0]  [3, 4, 0]  [0, 0, 0]] ``` Now `lax.reduce_window(lax.sum)` returns: ``` [[10, 6]  [ 7, 4]] ``` But `tf.nn.avg_pool` returns: ``` [[2.5, 3]  [3.5, 4]] ``` As we can see, the averaging happens only over the nonpadding tokens! The average `3` is computed by doing `(2+4)/2`, rather than `(2+4)/4`. As a result, when we now multiply this output with `np.prod(window_size)`, we get: ``` [[10, 12]  [14, 16]] ``` This is wrong!  Unfortunately TF does not allow us to choose whether to include or exclude padding tokens. This is set in the source code but is not exposed to the user.  Solution One way to resolve this is to manually pad the input when we are doing SAME padding.  In our example, this would mean the following: ``` x_pad = tf.pad(x, tf.constant([[0, 0], [0, 1], [0, 1], [0, 0]])) tf.nn.avg_pool(x_pad, [2, 2], [1, 1], ""VALID"") ``` This then gives the correct output. One thing to consider is that `lax.pads_to_padtype` has a different padding algorithm than TF, so we should probably implement it manually (explained here).",2022-08-12T11:33:26Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/11874,"Thinking about this further, I don't think the TF approach is wrong, and is preferable to the Jax implementation. Imagine we have a continuous signal, and in order to estimate its moving mean **m**, we sample it and compute an avgpool operation on the sampled version. Including the padding in the mean estimate will bias the estimate of **m** towards zero. Omitting the padding will lead to an unbiased (but noisier) estimate of **m**.  In general, the padding is an implementation detail included to ease implementation of the pooling op. If we include it in the pooling operation, we're letting the implementation details impact our computations, which is sad panda.",">  I don't think the TF approach is wrong Yes, I agree! I think including padding tokes in computing the average is a bit meaningless. I don't mean to say here TF is wrong, but rather that we should make sure we are aware of this discrepancy since it will lead to different results when using jax2tf.","Yah, I'm thinking the best strategy here is raising this as a bug with the Jax/Linen pooling operation, and thus getting Jax to match the TensorFlow approach.",Why is this a bug in JAX/Linen? Isn't this a bug strictly in `jax2tf` where it fails to correctly implement the JAX sumpooling semantics?,"Per my previous comment: Incorporating the padding values into the pooling operation is bad behavior. Flax handles the min/max pool correctly by using +/ jnp.inf as 'init' values for the reduction operation; thus, the padding never has an effect on the actual output values. Otherwise, if you took the min_pool over a bunch of strictly positive data, you would get zeros at the edges, instead of useful info about your array. In the case of average padding, Flax computes sum_pool(x) / window_size. But in the presence of zeropadding the window_size needs to be a bit smaller at the edges to reflect the actual number of nonpadding data values being averaged.","Correct, if Flax is doing that, then it's wrong. It should be the ratio of two `sum_pool` in that case, see how `tf2xla` does it: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/xla/client/lib/pooling.cc;l=28 (*edit* fixed link) The same translation should work for `lax`.","Here's the Flax implementation of avg_pool, with the division by np.prod(window_shape): https://github.com/google/flax/blob/main/flax/linen/pooling.pyL87 (Thanks for surfacing the tf2xla implementation; I was having trouble finding it behind the 'gen_nn_ops' which hides all the juicy details in the TF code.)","Just to be clear, I think there are two separate issues here: * Implementing `lax.reduce_window` with computation `lax.add` for jax2tf with `enable_xla=False`. That is what this issue is about: we implement this using `tf.nn.avg_pool`, but this gives wrong results when using SAME padding. I describe this in the issue description.  * Implementing avg_pool correctly in Flax. Flax is currently counting the padding tokens when doing avg_pool, which I agree seems wrong, but it independent of this issue and should be fixed/discussing in the Flax repo. I will raise a separate issue for this."
2223,"以下是一个github上的jax下的一个issue, 标题是(Add eager pmap support)， 内容是 (JAX's `jax_debug_nan`s and `jax_disable_jit` flags are extremely useful for debugging JITcompiled computations. Unfortunately, up to this point, they have not worked with `jax.pmap` (i.e. they do not disable the pmap compilation). This PR changes that!   Why is this useful/important? After this PR (and after enabling the `jax_eager_pmap` flag), the `jax_disable_jit` flag now enables executing `jax.pmap`ed functions **eagerly**. This makes `pmap` behave a lot more like `vmap` and therefore allows us to (at the very least)  1. print values inside of a `pmap` with Python's `print`,  3. use `pdb`/Python's builtin `breakpoint` while executing a `pmap`ed function. This is potentially very useful for debugging. When we inspect values inside of a eagerly `pmap`ed function, we should see objects that carry around sharding information as well as their underlying arrays. Before, we'd see abstracted tracers (no values only shape/dtype information).  How is this implemented?  JAX works by stacking ""interpreters"" on top of each other. For example `jax.vmap(jax.grad(f))` works by stacking a `vmap` interpreter on top of a `grad` interpreter. There is always a default bottommost interpreter, however, called the `Eval` interpreter which just invokes the implementation rules of a primitive (i.e. `jnp.sin(x)` will evaluate sin of x). Transformations that compile JAX functions (e.g. `jax.pmap`, `jax.jit`) swap out the `Eval` interpreter in favor of one that first builds a jaxpr (i.e. records all the primitive applications into a data structure). The jaxpr is then ""lowered"" into a more compiler friendly representation. When `jax_disable_jit` is enabled and we are `pmap`ing, this PR skips the jaxpr staging part entirely and swaps out the `Eval` interpreter in favor of a new `Map` interpreter. The `Map` interpreter keeps track of the name > axis index mapping on each tracer and instead of calling primitive's default implementations like the `Eval` trace, it calls `jax.pmap` on them with JITcompilation enabled.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add eager pmap support,"JAX's `jax_debug_nan`s and `jax_disable_jit` flags are extremely useful for debugging JITcompiled computations. Unfortunately, up to this point, they have not worked with `jax.pmap` (i.e. they do not disable the pmap compilation). This PR changes that!   Why is this useful/important? After this PR (and after enabling the `jax_eager_pmap` flag), the `jax_disable_jit` flag now enables executing `jax.pmap`ed functions **eagerly**. This makes `pmap` behave a lot more like `vmap` and therefore allows us to (at the very least)  1. print values inside of a `pmap` with Python's `print`,  3. use `pdb`/Python's builtin `breakpoint` while executing a `pmap`ed function. This is potentially very useful for debugging. When we inspect values inside of a eagerly `pmap`ed function, we should see objects that carry around sharding information as well as their underlying arrays. Before, we'd see abstracted tracers (no values only shape/dtype information).  How is this implemented?  JAX works by stacking ""interpreters"" on top of each other. For example `jax.vmap(jax.grad(f))` works by stacking a `vmap` interpreter on top of a `grad` interpreter. There is always a default bottommost interpreter, however, called the `Eval` interpreter which just invokes the implementation rules of a primitive (i.e. `jnp.sin(x)` will evaluate sin of x). Transformations that compile JAX functions (e.g. `jax.pmap`, `jax.jit`) swap out the `Eval` interpreter in favor of one that first builds a jaxpr (i.e. records all the primitive applications into a data structure). The jaxpr is then ""lowered"" into a more compiler friendly representation. When `jax_disable_jit` is enabled and we are `pmap`ing, this PR skips the jaxpr staging part entirely and swaps out the `Eval` interpreter in favor of a new `Map` interpreter. The `Map` interpreter keeps track of the name > axis index mapping on each tracer and instead of calling primitive's default implementations like the `Eval` trace, it calls `jax.pmap` on them with JITcompilation enabled.",2022-08-11T18:37:42Z,kokoro:force-run pull ready,closed,2,0,https://github.com/jax-ml/jax/issues/11854
778,"以下是一个github上的jax下的一个issue, 标题是(BUG: jnp.linalg.matrix_rank does not operate on stacks of matrices)， 内容是 ( Description ``` myInput = np.random.normal(size=(9,10,10)) myInput.shape  (9,10,10) np.linalg.matrix_rank(myInput)  array([10, 10, 10, 10, 10, 10, 10, 10, 10]) ``` jax fails on this same stack: ``` jax.numpy.linalg.matrix_rank(jnp.array(myInput))  TypeError: array should have 2 or fewer dimensions  Expected: DeviceArray([10, 10, 10, 10, 10, 10, 10, 10, 10]) ```  What jax/jaxlib version are you using? jax 0.3.13, jaxlib 0.3.10  Which accelerator(s) are you using?  [ ] CPU  [X] GPU  [ ] TPU  Additional System Info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,BUG: jnp.linalg.matrix_rank does not operate on stacks of matrices," Description ``` myInput = np.random.normal(size=(9,10,10)) myInput.shape  (9,10,10) np.linalg.matrix_rank(myInput)  array([10, 10, 10, 10, 10, 10, 10, 10, 10]) ``` jax fails on this same stack: ``` jax.numpy.linalg.matrix_rank(jnp.array(myInput))  TypeError: array should have 2 or fewer dimensions  Expected: DeviceArray([10, 10, 10, 10, 10, 10, 10, 10, 10]) ```  What jax/jaxlib version are you using? jax 0.3.13, jaxlib 0.3.10  Which accelerator(s) are you using?  [ ] CPU  [X] GPU  [ ] TPU  Additional System Info _No response_",2022-08-08T17:54:35Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/11795,"i haven't tested, but suspect this might apply to the jax version of `numpy.linalg.svd` as well.",Thanks for the report  I'll take a look.
5691,"以下是一个github上的jax下的一个issue, 标题是(Bump EnricoMi/publish-unit-test-result-action from 1 to 2)， 内容是 (Bumps EnricoMi/publishunittestresultaction from 1 to 2.  Release notes Sourced from EnricoMi/publishunittestresultaction's releases.  v2.0.0 Adds the following features:  Add support for TRX files ( CC(Fix some TODOs that were previously blocked on a Jaxlib release.)) Add support for NUnit and XUnit XML files ( CC(Added batching rules for convolutions + pooling.) and  CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.)) Move to lxml parsing library ( CC(Implement translation rule for select_and_gather_add (issue 274).)) Upgrade junitparser to 2.7.0, lxml to 4.9.1, urllib3 to 1.26.11 Support large CDATA in JUnit XML files with ignore_runs Improved logging of parse errors and tracebacks ( CC(Implement np.{empty,empty_like,ptp,isreal,iscomplex,sinc,vander,positive}.),  CC(jax missing scipy.special.expit)) Fix state leakage with ignore_runs ( CC(tweaks so einsum and spstats tests run internally)) Support NUnit results with inner test suites ( CC(AssertionError (must be unit) caused by strange combination of _while_loop, jit and nested init_state))  This release contains the following breaking changes:  Default value for option check_name changed from &quot;Unit Test Results&quot; to &quot;Test Results&quot;. If check_name is set in your config, this change is not of your concern. Add check_name: &quot;Unit Test Results&quot; to your config, if you want to keep the old name. Otherwise expect comments and checks with old and new name for open pull requests when moving to version 2. Default value for option comment_title changed from &quot;Unit Test Results&quot; to &quot;Test Results&quot;. If either check_name or comment_title is set in your config, this change is not of your concern. Modes create new and update last removed for option comment_mode. Remove option comment_mode if used with either of these modes. Option hiding_comments removed. Please remove this option from your config. Option comment_on_pr removed. Please remove this option from your config.  This release deprecates the following features:  Option files is deprecated, use junit_files instead ( CC(Fix average pooling to align the window element counts with the spatial dimensions.))  See README.md for further details on moving to version 2. v2.0.0beta.4 Adds the following features:  Rename action name in Marketplace to &quot;Publish Test Results&quot; ( CC(rename ""minmax"" > ""optimizers""))  v2.0.0beta.3 Adds the following features:  Support NUnit results with inner test suites ( CC(AssertionError (must be unit) caused by strange combination of _while_loop, jit and nested init_state))  v2.0.0beta.2 Adds the following features:  Support large CDATA in JUnit XML files with ignore_runs Improved logging of parse errors and tracebacks ( CC(Implement np.{empty,empty_like,ptp,isreal,iscomplex,sinc,vander,positive}.),  CC(jax missing scipy.special.expit)) Fix state leakage with ignore_runs ( CC(tweaks so einsum and spstats tests run internally))  v2.0.0beta Adds the following features:  Minor improvements on warnings and logging.  v2.0.0alpha Adds the following features:  Add support for TRX files ( CC(Fix some TODOs that were previously blocked on a Jaxlib release.)) Add support for NUnit and XUnit XML files ( CC(Added batching rules for convolutions + pooling.) and  CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.))    ... (truncated)   Commits  2a60c5d Releasing v2.0.0 be326a8 Mention file formats in action description 9b56b79 Fix lxml upgrade ( CC(Change implementation of negative loglikelihood in README toy example)) 04677bd Upgrade lxml and urllib3 acd986f Merge branch 'master2.x' 6fc8d1f Releasing v2.0.0beta.4 2f820a8 Remove remaining unit terms except for repository name ( CC(rename ""minmax"" > ""optimizers"")) cc8fa88 Remove support for macOS 10.15 577d995 Releasing v2.0.0beta.3 9c0a5c1 Rename testcaseintestcase test to testsuiteintestsuite Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Bump EnricoMi/publish-unit-test-result-action from 1 to 2,"Bumps EnricoMi/publishunittestresultaction from 1 to 2.  Release notes Sourced from EnricoMi/publishunittestresultaction's releases.  v2.0.0 Adds the following features:  Add support for TRX files ( CC(Fix some TODOs that were previously blocked on a Jaxlib release.)) Add support for NUnit and XUnit XML files ( CC(Added batching rules for convolutions + pooling.) and  CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.)) Move to lxml parsing library ( CC(Implement translation rule for select_and_gather_add (issue 274).)) Upgrade junitparser to 2.7.0, lxml to 4.9.1, urllib3 to 1.26.11 Support large CDATA in JUnit XML files with ignore_runs Improved logging of parse errors and tracebacks ( CC(Implement np.{empty,empty_like,ptp,isreal,iscomplex,sinc,vander,positive}.),  CC(jax missing scipy.special.expit)) Fix state leakage with ignore_runs ( CC(tweaks so einsum and spstats tests run internally)) Support NUnit results with inner test suites ( CC(AssertionError (must be unit) caused by strange combination of _while_loop, jit and nested init_state))  This release contains the following breaking changes:  Default value for option check_name changed from &quot;Unit Test Results&quot; to &quot;Test Results&quot;. If check_name is set in your config, this change is not of your concern. Add check_name: &quot;Unit Test Results&quot; to your config, if you want to keep the old name. Otherwise expect comments and checks with old and new name for open pull requests when moving to version 2. Default value for option comment_title changed from &quot;Unit Test Results&quot; to &quot;Test Results&quot;. If either check_name or comment_title is set in your config, this change is not of your concern. Modes create new and update last removed for option comment_mode. Remove option comment_mode if used with either of these modes. Option hiding_comments removed. Please remove this option from your config. Option comment_on_pr removed. Please remove this option from your config.  This release deprecates the following features:  Option files is deprecated, use junit_files instead ( CC(Fix average pooling to align the window element counts with the spatial dimensions.))  See README.md for further details on moving to version 2. v2.0.0beta.4 Adds the following features:  Rename action name in Marketplace to &quot;Publish Test Results&quot; ( CC(rename ""minmax"" > ""optimizers""))  v2.0.0beta.3 Adds the following features:  Support NUnit results with inner test suites ( CC(AssertionError (must be unit) caused by strange combination of _while_loop, jit and nested init_state))  v2.0.0beta.2 Adds the following features:  Support large CDATA in JUnit XML files with ignore_runs Improved logging of parse errors and tracebacks ( CC(Implement np.{empty,empty_like,ptp,isreal,iscomplex,sinc,vander,positive}.),  CC(jax missing scipy.special.expit)) Fix state leakage with ignore_runs ( CC(tweaks so einsum and spstats tests run internally))  v2.0.0beta Adds the following features:  Minor improvements on warnings and logging.  v2.0.0alpha Adds the following features:  Add support for TRX files ( CC(Fix some TODOs that were previously blocked on a Jaxlib release.)) Add support for NUnit and XUnit XML files ( CC(Added batching rules for convolutions + pooling.) and  CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.))    ... (truncated)   Commits  2a60c5d Releasing v2.0.0 be326a8 Mention file formats in action description 9b56b79 Fix lxml upgrade ( CC(Change implementation of negative loglikelihood in README toy example)) 04677bd Upgrade lxml and urllib3 acd986f Merge branch 'master2.x' 6fc8d1f Releasing v2.0.0beta.4 2f820a8 Remove remaining unit terms except for repository name ( CC(rename ""minmax"" > ""optimizers"")) cc8fa88 Remove support for macOS 10.15 577d995 Releasing v2.0.0beta.3 9c0a5c1 Rename testcaseintestcase test to testsuiteintestsuite Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-08-08T17:12:27Z,pull ready dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/11794,"Interesting, the action failed, but that may be because it's running off a branch rather than running on main. I'm going to close this for now, we could probably spend some time diagnosing the issue, but I don't think it's worth it at this point.","OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1744,"以下是一个github上的jax下的一个issue, 标题是(BUG: lax.conv_.. outputs are slightly different than Torch counterparts)， 内容是 ( Description Hi,  I am trying to compare results of a convolution2d operation in PyTorch and Jax. The results seem to be slightly different and was wondering if this mismatch is normal? Or perhaps, there is a better way to do such a comparison. The last assert fails with the default tolerance. `dtypes` are `float32` for both the frameworks. ```python import jax import jax.numpy as jnp import jax.lax as lax import torch.nn as tnn import torch import numpy as np def test_conv2d():     random_image = jax.random.uniform(key=jax.random.PRNGKey(0), shape=(1, 3, 224, 224))     t_cv = tnn.Conv2d(3, 3, kernel_size=3)     j_w, j_b = jnp.asarray(t_cv.weight.detach().numpy()), jnp.asarray(t_cv.bias.detach().numpy().reshape((3, 1, 1)))     assert jnp.equal(j_w, t_cv.weight.detach().numpy()).all()     assert jnp.equal(j_b, t_cv.bias.detach().numpy().reshape(3, 1, 1)).all()     j_out = lax.conv_general_dilated(         lhs=random_image,         rhs=j_w,         window_strides=(1, 1),         padding=((0, 0), (0, 0)),         rhs_dilation=(1, 1),         feature_group_count=1,         precision=jax.lax.Precision.HIGHEST     )     j_out = j_out + j_b     with torch.no_grad():         t_out = jnp.asarray(t_cv(torch.tensor(np.asarray(random_image))).numpy())     assert t_out.shape == j_out.shape     assert jnp.isclose(t_out, j_out).all() ```  What jax/jaxlib version are you using? 0.3.15  Which accelerator(s) are you using?  [X] CPU  [ ] GPU  [ ] TPU  Additional System Info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,BUG: lax.conv_.. outputs are slightly different than Torch counterparts," Description Hi,  I am trying to compare results of a convolution2d operation in PyTorch and Jax. The results seem to be slightly different and was wondering if this mismatch is normal? Or perhaps, there is a better way to do such a comparison. The last assert fails with the default tolerance. `dtypes` are `float32` for both the frameworks. ```python import jax import jax.numpy as jnp import jax.lax as lax import torch.nn as tnn import torch import numpy as np def test_conv2d():     random_image = jax.random.uniform(key=jax.random.PRNGKey(0), shape=(1, 3, 224, 224))     t_cv = tnn.Conv2d(3, 3, kernel_size=3)     j_w, j_b = jnp.asarray(t_cv.weight.detach().numpy()), jnp.asarray(t_cv.bias.detach().numpy().reshape((3, 1, 1)))     assert jnp.equal(j_w, t_cv.weight.detach().numpy()).all()     assert jnp.equal(j_b, t_cv.bias.detach().numpy().reshape(3, 1, 1)).all()     j_out = lax.conv_general_dilated(         lhs=random_image,         rhs=j_w,         window_strides=(1, 1),         padding=((0, 0), (0, 0)),         rhs_dilation=(1, 1),         feature_group_count=1,         precision=jax.lax.Precision.HIGHEST     )     j_out = j_out + j_b     with torch.no_grad():         t_out = jnp.asarray(t_cv(torch.tensor(np.asarray(random_image))).numpy())     assert t_out.shape == j_out.shape     assert jnp.isclose(t_out, j_out).all() ```  What jax/jaxlib version are you using? 0.3.15  Which accelerator(s) are you using?  [X] CPU  [ ] GPU  [ ] TPU  Additional System Info _No response_",2022-08-08T12:13:56Z,bug needs info,closed,1,8,https://github.com/jax-ml/jax/issues/11790,I can't reproduce this. Can you share what output you got? (And please verify this is on CPU?) A small amount of variation between systems is normal and expected due to things like orders of operations.,"Hi , thanks for looking into this.   Yes, it is on the CPU.  The output ndarrays are attached. To be loaded with `jnp.load(f)` outputs.zip So when you say it is not reproduced does it mean that the assertion passes? ",Correct: the assertion passes for me on every machine I have access to. By share the outputs I just meant: what does the script print? How large is the error?,"Oh I see, If I lower the tolerance `atol=1e7` the check passes.  The issue is that I am porting weights from PyTorch and during debugging I saw that differences start off small and then eventually diverge as the depth increases.   Pasting the console output for the above script:  ```bash E       assert DeviceArray(False, dtype=bool) E        +  where DeviceArray(False, dtype=bool) = () E        +    where  = DeviceArray([[[[ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               ...,\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True]],\n\n              [[ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               ...,\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True]],\n\n              [[ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               ...,\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True]]]],            dtype=bool).all E        +      where DeviceArray([[[[ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               ...,\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True]],\n\n              [[ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               ...,\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True]],\n\n              [[ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               ...,\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True],\n               [ True,  True,  True, ...,  True,  True,  True]]]],            dtype=bool) = >(DeviceArray([[[[0.11918862, 0.39337683, 0.10415261, ...,\n                0.14490902,  0.08345226, 0.38034382],\n               [0.2665483 , 0.08656419, 0.41431338, ...,\n                0.24779865, 0.14176738, 0.17808765],\n               [0.11191443, 0.34973115, 0.31308794, ...,\n                0.04645528, 0.314068  , 0.11709285],\n               ...,\n               [0.04562423,  0.02439445, 0.3495699 , ...,\n                0.21667773, 0.10611602, 0.04519989],\n               [0.20471334, 0.550535  , 0.13338754, ...,\n                0.11880863, 0.17236012,  0.168724  ],\n               [0.2530645 , 0.23086065, 0.17092755, ...,\n                 0.12365223, 0.20544101, 0.22553827]],\n\n              [[ 0.12898597, 0.23355147, 0.09015972, ...,\n                0.33356306, 0.21871755,  0.11076313],\n               [0.06024742, 0.16666049, 0.08918659, ...,\n                0.1279987 , 0.1723417 , 0.11240471],\n               [ 0.18849662,  0.08548687,  0.1092547 , ...,\n                0.00623718, 0.06411915,  0.05974886],\n               ...,\n               [0.02477138, 0.27820313, 0.12436383, ...,\n                 0.00866155, 0.1631342 , 0.13039222],\n               [0.14874887, 0.00196842,  0.00810845, ...,\n                0.17311396,  0.07414906, 0.06788205],\n               [0.07409473, 0.14532988,  0.043919  , ...,\n                0.00590803, 0.21093307,  0.16623716]],\n\n              [[0.14535953, 0.25476003, 0.3117075 , ...,\n                0.35012716,  0.13202074,  0.10703781],\n               [0.19592267, 0.35908204,  0.01105969, ...,\n                0.1548535 ,  0.04212352, 0.33539474],\n               [0.46050325, 0.2708272 , 0.41181085, ...,\n                 0.09065992,  0.10939495, 0.44372863],\n               ...,\n               [ 0.22833237,  0.05699068, 0.17989516, ...,\n                0.01604492, 0.3436626 , 0.02549492],\n               [0.30278763, 0.00453902, 0.20900564, ...,\n                0.33052015, 0.19383013, 0.02311407],\n               [0.21668078, 0.13806498, 0.22233306, ...,\n                 0.01706599,  0.20324235, 0.07500426]]]], dtype=float32), DeviceArray([[[[0.11918864, 0.39337686, 0.1041526 , ...,\n                0.14490902,  0.08345223, 0.38034382],\n               [0.2665483 , 0.08656421, 0.4143134 , ...,\n                0.24779865, 0.14176737, 0.17808762],\n               [0.1119144 , 0.34973115, 0.31308797, ...,\n                0.04645528, 0.31406796, 0.11709286],\n               ...,\n               [0.04562422,  0.02439445, 0.34956986, ...,\n                0.21667776, 0.10611602, 0.04519988],\n               [0.20471331, 0.550535  , 0.13338757, ...,\n                0.11880863, 0.17236014,  0.16872403],\n               [0.2530645 , 0.23086065, 0.17092752, ...,\n                 0.12365226, 0.20544103, 0.22553828]],\n\n              [[ 0.12898599, 0.23355149, 0.09015968, ...,\n                0.33356303, 0.21871753,  0.11076312],\n               [0.0602474 , 0.16666049, 0.08918659, ...,\n                0.12799868, 0.1723417 , 0.11240473],\n               [ 0.18849666,  0.08548681,  0.1092547 , ...,\n                0.00623718, 0.06411918,  0.05974886],\n               ...,\n               [0.02477136, 0.2782031 , 0.12436387, ...,\n                 0.00866158, 0.1631342 , 0.13039221],\n               [0.14874887, 0.00196841,  0.0081085 , ...,\n                0.17311388,  0.07414907, 0.06788205],\n               [0.07409474, 0.14532986,  0.043919  , ...,\n                0.00590806, 0.21093306,  0.16623713]],\n\n              [[0.14535953, 0.25476003, 0.3117075 , ...,\n                0.35012713,  0.13202079,  0.10703778],\n               [0.19592264, 0.35908198,  0.01105966, ...,\n                0.15485354,  0.04212352, 0.33539477],\n               [0.46050325, 0.2708272 , 0.41181085, ...,\n                 0.09065993,  0.10939495, 0.44372863],\n               ...,\n               [ 0.22833231,  0.05699069, 0.1798952 , ...,\n                0.01604491, 0.34366256, 0.02549493],\n               [0.3027876 , 0.004539  , 0.20900562, ...,\n                0.33052015, 0.19383013, 0.02311408],\n               [0.21668075, 0.13806495, 0.22233312, ...,\n                 0.01706599,  0.20324233, 0.07500427]]]], dtype=float32)) E        +        where > = jnp.isclose test_conv.py:40: AssertionError ```","Yes, I'd suggest that simply means that your test tolerances are too tight. Feel free to reopen if you see large numerical differences!", do I understand correctly that this issue is preventing porting e.g. an Inception net from PyTorch to JAX? Or something else? Can you share an example for which the divergence between models is nontrivial?,"kidger At the moment, the results of a ported VGG11 varied greatly from torchvision implementation (~30% top1 accuracy).  As I am trying to debug layer by layer, it seems that `1e7` to `1e4` difference is expected (?). The bulk of the difference arises after a specific pooling module (`adaptive pooling`). However, this behaviour is not replicated by other ported networks which perform under + 0.5% variance.  I'll go further into this to try and isolate the issue.  Thanks for your help!      ","Right. The differences between convolutions seem unfortunate but perhaps not too big of a deal. The differences under adaptive pooling are definitely expected, as Equinox and PyTorch consciously choose to do these differently."
371,"以下是一个github上的jax下的一个issue, 标题是(Releases are off?)， 内容是 (Just wanted to signal that most releases disappeared from https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html. It would be great if you could put them back! :) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Releases are off?,Just wanted to signal that most releases disappeared from https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html. It would be great if you could put them back! :) ,2022-08-07T17:31:22Z,build P0 (urgent) NVIDIA GPU CI,closed,2,7,https://github.com/jax-ml/jax/issues/11785,Yeah same for me too (althought I thought it was my fault and spent two hours trying to work out why I couldnt install jax),Should be fixed. Can you please check and confirm? (you might need to refresh to get the updated index),"Works, thanks!",Awesome! Thanks for confirming.,"Hi, would it be possible to update the website https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html with the newest releases? I see the releases through jaxlib0.3.15 from July, but not since then.","We haven't made any jaxlib releases since July (only `jax`, which can be released separately from `jaxlib`). We're trying to make a new one now, though.","Ah, okay. I figured out my issue, thanks. "
685,"以下是一个github上的jax下的一个issue, 标题是(Error in debug print if unused arguments are provided)， 内容是 (Why? If you do something like this: ```python jax.debug.print(f""{x}"", x=x) ``` i.e use an fstring, you'll print out a traced `x` inside of the JIT instead of the actual value.  After this PR, we'll raise an error saying that the provided `x=x` keyword argument to `jax.debug.print` was left unused and suggest that you might be using an fstring. This can help catch this issue earlier as opposed to after JIT compilation. Thanks to  for the suggestion!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Error in debug print if unused arguments are provided,"Why? If you do something like this: ```python jax.debug.print(f""{x}"", x=x) ``` i.e use an fstring, you'll print out a traced `x` inside of the JIT instead of the actual value.  After this PR, we'll raise an error saying that the provided `x=x` keyword argument to `jax.debug.print` was left unused and suggest that you might be using an fstring. This can help catch this issue earlier as opposed to after JIT compilation. Thanks to  for the suggestion!",2022-08-05T23:47:09Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/11770,Hi  ! I found this pull request because when running RL experiments I got the error message you implemented and I was impressed by how helpful it is. Kudos!,Thank you!
3336,"以下是一个github上的jax下的一个issue, 标题是(prototype unfettered element types in jaxpr arrays)， 内容是 (Intensely coauthored with  ```ocaml { lambda ; a:foo[3,4,5]. let     b:f32[3,4,5] = scan[       jaxpr={ lambda ; c:foo[4,5]. let           d:foo[5,4] = bake c           e:foo[4,5] = bake d           f:f32[4,5] = take e         in (f,) }       length=3     ] a   in (b,) } ``` From where comes the set of element types in jaxprs? Historically, from NumPy and XLA element types. But why would jaxprs be constrained to those? After all, jaxprs are just symbols, my friends. Those symbols need to be grounded when we translate to another compiler's IR, or when we have input or output values with a jaxpr evaluation. So if we're lowering we need ways to map jaxpr types to lowered IR types, and also ways to map any operations allowed on these types to lowered IR operations. And we may want Python objects representing values of these types. But once we have those mappings we don't need to be limited by NumPy/XLA element types. Within jaxprs, we also need to handle transformations with these types. In this change we started unfettering jaxpr element types from their vestigial NumPy/XLA constraints. Concretely, that means:   * allowing `ShapedArray` to have any object for its 'dtype' attribute   * added `core.custom_eltype` set   * extended existing handlers for `ShapedArray` to call the corresponding custom     element type handlers   * mlir lowerings of some fullyelementtypepolymorphic primitives   * tests In this PR, we only actually use these new extension points in tests. The applications to come that we have in mind are:   * arrays of prngkeys (and even custom prngs, as well as reuse error checking)   * arrays of bounded int type for dynamic shapes (and especially raggedness)   * `float0` arrays We do not have in mind opening these mechanisms up to users. Think of these as yet another JAXinternal extension point, like all our existing 'handler' tables. Jargonwise, we may want to distinguish:   * 'eltype' meaning jaxpr element types   * 'dtype' meaning numpy dtypes (an existing convention)   * 'etype' meaning hlo/mhlo element types (an existing convention) But the code doesn't model this jargon at the moment, since we left a lot of attributes and helper functions referring to 'dtype'. We haven't yet handled all the elementtypepolymorphic primitives. Here's the list we've thought of so far:   * [x] broadcast   * [ ] reshape   * [x] transpose   * [ ] pad   * [x] slice, dynamic_slice, dynamic_update_slice   * [ ] concatenate   * [ ] all_to_all, gather, scatter, all_gather, collective_permute   * [x] make empty scalar (only appears in internalabouttolowerjaxpr dialect) That last one is interesting: we introduced it so that the scan lowering rule, which lowers first to a ""lowered jaxpr dialect"" involving only those eltypes which correspond to etypes and involving only while_loop, ds/dus, etc, can be made simpler. Otherwise we'd need scan, itself a fullyeltypepolymorphic primitive, have a more    complicated lowering rule. We also haven't handled AD. Our main applications (at least the first two listed above) don't involve AD types, so it seemed good to skip for now.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,prototype unfettered element types in jaxpr arrays,"Intensely coauthored with  ```ocaml { lambda ; a:foo[3,4,5]. let     b:f32[3,4,5] = scan[       jaxpr={ lambda ; c:foo[4,5]. let           d:foo[5,4] = bake c           e:foo[4,5] = bake d           f:f32[4,5] = take e         in (f,) }       length=3     ] a   in (b,) } ``` From where comes the set of element types in jaxprs? Historically, from NumPy and XLA element types. But why would jaxprs be constrained to those? After all, jaxprs are just symbols, my friends. Those symbols need to be grounded when we translate to another compiler's IR, or when we have input or output values with a jaxpr evaluation. So if we're lowering we need ways to map jaxpr types to lowered IR types, and also ways to map any operations allowed on these types to lowered IR operations. And we may want Python objects representing values of these types. But once we have those mappings we don't need to be limited by NumPy/XLA element types. Within jaxprs, we also need to handle transformations with these types. In this change we started unfettering jaxpr element types from their vestigial NumPy/XLA constraints. Concretely, that means:   * allowing `ShapedArray` to have any object for its 'dtype' attribute   * added `core.custom_eltype` set   * extended existing handlers for `ShapedArray` to call the corresponding custom     element type handlers   * mlir lowerings of some fullyelementtypepolymorphic primitives   * tests In this PR, we only actually use these new extension points in tests. The applications to come that we have in mind are:   * arrays of prngkeys (and even custom prngs, as well as reuse error checking)   * arrays of bounded int type for dynamic shapes (and especially raggedness)   * `float0` arrays We do not have in mind opening these mechanisms up to users. Think of these as yet another JAXinternal extension point, like all our existing 'handler' tables. Jargonwise, we may want to distinguish:   * 'eltype' meaning jaxpr element types   * 'dtype' meaning numpy dtypes (an existing convention)   * 'etype' meaning hlo/mhlo element types (an existing convention) But the code doesn't model this jargon at the moment, since we left a lot of attributes and helper functions referring to 'dtype'. We haven't yet handled all the elementtypepolymorphic primitives. Here's the list we've thought of so far:   * [x] broadcast   * [ ] reshape   * [x] transpose   * [ ] pad   * [x] slice, dynamic_slice, dynamic_update_slice   * [ ] concatenate   * [ ] all_to_all, gather, scatter, all_gather, collective_permute   * [x] make empty scalar (only appears in internalabouttolowerjaxpr dialect) That last one is interesting: we introduced it so that the scan lowering rule, which lowers first to a ""lowered jaxpr dialect"" involving only those eltypes which correspond to etypes and involving only while_loop, ds/dus, etc, can be made simpler. Otherwise we'd need scan, itself a fullyeltypepolymorphic primitive, have a more    complicated lowering rule. We also haven't handled AD. Our main applications (at least the first two listed above) don't involve AD types, so it seemed good to skip for now.",2022-08-05T22:17:17Z,pull ready,closed,3,10,https://github.com/jax-ml/jax/issues/11768,"My first reaction when I read description ""ooh, I can have Index type"" :)","Just my two cents, but I would consider still calling these ""JAX dtypes"" (at least in the public APIs) rather than inventing a new name.","Yeah, we had decided the same! Glad you agree. Haven't followed up yet (neither in comments, nor in code).","While working to adapt the lowering rules to dynamic shapes I noticed that the custom lowering rules for opaque types are written to return a sequence of MHLO ops rather than just a MHLO op. I am wondering if this was intended (for some future extensibility) or it was just an accident.  For example, I want to write a helper function for generating `mhlo.BroadcastInDimOp` or `mhlo.DynamicBroadcastInDimOp` depending on whether the output value is statically shaped or not. If this helper function also wants to handle opaque types, and call the `_rules.broadcast_in_dim` then the helper function is forced to return a sequence of `ir.Value`. This makes the helper function not usable in many other contexts where we want to broadcast during lowering, e.g., in linalg.py lowering rules. Those contexts do not know how to handle a broadcast that results in multiple ops. As another example, currently the broadcast_in_dim lowering rule simply returns the result of  `_rules.broadcast_in_dim_mlir`, in which case it is acceptable that the result of the latter should be a singleton list of `ir.Op`. In fact, that lowering rule would not know what to do with multiple returned ops. I am tempted to think that the custom lowering rules should return a single op. One thing that gives me pause is that the custom `physical_avals` seem to be intended to rewrite one `aval` into multiple physical ones. I have not seen this design issue discussed anywhere, so my question is if this is really intended. If it is, not many called of these custom rules would know what to do with multiple values, e.g., from a broadcast.", PTAL,"Thinking about it some more, JAX lowering rules already support avals that are represented after lowering by multiple IR values. Do we plan to support avals that are opaque and are represented by multiple IR values? If so, then I need to rethink who to write the lowering functions that I have in mind.","> [...] JAX lowering rules already support avals that are represented after lowering by multiple IR values. Do we plan to support avals that are opaque and are represented by multiple IR values? Yeah, the original intent was to support lowering to multiple IR values. It isn't necessary for either of our immediate uses of opaque dtypes: RNG key types (for existing RNG implementations) and in bounded ints. But (a), as you say, it's consistent with the multiIRvalue support we have in other lowering rules. And (b), some use cases are imaginable, like if we wanted to introduce int128 or float128 dtypes in Jaxpr, or if we wanted to pursue RNG reuse checking techniques that keep an extra ""used"" bit for each array element. The imagined use cases are currently more hypothetical than not, so perhaps we can revisit the multiIRvalue lowering if it's too complicated to maintain."," – Considering that CC(Improves handling of opaque types for dynamic shapes) went in as it did, are we committing to ""single return value"" for now?",At the moment we only support single value. This can be revisited.  ,"Sounds good. Perhaps the fact that `physical_avals` is still plural, but then asserted to be of length 1 by the slice lowerings, is a fine reminder that slice lowerings are why we decided to constrain things for now. For example: https://github.com/google/jax/blob/aeb425874b8090d877fc50a6532d28f12be48445/jax/_src/prng.pyL408L410 It'd good to resolve the mismatch more wholly one way or another at some point."
1324,"以下是一个github上的jax下的一个issue, 标题是(Fix jax numpy norm bug where passing `ord=""inf""` always returns one)， 内容是 (This fixes a silent bug where passing in `ord=""inf""` always returns one. Calculating this norm requires passing `jnp.inf` instead. ```py >>> import jax.numpy as jnp >>> jnp.linalg.norm(jnp.array([10, 20]), ord=jnp.inf) DeviceArray(20., dtype=float32) >>> jnp.linalg.norm(jnp.array([10, 20]), ord=""inf"") DeviceArray(1., dtype=float32) ``` Either it should calculate the infinity norm or raise an exception. With this change, we match numpy behavior and raise an exception. ```py >>> import numpy as np >>> np.linalg.norm(np.array([10, 20]), ord=np.inf) 20.0 >>> np.linalg.norm(np.array([10, 20]), ord=""inf"") Traceback (most recent call last): ... ValueError: Invalid norm order 'inf' for vectors >>> jnp.linalg.norm(jnp.array([10, 20]), ord=""inf"")  (With this PR) Traceback (most recent call last): ... ValueError: Invalid order 'inf' for vector norm. Use 'jax.numpy.inf' instead. ``` This might break existing usage of `norm` that pass `""inf""`. I tried to search for instances of `jnp.linalg.norm(..., ord=""inf"", ...)` on GitHub and Google Search but couldn't find any matches.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Fix jax numpy norm bug where passing `ord=""inf""` always returns one","This fixes a silent bug where passing in `ord=""inf""` always returns one. Calculating this norm requires passing `jnp.inf` instead. ```py >>> import jax.numpy as jnp >>> jnp.linalg.norm(jnp.array([10, 20]), ord=jnp.inf) DeviceArray(20., dtype=float32) >>> jnp.linalg.norm(jnp.array([10, 20]), ord=""inf"") DeviceArray(1., dtype=float32) ``` Either it should calculate the infinity norm or raise an exception. With this change, we match numpy behavior and raise an exception. ```py >>> import numpy as np >>> np.linalg.norm(np.array([10, 20]), ord=np.inf) 20.0 >>> np.linalg.norm(np.array([10, 20]), ord=""inf"") Traceback (most recent call last): ... ValueError: Invalid norm order 'inf' for vectors >>> jnp.linalg.norm(jnp.array([10, 20]), ord=""inf"")  (With this PR) Traceback (most recent call last): ... ValueError: Invalid order 'inf' for vector norm. Use 'jax.numpy.inf' instead. ``` This might break existing usage of `norm` that pass `""inf""`. I tried to search for instances of `jnp.linalg.norm(..., ord=""inf"", ...)` on GitHub and Google Search but couldn't find any matches.",2022-08-05T17:31:19Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/11762
1854,"以下是一个github上的jax下的一个issue, 标题是([call_tf] implementation of batching rule for vmap)， 内容是 (Hi, I'm trying to use call_tf in combination with jacrev and hence vmap. The primitive_batcher of type call_tf_p is not implemented. I tried to modify the source code of `jax/experimental/jax2tf/call_tf.py` by adding the generic broadcaster `batching.defbroadcasting(call_tf_p)` but this doesn't work (wrong result for vmap, error for jacrev).  I'm not sure how to finish this as I'm not familiar with the internals of jax:  ``` def _tf_batcher(args, dims, callable_flat_tf, function_flat_tf, args_flat_sig_tf, **params):     flat_shape = np.array(args_flat_sig_tf[0].shape)      do smthg, probably call bind as many times with slice according to batched dim     call_tf_p.bind(args, callable_flat_tf=callable_flat_tf) batching.primitive_batchers[call_tf_p] = _tf_batcher ``` ____________________ As a test case, I'd would like this tensorflow roundtrip code to work as expected. ``` import jax import jax.numpy as jnp import numpy as np import tensorflow as tf from jax import jacrev, jit, vmap from jax.experimental import jax2tf def test_call_tf_batcher():          def f(x):         return jnp.sum(x * x)     f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))     x = np.random.random((7))     y = np.random.random((5, 7))      test f==f_rt     print(f""{f(x)}, {f_rt(x)}"")     print(f""{jax.grad(f)(x)}, {jax.grad(f_rt)(x)}"")      test vmap     f_rt_v = vmap(f_rt)     f_v = vmap(f)     print(f""{f_v(y)} == {f_rt_v(y)}"")      test jacobian     j = jacrev(f)     j_rt = jacrev(f_rt)     print(f""{j(x)} == {j_rt(x)}"") if __name__ == ""__main__"":     test_call_tf_batcher() ``` Any help would be greatly appreciated)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[call_tf] implementation of batching rule for vmap,"Hi, I'm trying to use call_tf in combination with jacrev and hence vmap. The primitive_batcher of type call_tf_p is not implemented. I tried to modify the source code of `jax/experimental/jax2tf/call_tf.py` by adding the generic broadcaster `batching.defbroadcasting(call_tf_p)` but this doesn't work (wrong result for vmap, error for jacrev).  I'm not sure how to finish this as I'm not familiar with the internals of jax:  ``` def _tf_batcher(args, dims, callable_flat_tf, function_flat_tf, args_flat_sig_tf, **params):     flat_shape = np.array(args_flat_sig_tf[0].shape)      do smthg, probably call bind as many times with slice according to batched dim     call_tf_p.bind(args, callable_flat_tf=callable_flat_tf) batching.primitive_batchers[call_tf_p] = _tf_batcher ``` ____________________ As a test case, I'd would like this tensorflow roundtrip code to work as expected. ``` import jax import jax.numpy as jnp import numpy as np import tensorflow as tf from jax import jacrev, jit, vmap from jax.experimental import jax2tf def test_call_tf_batcher():          def f(x):         return jnp.sum(x * x)     f_rt = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))     x = np.random.random((7))     y = np.random.random((5, 7))      test f==f_rt     print(f""{f(x)}, {f_rt(x)}"")     print(f""{jax.grad(f)(x)}, {jax.grad(f_rt)(x)}"")      test vmap     f_rt_v = vmap(f_rt)     f_v = vmap(f)     print(f""{f_v(y)} == {f_rt_v(y)}"")      test jacobian     j = jacrev(f)     j_rt = jacrev(f_rt)     print(f""{j(x)} == {j_rt(x)}"") if __name__ == ""__main__"":     test_call_tf_batcher() ``` Any help would be greatly appreciated",2022-08-05T10:42:00Z,enhancement,open,1,7,https://github.com/jax-ml/jax/issues/11753,"In general, it is not possible to add a proper batching rule for `call_tf` because the called function can in principle be an arbitrary TF function for which one cannot define a generic batching rule (except perhaps by running the function in a loop). Even in the case when the called function originates from JAX, by the time it gets to `call_tf` the function has been staged out to HLO so it is not possible to use the JAX batching rules anymore.  For a roundtrip to TF, you have to apply the vmap before converting to TF.  What is the use case for which you want to do a round trip through TensorFlow?","I don't really need a round trip, just code to convert a tensorflow function to to jax function f: R^(nx3)>R^m   (i.e a m dimensional function of n 3d molecular coordinates). I need the jacobian matrix. I've managed to implement the batching procedure as a loop: ``` def loop_batcher(prim, args, dims,  **params):      determine new axis size     for ni, di in enumerate(dims):         if di is not None:             axis_size = args[ni].shape[di]             break      generate combination of indices for different arguments     args_indices = []     for xi, ia in zip(args, dims):         xs = xi.shape         indices = [slice(None) if i !=                    ia else None for i, _ in enumerate(xs)]         l = []         for i in range(axis_size):             if ia is not None:                 indices[ia] = i             l.append(tuple(indices))         args_indices.append(l)      apply function     out = []     for inds in list(zip(*args_indices),):         outp = prim.bind(*[a[b] for a, b in zip(             args, inds)], **params)         if not isinstance(outp, Iterable):             outp = tuple((outp,))         out.append(outp)     ret = []      collect output in arrays     for out_args in list(zip(*out)):         val = jax.numpy.hstack(out_args)         val = jax.numpy.reshape(val,  (axis_size,  *out_args[0].shape))         ret.append(val)     return (ret,  (0,)*len(ret)) batching.primitive_batchers[call_tf_p] = functools.partial(     loop_batcher, call_tf_p) ``` Below the testing code ``` import jax.numpy as jnp import numpy as np from jax import grad, jacrev, jit, vmap from jax.experimental import jax2tf def test_call_tf_batcher():          def f(x):         return jnp.array([jnp.sum(x * x),  jnp.product(x)])     f_t = jax2tf.call_tf(jax2tf.convert(f, with_gradient=True))     x = np.random.random((7))     y = np.random.random((5, 7))     print(f""_2 = { jnp.linalg.norm( j(x)j_t(x) )  }"") if __name__ == ""__main__"":     test_call_tf_batcher() ``` The code seems to work fine for vmap and jacobian and it can be jit compiled. In my case the performance penalty is low because the output dimension m is low (23). As said before, I don't know the internals of jax so there might still be some errors in my code. This is quite general so it might also be used in other places where batching is not implemented :)","Your solution should work in principle (I have not checked it in all detail), but I do not feel that it is a solution that we want to upstream to JAX because it breaks the expectation that `jax.vmap` is generally more efficient than running the underlying function repeatedly for all mapped elements. One thing we could do is to add a reference to this issue and your solution in the documentation of call_tf and let the users decide whether this workaround works for them or not. WDYT?","Seems good to me. Another option would be to add a configurable flag `SLOW_TF_VMAP = False`. If the flag is not set to `True`, raise an error within batching function with a link to the right webpage. p.s. there was a small error in the code, I changed my last post. The code seems to work now for the more complicated problem I tried to solve in the first place :) ","Some parts of my code were annoyingly slow, so I've reimplemented the batcher using the `tf.vectorized_map` function. Basically I redo the call_tf on the vectorized tensoflow function and pipe the results to jax. The unmapped dimemsions are applied beforehand.    ``` def loop_batcher(prim, args, dims,  **params):      do partial application of args on given position""     def apply(batch_args, f, static_args, static_pos):         arguments = []         l = len(batch_args)+len(static_args)         j = 0         k = 0         for i in range(l):             if i in static_pos:                 arguments.append(static_args[j])                 j += 1             else:                 arguments.append(batch_args[k])                 k += 1         return f(*arguments)     static_pos = []     static_args = []     batch_args = []      find arguments for partial application     for i, (arg, batch_axis) in enumerate(zip(args, dims)):         if batch_axis is None:             static_pos.append(i)             static_args.append(arg)         else:             assert batch_axis == 0, 'other position not yet implemented'             batch_args.append(arg)      vectorize     def par_fun(batch_args, static_args):         return tf.vectorized_map(fn=functools.partial(apply, f=params['callable_flat_tf'], static_args=static_args, static_pos=static_pos), elems=batch_args)     if len(batch_args) != 1:         raise NotImplementedError      execute with given arguments     ret = call_tf(par_fun)(batch_args, static_args)     return (ret,  (0,)*len(ret)) ``` What are your thoughts on this? The function needs some more work if the batch dimensions are not in position 0 or more than one array should be batched at once.","In case anyone is interested, I was also looking at this since I wanted to load, run and save JAX models I previously saved to tf SavedModels and repeat that an arbitrary number of times  that is, composing savedmodels. My use case only needs a single batch dimension as the first dimension but may be possible to adapt the code: ```python (eq=True, kw_only=True, order=True, frozen=True) class _ShapeAndDtype:     shape: tuple     dtype: type  Passthrough batcher for tf saved models. Assumes the first dimension is batched and no other. def _tf_passthrough_batcher(     fn: Callable,     inp: JaxArrayOrMap,     batched_args: tuple,     batched_dims: tuple,     call_tf_graph: bool,     callable_flat_tf: Callable[[list[TfVal]], Sequence[TfVal]],     **_kwargs: dict, ) > tuple:     assert len(batched_dims) == 1     assert len(batched_args) == 1     treedef = jax.tree_structure(inp)      Map nonintegers to 1 to handle polymorphic inputs, e.g. on save.     input_shape = tuple([int(v) if isinstance(v, int) else 1 for v in batched_args[0].shape])      Force call callable_flat_tf to fill `res_treedef` inside it.     out = callable_flat_tf(np.zeros(input_shape))   type: ignore[argtype]     assert len(out) == 1     output_shape: list[int] = list(out[0].shape)      Grab value which may be noninteger (polymorphic) from the batch dimension.     batched_output_shape = (batched_args[0].shape[0], *output_shape[1:])      Assumes a single output.     output_shape_dtype = _ShapeAndDtype(shape=batched_output_shape, dtype=np.float32)     args = treedef.unflatten(batched_args)     ret = jax2tf.call_tf(fn, call_tf_graph=call_tf_graph, output_shape_dtype=output_shape_dtype)(         args     )     return ([ret], (0,)) ``` It basically just passes through the batch operation since it assumes that the tf SavedModel you're using has a polymorphic input on the batch dimension (which is true for my use case). You need to invoke it like this: ```python batching.primitive_batchers[call_tf_p] = functools.partial(_tf_passthrough_batcher, fn, inp)  call_tf_graph supports polymorphic inputs for saving to a SavedModel.  But, it does not work for training / running at all, just for saving. jax2tf.call_tf(fn, call_tf_graph=is_saving)(inp) ``` There is some additional complication around saving vs running/training a model that contains the loaded SavedModel. For training/running, you want to use eager execution (the default), otherwise it won't work (don't remember the exact reason). For saving, I want to preserve the polymorphic input dimension (batch dimension)  trying to save with eager execution gives an error: ``` ValueError: Error compiling TensorFlow function (see below for the caught exception). call_tf can used in a staged context (under jax.jit, lax.scan, etc.) only with compilable functions with static output shapes. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdlimitationsofcall_tf for a discussion. ``` So, you need to use the experimental call_tf_graph flag *only for saving the model*. This doesn't work with eager execution, since it needs the tensorflow graph object, so we need to disable eager execution when saving and enable call_tf_graph for call_tf: ```python tf.compat.v1.disable_eager_execution() ``` Now it is not using eager execution, it needs to access a bunch of tf variables from the saved model we loaded as part of our larger model, but they are not initialized, so when we export using orbax we have to provide the extra trackable tf resources. ```python jax_module = JaxModule(     ...,     {         ""predict"": ...     },      Our jax code is polymorphic on the batch size, so let the export do that too.     input_polymorphic_shape={""predict"": ""(b, ...)""}, ) serving_configs = [     ServingConfig(         ...         extra_trackable_resources=extra_trackable_resources,     ) ] ``` The extra trackable resources need to be the variables from the saved tf model we loaded, for example: ```python self.model.signatures[""serving_default""].variables ``` Then to actually export, we need to set up a tensorflow session with the extra trackable resources, and also the tf variables the jax module needs (e.g. from the larger model we embedded the existing saved model in) initialized: ```python with tf.compat.v1.Session(     graph=extra_trackable_resources[0].graph ).as_default() as sess:      Run initializers     if extra_trackable_resources:         sess.run([v.initializer for v in extra_trackable_resources])     sess.run([v.initializer for v in jax_module.variables])     export_mgr = ExportManager(jax_module, serving_configs=serving_configs)     export_mgr.save(path) ``` Unfortunately this isn't quite enough because call_tf doesn't support noneager execution (see also https://github.com/google/jax/issues/18315 ), so we need to patch `_call_tf_lowering` to get the variables out of the graph if we're not executing eagerly: ```python   if tf.executing_eagerly():     np_captured_inputs = [np.asarray(inp) for inp in captured_inputs]   else:     if captured_inputs:       with tf.compat.v1.Session(graph=captured_inputs[0].graph) as sess:            Get all global variables within this graph            Run initializers           sess.run([v.initializer for v in captured_inputs])            Get values           np_captured_inputs = sess.run(captured_inputs)     else:       np_captured_inputs = []   captured_ops = tuple(       mlir.ir_constant(inp)       for inp in np_captured_inputs   ) ``` And also there's a small bug in jax_module.py _to_tf_variable where it does not pass in the correct device name, need to add `.name` to `default_cpu_device`. ```python with tf.device(default_cpu_device.name):     return tf.Variable(         x, trainable=trainable, shape=x.shape, dtype=x.dtype, name=name     ) ``` With this it's possible to round trip a jax model with a batch dimension to tf savedmodel, then load it back up again and train it as part of a larger model and save it again, preserving the batch dimension, so the batching can be closed under saving. Unrelated, but `_call_tf_lowering` has an implicit dependency on tf >= 2.16 since it passes the platform argument to `experimental_get_compiler_ir`. I don't think this dependency is documented anywhere.","I recently came across this issue as well. I have 3 libraries, A, B, and C: ```                   jax2tf.call_tf A (jax w/ vmap) ==================> B (TF) ====> C (TF) ``` I need to upgrade C to JAX. Unfortunately, that leaves me with ```                   jax2tf.call_tf             jax2tf.convert  A (jax w/ vmap) ==================> B (TF) ==================> C (jax) ``` which does not work due to this issue. All in all, the experience leaves me feeling that jax2tf is a leaky abstraction. I understand that there are practical considerations at play here limiting interop between TF and JAX, but I would eagerly accept a slower forloop implementation over no implementation at all in this case. Having support for this would unlock a lot of potential TFtoJAX rewrites for me :)"
6602,"以下是一个github上的jax下的一个issue, 标题是(CUDA install broken?)， 内容是 (Hi everyone, I was wondering if for some reason the CUDA installation on linux is broken? I get the following weird behaviour on a clean python environment ```bash $ pip install upgrade pip Requirement already satisfied: pip in ./.venv/lib/python3.10/sitepackages (22.0.4) Collecting pip   Using cached pip22.2.2py3noneany.whl (2.0 MB) Installing collected packages: pip   Attempting uninstall: pip     Found existing installation: pip 22.0.4     Uninstalling pip22.0.4:       Successfully uninstalled pip22.0.4 Successfully installed pip22.2.2 $ pip install  nocachedir upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda]   Downloading jax0.3.15.tar.gz (1.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 89.0 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done Collecting abslpy   Downloading absl_py1.2.0py3noneany.whl (123 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 kB 453.5 MB/s eta 0:00:00 Collecting numpy>=1.19   Downloading numpy1.23.1cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/17.0 MB 115.1 MB/s eta 0:00:00 Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 399.3 MB/s eta 0:00:00 Collecting scipy>=1.5   Downloading scipy1.9.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.9/43.9 MB 95.1 MB/s eta 0:00:00 Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.6.0py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.1/98.1 kB 391.0 MB/s eta 0:00:00 Collecting jax[cuda]   Downloading jax0.3.14.tar.gz (990 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 990.1/990.1 kB 114.1 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.13.tar.gz (951 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 951.0/951.0 kB 127.6 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.12.tar.gz (947 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 947.1/947.1 kB 122.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.11.tar.gz (947 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 947.1/947.1 kB 132.3 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.10.tar.gz (939 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 939.7/939.7 kB 127.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.9.tar.gz (937 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 938.0/938.0 kB 126.6 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.8.tar.gz (935 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 935.0/935.0 kB 136.1 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.7.tar.gz (944 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.2/944.2 kB 124.5 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.6.tar.gz (936 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 936.8/936.8 kB 124.3 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.5.tar.gz (946 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 946.8/946.8 kB 125.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.4.tar.gz (924 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 925.0/925.0 kB 130.3 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.3.tar.gz (924 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 924.4/924.4 kB 129.5 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.2.tar.gz (926 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 926.4/926.4 kB 129.6 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.1.tar.gz (912 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 912.1/912.1 kB 127.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.0.tar.gz (896 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 896.3/896.3 kB 146.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.28.tar.gz (887 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 887.3/887.3 kB 130.1 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.27.tar.gz (873 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 873.8/873.8 kB 131.1 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.26.tar.gz (850 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 850.1/850.1 kB 147.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.25.tar.gz (786 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 786.4/786.4 kB 123.9 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.24.tar.gz (786 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 786.8/786.8 kB 117.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.22.tar.gz (776 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 776.0/776.0 kB 145.2 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done WARNING: jax 0.2.22 does not provide the extra 'cuda' Using legacy 'setup.py install' for jax, since package 'wheel' is not installed. Installing collected packages: numpy, abslpy, scipy, opt_einsum, jax   Running setup.py install for jax ... done Successfully installed abslpy1.2.0 jax0.2.22 numpy1.23.1 opt_einsum3.3.0 scipy1.9.0 ``` For reference, here's my cuda installation (but everything worked fine until a couple of days ago, and I'm fairly sure that I have not updated my system): ```bash $ cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR A 2                                                                define CUDNN_MAJOR 8 define CUDNN_MINOR 4 define CUDNN_PATCHLEVEL 1  define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL) endif /* CUDNN_VERSION_H */ $ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 $ python version Python 3.10.5 ``` Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,CUDA install broken?,"Hi everyone, I was wondering if for some reason the CUDA installation on linux is broken? I get the following weird behaviour on a clean python environment ```bash $ pip install upgrade pip Requirement already satisfied: pip in ./.venv/lib/python3.10/sitepackages (22.0.4) Collecting pip   Using cached pip22.2.2py3noneany.whl (2.0 MB) Installing collected packages: pip   Attempting uninstall: pip     Found existing installation: pip 22.0.4     Uninstalling pip22.0.4:       Successfully uninstalled pip22.0.4 Successfully installed pip22.2.2 $ pip install  nocachedir upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda]   Downloading jax0.3.15.tar.gz (1.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 89.0 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done Collecting abslpy   Downloading absl_py1.2.0py3noneany.whl (123 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.4/123.4 kB 453.5 MB/s eta 0:00:00 Collecting numpy>=1.19   Downloading numpy1.23.1cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.0/17.0 MB 115.1 MB/s eta 0:00:00 Collecting opt_einsum   Downloading opt_einsum3.3.0py3noneany.whl (65 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 399.3 MB/s eta 0:00:00 Collecting scipy>=1.5   Downloading scipy1.9.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.9/43.9 MB 95.1 MB/s eta 0:00:00 Collecting typing_extensions   Downloading typing_extensions4.3.0py3noneany.whl (25 kB) Collecting etils[epath]   Downloading etils0.6.0py3noneany.whl (98 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.1/98.1 kB 391.0 MB/s eta 0:00:00 Collecting jax[cuda]   Downloading jax0.3.14.tar.gz (990 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 990.1/990.1 kB 114.1 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.13.tar.gz (951 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 951.0/951.0 kB 127.6 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.12.tar.gz (947 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 947.1/947.1 kB 122.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.11.tar.gz (947 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 947.1/947.1 kB 132.3 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.10.tar.gz (939 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 939.7/939.7 kB 127.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.9.tar.gz (937 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 938.0/938.0 kB 126.6 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.8.tar.gz (935 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 935.0/935.0 kB 136.1 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.7.tar.gz (944 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 944.2/944.2 kB 124.5 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.6.tar.gz (936 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 936.8/936.8 kB 124.3 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.5.tar.gz (946 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 946.8/946.8 kB 125.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.4.tar.gz (924 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 925.0/925.0 kB 130.3 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.3.tar.gz (924 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 924.4/924.4 kB 129.5 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.2.tar.gz (926 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 926.4/926.4 kB 129.6 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.1.tar.gz (912 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 912.1/912.1 kB 127.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.3.0.tar.gz (896 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 896.3/896.3 kB 146.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.28.tar.gz (887 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 887.3/887.3 kB 130.1 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.27.tar.gz (873 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 873.8/873.8 kB 131.1 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.26.tar.gz (850 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 850.1/850.1 kB 147.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.25.tar.gz (786 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 786.4/786.4 kB 123.9 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.24.tar.gz (786 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 786.8/786.8 kB 117.8 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done   Downloading jax0.2.22.tar.gz (776 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 776.0/776.0 kB 145.2 MB/s eta 0:00:00   Preparing metadata (setup.py) ... done WARNING: jax 0.2.22 does not provide the extra 'cuda' Using legacy 'setup.py install' for jax, since package 'wheel' is not installed. Installing collected packages: numpy, abslpy, scipy, opt_einsum, jax   Running setup.py install for jax ... done Successfully installed abslpy1.2.0 jax0.2.22 numpy1.23.1 opt_einsum3.3.0 scipy1.9.0 ``` For reference, here's my cuda installation (but everything worked fine until a couple of days ago, and I'm fairly sure that I have not updated my system): ```bash $ cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR A 2                                                                define CUDNN_MAJOR 8 define CUDNN_MINOR 4 define CUDNN_PATCHLEVEL 1  define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL) endif /* CUDNN_VERSION_H */ $ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 $ python version Python 3.10.5 ``` Thanks!",2022-08-04T15:01:38Z,bug,closed,10,2,https://github.com/jax-ml/jax/issues/11732,"Same issue here, worked fine a few days ago.",Fixed now. Our new nightly `jaxlib` release process accidentally overwrote the main index instead of using its own index file.
907,"以下是一个github上的jax下的一个issue, 标题是(Error importing jax after building from source)， 内容是 (I have built jaxlib from source on a supercomputer cluster using linux Centos 7 Haswell. I used gcc 8.3.0 because using anything newer gave me the error message "" unsupported GNU version! gcc versions later than 9 are not supported!"" I also used an ""old"" commit of jax (0b4b0ba072aead2128233307f567b202f34e1c0c) because I need jaxlib 0.3.15  I installed it with GPU capabilities, using cuda11.6.2 and cudnn8.4.0.27 When trying to import jax I get the following error message ImportError: /.conda/envs/sourceXena_8.3.0/lib/python3.10/sitepackages/jaxlib/xla_extension.so: undefined symbol: cublasGetStatusString I have not been able to find anything regarding this. Any suggestions?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Error importing jax after building from source,"I have built jaxlib from source on a supercomputer cluster using linux Centos 7 Haswell. I used gcc 8.3.0 because using anything newer gave me the error message "" unsupported GNU version! gcc versions later than 9 are not supported!"" I also used an ""old"" commit of jax (0b4b0ba072aead2128233307f567b202f34e1c0c) because I need jaxlib 0.3.15  I installed it with GPU capabilities, using cuda11.6.2 and cudnn8.4.0.27 When trying to import jax I get the following error message ImportError: /.conda/envs/sourceXena_8.3.0/lib/python3.10/sitepackages/jaxlib/xla_extension.so: undefined symbol: cublasGetStatusString I have not been able to find anything regarding this. Any suggestions?",2022-08-04T13:41:33Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/11729,"The issue is that the particular commit you chose was in a temporarily broken state, since fixed. You need to use a newer commit that fixes the bug (or a preceding commit before the bug was introduced). The relevant commit will change the `WORKSPACE` file, so just look at the history of that file to find the subsequent commits that matter. Hope that helps!", So then if I want jaxlib 0.3.15 should I just change jax/jax/version.py?  I had changed to the last commit before the version change just in case any other files needed to be different in order to have this version of jaxlib,"Try checking out the `jaxlibv0.3.15` tag? You should not need to change anything, you just need to check out the right version.",">   sorry for the ignorance, but what do you mean by ""checking out"" the tag? Is this an argument in the build command?","In your JAX git checkout, run `git checkout jaxlibv0.3.15`","Hello again Peter, I followed your instructions and built it again and I am receiving a similar error now   jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: CustomCall failed: jaxlib/cuda/cuda_prng_kernels.cc:32: operation cudaGetLastError() failed: cudaGetErrorString symbol not found."
9275,"以下是一个github上的jax下的一个issue, 标题是(`dtype('bool')` test failures)， 内容是 (Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: 1. Build from source. 2. Run the full test suite.  [x] If applicable, include full error messages/tracebacks.  Errors I'm seeing loads of errors of the form ``` 20220728T00:41:03.1426552Z stderr: >     self.assertEqual(tangents[1].dtype, dtypes.float0) 20220728T00:41:03.1427091Z stderr: E     AssertionError: dtype('bool') != dtype([('float0', 'V')]) ``` It seems to be affecting 59 tests: ``` 20220728T00:41:03.1428063Z stderr: =========================== short test summary info ============================ 20220728T00:41:03.1456495Z stderr: FAILED tests/api_test.py::APITest::test_float0_reshape  AssertionError: dtyp... 20220728T00:41:03.1457678Z stderr: FAILED tests/api_test.py::APITest::test_float0_error  KeyError: dtype([('flo... 20220728T00:41:03.1461929Z stderr: FAILED tests/api_test.py::APITest::test_grad_of_bool  KeyError: dtype([('flo... 20220728T00:41:03.1462520Z stderr: FAILED tests/api_test.py::APITest::test_jit_jvp_of_int  AssertionError: (dty... 20220728T00:41:03.1463325Z stderr: FAILED tests/api_test.py::APITest::test_grad_of_int  KeyError: dtype([('floa... 20220728T00:41:03.1463925Z stderr: FAILED tests/api_test.py::APITest::test_grad_of_int_index  TypeError: a byte... 20220728T00:41:03.1464520Z stderr: FAILED tests/api_test.py::APITest::test_issue_871  TypeError: reduce_sum doe... 20220728T00:41:03.1465103Z stderr: FAILED tests/api_test.py::APITest::test_vjp_of_int_index  TypeError: a bytes... 20220728T00:41:03.1465685Z stderr: FAILED tests/api_test.py::APITest::test_jvp_of_int_add  AssertionError: (dty... 20220728T00:41:03.1466273Z stderr: FAILED tests/api_test.py::APITest::test_jit_grad_of_int  TypeError: a bytes... 20220728T00:41:03.1466856Z stderr: FAILED tests/api_test.py::APITest::test_jit_vjp_of_int  TypeError: Called ad... 20220728T00:41:03.1467420Z stderr: FAILED tests/api_test.py::APITest::test_vjp_of_int_shapes  TypeError: (Shape... 20220728T00:41:03.1468005Z stderr: FAILED tests/api_test.py::APITest::test_vjp_of_int_fulllike  KeyError: dtype... 20220728T00:41:03.1468604Z stderr: FAILED tests/api_test.py::CustomJVPTest::test_float0  AssertionError: dtype(... 20220728T00:41:03.1469198Z stderr: FAILED tests/api_test.py::CustomJVPTest::test_float0_initial_style  Assertio... 20220728T00:41:03.1469786Z stderr: FAILED tests/api_test.py::CustomVJPTest::test_float0  KeyError: dtype([('flo... 20220728T00:41:03.1470267Z stderr: FAILED tests/api_test.py::CleanupTest::test_call_wrapped_second_phase_cleanup 20220728T00:41:03.1470855Z stderr: FAILED tests/api_test.py::CustomVJPTest::test_float0_initial_style  Assertio... 20220728T00:41:03.1471446Z stderr: FAILED tests/core_test.py::CoreTest::test_reference_cycles_jit  AssertionErr... 20220728T00:41:03.1471942Z stderr: FAILED tests/lax_numpy_indexing_test.py::IndexingTest::testJVPOfGradOfIndexing 20220728T00:41:03.1472485Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testOperatorOverload__lshift___uint16_uint32[] 20220728T00:41:03.1473052Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testOperatorOverload__rshift___uint16_uint32[] 20220728T00:41:03.1473765Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testDeleteIndexArray_bool[1,4]_axis=1_idx=pyint64 20220728T00:41:03.1474342Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testDeleteIndexArray_uint16[2,3,4]_axis=2_idx=int64 20220728T00:41:03.1474905Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testDeleteIndexArray_uint32[2,3,4]_axis=2_idx=int64 20220728T00:41:03.1475440Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testPiecewise_bfloat16_ncond=3_nfunc=3 20220728T00:41:03.1475967Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testPiecewise_int16_ncond=1_nfunc=2 20220728T00:41:03.1476496Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testPiecewise_pyint32_ncond=2_nfunc=2 20220728T00:41:03.1477073Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testRightOperatorOverload__rlshift___uint16_uint32[] 20220728T00:41:03.1477781Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testRightOperatorOverload__rrshift___uint16_uint32[] 20220728T00:41:03.1478374Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testTake_bfloat16[3,4]_index=pyint32_axis=None_mode=clip 20220728T00:41:03.1478937Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testTake_int16[3,4]_index=pyint16_axis=None_mode=None 20220728T00:41:03.1479638Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testTake_int32[3,4,5]_index=int32_axis=1_mode=None 20220728T00:41:03.1480350Z stderr: FAILED tests/lax_numpy_test.py::NumpySignaturesTest::testWrappedSignaturesMatch 20220728T00:41:03.1480994Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGrad  KeyError: dtype(... 20220728T00:41:03.1481614Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGradInf  KeyError: dty... 20220728T00:41:03.1482225Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGradNan  KeyError: dty... 20220728T00:41:03.1482946Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGradNegInf  KeyError: ... 20220728T00:41:03.1483575Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGradZero  KeyError: dt... 20220728T00:41:03.1484343Z stderr: FAILED tests/random_test.py::LaxRandomTest::testPermutation_dtype=_range_or_shape=(0, 5)_axis=1_independent=False 20220728T00:41:03.1485153Z stderr: FAILED tests/random_test.py::LaxRandomTest::testPermutation_dtype=_range_or_shape=(0, 5)_axis=2_independent=True 20220728T00:41:03.1486016Z stderr: FAILED tests/random_test.py::LaxRandomTest::testPermutation_dtype=_range_or_shape=(0, 5)_axis=1_independent=False 20220728T00:41:03.1486822Z stderr: FAILED tests/random_test.py::LaxRandomTest::testPermutation_dtype=_range_or_shape=(0,)_axis=1_independent=False 20220728T00:41:03.1487372Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_complex64[5,5] 20220728T00:41:03.1487859Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_complex64[5,8] 20220728T00:41:03.1488375Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_complex64[8,5] 20220728T00:41:03.1488849Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_complex64[8,8] 20220728T00:41:03.1489320Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_float32[5,5] 20220728T00:41:03.1489790Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_float32[5,8] 20220728T00:41:03.1490252Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_float32[8,5] 20220728T00:41:03.1490709Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_float32[8,8] 20220728T00:41:03.1491169Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_complex64[5,5] 20220728T00:41:03.1491650Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_complex64[5,8] 20220728T00:41:03.1492124Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_complex64[8,5] 20220728T00:41:03.1492595Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_complex64[8,8] 20220728T00:41:03.1493069Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_float32[5,5] 20220728T00:41:03.1493516Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_float32[5,8] 20220728T00:41:03.1493974Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_float32[8,5] 20220728T00:41:03.1494438Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_float32[8,8] 20220728T00:41:03.1494868Z stderr: ========= 59 failed, 14190 passed, 1447 skipped in 1589.83s (0:26:29) ========== ``` You can access the full build logs here: https://github.com/samuela/nixpkgsupkeep/runs/7551101765?check_suite_focus=true. To me, this indicates a dependency version issue, possibly with numpy. However, all of the versions are within their constraints as far as `python setup.py build` is concerned. Here are the relevant pip freeze dependency versions: ``` [nixshell:~/dev/nixpkgs]$ pip freeze abslpy @ file:///build/abslpy1.0.0/dist/absl_py1.0.0py3noneany.whl flatbuffers @ file:///build/source/python/dist/flatbuffers2.0.0py2.py3noneany.whl jax @ file:///build/source/dist/jax0.3.6py3noneany.whl jaxlib @ file:///build/dist/jaxlib0.3.0cp39nonemanylinux2010_x86_64.whl numpy @ file:///build/numpy1.21.5/dist/numpy1.21.5cp39cp39linux_x86_64.whl opteinsum @ file:///build/opt_einsum3.3.0/dist/opt_einsum3.3.0py3noneany.whl SciPy @ file:///build/scipy1.8.0/dist/SciPy1.8.0cp39cp39linux_x86_64.whl six @ file:///build/six1.16.0/dist/six1.16.0py2.py3noneany.whl typing_extensions @ file:///build/typing_extensions4.1.1/dist/typing_extensions4.1.1py3noneany.whl ``` More information is available in the CI run linked above.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`dtype('bool')` test failures,"Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: 1. Build from source. 2. Run the full test suite.  [x] If applicable, include full error messages/tracebacks.  Errors I'm seeing loads of errors of the form ``` 20220728T00:41:03.1426552Z stderr: >     self.assertEqual(tangents[1].dtype, dtypes.float0) 20220728T00:41:03.1427091Z stderr: E     AssertionError: dtype('bool') != dtype([('float0', 'V')]) ``` It seems to be affecting 59 tests: ``` 20220728T00:41:03.1428063Z stderr: =========================== short test summary info ============================ 20220728T00:41:03.1456495Z stderr: FAILED tests/api_test.py::APITest::test_float0_reshape  AssertionError: dtyp... 20220728T00:41:03.1457678Z stderr: FAILED tests/api_test.py::APITest::test_float0_error  KeyError: dtype([('flo... 20220728T00:41:03.1461929Z stderr: FAILED tests/api_test.py::APITest::test_grad_of_bool  KeyError: dtype([('flo... 20220728T00:41:03.1462520Z stderr: FAILED tests/api_test.py::APITest::test_jit_jvp_of_int  AssertionError: (dty... 20220728T00:41:03.1463325Z stderr: FAILED tests/api_test.py::APITest::test_grad_of_int  KeyError: dtype([('floa... 20220728T00:41:03.1463925Z stderr: FAILED tests/api_test.py::APITest::test_grad_of_int_index  TypeError: a byte... 20220728T00:41:03.1464520Z stderr: FAILED tests/api_test.py::APITest::test_issue_871  TypeError: reduce_sum doe... 20220728T00:41:03.1465103Z stderr: FAILED tests/api_test.py::APITest::test_vjp_of_int_index  TypeError: a bytes... 20220728T00:41:03.1465685Z stderr: FAILED tests/api_test.py::APITest::test_jvp_of_int_add  AssertionError: (dty... 20220728T00:41:03.1466273Z stderr: FAILED tests/api_test.py::APITest::test_jit_grad_of_int  TypeError: a bytes... 20220728T00:41:03.1466856Z stderr: FAILED tests/api_test.py::APITest::test_jit_vjp_of_int  TypeError: Called ad... 20220728T00:41:03.1467420Z stderr: FAILED tests/api_test.py::APITest::test_vjp_of_int_shapes  TypeError: (Shape... 20220728T00:41:03.1468005Z stderr: FAILED tests/api_test.py::APITest::test_vjp_of_int_fulllike  KeyError: dtype... 20220728T00:41:03.1468604Z stderr: FAILED tests/api_test.py::CustomJVPTest::test_float0  AssertionError: dtype(... 20220728T00:41:03.1469198Z stderr: FAILED tests/api_test.py::CustomJVPTest::test_float0_initial_style  Assertio... 20220728T00:41:03.1469786Z stderr: FAILED tests/api_test.py::CustomVJPTest::test_float0  KeyError: dtype([('flo... 20220728T00:41:03.1470267Z stderr: FAILED tests/api_test.py::CleanupTest::test_call_wrapped_second_phase_cleanup 20220728T00:41:03.1470855Z stderr: FAILED tests/api_test.py::CustomVJPTest::test_float0_initial_style  Assertio... 20220728T00:41:03.1471446Z stderr: FAILED tests/core_test.py::CoreTest::test_reference_cycles_jit  AssertionErr... 20220728T00:41:03.1471942Z stderr: FAILED tests/lax_numpy_indexing_test.py::IndexingTest::testJVPOfGradOfIndexing 20220728T00:41:03.1472485Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testOperatorOverload__lshift___uint16_uint32[] 20220728T00:41:03.1473052Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testOperatorOverload__rshift___uint16_uint32[] 20220728T00:41:03.1473765Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testDeleteIndexArray_bool[1,4]_axis=1_idx=pyint64 20220728T00:41:03.1474342Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testDeleteIndexArray_uint16[2,3,4]_axis=2_idx=int64 20220728T00:41:03.1474905Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testDeleteIndexArray_uint32[2,3,4]_axis=2_idx=int64 20220728T00:41:03.1475440Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testPiecewise_bfloat16_ncond=3_nfunc=3 20220728T00:41:03.1475967Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testPiecewise_int16_ncond=1_nfunc=2 20220728T00:41:03.1476496Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testPiecewise_pyint32_ncond=2_nfunc=2 20220728T00:41:03.1477073Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testRightOperatorOverload__rlshift___uint16_uint32[] 20220728T00:41:03.1477781Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testRightOperatorOverload__rrshift___uint16_uint32[] 20220728T00:41:03.1478374Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testTake_bfloat16[3,4]_index=pyint32_axis=None_mode=clip 20220728T00:41:03.1478937Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testTake_int16[3,4]_index=pyint16_axis=None_mode=None 20220728T00:41:03.1479638Z stderr: FAILED tests/lax_numpy_test.py::LaxBackedNumpyTests::testTake_int32[3,4,5]_index=int32_axis=1_mode=None 20220728T00:41:03.1480350Z stderr: FAILED tests/lax_numpy_test.py::NumpySignaturesTest::testWrappedSignaturesMatch 20220728T00:41:03.1480994Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGrad  KeyError: dtype(... 20220728T00:41:03.1481614Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGradInf  KeyError: dty... 20220728T00:41:03.1482225Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGradNan  KeyError: dty... 20220728T00:41:03.1482946Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGradNegInf  KeyError: ... 20220728T00:41:03.1483575Z stderr: FAILED tests/nn_test.py::NNFunctionsTest::testSoftplusGradZero  KeyError: dt... 20220728T00:41:03.1484343Z stderr: FAILED tests/random_test.py::LaxRandomTest::testPermutation_dtype=_range_or_shape=(0, 5)_axis=1_independent=False 20220728T00:41:03.1485153Z stderr: FAILED tests/random_test.py::LaxRandomTest::testPermutation_dtype=_range_or_shape=(0, 5)_axis=2_independent=True 20220728T00:41:03.1486016Z stderr: FAILED tests/random_test.py::LaxRandomTest::testPermutation_dtype=_range_or_shape=(0, 5)_axis=1_independent=False 20220728T00:41:03.1486822Z stderr: FAILED tests/random_test.py::LaxRandomTest::testPermutation_dtype=_range_or_shape=(0,)_axis=1_independent=False 20220728T00:41:03.1487372Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_complex64[5,5] 20220728T00:41:03.1487859Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_complex64[5,8] 20220728T00:41:03.1488375Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_complex64[8,5] 20220728T00:41:03.1488849Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_complex64[8,8] 20220728T00:41:03.1489320Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_float32[5,5] 20220728T00:41:03.1489790Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_float32[5,8] 20220728T00:41:03.1490252Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_float32[8,5] 20220728T00:41:03.1490709Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_csr_fromdense_ad_float32[8,8] 20220728T00:41:03.1491169Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_complex64[5,5] 20220728T00:41:03.1491650Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_complex64[5,8] 20220728T00:41:03.1492124Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_complex64[8,5] 20220728T00:41:03.1492595Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_complex64[8,8] 20220728T00:41:03.1493069Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_float32[5,5] 20220728T00:41:03.1493516Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_float32[5,8] 20220728T00:41:03.1493974Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_float32[8,5] 20220728T00:41:03.1494438Z stderr: FAILED tests/sparse_test.py::cuSparseTest::test_coo_fromdense_ad_float32[8,8] 20220728T00:41:03.1494868Z stderr: ========= 59 failed, 14190 passed, 1447 skipped in 1589.83s (0:26:29) ========== ``` You can access the full build logs here: https://github.com/samuela/nixpkgsupkeep/runs/7551101765?check_suite_focus=true. To me, this indicates a dependency version issue, possibly with numpy. However, all of the versions are within their constraints as far as `python setup.py build` is concerned. Here are the relevant pip freeze dependency versions: ``` [nixshell:~/dev/nixpkgs]$ pip freeze abslpy @ file:///build/abslpy1.0.0/dist/absl_py1.0.0py3noneany.whl flatbuffers @ file:///build/source/python/dist/flatbuffers2.0.0py2.py3noneany.whl jax @ file:///build/source/dist/jax0.3.6py3noneany.whl jaxlib @ file:///build/dist/jaxlib0.3.0cp39nonemanylinux2010_x86_64.whl numpy @ file:///build/numpy1.21.5/dist/numpy1.21.5cp39cp39linux_x86_64.whl opteinsum @ file:///build/opt_einsum3.3.0/dist/opt_einsum3.3.0py3noneany.whl SciPy @ file:///build/scipy1.8.0/dist/SciPy1.8.0cp39cp39linux_x86_64.whl six @ file:///build/six1.16.0/dist/six1.16.0py2.py3noneany.whl typing_extensions @ file:///build/typing_extensions4.1.1/dist/typing_extensions4.1.1py3noneany.whl ``` More information is available in the CI run linked above.",2022-08-03T23:22:30Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/11722,Where is `float0` documented? I'm not able to find it in either numpy or jax.numpy docs.,"I'm also seeing another error of a slightly different flavor: ``` =================================== FAILURES =================================== _____________________ IndexingTest.testJVPOfGradOfIndexing _____________________ [gw0] linux  Python 3.10.5 /nix/store/rc9cz7z4qlgmsbwvpw2acig5g2rdws46python33.10.5/bin/python3.10 self =      def testJVPOfGradOfIndexing(self):        Should return a value, even though we didn't pass a symbolic zero as the        index tangent.       x = jnp.ones((3, 4), jnp.float32)       i = jnp.ones((3,), jnp.int32)       f = lambda x, i: jnp.sum(x[i]) >     primals, tangents = jax.jvp(jax.grad(f), (x, i),                                   (x, np.zeros(i.shape, dtypes.float0))) tests/lax_numpy_indexing_test.py:839:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/api.py:2204: in jvp     return _jvp(lu.wrap_init(fun), primals, tangents, has_aux=has_aux) jax/_src/api.py:2233: in _jvp     out_primals, out_tangents = ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat) jax/linear_util.py:168: in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs)) jax/_src/api.py:911: in grad_f     _, g = value_and_grad_f(*args, **kwargs) jax/_src/api.py:987: in value_and_grad_f     ans, vjp_py = _vjp(f_partial, *dyn_args, reduce_axes=reduce_axes) jax/_src/api.py:2446: in _vjp     out_primal, out_vjp = ad.vjp( jax/interpreters/ad.py:129: in vjp     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals) jax/interpreters/ad.py:116: in linearize     jaxpr, out_pvals, consts = pe.trace_to_jaxpr(jvpfun_flat, in_pvals) jax/_src/profiler.py:206: in wrapper     return func(*args, **kwargs) jax/interpreters/partial_eval.py:606: in trace_to_jaxpr     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals) jax/linear_util.py:168: in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs)) tests/lax_numpy_indexing_test.py:838: in      f = lambda x, i: jnp.sum(x[i]) jax/core.py:580: in __getitem__     def __getitem__(self, idx): return self.aval._getitem(self, idx) jax/_src/numpy/lax_numpy.py:3494: in _rewriting_take     return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, jax/_src/numpy/lax_numpy.py:3503: in _gather     indexer = _index_to_gather(shape(arr), idx)   shared with _scatter_update jax/_src/numpy/lax_numpy.py:3630: in _index_to_gather     advanced_indexes, idx_advanced_axes, x_advanced_axes = zip(*advanced_pairs) jax/_src/numpy/lax_numpy.py:3628: in      advanced_pairs = ((_normalize_index(e, x_shape[j]), i, j) jax/_src/numpy/lax_numpy.py:3399: in _normalize_index     return lax.select( jax/_src/lax/lax.py:850: in select     return select_n_p.bind(pred, on_false, on_true) jax/core.py:323: in bind     return self.bind_with_trace(find_top_trace(args), args, params) jax/core.py:326: in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params) jax/interpreters/ad.py:309: in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params) jax/_src/lax/lax.py:3348: in _select_jvp     z = _zeros(next(t for t in case_tangents if type(t) is not ad_util.Zero)) jax/_src/lax/lax.py:1238: in full_like     return full(fill_shape, _convert_element_type(fill_value, dtype, weak_type)) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  operand = 0, new_dtype = dtype([('float0', 'V')]), weak_type = False     def _convert_element_type(operand: Array, new_dtype: Optional[DType] = None,                               weak_type: bool = False):        Don't canonicalize old_dtype because x64 context might cause        uncanonicalized operands to be passed in.       old_dtype = dtypes.dtype(operand, canonicalize=False)       old_weak_type = dtypes.is_weakly_typed(operand)       if new_dtype is None:         new_dtype = old_dtype       else:         new_dtype = np.dtype(new_dtype)       new_dtype = dtypes.dtype(new_dtype, canonicalize=True)       new_weak_type = bool(weak_type)       if (dtypes.issubdtype(old_dtype, np.complexfloating) and           not dtypes.issubdtype(new_dtype, np.complexfloating)):         msg = ""Casting complex values to real discards the imaginary part""         warnings.warn(msg, np.ComplexWarning, stacklevel=2)        Python has big integers, but convert_element_type(2 ** 100, np.float32) need        not be an error since the target dtype fits the value. Handle this case by        converting to a NumPy array before calling bind. Without this step, we'd        first canonicalize the input to a value of dtype int32 or int64, leading to        an overflow error.       if type(operand) is int: >       operand = np.asarray(operand, new_dtype) E       TypeError: a byteslike object is required, not 'int' jax/_src/lax/lax.py:514: TypeError =========================== short test summary info ============================ FAILED tests/lax_numpy_indexing_test.py::IndexingTest::testJVPOfGradOfIndexing =========== 1 failed, 8154 passed, 802 skipped in 514.34s (0:08:34) ============ ```","This looks a lot like CC(autodiff fails under numpy 1.23), which happens when jax v0.3.13 or older is used with numpy version 1.23.0 or newer. I'd suggest upgrading to the latest JAX version, or if you need to use an earlier JAX version, install numpy 1.22 or older.",Thanks ! I'm working on a jax/jaxlib upgrade now but it may take a few days for us to execute. I'll let you know how it goes...,"Closing, on the assumption that  's theory is correct. Please reopen if there's still a problem!"
4789,"以下是一个github上的jax下的一个issue, 标题是(UNKNOWN bug)， 内容是 (Please:  [x ] Check for duplicate issues.  [x ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```bash  clone jkonet from github  follow jkonet setup instructions $ python main.py out_dir results config_folder configs task semicircle ```  [x ] If applicable, include full error messages/tracebacks. ``` /home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.   PyTreeDef = type(jax.tree_structure(None)) Started run. 20220801 17:28:07.861579: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_asm_compiler.cc:57] cuLinkAddData fails. This is usually caused by stale driver version. 20220801 17:28:07.861611: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:1325] The CUDA linking API did not work. Please use XLA_FLAGS=xla_gpu_force_compilation_parallelism=1 to bypass it, but expect to get longer compilation time due to the lack of multithreading. Traceback (most recent call last):   File ""/home/USER/projects/jkonet/main.py"", line 292, in      main(args)   File ""/home/USER/projects/jkonet/main.py"", line 266, in main     run_jko(config, task_dir=task_dir, logging=args.wandb)   File ""/home/USER/projects/jkonet/main.py"", line 32, in run_jko     rng = jax.random.PRNGKey(int(time.time()))   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/random.py"", line 125, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/prng.py"", line 233, in seed_with_impl     return PRNGKeyArray(impl, impl.seed(seed))   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/prng.py"", line 272, in threefry_seed     lax.shift_right_logical(seed_arr, lax_internal._const(seed_arr, 32)))   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 487, in shift_right_logical     return shift_right_logical_p.bind(x, y)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/core.py"", line 324, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/core.py"", line 327, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/core.py"", line 684, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 99, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/util.py"", line 220, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/util.py"", line 213, in cached     return f(*args, **kwargs)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 164, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), device, None,   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 248, in _xla_callable_uncached     return lower_xla_callable(fun, device, backend, name, donated_invars, False,   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 827, in compile     self._executable = XlaCompiledComputation.from_xla_computation(   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 934, in from_xla_computation     compiled = compile_or_get_cached(backend, xla_computation, options,   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 899, in compile_or_get_cached     return backend_compile(backend, computation, compile_options, host_callbacks)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 294, in wrapper     return func(*args, **kwargs)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 843, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: no kernel image is available for execution on the device in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_asm_compiler.cc(60): 'status' ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,UNKNOWN bug,"Please:  [x ] Check for duplicate issues.  [x ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```bash  clone jkonet from github  follow jkonet setup instructions $ python main.py out_dir results config_folder configs task semicircle ```  [x ] If applicable, include full error messages/tracebacks. ``` /home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.   PyTreeDef = type(jax.tree_structure(None)) Started run. 20220801 17:28:07.861579: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_asm_compiler.cc:57] cuLinkAddData fails. This is usually caused by stale driver version. 20220801 17:28:07.861611: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:1325] The CUDA linking API did not work. Please use XLA_FLAGS=xla_gpu_force_compilation_parallelism=1 to bypass it, but expect to get longer compilation time due to the lack of multithreading. Traceback (most recent call last):   File ""/home/USER/projects/jkonet/main.py"", line 292, in      main(args)   File ""/home/USER/projects/jkonet/main.py"", line 266, in main     run_jko(config, task_dir=task_dir, logging=args.wandb)   File ""/home/USER/projects/jkonet/main.py"", line 32, in run_jko     rng = jax.random.PRNGKey(int(time.time()))   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/random.py"", line 125, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/prng.py"", line 233, in seed_with_impl     return PRNGKeyArray(impl, impl.seed(seed))   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/prng.py"", line 272, in threefry_seed     lax.shift_right_logical(seed_arr, lax_internal._const(seed_arr, 32)))   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 487, in shift_right_logical     return shift_right_logical_p.bind(x, y)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/core.py"", line 324, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/core.py"", line 327, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/core.py"", line 684, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 99, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/util.py"", line 220, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/util.py"", line 213, in cached     return f(*args, **kwargs)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 164, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), device, None,   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 248, in _xla_callable_uncached     return lower_xla_callable(fun, device, backend, name, donated_invars, False,   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 827, in compile     self._executable = XlaCompiledComputation.from_xla_computation(   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 934, in from_xla_computation     compiled = compile_or_get_cached(backend, xla_computation, options,   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 899, in compile_or_get_cached     return backend_compile(backend, computation, compile_options, host_callbacks)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 294, in wrapper     return func(*args, **kwargs)   File ""/home/USER/anaconda3/envs/jko/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 843, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: UNKNOWN: no kernel image is available for execution on the device in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_asm_compiler.cc(60): 'status' ```",2022-08-01T21:30:06Z,bug NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/11699,"I strongly suspect that this means your NVidia driver is too old for your CUDA/`ptxas` release. Please see the requirements listed here: https://github.com/google/jaxpipinstallationgpucuda notably the sentence ""You must use an NVidia driver version that is at least as new as your CUDA toolkit's corresponding driver version."" What do `nvidiasmi` and `ptxas version` show? Closing since this is very likely an NVidia driver version problem.",  `nvidiasmi` v510.68.02 `nvcc version` 11.7.64 `ptxas version` 11.7.64 Sorry if this is basic how do I install the latest drivers on wsl2?,I think you just update your Windows NVidia driver.
994,"以下是一个github上的jax下的一个issue, 标题是(fix _scan_partial_eval_custom loop-hoisting bug)， 内容是 (After applying the constanthoisting optimization in the new scan partial eval rule, we need to account for the potentially new constant part of the signature of `jaxpr_known_loop` compared to `jaxpr_known`; that is, `jaxpr_known_loop` might take different constant inputs than `jaxpr_known`. Before this fix we were (re)using the stale `const_uk` and `unks_in` values to set up the `num_consts` and `linear` for the known part of the scan. We could repro the error by making a loop where the number of constants of `jaxpr_known` is different from the number of constants for `jaxpr_known_loop`, e.g. by having two loopinvariant values computed from one constant input, as in the test added here. I verified this test fails on main. Also passes a realworld test from .)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fix _scan_partial_eval_custom loop-hoisting bug,"After applying the constanthoisting optimization in the new scan partial eval rule, we need to account for the potentially new constant part of the signature of `jaxpr_known_loop` compared to `jaxpr_known`; that is, `jaxpr_known_loop` might take different constant inputs than `jaxpr_known`. Before this fix we were (re)using the stale `const_uk` and `unks_in` values to set up the `num_consts` and `linear` for the known part of the scan. We could repro the error by making a loop where the number of constants of `jaxpr_known` is different from the number of constants for `jaxpr_known_loop`, e.g. by having two loopinvariant values computed from one constant input, as in the test added here. I verified this test fails on main. Also passes a realworld test from .",2022-07-26T08:02:39Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/11618
1208,"以下是一个github上的jax下的一个issue, 标题是(unexpected ppermute behavior for host-ragged arrays)， 内容是 (When ppermute is invoked on arrays that are ragged in the nonpmap'd dimension (ie each host has a different length array) the results are surprising. It seems to take the shape from the destination host's current array and the values (padded with 0) from the source host. On a system with 4 hosts and two local devices per host the following code will produce the following output: ```py data = jp.arange(4*(jax.process_index()+1)).reshape(2, 1) * jax.process_index() out = jax.pmap(ft.partial(jax.lax.ppermute, axis_name='host', perm=[(0, 4), (1, 5), (2, 6), (3, 7), (4, 0), (5, 1), (6, 2), (7, 3)]), 'host')(data) print('result: ', jax.process_index(), data, out) ``` ``` result:  0 [[0 0]  [0 0]] [[ 0  2]  [12 14]] result:  1 [[0 1 2 3]  [4 5 6 7]] [[ 0  3  6  9]  [24 27 30 33]] result:  2 [[ 0  2  4  6  8 10]  [12 14 16 18 20 22]] [[0 0 0 0 0 0]  [0 0 0 0 0 0]] result:  3 [[ 0  3  6  9 12 15 18 21]  [24 27 30 33 36 39 42 45]] [[0 1 2 3 0 0 0 0]  [4 5 6 7 0 0 0 0]] ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,unexpected ppermute behavior for host-ragged arrays,"When ppermute is invoked on arrays that are ragged in the nonpmap'd dimension (ie each host has a different length array) the results are surprising. It seems to take the shape from the destination host's current array and the values (padded with 0) from the source host. On a system with 4 hosts and two local devices per host the following code will produce the following output: ```py data = jp.arange(4*(jax.process_index()+1)).reshape(2, 1) * jax.process_index() out = jax.pmap(ft.partial(jax.lax.ppermute, axis_name='host', perm=[(0, 4), (1, 5), (2, 6), (3, 7), (4, 0), (5, 1), (6, 2), (7, 3)]), 'host')(data) print('result: ', jax.process_index(), data, out) ``` ``` result:  0 [[0 0]  [0 0]] [[ 0  2]  [12 14]] result:  1 [[0 1 2 3]  [4 5 6 7]] [[ 0  3  6  9]  [24 27 30 33]] result:  2 [[ 0  2  4  6  8 10]  [12 14 16 18 20 22]] [[0 0 0 0 0 0]  [0 0 0 0 0 0]] result:  3 [[ 0  3  6  9 12 15 18 21]  [24 27 30 33 36 39 42 45]] [[0 1 2 3 0 0 0 0]  [4 5 6 7 0 0 0 0]] ```",2022-07-25T23:55:20Z,,closed,0,1,https://github.com/jax-ml/jax/issues/11614,"> arrays that are ragged in the nonpmap'd dimension (ie each host has a different length array) This is undefined behavior in jax! See the first bullet point at the end of https://jax.readthedocs.io/en/latest/multi_process.htmlrunningmultiprocesscomputations: > Processes passing differentlyshaped inputs to the same parallel function can cause hangs or incorrect return values. Differentlyshaped inputs are safe so long as they result in identicallyshaped perdevice data shards across processes; e.g. passing in different leading batch sizes in order to run on different numbers of local devices per process is ok, but having each process pad its batch to a different max example length is not. We have some work inprogress to raise an error message instead of silently doing the wrong thing, at least on TPU, but it's been stalled for a while and is blocked on a larger change that won't land for a few months. I'll see if we make this happen eventually, but for now, don't do this :)"
7640,"以下是一个github上的jax下的一个issue, 标题是(jaxlib v0.3.15 changelog missing? Significant build changes?)， 内容是 (There is at least one other report of a build failure: https://github.com/google/jax/issues/11597. In condaforge, our jaxlib builds completely failed for different reasons after trying to update to jaxlib v0.3.15. They fail differently on cuda and noncuda builds. For cuda, which is the more pertinent error, it looks like: ``` INFO: Found 1 target... [0 / 4] [Prepa] Creating source manifest for //build:build_wheel [27 / 143] Compiling src/google/protobuf/stubs/structurally_valid.cc; 0s local ... (2 actions, 1 running) [38 / 143] Compiling src/google/protobuf/generated_message_util.cc; 1s local ... (2 actions running) ERROR: /home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/share/bazel/8372a0709531b2dba2d9a9ce09c7583d/external/com_google_protobuf/BUILD:111:11: Linking external/com_google_protobuf/libprotobuf_lite.lo failed: (Exit 1): ar failed: error executing command    (cd /home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/share/bazel/8372a0709531b2dba2d9a9ce09c7583d/execroot/__main__ && \   exec env  \     CUDA_TOOLKIT_PATH=/usr/local/cuda \     CUDNN_INSTALL_PATH=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac \     GCC_HOST_COMPILER_PATH=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin/x86_64condalinuxgnugcc \     GCC_HOST_COMPILER_PREFIX=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin \     PATH=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin:/opt/conda/condabin:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/conda/bin:/usr/local/cuda/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_50,sm_60,sm_62,sm_70,sm_72,sm_75,sm_80,sm_86,compute_86 \     TF_CUDA_PATHS=/usr/local/cuda,/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac \     TF_CUDA_VERSION=11.2 \     TF_CUDNN_VERSION=8 \   /home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin/ar out/k8opt/bin/external/com_google_protobuf/libprotobuf_lite.lo2.params)  Configuration: 740083763ed3206026a09abc2faa9ac5a84e6f9dfcfd22c27729d3542ad1c4a3  Execution platform: //:platform src/main/tools/processwrapperlegacy.cc:80: ""execvp(/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin/ar, ...)"": No such file or directory Target //build:build_wheel failed to build INFO: Elapsed time: 56.913s, Critical Path: 3.40s INFO: 47 processes: 18 internal, 29 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully ``` On noncuda, we get this error, which I think is more likely something on our end, but I cannot figure it out yet...  ``` [6,508 / 6,659] Compiling llvm/lib/Target/X86/X86ISelLowering.cpp; 35s local ... (2 actions running) Target //build:build_wheel uptodate:   bazelbin/build/build_wheel INFO: Elapsed time: 13046.285s, Critical Path: 205.49s INFO: 7181 processes: 616 internal, 6565 local. INFO: Build completed successfully, 7181 total actions INFO: Running command line: bazelbin/build/build_wheel 'output_path=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/work/dist' 'cpu=x86_64' INFO: Build completed successfully, 7181 total actions /home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib/python3.8/sitepackages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standardsbased tools.   warnings.warn( Output wheel: /home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/work/dist/jaxlib0.3.15cp38nonemanylinux2014_x86_64.whl To install the newlybuilt jaxlib wheel, run:   pip install /home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/work/dist/jaxlib0.3.15cp38nonemanylinux2014_x86_64.whl INFO: Reading rc options for 'shutdown' from /home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/work/.jax_configure.bazelrc:   Inherited 'common' options: crosstool_top=//bazel_toolchain:toolchain logging=6 verbose_failures toolchain_resolution_debug define=PREFIX=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac define=PROTOBUF_INCLUDE_PATH=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/include local_cpu_resources=2 cpu k8 ERROR: crosstool_top=//bazel_toolchain:toolchain :: Unrecognized option: crosstool_top=//bazel_toolchain:toolchain b'' ```  1. Could the changelog be updated at some point? Note that at least one more PR seems to be destined for v0.3.15 has been added, even though v0.3.15 has been released already on pypi... https://github.com/google/jax/pull/11602 2. Could someone have a look at the above errors and see if something obvious we should be aware of in packaging at condaforge? As a reminder we have our own custom bazel toolchain, so most likely at least some of the errors are related to that. I am still working on determining what exactly. I only recently got involved in packaging using bazel on condaforge... Current live PR: https://github.com/condaforge/jaxlibfeedstock/pull/118 Previous PR with failures: https://github.com/condaforge/jaxlibfeedstock/pull/116  Please:  [ ] Check for duplicate issues.  [ ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import jax.numpy as jnp print(jnp.arange(10))  [0 1 2 3 4 5 6 7 8 9] ```  [ ] If applicable, include full error messages/tracebacks.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jaxlib v0.3.15 changelog missing? Significant build changes?,"There is at least one other report of a build failure: https://github.com/google/jax/issues/11597. In condaforge, our jaxlib builds completely failed for different reasons after trying to update to jaxlib v0.3.15. They fail differently on cuda and noncuda builds. For cuda, which is the more pertinent error, it looks like: ``` INFO: Found 1 target... [0 / 4] [Prepa] Creating source manifest for //build:build_wheel [27 / 143] Compiling src/google/protobuf/stubs/structurally_valid.cc; 0s local ... (2 actions, 1 running) [38 / 143] Compiling src/google/protobuf/generated_message_util.cc; 1s local ... (2 actions running) ERROR: /home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/share/bazel/8372a0709531b2dba2d9a9ce09c7583d/external/com_google_protobuf/BUILD:111:11: Linking external/com_google_protobuf/libprotobuf_lite.lo failed: (Exit 1): ar failed: error executing command    (cd /home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/share/bazel/8372a0709531b2dba2d9a9ce09c7583d/execroot/__main__ && \   exec env  \     CUDA_TOOLKIT_PATH=/usr/local/cuda \     CUDNN_INSTALL_PATH=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac \     GCC_HOST_COMPILER_PATH=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin/x86_64condalinuxgnugcc \     GCC_HOST_COMPILER_PREFIX=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin \     PATH=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin:/opt/conda/condabin:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac:/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/conda/bin:/usr/local/cuda/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_50,sm_60,sm_62,sm_70,sm_72,sm_75,sm_80,sm_86,compute_86 \     TF_CUDA_PATHS=/usr/local/cuda,/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac \     TF_CUDA_VERSION=11.2 \     TF_CUDNN_VERSION=8 \   /home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin/ar out/k8opt/bin/external/com_google_protobuf/libprotobuf_lite.lo2.params)  Configuration: 740083763ed3206026a09abc2faa9ac5a84e6f9dfcfd22c27729d3542ad1c4a3  Execution platform: //:platform src/main/tools/processwrapperlegacy.cc:80: ""execvp(/home/conda/feedstock_root/build_artifacts/jaxlib_1658602566191/_build_env/bin/ar, ...)"": No such file or directory Target //build:build_wheel failed to build INFO: Elapsed time: 56.913s, Critical Path: 3.40s INFO: 47 processes: 18 internal, 29 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully ``` On noncuda, we get this error, which I think is more likely something on our end, but I cannot figure it out yet...  ``` [6,508 / 6,659] Compiling llvm/lib/Target/X86/X86ISelLowering.cpp; 35s local ... (2 actions running) Target //build:build_wheel uptodate:   bazelbin/build/build_wheel INFO: Elapsed time: 13046.285s, Critical Path: 205.49s INFO: 7181 processes: 616 internal, 6565 local. INFO: Build completed successfully, 7181 total actions INFO: Running command line: bazelbin/build/build_wheel 'output_path=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/work/dist' 'cpu=x86_64' INFO: Build completed successfully, 7181 total actions /home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/lib/python3.8/sitepackages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standardsbased tools.   warnings.warn( Output wheel: /home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/work/dist/jaxlib0.3.15cp38nonemanylinux2014_x86_64.whl To install the newlybuilt jaxlib wheel, run:   pip install /home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/work/dist/jaxlib0.3.15cp38nonemanylinux2014_x86_64.whl INFO: Reading rc options for 'shutdown' from /home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/work/.jax_configure.bazelrc:   Inherited 'common' options: crosstool_top=//bazel_toolchain:toolchain logging=6 verbose_failures toolchain_resolution_debug define=PREFIX=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac define=PROTOBUF_INCLUDE_PATH=/home/conda/feedstock_root/build_artifacts/jaxlib_1658602489386/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/include local_cpu_resources=2 cpu k8 ERROR: crosstool_top=//bazel_toolchain:toolchain :: Unrecognized option: crosstool_top=//bazel_toolchain:toolchain b'' ```  1. Could the changelog be updated at some point? Note that at least one more PR seems to be destined for v0.3.15 has been added, even though v0.3.15 has been released already on pypi... https://github.com/google/jax/pull/11602 2. Could someone have a look at the above errors and see if something obvious we should be aware of in packaging at condaforge? As a reminder we have our own custom bazel toolchain, so most likely at least some of the errors are related to that. I am still working on determining what exactly. I only recently got involved in packaging using bazel on condaforge... Current live PR: https://github.com/condaforge/jaxlibfeedstock/pull/118 Previous PR with failures: https://github.com/condaforge/jaxlibfeedstock/pull/116  Please:  [ ] Check for duplicate issues.  [ ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import jax.numpy as jnp print(jnp.arange(10))  [0 1 2 3 4 5 6 7 8 9] ```  [ ] If applicable, include full error messages/tracebacks.",2022-07-24T13:39:06Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/11603,"A small update, I am making progress slowly by working through abseil_cpp and grpc_cpp dependencies, so 🤞 ",(This is likely an issue on our end with abseil_cpp and grpc_cpp versions... hence closing for now)
42643,"以下是一个github上的jax下的一个issue, 标题是(Build failure for jax v0.3.15 on Windows 10)， 内容是 (On windows 10 box with python 3.9, cuda 11.7, cudnn  8.4.0 using the following command `python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cuda_compute_capabilities=""7.5"" cuda_version=""11.7"" cudnn_version=""8.4.0"" noenable_rocm noenable_tpu` fails to build from source with the following error. ``` ERROR: C:/users/adam/_bazel_adam/rjlwuxqw/external/org_tensorflow/tensorflow/compiler/mlir/xla/BUILD:560:11: Compiling tensorflow/compiler/mlir/xla/hlo_function_importer.: (Exit 2): python.exe failed: error executing command   cd /d C:/users/adam/_bazel_adam/rjlwuxqw/execroot/__main__   SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt     SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64     SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja     SET PWD=/proc/self/cwd     SET RUNFILES_MANIFEST_ONLY=1     SET TEMP=C:\Users\Adam\AppData\Local\Temp     SET TF_CUDA_COMPUTE_CAPABILITIES=7.5     SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET TF_CUDA_VERSION=11.7     SET TF_CUDNN_VERSION=8.4.0     SET TMP=C:\Users\Adam\AppData\Local\Temp   C:\Users\Adam\anaconda3\envs\jax_latest\python.exe B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/org_tensorflow /Ibazelout/x64_windowsopt/bin/external/org_tensorflow /Iexternal/llvmproject /Ibazelout/x64_windowsopt/bin/external/llvmproject /Iexternal/llvm_terminfo /Ibazelout/x64_windowsopt/bin/external/llvm_terminfo /Iexternal/llvm_zlib /Ibazelout/x64_windowsopt/bin/external/llvm_zlib /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazelout/x64_windowsopt/bin/external/com_google_absl /Iexternal/nsync /Ibazelout/x64_windowsopt/bin/external/nsync /Iexternal/gif /Ibazelout/x64_windowsopt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazelout/x64_windowsopt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazelout/x64_windowsopt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazelout/x64_windowsopt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazelout/x64_windowsopt/bin/external/fft2d /Iexternal/highwayhash /Ibazelout/x64_windowsopt/bin/external/highwayhash /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/double_conversion /Ibazelout/x64_windowsopt/bin/external/double_conversion /Iexternal/snappy /Ibazelout/x64_windowsopt/bin/external/snappy /Iexternal/local_config_cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/canonicalize_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinDialectIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinLocationAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CastOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FunctionInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SubElementInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorEncodingIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_pattern_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/InferIntRangeInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LoopLikeInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ViewLikeInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithmeticBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithmeticCanonicalizationIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithmeticOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ControlFlowOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FuncIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CopyOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ParserTokenKinds /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/QuantOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/QuantPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLTypesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ConversionPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformsPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapeOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ParallelCombiningOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TilingInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorAttrDefsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorOpsIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_structs_inc_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AffineOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AllocationOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizableOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationOpsIncGen /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Iexternal/org_tensorflow/tensorflow/compiler/mlir/hlo/include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/include /Iexternal/llvmproject/llvm/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/include /Iexternal/llvmproject/mlir/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/include /Iexternal/org_tensorflow/third_party/eigen3/mkl_include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazelout/x64_windowsopt/bin/external/nsync/public /Iexternal/gif /Ibazelout/x64_windowsopt/bin/external/gif /Iexternal/gif/windows /Ibazelout/x64_windowsopt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazelout/x64_windowsopt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/org_tensorflow/tensorflow/compiler/mlir/xla/include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/xla/include /Iexternal/local_config_cuda/cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLTDL_SHLIB_EXT="".dll"" /DLLVM_PLUGIN_EXT="".dll"" /DLLVM_NATIVE_ARCH=""X86"" /DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser /DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter /DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler /DLLVM_NATIVE_TARGET=LLVMInitializeX86Target /DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo /DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC /DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA /DLLVM_HOST_TRIPLE=""x86_64pcwin32"" /DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64pcwin32"" /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DBLAKE3_USE_NEON=0 /DBLAKE3_NO_AVX2 /DBLAKE3_NO_AVX512 /DBLAKE3_NO_SSE2 /DBLAKE3_NO_SSE41 /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG /D_USE_MATH_DEFINES DWIN32_LEAN_AND_MEAN DNOGDI /Zc:preprocessor DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. /std:c++17 /Fobazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/xla/_objs/hlo_module_importer/hlo_function_importer.obj /c external/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc  Configuration: ca98985668391b64b455964c5200157bb79165bf6145a97d1e521c47bb0387fb  Execution platform: //:platform Target //build:build_wheel failed to build INFO: Elapsed time: 4542.323s, Critical Path: 506.90s INFO: 9670 processes: 3308 internal, 6362 local. FAILED: Build did NOT complete successfully FAILED: Build did NOT complete successfully b""external/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/Support/LogicalResult.h(78): note: see reference to class template instantiation 'llvm::Optional' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen\\mlir/IR/BuiltinAttributeInterfaces.h.inc(197): note: see reference to class template instantiation 'mlir::FailureOr' being compiled\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen\\mlir/IR/BuiltinAttributeInterfaces.h.inc(336): note: see reference to class template instantiation 'mlir::detail::ElementsAttrTrait' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1185): note: see reference to class template instantiation 'llvm::SmallVectorImpl' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/IR/OperationSupport.h(638): note: see reference to class template instantiation 'llvm::SmallVector>,1>' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/IR/Diagnostics.h(395): note: see reference to class template instantiation 'llvm::Optional' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1185): note: see reference to class template instantiation 'llvm::SmallVectorImpl' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1161): note: see reference to class template instantiation 'llvm::SmallVector' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/Interfaces/InferTypeOpInterface.h(29): note: see reference to class template instantiation 'llvm::CalculateSmallVectorDefaultInlinedElements' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorInterfacesIncGen\\mlir/Interfaces/VectorInterfaces.h.inc(433): note: see reference to class template instantiation 'llvm::Optional>' being compiled\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorInterfacesIncGen\\mlir/Interfaces/VectorInterfaces.h.inc(442): note: see reference to class template instantiation 'mlir::detail::VectorUnrollOpInterfaceTrait' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SymbolInterfacesIncGen\\mlir/IR/SymbolInterfaces.h.inc(178): note: see reference to class template instantiation 'llvm::Optional' being compiled\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SymbolInterfacesIncGen\\mlir/IR/SymbolInterfaces.h.inc(229): note: see reference to class template instantiation 'mlir::detail::SymbolOpInterfaceTrait' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/Support/LogicalResult.h(78): note: see reference to class template instantiation 'llvm::Optional' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/IR/DialectImplementation.h(100): note: see reference to class template instantiation 'mlir::FailureOr' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1185): note: see reference to class template instantiation 'llvm::SmallVectorImpl' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/SourceMgr.h(290): note: see reference to class template instantiation 'llvm::SmallVector' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1378): error C2665: 'mlir::mhlo::symbolizeDomainKind': none of the 2 overloads could convert all the argument types\r\nbazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/include\\mlirhlo/Dialect/mhlo/IR/hlo_ops_base_enums.h.inc(243): note: could be 'llvm::Optional mlir::mhlo::symbolizeDomainKind(llvm::StringRef)'\r\nbazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/include\\mlirhlo/Dialect/mhlo/IR/hlo_ops_base_enums.h.inc(241): note: or       'llvm::Optional mlir::mhlo::symbolizeDomainKind(uint32_t)'\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1379): note: while trying to match the argument list '(absl::lts_20211102::string_view)'\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): error C3536: 'domain_kind': cannot be used before it is initialized\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): error C2100: illegal indirection\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): error C2440: 'type cast': cannot convert from 'mlir::mhlo::DomainKind' to 'unsigned int'\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): note: This conversion requires an explicit cast (static_cast, Cstyle cast or functionstyle cast)\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): error C2440: '!=': cannot convert from 'int' to 'mlir::mhlo::DomainKind'\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): note: Conversion to enumeration type requires an explicit cast (static_cast, Cstyle cast or functionstyle cast)\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1387): error C2100: illegal indirection\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1385): error C2660: 'mlir::Builder::getNamedAttr': function does not take 1 arguments\r\nexternal/llvmproject/mlir/include\\mlir/IR/Builders.h(95): note: see declaration of 'mlir::Builder::getNamedAttr'\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1185): note: see reference to class template instantiation 'llvm::SmallVectorImpl' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1630): note: see reference to class template instantiation 'llvm::SmallVector' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\n"" Traceback (most recent call last):   File ""C:\sdks\jaxjaxlibv0.3.15\build\build.py"", line 555, in      main()   File ""C:\sdks\jaxjaxlibv0.3.15\build\build.py"", line 550, in main     shell(command)   File ""C:\sdks\jaxjaxlibv0.3.15\build\build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['.\\bazel5.1.1windowsx86_64.exe', 'run', 'verbose_failures=true', ':build_wheel', '', 'output_path=C:\\sdks\\jaxjaxlibv0.3.15\\dist', 'cpu=AMD64']' returned nonzero exit status 1. ``` ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Build failure for jax v0.3.15 on Windows 10,"On windows 10 box with python 3.9, cuda 11.7, cudnn  8.4.0 using the following command `python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cuda_compute_capabilities=""7.5"" cuda_version=""11.7"" cudnn_version=""8.4.0"" noenable_rocm noenable_tpu` fails to build from source with the following error. ``` ERROR: C:/users/adam/_bazel_adam/rjlwuxqw/external/org_tensorflow/tensorflow/compiler/mlir/xla/BUILD:560:11: Compiling tensorflow/compiler/mlir/xla/hlo_function_importer.: (Exit 2): python.exe failed: error executing command   cd /d C:/users/adam/_bazel_adam/rjlwuxqw/execroot/__main__   SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt     SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64     SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja     SET PWD=/proc/self/cwd     SET RUNFILES_MANIFEST_ONLY=1     SET TEMP=C:\Users\Adam\AppData\Local\Temp     SET TF_CUDA_COMPUTE_CAPABILITIES=7.5     SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET TF_CUDA_VERSION=11.7     SET TF_CUDNN_VERSION=8.4.0     SET TMP=C:\Users\Adam\AppData\Local\Temp   C:\Users\Adam\anaconda3\envs\jax_latest\python.exe B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/org_tensorflow /Ibazelout/x64_windowsopt/bin/external/org_tensorflow /Iexternal/llvmproject /Ibazelout/x64_windowsopt/bin/external/llvmproject /Iexternal/llvm_terminfo /Ibazelout/x64_windowsopt/bin/external/llvm_terminfo /Iexternal/llvm_zlib /Ibazelout/x64_windowsopt/bin/external/llvm_zlib /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazelout/x64_windowsopt/bin/external/com_google_absl /Iexternal/nsync /Ibazelout/x64_windowsopt/bin/external/nsync /Iexternal/gif /Ibazelout/x64_windowsopt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazelout/x64_windowsopt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazelout/x64_windowsopt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazelout/x64_windowsopt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazelout/x64_windowsopt/bin/external/fft2d /Iexternal/highwayhash /Ibazelout/x64_windowsopt/bin/external/highwayhash /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/double_conversion /Ibazelout/x64_windowsopt/bin/external/double_conversion /Iexternal/snappy /Ibazelout/x64_windowsopt/bin/external/snappy /Iexternal/local_config_cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/canonicalize_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinDialectIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinLocationAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CastOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FunctionInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SubElementInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorEncodingIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_pattern_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/InferIntRangeInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LoopLikeInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ViewLikeInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithmeticBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithmeticCanonicalizationIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithmeticOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ControlFlowOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FuncIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CopyOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ParserTokenKinds /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/QuantOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/QuantPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLTypesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ConversionPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformsPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapeOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ParallelCombiningOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TilingInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorAttrDefsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorOpsIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_structs_inc_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AffineOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AllocationOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizableOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationOpsIncGen /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Iexternal/org_tensorflow/tensorflow/compiler/mlir/hlo/include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/include /Iexternal/llvmproject/llvm/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/include /Iexternal/llvmproject/mlir/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/include /Iexternal/org_tensorflow/third_party/eigen3/mkl_include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazelout/x64_windowsopt/bin/external/nsync/public /Iexternal/gif /Ibazelout/x64_windowsopt/bin/external/gif /Iexternal/gif/windows /Ibazelout/x64_windowsopt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazelout/x64_windowsopt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/org_tensorflow/tensorflow/compiler/mlir/xla/include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/xla/include /Iexternal/local_config_cuda/cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLTDL_SHLIB_EXT="".dll"" /DLLVM_PLUGIN_EXT="".dll"" /DLLVM_NATIVE_ARCH=""X86"" /DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser /DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter /DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler /DLLVM_NATIVE_TARGET=LLVMInitializeX86Target /DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo /DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC /DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA /DLLVM_HOST_TRIPLE=""x86_64pcwin32"" /DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64pcwin32"" /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DBLAKE3_USE_NEON=0 /DBLAKE3_NO_AVX2 /DBLAKE3_NO_AVX512 /DBLAKE3_NO_SSE2 /DBLAKE3_NO_SSE41 /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG /D_USE_MATH_DEFINES DWIN32_LEAN_AND_MEAN DNOGDI /Zc:preprocessor DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. /std:c++17 /Fobazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/xla/_objs/hlo_module_importer/hlo_function_importer.obj /c external/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc  Configuration: ca98985668391b64b455964c5200157bb79165bf6145a97d1e521c47bb0387fb  Execution platform: //:platform Target //build:build_wheel failed to build INFO: Elapsed time: 4542.323s, Critical Path: 506.90s INFO: 9670 processes: 3308 internal, 6362 local. FAILED: Build did NOT complete successfully FAILED: Build did NOT complete successfully b""external/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/Support/LogicalResult.h(78): note: see reference to class template instantiation 'llvm::Optional' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen\\mlir/IR/BuiltinAttributeInterfaces.h.inc(197): note: see reference to class template instantiation 'mlir::FailureOr' being compiled\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen\\mlir/IR/BuiltinAttributeInterfaces.h.inc(336): note: see reference to class template instantiation 'mlir::detail::ElementsAttrTrait' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::detail::ElementsAttrIndexer\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1185): note: see reference to class template instantiation 'llvm::SmallVectorImpl' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/IR/OperationSupport.h(638): note: see reference to class template instantiation 'llvm::SmallVector>,1>' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=std::unique_ptr>\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/IR/Diagnostics.h(395): note: see reference to class template instantiation 'llvm::Optional' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::Diagnostic\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1185): note: see reference to class template instantiation 'llvm::SmallVectorImpl' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1161): note: see reference to class template instantiation 'llvm::SmallVector' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/Interfaces/InferTypeOpInterface.h(29): note: see reference to class template instantiation 'llvm::CalculateSmallVectorDefaultInlinedElements' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorInterfacesIncGen\\mlir/Interfaces/VectorInterfaces.h.inc(433): note: see reference to class template instantiation 'llvm::Optional>' being compiled\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorInterfacesIncGen\\mlir/Interfaces/VectorInterfaces.h.inc(442): note: see reference to class template instantiation 'mlir::detail::VectorUnrollOpInterfaceTrait' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SmallVector\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SymbolInterfacesIncGen\\mlir/IR/SymbolInterfaces.h.inc(178): note: see reference to class template instantiation 'llvm::Optional' being compiled\r\nbazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SymbolInterfacesIncGen\\mlir/IR/SymbolInterfaces.h.inc(229): note: see reference to class template instantiation 'mlir::detail::SymbolOpInterfaceTrait' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=mlir::SymbolTable::UseRange\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/Support/LogicalResult.h(78): note: see reference to class template instantiation 'llvm::Optional' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/mlir/include\\mlir/IR/DialectImplementation.h(100): note: see reference to class template instantiation 'mlir::FailureOr' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=std::string\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1185): note: see reference to class template instantiation 'llvm::SmallVectorImpl' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/SourceMgr.h(290): note: see reference to class template instantiation 'llvm::SmallVector' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::SMFixIt\r\n        ]\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1378): error C2665: 'mlir::mhlo::symbolizeDomainKind': none of the 2 overloads could convert all the argument types\r\nbazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/include\\mlirhlo/Dialect/mhlo/IR/hlo_ops_base_enums.h.inc(243): note: could be 'llvm::Optional mlir::mhlo::symbolizeDomainKind(llvm::StringRef)'\r\nbazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/mlir/hlo/include\\mlirhlo/Dialect/mhlo/IR/hlo_ops_base_enums.h.inc(241): note: or       'llvm::Optional mlir::mhlo::symbolizeDomainKind(uint32_t)'\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1379): note: while trying to match the argument list '(absl::lts_20211102::string_view)'\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): error C3536: 'domain_kind': cannot be used before it is initialized\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): error C2100: illegal indirection\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): error C2440: 'type cast': cannot convert from 'mlir::mhlo::DomainKind' to 'unsigned int'\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): note: This conversion requires an explicit cast (static_cast, Cstyle cast or functionstyle cast)\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): error C2440: '!=': cannot convert from 'int' to 'mlir::mhlo::DomainKind'\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1380): note: Conversion to enumeration type requires an explicit cast (static_cast, Cstyle cast or functionstyle cast)\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1387): error C2100: illegal indirection\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1385): error C2660: 'mlir::Builder::getNamedAttr': function does not take 1 arguments\r\nexternal/llvmproject/mlir/include\\mlir/IR/Builders.h(95): note: see declaration of 'mlir::Builder::getNamedAttr'\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(79): warning C4624: 'llvm::detail::copy_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(631): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(1185): note: see reference to class template instantiation 'llvm::SmallVectorImpl' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc(1630): note: see reference to class template instantiation 'llvm::SmallVector' being compiled\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(86): warning C4624: 'llvm::detail::move_construction_triviality_helper': destructor was implicitly defined as deleted\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30133\\include\\type_traits(662): note: see reference to class template instantiation 'llvm::detail::move_construction_triviality_helper' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/Support/type_traits.h(111): note: see reference to class template instantiation 'std::is_move_constructible>' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\nexternal/llvmproject/llvm/include\\llvm/ADT/SmallVector.h(308): note: see reference to class template instantiation 'llvm::is_trivially_move_constructible' being compiled\r\n        with\r\n        [\r\n            T=llvm::APInt\r\n        ]\r\n"" Traceback (most recent call last):   File ""C:\sdks\jaxjaxlibv0.3.15\build\build.py"", line 555, in      main()   File ""C:\sdks\jaxjaxlibv0.3.15\build\build.py"", line 550, in main     shell(command)   File ""C:\sdks\jaxjaxlibv0.3.15\build\build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['.\\bazel5.1.1windowsx86_64.exe', 'run', 'verbose_failures=true', ':build_wheel', '', 'output_path=C:\\sdks\\jaxjaxlibv0.3.15\\dist', 'cpu=AMD64']' returned nonzero exit status 1. ``` ```",2022-07-23T19:20:53Z,bug contributions welcome Windows,closed,0,5,https://github.com/jax-ml/jax/issues/11597,That error corresponds to external/org_tensorflow/tensorflow/compiler/mlir/xla/hlo_function_importer.cc:1378 which should work with  __cplusplus > 201402L and you have c++17 set. It may be due to ABSL string_view type used instead of std::string_view. Changing to ```c++ auto domain_kind = mlir::mhlo::symbolizeDomainKind(           std::string(instruction>user_side_metadata().Kind())); ``` should work (the materialization can be avoided too by explicitly performing the conversion that std::string_view would have taken in converting to llvm::StringRef).,"(Note the usual caveat: the JAX team does not support Windows builds: they are communitysupported. We welcome a patch to fix the problem, but we ourselves aren't going to look at this issue.)","The suggested fix works, but I presume needs plugging permanently in Tensorflow?","Indeed, should be quick review there and  could help if you run into any issues.","Closing, since this is probably stale."
392,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Raise errors for experimental_native_lowering and custom_call)， 内容是 (Raise explicit error when the experimental_native_lowering encounters a mhlo.custom_call. This would lead to failure when trying to run in TF.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Raise errors for experimental_native_lowering and custom_call,Raise explicit error when the experimental_native_lowering encounters a mhlo.custom_call. This would lead to failure when trying to run in TF.,2022-07-21T15:02:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/11575
2921,"以下是一个github上的jax下的一个issue, 标题是(Missing positional argument loops)， 内容是 (On Google colab ``` !apt install zstd !time wget c https://theeye.eu/public/AI/GPTJ6B/step_383500_slim.tar.zstd !time tar I zstd xf step_383500_slim.tar.zstd !git clone https://github.com/kingoflolz/meshtransformerjax.git !pip install r meshtransformerjax/requirements.txt !pip install meshtransformerjax/ jax==0.2.12 tensorflow==2.5.0 !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html !pip install upgrade pip !pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html !pip install jax==0.3.14 jaxlib==0.3.14 f https://storage.googleapis.com/jaxreleases/jax_releases.html  these two version combo only I am able to run without error till the current error line. import os import requests  from jax.config import config colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0] url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607' requests.post(url)  The following is required to use TPU Driver as JAX's backend. config.FLAGS.jax_xla_backend = ""tpu_driver"" config.FLAGS.jax_backend_target = ""grpc://"" + os.environ['COLAB_TPU_ADDR'] import time import jax from jax.experimental import maps import numpy as np import optax import transformers from mesh_transformer.checkpoint import read_ckpt_lowmem from mesh_transformer.sampling import nucleaus_sample from mesh_transformer.transformer_shard import CausalTransformer params = {   ""layers"": 28,   ""d_model"": 4096,   ""n_heads"": 16,   ""n_vocab"": 50400,   ""norm"": ""layernorm"",   ""pe"": ""rotary"",   ""pe_rotary_dims"": 64,   ""seq"": 2048,   ""cores_per_replica"": 8,   ""per_replica_batch"": 1, } per_replica_batch = params[""per_replica_batch""] cores_per_replica = params[""cores_per_replica""] seq = params[""seq""] params[""sampler""] = nucleaus_sample  here we ""remove"" the optimizer parameters from the model (as we don't need them for inference) params[""optimizer""] = optax.scale(0) import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)  number of device count was 8 when I executed maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp'))) devices = np.array(jax.devices()).reshape(mesh_shape) x = maps.Mesh(devices, ('dp', 'mp'))      look like Mesh(array([[0, 1, 2, 3, 4, 5, 6, 7]]), ('dp', 'mp')) maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp'))) ```  TypeError                                 Traceback (most recent call last) [](https://localhost:8080/) in () > 1 maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp'))) TypeError: __new__() missing 1 required positional argument: 'loops' !error_doubt)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Missing positional argument loops,"On Google colab ``` !apt install zstd !time wget c https://theeye.eu/public/AI/GPTJ6B/step_383500_slim.tar.zstd !time tar I zstd xf step_383500_slim.tar.zstd !git clone https://github.com/kingoflolz/meshtransformerjax.git !pip install r meshtransformerjax/requirements.txt !pip install meshtransformerjax/ jax==0.2.12 tensorflow==2.5.0 !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html !pip install upgrade pip !pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html !pip install jax==0.3.14 jaxlib==0.3.14 f https://storage.googleapis.com/jaxreleases/jax_releases.html  these two version combo only I am able to run without error till the current error line. import os import requests  from jax.config import config colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0] url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607' requests.post(url)  The following is required to use TPU Driver as JAX's backend. config.FLAGS.jax_xla_backend = ""tpu_driver"" config.FLAGS.jax_backend_target = ""grpc://"" + os.environ['COLAB_TPU_ADDR'] import time import jax from jax.experimental import maps import numpy as np import optax import transformers from mesh_transformer.checkpoint import read_ckpt_lowmem from mesh_transformer.sampling import nucleaus_sample from mesh_transformer.transformer_shard import CausalTransformer params = {   ""layers"": 28,   ""d_model"": 4096,   ""n_heads"": 16,   ""n_vocab"": 50400,   ""norm"": ""layernorm"",   ""pe"": ""rotary"",   ""pe_rotary_dims"": 64,   ""seq"": 2048,   ""cores_per_replica"": 8,   ""per_replica_batch"": 1, } per_replica_batch = params[""per_replica_batch""] cores_per_replica = params[""cores_per_replica""] seq = params[""seq""] params[""sampler""] = nucleaus_sample  here we ""remove"" the optimizer parameters from the model (as we don't need them for inference) params[""optimizer""] = optax.scale(0) import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)  number of device count was 8 when I executed maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp'))) devices = np.array(jax.devices()).reshape(mesh_shape) x = maps.Mesh(devices, ('dp', 'mp'))      look like Mesh(array([[0, 1, 2, 3, 4, 5, 6, 7]]), ('dp', 'mp')) maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp'))) ```  TypeError                                 Traceback (most recent call last) [](https://localhost:8080/) in () > 1 maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp'))) TypeError: __new__() missing 1 required positional argument: 'loops' !error_doubt",2022-07-20T16:48:01Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/11564
1010,"以下是一个github上的jax下的一个issue, 标题是(ShardedDeviceArray from tf.data.Dataset.shard)， 内容是 (Hi, I have a question regarding the data pipeline in multihost training. In particular, I have multiple workers equipped with GPU and each worker can access central data storage. I want to use `tf.data.Dataset.shard` to load part of the batch independently on each worker and join the shard in a single `ShardedDeviceArray` that can be handled by` pmap`. It looks like `jax.device_put_sharded` does the job but it requires a list of shards on the host, and I want to load them independently on the workers. I imagine my loop to be like ```python for it in ds.shard(n,jax.process_index()):   sharded_batch = unknow_function(it,...)   pmaped_train_step(sharded_batch) ``` What is the most efficient way to do it? _Originally posted by  in https://github.com/google/jax/discussions/11272_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,ShardedDeviceArray from tf.data.Dataset.shard,"Hi, I have a question regarding the data pipeline in multihost training. In particular, I have multiple workers equipped with GPU and each worker can access central data storage. I want to use `tf.data.Dataset.shard` to load part of the batch independently on each worker and join the shard in a single `ShardedDeviceArray` that can be handled by` pmap`. It looks like `jax.device_put_sharded` does the job but it requires a list of shards on the host, and I want to load them independently on the workers. I imagine my loop to be like ```python for it in ds.shard(n,jax.process_index()):   sharded_batch = unknow_function(it,...)   pmaped_train_step(sharded_batch) ``` What is the most efficient way to do it? _Originally posted by  in https://github.com/google/jax/discussions/11272_",2022-07-18T14:24:57Z,needs info,closed,0,2,https://github.com/jax-ml/jax/issues/11529,"For pmap, each process only needs to feed its own shard of data to pmap, like MPI. See https://jax.readthedocs.io/en/latest/multi_process.html I guess I'm not entirely sure why you would want to create a crossprocess global inputs for pmap?",I was under impression that I must manually create shared input even in the multiprocess case. Now it looks like I just need to use `psum` for  loss aggregation and it should work. Thanks! JAX is even better than I thought.
237,"以下是一个github上的jax下的一个issue, 标题是([ROCm] Disable new array_interoperability dlpack tests.)， 内容是 (/ )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[ROCm] Disable new array_interoperability dlpack tests.,/ ,2022-07-17T04:50:05Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/11518,I'm not sure  I haven't looked into it.  asked me to create that feature ticket so I assumed that either he or  would look into it. But if you/they want me to look into it  I would be happy to.
830,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] A new experimental version with JAX native lowering)， 内容是 (In the future JAX will be able to use a serialization format based on a variant of MHLO. This is not yet ready, but in this PR we are starting to get jax2tf ready for this. As a temporary step, we had introduced a TF op called XlaCallModule which carries a serialized MHLO module and which e can use to wrap the JAX native MHLO as a TF op. We still reuse parts of jax2tf, in particular the gradient machinery. This functionality can be enabled locally with a `experimental_native_lowering` flag for `jax2tf.convert`, or  globally with the flag `jax2tf_default_experimental_native_lowering`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] A new experimental version with JAX native lowering,"In the future JAX will be able to use a serialization format based on a variant of MHLO. This is not yet ready, but in this PR we are starting to get jax2tf ready for this. As a temporary step, we had introduced a TF op called XlaCallModule which carries a serialized MHLO module and which e can use to wrap the JAX native MHLO as a TF op. We still reuse parts of jax2tf, in particular the gradient machinery. This functionality can be enabled locally with a `experimental_native_lowering` flag for `jax2tf.convert`, or  globally with the flag `jax2tf_default_experimental_native_lowering`.",2022-07-15T07:05:29Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/11506
827,"以下是一个github上的jax下的一个issue, 标题是(Reduce the verbosity of treedef printing for custom nodes.)， 内容是 (Reduce the verbosity of treedef printing for custom nodes. For very large trees of custom nodes this printing can be very verbose with a lot or repetition. Our internal repository also encourages very deep package names which exacerbates this issue. Users encounter treedef printing when interacting with some staging APIs in JAX, for example:     >>> params = { .. some params .. }     >>> f = jax.jit(..).lower(params).compile()     >>> f(params)   fine     >>> params['some_new_thing'] = something     >>> f(params)     TypeError: function compiled for {treedef}, called with {treedef}.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Reduce the verbosity of treedef printing for custom nodes.,"Reduce the verbosity of treedef printing for custom nodes. For very large trees of custom nodes this printing can be very verbose with a lot or repetition. Our internal repository also encourages very deep package names which exacerbates this issue. Users encounter treedef printing when interacting with some staging APIs in JAX, for example:     >>> params = { .. some params .. }     >>> f = jax.jit(..).lower(params).compile()     >>> f(params)   fine     >>> params['some_new_thing'] = something     >>> f(params)     TypeError: function compiled for {treedef}, called with {treedef}.",2022-07-14T20:29:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/11500
492,"以下是一个github上的jax下的一个issue, 标题是(Implement the `__dlpack__` interface)， 内容是 (This is described at https://github.com/numpy/numpy/issues/19013, and seems to be developing as a standard Test case would look something like this: ```python import jax.numpy as jnp import numpy as np x = jnp.arange(10) x_view = np.from_dlpack(x) ``` (related to CC(未找到相关数据)))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Implement the `__dlpack__` interface,"This is described at https://github.com/numpy/numpy/issues/19013, and seems to be developing as a standard Test case would look something like this: ```python import jax.numpy as jnp import numpy as np x = jnp.arange(10) x_view = np.from_dlpack(x) ``` (related to CC(未找到相关数据))",2022-07-14T15:52:55Z,enhancement P2 (eventual),closed,0,0,https://github.com/jax-ml/jax/issues/11496
646,"以下是一个github上的jax下的一个issue, 标题是(Add window reversal option to JAX convolutions)， 内容是 (XLA supports a `window_reversal` parameter to convolutions (such as the `xla::ConvGeneralDilated` builder function), and parts of JAX (such as the MLIR generator) set it. However, the userlevel convolution APIs in JAX do not provide the option to reverse convolution windows. It would sometimes be convenient to write JAX code that performs reversal implicitly in the convolution rather than modifying the kernel itself.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add window reversal option to JAX convolutions,"XLA supports a `window_reversal` parameter to convolutions (such as the `xla::ConvGeneralDilated` builder function), and parts of JAX (such as the MLIR generator) set it. However, the userlevel convolution APIs in JAX do not provide the option to reverse convolution windows. It would sometimes be convenient to write JAX code that performs reversal implicitly in the convolution rather than modifying the kernel itself.",2022-07-13T18:02:03Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/11483,"Thanks for the request! We (, , and I) discussed more offthread and found that: 1. using the `window_reversal` option is semantically equivalent to (i.e. computes the same values as) applying a `jax.lax.rev(rhs, [i for i, b in enumerate(window_reversal) if b])` to the `rhs` input of the convolution, and 2. XLA:TPU and XLA:GPU will fuse a `Rev` HLO on the `rhs` input into the generated convolution, so there's no  efficiency win to be had to build the `window_reversal` option into the `conv_general_dilated` JAX primitive compared to just applying the `jax.lax.rev` function separately. So the recommendation here is just to apply a ` jax.lax.rev`, like `jax.lax.rev(rhs, [i for i, b in enumerate(window_reversal) if b])` or similar, if you want to express something like this!"
450,"以下是一个github上的jax下的一个issue, 标题是(The gradient of `i0` for input 0 should be 0)， 内容是 (Based on the definition of `i0`, its gradient for input 0 should be 0. It is differentiable at the point 0, unlike `abs` ```py import jax a = jax.numpy.array([0.]) print(jax.jacrev(jax.numpy.i0)(a))  [[1.]]  It should be 0 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,The gradient of `i0` for input 0 should be 0,"Based on the definition of `i0`, its gradient for input 0 should be 0. It is differentiable at the point 0, unlike `abs` ```py import jax a = jax.numpy.array([0.]) print(jax.jacrev(jax.numpy.i0)(a))  [[1.]]  It should be 0 ```",2022-07-13T11:26:14Z,bug P2 (eventual),closed,0,2,https://github.com/jax-ml/jax/issues/11479,any update of this bug?,Thanks  sorry this fell through the cracks. It looks like this could be fixed by adding a custom JVP rule to `jax.numpy.i0`. Is this something you'd be interested in contributing? If not we can have someone on the team take care of it.
2876,"以下是一个github上的jax下的一个issue, 标题是(Bogus gradient value from value_and_grad for committed DeviceArray on a multi-GPU host)， 内容是 (Hi, I'm reporting this issue with a disclaimer that it's probably not reproducible (sorry!) but hoping it's of some worth for the awesome JAX developers: We have a machine with 5 NVIDIA A100 (80GB) cards, and we see that the gradient computed by `value_and_grad` is wrong when DeviceArray committed two specific devices are passed as the function argument. Here is the simplest example: ``` import jax fn = jax.value_and_grad(lambda x: x * 1) for i in range(jax.device_count()):     x = jax.device_put(1., device=jax.devices()[i])     v, g = fn(x)     print(v, g) ``` Printout: ``` 1.0 1.0 1.0 1.0 1.0 1e45 1.0 1e45 1.0 1.0 ``` When the function is `lambda x: x` or `lambda x: x * 1.` (note the dot after 1  multiplying by a float here) the result from all devices are correct (`1.0 1.0`). Also, if the call to `fn` is put in the default_device context, as in: ``` import jax fn = jax.value_and_grad(lambda x: x * 1) for i in range(jax.device_count()):     x = jax.device_put(1., device=jax.devices()[i])     with jax.default_device(jax.devices()[i]):         v, g = fn(x)     print(v, g) ``` the output gradient is correct for all devices. Which makes me suspect that uncommitted arrays are involved somewhere in the gradient calculation, even when the input is committed. And also that's only in some specific cases. The only information that may be even remotely relevant that I can think of is the device topology: ``` $ nvidiasmi topo m 	GPU0	GPU1	GPU2	GPU3	GPU4	CPU Affinity	NUMA Affinity GPU0	 X 	NV12	PXB	PXB	SYS	031	0 GPU1	NV12	 X 	PXB	PXB	SYS	031	0 GPU2	PXB	PXB	 X 	NV12	SYS	031	0 GPU3	PXB	PXB	NV12	 X 	SYS	031	0 GPU4	SYS	SYS	SYS	SYS	 X 	3263	1 Legend:   X    = Self   SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)   NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node   PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)   PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)   PIX  = Connection traversing at most a single PCIe bridge   NV  = Connection traversing a bonded set of  NVLinks ``` That is, device pairs 0&1 and 2&3 are each connected via NVLINK, and 4 is isolated. Also, GPU 4 was recently installed, but the problem existed even before that. The problem is not reproduced even on other multiGPU machines that we have (although the hardware configurations are different), so it's OK if this issue is dismissed. As I said at the top, I'm just reporting in case this is of any interest to the developers.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bogus gradient value from value_and_grad for committed DeviceArray on a multi-GPU host,"Hi, I'm reporting this issue with a disclaimer that it's probably not reproducible (sorry!) but hoping it's of some worth for the awesome JAX developers: We have a machine with 5 NVIDIA A100 (80GB) cards, and we see that the gradient computed by `value_and_grad` is wrong when DeviceArray committed two specific devices are passed as the function argument. Here is the simplest example: ``` import jax fn = jax.value_and_grad(lambda x: x * 1) for i in range(jax.device_count()):     x = jax.device_put(1., device=jax.devices()[i])     v, g = fn(x)     print(v, g) ``` Printout: ``` 1.0 1.0 1.0 1.0 1.0 1e45 1.0 1e45 1.0 1.0 ``` When the function is `lambda x: x` or `lambda x: x * 1.` (note the dot after 1  multiplying by a float here) the result from all devices are correct (`1.0 1.0`). Also, if the call to `fn` is put in the default_device context, as in: ``` import jax fn = jax.value_and_grad(lambda x: x * 1) for i in range(jax.device_count()):     x = jax.device_put(1., device=jax.devices()[i])     with jax.default_device(jax.devices()[i]):         v, g = fn(x)     print(v, g) ``` the output gradient is correct for all devices. Which makes me suspect that uncommitted arrays are involved somewhere in the gradient calculation, even when the input is committed. And also that's only in some specific cases. The only information that may be even remotely relevant that I can think of is the device topology: ``` $ nvidiasmi topo m 	GPU0	GPU1	GPU2	GPU3	GPU4	CPU Affinity	NUMA Affinity GPU0	 X 	NV12	PXB	PXB	SYS	031	0 GPU1	NV12	 X 	PXB	PXB	SYS	031	0 GPU2	PXB	PXB	 X 	NV12	SYS	031	0 GPU3	PXB	PXB	NV12	 X 	SYS	031	0 GPU4	SYS	SYS	SYS	SYS	 X 	3263	1 Legend:   X    = Self   SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)   NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node   PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)   PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)   PIX  = Connection traversing at most a single PCIe bridge   NV  = Connection traversing a bonded set of  NVLinks ``` That is, device pairs 0&1 and 2&3 are each connected via NVLINK, and 4 is isolated. Also, GPU 4 was recently installed, but the problem existed even before that. The problem is not reproduced even on other multiGPU machines that we have (although the hardware configurations are different), so it's OK if this issue is dismissed. As I said at the top, I'm just reporting in case this is of any interest to the developers.",2022-07-13T08:32:09Z,bug needs info NVIDIA GPU,open,0,11,https://github.com/jax-ml/jax/issues/11478,"I'm curious about your issue. If you run it many times, is it always the same GPUs that have an issue? If so, can you try this: CUDA_VISIBLE_DEVICES=2,3,4,0,1 python your_script.py This change the order of GPUs. Does the error continue to be on the same GPU?","Thanks for picking up this issue! Yes, with the example above it's always the third and fourth lines (GPUs 2 & 3) that have the issue. On the other hand, indeed changing the order of the devices actually affects the symptom. With CUDA_VISIBLE_DEVICES=2,3,4,0,1 it was the fourth and fifth lines (GPUs 0 & 1) whose gradient became 1e45. This looks like a lead so I tested all 120 device order permutations. The result is:  Whenever GPU 0 or 1 comes first, the gradients computed on 2 and 3 become corrupted.  Whenever GPU 2 or 3 comes first, the gradients computed on 0 and 1 become corrupted.  When GPU 4 comes first, all gradients are properly computed.","Also I realized I had forgotten to put the most basic information out: I'm seeing this effect in   JAX 0.3.14 + CUDA 11.4.0  JAX 0.3.15 + CUDA 11.6.2. JAX was installed with ``` pip3 install ""jax[cuda]=="" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```","Thanks for the results.  What computer is this? 5 GPUs isn't a frequent config. What GPUs it is? Are they all the same? If not (like on DGX stations), if you use enable only the GPU that are the same, does it works well?", maybe you could also try debugging with nccltests,"> Thanks for the results. What computer is this? 5 GPUs isn't a frequent config. What GPUs it is? Are they all the same? If not (like on DGX stations), if you use enable only the GPU that are the same, does it works well?  Any update on that? If we do not have an update on 1 week, we will close this issue."," I'm sorry for the long silence. I was trying nccltests as suggested by  and was trying to understand what I was seeing. First, to answer your question: the machine is a custombuilt server that originally came with 2 A100s. We bought and installed additional A100s to open PCIe slots. In fact we recently added two more, so the machine now has 7 A100s. All are the same product (80GB), but I can't rule out the possibility that there are slight internal differences, because they were purchased at different times.  As for nccltests: I built the binaries as instructed in the README and ran `all_reduce_perf`. Commands ``` ./all_reduce_perf t 1 ``` and ``` ./all_reduce_perf t 2 ``` return some test results, but if I do `t 3` the test does not finish within at least 5 minutes. Do you know if that's normal, or is there indeed something wrong with our configuration?","If you limit yourself to only the 4 first GPU, does it work correctly? Also, what is the motherboard? Few motherboard can have  7 GPUs.","No, with `CUDA_VISIBLE_DEVICES=0,1,2,3` I get a gradient 1e45 for GPUs 2 and 3. The motherboard is a SuperMicro X12DPGOA6. With the recent addition of two GPUs, the topology has changed from what I wrote above. I don't want to bombard you with too much information / drag this issue for too long (my intention was to check if JAX experts would have some quick idea about what I was seeing), but just in case it helps, the current output of `nvidiasmi top m` is like this: ``` 	GPU0	GPU1	GPU2	GPU3	GPU4	GPU5	GPU6	CPU Affinity	NUMA Affinity GPU0	 X 	NV12	PXB	PXB	PXB	SYS	SYS	031	0 GPU1	NV12	 X 	PIX	PXB	PXB	SYS	SYS	031	0 GPU2	PXB	PIX	 X 	PXB	PXB	SYS	SYS	031	0 GPU3	PXB	PXB	PXB	 X 	NV12	SYS	SYS	031	0 GPU4	PXB	PXB	PXB	NV12	 X 	SYS	SYS	031	0 GPU5	SYS	SYS	SYS	SYS	SYS	 X 	PXB	3263	1 GPU6	SYS	SYS	SYS	SYS	SYS	PXB	 X 	3263	1 Legend:   X    = Self   SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)   NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node   PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)   PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)   PIX  = Connection traversing at most a single PCIe bridge   NV  = Connection traversing a bonded set of  NVLinks ``` Seen in terms of PCIe connections, the output of `lspci t v` (with the GPU device ID added at the end of the lines) is ```  +[0000:c9]+00.0  Intel Corporation Device 09a2                                 \10.0[57]00.0  NVIDIA Corporation Device 20b5   < GPU 4 (NVLINKed with GPU 3) ``` But then again, I'm not entirely certain if device topology is the right place to look into. It does seem to have some relevance though, because even with the new topology the symptom is  If an NVLINKed device (0, 1, 3, or 4) comes first in CUDA_VISIBLE_DEVICES, the gradient is correct in this device, the NVLINK partner (1, 0, 4, or 3), and devices that are connected via SYS (5 and 6), and incorrect in the other devices (connected via PIX or PXB).  If GPU 2 (not NVLINKed to any) comes first, gradient is correct in 5 and 6, and incorrect in all other devices.  If 5 or 6 comes first, the gradient is correct on all devices.","From this page: https://www.supermicro.com/en/support/resources/gpu?rsc=fltr_sku%3DSYS420GPTNR The A100 GPU isn't officially supported by this server. https://www.supermicro.com/en/products/system/GPU/4U/SYS420GPTNR Sorry, I do not have a magic answer. Did you test other frameworks then JAX? ","Hmm OK, so it may be a problem deep inside the GPU driver and the communication between the PCIe buses.. I myself use JAX exclusively, but others use PyTorch, and I haven't so far gotten complaints about unexpected behaviors. I will tell them to doublecheck their calculation results though. I guess issuewise we can't go any further then  thank you again for taking your time! One last question to   as I reported above, I couldn't really get any information out of nccltests. Was it expected that `./all_reduce_perf t 3` takes forever (I killed the process because it ran for more than a couple of hours), or is that a sign that we do indeed have a hardware configuration issue?"
624,"以下是一个github上的jax下的一个issue, 标题是(`ldexp` has wrong gradient `0.0`)， 内容是 (`ldexp` has wrong gradient `0.0`. For example, ```py import jax def fn(x1):     x2 = jax.numpy.array(1, dtype=jax.numpy.int32)     return jax.numpy.ldexp(x1, x2) x1 = jax.numpy.array(1., dtype=jax.numpy.float32) print(fn(x1))  2.0 print(jax.jacrev(fn)(x1))  0.0 ``` Because `ldexp` computes `x1 * 2**x2`, it should the gradient `2.0` in this example. It seems that it will return 0 as the gradient for any input.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`ldexp` has wrong gradient `0.0`,"`ldexp` has wrong gradient `0.0`. For example, ```py import jax def fn(x1):     x2 = jax.numpy.array(1, dtype=jax.numpy.int32)     return jax.numpy.ldexp(x1, x2) x1 = jax.numpy.array(1., dtype=jax.numpy.float32) print(fn(x1))  2.0 print(jax.jacrev(fn)(x1))  0.0 ``` Because `ldexp` computes `x1 * 2**x2`, it should the gradient `2.0` in this example. It seems that it will return 0 as the gradient for any input.",2022-07-12T09:08:22Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/11467,"Thanks for the report – I think this is probably working as expected, because `ldexp` is a strange function that is not actually welldefined for real numbers, but rather operates on details of the floatingpoint representation of numbers in a way that is not compatible with autodiff. If you want a roughly equivalent algebraic function that will be compatible with autodiff, I'd suggest using something like ```python def almost_ldexp(x1, x2):   return x1 * 2 ** x2 ```","Thanks for your explanation! For this kind of incompatible function with autodiff, I think it'd be better to raise an exception instead of returning all zero gradient. What do you think?","Sure, that seems like a reasonable option",I'm more than happy to help collect such kind of APIs.," Should throw exception  trim_zeros `trim_zeros` will return 0 as the gradient. I think it is not compatible (or not implemented?) so it is better to raise an exception ```py import jax def fn(input):     return jax.numpy.trim_zeros(input) input = jax.numpy.array([1., 2., 3.], dtype=jax.numpy.float32) res = fn(input) print(res)  [1. 2. 3.] grad = jax.jacrev(fn)(input) print(grad)  [[0. 0. 0.]   [0. 0. 0.]   [0. 0. 0.]] ```  bitcast_convert_type `bitcast_convert_type` will return 0 as the gradient when the `new_dtype` is the same as `input`. I think this API cannot be differentiated so it may raise an exception. ```py import jax def fn(input):     newtype = jax.numpy.float32     return jax.lax.bitcast_convert_type(input, newtype) input = jax.numpy.array([1., 1.], dtype=jax.numpy.float32) res = fn(input) print(res)  [1. 1.] jac = jax.jacrev(fn)(input) print(jac)  [[0. 0.]   [0. 0.]] ```  union1d, setdiff1d, setxor1d I think these three APIs cannot be differentiated, which will return 0 as the gradient for any input ```py import jax def fn(ar1):     ar2 = jax.numpy.array([1, 2, 3])     return jax.numpy.union1d(ar1, ar2) ar1 = jax.numpy.array([2., 1.]) res = fn(ar1) print(res)  [2. 1.  1.  2.  3.] jac = jax.jacrev(fn)(ar1) print(jac)  [[0. 0.]   [0. 0.]   [0. 0.]   [0. 0.]   [0. 0.]] ```  polymul with `trim_leading_zeros=True` When `trim_leading_zeros=True`, `polymul` will give 0 as the gradient for any input ```py import jax def fn(input):     other = jax.numpy.array([1, 1, 1])     return jax.numpy.polymul(input, other, trim_leading_zeros=True) input = jax.numpy.array([1., 1., 1.,], dtype=jax.numpy.float32) res = fn(input) print(res)  [1. 2. 3. 2. 1.] jac = jax.jacrev(fn)(input) print(jac)  [[0. 0. 0.]   [0. 0. 0.]   [0. 0. 0.]   [0. 0. 0.]   [0. 0. 0.]] ```",Thanks  are you interested in contributing pull requests to fix these issues?,"I am interested in fixing these issues. But I am new here, where should I add the exception checker? Are there any material/pull requests that I can refer to? Thanks in advance.",`trim_zeros` we should be able to fix pretty easily by not concretizing the input, How about a custom jvp for ldexp that simply returns `2 ** x2`? I can submit a PR for that.,"Given CC(Add custom_jvp to jax.numpy.ldexp.), I think this can be closed."
5906,"以下是一个github上的jax下的一个issue, 标题是(Memory leak in compiled loops closing over large constants)， 内容是 (I was running into an issue that I could condense to an elementary example. It seems to me as if closing over large constants in lax loops creates a memory leak while unrolling the loop avoids the issue. A disclaimer: my knowledge of how JAX is limited. Thus, the issue described may not be a memory leak but a misunderstanding on my part of how JAX or XLA buffers compiled computations. In the MRE, each of the sideeffectfree functions  `leak` and `no_leak` create in their body a large constant and later compile function which closes over the large constant. The version which includes a lax loop closing over the constant seems to leak memory.  I'm not sure whether this issue is related to CC(jit cache leaks memory). However, since the function without a lax loop doesn't seem to leak memory, I think it is different. From my understanding of 's comment on caching of compiled functions, I'd expect that the cache is cleared and memory should be released once `g`, `jg` and `data` go out of scope or once the references are deleted manually. However, and here I'm not sure if I use `jaxlib.xla_extension.Client` correctly, it seems that using the lax loop creates a live buffer that doesn't get dropped when both functions, the compiled function and the original function, go out of scope (see output below). I also tried to avoid the jitcompilation of the lax loop (see function `leak_no_compile`). This seems to create similar behavior of the program with the difference that, additionally to the live buffer, also a live executable can be found via the `backend.live_executables()` after the function went out of scope.  MRE contents of leak.py ```py import numpy as np import jax import psutil def print_mem_usage() > None:         print(         f""Resident memory: {psutil.Process().memory_info().rss // (1024 * 1024)} MB""     ) def print_live_buffers() > None:     lbs = jax.lib.xla_bridge.get_backend().live_buffers()     print(f""Number live buffers: {len(lbs)}"")     for idx, lb in enumerate(lbs):         print(f""\tlbs[{idx}]: shape {lb.shape} at 0x{lb.unsafe_buffer_pointer():x}"") def print_num_live_executables() > None:     les = jax.lib.xla_bridge.get_backend().live_executables()     print(f""Number live executables: {len(les)}"") def leak():      ~ 1 GB of float32s     size = 1024 * 1024 * 1024 // 4     data = jax.device_put(np.empty((size, ), dtype=np.float32) + 1)     def g():         return jax.lax.fori_loop(0, 1, lambda _, __: data[0], 0.0)     jg = jax.jit(g)     _ = jg().block_until_ready()     del g, jg, data, _ def no_leak():      ~ 1 GB of float32s     size = 1024 * 1024 * 1024 // 4     data = jax.device_put(np.empty((size, ), dtype=np.float32) + 1)     def g():         val = None         for _ in range(1):             val = data[0]         return val     jg = jax.jit(g)     _ = jg().block_until_ready()     del g, jg, data, _ def leak_no_compile():      ~ 1 GB of float32s     size = 1024 * 1024 * 1024 // 4     data = jax.device_put(np.empty((size, ), dtype=np.float32) + 1)     def g():         return jax.lax.fori_loop(0, 1, lambda _, __: data[0], 0.0)     _ = g().block_until_ready()     del g, data, _ def run() > None:     for iter in range(3):         print(f""iter: {iter} ================================================================"")         print_mem_usage()         _ = leak()      Output with `leak()` ``` $ python leak.py iter: 0 ================================================================ Resident memory: 114 MB Resident memory: 1170 MB Number live buffers: 1 	lbs[0]: shape (268435456,) at 0x288dc4000 Number live executables: 0 iter: 1 ================================================================ Resident memory: 1170 MB Resident memory: 2196 MB Number live buffers: 2 	lbs[0]: shape (268435456,) at 0x2c8dc4000 	lbs[1]: shape (268435456,) at 0x288dc4000 Number live executables: 0 iter: 2 ================================================================ Resident memory: 2196 MB Resident memory: 3222 MB Number live buffers: 3 	lbs[0]: shape (268435456,) at 0x308dc4000 	lbs[1]: shape (268435456,) at 0x2c8dc4000 	lbs[2]: shape (268435456,) at 0x288dc4000 Number live executables: 0 ```     Output with `no_leak()` ``` $ python leak.py iter: 0 ================================================================ Resident memory: 116 MB Resident memory: 1172 MB Number live buffers: 0 Number live executables: 0 iter: 1 ================================================================ Resident memory: 1172 MB Resident memory: 1174 MB Number live buffers: 0 Number live executables: 0 iter: 2 ================================================================ Resident memory: 1174 MB Resident memory: 1176 MB Number live buffers: 0 Number live executables: 0 ```     Output with `leak_no_compile()` ``` $ python leak.py iter: 0 ================================================================ Resident memory: 116 MB Resident memory: 1173 MB Number live buffers: 1 	lbs[0]: shape (268435456,) at 0x298000000 Number live executables: 1 iter: 1 ================================================================ Resident memory: 1173 MB Resident memory: 2198 MB Number live buffers: 2 	lbs[0]: shape (268435456,) at 0x2d8000000 	lbs[1]: shape (268435456,) at 0x298000000 Number live executables: 2 iter: 2 ================================================================ Resident memory: 2198 MB Resident memory: 3223 MB Number live buffers: 3 	lbs[0]: shape (268435456,) at 0x318000000 	lbs[1]: shape (268435456,) at 0x2d8000000 	lbs[2]: shape (268435456,) at 0x298000000 Number live executables: 3 ```   System I'm using jax == 0.3.14 and jaxlib == 0.3.14 on python 3.10.5 on a Mac with M1Pro/Ultimate.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Memory leak in compiled loops closing over large constants,"I was running into an issue that I could condense to an elementary example. It seems to me as if closing over large constants in lax loops creates a memory leak while unrolling the loop avoids the issue. A disclaimer: my knowledge of how JAX is limited. Thus, the issue described may not be a memory leak but a misunderstanding on my part of how JAX or XLA buffers compiled computations. In the MRE, each of the sideeffectfree functions  `leak` and `no_leak` create in their body a large constant and later compile function which closes over the large constant. The version which includes a lax loop closing over the constant seems to leak memory.  I'm not sure whether this issue is related to CC(jit cache leaks memory). However, since the function without a lax loop doesn't seem to leak memory, I think it is different. From my understanding of 's comment on caching of compiled functions, I'd expect that the cache is cleared and memory should be released once `g`, `jg` and `data` go out of scope or once the references are deleted manually. However, and here I'm not sure if I use `jaxlib.xla_extension.Client` correctly, it seems that using the lax loop creates a live buffer that doesn't get dropped when both functions, the compiled function and the original function, go out of scope (see output below). I also tried to avoid the jitcompilation of the lax loop (see function `leak_no_compile`). This seems to create similar behavior of the program with the difference that, additionally to the live buffer, also a live executable can be found via the `backend.live_executables()` after the function went out of scope.  MRE contents of leak.py ```py import numpy as np import jax import psutil def print_mem_usage() > None:         print(         f""Resident memory: {psutil.Process().memory_info().rss // (1024 * 1024)} MB""     ) def print_live_buffers() > None:     lbs = jax.lib.xla_bridge.get_backend().live_buffers()     print(f""Number live buffers: {len(lbs)}"")     for idx, lb in enumerate(lbs):         print(f""\tlbs[{idx}]: shape {lb.shape} at 0x{lb.unsafe_buffer_pointer():x}"") def print_num_live_executables() > None:     les = jax.lib.xla_bridge.get_backend().live_executables()     print(f""Number live executables: {len(les)}"") def leak():      ~ 1 GB of float32s     size = 1024 * 1024 * 1024 // 4     data = jax.device_put(np.empty((size, ), dtype=np.float32) + 1)     def g():         return jax.lax.fori_loop(0, 1, lambda _, __: data[0], 0.0)     jg = jax.jit(g)     _ = jg().block_until_ready()     del g, jg, data, _ def no_leak():      ~ 1 GB of float32s     size = 1024 * 1024 * 1024 // 4     data = jax.device_put(np.empty((size, ), dtype=np.float32) + 1)     def g():         val = None         for _ in range(1):             val = data[0]         return val     jg = jax.jit(g)     _ = jg().block_until_ready()     del g, jg, data, _ def leak_no_compile():      ~ 1 GB of float32s     size = 1024 * 1024 * 1024 // 4     data = jax.device_put(np.empty((size, ), dtype=np.float32) + 1)     def g():         return jax.lax.fori_loop(0, 1, lambda _, __: data[0], 0.0)     _ = g().block_until_ready()     del g, data, _ def run() > None:     for iter in range(3):         print(f""iter: {iter} ================================================================"")         print_mem_usage()         _ = leak()      Output with `leak()` ``` $ python leak.py iter: 0 ================================================================ Resident memory: 114 MB Resident memory: 1170 MB Number live buffers: 1 	lbs[0]: shape (268435456,) at 0x288dc4000 Number live executables: 0 iter: 1 ================================================================ Resident memory: 1170 MB Resident memory: 2196 MB Number live buffers: 2 	lbs[0]: shape (268435456,) at 0x2c8dc4000 	lbs[1]: shape (268435456,) at 0x288dc4000 Number live executables: 0 iter: 2 ================================================================ Resident memory: 2196 MB Resident memory: 3222 MB Number live buffers: 3 	lbs[0]: shape (268435456,) at 0x308dc4000 	lbs[1]: shape (268435456,) at 0x2c8dc4000 	lbs[2]: shape (268435456,) at 0x288dc4000 Number live executables: 0 ```     Output with `no_leak()` ``` $ python leak.py iter: 0 ================================================================ Resident memory: 116 MB Resident memory: 1172 MB Number live buffers: 0 Number live executables: 0 iter: 1 ================================================================ Resident memory: 1172 MB Resident memory: 1174 MB Number live buffers: 0 Number live executables: 0 iter: 2 ================================================================ Resident memory: 1174 MB Resident memory: 1176 MB Number live buffers: 0 Number live executables: 0 ```     Output with `leak_no_compile()` ``` $ python leak.py iter: 0 ================================================================ Resident memory: 116 MB Resident memory: 1173 MB Number live buffers: 1 	lbs[0]: shape (268435456,) at 0x298000000 Number live executables: 1 iter: 1 ================================================================ Resident memory: 1173 MB Resident memory: 2198 MB Number live buffers: 2 	lbs[0]: shape (268435456,) at 0x2d8000000 	lbs[1]: shape (268435456,) at 0x298000000 Number live executables: 2 iter: 2 ================================================================ Resident memory: 2198 MB Resident memory: 3223 MB Number live buffers: 3 	lbs[0]: shape (268435456,) at 0x318000000 	lbs[1]: shape (268435456,) at 0x2d8000000 	lbs[2]: shape (268435456,) at 0x298000000 Number live executables: 3 ```   System I'm using jax == 0.3.14 and jaxlib == 0.3.14 on python 3.10.5 on a Mac with M1Pro/Ultimate.",2022-07-11T17:01:42Z,bug,closed,2,3,https://github.com/jax-ml/jax/issues/11448,"Thanks for the report. This is very similar to that bug. You're baking these constants into the computation each time you call this function. Your code would probably be better if you promoted these large constants to be additional parameters to the jitted function instead. Regardless, in the `fori_loop` version, the buffers are getting leaked in the `_initial_style_jaxpr` cache. I've put together https://github.com/google/jax/pull/11461 to address this problem.","Thank you for your prompt reply and the PR. I'm aware that the code above is not optimal. However, I wanted to provide very simple example demonstrating the issue. I also noticed that the buffers also get leaked when using `jax.lax.scan` or `jax.lax.cond` instead of `fori_loop`, e.g., ```py def g():         xs = jnp.arange(1)         return jax.lax.scan(lambda _, __: (data[0], ()), 0.0, xs) ```     output: ``` $ python leak2.py iter: 0 ================================================================ Resident memory: 115 MB Resident memory: 1171 MB Number live buffers: 1 	lbs[0]: shape (268435456,) at 0x288000000 Number live executables: 0 iter: 1 ================================================================ Resident memory: 1171 MB Resident memory: 2196 MB Number live buffers: 2 	lbs[0]: shape (268435456,) at 0x2c8000000 	lbs[1]: shape (268435456,) at 0x288000000 Number live executables: 0 iter: 2 ================================================================ Resident memory: 2196 MB Resident memory: 3222 MB Number live buffers: 3 	lbs[0]: shape (268435456,) at 0x308000000 	lbs[1]: shape (268435456,) at 0x2c8000000 	lbs[2]: shape (268435456,) at 0x288000000 Number live executables: 0 ```  or ```py def g():         return jax.lax.cond(             True,             lambda: data[0],             lambda: data[1],         ) ```     output: ``` $ python leak2.py iter: 0 ================================================================ Resident memory: 114 MB Resident memory: 1169 MB Number live buffers: 1 	lbs[0]: shape (268435456,) at 0x280000000 Number live executables: 0 iter: 1 ================================================================ Resident memory: 1169 MB Resident memory: 2195 MB Number live buffers: 2 	lbs[0]: shape (268435456,) at 0x2c0000000 	lbs[1]: shape (268435456,) at 0x280000000 Number live executables: 0 iter: 2 ================================================================ Resident memory: 2195 MB Resident memory: 3221 MB Number live buffers: 3 	lbs[0]: shape (268435456,) at 0x300000000 	lbs[1]: shape (268435456,) at 0x2c0000000 	lbs[2]: shape (268435456,) at 0x280000000 Number live executables: 0 ```  Is your PR CC(Use weakref to avoid cache leaks in fori_loop.) already addressing these similar issues too by using the `weakref_lru_cache` on `_initial_style_jaxpr`? Finally, do you know if there is a recommended way to delete leaked buffers given that I know they won't be used anymore? I figured that using ```py backend = jax.lib.xla_bridge.get_backend() for buf in backend.live_buffers():     buf.delete() ``` on the example code frees the memory.","The issue seems to be addressed. So, I will close this issue. Thank you very much for responding so quickly. However, closing over large constants within `jax.lax.cond` still creates a memory leak on my system using the current jax/jaxlib release. I created a issue ( CC(Memory leak when closing over large constants in compiled jax.lax.cond statement)) to address this."
640,"以下是一个github上的jax下的一个issue, 标题是(Point to the right download for jaxlib)， 内容是 (The download right now instructs to express the download as  ```bash pip install ""jax[cuda11_cudnn84]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` in the codeblock for jaxlib. This only downloads jax itself. Instead pointing to jaxlib as in  ```bash pip install upgrade ""jaxlib[cuda11_cudnn84]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Downloads the right jaxlib.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Point to the right download for jaxlib,"The download right now instructs to express the download as  ```bash pip install ""jax[cuda11_cudnn84]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` in the codeblock for jaxlib. This only downloads jax itself. Instead pointing to jaxlib as in  ```bash pip install upgrade ""jaxlib[cuda11_cudnn84]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Downloads the right jaxlib.",2022-07-11T13:30:00Z,,closed,0,4,https://github.com/jax-ml/jax/issues/11441,"I don't think this is correct: these extras are defined in the `setup.py` file for `jax`, not `jaxlib`. You can see this here: https://github.com/google/jax/blob/df74907257e5fd6c5c45fbb18d28753043e14746/setup.pyL49L77 The `setup.py` for jaxlib does not contain any extras relating to cuda libraries, as you can see here: https://github.com/google/jax/blob/main/jaxlib/setup.py","I guess ```python pip install upgrade ""jax[cuda11_cudnn84]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Should be the ""best"". It seems that if newest `jax` was installed but `jaxlib` is stale(or without cuda), `pip install ""jax[cuda11_cudnn84]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` won't install desired `jaxlib` since `jax` requirement already satisfied.","I've seen variations of this before – `pip`'s handling of optional dependences (the things in brackets) does not always interact well with `upgrade`. To ensure you're getting the most recent versions of what you specify, I'd suggest this instead: ``` pip uninstall jax jaxlib pip install jax[... ```",Closing; I don't think we're going to merge this.
1677,"以下是一个github上的jax下的一个issue, 标题是(lstsq is inaccurate)， 内容是 (`jax.numpy.linalg.lstsq` seems to be much less accurate than `numpy.linalg.lstsq`. I was trying to implement a random perspective transform inspired by torchvision. One of the steps is to find a leastsquares solution of `a @ x = b`. Both in NumPy and PyTorch (using the right args), I am able to get a sufficientlyaccurate answer. However, the results from JAX are unusable for me. Here is a specific example that I was trying to solve: ```python import numpy as np import jax.numpy as jp  data a = jp.array([[122, 1141, 1, 0, 0, 0, 0, 0],               [0, 0, 0, 122, 1141, 1, 156536, 1459977],               [686, 1209, 1, 0, 0, 0, 493286, 869838],               [0, 0, 0, 686, 1209, 1, 877486, 1547319],               [626, 2, 1, 0, 0, 0, 450148, 1812],               [0, 0, 0, 626, 2, 1, 0, 0],               [23, 339, 1, 0, 0, 0, 0, 0],               [0, 0, 0, 23, 339, 1, 0, 0]], dtype=jp.float32) b = jp.array([0, 1279, 719, 1279, 719, 0, 0, 0], dtype=jp.float32)  numpy x = np.linalg.lstsq(a, b, rcond=None)[0] print(np.abs(a @ x  b).sum())  0.0005  jax x = jp.linalg.lstsq(a, b, rcond=None)[0] print(jp.abs(a @ x  b).sum())  562 ``` In order to get an accurate answer in JAX, I had to use the following `lstsq` solver: ```python  jax (custom) lstsq = lambda a, b: jp.linalg.solve(a.T @ a, a.T @ b) x = lstsq(a, b) print(jp.abs(a @ x  b).sum())  0.005 ``` Is this sort of low accuracy expected or is it a bug? I tested this in JAX 0.3.14 on CPU and 0.3.8 on GPU and TPU.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,lstsq is inaccurate,"`jax.numpy.linalg.lstsq` seems to be much less accurate than `numpy.linalg.lstsq`. I was trying to implement a random perspective transform inspired by torchvision. One of the steps is to find a leastsquares solution of `a @ x = b`. Both in NumPy and PyTorch (using the right args), I am able to get a sufficientlyaccurate answer. However, the results from JAX are unusable for me. Here is a specific example that I was trying to solve: ```python import numpy as np import jax.numpy as jp  data a = jp.array([[122, 1141, 1, 0, 0, 0, 0, 0],               [0, 0, 0, 122, 1141, 1, 156536, 1459977],               [686, 1209, 1, 0, 0, 0, 493286, 869838],               [0, 0, 0, 686, 1209, 1, 877486, 1547319],               [626, 2, 1, 0, 0, 0, 450148, 1812],               [0, 0, 0, 626, 2, 1, 0, 0],               [23, 339, 1, 0, 0, 0, 0, 0],               [0, 0, 0, 23, 339, 1, 0, 0]], dtype=jp.float32) b = jp.array([0, 1279, 719, 1279, 719, 0, 0, 0], dtype=jp.float32)  numpy x = np.linalg.lstsq(a, b, rcond=None)[0] print(np.abs(a @ x  b).sum())  0.0005  jax x = jp.linalg.lstsq(a, b, rcond=None)[0] print(jp.abs(a @ x  b).sum())  562 ``` In order to get an accurate answer in JAX, I had to use the following `lstsq` solver: ```python  jax (custom) lstsq = lambda a, b: jp.linalg.solve(a.T @ a, a.T @ b) x = lstsq(a, b) print(jp.abs(a @ x  b).sum())  0.005 ``` Is this sort of low accuracy expected or is it a bug? I tested this in JAX 0.3.14 on CPU and 0.3.8 on GPU and TPU.",2022-07-10T16:30:09Z,bug,open,1,7,https://github.com/jax-ml/jax/issues/11433,"I think you should enable the 64bit precision to match the Numpy precision. ```python import jax jax.config.update(""jax_enable_x64"", True) ```","Thank you, this solves the problem on CPU/GPU. Using float64, I get the following errors: NumPy: `3.5e09` JAX: `4.7e09` Custom: `5e12` However, I intend to run the code on TPUs, so float64 is not really an option. In PyTorch, for example, I can get goodenough results with float32 when I set `driver=gels`: ```python import torch at = torch.tensor(a.tolist(), dtype=torch.float32) bt = torch.tensor(b.tolist(), dtype=torch.float32) for driver in ['gelsy', 'gels', 'gelsd', 'gelss']:     xt = torch.linalg.lstsq(at, bt, driver=driver)[0]     error = torch.abs(at @ xt  bt).sum()     print(driver, float(error))  gelsy 3198  gels 0.001  gelsd 562  gelss 562 ``` Perhaps the results in JAX are not a bug, I was simply surprised that they're so different from NumPy. Do you think this could at least be mentioned in the docs? ",You're welcome.  There is a section related to 64precision in the doc.,"I suspect this comes down to the algorithm used to compute the least squares solution. Currently we use a SVD to do so: https://github.com/google/jax/blob/b666f665ece2b91e7c07c65d27cf485a779638b6/jax/_src/numpy/linalg.pyL571 This is one of the textbook answers on how to compute the solution to a least squares problems; the others are based on Cholesky decomposition of the `a.T @ a` matrix (much as you have done, although actually you're using LU decomposition which isn't necessary because `a.T @ a` is PSD) and based on QR decomposition. The solution based on computing `a.T @ a` is usually supposed to be *less* accurate, because it ""squares"" the condition number of the matrix. It seems for your matrix the SVDbased methods, both ours and those of LAPACK, are inaccurate. I wonder why! Hopefully one of our numerical experts can chime in. (Aside: On CPU and GPU, we should probably call LAPACK and Cusolver to compute least squares rather than rolling our own. The main thing missing that would enable us to do this is that we would need to define gradients for the leastsquares computation directly. It would then not be difficult to make `lstsq` a primitive call to a LAPACK function: indeed the same ones that NumPy/Scipy/PyTorch use. Since you mentioned that you are interested in TPU, on TPU this isn't an option: there's no other library to resort to.) On TPU I would be tempted to try the QRdecomposition based version which incidentally is not dissimilar to how the LAPACK function `sgels` works. (See http://www.netlib.org/lapack/explorehtml/d0/db8/group__real_g_esolve_ga166c189c7bcd808a9468c05e53da816f.html for the details of the LAPACK function): For a square matrix it looks something like this: ``` def lstsq(a, b):   q, r = jp.linalg.qr(a)   z = q.T @ b   return lax.linalg.triangular_solve(r, z, left_side=True) ``` It might make sense to allow the user to select the algorithm, as Scipy and Pytorch do.","> _both ours and those of LAPACK, are inaccurate_ This could be because we are using single precision for a computationally intense operation and you are losing accuracy quickly as a result. For example, the answer in this thread compares the FLOPS for SVD, QR, and Cholesky (https://math.stackexchange.com/questions/51150/exactcomputationalcostsflopcountforalgorithms).",saadat I agree that double precision would be the most popular choice here. Certainly most of the time NumPy will silently upcast and compute in double precision. We intentionally do not do this in JAX because it is a very unfriendly thing to do on accelerators. It does not however explain why we see such different precision behavior between different algorithms computed *at the same precision*.,"More data points, mostly echoing the above: this is more of an algorithm question than a CPU/GPU/TPU question imho. `A` is very illconditioned. Its singular values go from 2e6 down to 4e1 ``` [2.4730732e+06 6.3774088e+05 1.7518223e+03 7.7507446e+02 5.2085254e+02  3.9505237e+02 8.7445968e01 4.3333033e01] ``` The SVD of A itself is fairly accurate, even for JAX: `` is about 1e6. This is expected: the SVD can be compute accurately even for illconditioned / singular matrices.  But this does not mean that all singular values are accurate. Only the very large ones are well resolved, with near full accuracy. The very small one probably have at most 1 digit of accuracy, if not none. JAX uses the SVD directly (pretty much the textbook algorithm, as mentioned above) for computing the minimal norm least square. But this requires inverting the singular values (larger than `rcond`). Since the smallest singular values are the least accurate, this is probably pretty inaccurate. Numpy should be using gelsd https://www.smcm.iqfr.csic.es/docs/intel/mkl/mkl_manual/lse/functn_gelsd.htm It's possible that gelsd has better accuracy / properties and is less sensible to the above, although I am not sure how it is implemented.  For the sake of experiments, consider adding this to jaxlib.  With that, adding `CUSOLVER_INCREASE_PRECISION=1` will force cuSolver to run one more iteration to get slightly better accuracy. ``` diff git a/jaxlib/cuda/cusolver_kernels./jaxlib/cuda/cusolver_kernels...6af4e6fde 100644  a/jaxlib/cuda/cusolver_kernels.cc +++ b/jaxlib/cuda/cusolver_kernels.,6 +20,8 @@ limitations under the License.  include   include   include  +include  +include   include ""absl/status/status.h""  include ""absl/status/statusor.h"" @@ 859,9 +861,19 @@ static absl::Status Gesvdj_(cudaStream_t stream, void** buffers,          float* s = static_cast(buffers[2]);          float* u = static_cast(buffers[3]);          float* v = static_cast(buffers[4]); +        if(std::getenv(""CUSOLVER_INCREASE_PRECISION"") != nullptr) { +          JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusolverDnXgesvdjSetTolerance(params, std::numeric_limits::epsilon()))); +          JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusolverDnXgesvdjSetMaxSweeps(params, 1000))); +        }          JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusolverDnSgesvdj(              handle.get(), d.jobz, d.econ, d.m, d.n, a, d.m, s, u, d.m, v,              d.n, static_cast(work), d.lwork, info, params))); +        int executed_sweeps = 1; +        double residual = 1.0; +        JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusolverDnXgesvdjGetSweeps(handle.get(), params, &executed_sweeps))); +        JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusolverDnXgesvdjGetResidual(handle.get(), params, &residual))); +        printf(""Dgesvdj residual = %e\n"", residual); +        printf(""Dgesvdj number of executed sweeps = %d \n"", executed_sweeps);          break;        }        case CusolverType::F64: { ``` Also consider  ```  bug_11433.py import numpy as np import jax.numpy as jp  data a = jp.array([[122, 1141, 1, 0, 0, 0, 0, 0],               [0, 0, 0, 122, 1141, 1, 156536, 1459977],               [686, 1209, 1, 0, 0, 0, 493286, 869838],               [0, 0, 0, 686, 1209, 1, 877486, 1547319],               [626, 2, 1, 0, 0, 0, 450148, 1812],               [0, 0, 0, 626, 2, 1, 0, 0],               [23, 339, 1, 0, 0, 0, 0, 0],               [0, 0, 0, 23, 339, 1, 0, 0]], dtype=jp.float32) b = jp.array([0, 1279, 719, 1279, 719, 0, 0, 0], dtype=jp.float32) def residual(a, b, x):   return np.linalg.norm(np.array(a) @ np.array(x)  np.array(b)) / np.linalg.norm(np.array(b))  numpy x = np.linalg.lstsq(a, b, rcond=None)[0] print(""Numpy (dgelsd): "", residual(a, b, x))  jax x = jp.linalg.lstsq(a, b, rcond=None)[0] print(""JAX (sgesvdj): "", residual(a, b, x))  jax with 0 rcond x = jp.linalg.lstsq(a, b, rcond=0)[0] print(""JAX (sgesvdj + rcond=0): "", residual(a, b, x)) ``` Then [1] Vanilla: Numpy 8e8, JAX 1e1 ``` $ python3 bug_11433.py Numpy (dgelsd):  8.859738e08 Dgesvdj residual = 6.153713e07 Dgesvdj number of executed sweeps = 4 JAX (sgesvdj):  0.12339112  ... JAX (sgesvdj + rcond=0):  0.26171464 ``` [2] Increasing cuSOLVER's accuracy: Numpy 8e8, JAX 1e1 (rcond=None) or 3e4 (rcond=0) ``` $ CUSOLVER_INCREASE_PRECISION=1 python3 bug_11433.py Numpy (dgelsd):  8.859738e08 Dgesvdj residual = 3.484288e09 Dgesvdj number of executed sweeps = 5 JAX (sgesvdj):  0.12339121 ... JAX (sgesvdj + rcond=0):  0.0003707072 ``` [3] Using JAX on CPU: Numpy 8e8, JAX 1e1 (rcond=None), JAX 4e3 (rcond=0) ``` $ CUDA_VISIBLE_DEVICES="""" python3 bug_11433.py Numpy (dgelsd):  8.859738e08 JAX (sgesvdj):  0.123391196 JAX (sgesvdj + rcond=0):  0.005672992 ``` So  Increasing the accuracy of the SVD  *and* decreasing `rcond` (to capture more singular values)  give a better solution, but still not as good as gelsd."
684,"以下是一个github上的jax下的一个issue, 标题是(Tensor Model Parallelism - `xmap` vs `pjit`)， 内容是 (Hey folks, I’m trying to write a model parallel Transformer implementation but have come across the seemingly similar xmap and pjit APIs; xmap is semidocumented in the Jax docs, but it seems pjit has been used more in practice (T5X). Which API is the current best practice/what are the trade offs of each? Are there minimal examples that might be good to work through (happy to help create one if not, if y’all can just point me in the right direction)? Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Tensor Model Parallelism - `xmap` vs `pjit`,"Hey folks, I’m trying to write a model parallel Transformer implementation but have come across the seemingly similar xmap and pjit APIs; xmap is semidocumented in the Jax docs, but it seems pjit has been used more in practice (T5X). Which API is the current best practice/what are the trade offs of each? Are there minimal examples that might be good to work through (happy to help create one if not, if y’all can just point me in the right direction)? Thanks!",2022-07-10T11:31:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/11432
1921,"以下是一个github上的jax下的一个issue, 标题是(Add loop-invariant residual optimization for `for`)， 内容是 (Right now we assume all residuals are dependent on the loop index. During `partial_eval` when we run the ""known"" jaxpr to compute the residual, we create a `Ref` for the residual that is loopvarying and write the residual into it each iteration. This unnecessarily broadcasts the residual. By using `partial_eval`, we can detect which residuals are actually loopinvariant and avoid broadcasting it across the loop. We actually need a fixpoint to determine which residuals are loop invariant. This is because we don't know which inputs to the `for` are loopinvariant as well. Check out this example `jaxpr`: ``` { lambda ; a:i32[] b:Ref{float32[4]} c:Ref{float32[5]} z_ref:Ref{float32[5]}. let     e:f32[4] < b[]     z:f32[] < z_ref     f:f32[] < c[a]     j:f32[4] = sin f     b[] < j     z_ref[] < e   in (z) } ``` We're going to think about whether or not the residual `z` is loopinvariant. If we assume all of the input `Ref`s loopinvariant and the loop index `a` loopvarying, we find that `z` is also loopinvariant because `z` is just read from `z_ref` and returned. However, we *cannot* assume all `Ref`s are loop invariant because across many iterations, they might ""pollute"" each other. For example, in this jaxpr, we see that at the end, we write `e` into `z_ref`. `e` is read from `b[]`. At the end, we write `j` into `b[]` and `j` is actually loopvarying! So, `b` becomes loopvarying because we write `j` into it. The next iteration `z_ref` becomes loopvarying because we wrote `e` into it. Therefore `z` *is* loopvarying. The fixpoint does this propagation to ensure we correctly identify which residuals are loopinvariant. Tracker: CC(Tracker: decomposing scan (aka ""Five Loop"")) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add loop-invariant residual optimization for `for`,"Right now we assume all residuals are dependent on the loop index. During `partial_eval` when we run the ""known"" jaxpr to compute the residual, we create a `Ref` for the residual that is loopvarying and write the residual into it each iteration. This unnecessarily broadcasts the residual. By using `partial_eval`, we can detect which residuals are actually loopinvariant and avoid broadcasting it across the loop. We actually need a fixpoint to determine which residuals are loop invariant. This is because we don't know which inputs to the `for` are loopinvariant as well. Check out this example `jaxpr`: ``` { lambda ; a:i32[] b:Ref{float32[4]} c:Ref{float32[5]} z_ref:Ref{float32[5]}. let     e:f32[4] < b[]     z:f32[] < z_ref     f:f32[] < c[a]     j:f32[4] = sin f     b[] < j     z_ref[] < e   in (z) } ``` We're going to think about whether or not the residual `z` is loopinvariant. If we assume all of the input `Ref`s loopinvariant and the loop index `a` loopvarying, we find that `z` is also loopinvariant because `z` is just read from `z_ref` and returned. However, we *cannot* assume all `Ref`s are loop invariant because across many iterations, they might ""pollute"" each other. For example, in this jaxpr, we see that at the end, we write `e` into `z_ref`. `e` is read from `b[]`. At the end, we write `j` into `b[]` and `j` is actually loopvarying! So, `b` becomes loopvarying because we write `j` into it. The next iteration `z_ref` becomes loopvarying because we wrote `e` into it. Therefore `z` *is* loopvarying. The fixpoint does this propagation to ensure we correctly identify which residuals are loopinvariant. Tracker: CC(Tracker: decomposing scan (aka ""Five Loop"")) ",2022-07-08T02:40:00Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/11410
3008,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Support shape polymorphism for strided convolutions)， 内容是 (Hello! I'm trying to export an audio model with variable batch and/or time dimensions. This brings a couple issues. **Case 0:** enable_xla=True, batch=None: Seems to work! Created a saved_model from the concrete_function, and subsequently managed to load and evaluate with a different batch size. The resulting model is, of course, XLAbased, though, so can't export to TFLite. **Case 1:** enable_xla=False, batch=None: Fails, due to the output_shape argument in tf.nn.conv2d_transpose:   ```TypeError: Expected int32 passed to parameter 'input_sizes' of op 'Conv2DBackpropInput', got (None, 500, 1, 512) of type 'tuple' instead. Error: Expected int32, but got None of type 'NoneType'.```   This one should be pretty easy to fix, I think: Using tf.shape(x) on a tensor creates a polymorphic shape that plays nice with conv2d_transpose. (specifically, the tf.shape(x) is a KerasTensor with a dtype of int32 and None value in some entries. Then the op knows it's eventually getting int32 values, and doesn't complain.) **Case 2:** enable_xla=False, time=None   The inference of padding_type from the padding values causes problems here. The call to lax.padtype_to_pads for 'SAME' padding computes the padding amount from the array shape, which causes problems when the array shape is unknown. Here's a truncated trace: ``` File ""/tmp/lyra_notebook.par/google3/third_party/py/flax/linen/linear.py"", line 411, in __call__  *         y = lax.conv_general_dilated(     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/_src/lax/convolution.py"", line 142, in conv_general_dilated  *         padding = lax.padtype_to_pads(     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/_src/lax/lax.py"", line 4324, in padtype_to_pads  *         out_shape = _ceil_divide(in_shape, window_strides)     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/_src/lax/lax.py"", line 4309, in _ceil_divide  *         return np.floor_divide(np.negative(x1), x2)     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/experimental/jax2tf/shape_poly.py"", line 344, in __floordiv__  **         return self.divmod(divisor)[0]     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/experimental/jax2tf/shape_poly.py"", line 323, in divmod         raise InconclusiveDimensionOperation(err_msg)     InconclusiveDimensionOperation: Dimension polynomial '1*t' is not a multiple of '32' ``` I'm really not sure if there's a great way out of this one... We can still infer VALID padding (all zeros), but it may be tough to tell the difference between SAME and EXPLICIT padding. Also note that transpose_convs seem doable, since the padding values don't depend on the input size in that case. **Case 3:** enable_xla=True, time=None   This case also hits the lax.padtype_to_pads path.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Support shape polymorphism for strided convolutions,"Hello! I'm trying to export an audio model with variable batch and/or time dimensions. This brings a couple issues. **Case 0:** enable_xla=True, batch=None: Seems to work! Created a saved_model from the concrete_function, and subsequently managed to load and evaluate with a different batch size. The resulting model is, of course, XLAbased, though, so can't export to TFLite. **Case 1:** enable_xla=False, batch=None: Fails, due to the output_shape argument in tf.nn.conv2d_transpose:   ```TypeError: Expected int32 passed to parameter 'input_sizes' of op 'Conv2DBackpropInput', got (None, 500, 1, 512) of type 'tuple' instead. Error: Expected int32, but got None of type 'NoneType'.```   This one should be pretty easy to fix, I think: Using tf.shape(x) on a tensor creates a polymorphic shape that plays nice with conv2d_transpose. (specifically, the tf.shape(x) is a KerasTensor with a dtype of int32 and None value in some entries. Then the op knows it's eventually getting int32 values, and doesn't complain.) **Case 2:** enable_xla=False, time=None   The inference of padding_type from the padding values causes problems here. The call to lax.padtype_to_pads for 'SAME' padding computes the padding amount from the array shape, which causes problems when the array shape is unknown. Here's a truncated trace: ``` File ""/tmp/lyra_notebook.par/google3/third_party/py/flax/linen/linear.py"", line 411, in __call__  *         y = lax.conv_general_dilated(     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/_src/lax/convolution.py"", line 142, in conv_general_dilated  *         padding = lax.padtype_to_pads(     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/_src/lax/lax.py"", line 4324, in padtype_to_pads  *         out_shape = _ceil_divide(in_shape, window_strides)     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/_src/lax/lax.py"", line 4309, in _ceil_divide  *         return np.floor_divide(np.negative(x1), x2)     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/experimental/jax2tf/shape_poly.py"", line 344, in __floordiv__  **         return self.divmod(divisor)[0]     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/experimental/jax2tf/shape_poly.py"", line 323, in divmod         raise InconclusiveDimensionOperation(err_msg)     InconclusiveDimensionOperation: Dimension polynomial '1*t' is not a multiple of '32' ``` I'm really not sure if there's a great way out of this one... We can still infer VALID padding (all zeros), but it may be tough to tell the difference between SAME and EXPLICIT padding. Also note that transpose_convs seem doable, since the padding values don't depend on the input size in that case. **Case 3:** enable_xla=True, time=None   This case also hits the lax.padtype_to_pads path.",2022-07-07T20:19:18Z,enhancement,closed,0,8,https://github.com/jax-ml/jax/issues/11402,"I noticed that we do not test the shape polymorphism for conv_general_dilated with enable_xla=False. However, the first test that I added for this passed. Can you please give us a reproducing example?","Hi, George; I created a Colab notebook with a minimal reproducing example: https://colab.corp.google.com/drive/1FX99EPcaX1mAVnpnpUQwkkR0WZ_6o3h?usp=sharing Oddly, it seems like the ConvTranspose with batch polymorphism only fails when there's nontrivial stride, which is probably why you weren't able to reproduce the issue initially. The time polymorphism seems to fail in either case.","I added an additional small experiment at the bottom of the colab with a workaround for ""Case 2: enable_xla=False, time=None"". In this case for my original large audio model it was the stride in a standard (nontranspose) convolution that was causing issues. To resolve this, I added some framing to the input, which then gets flattened during inference. Concretely: suppose we have 1 second of audio with shape [1, 32000], and I happen to know that the product of all the strides in my model is 320. Then at inference time, I will feed in framed input with shape [1, F, 320], which gets flattened to shape `[1, 320*F, 1]` before being fed to the actual Jax model. The shape polynomials should then hopefully handle the convolution striding appropriately, because `320*F` has the right set of divisors. This strategy seems to work with XLA enabled! With XLA disabled, we hit another issue in lax.padtype_to_pads: ``` File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/experimental/jax2tf/impl_no_xla.py"", line 250, in _conv_general_dilated  *         padding_type = pads_to_padtype(     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/experimental/jax2tf/impl_no_xla.py"", line 94, in pads_to_padtype  *         pads = lax.padtype_to_pads(in_shape, window_shape, window_strides, pad_str)     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/_src/lax/lax.py"", line 4513, in padtype_to_pads  *         out_shape = _ceil_divide(in_shape, window_strides)     File ""/tmp/lyra_notebook.par/google3/third_party/py/jax/_src/lax/lax.py"", line 4498, in _ceil_divide  *         return np.floor_divide(np.negative(x1), x2)     TypeError: bad operand type for unary : 'NoneType' ```"," I am afraid that I do not know how to handle the case of padding ""SAME"" with non1 stride. In the process of computing the padding we need to divide by the stride, and this cannot be done in the current system, unless we know e.g., that the dimension is a multiple of the stride.  Your trick with `reshape` is a good one. At the moment there is also an undocumented shortcut: if a dimension is specified as ""320*b"" then the system will know that it is a multiple of 320 (and also >= 320). This ""feature"" may be removed, so the ""reshape"" is a better workaround.","Ah, nice; I was going to ask whether it was possible to feed in nontrivial shape polynomials as an alternative... But I tihnk it's not really necessary. In practice, for streaming audio models it is common to have a known frame size and a buffer for ensuring that the correct number of samples is fed in. The exact input shape becomes a bit of a trivial implementation detail, since reshaping [T] to [T/F, F] doesn't change the memory layout...","I'll think a bit more about SAME padding. For framed audio, we know that the stride divides the number of time steps, which might be helpful. Otherwise, I wonder if there's ways we can add a bit of outside information to the jaxpr trace? (I'm not (yet?) familiar with how that part of the system works, so unsure what's possible.)  The other interesting case is CAUSAL padding, which I've added at the Flax level, and is supported by the TF conv1d call: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D and will appear as 'EXPLICIT' padding when passed to this level. It's basically SAME padding but with all of the padding applied on one side, instead of split between the two... Which is just to say that there will be interesting cases for 'explicit' padding as well, and it probably won't fly to assume 'SAME' instead of assuming 'EXPLICIT'.","Following a fix for conv1d padding, my colab notebook examples are all working. https://github.com/google/jax/pull/11524 Crucially, the SAMEpadded conv1d op works when the input is framed, as described above. It may be worth adding the framing trick as an example in the documentation. I'll try converting a full model next, and will open a new issue if anything goes wrong.",(Small update: I successfully exported the separation model with polymorphic batch and time axes! w00t!)
739,"以下是一个github上的jax下的一个issue, 标题是(Do not normalize FFT by a constant ""1"" if no normalization is provided (i.e., norm is None).)， 内容是 (Do not normalize FFT by a constant ""1"" if no normalization is provided (i.e., norm is None). Without this, the compiled graph will still contain a node multipying a complex number with a constant 1+0j (1 is cast to complex because the other term is complex as well). This is problematic when converting to TFLite using jax2tf, because multiplying complex numbers is not supported in TFLite. With this change, the multiplication is removed from the graph all together.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Do not normalize FFT by a constant ""1"" if no normalization is provided (i.e., norm is None).","Do not normalize FFT by a constant ""1"" if no normalization is provided (i.e., norm is None). Without this, the compiled graph will still contain a node multipying a complex number with a constant 1+0j (1 is cast to complex because the other term is complex as well). This is problematic when converting to TFLite using jax2tf, because multiplying complex numbers is not supported in TFLite. With this change, the multiplication is removed from the graph all together.",2022-07-07T10:53:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/11393
1263,"以下是一个github上的jax下的一个issue, 标题是([dynamic-shapes] revive basic bounded int machinery, add tests)， 内容是 (Bounded integer types are useful for lowering dynamic shape computations to XLA (though they also work with IREE). They're also necessary for the paddedandmasked raggedness representations we need when dynamic shape computations are transformed with vmap. This PR is a first step in reviving some bounded int machinery. The next steps are to add some tests for endtoend examples, and then to add the raggedness features. In addition to revival, we're also making some minor changes to past BInt experiments: most importantly, we're going to maintain the invariant that _the physical representation of an array is a function only of its type_. In particular, when an array has a BInt axis size, that means the physical buffer representation is effectively padded out along that axis to the size of the BInt's bound. There's a new `PaddedArray` type here (to be extended later to handle raggedness, and arrays with BInt element type), which is basically the runtime data representation of an array with BInts as axis sizes.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"[dynamic-shapes] revive basic bounded int machinery, add tests","Bounded integer types are useful for lowering dynamic shape computations to XLA (though they also work with IREE). They're also necessary for the paddedandmasked raggedness representations we need when dynamic shape computations are transformed with vmap. This PR is a first step in reviving some bounded int machinery. The next steps are to add some tests for endtoend examples, and then to add the raggedness features. In addition to revival, we're also making some minor changes to past BInt experiments: most importantly, we're going to maintain the invariant that _the physical representation of an array is a function only of its type_. In particular, when an array has a BInt axis size, that means the physical buffer representation is effectively padded out along that axis to the size of the BInt's bound. There's a new `PaddedArray` type here (to be extended later to handle raggedness, and arrays with BInt element type), which is basically the runtime data representation of an array with BInts as axis sizes.",2022-07-06T22:40:59Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/11387
2922,"以下是一个github上的jax下的一个issue, 标题是(Different runtimes for `pmap` vs `pjit` )， 内容是 (Hi, I have a simple script below that compares runtimes for `pmap` vs `pjit`. I expected that the runtime for `pjit` with full data parallelism would be the same for `pmap`, as they are functionally the same (shard across data, replicate the model), but it's around `1.5x2x` slower in my experiments.  ```python  script.py import jax from jax.experimental.pjit import pjit from jax.experimental.maps import Mesh from jax.experimental import PartitionSpec as P import numpy as np parser = argparse.ArgumentParser() parser.add_argument('m', 'mode', type=str, choices=['pmap', 'pjit'], default='pmap') args = parser.parse_args()  Init data x = np.random.randn(32, 1024).astype(np.float32) W = np.random.randn(1024, 8).astype(np.float32) def step(x, W):     return jax.lax.dot(x, W)  Compute pmap or pjit functions  Preload batch data and model parameters onto the devices as ShardedDeviceArrays if args.mode == 'pmap':     p_step = jax.pmap(step, axis_name='batch')     x = np.reshape(x, (jax.local_device_count(), 1, x.shape[1]))      Gets correct device order that matches pmap     devices = jax.lib.xla_bridge.get_backend().get_default_device_assignment(jax.device_count())     x = jax.device_put_sharded(list(x), devices)     W = jax.device_put_replicated(W, devices) else:     mesh = Mesh(np.asarray(jax.devices(), dtype=object).reshape(jax.local_device_count(), 1), ['dp', 'mp'])     jax.experimental.maps.thread_resources.env = (         jax.experimental.maps.ResourceEnv(physical_mesh=mesh, loops=())     )     p_step = pjit(step, in_axis_resources=(P('dp'), P('mp', None)), out_axis_resources=P('dp'))      Map batch and weights to devices     p_init = pjit(lambda x, W: (x, W), in_axis_resources=(P('dp'), P('mp', None)), out_axis_resources=(P('dp'), P('mp', None)))     x, W = p_init(x, W)  Warmup for initial compilation p_step(x, W).block_until_ready()  Time iterations = 1000 avg = timeit.timeit(lambda: p_step(x, W).block_until_ready(), number=iterations) / iterations print('Estimated Time:', avg, 'per itr') ``` I see the following outputs: ``` wilsonn588e2d9fw0:~$ python3 script.py m pmap Estimated Time: 0.0006339213080937043 per itr ``` ``` wilsonn588e2d9fw0:~$ python3 script.py m pjit Estimated Time: 0.0009550822210730985 per itr ``` Note that there is some noise in the output, but even when turning up the number of samples, there's a pretty substantial difference  even more so if I run larger models (i.e. an MLP, or transformer vs just a linear layer). All of these commands are done on a `v38` TPU instance, with `jax==0.3.14`, `jaxlib==0.3.14`, `libtpunightly==0.1.dev20220627`. Any help or insight would be appreciated on this! (Or, if there's an issue with any of my code))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Different runtimes for `pmap` vs `pjit` ,"Hi, I have a simple script below that compares runtimes for `pmap` vs `pjit`. I expected that the runtime for `pjit` with full data parallelism would be the same for `pmap`, as they are functionally the same (shard across data, replicate the model), but it's around `1.5x2x` slower in my experiments.  ```python  script.py import jax from jax.experimental.pjit import pjit from jax.experimental.maps import Mesh from jax.experimental import PartitionSpec as P import numpy as np parser = argparse.ArgumentParser() parser.add_argument('m', 'mode', type=str, choices=['pmap', 'pjit'], default='pmap') args = parser.parse_args()  Init data x = np.random.randn(32, 1024).astype(np.float32) W = np.random.randn(1024, 8).astype(np.float32) def step(x, W):     return jax.lax.dot(x, W)  Compute pmap or pjit functions  Preload batch data and model parameters onto the devices as ShardedDeviceArrays if args.mode == 'pmap':     p_step = jax.pmap(step, axis_name='batch')     x = np.reshape(x, (jax.local_device_count(), 1, x.shape[1]))      Gets correct device order that matches pmap     devices = jax.lib.xla_bridge.get_backend().get_default_device_assignment(jax.device_count())     x = jax.device_put_sharded(list(x), devices)     W = jax.device_put_replicated(W, devices) else:     mesh = Mesh(np.asarray(jax.devices(), dtype=object).reshape(jax.local_device_count(), 1), ['dp', 'mp'])     jax.experimental.maps.thread_resources.env = (         jax.experimental.maps.ResourceEnv(physical_mesh=mesh, loops=())     )     p_step = pjit(step, in_axis_resources=(P('dp'), P('mp', None)), out_axis_resources=P('dp'))      Map batch and weights to devices     p_init = pjit(lambda x, W: (x, W), in_axis_resources=(P('dp'), P('mp', None)), out_axis_resources=(P('dp'), P('mp', None)))     x, W = p_init(x, W)  Warmup for initial compilation p_step(x, W).block_until_ready()  Time iterations = 1000 avg = timeit.timeit(lambda: p_step(x, W).block_until_ready(), number=iterations) / iterations print('Estimated Time:', avg, 'per itr') ``` I see the following outputs: ``` wilsonn588e2d9fw0:~$ python3 script.py m pmap Estimated Time: 0.0006339213080937043 per itr ``` ``` wilsonn588e2d9fw0:~$ python3 script.py m pjit Estimated Time: 0.0009550822210730985 per itr ``` Note that there is some noise in the output, but even when turning up the number of samples, there's a pretty substantial difference  even more so if I run larger models (i.e. an MLP, or transformer vs just a linear layer). All of these commands are done on a `v38` TPU instance, with `jax==0.3.14`, `jaxlib==0.3.14`, `libtpunightly==0.1.dev20220627`. Any help or insight would be appreciated on this! (Or, if there's an issue with any of my code)",2022-07-05T01:29:20Z,bug,closed,1,8,https://github.com/jax-ml/jax/issues/11364,"If I remove the model parallelism component in the `pjit` portion of the code, i.e. only `['dp']` ```python mesh = Mesh(np.asarray(jax.devices(), dtype=object).reshape(jax.local_device_count(),), ['dp'])  jax.experimental.maps.thread_resources.env = (     jax.experimental.maps.ResourceEnv(physical_mesh=mesh, loops=()) ) p_step = pjit(step, in_axis_resources=(P('dp'), None), out_axis_resources=P('dp'))  Map batch and weights to devices p_init = pjit(lambda x, W: (x, W), in_axis_resources=(P('dp'), None), out_axis_resources=(P('dp'), None)) x, W = p_init(x, W) ``` It's slightly faster, but still slower than `pmap`: ``` wilsonn588e2d9fw0:~$ python3 test/script.py m pjit Estimated Time: 0.000860201039002277 per itr ``` Is `pjit` doing some extra unnecessary computations that `pmap` doesn't?","Would be interested in knowing the answer to this as well, as I am also observing it in my code.","This computation is actually quite fast, so I imagine the difference might be due to the pmap dispatch path being optimized (as it's implemented in C++) compared to pjit's dispatch (written entirely in Python).","I tried running similar code on some larger models, and get similar effects.  This is also done **with only data parallelism (no model axis in mesh)** Code is run on a `v38` TPUVM with   `jax==0.3.15`  `jaxlib==0.3.15`  `libtpunightly==0.1.dev20220722`  `flax==0.5.3` Transformer 1: `1.6M params, Transformer(hidden_dim=128, num_heads=4, num_layers=8)` (1.4x slower) `pmap: Estimated Time: 0.00693524660900016 per itr` `pjit: Estimated Time: 0.010101222762999896 per itr` Transformer 2:  `25M params, Transformer(hidden_dim=512, num_heads=8, num_layers=8)` (1.2x slower) `pmap: Estimated Time: 0.015546864855000194 per itr` `pjit: Estimated Time: 0.01896321659 per itr` Transformer 3:  `100M params, Transformer(hidden_dim=1024, num_heads=16, num_layers=8)` (1.08x slower) `pmap: Estimated Time: 0.037007930983999814 per itr` `pjit: Estimated Time: 0.04010667563000016 per itr` There seems to still be a difference, with the difference decreasing as I increase model size (though still statistically significant). I tried a different model that similar to one I'm using (on videos, framewise encoding / decoder with a transformer over spacetime). For some reason, under an identical setup, this model is **significantly slower between pmap and pjit (7x)** `pmap: Estimated Time: 0.042936902346999886 per itr` `pjit: Estimated Time: 0.3086913418910008 per itr` **Is there any reason why model architecture could cause this large difference in performance?** The code for both models is below (`script.py` corresponds to the Transformer, and `script2.py` corresponds to the other model): ```python  script.py import timeit import argparse import jax from jax.experimental.pjit import pjit from jax.experimental.maps import Mesh from jax.experimental import PartitionSpec as P import numpy as np import flax.linen as nn class Transformer(nn.Module):     hidden_dim: int     num_heads: int     num_layers: int     .compact     def __call__(self, x):         x = nn.Dense(self.hidden_dim)(x)         x = nn.LayerNorm()(x)         for _ in range(self.num_layers):             x = TransformerBlock(self.hidden_dim, self.num_heads)(x)         return x class TransformerBlock(nn.Module):     hidden_dim: int     num_heads: int     .compact     def __call__(self, x):         h = nn.LayerNorm()(x)         h = nn.SelfAttention(num_heads=self.num_heads)(x)         x = x + h         h = nn.LayerNorm()(x)         h = nn.Sequential([             nn.Dense(4 * self.hidden_dim),             nn.gelu,             nn.Dense(self.hidden_dim)         ])(h)         x = x + h         return x def print_model_size(params, name=''):     model_params_size = jax.tree_util.tree_map(lambda x: x.size, params)     total_params_size = sum(jax.tree_util.tree_flatten(model_params_size)[0])     print('model parameter count:', total_params_size) parser = argparse.ArgumentParser() parser.add_argument('m', 'mode', type=str, choices=['pmap', 'pjit'], default='pmap') args = parser.parse_args()  Init data x = np.random.randn(32, 1024, 64).astype(np.float32) model = Transformer(hidden_dim=128, num_heads=4, num_layers=8) variables = model.init(rngs=jax.random.PRNGKey(0), x=x) print_model_size(variables) def step(x, variables):     return model.apply(variables, x)  Compute pmap or pjit functions  Preload batch data and model parameters onto the devices as ShardedDeviceArrays if args.mode == 'pmap':     p_step = jax.pmap(step, axis_name='batch')     x = np.reshape(x, (jax.local_device_count(), 1, *x.shape[1:]))      Gets correct device order that matches pmap     devices = jax.lib.xla_bridge.get_backend().get_default_device_assignment(jax.device_count())     x = jax.device_put_sharded(list(x), devices)     variables = jax.device_put_replicated(variables, devices) else:     mesh = Mesh(np.asarray(jax.devices(), dtype=object).reshape(jax.local_device_count(),), ['dp'])     jax.experimental.maps.thread_resources.env = (         jax.experimental.maps.ResourceEnv(physical_mesh=mesh, loops=())     )     p_step = pjit(step, in_axis_resources=(P('dp'), None), out_axis_resources=P('dp'))      Map batch and weights to devices     p_init = pjit(lambda x, variables: (x, variables), in_axis_resources=(P('dp'), None), out_axis_resources=(P('dp'), None))     x, variables = p_init(x, variables)  Warmup for initial compilation p_step(x, variables).block_until_ready()  Time iterations = 1000 avg = timeit.timeit(lambda: p_step(x, variables).block_until_ready(), number=iterations) / iterations print('Estimated Time:', avg, 'per itr') ``` ```python  script2.py from typing import Tuple, Any import timeit import argparse import jax from jax.experimental.pjit import pjit from jax.experimental.maps import Mesh from jax.experimental import PartitionSpec as P import numpy as np import flax.linen as nn class Model(nn.Module):     enc_args: Any     tfm_args: Any     dec_args: Any     .compact     def __call__(self, x):         x = jax.vmap(Encoder(**self.enc_args), 1, 1)(x)         old_shape = x.shape[1:1]         x = x.reshape(x.shape[0], 1, x.shape[1])         x = Transformer(**self.tfm_args)(x)         x = x.reshape(x.shape[0], *old_shape, x.shape[1])         x = jax.vmap(Decoder(**self.dec_args), 1, 1)(x)         return x def block(x, depth):     skip = x     if skip.shape[1] != depth:         skip = nn.Conv(depth, [1, 1], use_bias=False)(skip)     x = nn.Sequential([         nn.GroupNorm(),         nn.elu,         nn.Conv(depth, [3, 3]),         nn.GroupNorm(),         nn.elu,         nn.Conv(depth, [3, 3])     ])(x)     return skip + 0.1 * x class Encoder(nn.Module):     depths: Tuple     blocks: int     .compact     def __call__(self, x):         x = nn.Conv(self.depths[0], [3, 3])(x)         for i in range(1, len(self.depths)):             x = nn.avg_pool(x, (2, 2), strides=(2, 2))             for _ in range(self.blocks):                 x = block(x, self.depths[i])         return x class Decoder(nn.Module):     depths: Tuple     blocks: int     .compact     def __call__(self, x):         for i in range(len(self.depths)  1):             for _ in range(self.blocks):                 x = block(x, self.depths[i])             x = jax.image.resize(x, (x.shape[0], 2 * x.shape[1], 2 * x.shape[2], x.shape[3]),                                  method='nearest')         x = nn.Conv(3, [3, 3])(x)         return x class Transformer(nn.Module):     hidden_dim: int     num_heads: int     num_layers: int     .compact     def __call__(self, x):         x = nn.Dense(self.hidden_dim)(x)         x = nn.LayerNorm()(x)         for _ in range(self.num_layers):             x = TransformerBlock(self.hidden_dim, self.num_heads)(x)         return x class TransformerBlock(nn.Module):     hidden_dim: int     num_heads: int     .compact     def __call__(self, x):         h = nn.LayerNorm()(x)         h = nn.SelfAttention(num_heads=self.num_heads)(x)         x = x + h         h = nn.LayerNorm()(x)         h = nn.Sequential([             nn.Dense(4 * self.hidden_dim),             nn.gelu,             nn.Dense(self.hidden_dim)         ])(h)         x = x + h         return x def print_model_size(params, name=''):     model_params_size = jax.tree_util.tree_map(lambda x: x.size, params)     total_params_size = sum(jax.tree_util.tree_flatten(model_params_size)[0])     print('model parameter count:', total_params_size) parser = argparse.ArgumentParser() parser.add_argument('m', 'mode', type=str, choices=['pmap', 'pjit'], default='pmap') args = parser.parse_args()  Init data x = np.random.randn(32, 100, 16, 16, 3).astype(np.float32) model = Model(enc_args=dict(depths=[64, 128, 256], blocks=2),               tfm_args=dict(hidden_dim=512, num_heads=8, num_layers=8),               dec_args=dict(depths=[256, 128, 64], blocks=2)) variables = model.init(rngs=jax.random.PRNGKey(0), x=x) print_model_size(variables) def step(x, variables):     return model.apply(variables, x)  Compute pmap or pjit functions  Preload batch data and model parameters onto the devices as ShardedDeviceArrays if args.mode == 'pmap':     p_step = jax.pmap(step, axis_name='batch')     x = np.reshape(x, (jax.local_device_count(), 1, *x.shape[1:]))      Gets correct device order that matches pmap     devices = jax.lib.xla_bridge.get_backend().get_default_device_assignment(jax.device_count())     x = jax.device_put_sharded(list(x), devices)     variables = jax.device_put_replicated(variables, devices) else:     mesh = Mesh(np.asarray(jax.devices(), dtype=object).reshape(jax.local_device_count(),), ['dp'])     jax.experimental.maps.thread_resources.env = (         jax.experimental.maps.ResourceEnv(physical_mesh=mesh, loops=())     )     p_step = pjit(step, in_axis_resources=(P('dp'), None), out_axis_resources=P('dp'))      Map batch and weights to devices     p_init = pjit(lambda x, variables: (x, variables), in_axis_resources=(P('dp'), None), out_axis_resources=(P('dp'), None))     x, variables = p_init(x, variables)  Warmup for initial compilation p_step(x, variables).block_until_ready()  Time iterations = 1000 avg = timeit.timeit(lambda: p_step(x, variables).block_until_ready(), number=iterations) / iterations print('Estimated Time:', avg, 'per itr') ```","I looked into it and I think it's basically that the SPMD partitioner ends up failing to propagate the dataparallel sharding through the full HLO. The HLO module does contain some while loops that might be confusing for it. SPMD partitioner is heuristic based, so it might not get everything right. This is why sometimes manual partitioning with pmap or xmap can lead to better results, since they can provide you with explicit guarantees.","Thanks for looking into it! Is there a good way to prevent this issue from happening codewise? i.e. differently coding the architecture or enforcing certain constraints to help the partitioner partition how we'd like it to (assuming knowledge of some prespecified data/model partitioning scheme)? Otherwise, the best option might be to switch to using xmap for the best mix of correct datasharding / model parallelism?","You could try sprinkling some `with_sharding_constraint`s over your code. It might help the SPMD partitioner do the right thing. But if you know what sharding you want to get, and you actually prefer explicit control, then `xmap` sounds like a better way to go. In the meantime, I've brought this up with the XLA team. I've opened an internal bug with them, and since there's nothing we can do on the JAX side to fix it, I'm going to close it. Feel free to reopen if you think there's still something more we could do. Thanks!",And also thanks so much for a detailed repro!
8272,"以下是一个github上的jax下的一个issue, 标题是(Update pillow requirement from <9.1.0,>=8.3.1 to >=8.3.1,<9.3.0)， 内容是 (Updates the requirements on pillow to permit the latest version.  Release notes Sourced from pillow's releases.  9.2.0 https://pillow.readthedocs.io/en/stable/releasenotes/9.2.0.html Changes  Fixed null check for fribidi_version_info in FriBiDi shim  CC(Silence some mypy errors seen with Python 3.9 and Numpy 1.20.) [@​nulano] Added GIF decompression bomb check  CC(autodidax: add 'open in colab' button, add numpy [@​radarhere] Handle PCF fonts files with less than 256 characters  CC(Factor out wheelbuilding functions in build_jaxlib_wheels.sh.) [@​dawidcrivelli] Improved GIF optimize condition  CC(未找到相关数据) [@​raygard] Reverted to array_interface with the release of NumPy 1.23  CC(bump minimum jaxlib version) [@​radarhere] Pad PCX palette to 768 bytes when saving  CC(Some updates to jax2tf) [@​radarhere] Fixed bug with rounding pixels to palette colors  CC(Pass axis name to _match_axes and add to error message.) [@​btrekkie] Use gnomescreenshot on Linux if available  CC(Remove `jax.argnums_partial`.) [@​radarhere] Fixed loading L mode BMP RLE8 images  CC(Implement segment_prod, segment_max, segment_min) [@​radarhere] Fixed incorrect operator in ImageCms error  CC(未找到相关数据) [@​LostBenjamin] Limit FPX tile size to avoid extending outside image  CC(Fix typo in rng_bit_generator comment.) [@​radarhere] Added support for decoding plain PPM formats  CC(未找到相关数据) [@​Piolie] Added apply_transparency()  CC(未找到相关数据) [@​radarhere] Fixed behaviour change from endian fix  CC(update version and changelog for pypi) [@​radarhere] Use python3  CC(Bidirectional array compatibility) [@​radarhere] Allow remapping P images with RGBA palettes  CC(jax.ops.segment_sum incompatible with jit) [@​radarhere] Revert &quot;Skip test_realloc_overflow unless libtiff 4.0.4 or higher&quot;  CC(Fix jnp.flip for axis tuples) [@​radarhere] [precommit.ci] precommit autoupdate  CC(jnp.flip crashes when given a tuple of axes) [@​precommitci] Only import ImageFont in ImageDraw when necessary  CC(未找到相关数据) [@​radarhere] Fixed drawing translucent 1px high polygons  CC(Tensorboard profiling causes random crashes) [@​radarhere] Pad COLORMAP to 768 items when saving TIFF  CC(Fix transpose rule for jnp.fft.irfft) [@​radarhere] Fix P &gt; PA conversion   CC(Reimplement lu_pivots_to_permutation as a JAX Primitive) [@​RedShy] Once exif data is parsed, do not reload unless it changes  CC(Modifying self in a scan function.) [@​radarhere] Only try to connect discontiguous corners at the end of edges  CC(未找到相关数据) [@​radarhere] Improve transparency handling when saving GIF images  CC(Fix tests broken in 6138) [@​radarhere] Do not update GIF frame position until local image is found  CC(Fix lax.all_gather inside xmap) [@​radarhere] Netscape GIF extension belongs after the global color table  CC(Fix flake8 error.) [@​radarhere] Only write GIF comments at the beginning of the file  CC(未找到相关数据) [@​raygard] Separate multiple GIF comment blocks with newlines  CC(未找到相关数据) [@​raygard] Always use GIF89a for comments  CC(Inconsistent behavior between vmap and pmap keeping track of axes inside of scan) [@​raygard] Ignore compression value from BMP info dictionary when saving as TIFF  CC(未找到相关数据) [@​radarhere] If font is filelike object, do not reread from object to get variant  CC(Work around CPython bug https://bugs.python.org/issue33261 exposed by changes to C++ JIT dispatch path.) [@​radarhere] Raise ValueError when trying to access internal fp after close  CC(Issue running Jax 101  Gradient only defined for scalaroutput functions) [@​radarhere] Support more affine expression forms in im.point()  CC(未找到相关数据) [@​benrg] Include 'twine check' in 'make sdist'  CC(Adjust JetTest.test_scatter_add tolerance for TPU) [@​hugovk] Ensure that furthest v is set in quantize2  CC(call the right utility function for custom_vjp_call_jaxpr batching) [@​radarhere]  Dependencies  Updated harfbuzz to 4.4.1  CC(DOC: fix typos.) [@​radarhere] Updated harfbuzz to 4.4.0  CC(Remove omnistaging references in CI build config.) [@​radarhere] Use SourceForge auto mirror capability  CC([jax2tf]  Add support for shape polymorphism for jax2tf; some refactoring of shape computations in JAX core) [@​raygard] Updated libtiff to 4.4.0  CC(Djax dot) [@​radarhere] Updated harfbuzz to 4.3.0  CC(Update changelog for 5868) [@​radarhere]    ... (truncated)   Changelog Sourced from pillow's changelog.  9.2.0 (20220701)   Deprecate ImageFont.getsize and related functions  CC(Testing: avoid global fixture for doctests) [nulano, radarhere]   Fixed null check for fribidi_version_info in FriBiDi shim  CC(Silence some mypy errors seen with Python 3.9 and Numpy 1.20.) [nulano]   Added GIF decompression bomb check  CC(autodidax: add 'open in colab' button, add numpy [radarhere]   Handle PCF fonts files with less than 256 characters  CC(Factor out wheelbuilding functions in build_jaxlib_wheels.sh.) [dawidcrivelli, radarhere]   Improved GIF optimize condition  CC(未找到相关数据) [raygard, radarhere]   Reverted to array_interface with the release of NumPy 1.23  CC(bump minimum jaxlib version) [radarhere]   Pad PCX palette to 768 bytes when saving  CC(Some updates to jax2tf) [radarhere]   Fixed bug with rounding pixels to palette colors  CC(Pass axis name to _match_axes and add to error message.) [btrekkie, radarhere]   Use gnomescreenshot on Linux if available  CC(Remove `jax.argnums_partial`.) [radarhere, nulano]   Fixed loading L mode BMP RLE8 images  CC(Implement segment_prod, segment_max, segment_min) [radarhere]   Fixed incorrect operator in ImageCms error  CC(未找到相关数据) [LostBenjamin, hugovk, radarhere]   Limit FPX tile size to avoid extending outside image  CC(Fix typo in rng_bit_generator comment.) [radarhere]   Added support for decoding plain PPM formats  CC(未找到相关数据) [Piolie, radarhere]   Added apply_transparency()  CC(未找到相关数据) [radarhere]   Fixed behaviour change from endian fix  CC(update version and changelog for pypi) [radarhere]   Allow remapping P images with RGBA palettes  CC(jax.ops.segment_sum incompatible with jit) [radarhere]     ... (truncated)   Commits  58acec3 Update CHANGES.rst [ci skip] dc518ac 9.2.0 version bump 488589b Merge pull request  CC(Testing: avoid global fixture for doctests) from nulano/deprecategetsize 8a6050e Replaced internal argument with warning filters 79329fb Merge pull request  CC(Fix handling of ad.Zero in _select_and_scatter_add_transpose.) from radarhere/furo 4ca99f7 Install furo if it is not available 729fe6f Updated indentation 74e0b95 test {ImageFont,TransposedFont}.getsize() deprecation a37c21e document planned removal date for ImageFont deprecations and release notes ad5271d Document replacements for individual deprecated font methods Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Update pillow requirement from <9.1.0,>=8.3.1 to >=8.3.1,<9.3.0","Updates the requirements on pillow to permit the latest version.  Release notes Sourced from pillow's releases.  9.2.0 https://pillow.readthedocs.io/en/stable/releasenotes/9.2.0.html Changes  Fixed null check for fribidi_version_info in FriBiDi shim  CC(Silence some mypy errors seen with Python 3.9 and Numpy 1.20.) [@​nulano] Added GIF decompression bomb check  CC(autodidax: add 'open in colab' button, add numpy [@​radarhere] Handle PCF fonts files with less than 256 characters  CC(Factor out wheelbuilding functions in build_jaxlib_wheels.sh.) [@​dawidcrivelli] Improved GIF optimize condition  CC(未找到相关数据) [@​raygard] Reverted to array_interface with the release of NumPy 1.23  CC(bump minimum jaxlib version) [@​radarhere] Pad PCX palette to 768 bytes when saving  CC(Some updates to jax2tf) [@​radarhere] Fixed bug with rounding pixels to palette colors  CC(Pass axis name to _match_axes and add to error message.) [@​btrekkie] Use gnomescreenshot on Linux if available  CC(Remove `jax.argnums_partial`.) [@​radarhere] Fixed loading L mode BMP RLE8 images  CC(Implement segment_prod, segment_max, segment_min) [@​radarhere] Fixed incorrect operator in ImageCms error  CC(未找到相关数据) [@​LostBenjamin] Limit FPX tile size to avoid extending outside image  CC(Fix typo in rng_bit_generator comment.) [@​radarhere] Added support for decoding plain PPM formats  CC(未找到相关数据) [@​Piolie] Added apply_transparency()  CC(未找到相关数据) [@​radarhere] Fixed behaviour change from endian fix  CC(update version and changelog for pypi) [@​radarhere] Use python3  CC(Bidirectional array compatibility) [@​radarhere] Allow remapping P images with RGBA palettes  CC(jax.ops.segment_sum incompatible with jit) [@​radarhere] Revert &quot;Skip test_realloc_overflow unless libtiff 4.0.4 or higher&quot;  CC(Fix jnp.flip for axis tuples) [@​radarhere] [precommit.ci] precommit autoupdate  CC(jnp.flip crashes when given a tuple of axes) [@​precommitci] Only import ImageFont in ImageDraw when necessary  CC(未找到相关数据) [@​radarhere] Fixed drawing translucent 1px high polygons  CC(Tensorboard profiling causes random crashes) [@​radarhere] Pad COLORMAP to 768 items when saving TIFF  CC(Fix transpose rule for jnp.fft.irfft) [@​radarhere] Fix P &gt; PA conversion   CC(Reimplement lu_pivots_to_permutation as a JAX Primitive) [@​RedShy] Once exif data is parsed, do not reload unless it changes  CC(Modifying self in a scan function.) [@​radarhere] Only try to connect discontiguous corners at the end of edges  CC(未找到相关数据) [@​radarhere] Improve transparency handling when saving GIF images  CC(Fix tests broken in 6138) [@​radarhere] Do not update GIF frame position until local image is found  CC(Fix lax.all_gather inside xmap) [@​radarhere] Netscape GIF extension belongs after the global color table  CC(Fix flake8 error.) [@​radarhere] Only write GIF comments at the beginning of the file  CC(未找到相关数据) [@​raygard] Separate multiple GIF comment blocks with newlines  CC(未找到相关数据) [@​raygard] Always use GIF89a for comments  CC(Inconsistent behavior between vmap and pmap keeping track of axes inside of scan) [@​raygard] Ignore compression value from BMP info dictionary when saving as TIFF  CC(未找到相关数据) [@​radarhere] If font is filelike object, do not reread from object to get variant  CC(Work around CPython bug https://bugs.python.org/issue33261 exposed by changes to C++ JIT dispatch path.) [@​radarhere] Raise ValueError when trying to access internal fp after close  CC(Issue running Jax 101  Gradient only defined for scalaroutput functions) [@​radarhere] Support more affine expression forms in im.point()  CC(未找到相关数据) [@​benrg] Include 'twine check' in 'make sdist'  CC(Adjust JetTest.test_scatter_add tolerance for TPU) [@​hugovk] Ensure that furthest v is set in quantize2  CC(call the right utility function for custom_vjp_call_jaxpr batching) [@​radarhere]  Dependencies  Updated harfbuzz to 4.4.1  CC(DOC: fix typos.) [@​radarhere] Updated harfbuzz to 4.4.0  CC(Remove omnistaging references in CI build config.) [@​radarhere] Use SourceForge auto mirror capability  CC([jax2tf]  Add support for shape polymorphism for jax2tf; some refactoring of shape computations in JAX core) [@​raygard] Updated libtiff to 4.4.0  CC(Djax dot) [@​radarhere] Updated harfbuzz to 4.3.0  CC(Update changelog for 5868) [@​radarhere]    ... (truncated)   Changelog Sourced from pillow's changelog.  9.2.0 (20220701)   Deprecate ImageFont.getsize and related functions  CC(Testing: avoid global fixture for doctests) [nulano, radarhere]   Fixed null check for fribidi_version_info in FriBiDi shim  CC(Silence some mypy errors seen with Python 3.9 and Numpy 1.20.) [nulano]   Added GIF decompression bomb check  CC(autodidax: add 'open in colab' button, add numpy [radarhere]   Handle PCF fonts files with less than 256 characters  CC(Factor out wheelbuilding functions in build_jaxlib_wheels.sh.) [dawidcrivelli, radarhere]   Improved GIF optimize condition  CC(未找到相关数据) [raygard, radarhere]   Reverted to array_interface with the release of NumPy 1.23  CC(bump minimum jaxlib version) [radarhere]   Pad PCX palette to 768 bytes when saving  CC(Some updates to jax2tf) [radarhere]   Fixed bug with rounding pixels to palette colors  CC(Pass axis name to _match_axes and add to error message.) [btrekkie, radarhere]   Use gnomescreenshot on Linux if available  CC(Remove `jax.argnums_partial`.) [radarhere, nulano]   Fixed loading L mode BMP RLE8 images  CC(Implement segment_prod, segment_max, segment_min) [radarhere]   Fixed incorrect operator in ImageCms error  CC(未找到相关数据) [LostBenjamin, hugovk, radarhere]   Limit FPX tile size to avoid extending outside image  CC(Fix typo in rng_bit_generator comment.) [radarhere]   Added support for decoding plain PPM formats  CC(未找到相关数据) [Piolie, radarhere]   Added apply_transparency()  CC(未找到相关数据) [radarhere]   Fixed behaviour change from endian fix  CC(update version and changelog for pypi) [radarhere]   Allow remapping P images with RGBA palettes  CC(jax.ops.segment_sum incompatible with jit) [radarhere]     ... (truncated)   Commits  58acec3 Update CHANGES.rst [ci skip] dc518ac 9.2.0 version bump 488589b Merge pull request  CC(Testing: avoid global fixture for doctests) from nulano/deprecategetsize 8a6050e Replaced internal argument with warning filters 79329fb Merge pull request  CC(Fix handling of ad.Zero in _select_and_scatter_add_transpose.) from radarhere/furo 4ca99f7 Install furo if it is not available 729fe6f Updated indentation 74e0b95 test {ImageFont,TransposedFont}.getsize() deprecation a37c21e document planned removal date for ImageFont deprecations and release notes ad5271d Document replacements for individual deprecated font methods Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-07-04T17:18:27Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/11363,"We cannot do this because of the deprecation of NEAREST. We could fix this in JAX, but Keras still uses the deprecated code, which causes our test suite to fail (tracked in https://github.com/kerasteam/tfkeras/issues/582). For now, the easiest thing to do is to just pin older versions.","OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
3023,"以下是一个github上的jax下的一个issue, 标题是(Unable to get jax.profiler to work on TPU in the latest version of JAX)， 内容是 (Although I just encountered this error when using JAX 0.3.14, I have encountered the error for many times with various versions of JAX. The cause of the error is obvious. `jax.profiler` depends on Tensorflow, while Tensorflow is not automatically installed when installing JAX in a new virtual environment. Therefore, `jax.profiler` will not work, and it shows this error: ``` 20220704 21:27:10.631029: E external/org_tensorflow/tensorflow/python/profiler/internal/python_hooks.cc:369] Can't import tensorflow.python.profiler.trace ``` It is not possible to resolve this issue by installing Tensorflow from pip directly, because the default Tensorflow version is not compiled with TPU support. The only way to solve the issue seems to be compiling Tensorflow from source, but compiling Tensorflow from source results in another error.  This is the method I used to compile Tensorflow from source. JAX version 0.3.14 Locate https://github.com/google/jax/blob/jaxlibv0.3.14/WORKSPACEL13. The line suggests that JAX 0.3.14 is compatible with Tensorflow commit `d250676d7776cfbca38e8690b75e1376afecf58d`. Then, I compiled Tensorflow by this script: ```sh !/bin/bash set e set v sudo rm rf ~/.cache/bazel   clear bazel cache (see tensorflow CC(未找到相关数据)) export TENSORFLOW_SHA=d250676d7776cfbca38e8690b75e1376afecf58d wget nc https://github.com/tensorflow/tensorflow/archive/$TENSORFLOW_SHA.tar.gz tar zxf $TENSORFLOW_SHA.tar.gz rm f $TENSORFLOW_SHA.tar.gz mv tensorflow$TENSORFLOW_SHA tensorflow cd tensorflow export BAZEL_VERSION=`cat .bazelversion` sudo mkdir p /usr/local/lib/bazel/bin sudo wget nc P /usr/local/lib/bazel/bin https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel$BAZEL_VERSIONlinuxx86_64 sudo chmod +x /usr/local/lib/bazel/bin/bazel$BAZEL_VERSIONlinuxx86_64 rm rf ~/.venv310 python3.10 m venv ~/.venv310 . ~/.venv310/bin/activate pip install U pip pip install U wheel pip install U packaging pip install U keras_preprocessing nodeps pip install ""jax[tpu]==0.3.14"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html yes '' | python configure.py   use the default settings bazel build config=tpu //tensorflow/tools/pip_package:build_pip_package ./bazelbin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg ls /tmp/tensorflow_pkg/tensorflow*.whl ``` However. I got this error: ``` 20220704 21:11:18.248361: F ./tensorflow/core/tpu/tpu_library_init_fns.inc:34] TpuEmbeddingEngineState_Create not available in this library. ``` I know that this error occurs when the libtpu version is not compatible with the Tensorflow version. Previously I always fixed it by recompiling Tensorflow from source. However, this time the error occurred when I was compiling Tensorflow from source, so I do not know what to do.  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unable to get jax.profiler to work on TPU in the latest version of JAX,"Although I just encountered this error when using JAX 0.3.14, I have encountered the error for many times with various versions of JAX. The cause of the error is obvious. `jax.profiler` depends on Tensorflow, while Tensorflow is not automatically installed when installing JAX in a new virtual environment. Therefore, `jax.profiler` will not work, and it shows this error: ``` 20220704 21:27:10.631029: E external/org_tensorflow/tensorflow/python/profiler/internal/python_hooks.cc:369] Can't import tensorflow.python.profiler.trace ``` It is not possible to resolve this issue by installing Tensorflow from pip directly, because the default Tensorflow version is not compiled with TPU support. The only way to solve the issue seems to be compiling Tensorflow from source, but compiling Tensorflow from source results in another error.  This is the method I used to compile Tensorflow from source. JAX version 0.3.14 Locate https://github.com/google/jax/blob/jaxlibv0.3.14/WORKSPACEL13. The line suggests that JAX 0.3.14 is compatible with Tensorflow commit `d250676d7776cfbca38e8690b75e1376afecf58d`. Then, I compiled Tensorflow by this script: ```sh !/bin/bash set e set v sudo rm rf ~/.cache/bazel   clear bazel cache (see tensorflow CC(未找到相关数据)) export TENSORFLOW_SHA=d250676d7776cfbca38e8690b75e1376afecf58d wget nc https://github.com/tensorflow/tensorflow/archive/$TENSORFLOW_SHA.tar.gz tar zxf $TENSORFLOW_SHA.tar.gz rm f $TENSORFLOW_SHA.tar.gz mv tensorflow$TENSORFLOW_SHA tensorflow cd tensorflow export BAZEL_VERSION=`cat .bazelversion` sudo mkdir p /usr/local/lib/bazel/bin sudo wget nc P /usr/local/lib/bazel/bin https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel$BAZEL_VERSIONlinuxx86_64 sudo chmod +x /usr/local/lib/bazel/bin/bazel$BAZEL_VERSIONlinuxx86_64 rm rf ~/.venv310 python3.10 m venv ~/.venv310 . ~/.venv310/bin/activate pip install U pip pip install U wheel pip install U packaging pip install U keras_preprocessing nodeps pip install ""jax[tpu]==0.3.14"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html yes '' | python configure.py   use the default settings bazel build config=tpu //tensorflow/tools/pip_package:build_pip_package ./bazelbin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg ls /tmp/tensorflow_pkg/tensorflow*.whl ``` However. I got this error: ``` 20220704 21:11:18.248361: F ./tensorflow/core/tpu/tpu_library_init_fns.inc:34] TpuEmbeddingEngineState_Create not available in this library. ``` I know that this error occurs when the libtpu version is not compatible with the Tensorflow version. Previously I always fixed it by recompiling Tensorflow from source. However, this time the error occurred when I was compiling Tensorflow from source, so I do not know what to do.  ",2022-07-04T13:48:23Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/11362,"What are you trying to do with the profiler? You may not need TensorFlow installed to accomplish what you want to do and the `Can't import tensorflow.python.profiler.trace` error may be misleading. (Note that I was able to get rid of the error by just running `pip install tensorflow`). I tried using `jax.profiler.trace` in a fresh Cloud TPU VM w/o TensorFlow installed and it worked fine. If you're using `tensorboard` or `python m jax.collect_profile`, you'll need TF installed.","> If you're using `tensorboard` or `python m jax.collect_profile`, you'll need TF installed. Yes, I am using `tensorboard`. > Note that I was able to get rid of the error by just running `pip install tensorflow` I've just tried and this is true, but I still want to install the appropriate TPU version of Tensorflow.","If you just want to get the profiler to work, you only need the TensorFlow from `pip` along with `tbpnightly`. I was able to capture a trace using the instructions here: https://jax.readthedocs.io/en/latest/profiling.htmlinstallation. Do you need TPUcompatible TensorFlow for something else?",I compiled a CPU version of Tensorflow and it works. For my other Tensorflow projects I will create a different virtual environment., do we expect TF TPU and JAX TPU to play nicely here?,"Not really. The only supported way to use TF in a Cloud TPU VM is to create a VM with a TF image (https://cloud.google.com/tpu/docs/runcalculationtensorflowcreate_a_vm_or_node_with_gcloud), although you can build from source if you know what you're doing like  apparently does :slightly_smiling_face: , and even with this we don't support running TPUenabled TF and JAX in the same process. So using regular CPUonly pipinstalled TF is the best way to go for profiling, like Sharad does above."
9075,"以下是一个github上的jax下的一个issue, 标题是(using `pmap` causing slowly gpu memory increases and eventually `OOM` in the end )， 内容是 (Dear team: I am trying to use `pmap` to train a simple CNN network across multiple GPUs, and, as title says, as training goes on, the gpu memory consumption is slowly increasing and finally causing `OOM` issue.  Let me use the following screen shot to better explain the issues. We use four 3090 GPUs for demonstration. As we just start training, we can see from the below screen shot that `GPU0` is using 23379MB memory and `GPU1``GPU3` are using 23355MB memory.  !Screen Shot 20220703 at 9 02 29 PM After we train for couple of steps, we can see from the below screen shot that`GPU0` are using 23419MB memory and `GPU1``GPU3` are using 23395MB memory. Almost 40MB memory increase on each GPU.  !Screen Shot 20220703 at 9 05 44 PM In the end, we shall reach the `OOM` situation... The following code is a minimal working example that could reproduce the error. I am using four 24GB 3090 GPUs.  ``` import os, sys import time from functools import reduce, partial from collections import deque from typing import * import numpy as np import jax import jax.numpy as jnp from jaxlib import xla_client as xc import optax import flax from flax.training import train_state, checkpoints from flax import linen as nn class SimpleConvLayer(nn.Module):     feature: int = 4     kernel: Tuple[int] = (3, 3, 3)     stride: int = 1     def setup(self):         self.conv = nn.Conv(self.feature, self.kernel, self.stride)         self.norm = nn.GroupNorm(num_groups=None, group_size=4)     def __call__(self, x):         x = self.conv(x)         x = self.norm(x)         x = nn.relu(x)         return x class YOLOV1(nn.Module):     .compact     def __call__(self, x):         x = SimpleConvLayer(feature=64, kernel=(7, 7), stride=2)(x)         x = nn.max_pool(x, (2, 2), (2, 2))         x = SimpleConvLayer(feature=192, kernel=(3, 3))(x)         x = nn.max_pool(x, (2, 2), (2, 2))         x = SimpleConvLayer(feature=128, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=256, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = nn.max_pool(x, (2, 2), (2, 2))         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=512, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = nn.max_pool(x, (2, 2), (2, 2))         x = SimpleConvLayer(feature=512, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=512, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3), stride=2)(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = jnp.reshape(x, (1, x.shape[1] * x.shape[2] * x.shape[3]))         x = nn.Dense(4096)(x)         x = nn.relu(x)         x = nn.Dense(4410)(x)         x = nn.sigmoid(x)         x = jnp.reshape(x, (1, 7, 7, 90))         return x def get_create_train_state(module: flax.linen.Module, devices=None):     def create_train_state(key, input_shape, lr=0.001):          1. Model instance         model = module()          2. Initialize the parameters of the model         params = model.init(key, jnp.ones(input_shape))['params']          3. Define the optimizer with the desired learning rate         optimizer = optax.adam(learning_rate=lr)         return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)     return jax.pmap(create_train_state, axis_name=""devices"", in_axes=0, out_axes=0, static_broadcasted_argnums=(1, 2),                     devices=devices) def get_calculate_loss(module: nn.Module,                        parallel_devices: Optional[List[xc.Device]] = None,                        loss_device: xc.Device = jax.devices('cpu')[1]):     def calculate_loss(params, xx, yy):          test whether there are some device placement requirement         def forward_pass(params, input_data):             prediction = module().apply({""params"": params}, input_data)              print(f'* prediction shape: {prediction.shape}')             return prediction         pmap_func = jax.pmap(             forward_pass,             static_broadcasted_argnums=(),   broadcast the paramss             axis_name=""device"",             in_axes=0,             out_axes=0,             devices=parallel_devices,             donate_argnums=()         )          be aware that all outputs are sharded array, then should be collect to a single device to calculate loss         prd_ = pmap_func(params, xx)         def forward_loss(lab, prd):             send_to_device = partial(jax.device_put, device=loss_device)             lab = jax.tree_map(send_to_device, lab)             prd = jax.tree_map(send_to_device, prd)              then remember to fetch the data to same device             loss = jnp.mean(jnp.square(lab  prd))             return loss          return jax.jit(forward_loss)(yy, prd_)         return forward_loss(yy, prd_)     return calculate_loss def get_apply_model(calculate_loss):     def apply_model(state, X, Y):         loss, grads = jax.value_and_grad(calculate_loss, has_aux=False)(state.params, X, Y)         return grads, loss      return jax.jit(apply_model)     return apply_model def get_update_model(devices: List[xc.Device]):     axis_name = ""devices""   a name for the axis 0     def update_model(state, grads):         grads = jax.lax.pmean(grads, axis_name=axis_name)         return state.apply_gradients(grads=grads)   apply two grads to states?     return jax.pmap(update_model, axis_name=axis_name, in_axes=0, out_axes=0, devices=devices) def try_get_device(device_type: str) > List[xc.Device]:     try:         return jax.devices(device_type)     except RuntimeError as re:         sys.stderr.write(""No device for device type: \""{}\""\n"".format(device_type))         return [] def fake_demostration() > None:     random_key = jax.random.PRNGKey(0)     random_key, spl = jax.random.split(random_key)     cpu_devices, gpu_devices = try_get_device(""cpu""), try_get_device(""gpu"")     forward_device = gpu_devices if gpu_devices else cpu_devices     weight_device = cpu_devices[1]     spl = jnp.array([spl])     spl = jnp.tile(spl, (len(forward_device), 1))     batch_size = 8     effective_batch_size = batch_size * len(forward_device)   effective batch size = batch_size * device count     epochs = 256     image_size = (448, 448, 1)   for update_model     grid_size = (7, 7, 90)     calculate_loss = get_calculate_loss(module=YOLOV1, parallel_devices=forward_device, loss_device=weight_device)     train_state_ = get_create_train_state(YOLOV1, devices=forward_device)(spl, (batch_size, *image_size), 1e4)     apply_model = get_apply_model(calculate_loss)     update_model = get_update_model(devices=forward_device)     loss_queue = deque(maxlen=10)     steps_per_epoch = 0     step_count = True     for epoch_num in range(epochs):         step_num = 0         while True:             dts = time.perf_counter_ns()             X0, X1 = np.random.randn(effective_batch_size, *image_size), np.random.randn(effective_batch_size, *grid_size)             dte = time.perf_counter_ns()             t0 = time.perf_counter_ns()             X0 = [jax.device_put(x) for x in X0.reshape((len(forward_device), batch_size, *image_size))]             X1 = [jax.device_put(x) for x in X1.reshape((len(forward_device), batch_size, *grid_size))]             X0 = jax.device_put_sharded(X0, devices=forward_device)             X1 = jax.device_put_sharded(X1, devices=forward_device)             grads, loss = apply_model(train_state_, X0, X1)             train_state_ = update_model(train_state_, grads)             loss_queue.append(loss)             t1 = time.perf_counter_ns()             step_num += 1             steps_per_epoch += 1 if step_count else 0             print(""[{3:4d}] {0:4d}/{1:4d} loss={2:.4e}, mean loss = {4:.4e}, data time = {5:.3e} s, ops time = {6:.3e} s"".format(                 step_num + 1,                 steps_per_epoch,                 loss,                 epoch_num + 1,                 sum(loss_queue) / len(loss_queue),                 (dte  dts) * 1e9,                 (t1  t0) * 1e9), end=""\r"")     sys.exit(0) if __name__ == ""__main__"":     fake_demostration() ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,using `pmap` causing slowly gpu memory increases and eventually `OOM` in the end ,"Dear team: I am trying to use `pmap` to train a simple CNN network across multiple GPUs, and, as title says, as training goes on, the gpu memory consumption is slowly increasing and finally causing `OOM` issue.  Let me use the following screen shot to better explain the issues. We use four 3090 GPUs for demonstration. As we just start training, we can see from the below screen shot that `GPU0` is using 23379MB memory and `GPU1``GPU3` are using 23355MB memory.  !Screen Shot 20220703 at 9 02 29 PM After we train for couple of steps, we can see from the below screen shot that`GPU0` are using 23419MB memory and `GPU1``GPU3` are using 23395MB memory. Almost 40MB memory increase on each GPU.  !Screen Shot 20220703 at 9 05 44 PM In the end, we shall reach the `OOM` situation... The following code is a minimal working example that could reproduce the error. I am using four 24GB 3090 GPUs.  ``` import os, sys import time from functools import reduce, partial from collections import deque from typing import * import numpy as np import jax import jax.numpy as jnp from jaxlib import xla_client as xc import optax import flax from flax.training import train_state, checkpoints from flax import linen as nn class SimpleConvLayer(nn.Module):     feature: int = 4     kernel: Tuple[int] = (3, 3, 3)     stride: int = 1     def setup(self):         self.conv = nn.Conv(self.feature, self.kernel, self.stride)         self.norm = nn.GroupNorm(num_groups=None, group_size=4)     def __call__(self, x):         x = self.conv(x)         x = self.norm(x)         x = nn.relu(x)         return x class YOLOV1(nn.Module):     .compact     def __call__(self, x):         x = SimpleConvLayer(feature=64, kernel=(7, 7), stride=2)(x)         x = nn.max_pool(x, (2, 2), (2, 2))         x = SimpleConvLayer(feature=192, kernel=(3, 3))(x)         x = nn.max_pool(x, (2, 2), (2, 2))         x = SimpleConvLayer(feature=128, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=256, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = nn.max_pool(x, (2, 2), (2, 2))         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=256, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=512, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=512, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = nn.max_pool(x, (2, 2), (2, 2))         x = SimpleConvLayer(feature=512, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=512, kernel=(1, 1))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3), stride=2)(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = SimpleConvLayer(feature=1024, kernel=(3, 3))(x)         x = jnp.reshape(x, (1, x.shape[1] * x.shape[2] * x.shape[3]))         x = nn.Dense(4096)(x)         x = nn.relu(x)         x = nn.Dense(4410)(x)         x = nn.sigmoid(x)         x = jnp.reshape(x, (1, 7, 7, 90))         return x def get_create_train_state(module: flax.linen.Module, devices=None):     def create_train_state(key, input_shape, lr=0.001):          1. Model instance         model = module()          2. Initialize the parameters of the model         params = model.init(key, jnp.ones(input_shape))['params']          3. Define the optimizer with the desired learning rate         optimizer = optax.adam(learning_rate=lr)         return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)     return jax.pmap(create_train_state, axis_name=""devices"", in_axes=0, out_axes=0, static_broadcasted_argnums=(1, 2),                     devices=devices) def get_calculate_loss(module: nn.Module,                        parallel_devices: Optional[List[xc.Device]] = None,                        loss_device: xc.Device = jax.devices('cpu')[1]):     def calculate_loss(params, xx, yy):          test whether there are some device placement requirement         def forward_pass(params, input_data):             prediction = module().apply({""params"": params}, input_data)              print(f'* prediction shape: {prediction.shape}')             return prediction         pmap_func = jax.pmap(             forward_pass,             static_broadcasted_argnums=(),   broadcast the paramss             axis_name=""device"",             in_axes=0,             out_axes=0,             devices=parallel_devices,             donate_argnums=()         )          be aware that all outputs are sharded array, then should be collect to a single device to calculate loss         prd_ = pmap_func(params, xx)         def forward_loss(lab, prd):             send_to_device = partial(jax.device_put, device=loss_device)             lab = jax.tree_map(send_to_device, lab)             prd = jax.tree_map(send_to_device, prd)              then remember to fetch the data to same device             loss = jnp.mean(jnp.square(lab  prd))             return loss          return jax.jit(forward_loss)(yy, prd_)         return forward_loss(yy, prd_)     return calculate_loss def get_apply_model(calculate_loss):     def apply_model(state, X, Y):         loss, grads = jax.value_and_grad(calculate_loss, has_aux=False)(state.params, X, Y)         return grads, loss      return jax.jit(apply_model)     return apply_model def get_update_model(devices: List[xc.Device]):     axis_name = ""devices""   a name for the axis 0     def update_model(state, grads):         grads = jax.lax.pmean(grads, axis_name=axis_name)         return state.apply_gradients(grads=grads)   apply two grads to states?     return jax.pmap(update_model, axis_name=axis_name, in_axes=0, out_axes=0, devices=devices) def try_get_device(device_type: str) > List[xc.Device]:     try:         return jax.devices(device_type)     except RuntimeError as re:         sys.stderr.write(""No device for device type: \""{}\""\n"".format(device_type))         return [] def fake_demostration() > None:     random_key = jax.random.PRNGKey(0)     random_key, spl = jax.random.split(random_key)     cpu_devices, gpu_devices = try_get_device(""cpu""), try_get_device(""gpu"")     forward_device = gpu_devices if gpu_devices else cpu_devices     weight_device = cpu_devices[1]     spl = jnp.array([spl])     spl = jnp.tile(spl, (len(forward_device), 1))     batch_size = 8     effective_batch_size = batch_size * len(forward_device)   effective batch size = batch_size * device count     epochs = 256     image_size = (448, 448, 1)   for update_model     grid_size = (7, 7, 90)     calculate_loss = get_calculate_loss(module=YOLOV1, parallel_devices=forward_device, loss_device=weight_device)     train_state_ = get_create_train_state(YOLOV1, devices=forward_device)(spl, (batch_size, *image_size), 1e4)     apply_model = get_apply_model(calculate_loss)     update_model = get_update_model(devices=forward_device)     loss_queue = deque(maxlen=10)     steps_per_epoch = 0     step_count = True     for epoch_num in range(epochs):         step_num = 0         while True:             dts = time.perf_counter_ns()             X0, X1 = np.random.randn(effective_batch_size, *image_size), np.random.randn(effective_batch_size, *grid_size)             dte = time.perf_counter_ns()             t0 = time.perf_counter_ns()             X0 = [jax.device_put(x) for x in X0.reshape((len(forward_device), batch_size, *image_size))]             X1 = [jax.device_put(x) for x in X1.reshape((len(forward_device), batch_size, *grid_size))]             X0 = jax.device_put_sharded(X0, devices=forward_device)             X1 = jax.device_put_sharded(X1, devices=forward_device)             grads, loss = apply_model(train_state_, X0, X1)             train_state_ = update_model(train_state_, grads)             loss_queue.append(loss)             t1 = time.perf_counter_ns()             step_num += 1             steps_per_epoch += 1 if step_count else 0             print(""[{3:4d}] {0:4d}/{1:4d} loss={2:.4e}, mean loss = {4:.4e}, data time = {5:.3e} s, ops time = {6:.3e} s"".format(                 step_num + 1,                 steps_per_epoch,                 loss,                 epoch_num + 1,                 sum(loss_queue) / len(loss_queue),                 (dte  dts) * 1e9,                 (t1  t0) * 1e9), end=""\r"")     sys.exit(0) if __name__ == ""__main__"":     fake_demostration() ```",2022-07-03T13:09:41Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/11357,A couple of quick questions: * what `jax` and `jaxlib` versions are you using? * have you seen this on any other GPUs?," hi, I am using `jax==0.3.14` and `jaxlib==0.3.14`. I only tested this on 2080 and 3090 GPU, both of them experiencing the same issue","Thanks for the report. This is due to repeated compilation inside calculate_loss() (if you lift pmap_func outside, the leak will go away and is probably what you want). Also, as a side note, you can just pmean your loss and then pmap the entire train step (rather than running value_and_grad over the forward pmap). It has certainly been interesting trying to track down the leak in the repeated compilation process, but no luck yet."
305,"以下是一个github上的jax下的一个issue, 标题是(Trouble using JAX with openCV)， 内容是 (I am trying to use JAX for realtime object detection, but the input is not supported by open CV.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Trouble using JAX with openCV,"I am trying to use JAX for realtime object detection, but the input is not supported by open CV.",2022-07-02T12:41:12Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/11351,Could you please give a repro of an error?  JAX arrays should behave like NumPy arrays in most circumstances and I believe OpenCV in Python is compatible w/ NumPy arrays.,"See I am using YoloV4 model and opencv to detect objects and in input it's taking my live feed by accessing camera through opencv and javascript , the thing is you pass an array in js through opencv right in that you use numpy array ,and in here i wanna use JAX instead of numpy",Instead of this: ```python my_opencv_function(jax_array) ``` You might try this: ```python import numpy as np np_array = np.asarray(jax_array) my_opencv_function(np_array) ```,Closing absent a reproduction. But I strongly suspect that Jake's answer is the correct one...
553,"以下是一个github上的jax下的一个issue, 标题是(Symlink xla_client and xla_extension into jaxlib rather than copying them into place in the wheel build.)， 内容是 (Symlink xla_client and xla_extension into jaxlib rather than copying them into place in the wheel build. Change in preparation for allowing JAX tests to run under Bazel. Remove code to patch paths in xla_client.py in the wheel build script; the patch is no longer used.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Symlink xla_client and xla_extension into jaxlib rather than copying them into place in the wheel build.,Symlink xla_client and xla_extension into jaxlib rather than copying them into place in the wheel build. Change in preparation for allowing JAX tests to run under Bazel. Remove code to patch paths in xla_client.py in the wheel build script; the patch is no longer used.,2022-07-01T18:26:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/11339
588,"以下是一个github上的jax下的一个issue, 标题是(Use symlink_files() to add version.py to jaxlib, rather than copying it in as part of the wheel assembly process.)， 内容是 (Use symlink_files() to add version.py to jaxlib, rather than copying it in as part of the wheel assembly process. Change in preparation for supporting running JAX tests under Bazel. This change allows the Bazel py_library() to see version.py. Update symlink_files Bazel macro to a newer version.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Use symlink_files() to add version.py to jaxlib, rather than copying it in as part of the wheel assembly process.","Use symlink_files() to add version.py to jaxlib, rather than copying it in as part of the wheel assembly process. Change in preparation for supporting running JAX tests under Bazel. This change allows the Bazel py_library() to see version.py. Update symlink_files Bazel macro to a newer version.",2022-07-01T14:39:02Z,,closed,0,0,https://github.com/jax-ml/jax/issues/11337
294,"以下是一个github上的jax下的一个issue, 标题是(Add start and end logging for commit to the storage layer)， 内容是 (Add start and end logging for commit to the storage layer)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add start and end logging for commit to the storage layer,Add start and end logging for commit to the storage layer,2022-06-30T23:04:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/11327
3703,"以下是一个github上的jax下的一个issue, 标题是(Functionality to chunk `vmap`.)， 内容是 (Occasionally I run into the problem that batchsizes are too large for GPU memory and using the current public API of Jax I either have to commit to one of the extremes of using `vmap` or using the much more limited `scan`. (I'm still relatively new to Jax, so please correct me if I'm wrong). Could there perhaps be a `chunk` argument to the `vmap` function that limits how much is passed to the `vmap`ped function at any time? I tried implementing this, and have the following mockup. At the moment this is based on a `num_chunks` parameter, but this could be more flexible such as in terms of `chunk_size`. It splits or repeats the inputs of the `vmap`ped function based on the given `in_axes`, pads all input arrays uniformly with zeroes along the batch dimension, and finally forloop the chunks through a conventional `vmap` function. The results are finally concatenated based on their canonical slices. I don't have all the functionalities of the normal `vmap` implemented, but it works in a slightly more narrow scope. Could something like this be supported by the public Jax API? ```python def pad_along_axis(array: jnp.ndarray, axis_length: int, axis: int = 0, *args, **kwargs) > jnp.ndarray:     target_size = axis_length  jnp.shape(array)[axis]     padding = [(0, 0)] * jnp.ndim(array)     padding[axis] = (0, target_size)     return jnp.pad(array, padding, *args, **kwargs) def chunked_vmap(fun, num_chunks: int = 1, in_axes=0, out_axes=0, axis_name=None, axis_size=None):      TODO: Compatibility on flattened in_axes. Implementation for out_axes, axis_name, axis_size.      Note, num_chunks == 1 is equivalent to just using `vmap_fun`.     vmap_fun = jax.vmap(fun, in_axes, out_axes, axis_name, axis_size)      Leaf structure of input splitting: ([chunk_a, chunk_b, ...], [pad_a, pad_b, ...])      splitted_treedef = jax.tree_structure(([1] * num_chunks,) * 2)     def split_fun(arg, ax):            Operates on pytree leaves.         if ax is None:             return [arg] * num_chunks, [0] * num_chunks         chunks = jnp.array_split(arg, num_chunks, axis=ax)         leading_size = jnp.shape(chunks[0])[ax]         batch_dims = jax.tree_map(lambda a: jnp.shape(a)[ax], chunks)         padded_chunks = jax.tree_map(partial(pad_along_axis, axis_length=leading_size, axis=ax), chunks)         return padded_chunks, batch_dims     def vmap_f(*args, **kwargs):   TODO: Incorporate kwargs?         splitted = jax.tree_map(split_fun, args, in_axes)         input_chunks, canonical_sizes = jax.tree_transpose(             jax.tree_structure(args), splitted_treedef, splitted         )         out_sizes = [max(jax.tree_leaves(s)) for s in canonical_sizes]          TODO: use jax.lax.scan? Note the dynamic shapes of jax.lax.slice and that in_axes is not yet supported.         results = [jax.lax.slice(vmap_fun(*c), (0, ), (s,)) for c, s in zip(input_chunks, out_sizes)]          TODO: collect all outputs immediately, or use a generator with `yield`?         out = jax.tree_map(lambda *a: jnp.concatenate(a), *results)         return out     return vmap_f ``` ```python def myfun(a, b, c):     return jnp.square(a) * c + jnp.squeeze(b['val']) v = jnp.arange(100) args = (v, {'val': v.reshape(1, 1, 1, 100)}, 0.4) in_axes = (0, {'val': 1}, None) vmap_fun = jax.vmap(myfun, in_axes=in_axes) out = vmap_fun(*args) for chunk_size in [1, 2, 5, 10, 50]:     chunk_out = chunked_vmap(myfun, chunk_size, in_axes=in_axes)(*args)     assert jnp.isclose(out, chunk_out).all()   Runs fine ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Functionality to chunk `vmap`.,"Occasionally I run into the problem that batchsizes are too large for GPU memory and using the current public API of Jax I either have to commit to one of the extremes of using `vmap` or using the much more limited `scan`. (I'm still relatively new to Jax, so please correct me if I'm wrong). Could there perhaps be a `chunk` argument to the `vmap` function that limits how much is passed to the `vmap`ped function at any time? I tried implementing this, and have the following mockup. At the moment this is based on a `num_chunks` parameter, but this could be more flexible such as in terms of `chunk_size`. It splits or repeats the inputs of the `vmap`ped function based on the given `in_axes`, pads all input arrays uniformly with zeroes along the batch dimension, and finally forloop the chunks through a conventional `vmap` function. The results are finally concatenated based on their canonical slices. I don't have all the functionalities of the normal `vmap` implemented, but it works in a slightly more narrow scope. Could something like this be supported by the public Jax API? ```python def pad_along_axis(array: jnp.ndarray, axis_length: int, axis: int = 0, *args, **kwargs) > jnp.ndarray:     target_size = axis_length  jnp.shape(array)[axis]     padding = [(0, 0)] * jnp.ndim(array)     padding[axis] = (0, target_size)     return jnp.pad(array, padding, *args, **kwargs) def chunked_vmap(fun, num_chunks: int = 1, in_axes=0, out_axes=0, axis_name=None, axis_size=None):      TODO: Compatibility on flattened in_axes. Implementation for out_axes, axis_name, axis_size.      Note, num_chunks == 1 is equivalent to just using `vmap_fun`.     vmap_fun = jax.vmap(fun, in_axes, out_axes, axis_name, axis_size)      Leaf structure of input splitting: ([chunk_a, chunk_b, ...], [pad_a, pad_b, ...])      splitted_treedef = jax.tree_structure(([1] * num_chunks,) * 2)     def split_fun(arg, ax):            Operates on pytree leaves.         if ax is None:             return [arg] * num_chunks, [0] * num_chunks         chunks = jnp.array_split(arg, num_chunks, axis=ax)         leading_size = jnp.shape(chunks[0])[ax]         batch_dims = jax.tree_map(lambda a: jnp.shape(a)[ax], chunks)         padded_chunks = jax.tree_map(partial(pad_along_axis, axis_length=leading_size, axis=ax), chunks)         return padded_chunks, batch_dims     def vmap_f(*args, **kwargs):   TODO: Incorporate kwargs?         splitted = jax.tree_map(split_fun, args, in_axes)         input_chunks, canonical_sizes = jax.tree_transpose(             jax.tree_structure(args), splitted_treedef, splitted         )         out_sizes = [max(jax.tree_leaves(s)) for s in canonical_sizes]          TODO: use jax.lax.scan? Note the dynamic shapes of jax.lax.slice and that in_axes is not yet supported.         results = [jax.lax.slice(vmap_fun(*c), (0, ), (s,)) for c, s in zip(input_chunks, out_sizes)]          TODO: collect all outputs immediately, or use a generator with `yield`?         out = jax.tree_map(lambda *a: jnp.concatenate(a), *results)         return out     return vmap_f ``` ```python def myfun(a, b, c):     return jnp.square(a) * c + jnp.squeeze(b['val']) v = jnp.arange(100) args = (v, {'val': v.reshape(1, 1, 1, 100)}, 0.4) in_axes = (0, {'val': 1}, None) vmap_fun = jax.vmap(myfun, in_axes=in_axes) out = vmap_fun(*args) for chunk_size in [1, 2, 5, 10, 50]:     chunk_out = chunked_vmap(myfun, chunk_size, in_axes=in_axes)(*args)     assert jnp.isclose(out, chunk_out).all()   Runs fine ```",2022-06-30T09:27:33Z,enhancement,open,14,13,https://github.com/jax-ml/jax/issues/11319,"Take a look at `jax.experimental.maps.xmap`, which is designed for exactly this sort of thing: https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.maps.xmap.html You can chunk vmap by making use of `SerialLoop`.",This here might be something you could use: https://netket.readthedocs.io/en/stable/api/_generated/jax/netket.jax.vmap_chunked.htmlnetket.jax.vmap_chunked,> This here might be something you could use: https://netket.readthedocs.io/en/stable/api/_generated/jax/netket.jax.vmap_chunked.htmlnetket.jax.vmap_chunked Thanks that is an excellent reference. I am of the opinion though that Jax could implement this by default given how generally useful this is. ,"Hi, more than a year later ;P, are there any plans on implementing this by default in Jax? Or would the authors be open for a PR?", we were just talking about this :),"If you do get to it, I would love if you also add support for `vjp` over 'chunked' axes like we did in https://netket.readthedocs.io/en/stable/api/_generated/jax/netket.jax.vjp_chunked.html ","+1 for adding this feature, it would be super useful in several projects for me.",Discussion https://github.com/google/jax/discussions/18398 asks for this as well.,+1 upvote for implementing this,"It would be nice if JAX's compiler could automatically convert computations from parallel to sequential when necessary, given a known memory constraint. Has this possibility been discussed anywhere?",`jax.lax.map` now has a `batch_size` argument that will chuck the computation and internally utilize `vmap` to operate over each batch in parallel. See CC(add batch_size argument to jax.lax.map). ,"I tried using JAX as an alternative to numba, since it is much easier to work in, and I can switch between CPU and GPU using a flag. The biggest problem I am facing now, is that `vmap` tries to put all of the data onto the GPU at the same time. I am already using vmap to apply the scalar function onto a 2D array, so I was wondering if and how `map` could help me to batch the data, so I am not getting of memory errors!","Try the `batch_size` argument to `jax.lax.map`? (Added in CC(add batch_size argument to jax.lax.map).) Think of it as a sequential loop over batches, where each batch is vmapped."
796,"以下是一个github上的jax下的一个issue, 标题是(Random stop)， 内容是 (When I run `jaxmain/examples/mnist_classifier.py`,It Random stop in angwhere.  Not finish.  No any error report. 运行环境 Operating environment：wsl2+unbuntu18.04+cuda11.5+python3.8.8+vscode ``` print(""\nStarting training..."")   for epoch in range(num_epochs):     start_time = time.time()     print(epoch)     for _ in range(num_batches):       print("""",end=""."")       opt_state = update(next(itercount), opt_state, next(batches))     epoch_time = time.time()  start_time ``` Stop in `opt_state = update(next(itercount), opt_state, next(batches))` !1656555511802(1) And GPU memery can not free: !1656555572095)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Random stop,"When I run `jaxmain/examples/mnist_classifier.py`,It Random stop in angwhere.  Not finish.  No any error report. 运行环境 Operating environment：wsl2+unbuntu18.04+cuda11.5+python3.8.8+vscode ``` print(""\nStarting training..."")   for epoch in range(num_epochs):     start_time = time.time()     print(epoch)     for _ in range(num_batches):       print("""",end=""."")       opt_state = update(next(itercount), opt_state, next(batches))     epoch_time = time.time()  start_time ``` Stop in `opt_state = update(next(itercount), opt_state, next(batches))` !1656555511802(1) And GPU memery can not free: !1656555572095",2022-06-30T02:21:23Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/11316,"I found I needed:`jax.device_ put(batch)` The problem disappeared, but I don't know why. ``` print(""\nStarting training..."")   for epoch in range(num_epochs):     start_time = time.time()     print(epoch)     for _ in range(num_batches):       print("""",end=""."")       batch=next(batches)       jax.device_put(batch)  the key point       opt_state = update(next(itercount), opt_state, batch)       print("""",end=""*"")     epoch_time = time.time()  start_time ```"
9729,"以下是一个github上的jax下的一个issue, 标题是(OOM when jax.jit is used to compile and execute mu2Net train_step function)， 内容是 ( Duplicate issues:  CC(cuda failed to allocate errors) and  CC(GPU memory allocation issues) are not suitable for this case.   How to reproduce the bug: [1] Just run mu2Net on 8gpus A100, use BENCHMARK = 'ViT large / Chars benchmark' [2] OOM error will occur when train_step function compiled by jax.jit is executed. [3] The A100 have sufficent 80GiB memory.  Core code: ```python (jax.jit, static_argnames=['model', 'optimizer'], donate_argnums=[0, 2]) def train_step(params, fixed_params, opt_state, images, labels, model, optimizer):   def loss_fn(params, fixed_params, images, labels):     logits = model.apply({'params': format_params(params, fixed_params)},                          images, train=USE_DROPOUT)     labels = jax.nn.one_hot(labels, logits.shape[1])     return jnp.mean(jnp.sum(labels * nn.log_softmax(logits), axis=1))   grads = jax.grad(loss_fn)(params, fixed_params, images, labels)   updates, opt_state = optimizer.update(grads, opt_state, params=params)   params = optax.apply_updates(params, updates)   return params, opt_state def train_loop(paths, ds_train, ds_validation, devices, exp_config):   global LOOP_START   global TRAIN_PARAMS_TRANSFER   global TRAIN   timing = {'start_time': time.time(),             'start_time_loop': LOOP_START}   task = paths[0].task    The following values should be shared by all paths in this generation batch.   for path in paths:     assert task == path.task     assert paths[0].hparams['ds_image_size'] == path.hparams['ds_image_size']   gc.collect()    Compile.   compile_train_batches_arr = jax.device_put_replicated(       get_sample_batch(         paths[0].hparams['ds_image_size'],         task.train_batch_size),       devices)   compile_eval_batches_arr = jax.device_put_replicated(       get_sample_batch(           paths[0].hparams['ds_image_size'],           task.validation_batch_size),       devices)   for p_id, path in enumerate(paths):     if VERBOSE:       print('Parent')       print(prp(path.parent))       print(prp(path))     path.device_id = p_id % len(devices)     path.device = devices[path.device_id]     print(""path:"", p_id, ""device:"", path.device)     path.optimizer = path.get_optimizer()     path.optimizer_init_fn = jax.jit(path.optimizer.init, device=path.device)     path.best_params_local = None     path.best_opt_state_local = None     path.best_quality = None     path.best_score = path.parent.score() if path.task is path.parent.task else np.inf     path.evals = []      Launch parallel compilation of eval and train step functions.     params_local = path.get_trainable_params()     check_is_local(params_local)     path.compile_params_device = jax.device_put(params_local, path.device)     path.compile_fixed_params_device = jax.device_put(         path.get_fixed_params(),         path.device)     path.compile_train = Thread(         target=train_step,         args=(path.compile_params_device,               path.compile_fixed_params_device,               path.optimizer_init_fn(params_local),               compile_train_batches_arr['image'][path.device_id],               compile_train_batches_arr['label'][path.device_id],               path.model,               path.optimizer))     path.compile_eval = Thread(         target=eval_step,         args=(format_params(                   path.compile_params_device,                   path.compile_fixed_params_device),               compile_eval_batches_arr['image'][path.device_id],               compile_eval_batches_arr['label'][path.device_id],               path.model))     path.compile_eval.start()   for path in paths:     path.compile_eval.join()     del path.compile_eval     timing['end_compile_eval'] = time.time()     path.compile_train.start()   del compile_eval_batches_arr   for path in paths:     path.compile_train.join()     del path.compile_train     del path.compile_params_device     del path.compile_fixed_params_device     timing['end_compile'] = time.time()   del compile_train_batches_arr   gc.collect()    Parameter transfer.   transfer_start = time.time()   for path in paths:     path.params_device = jax.device_put(         path.get_trainable_params(),         path.device)     path.fixed_params_device = jax.device_put(         path.get_fixed_params(),         path.device)     path.opt_state_device = path.optimizer_init_fn(path.params_device)      Set opt state.     for c in path.components:       if c.is_trainable():         assert c.name in path.opt_state_device[1][0].trace.keys()         if c.opt_state is not None:           path.opt_state_device = (               path.opt_state_device[0],               (optax.TraceState(                   trace=path.opt_state_device[1][0].trace.copy(                       {c.name: jax.device_put(c.opt_state,                                               path.device)})),                path.opt_state_device[1][1]                )           )     check_is_on_device(path.opt_state_device, path.device)   TRAIN_PARAMS_TRANSFER += time.time()  transfer_start   iter_ds_validation = iter(ds_validation)    TRAIN   for t_step, train_batch in zip(       range(exp_config.num_validations_per_path_training             * task.num_train_batches_between_validations),       ds_train,   ):     replicated_start = time.time()     train_batch_arr = jax.device_put_replicated(train_batch, devices)     TRAIN_PARAMS_TRANSFER += time.time()  replicated_start     for p_id, path in enumerate(paths):       if t_step == 0:         timing['end_prep'] = time.time()         t_step_0_time = time.time()       train_step_start = time.time()       path.params_device, path.opt_state_device = train_step(           path.params_device,           path.fixed_params_device,           path.opt_state_device,           train_batch_arr['image'][path.device_id],           train_batch_arr['label'][path.device_id],           path.model,           path.optimizer)       TRAIN += time.time()  train_step_start       if t_step == 0 and time.time()  t_step_0_time > 1:         print(f'WARNING: First train step took: {time.time()t_step_0_time:.2f} s')     del train_batch, train_batch_arr      EVAL      ... ```  Full error messages/tracebacks: ```bash Exception in thread Thread14: Traceback (most recent call last):   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py"", line 890, in _bootstrap     self._bootstrap_inner()   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py"", line 926, in _bootstrap_inner     self.run()   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py"", line 870, in run     self._target(*self._args, **self._kwargs)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/_src/api.py"", line 476, in cache_miss     donated_invars=donated_invars, inline=inline, keep_unused=keep_unused)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/core.py"", line 1765, in bind     return call_bind(self, fun, *args, **params)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/core.py"", line 1781, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/core.py"", line 678, in process_call     return primitive.impl(f, *tracers, **params)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 185, in _xla_call_impl     return compiled_fun(*args)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 615, in _execute_compiled     out_bufs_flat = compiled.execute(input_bufs_flat) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 122875791936 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    1.43GiB               constant allocation:         8B         maybe_live_out allocation:  581.19MiB      preallocated temp allocation:  114.44GiB   preallocated temp fragmentation:  146.50MiB (0.13%)                  total allocation:  115.86GiB               total fragmentation:  146.53MiB (0.12%) Peak buffers:         Buffer 1:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 2:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 3:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 4:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 5:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 6:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ========================== ```  environment: python: 3.7.13 jax: 0.3.14 jaxlib: 0.3.14+cuda11.cudnn82)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,OOM when jax.jit is used to compile and execute mu2Net train_step function," Duplicate issues:  CC(cuda failed to allocate errors) and  CC(GPU memory allocation issues) are not suitable for this case.   How to reproduce the bug: [1] Just run mu2Net on 8gpus A100, use BENCHMARK = 'ViT large / Chars benchmark' [2] OOM error will occur when train_step function compiled by jax.jit is executed. [3] The A100 have sufficent 80GiB memory.  Core code: ```python (jax.jit, static_argnames=['model', 'optimizer'], donate_argnums=[0, 2]) def train_step(params, fixed_params, opt_state, images, labels, model, optimizer):   def loss_fn(params, fixed_params, images, labels):     logits = model.apply({'params': format_params(params, fixed_params)},                          images, train=USE_DROPOUT)     labels = jax.nn.one_hot(labels, logits.shape[1])     return jnp.mean(jnp.sum(labels * nn.log_softmax(logits), axis=1))   grads = jax.grad(loss_fn)(params, fixed_params, images, labels)   updates, opt_state = optimizer.update(grads, opt_state, params=params)   params = optax.apply_updates(params, updates)   return params, opt_state def train_loop(paths, ds_train, ds_validation, devices, exp_config):   global LOOP_START   global TRAIN_PARAMS_TRANSFER   global TRAIN   timing = {'start_time': time.time(),             'start_time_loop': LOOP_START}   task = paths[0].task    The following values should be shared by all paths in this generation batch.   for path in paths:     assert task == path.task     assert paths[0].hparams['ds_image_size'] == path.hparams['ds_image_size']   gc.collect()    Compile.   compile_train_batches_arr = jax.device_put_replicated(       get_sample_batch(         paths[0].hparams['ds_image_size'],         task.train_batch_size),       devices)   compile_eval_batches_arr = jax.device_put_replicated(       get_sample_batch(           paths[0].hparams['ds_image_size'],           task.validation_batch_size),       devices)   for p_id, path in enumerate(paths):     if VERBOSE:       print('Parent')       print(prp(path.parent))       print(prp(path))     path.device_id = p_id % len(devices)     path.device = devices[path.device_id]     print(""path:"", p_id, ""device:"", path.device)     path.optimizer = path.get_optimizer()     path.optimizer_init_fn = jax.jit(path.optimizer.init, device=path.device)     path.best_params_local = None     path.best_opt_state_local = None     path.best_quality = None     path.best_score = path.parent.score() if path.task is path.parent.task else np.inf     path.evals = []      Launch parallel compilation of eval and train step functions.     params_local = path.get_trainable_params()     check_is_local(params_local)     path.compile_params_device = jax.device_put(params_local, path.device)     path.compile_fixed_params_device = jax.device_put(         path.get_fixed_params(),         path.device)     path.compile_train = Thread(         target=train_step,         args=(path.compile_params_device,               path.compile_fixed_params_device,               path.optimizer_init_fn(params_local),               compile_train_batches_arr['image'][path.device_id],               compile_train_batches_arr['label'][path.device_id],               path.model,               path.optimizer))     path.compile_eval = Thread(         target=eval_step,         args=(format_params(                   path.compile_params_device,                   path.compile_fixed_params_device),               compile_eval_batches_arr['image'][path.device_id],               compile_eval_batches_arr['label'][path.device_id],               path.model))     path.compile_eval.start()   for path in paths:     path.compile_eval.join()     del path.compile_eval     timing['end_compile_eval'] = time.time()     path.compile_train.start()   del compile_eval_batches_arr   for path in paths:     path.compile_train.join()     del path.compile_train     del path.compile_params_device     del path.compile_fixed_params_device     timing['end_compile'] = time.time()   del compile_train_batches_arr   gc.collect()    Parameter transfer.   transfer_start = time.time()   for path in paths:     path.params_device = jax.device_put(         path.get_trainable_params(),         path.device)     path.fixed_params_device = jax.device_put(         path.get_fixed_params(),         path.device)     path.opt_state_device = path.optimizer_init_fn(path.params_device)      Set opt state.     for c in path.components:       if c.is_trainable():         assert c.name in path.opt_state_device[1][0].trace.keys()         if c.opt_state is not None:           path.opt_state_device = (               path.opt_state_device[0],               (optax.TraceState(                   trace=path.opt_state_device[1][0].trace.copy(                       {c.name: jax.device_put(c.opt_state,                                               path.device)})),                path.opt_state_device[1][1]                )           )     check_is_on_device(path.opt_state_device, path.device)   TRAIN_PARAMS_TRANSFER += time.time()  transfer_start   iter_ds_validation = iter(ds_validation)    TRAIN   for t_step, train_batch in zip(       range(exp_config.num_validations_per_path_training             * task.num_train_batches_between_validations),       ds_train,   ):     replicated_start = time.time()     train_batch_arr = jax.device_put_replicated(train_batch, devices)     TRAIN_PARAMS_TRANSFER += time.time()  replicated_start     for p_id, path in enumerate(paths):       if t_step == 0:         timing['end_prep'] = time.time()         t_step_0_time = time.time()       train_step_start = time.time()       path.params_device, path.opt_state_device = train_step(           path.params_device,           path.fixed_params_device,           path.opt_state_device,           train_batch_arr['image'][path.device_id],           train_batch_arr['label'][path.device_id],           path.model,           path.optimizer)       TRAIN += time.time()  train_step_start       if t_step == 0 and time.time()  t_step_0_time > 1:         print(f'WARNING: First train step took: {time.time()t_step_0_time:.2f} s')     del train_batch, train_batch_arr      EVAL      ... ```  Full error messages/tracebacks: ```bash Exception in thread Thread14: Traceback (most recent call last):   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py"", line 890, in _bootstrap     self._bootstrap_inner()   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py"", line 926, in _bootstrap_inner     self.run()   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py"", line 870, in run     self._target(*self._args, **self._kwargs)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/_src/api.py"", line 476, in cache_miss     donated_invars=donated_invars, inline=inline, keep_unused=keep_unused)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/core.py"", line 1765, in bind     return call_bind(self, fun, *args, **params)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/core.py"", line 1781, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/core.py"", line 678, in process_call     return primitive.impl(f, *tracers, **params)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 185, in _xla_call_impl     return compiled_fun(*args)   File ""/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 615, in _execute_compiled     out_bufs_flat = compiled.execute(input_bufs_flat) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 122875791936 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:    1.43GiB               constant allocation:         8B         maybe_live_out allocation:  581.19MiB      preallocated temp allocation:  114.44GiB   preallocated temp fragmentation:  146.50MiB (0.13%)                  total allocation:  115.86GiB               total fragmentation:  146.53MiB (0.12%) Peak buffers:         Buffer 1:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 2:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 3:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 4:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 5:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ==========================         Buffer 6:                 Size: 1.27GiB                 XLA Label: customcall                 Shape: f32[64,16,577,577]                 ========================== ```  environment: python: 3.7.13 jax: 0.3.14 jaxlib: 0.3.14+cuda11.cudnn82",2022-06-29T09:30:02Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/11303,"It sounds to me like this is not a JAX bug: the model in question is simply running out of memory. It looks like the computation is attempting to allocate 122875791936 bytes (122GB), which is never going to fit on an 80GB A100. You'd need to either change your hyperparameters to fit or change how the model is parallelized across your GPUs. Both of these are things you would do in the model code. Hope that helps!"
2553,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Quantized model is blowing up the memory where JAX - Flax model was working perfectly)， 内容是 (I'm using a model for auto image enhancement `googleresearch/maxim` and it is working perfectly. So I was working with quantization of model and got the answer from the official sources on How to convert `JAX` model to `tflite` and I it worked.  Code for the `MAXIM` to quantize, I have answered my own question on stackoverflow05) **Code to quantize**: ``` import tensorflow as tf from jax.experimental import jax2tf def predict(input_img):   '''   Function to predict the output from the JAX model   '''   return model.apply({'params': flax.core.freeze(params)}, input_img) tf_predict = tf.function(     jax2tf.convert(predict, enable_xla=False),     input_signature=[         tf.TensorSpec(shape=[1, 704, 1024, 3], dtype=tf.float32, name='input_image')     ],     autograph=False) converter = tf.lite.TFLiteConverter.from_concrete_functions(     [tf_predict.get_concrete_function()], tf_predict) converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,   enable TensorFlow Lite ops.     tf.lite.OpsSet.SELECT_TF_OPS   enable TensorFlow ops. ] tflite_float_model = converter.convert() with open('float_model.tflite', ""wb"") as f: f.write(tflite_float_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_quantized_model = converter.convert() with open('./quantized.tflite', 'wb') as f: f.write(tflite_quantized_model) ``` **Problem**: Everything is fine, the model is being loaded and showing input and output shapes as: ``` tflite_interpreter_quant = tf.lite.Interpreter(model_path='./maxim/quantized.tflite') input_details = tflite_interpreter_quant.get_input_details() output_details = tflite_interpreter_quant.get_output_details() print(""== Input details =="") print(""name:"", input_details[0]['name']) print(""shape:"", input_details[0]['shape']) print(""type:"", input_details[0]['dtype']) print(""\n== Output details =="") print(""name:"", output_details[0]['name']) print(""shape:"", output_details[0]['shape']) print(""type:"", output_details[0]['dtype']) ``` **but when I do:** ``` tflite_interpreter_quant.allocate_tensors() ``` The memory runs out. This is super weird because my original `JAX` model was running quite fine but as soon as I try to allocate memory to the **QUANTIZED** version, I get this one. Any idea why this might be happening?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Quantized model is blowing up the memory where JAX - Flax model was working perfectly,"I'm using a model for auto image enhancement `googleresearch/maxim` and it is working perfectly. So I was working with quantization of model and got the answer from the official sources on How to convert `JAX` model to `tflite` and I it worked.  Code for the `MAXIM` to quantize, I have answered my own question on stackoverflow05) **Code to quantize**: ``` import tensorflow as tf from jax.experimental import jax2tf def predict(input_img):   '''   Function to predict the output from the JAX model   '''   return model.apply({'params': flax.core.freeze(params)}, input_img) tf_predict = tf.function(     jax2tf.convert(predict, enable_xla=False),     input_signature=[         tf.TensorSpec(shape=[1, 704, 1024, 3], dtype=tf.float32, name='input_image')     ],     autograph=False) converter = tf.lite.TFLiteConverter.from_concrete_functions(     [tf_predict.get_concrete_function()], tf_predict) converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,   enable TensorFlow Lite ops.     tf.lite.OpsSet.SELECT_TF_OPS   enable TensorFlow ops. ] tflite_float_model = converter.convert() with open('float_model.tflite', ""wb"") as f: f.write(tflite_float_model) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_quantized_model = converter.convert() with open('./quantized.tflite', 'wb') as f: f.write(tflite_quantized_model) ``` **Problem**: Everything is fine, the model is being loaded and showing input and output shapes as: ``` tflite_interpreter_quant = tf.lite.Interpreter(model_path='./maxim/quantized.tflite') input_details = tflite_interpreter_quant.get_input_details() output_details = tflite_interpreter_quant.get_output_details() print(""== Input details =="") print(""name:"", input_details[0]['name']) print(""shape:"", input_details[0]['shape']) print(""type:"", input_details[0]['dtype']) print(""\n== Output details =="") print(""name:"", output_details[0]['name']) print(""shape:"", output_details[0]['shape']) print(""type:"", output_details[0]['dtype']) ``` **but when I do:** ``` tflite_interpreter_quant.allocate_tensors() ``` The memory runs out. This is super weird because my original `JAX` model was running quite fine but as soon as I try to allocate memory to the **QUANTIZED** version, I get this one. Any idea why this might be happening?",2022-06-28T16:46:41Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/11291," this issue seems to be on the TFLite side, perhaps you could reroute it to someone from the TFLite team?","Hi,  , could you please share your pretrained maxim model? the colab you pointed (https://github.com/googleresearch/maxim/blob/main/colab_inference_demo.ipynb) seems broken","Never mind, I got the model. I could reproduce OOM in 12 GB RAM colab runtime environment. let me work on it more.",I've tested in the newly TF version 2.11.0 and the issues gone. could you please test it in the new version of TF in the colab?
1094,"以下是一个github上的jax下的一个issue, 标题是(Implement `jax.numpy.<fun>.accumulate`)， 内容是 (I was trying to run a cumulative minimum when I realized that this isn't part of the main `jax.numpy` API, while it is the case for `numpy`. For now I've managed simply swap it out with: ```python import jax import jax.numpy as jnp key = jax.random.PRNGKey(0) values = jax.random.normal(key, shape=(10,)) jax.lax.associative_scan(jax.numpy.minimum, values) >> DeviceArray([0.3721109, 0.3721109, 0.3721109, 0.7368197, 0.7368197,              0.7368197, 0.7368197, 0.7368197, 0.7368197, 0.7368197],            dtype=float32)  Numpy API: np.minimum.accumulate(values) >> array([0.3721109, 0.3721109, 0.3721109, 0.7368197, 0.7368197,        0.7368197, 0.7368197, 0.7368197, 0.7368197, 0.7368197],       dtype=float32) ``` So, I have a functional workaround, but this is not as clean as the main numpy API. Is this perhaps related to CC(Overrides of NumPy functions on JAX arrays)?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Implement `jax.numpy.<fun>.accumulate`,"I was trying to run a cumulative minimum when I realized that this isn't part of the main `jax.numpy` API, while it is the case for `numpy`. For now I've managed simply swap it out with: ```python import jax import jax.numpy as jnp key = jax.random.PRNGKey(0) values = jax.random.normal(key, shape=(10,)) jax.lax.associative_scan(jax.numpy.minimum, values) >> DeviceArray([0.3721109, 0.3721109, 0.3721109, 0.7368197, 0.7368197,              0.7368197, 0.7368197, 0.7368197, 0.7368197, 0.7368197],            dtype=float32)  Numpy API: np.minimum.accumulate(values) >> array([0.3721109, 0.3721109, 0.3721109, 0.7368197, 0.7368197,        0.7368197, 0.7368197, 0.7368197, 0.7368197, 0.7368197],       dtype=float32) ``` So, I have a functional workaround, but this is not as clean as the main numpy API. Is this perhaps related to CC(Overrides of NumPy functions on JAX arrays)?",2022-06-28T07:08:58Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/11281,Thanks for the request – I have a prototype of this in https://github.com/google/jax/pull/9529; it's unfortunately stalled in review.,"We're close to getting CC(jax.numpy: add ufuncstyle APIs for several jnp functions) marged – ETA probably early next week. On that branch, this works: ```python In [1]: import jax     ...: import jax.numpy as jnp     ...:      ...: key = jax.random.PRNGKey(0)     ...: values = jax.random.normal(key, shape=(10,))     ...:      ...: jnp.minimum.accumulate(values)                                                                                                                                                                   Out[1]:  DeviceArray([0.3721109, 0.3721109, 0.3721109, 0.7368197, 0.7368197,              0.7368197, 0.7368197, 0.7368197, 0.7368197, 0.7368197],            dtype=float32) ```",Now tracked in CC(`jax.numpy` APIs should be wrapped as ufuncs by default)
347,"以下是一个github上的jax下的一个issue, 标题是(jnp.average: support keepdims argument)， 内容是 (The `keepdims` argument was added in numpy 1.23.0 (https://numpy.org/devdocs/release/1.23.0notes.htmlkeepdimsparameterforaverage))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jnp.average: support keepdims argument,The `keepdims` argument was added in numpy 1.23.0 (https://numpy.org/devdocs/release/1.23.0notes.htmlkeepdimsparameterforaverage),2022-06-24T22:59:55Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/11259
1387,"以下是一个github上的jax下的一个issue, 标题是(How to properly run `linen.ConvTranspose` )， 内容是 (Hi there, I am trying to understand how to properly run `linen.ConvTranspose` to match `torch.nn.ConvTranspose2D` using the following snipped:  ```python import torch import torch.nn as nn import flax.linen as nn_flax import numpy as np import jax.numpy as jnp channels = 256 in_size = 24 factor = 4 resize = nn.ConvTranspose2d(channels, channels, kernel_size=factor, stride=factor, padding=0) resize_flax = nn_flax.ConvTranspose(     channels, kernel_size=(factor, factor), strides=(factor, factor), padding=""VALID"", use_bias=True ) input_tensor = np.random.rand(1, in_size, in_size, 256)  compare input output from flax and pytorch resize_flax_out = resize_flax.apply({""params"":{""kernel"":resize.weight.data.permute(2, 3, 1, 0).numpy(), ""bias"":resize.bias.data.numpy()}}, jnp.array(input_tensor)) resize_out = resize(torch.tensor(input_tensor).permute(0, 3, 1, 2).float()) torch.testing.assert_allclose(torch.from_numpy(np.array(resize_flax_out)).permute(0, 3, 1, 2), resize_out) ``` I the last line's test is not passing and I was wondering if the following is the correct way to run and use `ConvTranspose` or if I missed something :) !  Thanks a lot in advance!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,How to properly run `linen.ConvTranspose` ,"Hi there, I am trying to understand how to properly run `linen.ConvTranspose` to match `torch.nn.ConvTranspose2D` using the following snipped:  ```python import torch import torch.nn as nn import flax.linen as nn_flax import numpy as np import jax.numpy as jnp channels = 256 in_size = 24 factor = 4 resize = nn.ConvTranspose2d(channels, channels, kernel_size=factor, stride=factor, padding=0) resize_flax = nn_flax.ConvTranspose(     channels, kernel_size=(factor, factor), strides=(factor, factor), padding=""VALID"", use_bias=True ) input_tensor = np.random.rand(1, in_size, in_size, 256)  compare input output from flax and pytorch resize_flax_out = resize_flax.apply({""params"":{""kernel"":resize.weight.data.permute(2, 3, 1, 0).numpy(), ""bias"":resize.bias.data.numpy()}}, jnp.array(input_tensor)) resize_out = resize(torch.tensor(input_tensor).permute(0, 3, 1, 2).float()) torch.testing.assert_allclose(torch.from_numpy(np.array(resize_flax_out)).permute(0, 3, 1, 2), resize_out) ``` I the last line's test is not passing and I was wondering if the following is the correct way to run and use `ConvTranspose` or if I missed something :) !  Thanks a lot in advance!",2022-06-24T19:04:52Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/11252,It appears that the transposed convolution from PyTorch does not give the same result as the one from Flax/Jax. Closing this issue
306,"以下是一个github上的jax下的一个issue, 标题是(Remove type: ignore in sharding.py)， 内容是 (It actually seems to be a place that's a genuine type error. Let's see if mypy can catch it.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Remove type: ignore in sharding.py,It actually seems to be a place that's a genuine type error. Let's see if mypy can catch it.,2022-06-24T17:02:09Z,,open,0,0,https://github.com/jax-ml/jax/issues/11249
1269,"以下是一个github上的jax下的一个issue, 标题是(Reduce the size of jaxlib wheels)， 内容是 (On my machine (Mac ARM64), installed wheels approach the 250 MB mark, which makes it the biggest wheel installed by quite a bit (the closest second is scipy with 100MB). The wheel size is practically entirely determined by the size of three binaries, `xla_extension.so` (clocking in at 144MB), `libjaxlib_mlir_capi.dylib` (51MB) and `tpu_client_extension.so` (44MB). Looking at potential packaging uses (think Docker or otherwiseconstrained environments either in network bandwidth or storage), where every megabyte might count to get a lean solution, I wanted to ask if there are ways to reduce the size of these wheels, maybe also based on target platform. For example, I should probably not need the `tpu_client_extension` when I am on M1, right? I know of ways to shrink wheel sizes (and thus the resulting Docker image size) for other popular scientific Python packages, e.g. by stripping debug symbols at compilation. Is there a way to do the same with jaxlib? Possibly related to / interesting for CC(Provide instructions to build jaxlib via docker).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Reduce the size of jaxlib wheels,"On my machine (Mac ARM64), installed wheels approach the 250 MB mark, which makes it the biggest wheel installed by quite a bit (the closest second is scipy with 100MB). The wheel size is practically entirely determined by the size of three binaries, `xla_extension.so` (clocking in at 144MB), `libjaxlib_mlir_capi.dylib` (51MB) and `tpu_client_extension.so` (44MB). Looking at potential packaging uses (think Docker or otherwiseconstrained environments either in network bandwidth or storage), where every megabyte might count to get a lean solution, I wanted to ask if there are ways to reduce the size of these wheels, maybe also based on target platform. For example, I should probably not need the `tpu_client_extension` when I am on M1, right? I know of ways to shrink wheel sizes (and thus the resulting Docker image size) for other popular scientific Python packages, e.g. by stripping debug symbols at compilation. Is there a way to do the same with jaxlib? Possibly related to / interesting for CC(Provide instructions to build jaxlib via docker).",2022-06-23T10:05:42Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/11225,"The easiest thing we could do is to omit the `tpu_client_extension`. *Technically* you could use it from a Mac since it talks to remote TPUs. But in practice, I very much doubt any one does, and it's mostly deprecated anyway (in favor of only supporting local TPUs, which no Mac has). This is just a build flag change in our wheel build. It's also quite likely we can refactor things a bit to share code between the xla_extension and the jaxlib_mlir dylibs.","Sounds good. Anything I could contribute here to get it done, or is that more of an internal task?",The newlyreleased 0.3.14 wheels reduce the installed size of the Mac ARM wheel to 199MB by omitting the TPU extension. I hope that helps. We welcome PRs to reduce the size further!,"I wanted to check back in on this briefly, as the wheel is now back at ~220MB due to the MLIR .so file. Would it be possible to share a quick conceptual overview of what goes into the XLA and MLIR .so files that makes them so large? That way I might get an understanding that could help with the size management :)"
3456,"以下是一个github上的jax下的一个issue, 标题是(Adding `align_corners` to `jax.image.resize`)， 内容是 (Hi there !  I would like to reproduce the operations that are done under `torch.nn.Upsample()` with `flax` 🎉 . In PyTorch, it seems that the flag `align_corners` doesn't mean ""align the corners"" but rather ""sample with equal spacing""  ! Can we add this feature in `jax.image.resize`? 🙏 (Initially posted on `google/flax`)  Problem description: Ideally: ``` import torch import torch.nn as nn import jax.numpy as jnp import jax import numpy as np input_arr = jnp.array([     [0, 1, 2, 3, 4],     [5, 6, 7, 8, 9], ]) / 9 torch_input_arr = torch.from_numpy(np.array(input_arr)).unsqueeze(0).unsqueeze(0) upsample_with_align_corners = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) upsample_without_align_corners = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) output = jax.image.resize(     image=input_arr,     shape=(4,10),     method=""bilinear"") output_torch_upsample_with_align_corners = upsample_with_align_corners(torch_input_arr) output_torch_upsample_without_align_corners = upsample_without_align_corners(torch_input_arr) print(""jax: "", output) print(""torch upsample align corner:"", output_torch_upsample_with_align_corners) print(""torch upsample without align corners"",output_torch_upsample_without_align_corners) ``` I would like to match `output_torch_upsample_with_align_corners` and flax' `output`. Here is what I get: ```  jax default output DeviceArray([[0.        , 0.02777778, 0.08333334, 0.1388889 , 0.19444445,               0.25      , 0.30555555, 0.3611111 , 0.4166667 , 0.44444445],              [0.1388889 , 0.16666667, 0.22222222, 0.2777778 , 0.33333334,               0.3888889 , 0.44444442, 0.5       , 0.5555556 , 0.5833334 ],              [0.4166667 , 0.44444448, 0.5       , 0.5555555 , 0.6111111 ,               0.6666667 , 0.7222222 , 0.7777778 , 0.8333333 , 0.8611111 ],              [0.5555556 , 0.5833334 , 0.6388889 , 0.6944444 , 0.75      ,               0.8055556 , 0.8611111 , 0.9166667 , 0.9722222 , 1.        ]],            dtype=float32)  torch with align corner tensor([[[[0.0000, 0.0494, 0.0988, 0.1481, 0.1975, 0.2469, 0.2963, 0.3457,            0.3951, 0.4444],           [0.1852, 0.2346, 0.2840, 0.3333, 0.3827, 0.4321, 0.4815, 0.5309,            0.5802, 0.6296],           [0.3704, 0.4198, 0.4691, 0.5185, 0.5679, 0.6173, 0.6667, 0.7160,            0.7654, 0.8148],           [0.5556, 0.6049, 0.6543, 0.7037, 0.7531, 0.8025, 0.8519, 0.9012,            0.9506, 1.0000]]]])  torch without align corner tensor([[[[0.0000, 0.0278, 0.0833, 0.1389, 0.1944, 0.2500, 0.3056, 0.3611,            0.4167, 0.4444],           [0.1389, 0.1667, 0.2222, 0.2778, 0.3333, 0.3889, 0.4444, 0.5000,            0.5556, 0.5833],           [0.4167, 0.4444, 0.5000, 0.5556, 0.6111, 0.6667, 0.7222, 0.7778,            0.8333, 0.8611],           [0.5556, 0.5833, 0.6389, 0.6944, 0.7500, 0.8056, 0.8611, 0.9167,            0.9722, 1.0000]]]]) ```  Motivation I am converting Dense Prediction Transformers into `flax`, and would like to match the output betwen PyTorch's model and flax' implementation! https://github.com/huggingface/transformers/pull/17779   , moved the issue here :)  Would love to see if I can contribute as well but I may need more guidance on how to do so!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Adding `align_corners` to `jax.image.resize`,"Hi there !  I would like to reproduce the operations that are done under `torch.nn.Upsample()` with `flax` 🎉 . In PyTorch, it seems that the flag `align_corners` doesn't mean ""align the corners"" but rather ""sample with equal spacing""  ! Can we add this feature in `jax.image.resize`? 🙏 (Initially posted on `google/flax`)  Problem description: Ideally: ``` import torch import torch.nn as nn import jax.numpy as jnp import jax import numpy as np input_arr = jnp.array([     [0, 1, 2, 3, 4],     [5, 6, 7, 8, 9], ]) / 9 torch_input_arr = torch.from_numpy(np.array(input_arr)).unsqueeze(0).unsqueeze(0) upsample_with_align_corners = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) upsample_without_align_corners = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) output = jax.image.resize(     image=input_arr,     shape=(4,10),     method=""bilinear"") output_torch_upsample_with_align_corners = upsample_with_align_corners(torch_input_arr) output_torch_upsample_without_align_corners = upsample_without_align_corners(torch_input_arr) print(""jax: "", output) print(""torch upsample align corner:"", output_torch_upsample_with_align_corners) print(""torch upsample without align corners"",output_torch_upsample_without_align_corners) ``` I would like to match `output_torch_upsample_with_align_corners` and flax' `output`. Here is what I get: ```  jax default output DeviceArray([[0.        , 0.02777778, 0.08333334, 0.1388889 , 0.19444445,               0.25      , 0.30555555, 0.3611111 , 0.4166667 , 0.44444445],              [0.1388889 , 0.16666667, 0.22222222, 0.2777778 , 0.33333334,               0.3888889 , 0.44444442, 0.5       , 0.5555556 , 0.5833334 ],              [0.4166667 , 0.44444448, 0.5       , 0.5555555 , 0.6111111 ,               0.6666667 , 0.7222222 , 0.7777778 , 0.8333333 , 0.8611111 ],              [0.5555556 , 0.5833334 , 0.6388889 , 0.6944444 , 0.75      ,               0.8055556 , 0.8611111 , 0.9166667 , 0.9722222 , 1.        ]],            dtype=float32)  torch with align corner tensor([[[[0.0000, 0.0494, 0.0988, 0.1481, 0.1975, 0.2469, 0.2963, 0.3457,            0.3951, 0.4444],           [0.1852, 0.2346, 0.2840, 0.3333, 0.3827, 0.4321, 0.4815, 0.5309,            0.5802, 0.6296],           [0.3704, 0.4198, 0.4691, 0.5185, 0.5679, 0.6173, 0.6667, 0.7160,            0.7654, 0.8148],           [0.5556, 0.6049, 0.6543, 0.7037, 0.7531, 0.8025, 0.8519, 0.9012,            0.9506, 1.0000]]]])  torch without align corner tensor([[[[0.0000, 0.0278, 0.0833, 0.1389, 0.1944, 0.2500, 0.3056, 0.3611,            0.4167, 0.4444],           [0.1389, 0.1667, 0.2222, 0.2778, 0.3333, 0.3889, 0.4444, 0.5000,            0.5556, 0.5833],           [0.4167, 0.4444, 0.5000, 0.5556, 0.6111, 0.6667, 0.7222, 0.7778,            0.8333, 0.8611],           [0.5556, 0.5833, 0.6389, 0.6944, 0.7500, 0.8056, 0.8611, 0.9167,            0.9722, 1.0000]]]]) ```  Motivation I am converting Dense Prediction Transformers into `flax`, and would like to match the output betwen PyTorch's model and flax' implementation! https://github.com/huggingface/transformers/pull/17779   , moved the issue here :)  Would love to see if I can contribute as well but I may need more guidance on how to do so!",2022-06-22T15:00:29Z,enhancement,open,2,3,https://github.com/jax-ml/jax/issues/11206," FYI I have an implementation of resizing with aligned corners here: https://github.com/brentyi/pipsjax/blob/8ae798a31caf03dcdd1c237ac39ca5ff13d0ea4d/src/pips_jax/utils_bilerp.pyL60L83 I just adapted `jax.image.scale_and_translate`: ```python def resize_with_aligned_corners(     image: jax.Array,     shape: Tuple[int, ...],     method: Union[str, jax.image.ResizeMethod],     antialias: bool, ):     """"""Alternative to jax.image.resize(), which emulates align_corners=True in PyTorch's     interpolation functions.""""""     spatial_dims = tuple(         i         for i in range(len(shape))         if not jax.core.symbolic_equal_dim(image.shape[i], shape[i])     )     scale = jnp.array([(shape[i]  1.0) / (image.shape[i]  1.0) for i in spatial_dims])     translation = (scale / 2.0  0.5)     return jax.image.scale_and_translate(         image,         shape,         method=method,         scale=scale,         spatial_dims=spatial_dims,         translation=translation,         antialias=antialias,     ) ``` ps: are you still interested in getting a Flax implementation of DPT merged? This would be really useful to me.","Hi, just wondering are there any updates on this?  . The outputs from `jax.image.resize` also may be different from `F.interpolate` when using ""bilinear"" for 2D inputs. ","FYI Align corners aligns the centers of the corner pixels, this is only really useful because it happens to implement the translation that is necessary to align features from strided convolutions.  The better and more general way to do this is to use the more general function, scale_and_translate,  that  shows above.  (`jax.image.resize` uses `jax.image.scale_and_translate` underneath anyway)."
1578,"以下是一个github上的jax下的一个issue, 标题是(support gradients for  all_gather with tiled=True)， 内容是 (When writing some modelsharding code I ran into this limitation, and it suggested I open a feature request. So here I am! https://github.com/google/jax/blob/main/jax/_src/lax/parallel.pyL1205L1216 ``` import jax import jax.numpy as jnp import jax.lax as lax import jax.random as jrandom from jax.experimental.maps import xmap key = jrandom.PRNGKey(0) embeddings = jrandom.normal(key, shape=(4, 10, 8)) num_shards=4 def compute(embeddings, input_ids):     hidden_states = embeddings[input_ids]     hidden_states = lax.all_gather(hidden_states, axis_name=""shard"", tiled=True, axis=1)  this is the problematic code      pretend there's a transformer here     my_shard = lax.axis_index(""shard"")     hidden_states = hidden_states.reshape(hidden_states.shape[:1] + (num_shards, 1))     hidden_states = hidden_states[..., my_shard, :]     scores = hidden_states @ jnp.transpose(embeddings)     scores = lax.psum(scores, axis_name=""shard"")     return jnp.sum(jnp.sum(scores, axis=1)) compute_xmap = xmap(compute, in_axes=[(""shard"", ...), (..., )], out_axes=(..., )) compute_grad_xmap = xmap(jax.grad(compute), in_axes=[(""shard"", ...), (..., )], out_axes=(""shard"", ...)) if __name__ == ""__main__"":     input_ids = jnp.array([0, 1, 2, 3, 5, 6, 7])     print(compute_xmap(embeddings, input_ids))     print(compute_grad_xmap(embeddings, input_ids))   error! ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,support gradients for  all_gather with tiled=True,"When writing some modelsharding code I ran into this limitation, and it suggested I open a feature request. So here I am! https://github.com/google/jax/blob/main/jax/_src/lax/parallel.pyL1205L1216 ``` import jax import jax.numpy as jnp import jax.lax as lax import jax.random as jrandom from jax.experimental.maps import xmap key = jrandom.PRNGKey(0) embeddings = jrandom.normal(key, shape=(4, 10, 8)) num_shards=4 def compute(embeddings, input_ids):     hidden_states = embeddings[input_ids]     hidden_states = lax.all_gather(hidden_states, axis_name=""shard"", tiled=True, axis=1)  this is the problematic code      pretend there's a transformer here     my_shard = lax.axis_index(""shard"")     hidden_states = hidden_states.reshape(hidden_states.shape[:1] + (num_shards, 1))     hidden_states = hidden_states[..., my_shard, :]     scores = hidden_states @ jnp.transpose(embeddings)     scores = lax.psum(scores, axis_name=""shard"")     return jnp.sum(jnp.sum(scores, axis=1)) compute_xmap = xmap(compute, in_axes=[(""shard"", ...), (..., )], out_axes=(..., )) compute_grad_xmap = xmap(jax.grad(compute), in_axes=[(""shard"", ...), (..., )], out_axes=(""shard"", ...)) if __name__ == ""__main__"":     input_ids = jnp.array([0, 1, 2, 3, 5, 6, 7])     print(compute_xmap(embeddings, input_ids))     print(compute_grad_xmap(embeddings, input_ids))   error! ```",2022-06-21T21:37:09Z,enhancement,open,1,8,https://github.com/jax-ml/jax/issues/11193,"! Thanks for filing. Paging  for his familiarity with the state of implementation here. Since I notice that you are trying out `xmap`, heads up that its development is a bit less active right now as we're focusing on `pjit`.","Thanks! Yeah, I just learned that. I'll port my code over. On Mon, Jul 11, 2022, 6:45 PM Roy Frostig ***@***.***> wrote: >  ! Thanks for filing. Paging  >  for his familiarity with the state of > implementation here. > > Since I notice that you are trying out xmap, heads up that its > development is a bit less active right now as we're focusing on pjit > . > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you were mentioned.Message ID: > ***@***.***> >","How do gradients flow across devices, e.g. with xmap? Is the gradient calculation distributed or is it all done on the host node?","Broadly speaking, derivatives are computed on device(s) in correspondence with the primal computation. The interesting details depend on the specific operation.","Hi   I ran the code you mentioned on JAX version 0.4.23 with Colab on both CPU and GPU backends. It executed without any errors. However, it raises the same error asking for a feature request when run on a Colab TPU due to its limitation to support JAX versions till 0.3.25. Below is the output of the code when running on CPU/GPU: ```python import jax import jax.numpy as jnp import jax.lax as lax import jax.random as jrandom from jax.experimental.maps import xmap key = jrandom.PRNGKey(0) embeddings = jrandom.normal(key, shape=(4, 10, 8)) num_shards=4 def compute(embeddings, input_ids):     hidden_states = embeddings[input_ids]     hidden_states = lax.all_gather(hidden_states, axis_name=""shard"", tiled=True, axis=1)  this is the problematic code      pretend there's a transformer here     my_shard = lax.axis_index(""shard"")     hidden_states = hidden_states.reshape(hidden_states.shape[:1] + (num_shards, 1))     hidden_states = hidden_states[..., my_shard, :]     scores = hidden_states @ jnp.transpose(embeddings)     scores = lax.psum(scores, axis_name=""shard"")     return jnp.sum(jnp.sum(scores, axis=1)) compute_xmap = xmap(compute, in_axes=[(""shard"", ...), (..., )], out_axes=(..., )) compute_grad_xmap = xmap(jax.grad(compute), in_axes=[(""shard"", ...), (..., )], out_axes=(""shard"", ...)) if __name__ == ""__main__"":     input_ids = jnp.array([0, 1, 2, 3, 5, 6, 7])     print(compute_xmap(embeddings, input_ids))     print(compute_grad_xmap(embeddings, input_ids))   error! ``` Output: ``` 307.6236 [[[ 26.360273    23.722437    21.593668    5.0757284   14.667934     12.351794   23.29985     17.818474  ]   [ 26.360273    23.722437    21.593668    5.0757284   14.667934     12.351794   23.29985     17.818474  ]   [ 26.360273    23.722437    21.593668    5.0757284   14.667934     12.351794   23.29985     17.818474  ]   [ 26.360273    23.722437    21.593668    5.0757284   14.667934     12.351794   23.29985     17.818474  ]   [  9.055899     8.760309     9.494036    2.3148034    5.049895      9.559207   11.454371     8.111195  ]   [ 26.360273    23.722437    21.593668    5.0757284   14.667934     12.351794   23.29985     17.818474  ]   [ 26.360273    23.722437    21.593668    5.0757284   14.667934     12.351794   23.29985     17.818474  ]   [ 26.360271    23.722439    21.593668    5.075729    14.667934     12.351794   23.29985     17.818474  ]   [  9.055899     8.760309     9.494036    2.3148034    5.049895      9.559207   11.454371     8.111195  ]   [  9.055899     8.760309     9.494036    2.3148034    5.049895      9.559207   11.454371     8.111195  ]]  [[13.621416    8.900112   42.051353   26.818888    2.9556527     19.286644   19.299164     7.5898967 ]   [13.621416    8.900112   42.051353   26.818888    2.9556527     19.286644   19.299164     7.5898967 ]   [13.621416    8.900112   42.051353   26.818888    2.9556527     19.286644   19.299164     7.5898967 ]   [13.621416    8.900112   42.051353   26.818888    2.9556527     19.286644   19.299164     7.5898967 ]   [ 1.3414402   6.754506   20.29285    19.30616     4.648571      8.727608   13.953814     0.9600564 ]   [13.621416    8.900112   42.051353   26.818888    2.9556527     19.286644   19.299164     7.5898967 ]   [13.621416    8.900112   42.051353   26.818888    2.9556527     19.286644   19.299164     7.5898967 ]   [13.621416    8.900112   42.05135    26.81889     2.9556541     19.286644   19.299164     7.589896  ]   [ 1.3414402   6.754506   20.29285    19.30616     4.648571      8.727608   13.953814     0.9600564 ]   [ 1.3414402   6.754506   20.29285    19.30616     4.648571      8.727608   13.953814     0.9600564 ]]  [[19.816143   35.93858    41.06128     24.642227    8.617016    29.027905    7.9242506   16.948212  ]   [19.816143   35.93858    41.06128     24.642227    8.617016    29.027905    7.9242506   16.948212  ]   [19.816143   35.93858    41.06128     24.642227    8.617016    29.027905    7.9242506   16.948212  ]   [19.816143   35.93858    41.06128     24.642227    8.617016    29.027905    7.9242506   16.948212  ]   [10.614532   10.974433   17.32348      8.323803    1.7500695    13.531995    4.691452     6.64594   ]   [19.816143   35.93858    41.06128     24.642227    8.617016    29.027905    7.9242506   16.948212  ]   [19.816143   35.93858    41.06128     24.642227    8.617016    29.027905    7.9242506   16.948212  ]   [19.816143   35.93858    41.061287    24.642227    8.617017    29.027905    7.92425     16.948212  ]   [10.614532   10.974433   17.32348      8.323803    1.7500695    13.531995    4.691452     6.64594   ]   [10.614532   10.974433   17.32348      8.323803    1.7500695    13.531995    4.691452     6.64594   ]]  [[  9.344713   45.858727   30.775932    40.07314    42.416855     34.77363     8.743544   35.934025  ]   [  9.344713   45.858727   30.775932    40.07314    42.416855     34.77363     8.743544   35.934025  ]   [  9.344713   45.858727   30.775932    40.07314    42.416855     34.77363     8.743544   35.934025  ]   [  9.344713   45.858727   30.775932    40.07314    42.416855     34.77363     8.743544   35.934025  ]   [  8.339038   24.767185   12.350559    20.605862   20.295427     14.2957115    0.72376066 17.121685  ]   [  9.344713   45.858727   30.775932    40.07314    42.416855     34.77363     8.743544   35.934025  ]   [  9.344713   45.858727   30.775932    40.07314    42.416855     34.77363     8.743544   35.934025  ]   [  9.344714   45.858723   30.775932    40.07314    42.416855     34.773624    8.743544   35.934025  ]   [  8.339038   24.767185   12.350559    20.605862   20.295427     14.2957115    0.72376066 17.121685  ]   [  8.339038   24.767185   12.350559    20.605862   20.295427     14.2957115    0.72376066 17.121685  ]]] ``` Attaching the gists on CPU and GPU for reference. Also verified this behavior on cloud VM (Ubuntu 22.04 having 4 CPUS and 4 Tesla T4 GPUs) with JAX version 0.4.25 and it works fine. ```python >>> import jax >>> jax.__version__ '0.4.25' >>> jax.devices() [cuda(id=0), cuda(id=1), cuda(id=2), cuda(id=3)] >>> import jax.numpy as jnp >>> import jax.lax as lax >>> import jax.random as jrandom >>> from jax.experimental.maps import xmap >>>  >>> key = jrandom.PRNGKey(0) >>>  >>> embeddings = jrandom.normal(key, shape=(4, 10, 8)) >>>  >>> num_shards=4 >>>  >>>  >>> def compute(embeddings, input_ids): ...     hidden_states = embeddings[input_ids] ...     hidden_states = lax.all_gather(hidden_states, axis_name=""shard"", tiled=True, axis=1)  this is the problematic code ...      pretend there's a transformer here ...     my_shard = lax.axis_index(""shard"") ...     hidden_states = hidden_states.reshape(hidden_states.shape[:1] + (num_shards, 1)) ...     hidden_states = hidden_states[..., my_shard, :] ...     scores = hidden_states @ jnp.transpose(embeddings) ...     scores = lax.psum(scores, axis_name=""shard"") ...     return jnp.sum(jnp.sum(scores, axis=1)) ...  >>>  >>> compute_xmap = xmap(compute, in_axes=[(""shard"", ...), (..., )], out_axes=(..., )) >>>  >>> compute_grad_xmap = xmap(jax.grad(compute), in_axes=[(""shard"", ...), (..., )], out_axes=(""shard"", ...)) >>>  >>> if __name__ == ""__main__"": ...     input_ids = jnp.array([0, 1, 2, 3, 5, 6, 7]) ...     print(compute_xmap(embeddings, input_ids)) ...     print(compute_grad_xmap(embeddings, input_ids))   error! ...  307.62357 [[[ 26.360273   23.722437   21.59367    5.0757284  14.667936     12.351794  23.29985    17.818474 ]   [ 26.360273   23.722437   21.59367    5.0757284  14.667936     12.351794  23.29985    17.818474 ]   [ 26.360273   23.722437   21.59367    5.0757284  14.667936     12.351794  23.29985    17.818474 ]   [ 26.360273   23.722437   21.59367    5.0757284  14.667936     12.351794  23.29985    17.818474 ]   [  9.055899    8.760309    9.494038   2.3148034   5.049896      9.559207  11.454371    8.111195 ]   [ 26.360273   23.722437   21.59367    5.0757284  14.667936     12.351794  23.29985    17.818474 ]   [ 26.360273   23.722437   21.59367    5.0757284  14.667936     12.351794  23.29985    17.818474 ]   [ 26.360273   23.722437   21.59367    5.0757284  14.667936     12.351794  23.29985    17.818474 ]   [  9.055899    8.760309    9.494038   2.3148034   5.049896      9.559207  11.454371    8.111195 ]   [  9.055899    8.760309    9.494038   2.3148034   5.049896      9.559207  11.454371    8.111195 ]]  [[13.621416   8.900112  42.051353  26.818892   2.9556527     19.28664   19.299164    7.5898976]   [13.621416   8.900112  42.051353  26.818892   2.9556527     19.28664   19.299164    7.5898976]   [13.621416   8.900112  42.051353  26.818892   2.9556527     19.28664   19.299164    7.5898976]   [13.621416   8.900112  42.051353  26.818892   2.9556527     19.28664   19.299164    7.5898976]   [ 1.3414402  6.754506  20.29285   19.306162   4.648571      8.727606  13.953814    0.9600569]   [13.621416   8.900112  42.051353  26.818892   2.9556527     19.28664   19.299164    7.5898976]   [13.621416   8.900112  42.051353  26.818892   2.9556527     19.28664   19.299164    7.5898976]   [13.621416   8.900112  42.051353  26.818892   2.9556527     19.28664   19.299164    7.5898976]   [ 1.3414402  6.754506  20.29285   19.306162   4.648571      8.727606  13.953814    0.9600569]   [ 1.3414402  6.754506  20.29285   19.306162   4.648571      8.727606  13.953814    0.9600569]]  [[19.816143  35.93858   41.06128    24.642227   8.617017    29.027905   7.9242506  16.948214 ]   [19.816143  35.93858   41.06128    24.642227   8.617017    29.027905   7.9242506  16.948214 ]   [19.816143  35.93858   41.06128    24.642227   8.617017    29.027905   7.9242506  16.948214 ]   [19.816143  35.93858   41.06128    24.642227   8.617017    29.027905   7.9242506  16.948214 ]   [10.6145315 10.974433  17.32348     8.323803   1.7500695    13.531995   4.691452    6.645941 ]   [19.816143  35.93858   41.06128    24.642227   8.617017    29.027905   7.9242506  16.948214 ]   [19.816143  35.93858   41.06128    24.642227   8.617017    29.027905   7.9242506  16.948214 ]   [19.816143  35.93858   41.06128    24.642227   8.617017    29.027905   7.9242506  16.948214 ]   [10.6145315 10.974433  17.32348     8.323803   1.7500695    13.531995   4.691452    6.645941 ]   [10.6145315 10.974433  17.32348     8.323803   1.7500695    13.531995   4.691452    6.645941 ]]  [[  9.344717  45.858727  30.775932   40.07314   42.416855     34.77363    8.743545  35.93402  ]   [  9.344717  45.858727  30.775932   40.07314   42.416855     34.77363    8.743545  35.93402  ]   [  9.344717  45.858727  30.775932   40.07314   42.416855     34.77363    8.743545  35.93402  ]   [  9.344717  45.858727  30.775932   40.07314   42.416855     34.77363    8.743545  35.93402  ]   [  8.33904   24.767185  12.350559   20.605862  20.295427     14.2957115   0.7237602 17.121683 ]   [  9.344717  45.858727  30.775932   40.07314   42.416855     34.77363    8.743545  35.93402  ]   [  9.344717  45.858727  30.775932   40.07314   42.416855     34.77363    8.743545  35.93402  ]   [  9.344717  45.858727  30.775932   40.07314   42.416855     34.77363    8.743545  35.93402  ]   [  8.33904   24.767185  12.350559   20.605862  20.295427     14.2957115   0.7237602 17.121683 ]   [  8.33904   24.767185  12.350559   20.605862  20.295427     14.2957115   0.7237602 17.121683 ]]] >>> ``` Could you please verify and confirm the same. Thank you.","I've moved on from xmap (it's sort of quasideprecated I think), and you should probably do the same...","Yeah, work on moving and removing things is ongoing."," Just to be a bit more helpful: you should use jit (formerly pjit) with shardings. The official docs are here https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html , which are pretty good for understanding the mechanics. If I may shill my own JAX wrapper, take a look at my scaling with named tensors which uses a named tensor mapping, which I personally find more intuitive (and if you like xmap, you might too!)"
4149,"以下是一个github上的jax下的一个issue, 标题是([GPU] CUDA XLA Continuous Core Dump - nvptx_compiler.cc:460)， 内容是 (I have a set of GCNNs I have been training on CPUs, and for the past couple of weeks I have been trying to transfer part of my workload to GPUs. After getting the latest Nvidia driver set up, installing CUDA 11.7 and cudNN 8.4, I would continuously get the same error: ``` 20220621 20:49:59.180590: F external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:460] ptxas  returned an error during compilation of ptx to sass: 'INTERNAL: couldn't get temp CUBIN file name'  If the error  message indicates that a file could not be written, please verify that sufficient filesystem space is provided. Aborted (core dumped) ``` I tried downgrading to CUDA 11.4 (w respective driver) and cudNN 8.3, but the process would keep crashing with the same message as above. Specs: `nvidiasmi` ``` ++  ``` `nvcc version` ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 ```  CPU ``` Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                          8 Online CPU(s) list:             07 Thread(s) per core:              1 Core(s) per socket:              8 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       GenuineIntel CPU family:                      6 Model:                           79 Model name:                      Intel(R) Xeon(R) CPU E52690 v4 @ 2.60GHz Stepping:                        1 CPU MHz:                         2599.980 BogoMIPS:                        5200.03 Hypervisor vendor:               Xen Virtualization type:             full L1d cache:                       256 KiB L1i cache:                       256 KiB L2 cache:                        2 MiB L3 cache:                        280 MiB ```  Python libraries ``` Package           Version   abslpy           1.1.0 asttokens         2.0.5 backcall          0.2.0 bokeh             2.4.3 certifi           2022.5.18.1 chex              0.1.3 click             8.1.3 cloudpickle       2.1.0 cupy              10.5.0 cycler            0.11.0 dask              2022.6.0 debugpy           1.6.0 decorator         5.1.1 distributed       2022.6.0 dmtree           0.1.7 entrypoints       0.4 executing         0.8.3 fastrlock         0.8 flatbuffers       2.0 fonttools         4.33.3 fsspec            2022.5.0 HeapDict          1.0.1 ipykernel         6.15.0 ipython           8.4.0 jax               0.3.13 jaxlib            0.3.8+cuda11.cudnn82 jedi              0.18.1 Jinja2            3.1.2 joblib            1.1.0 jupyterclient    7.3.4 jupytercore      4.10.0 kiwisolver        1.4.3 locket            1.0.0 MarkupSafe        2.1.1 matplotlib        3.5.2 matplotlibinline 0.1.3 msgpack           1.0.4 nestasyncio      1.5.5 numpy             1.22.4 opteinsum        3.3.0 optax             0.1.2 packaging         21.3 pandas            1.4.2 parso             0.8.3 partd             1.2.0 pexpect           4.8.0 pickleshare       0.7.5 Pillow            9.1.1 pip               21.2.4 prompttoolkit    3.0.29 psutil            5.9.1 ptyprocess        0.7.0 pureeval         0.2.2 Pygments          2.12.0 pyparsing         3.0.9 pythondateutil   2.8.2 pytz              2022.1 PyYAML            6.0 pyzmq             23.1.0 scikitlearn      1.1.1 scipy             1.8.1 setuptools        62.4.0 six               1.16.0 sklearn           0.0 sortedcontainers  2.4.0 stackdata        0.3.0 tblib             1.7.0 threadpoolctl     3.1.0 toolz             0.11.2 tornado           6.1 traitlets         5.3.0 typing_extensions 4.2.0 urllib3           1.26.9 wcwidth           0.2.5 wheel             0.37.1 zict              2.2.0 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[GPU] CUDA XLA Continuous Core Dump - nvptx_compiler.cc:460,"I have a set of GCNNs I have been training on CPUs, and for the past couple of weeks I have been trying to transfer part of my workload to GPUs. After getting the latest Nvidia driver set up, installing CUDA 11.7 and cudNN 8.4, I would continuously get the same error: ``` 20220621 20:49:59.180590: F external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:460] ptxas  returned an error during compilation of ptx to sass: 'INTERNAL: couldn't get temp CUBIN file name'  If the error  message indicates that a file could not be written, please verify that sufficient filesystem space is provided. Aborted (core dumped) ``` I tried downgrading to CUDA 11.4 (w respective driver) and cudNN 8.3, but the process would keep crashing with the same message as above. Specs: `nvidiasmi` ``` ++  ``` `nvcc version` ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 ```  CPU ``` Architecture:                    x86_64 CPU opmode(s):                  32bit, 64bit Byte Order:                      Little Endian Address sizes:                   46 bits physical, 48 bits virtual CPU(s):                          8 Online CPU(s) list:             07 Thread(s) per core:              1 Core(s) per socket:              8 Socket(s):                       1 NUMA node(s):                    1 Vendor ID:                       GenuineIntel CPU family:                      6 Model:                           79 Model name:                      Intel(R) Xeon(R) CPU E52690 v4 @ 2.60GHz Stepping:                        1 CPU MHz:                         2599.980 BogoMIPS:                        5200.03 Hypervisor vendor:               Xen Virtualization type:             full L1d cache:                       256 KiB L1i cache:                       256 KiB L2 cache:                        2 MiB L3 cache:                        280 MiB ```  Python libraries ``` Package           Version   abslpy           1.1.0 asttokens         2.0.5 backcall          0.2.0 bokeh             2.4.3 certifi           2022.5.18.1 chex              0.1.3 click             8.1.3 cloudpickle       2.1.0 cupy              10.5.0 cycler            0.11.0 dask              2022.6.0 debugpy           1.6.0 decorator         5.1.1 distributed       2022.6.0 dmtree           0.1.7 entrypoints       0.4 executing         0.8.3 fastrlock         0.8 flatbuffers       2.0 fonttools         4.33.3 fsspec            2022.5.0 HeapDict          1.0.1 ipykernel         6.15.0 ipython           8.4.0 jax               0.3.13 jaxlib            0.3.8+cuda11.cudnn82 jedi              0.18.1 Jinja2            3.1.2 joblib            1.1.0 jupyterclient    7.3.4 jupytercore      4.10.0 kiwisolver        1.4.3 locket            1.0.0 MarkupSafe        2.1.1 matplotlib        3.5.2 matplotlibinline 0.1.3 msgpack           1.0.4 nestasyncio      1.5.5 numpy             1.22.4 opteinsum        3.3.0 optax             0.1.2 packaging         21.3 pandas            1.4.2 parso             0.8.3 partd             1.2.0 pexpect           4.8.0 pickleshare       0.7.5 Pillow            9.1.1 pip               21.2.4 prompttoolkit    3.0.29 psutil            5.9.1 ptyprocess        0.7.0 pureeval         0.2.2 Pygments          2.12.0 pyparsing         3.0.9 pythondateutil   2.8.2 pytz              2022.1 PyYAML            6.0 pyzmq             23.1.0 scikitlearn      1.1.1 scipy             1.8.1 setuptools        62.4.0 six               1.16.0 sklearn           0.0 sortedcontainers  2.4.0 stackdata        0.3.0 tblib             1.7.0 threadpoolctl     3.1.0 toolz             0.11.2 tornado           6.1 traitlets         5.3.0 typing_extensions 4.2.0 urllib3           1.26.9 wcwidth           0.2.5 wheel             0.37.1 zict              2.2.0 ```",2022-06-21T21:24:18Z,bug XLA P1 (soon) NVIDIA GPU,closed,0,13,https://github.com/jax-ml/jax/issues/11190,"```c++   if (!env>LocalTempFilename(&cubin_path)) {     return InternalError(""couldn't get temp CUBIN file name"");   } ``` That error came from tensorflow/compiler/xla/service/gpu/nvptx_compiler.'s not a filesystem related issue? ```c++ bool Env::LocalTempFilename(string* filename) {   std::vector dirs;   GetLocalTempDirectories(&dirs);   // Try each directory, as they might be full, have inappropriate   // permissions or have different problems at times.   for (const string& dir : dirs) {     *filename = io::JoinPath(dir, ""tempfile"");     if (CreateUniqueFileName(filename, """")) {       return true;     }   }   return false; } ``` https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/env.cc;l=421?q=LocalTempFilename https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/default/env.cc;l=261?q=GetLocalTempDirectories Can you try write a file in the `cubin_path`?","Thanks, . Is there a way I can trace which path is being assigned to it directly? I would assume it would be a subdirectory in `$TEMP` environment variable, no? ","Hi , were you able to solve this issue please?","If I had to guess, do you have limited space or no write permissions? Try setting the environment variable `TMPDIR=/somewhere/with/space`."," and , the issue persists. And I do have writing permissions to `TMPDIR`.","thanks gusmaogabriels and hawkinsp for your replies! In my case I am using Triton Server to serve TF Model using Docker container. I joined this thread because I am getting the same error. However, there are no temp dirs variables defined inside the container. In addition, I freed up some space and the issue persists. Also in the container things run as root. The issue happens as a warning also when I train TF Model using the ODAPI, my versions: TF 2.8, CUDA 11.2, cuDNN 8.1","Can you share the full error message? If there is many, give all of them? We will see if we can make this error message better.","Note, I made a TF PR that give a new warning message if we can't find a suitable directory for temporary files: https://github.com/tensorflow/tensorflow/pull/57184 I'm not sure if this is the current issue or not, but at least it will help make clear if this is the issue or not.","Hi , thank you so much for your PR. It will definitely help. Firstly, the problem resolved, after a reboot! Anyway this was the error (the last line): ``` I0804 13:52:33.566598 1 backend_model_instance.cc:687] Starting backend thread for tf_model_0 at nice 0 on device 0... I0804 13:52:33.566598 1 backend_model_instance.cc:551] model 'tf_model' instance tf_model_0 is running warmup sample 'warmup_1' I0804 13:52:33.566598 1 tensorflow.cc:2401] model tf_model, instance tf_model_0, executing 1 requests I0804 13:52:33.566598 1 tensorflow.cc:1575] TRITONBACKEND_ModelExecute: Running tf_model_0 with 1 requests I0804 13:52:33.566598 1 tensorflow.cc:1827] TRITONBACKEND_ModelExecute: input 'inputs' is GPU tensor: false 20220804 13:52:43.860583: F tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:622] ptxas returned an error during compilation of ptx to sass: 'INTERNAL: couldn't get temp CUBIN file name'  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided. ``` As I mentioned before, it used to happen when I run the Triton Server, and after the error occurs, the server exits. Also I was getting similar message as a warning when I train a Tensorflow Object Detection model, at the same time when I was having the previous problem with Triton. But due to GPUs problems, I rebooted the machine, problems like `Unable to determine the device handle for GPU 0000:82:00.0: Unknown Error` when running `nvidiasmi` and ` Xid 79, GPU has fallen off the bus` in the system logs (Maybe overheating problem since I was running many things at once (?)). And after the reboot, The error and the warning message didn't happen with both the Training and Triton! This was the warning message during the training: ```  I0729 11:28:02.408597 140660397020928 convolutional_keras_box_predictor.py:152] depth of additional conv before box predictor: 0  20220729 11:28:45.756070: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101  20220729 11:28:46.848138: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101  20220729 11:28:47.000148: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: couldn't get temp CUBIN file name  Relying on driver to perform ptx compilation.   Modify $PATH to customize ptxas location.  This message will be only logged once.  ``` I am not sure what was the exact cause. EDIT: Regarding `Modify $PATH to customize ptxas location` in the warning message, I have already tried many suggested solutions but nothing solved it.","Thanks for the reply. The reboot could have cleared the /tmp directory. So if the spaces available was too low there, it could explain why this error is fixed. If there was a driver update done, but the driver wasn't reloaded a reboot would reload a new version of the driver. This could also fix in theory other issues.  Anyway, it is good to know your issues is fixed. I'll way a little bit to know if the original author also reply. But otherwise, I'll close this as we can't repro and so we can't do more then making the error message better.","Fixed on our side. So closing. If you still have issues, open a new bug.",I do not clear about how to fix the problem. Only to reboot ? Is there any other method?, Can you please open a new issue with a full description of your problem?
3365,"以下是一个github上的jax下的一个issue, 标题是(Bump myst-nb from 0.15.0 to 0.16.0)， 内容是 (Bumps mystnb from 0.15.0 to 0.16.0.  Release notes Sourced from mystnb's releases.  v0.16.0 What's Changed  📚 DOCS: Fix link in README by @​bsipocz in executablebooks/MySTNB CC(add lax.cond (initialstyle implementation)) [precommit.ci] precommit autoupdate by @​precommitci in executablebooks/MySTNB CC(Implement np.gcd and np.lcm.) ⬆️ UPGRADE: Add Python 3.10 support by @​jarrodmillman in executablebooks/MySTNB CC(Implement np.power for integers) ⬆️ UPDATE: Sphinx v5, mystparser v0.18 by @​chrisjsewell in executablebooks/MySTNB CC(Grad should raise an error for noninexact types) 🚀 RELEASE: v0.16.0 by @​chrisjsewell in executablebooks/MySTNB CC(remove unused jax.numpy.array case (was typo))  New Contributors  @​bsipocz made their first contribution in executablebooks/MySTNB CC(add lax.cond (initialstyle implementation)) @​jarrodmillman made their first contribution in executablebooks/MySTNB CC(Implement np.power for integers)  Full Changelog: https://github.com/executablebooks/MySTNB/compare/v0.15.0...v0.16.0    Changelog Sourced from mystnb's changelog.  v0.16.0  20220613 Full changelog  ⬆️ UPGRADE: Sphinx v5 and drop v3 (see changelog), mystparser v0.18 (see changelog) ⬆️ UPGRADE: Add Python 3.10 support     Commits  c72a3a2 🚀 RELEASE: v0.16.0 ( CC(remove unused jax.numpy.array case (was typo))) 8b6ab06 ⬆️ UPDATE: Sphinx v5, mystparser v0.18 ( CC(Grad should raise an error for noninexact types)) 3560d1a ⬆️ UPGRADE: Add Python 3.10 support ( CC(Implement np.power for integers)) f2484fa [precommit.ci] precommit autoupdate ( CC(Implement np.gcd and np.lcm.)) 2428268 📚 DOCS: Fix link in README ( CC(add lax.cond (initialstyle implementation))) See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Bump myst-nb from 0.15.0 to 0.16.0,"Bumps mystnb from 0.15.0 to 0.16.0.  Release notes Sourced from mystnb's releases.  v0.16.0 What's Changed  📚 DOCS: Fix link in README by @​bsipocz in executablebooks/MySTNB CC(add lax.cond (initialstyle implementation)) [precommit.ci] precommit autoupdate by @​precommitci in executablebooks/MySTNB CC(Implement np.gcd and np.lcm.) ⬆️ UPGRADE: Add Python 3.10 support by @​jarrodmillman in executablebooks/MySTNB CC(Implement np.power for integers) ⬆️ UPDATE: Sphinx v5, mystparser v0.18 by @​chrisjsewell in executablebooks/MySTNB CC(Grad should raise an error for noninexact types) 🚀 RELEASE: v0.16.0 by @​chrisjsewell in executablebooks/MySTNB CC(remove unused jax.numpy.array case (was typo))  New Contributors  @​bsipocz made their first contribution in executablebooks/MySTNB CC(add lax.cond (initialstyle implementation)) @​jarrodmillman made their first contribution in executablebooks/MySTNB CC(Implement np.power for integers)  Full Changelog: https://github.com/executablebooks/MySTNB/compare/v0.15.0...v0.16.0    Changelog Sourced from mystnb's changelog.  v0.16.0  20220613 Full changelog  ⬆️ UPGRADE: Sphinx v5 and drop v3 (see changelog), mystparser v0.18 (see changelog) ⬆️ UPGRADE: Add Python 3.10 support     Commits  c72a3a2 🚀 RELEASE: v0.16.0 ( CC(remove unused jax.numpy.array case (was typo))) 8b6ab06 ⬆️ UPDATE: Sphinx v5, mystparser v0.18 ( CC(Grad should raise an error for noninexact types)) 3560d1a ⬆️ UPGRADE: Add Python 3.10 support ( CC(Implement np.power for integers)) f2484fa [precommit.ci] precommit autoupdate ( CC(Implement np.gcd and np.lcm.)) 2428268 📚 DOCS: Fix link in README ( CC(add lax.cond (initialstyle implementation))) See full diff in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-06-20T17:11:21Z,dependencies python,closed,0,3,https://github.com/jax-ml/jax/issues/11177,"Looks OK, but we should also remove the comment above this line as part of the change. (Should have been removed in CC(DOC: update mystnb to v0.15.0))","OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it.",Replaced by CC(CI: build docs with most recent mystnb version)
795,"以下是一个github上的jax下的一个issue, 标题是(Feature request: Building jaxlib (cpu) with threading disabled?)， 内容是 ( Discussed in https://github.com/google/jax/discussions/11130  Originally posted by **josephrocca** June 17, 2022 Hello, I'm currently trying to get JAX (cpu only) working in the browser. This requires compiling with Emscripten, and apparently I need to compile without threading. IIUC it's possible to restrict JAX to a single thread at run time, but I wasn't able to find any info on compiling without threading. I'm wondering if there's a flag for this that I can set somewhere? If not, is there any viable (even if very hacky) way to do it? Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Feature request: Building jaxlib (cpu) with threading disabled?," Discussed in https://github.com/google/jax/discussions/11130  Originally posted by **josephrocca** June 17, 2022 Hello, I'm currently trying to get JAX (cpu only) working in the browser. This requires compiling with Emscripten, and apparently I need to compile without threading. IIUC it's possible to restrict JAX to a single thread at run time, but I wasn't able to find any info on compiling without threading. I'm wondering if there's a flag for this that I can set somewhere? If not, is there any viable (even if very hacky) way to do it? Thanks!",2022-06-19T10:58:40Z,enhancement contributions welcome,open,1,1,https://github.com/jax-ml/jax/issues/11168," do you mean compiling without any references to threading? I don't think that's easily doable in JAX. If you just want fewer threads, there might need to be a few sourcelevel changes possible 1. `tfrt::CreateMultiThreadedWorkQueue(1, 1)` https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/xla/pjrt/tfrt_cpu_pjrt_client.cc;l=133135?q=tfrt_cpu_pjrt_client&ss=tensorflow 2. Set env var NPROC=1 https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/xla/pjrt/utils.cc;l=272?q=DefaultThreadPoolSize&ss=tensorflow Note that you'd need to recompile jaxlib from source. https://jax.readthedocs.io/en/latest/developer.htmlbuildingjaxlibfromsource But even these won't get you to exactly 1 thread."
4187,"以下是一个github上的jax下的一个issue, 标题是(MLIR translation rule for primitive 'remat_call' not found for platform 'iree')， 内容是 (I'm trying to run dalleplayground using the IREE backend, to utilise IREE/Vulkan for GPU support on M1 Mac (https://github.com/google/jax/issues/8074). When running `generate_images()`, Jax raises an error: ``` NotImplementedError: MLIR translation rule for primitive 'remat_call' not found for platform iree ``` Error raised from here:   https://github.com/google/jax/blob/4f5115c288c5d9ed55ae2192be00a3870b6fa65a/jax/interpreters/mlir.pyL932 Traceback:   https://gist.github.com/Birchsan/daaff059214cdde0c777bd36857771fb I have to confess I'm not sure how to distill a minimal repro of this. I tried to research how to serialize my jaxpr to give you an input with which to invoke `jaxpr_subcomp()`, but I'm not sure that's possible. we can reproduce the important part of `jaxpr_subcomp()` like so: ```python from jax.core import Primitive from jax.interpreters.mlir import _platform_specific_lowerings, _lowerings import jax.interpreters.xla as xla from jax.interpreters.xla import _translations import pickle platform = 'iree'  I did a pickle.dumps(eqn.primitive) from  https://github.com/google/jax/blob/4f5115c288c5d9ed55ae2192be00a3870b6fa65a/jax/interpreters/mlir.pyL932 primitive: Primitive = pickle.loads(b'\x80\x04\x95Q\x00\x00\x00\x00\x00\x00\x00\x8c\x08jax.core\x94\x8c\rCallPrimitive\x94\x93\x94)\x81\x94}\x94(\x8c\x04name\x94\x8c\nremat_call\x94\x8c\x04impl\x94h\x00\x8c\tcall_impl\x94\x93\x94ub.') if primitive in _platform_specific_lowerings[platform]:   print(""oh, seems like it worked; I guess there's been a misunderstanding"") elif primitive in xla._backend_specific_translations[platform]:   print(""oh, seems like it worked; I guess there's been a misunderstanding"") elif primitive in _lowerings:   print(""oh, seems like it worked; I guess there's been a misunderstanding"") elif primitive in xla._translations:   print(""oh, seems like it worked; I guess there's been a misunderstanding"") else:   raise NotImplementedError(       f""MLIR translation rule for primitive '{primitive.name}' not ""       f""found for platform {platform}"") ``` Maybe the `print(eqn)` will help (it includes the details of the `remat_call`)? ```python _:f16[] _:f16[] _:f32[] _:f16[] _:f32[] _:f32[] _:f32[] _:f32[] _:i32[] _:f16[1,1,1,64]   _:f16[1,1,1,64] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[]   _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] = remat_call[   call_jaxpr={ lambda ; . let       a:f32[1,1,1,64] = broadcast_in_dim[         broadcast_dimensions=()         shape=(1, 1, 1, 64)       ] 0.0       b:f16[1,1,1,64] = convert_element_type[new_dtype=float16 weak_type=False] a       c:f32[1,1,1,64] = broadcast_in_dim[         broadcast_dimensions=()         shape=(1, 1, 1, 64)       ] inf       d:f16[1,1,1,64] = convert_element_type[new_dtype=float16 weak_type=False] c       e:f32[] = sqrt 64.0       f:f16[] = convert_element_type[new_dtype=float16 weak_type=False] e        = xla_call[call_jaxpr={ lambda ; . let  in () } name=_einsum]         = xla_call[call_jaxpr={ lambda ; . let  in () } name=_einsum]      in (1.0, 1.0, 2048.0, f, 2048.0, 9.999999747378752e06, 0.0, 2048.0, 0, d, b,       9.999999747378752e06, 0.0, 2048.0, 4096.0, 2.0, 1.0, 1.4142135623730951, 2048.0,       9.999999747378752e06, 0.0, 2048.0, 9.999999747378752e06, 0.0, 4096.0) }   concrete=False   differentiated=False   name=core_fn   policy=None   prevent_cse=False ] ``` I have instructions for setting up the dalleplayground repository (but instead of installing IREE release candidate via pip, you'd need to buildfromsource this branch). I suspect that's a bit much though. Using:  jax+jaxlib commit https://github.com/google/jax/commit/345cc19949273cc414d94e6f13d0620b780af465 (which I built myself locally)    latest commit(ish) of iree compiler+runtime. plus small patch for legalizing mhlo.scatter, to get past https://github.com/google/iree/issues/9361 (built myself locally)  dalleplayground launched like so)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,MLIR translation rule for primitive 'remat_call' not found for platform 'iree',"I'm trying to run dalleplayground using the IREE backend, to utilise IREE/Vulkan for GPU support on M1 Mac (https://github.com/google/jax/issues/8074). When running `generate_images()`, Jax raises an error: ``` NotImplementedError: MLIR translation rule for primitive 'remat_call' not found for platform iree ``` Error raised from here:   https://github.com/google/jax/blob/4f5115c288c5d9ed55ae2192be00a3870b6fa65a/jax/interpreters/mlir.pyL932 Traceback:   https://gist.github.com/Birchsan/daaff059214cdde0c777bd36857771fb I have to confess I'm not sure how to distill a minimal repro of this. I tried to research how to serialize my jaxpr to give you an input with which to invoke `jaxpr_subcomp()`, but I'm not sure that's possible. we can reproduce the important part of `jaxpr_subcomp()` like so: ```python from jax.core import Primitive from jax.interpreters.mlir import _platform_specific_lowerings, _lowerings import jax.interpreters.xla as xla from jax.interpreters.xla import _translations import pickle platform = 'iree'  I did a pickle.dumps(eqn.primitive) from  https://github.com/google/jax/blob/4f5115c288c5d9ed55ae2192be00a3870b6fa65a/jax/interpreters/mlir.pyL932 primitive: Primitive = pickle.loads(b'\x80\x04\x95Q\x00\x00\x00\x00\x00\x00\x00\x8c\x08jax.core\x94\x8c\rCallPrimitive\x94\x93\x94)\x81\x94}\x94(\x8c\x04name\x94\x8c\nremat_call\x94\x8c\x04impl\x94h\x00\x8c\tcall_impl\x94\x93\x94ub.') if primitive in _platform_specific_lowerings[platform]:   print(""oh, seems like it worked; I guess there's been a misunderstanding"") elif primitive in xla._backend_specific_translations[platform]:   print(""oh, seems like it worked; I guess there's been a misunderstanding"") elif primitive in _lowerings:   print(""oh, seems like it worked; I guess there's been a misunderstanding"") elif primitive in xla._translations:   print(""oh, seems like it worked; I guess there's been a misunderstanding"") else:   raise NotImplementedError(       f""MLIR translation rule for primitive '{primitive.name}' not ""       f""found for platform {platform}"") ``` Maybe the `print(eqn)` will help (it includes the details of the `remat_call`)? ```python _:f16[] _:f16[] _:f32[] _:f16[] _:f32[] _:f32[] _:f32[] _:f32[] _:i32[] _:f16[1,1,1,64]   _:f16[1,1,1,64] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[]   _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] _:f32[] = remat_call[   call_jaxpr={ lambda ; . let       a:f32[1,1,1,64] = broadcast_in_dim[         broadcast_dimensions=()         shape=(1, 1, 1, 64)       ] 0.0       b:f16[1,1,1,64] = convert_element_type[new_dtype=float16 weak_type=False] a       c:f32[1,1,1,64] = broadcast_in_dim[         broadcast_dimensions=()         shape=(1, 1, 1, 64)       ] inf       d:f16[1,1,1,64] = convert_element_type[new_dtype=float16 weak_type=False] c       e:f32[] = sqrt 64.0       f:f16[] = convert_element_type[new_dtype=float16 weak_type=False] e        = xla_call[call_jaxpr={ lambda ; . let  in () } name=_einsum]         = xla_call[call_jaxpr={ lambda ; . let  in () } name=_einsum]      in (1.0, 1.0, 2048.0, f, 2048.0, 9.999999747378752e06, 0.0, 2048.0, 0, d, b,       9.999999747378752e06, 0.0, 2048.0, 4096.0, 2.0, 1.0, 1.4142135623730951, 2048.0,       9.999999747378752e06, 0.0, 2048.0, 9.999999747378752e06, 0.0, 4096.0) }   concrete=False   differentiated=False   name=core_fn   policy=None   prevent_cse=False ] ``` I have instructions for setting up the dalleplayground repository (but instead of installing IREE release candidate via pip, you'd need to buildfromsource this branch). I suspect that's a bit much though. Using:  jax+jaxlib commit https://github.com/google/jax/commit/345cc19949273cc414d94e6f13d0620b780af465 (which I built myself locally)    latest commit(ish) of iree compiler+runtime. plus small patch for legalizing mhlo.scatter, to get past https://github.com/google/iree/issues/9361 (built myself locally)  dalleplayground launched like so",2022-06-18T19:45:04Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/11166,"Seems that  has looked into this recently, I'll reassign to him","I think this was already fixed by https://github.com/google/jax/commit/aae525c4f7d5b71fefcb8f7c30cb48d4df0712f5 Previously we registered a specific remat rule for each platform, but that meant if a new platform came along, e.g., `iree`, there was no ""default"" behavior. That commit changed things so we register a default rule and override it for the one case that matters (GPU on XLA)."
1866,"以下是一个github上的jax下的一个issue, 标题是(No matching distribution found for jaxlib==0.3.10+cuda11.cudnn82; extra == ""cuda"" (from jax[cuda]))， 内容是 (when i'm typing this command: `pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html` then it returns this error: ``` Looking in links: https://storage.googleapis.com/jaxreleases/jax_releases.html Requirement already uptodate: jax[cuda] in /home/shalev/.local/lib/python3.8/sitepackages (0.3.13) Requirement already satisfied, skipping upgrade: abslpy in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (1.1.0) Requirement already satisfied, skipping upgrade: numpy>=1.19 in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (1.22.4) Requirement already satisfied, skipping upgrade: opteinsum in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (3.3.0) Requirement already satisfied, skipping upgrade: scipy>=1.2.1 in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (1.8.1) Requirement already satisfied, skipping upgrade: typingextensions in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (4.2.0) ERROR: Could not find a version that satisfies the requirement jaxlib==0.3.10+cuda11.cudnn82; extra == ""cuda"" (from jax[cuda]) (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10) ERROR: No matching distribution found for jaxlib==0.3.10+cuda11.cudnn82; extra == ""cuda"" (from jax[cuda]) ``` how can i fix that?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"No matching distribution found for jaxlib==0.3.10+cuda11.cudnn82; extra == ""cuda"" (from jax[cuda])","when i'm typing this command: `pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html` then it returns this error: ``` Looking in links: https://storage.googleapis.com/jaxreleases/jax_releases.html Requirement already uptodate: jax[cuda] in /home/shalev/.local/lib/python3.8/sitepackages (0.3.13) Requirement already satisfied, skipping upgrade: abslpy in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (1.1.0) Requirement already satisfied, skipping upgrade: numpy>=1.19 in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (1.22.4) Requirement already satisfied, skipping upgrade: opteinsum in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (3.3.0) Requirement already satisfied, skipping upgrade: scipy>=1.2.1 in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (1.8.1) Requirement already satisfied, skipping upgrade: typingextensions in /home/shalev/.local/lib/python3.8/sitepackages (from jax[cuda]) (4.2.0) ERROR: Could not find a version that satisfies the requirement jaxlib==0.3.10+cuda11.cudnn82; extra == ""cuda"" (from jax[cuda]) (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10) ERROR: No matching distribution found for jaxlib==0.3.10+cuda11.cudnn82; extra == ""cuda"" (from jax[cuda]) ``` how can i fix that?",2022-06-17T13:07:22Z,bug,closed,1,4,https://github.com/jax-ml/jax/issues/11142,I think the correct URL for CUDA releases is `https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`. Does the install command work if you use that?,More information at https://github.com/google/jaxpipinstallationgpucuda,We recently split the `cuda` index out from the noncuda index to fix https://github.com/google/jax/issues/11087 . The new path has `cuda` in the name. (Perhaps it would have been less disruptive to leave the CUDA wheel index where it was and to have renamed the nonCUDA index. But changing it again is probably just as disruptive.),"Fell into the same problem and spent some days, thinking it could be a server issue soon to be fixed. I guess some folks would experience the same for a while. Bless you if you reach here 😅"
352,"以下是一个github上的jax下的一个issue, 标题是([x64] make lax_numpy_indexing_test pass with strict dtype promotion)， 内容是 (Part of https://github.com/google/jax/pull/10840. Also refactor common helper into `jax._src.test_utils`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[x64] make lax_numpy_indexing_test pass with strict dtype promotion,Part of https://github.com/google/jax/pull/10840. Also refactor common helper into `jax._src.test_utils`.,2022-06-16T21:01:22Z,kokoro:force-run pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/11131
1747,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: module 'jaxlib.gpu_linalg' has no attribute 'rocm_lu_pivots_to_permutation')， 内容是 (Using jax 0.3.13 with jaxlib built from main branch of github (today) on a Windows 10 machine with Cuda 11.2 and python 3.9 I get the following error when importing jax. ``` Traceback (most recent call last):    import jax   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\__init__.py"", line 120, in      from jax.experimental.maps import soft_pmap as soft_pmap   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\experimental\maps.py"", line 26, in      from jax import numpy as jnp   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\numpy\__init__.py"", line 18, in      from jax.numpy import fft as fft   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\numpy\fft.py"", line 15, in      from jax._src.numpy.fft import (   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\_src\numpy\fft.py"", line 19, in      from jax import lax   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\lax\__init__.py"", line 359, in      from jax.lax import linalg as linalg   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\lax\linalg.py"", line 15, in      from jax._src.lax.linalg import (   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\_src\lax\linalg.py"", line 984, in      gpu_linalg.rocm_lu_pivots_to_permutation), AttributeError: module 'jaxlib.gpu_linalg' has no attribute 'rocm_lu_pivots_to_permutation' Process finished with exit code 1 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AttributeError: module 'jaxlib.gpu_linalg' has no attribute 'rocm_lu_pivots_to_permutation',"Using jax 0.3.13 with jaxlib built from main branch of github (today) on a Windows 10 machine with Cuda 11.2 and python 3.9 I get the following error when importing jax. ``` Traceback (most recent call last):    import jax   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\__init__.py"", line 120, in      from jax.experimental.maps import soft_pmap as soft_pmap   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\experimental\maps.py"", line 26, in      from jax import numpy as jnp   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\numpy\__init__.py"", line 18, in      from jax.numpy import fft as fft   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\numpy\fft.py"", line 15, in      from jax._src.numpy.fft import (   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\_src\numpy\fft.py"", line 19, in      from jax import lax   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\lax\__init__.py"", line 359, in      from jax.lax import linalg as linalg   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\lax\linalg.py"", line 15, in      from jax._src.lax.linalg import (   File ""C:\Users\Adam\anaconda3\envs\jax_latest\lib\sitepackages\jax\_src\lax\linalg.py"", line 984, in      gpu_linalg.rocm_lu_pivots_to_permutation), AttributeError: module 'jaxlib.gpu_linalg' has no attribute 'rocm_lu_pivots_to_permutation' Process finished with exit code 1 ```",2022-06-16T00:16:12Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/11118,Solved by upgrading jax itself to code from main branch. 
890,"以下是一个github上的jax下的一个issue, 标题是(BUG: avoid warning when specifying dtype=complex in X32 mode)， 内容是 (Because of a bug in `lax.py`, currently the following produces a warning when it shouldn't: ```python >>> import jax.numpy as jnp >>> jnp.array(1, dtype=complex) /github/google/jax/jax/_src/numpy/lax_numpy.py:1832: UserWarning: Explicitly requested dtype  requested in array is not available, and will be truncated to dtype complex64. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jaxcurrentgotchas for more.   lax_internal._check_user_dtype_supported(dtype, ""array"") DeviceArray(1.+0.j, dtype=complex64) ``` This PR fixes the issue and adds a test.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,BUG: avoid warning when specifying dtype=complex in X32 mode,"Because of a bug in `lax.py`, currently the following produces a warning when it shouldn't: ```python >>> import jax.numpy as jnp >>> jnp.array(1, dtype=complex) /github/google/jax/jax/_src/numpy/lax_numpy.py:1832: UserWarning: Explicitly requested dtype  requested in array is not available, and will be truncated to dtype complex64. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jaxcurrentgotchas for more.   lax_internal._check_user_dtype_supported(dtype, ""array"") DeviceArray(1.+0.j, dtype=complex64) ``` This PR fixes the issue and adds a test.",2022-06-15T22:04:13Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/11117
736,"以下是一个github上的jax下的一个issue, 标题是(Enable bitwise reductions for integer dtypes in `lax.reduce`)， 内容是 (Followup of CC(Add bitwise XOR reducer to `lax.reduce`). This commit extends the existing bitwise reduction primitives to work with all nonfloating dtypes, including signed and unsigned integers. Tests against numpy were added, in similar style to existing test coverage, to assert the correct results for these new reductions. Secondly, existing test coverage for `lax.reduce` was extended to check for the correct primitive being produced in the resulting jaxpr for all `lax.reduce` operations.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Enable bitwise reductions for integer dtypes in `lax.reduce`,"Followup of CC(Add bitwise XOR reducer to `lax.reduce`). This commit extends the existing bitwise reduction primitives to work with all nonfloating dtypes, including signed and unsigned integers. Tests against numpy were added, in similar style to existing test coverage, to assert the correct results for these new reductions. Secondly, existing test coverage for `lax.reduce` was extended to check for the correct primitive being produced in the resulting jaxpr for all `lax.reduce` operations.",2022-06-15T19:34:25Z,pull ready,closed,0,15,https://github.com/jax-ml/jax/issues/11115,"I am seeing a number of test failures locally using `JAX_NUM_GENERATED_CASES=100`, but I wanted to post this to discuss. To me, it looks like the failures can be categorized into three different types: 1) numpy vs jax dtype mismatches 2) numpy vs jax result mismatches for a given tolerance (for some uint{8,16} tests, too) 3) Wrong primitive being produced in the lowerings for some `lax.{min,max}` tests. I would be happy about some feedback!",Would you like to review this ? Since you handled the previous PR as well,"Thanks for the initial review! I incorporated your changes in the latest commit, I hope I understood everything correctly.","Thanks for the followup, I will get on it asap. What do you think about merging the bitwise test from the last PR with this one, while converting it to the `ReducerOpRecord` implementation? That way we can deduplicate some code.","> What do you think about merging the bitwise test from the last PR with this one, while converting it to the ReducerOpRecord implementation? That way we can deduplicate some code. That seems like a good plan.","I pushed your suggested changes again. It seems that now we are getting the dtype mismatch errors that I saw locally. Do you think this is related to type promotion, or something else?","It looks like these are dtype mismatches between the corresponding jax and numpy functions. The issue comes from this: ```python In [1]: from jax import lax                                                                                                                                                                    In [2]: import numpy as np                                                                                                                                                                     In [3]: x = np.arange(10, dtype='int16')                                                                                                                                                       In [4]: np.sum(x).dtype                                                                                                                                                                        Out[4]: dtype('int64') In [5]: lax.reduce([x], [0], lax.add, (0,)).dtype                                                                                                                                              Out[5]: dtype('int16') ``` I'd handle this by ensuring that `reference_fun` returns the expected dtype.","Seems like mypy is unhappy, though that's strange because it's due to the same return pattern used elsewhere in the function. But in general, I find mypy tends to respond to butterflies flapping their wings in Brazil...","Yeah, I saw. It seems to be because unlike elsewhere, the `np.equal` aval check is called on an array as opposed to a scalar. Calling `array.item()` on each of these turns everything green again. By the way, locally with `JAX_NUM_GENERATED_CASES=100`: ``` ====================================================================== FAIL: testReduce_op=bitwise_and_inshape=int8[3,4,5]_reducedims=(0,)_initval=1 (__main__.LaxTest) LaxTest.testReduce_op=bitwise_and_inshape=int8[3,4,5]_reducedims=(0,)_initval=1 testReduce_op=bitwise_and_inshape=int8[3,4,5]_reducedims=(0,)_initval=1(op=, reference_op=, init_val=1, shape=(3, 4, 5), dtype=, dims=(0,), primitive=reduce_and)  (Traceback) AssertionError:  Not equal to tolerance rtol=1e07, atol=0 Mismatched elements: 4 / 20 (20%) Max absolute difference: 6 Max relative difference: 1.  x: array([[0, 1, 0, 0, 0],        [0, 0, 1, 0, 0],        [0, 0, 0, 0, 0],        [0, 0, 0, 0, 0]], dtype=int8)  y: array([[0, 7, 0, 0, 0],        [0, 0, 3, 0, 0],        [2, 0, 0, 0, 0],        [4, 0, 0, 0, 0]], dtype=int8) ====================================================================== FAIL: testReduce_op=bitwise_and_inshape=uint16[3,4,5]_reducedims=(0,)_initval=1 (__main__.LaxTest) LaxTest.testReduce_op=bitwise_and_inshape=uint16[3,4,5]_reducedims=(0,)_initval=1 testReduce_op=bitwise_and_inshape=uint16[3,4,5]_reducedims=(0,)_initval=1(op=, reference_op=, init_val=1, shape=(3, 4, 5), dtype=, dims=(0,), primitive=reduce_and)  Traceback (most recent call last): [...] AssertionError:  Not equal to tolerance rtol=1e07, atol=0 Mismatched elements: 3 / 20 (15%) Max absolute difference: 65534 Max relative difference: 32767.  x: array([[0, 0, 1, 0, 0],        [0, 0, 0, 0, 0],        [0, 0, 0, 0, 0],        [0, 0, 0, 0, 0]], dtype=uint16)  y: array([[0, 2, 1, 0, 0],        [0, 0, 0, 0, 2],        [0, 0, 2, 0, 0],        [0, 0, 0, 0, 0]], dtype=uint16) ====================================================================== FAIL: testReduce_op=bitwise_and_inshape=uint32[3,4,5]_reducedims=(0,)_initval=1 (__main__.LaxTest) LaxTest.testReduce_op=bitwise_and_inshape=uint32[3,4,5]_reducedims=(0,)_initval=1 testReduce_op=bitwise_and_inshape=uint32[3,4,5]_reducedims=(0,)_initval=1(op=, reference_op=, init_val=1, shape=(3, 4, 5), dtype=, dims=(0,), primitive=reduce_and)  Traceback (most recent call last): [...] AssertionError:  Not equal to tolerance rtol=1e07, atol=0 Mismatched elements: 2 / 20 (10%) Max absolute difference: 4294967294 Max relative difference: 2.14748365e+09  x: array([[0, 0, 0, 0, 0],        [0, 0, 0, 0, 0],        [0, 0, 1, 0, 0],        [0, 0, 0, 0, 0]], dtype=uint32)  y: array([[0, 2, 0, 0, 0],        [0, 0, 0, 0, 0],        [0, 0, 1, 0, 0],        [0, 0, 0, 2, 0]], dtype=uint32) ====================================================================== FAIL: testReduce_op=bitwise_and_inshape=uint8[3,4,5]_reducedims=(0,)_initval=1 (__main__.LaxTest) LaxTest.testReduce_op=bitwise_and_inshape=uint8[3,4,5]_reducedims=(0,)_initval=1 testReduce_op=bitwise_and_inshape=uint8[3,4,5]_reducedims=(0,)_initval=1(op=, reference_op=, init_val=1, shape=(3, 4, 5), dtype=, dims=(0,), primitive=reduce_and)  Traceback (most recent call last): [...] AssertionError:  Not equal to tolerance rtol=1e07, atol=0 Mismatched elements: 1 / 20 (5%) Max absolute difference: 254 Max relative difference: 127.  x: array([[0, 0, 0, 0, 0],        [0, 0, 0, 0, 0],        [0, 0, 0, 0, 0],        [0, 0, 0, 0, 0]], dtype=uint8)  y: array([[0, 0, 0, 0, 0],        [0, 0, 0, 2, 0],        [0, 0, 0, 0, 0],        [0, 0, 0, 0, 0]], dtype=uint8) ``` Do you think those are flaky tests, or is there something deeper going on? (There are some other failures, but those are because of disabled x64 mode.)","Thanks  regarding those failures, I don't see any obvious reason why they'd be flakes. I would suggest looking into it to try to understand where the difference is coming from – I suspect it's a real bug.","Alright. I will attempt to work more on it tomorrow. I ran all generated tests, and failures can be summarized by:  `lax.bitwise_and` with `dims=(0,)` and `shape=[3,4,5]`, for dtypes `uint[8,16,32]` and `int[8,16,32]`. Reason is the disagreement of some values within test tolerance.  `lax.{min,max}` for all shapes and dims, for dtypes `int64, uint64`. Reason is that the lowered primitive is `reduce_p`, probably because x64 mode was not enabled. The first batch of errors happening, for reduction dimension (0,) only, but for all integer dtypes, is weird to me. I will keep investigating.",Regarding the 64bit types: we might need to call `dtypes.canonicalize_dtype` somewhere in the mix in order to make this work correctly,"I opted to skip those tests for now if x64 mode is disabled, as that was done in other lax tests (and the `canonicalize_dtype` route has not worked for me so far). For the `bitwise_and` errors, the traceback, unfortunately left out previously, points to the check against the JITcompiled version of `lax.reduce`. So it might be possible that there is something going wrong in XLA? ``` ====================================================================== FAIL: testReduceBare_op=bitwise_and_inshape=uint8[3,4,5]_reducedims=(0,)_initval=1 (__main__.LaxTest) LaxTest.testReduceBare_op=bitwise_and_inshape=uint8[3,4,5]_reducedims=(0,)_initval=1 testReduceBare_op=bitwise_and_inshape=uint8[3,4,5]_reducedims=(0,)_initval=1(op=, reference_op=, init_val=1, shape=(3, 4, 5), dtype=, dims=(0,), primitive=reduce_and)  Traceback (most recent call last):   File ""/Users/nicholasjunge/Workspaces/python/jax/venv/lib/python3.10/sitepackages/absl/testing/parameterized.py"", line 312, in bound_param_test     return test_method(self, **testcase_params)   File ""/Users/nicholasjunge/Workspaces/python/jax/tests/lax_test.py"", line 1767, in testReduceBare     self._CompileAndCheck(fun, args_maker)    < fun is lambda operand, init_val: lax.reduce(operand, init_val, op, dims)   File ""/Users/nicholasjunge/Workspaces/python/jax/jax/_src/test_util.py"", line 838, in _CompileAndCheck     self.assertAllClose(python_ans, monitored_ans, check_dtypes=check_dtypes,   File ""/Users/nicholasjunge/Workspaces/python/jax/jax/_src/test_util.py"", line 787, in assertAllClose     self.assertArraysAllClose(x, y, check_dtypes=False, atol=atol, rtol=rtol,   File ""/Users/nicholasjunge/Workspaces/python/jax/jax/_src/test_util.py"", line 752, in assertArraysAllClose     _assert_numpy_allclose(x, y, atol=atol, rtol=rtol, err_msg=err_msg)   File ""/Users/nicholasjunge/Workspaces/python/jax/jax/_src/public_test_util.py"", line 96, in _assert_numpy_allclose     np.testing.assert_allclose(a, b, **kw, err_msg=err_msg)   File ""/Users/nicholasjunge/Workspaces/python/jax/venv/lib/python3.10/sitepackages/numpy/testing/_private/utils.py"", line 1530, in assert_allclose     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),   File ""/Users/nicholasjunge/Workspaces/python/jax/venv/lib/python3.10/sitepackages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare   AssertionError:  Not equal to tolerance rtol=1e07, atol=0 Mismatched elements: 1 / 20 (5%) Max absolute difference: 254 Max relative difference: 127.  x: array([[0, 1, 0, 0, 0],        [0, 0, 0, 0, 1],        [0, 0, 0, 1, 0],        [0, 0, 0, 0, 0]], dtype=uint8)  y: array([[0, 1, 0, 0, 0],        [0, 0, 0, 0, 1],        [0, 2, 0, 1, 0],        [0, 0, 0, 0, 0]], dtype=uint8) ```","I updated the lowering registration. Sorry for wasting your time there, I should have caught that. Bar two possible improvements (read polishing), I think this is ready:  Substituting `np.array(False, dtype)` to `np.array(0, dtype)` in the `or/xor` lowerings to increase code verbosity for integer types  Adding `_get_{or,and,xor}_identity` functions similar to the existing ones that return the identity element for these ops, again for better readability.","My pleasure, thank you for the help throughout!"
961,"以下是一个github上的jax下的一个issue, 标题是(Can't jit `view` with reinterpret from int32 to int64)， 内容是 (```python import jax import jax.numpy as jnp from jax.experimental import enable_x64 .jit def f(x):     with enable_x64():         return x.view(jnp.int64) f(jnp.arange(10).reshape(5,2)) ``` ```  RuntimeError                              Traceback (most recent call last) main.py in  > 1 pack_fragments(jnp.array(10.01), jnp.array(1))     [... skipping hidden 14 frame] .../python3.9/sitepackages/jax/_src/util.py in safe_map(f, *args)      42   for arg in args[1:]:      43     assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args))) > 44   return list(map(f, *args))      45       46 def unzip2(xys): RuntimeError: INVALID_ARGUMENT: Binary op shiftleft with different element types: u64[1,2] and u32[1,2]. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Can't jit `view` with reinterpret from int32 to int64,"```python import jax import jax.numpy as jnp from jax.experimental import enable_x64 .jit def f(x):     with enable_x64():         return x.view(jnp.int64) f(jnp.arange(10).reshape(5,2)) ``` ```  RuntimeError                              Traceback (most recent call last) main.py in  > 1 pack_fragments(jnp.array(10.01), jnp.array(1))     [... skipping hidden 14 frame] .../python3.9/sitepackages/jax/_src/util.py in safe_map(f, *args)      42   for arg in args[1:]:      43     assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args))) > 44   return list(map(f, *args))      45       46 def unzip2(xys): RuntimeError: INVALID_ARGUMENT: Binary op shiftleft with different element types: u64[1,2] and u32[1,2]. ```",2022-06-14T14:24:11Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/11092,"Using the `experimental` `enable_x64` scope is buggy. (It is tagged `experimental` for a reason.) For now, try enabling x64 mode globally at the start of the program, like this: ``` jax.config.update('jax_enable_x64', True) ```",Thanks! that works  I suppose this should be a feature request.,"This feature is something that we're not going to implement, because to do so would require a deep restructure of how JAX transforms work. The issue you raised here is one of the motivating reasons for our goal to excise the X64 modalism entirely from JAX; you can see https://github.com/google/jax/issues/8178 for more.",I'm going to close this as a duplicate of https://github.com/google/jax/issues/5982; thanks for the report!
1220,"以下是一个github上的jax下的一个issue, 标题是(jaxlib 0.1.61 without CUDA support not available in storage.googleapis.com)， 内容是 (A while ago some of us were depending on jaxlib 0.1.59, but that got removed from https://pypi.org/project/jaxlib/history so the workaround was to upgrade jaxlib to 0.1.61. Since upgrading is not always desirable/possible (i.e. dependencies, backwards compatibility etc), we switched to installing jaxlib 0.1.61 from https://storage.googleapis.com/jaxreleases/jax_releases.html instead of https://pypi.org/project/jaxlib/history in case 0.1.61 gets removed from pypi as well. However, we're getting lots of warnings when loading jax: ``` 20220209 03:52:23.782602: W external/org_tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /backend/erts11.2.2.6/lib: ``` Is there a way to add 0.1.61 for (linux x86_64, linux arm, macos x86_64, macos arm) to https://storage.googleapis.com/ as well?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jaxlib 0.1.61 without CUDA support not available in storage.googleapis.com,"A while ago some of us were depending on jaxlib 0.1.59, but that got removed from https://pypi.org/project/jaxlib/history so the workaround was to upgrade jaxlib to 0.1.61. Since upgrading is not always desirable/possible (i.e. dependencies, backwards compatibility etc), we switched to installing jaxlib 0.1.61 from https://storage.googleapis.com/jaxreleases/jax_releases.html instead of https://pypi.org/project/jaxlib/history in case 0.1.61 gets removed from pypi as well. However, we're getting lots of warnings when loading jax: ``` 20220209 03:52:23.782602: W external/org_tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /backend/erts11.2.2.6/lib: ``` Is there a way to add 0.1.61 for (linux x86_64, linux arm, macos x86_64, macos arm) to https://storage.googleapis.com/ as well?",2022-06-14T06:20:41Z,bug needs info,closed,0,7,https://github.com/jax-ml/jax/issues/11087,"What `pip` command did you run to install `jaxlib`? If you look in the index, there are `nocuda` wheels there, e.g., https://storage.googleapis.com/jaxreleases/nocuda/jaxlib0.1.61cp39nonemanylinux2010_x86_64.whl and in general we are archiving releases in that GCS bucket going forward. I should note we have never released Linux ARM wheels, and while we now release Mac ARM I don't think we did that far back.","> What `pip` command did you run to install `jaxlib`? >  > If you look in the index, there are `nocuda` wheels there, e.g., https://storage.googleapis.com/jaxreleases/nocuda/jaxlib0.1.61cp39nonemanylinux2010_x86_64.whl and in general we are archiving releases in that GCS bucket going forward. >  > I should note we have never released Linux ARM wheels, and while we now release Mac ARM I don't think we did that far back. I'm using `pip install f https://storage.googleapis.com/jaxreleases/jax_releases.html jaxlib==0.1.61`","Ah, I think I see the problem. We use local version tags, e.g. `0.1.61+cuda112` to distinguish the CUDA wheel from the nocuda wheel, e.g., `0.1.61`. Unfortunately `pip` accepts both versions as matching the constraint `==0.1.61`, so you'll randomly get either of them if you install that way. For now, I'd suggest installing a specific wheel file, e.g.,: ``` pip install https://storage.googleapis.com/jaxreleases/nocuda/jaxlib0.1.61cp37nonemanylinux2010_x86_64.whl ``` For the future, we're planning to restructure how our `jaxlib` wheels are structured so the CUDA wheel is a plugin for the noncuda wheel. At that point we won't need this local versioning scheme any more. Admittedly that doesn't help for older `jaxlib` versions; the only other thing we could do there is generate an index `.html` file that contains only the `nocuda` packages that you could point `pip` at. Does the workaround of installing a specific wheel file work for you? (Ultimately we'd prefer you upgraded, of course...)","I see. Correct me if I'm wrong, but in this case the only issue with the above solution is that the command would only work with linux systems, right? My constraint is that I have the same `requirements.txt` file for multiple platforms (linux intel, linux arm, macos intel, macos arm). Also, do you have any idea if and when 0.1.61 would be removed from PyPI? Is that still a concern?","Correct, if you use a specific file URL, it will work only for a particular platform and python version. The reason we remove wheels from pypi is because of space limits, so the answer is ""when we run out of space"". I would advocate you upgrade sooner than that. As a temporary measure, I suggest we add another package index file, with the intention of removing it again at some point when we have separated the CUDA parts of `jaxlib` into their own distinct plugin package."," Could we, as part of our release process, generate a separate `nocuda` package index that contains only the `nocuda` wheels?",Done. Cuda index file: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html All others: https://storage.googleapis.com/jaxreleases/jax_releases.html
5078,"以下是一个github上的jax下的一个issue, 标题是(Bump sphinx-autodoc-typehints from 1.11.1 to 1.18.3)， 内容是 (Bumps sphinxautodoctypehints from 1.11.1 to 1.18.3.  Release notes Sourced from sphinxautodoctypehints's releases.  1.18.3 What's Changed  Fix for new nptyping by @​gaborbernat in toxdev/sphinxautodoctypehints CC(double linalg test tolerances to avoid flakiness)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.18.2...1.18.3 1.18.2 What's Changed  [precommit.ci] precommit autoupdate by @​precommitci in toxdev/sphinxautodoctypehints CC(Indexing bug with multiple index arrays) Support and require nptyping 2.1.1 by @​gaborbernat in toxdev/sphinxautodoctypehints CC(fix isinstance check in indexing (fixes 227))  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.18.1...1.18.2 1.18.1 No release notes provided. 1.18.0 No release notes provided. 1.17.1 No release notes provided. typehints_use_rtype support and handle TypeError No release notes provided. 1.16.0 No release notes provided. 1.15.3 No release notes provided. 1.15.2 No release notes provided. 1.15.1 No release notes provided. 1.15.0 No release notes provided. 1.14.1 No release notes provided. Added document_defaults config option No release notes provided. Fix NewType is inserting a reference as first argument No release notes provided.   ... (truncated)   Changelog Sourced from sphinxautodoctypehints's changelog.  1.18.3  Support and require nptyping&gt;=2.1.2  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked module import not working when used as guarded import  1.18.0  Support and require nptyping&gt;=2 Handle UnionType  1.17.1  Mark it as requiring nptyping&lt;2  1.17.0  Add typehints_use_rtype option Handles TypeError when getting source code via inspect  1.16.0  Add support for type subscriptions with multiple elements, where one or more elements are tuples; e.g., nptyping.NDArray[(Any, ...), nptyping.Float] Fix bug for arbitrary types accepting singleton subscriptions; e.g., nptyping.Float[64] Resolve forward references Expand and better handle TypeVar Add intershpinx reference link for ... to Ellipsis (as is just an alias)  1.15.3  Prevents reaching inner blocks that contains if TYPE_CHECKING  1.15.2  Log a warning instead of crashing when a type guard import fails to resolve When resolving type guard imports if the target module does not have source code (such is the case for Cextension modules) do nothing instead of crashing  1.15.1  Fix fully_qualified should be typehints_fully_qualified  1.15.0   ... (truncated)   Commits  bf27bef Fix for new nptyping ( CC(double linalg test tolerances to avoid flakiness)) 7ee3891 Support and require nptyping 2.1.1 ( CC(fix isinstance check in indexing (fixes 227))) bff0765 [precommit.ci] precommit autoupdate ( CC(Indexing bug with multiple index arrays)) 73aa9b6 Fix mock imports on guarded imports ( CC(Test more Numpy ops for complex types.)) 4d5867d Handle UnionType ( CC(Batching rule for 'sort_key_val' not implemented)) 13ca2b4 [precommit.ci] precommit autoupdate ( CC(Inplace updating can lead to counterintuitive behavior)) def37f7 Support and require nptyping 2 ede082a Require nptyping&lt;2 ( CC(Failing lax_numpy_indexing_test.py tests on Python 3.7)) f9219b2 [precommit.ci] precommit autoupdate ( CC(jax.random.randint range must be valid)) a9b9023 Fix typos ( CC(add examples and docs for how to use lax.conv functions)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bump sphinx-autodoc-typehints from 1.11.1 to 1.18.3,"Bumps sphinxautodoctypehints from 1.11.1 to 1.18.3.  Release notes Sourced from sphinxautodoctypehints's releases.  1.18.3 What's Changed  Fix for new nptyping by @​gaborbernat in toxdev/sphinxautodoctypehints CC(double linalg test tolerances to avoid flakiness)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.18.2...1.18.3 1.18.2 What's Changed  [precommit.ci] precommit autoupdate by @​precommitci in toxdev/sphinxautodoctypehints CC(Indexing bug with multiple index arrays) Support and require nptyping 2.1.1 by @​gaborbernat in toxdev/sphinxautodoctypehints CC(fix isinstance check in indexing (fixes 227))  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.18.1...1.18.2 1.18.1 No release notes provided. 1.18.0 No release notes provided. 1.17.1 No release notes provided. typehints_use_rtype support and handle TypeError No release notes provided. 1.16.0 No release notes provided. 1.15.3 No release notes provided. 1.15.2 No release notes provided. 1.15.1 No release notes provided. 1.15.0 No release notes provided. 1.14.1 No release notes provided. Added document_defaults config option No release notes provided. Fix NewType is inserting a reference as first argument No release notes provided.   ... (truncated)   Changelog Sourced from sphinxautodoctypehints's changelog.  1.18.3  Support and require nptyping&gt;=2.1.2  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked module import not working when used as guarded import  1.18.0  Support and require nptyping&gt;=2 Handle UnionType  1.17.1  Mark it as requiring nptyping&lt;2  1.17.0  Add typehints_use_rtype option Handles TypeError when getting source code via inspect  1.16.0  Add support for type subscriptions with multiple elements, where one or more elements are tuples; e.g., nptyping.NDArray[(Any, ...), nptyping.Float] Fix bug for arbitrary types accepting singleton subscriptions; e.g., nptyping.Float[64] Resolve forward references Expand and better handle TypeVar Add intershpinx reference link for ... to Ellipsis (as is just an alias)  1.15.3  Prevents reaching inner blocks that contains if TYPE_CHECKING  1.15.2  Log a warning instead of crashing when a type guard import fails to resolve When resolving type guard imports if the target module does not have source code (such is the case for Cextension modules) do nothing instead of crashing  1.15.1  Fix fully_qualified should be typehints_fully_qualified  1.15.0   ... (truncated)   Commits  bf27bef Fix for new nptyping ( CC(double linalg test tolerances to avoid flakiness)) 7ee3891 Support and require nptyping 2.1.1 ( CC(fix isinstance check in indexing (fixes 227))) bff0765 [precommit.ci] precommit autoupdate ( CC(Indexing bug with multiple index arrays)) 73aa9b6 Fix mock imports on guarded imports ( CC(Test more Numpy ops for complex types.)) 4d5867d Handle UnionType ( CC(Batching rule for 'sort_key_val' not implemented)) 13ca2b4 [precommit.ci] precommit autoupdate ( CC(Inplace updating can lead to counterintuitive behavior)) def37f7 Support and require nptyping 2 ede082a Require nptyping&lt;2 ( CC(Failing lax_numpy_indexing_test.py tests on Python 3.7)) f9219b2 [precommit.ci] precommit autoupdate ( CC(jax.random.randint range must be valid)) a9b9023 Fix typos ( CC(add examples and docs for how to use lax.conv functions)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-06-13T17:09:40Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/11081,Fails because of guarded imports; e.g. https://github.com/toxdev/sphinxautodoctypehints/blob/main/CHANGELOG.md CC(Batch norm slows down custom convolution),"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
5515,"以下是一个github上的jax下的一个issue, 标题是(Bump actions/setup-python from 3 to 4)， 内容是 (Bumps actions/setuppython from 3 to 4.  Release notes Sourced from actions/setuppython's releases.  v4.0.0 What's Changed  Support for pythonversionfile input:  CC(rename ""minmax"" > ""optimizers"")  Example of usage:  uses: actions/setuppython   with:     pythonversionfile: '.pythonversion'  Read python version from a file  run: python my_script.py  There is no default python version for this setuppython major version, the action requires to specify either pythonversion input or pythonversionfile input. If the pythonversion input is not specified the action will try to read required version from file from pythonversionfile input.  Use pypyX.Y for PyPy pythonversion input:  CC(Error importing jax after certain tensorflow import)  Example of usage:  uses: actions/setuppython   with:     pythonversion: 'pypy3.9'  pypyX.Y kept for backward compatibility  run: python my_script.py    RUNNER_TOOL_CACHE environment variable is equal AGENT_TOOLSDIRECTORY:  CC(Change implementation of negative loglikelihood in README toy example)   Bugfix: create missing pypyX.Y symlinks:  CC(Issue taking gradients through np.where when one of branches is nan.)   PKG_CONFIG_PATH environment variable:  CC(Support tuples in translation rule for zeros_like_p.)   Added pythonpath output:  CC(Remove obsolete workarounds for bugs that seem fixed.) pythonpath output contains Python executable path.   Updated zeit/ncc to vercel/ncc package:  CC(override __new__ in dtype classes)   Bugfix: fixed output for prerelease version of poetry:  CC(Use a regular import to add jax.__version__ rather than exec() trickery.)   Made pythonLocation environment variable consistent for Python and PyPy:  CC(Document jax.disable_jit. Add an example to jax.grad.)   Bugfix for 3.xdev syntax:  CC(Default XLA/GPU memory allocator is synchronous/slow.)   Other improvements:  CC(actually fix nondeterminism in einsum)  CC(clarification to README)  CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.)  CC(Hessian calculation finds an UnshapedArray when jitted.)  CC(__invert__ doesn't take an argument.)   Update actions/cache version to 2.0.2 In scope of this release we updated actions/cache package as the new version contains fixes related to GHES 3.5 (actions/setuppython CC(Improve JAX API docs.)) Add &quot;cachehit&quot; output and fix &quot;pythonversion&quot; output for PyPy This release introduces new output cachehit (actions/setuppython CC(add jax.random to jax.readthedocs.io)) and fix pythonversion output for PyPy (actions/setuppython CC(add support for list arguments to jax.numpy funcs, like np.sin([0.2, 0.4]))) The cachehit output contains boolean value indicating that an exact match was found for the key. It shows that the action uses already existing cache or not. The output is available only if cache is enabled.   ... (truncated)   Commits  d09bd5e fix: 3.xdev can install a 3.y version ( CC(Default XLA/GPU memory allocator is synchronous/slow.)) f72db17 Made env.var pythonLocation consistent for Python and PyPy ( CC(Document jax.disable_jit. Add an example to jax.grad.)) 53e1529 add support for pythonversionfile ( CC(rename ""minmax"" > ""optimizers"")) 3f82819 Fix output for prerelease version of poetry ( CC(Use a regular import to add jax.__version__ rather than exec() trickery.)) 397252c Update zeit/ncc to vercel/ncc ( CC(override __new__ in dtype classes)) de977ad Merge pull request  CC(Implement np.gcd and np.lcm.) from vsafonkin/vvsafonkin/fixpoetrycachetest 22c6af9 Change PyPy version to rebuild cache 081a3cf Merge pull request  CC(Remove obsolete workarounds for bugs that seem fixed.) from mayeut/interpreterpath ff70656 feature: add a pythonpath output fff15a2 Use pypyX.Y for PyPy pythonversion input ( CC(Error importing jax after certain tensorflow import)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,Bump actions/setup-python from 3 to 4,"Bumps actions/setuppython from 3 to 4.  Release notes Sourced from actions/setuppython's releases.  v4.0.0 What's Changed  Support for pythonversionfile input:  CC(rename ""minmax"" > ""optimizers"")  Example of usage:  uses: actions/setuppython   with:     pythonversionfile: '.pythonversion'  Read python version from a file  run: python my_script.py  There is no default python version for this setuppython major version, the action requires to specify either pythonversion input or pythonversionfile input. If the pythonversion input is not specified the action will try to read required version from file from pythonversionfile input.  Use pypyX.Y for PyPy pythonversion input:  CC(Error importing jax after certain tensorflow import)  Example of usage:  uses: actions/setuppython   with:     pythonversion: 'pypy3.9'  pypyX.Y kept for backward compatibility  run: python my_script.py    RUNNER_TOOL_CACHE environment variable is equal AGENT_TOOLSDIRECTORY:  CC(Change implementation of negative loglikelihood in README toy example)   Bugfix: create missing pypyX.Y symlinks:  CC(Issue taking gradients through np.where when one of branches is nan.)   PKG_CONFIG_PATH environment variable:  CC(Support tuples in translation rule for zeros_like_p.)   Added pythonpath output:  CC(Remove obsolete workarounds for bugs that seem fixed.) pythonpath output contains Python executable path.   Updated zeit/ncc to vercel/ncc package:  CC(override __new__ in dtype classes)   Bugfix: fixed output for prerelease version of poetry:  CC(Use a regular import to add jax.__version__ rather than exec() trickery.)   Made pythonLocation environment variable consistent for Python and PyPy:  CC(Document jax.disable_jit. Add an example to jax.grad.)   Bugfix for 3.xdev syntax:  CC(Default XLA/GPU memory allocator is synchronous/slow.)   Other improvements:  CC(actually fix nondeterminism in einsum)  CC(clarification to README)  CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.)  CC(Hessian calculation finds an UnshapedArray when jitted.)  CC(__invert__ doesn't take an argument.)   Update actions/cache version to 2.0.2 In scope of this release we updated actions/cache package as the new version contains fixes related to GHES 3.5 (actions/setuppython CC(Improve JAX API docs.)) Add &quot;cachehit&quot; output and fix &quot;pythonversion&quot; output for PyPy This release introduces new output cachehit (actions/setuppython CC(add jax.random to jax.readthedocs.io)) and fix pythonversion output for PyPy (actions/setuppython CC(add support for list arguments to jax.numpy funcs, like np.sin([0.2, 0.4]))) The cachehit output contains boolean value indicating that an exact match was found for the key. It shows that the action uses already existing cache or not. The output is available only if cache is enabled.   ... (truncated)   Commits  d09bd5e fix: 3.xdev can install a 3.y version ( CC(Default XLA/GPU memory allocator is synchronous/slow.)) f72db17 Made env.var pythonLocation consistent for Python and PyPy ( CC(Document jax.disable_jit. Add an example to jax.grad.)) 53e1529 add support for pythonversionfile ( CC(rename ""minmax"" > ""optimizers"")) 3f82819 Fix output for prerelease version of poetry ( CC(Use a regular import to add jax.__version__ rather than exec() trickery.)) 397252c Update zeit/ncc to vercel/ncc ( CC(override __new__ in dtype classes)) de977ad Merge pull request  CC(Implement np.gcd and np.lcm.) from vsafonkin/vvsafonkin/fixpoetrycachetest 22c6af9 Change PyPy version to rebuild cache 081a3cf Merge pull request  CC(Remove obsolete workarounds for bugs that seem fixed.) from mayeut/interpreterpath ff70656 feature: add a pythonpath output fff15a2 Use pypyX.Y for PyPy pythonversion input ( CC(Error importing jax after certain tensorflow import)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-06-13T17:09:31Z,pull ready dependencies github_actions,closed,0,0,https://github.com/jax-ml/jax/issues/11080
11772,"以下是一个github上的jax下的一个issue, 标题是(Segmentation fault in a multithreaded application.)， 内容是 (Please:  [x] Check for duplicate issues.  [ ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: I am sorry for not having a deadsimple example that can be used to reproduce this issue at this point. It seems to happen in a specific setting where I tried to train a pixelbased agent with DeepMind's Acme library.  The code to reproduce the problem can be found in https://github.com/deepmind/acme/issues/233, and the original issue is https://github.com/deepmind/acme/issues/235, another related issue may be https://github.com/google/jax/discussions/10763.  I will try to narrow done to the simplest example that can cause this, but at the moment I haven't successfully been able to do so. For this specific application, I am running JAX on both CPU and GPU concurrently on a single process with multithreading. In particular, I am working with a standard RL actor learner setup where the acting happens on the CPU and learning happens on the GPU.  The actor loads the latest weights from the learner asynchronously with a ThreadPoolExecutor. I have 2 3080 on my machine, but I only use a single GPU for training. I am using `jax[cuda]==0.3.6`.  [x] If applicable, include full error messages/tracebacks. I set a gdb session to catch the segmentation fault, and here's the backtrace from the segfault thread. The segmentation fault seems to come from the xla_extension.so in jaxlib. ``` CC(未找到相关数据)  0x00000000005b9fa3 in PyObject_Malloc () CC(Python 3 compatibility issues)  0x00000000005a9c18 in PyType_GenericAlloc () CC(Explicit tuples are not valid function parameters in Python 3)  0x00007fff7165a5bd in xla::PyBuffer::Make(std::shared_ptr, std::shared_ptr, std::shared_ptr) ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from ..core import JaxTuple)  0x00007fff71666228 in xla::PyClient::BufferFromPyval(pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics) ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from six.moves import xrange)  0x00007fff713f608d in pybind11::cpp_function::initialize, xla::PyClient, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v>(tensorflow::StatusOr (xla::PyClient::*)(pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&)::{lambda(xla::PyClient*, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics) CC(Python 3 compatibility issues)}, tensorflow::StatusOr, xla::PyClient*, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v>(pybind11::cpp_function::initialize, xla::PyClient, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v>(tensorflow::StatusOr (xla::PyClient::*)(pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&)::{lambda(xla::PyClient*, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics) CC(Python 3 compatibility issues)}&&, tensorflow::StatusOr (*)(xla::PyClient*, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&)::{lambda(pybind11::detail::function_call&) CC(Undefined name: from ..core import JaxTuple)}::operator()(pybind11::detail::function_call) const ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Building on OSX with CUDA)  0x00007fff713edc7b in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Made a shim to handle configuration without having absl parse flags)  0x00000000005f3989 in PyCFunction_Call () CC(Quickish check)  0x00000000005f3e1e in _PyObject_MakeTpCall () CC(Quickish check)  0x000000000050b183 in ?? () CC(Adding quickstart notebook, and corresponding gitignore rules)  0x0000000000570035 in _PyEval_EvalFrameDefault () CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Split out `jax` and `jaxlib` packages) 0x000000000056b0ae in _PyEval_EvalFrameDefault () CC(Update the quickstart notebook.) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(Fixing logo size so resize is not required) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Add copyright notice to quickstart notebook.) 0x00000000005f3547 in PyObject_Call () CC(rename in_bdims, out_bdims > in_axes, out_axes) 0x000000000056c8cd in _PyEval_EvalFrameDefault () Type  for more, q to quit, c to continue without paging CC(Add wheelbuilding scripts) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Implement np.repeat for scalar repeats.) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Populate readme) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Notebook showing how to write gufuncs with vmap) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Fix link in gufuncs notebook) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(Typo) 0x000000000050aaa0 in ?? () CC(differention > differentiation) 0x000000000056c28c in _PyEval_EvalFrameDefault () CC(Typo, Python parens) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(attempt to centerjustify the jax logo in readme) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Barebones neural network and data loading example notebook) 0x00000000005f3547 in PyObject_Call () CC(fix symbolic zero handling in concat transpose) 0x000000000056c8cd in _PyEval_EvalFrameDefault () CC(Cloud TPU Support) 0x00000000005006d4 in ?? () CC(examples/datasets.py doesn’t work in python3) 0x0000000000510b02 in PyIter_Next () CC(Add support for `np.trace` ) 0x00007fff7146c6a6 in pybind11::iterator::advance() () from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Error on NaN?) 0x00007fff7160c4d5 in pybind11::object xla::PyTreeDef::UnflattenImpl(pybind11::iterable) const ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Bug in examples?) 0x00007fff7160c98d in xla::PyTreeDef::Unflatten(pybind11::iterable) const ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Fix the bug in classifier example, batching_test and README) 0x00007fff7160854b in pybind11::cpp_function::initialize(pybind11::object (xla::PyTreeDef::*)(pybind11::iterable) const, pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(xla::PyTreeDef const*, pybind11::iterable) CC(Python 3 compatibility issues)}, pybind11::object, xla::PyTreeDef const*, pybind11::iterable, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11::cpp_function::initialize(pybind11::object (xla::PyTreeDef::*)(pybind11::iterable) const, pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(xla::PyTreeDef const*, pybind11::iterable) CC(Python 3 compatibility issues)}&&, pybind11::object (*)(xla::PyTreeDef const*, pybind11::iterable), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&) CC(Undefined name: from ..core import JaxTuple)}::_FUN(pybind11::detail::function_call) () from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Broadcasting of size0 dimensions not implemented) 0x00007fff713edc7b in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(minor spelling tweaks) 0x00000000005f3989 in PyCFunction_Call () CC(CUDA90 and py3 ) 0x00000000005f3e1e in _PyObject_MakeTpCall () CC(add dot_general batching rule) 0x000000000050b183 in ?? () CC(np.einsum support) 0x0000000000570035 in _PyEval_EvalFrameDefault () CC(Require protobuf 3.6.0 or later) 0x000000000056939a in _PyEval_EvalCodeWithName () Type  for more, q to quit, c to continue without paging CC(Hard crash when no compatible cuda devices found) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") 0x000000000056b0ae in _PyEval_EvalFrameDefault () CC(Fix support for arrays with size0 dimensions.) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(Set distinct_host_configuration=false in the bazel options.) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Open Source Contributions) 0x0000000000570035 in _PyEval_EvalFrameDefault () CC(np.linalg.inv support) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Feature request: export TF ops) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Update XLA and reenable numpy tests that failed on Mac) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(jacrev and jacfwd usage example) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Unimplemented: binary integer op 'power') 0x000000000056b0ae in _PyEval_EvalFrameDefault () CC(Update neural_network_and_data_loading.ipynb) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Update README.md) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(add docstrings for major public functions) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Scenarios to prefer over cupy) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(More informative error on trying to concatenate zerodimensional arrays) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Batching rules for pad and concatenate primitives not implemented) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(np.rot90 support) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Improving jax.scipy.stats) 0x00000000005f3547 in PyObject_Call () CC(v0.2 tasks) 0x000000000056c8cd in _PyEval_EvalFrameDefault () CC(Frequenty asked questions doc) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Autodiff cookbook) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Vmap cookbook) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Docstrings in api.py) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Upload existing JAX talks to docs/ or talks/ folder) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Keras NumPy backend demo) 0x000000000050aa2c in ?? () CC(JAX philosophy, or ""culture and values"" doc) 0x00000000005f3547 in PyObject_Call () CC(Rename jaxlib to xlapy) 0x0000000000655a9c in ?? () CC(Conda installations) 0x0000000000675738 in ?? () CC(Open source tests) 0x00007ffff7da0609 in start_thread (arg=) at pthread_create.c:477 CC(Travis CI automated testing) 0x00007ffff7eda163 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95 ``` Is there any guess for why this happens in my case?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Segmentation fault in a multithreaded application.,"Please:  [x] Check for duplicate issues.  [ ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: I am sorry for not having a deadsimple example that can be used to reproduce this issue at this point. It seems to happen in a specific setting where I tried to train a pixelbased agent with DeepMind's Acme library.  The code to reproduce the problem can be found in https://github.com/deepmind/acme/issues/233, and the original issue is https://github.com/deepmind/acme/issues/235, another related issue may be https://github.com/google/jax/discussions/10763.  I will try to narrow done to the simplest example that can cause this, but at the moment I haven't successfully been able to do so. For this specific application, I am running JAX on both CPU and GPU concurrently on a single process with multithreading. In particular, I am working with a standard RL actor learner setup where the acting happens on the CPU and learning happens on the GPU.  The actor loads the latest weights from the learner asynchronously with a ThreadPoolExecutor. I have 2 3080 on my machine, but I only use a single GPU for training. I am using `jax[cuda]==0.3.6`.  [x] If applicable, include full error messages/tracebacks. I set a gdb session to catch the segmentation fault, and here's the backtrace from the segfault thread. The segmentation fault seems to come from the xla_extension.so in jaxlib. ``` CC(未找到相关数据)  0x00000000005b9fa3 in PyObject_Malloc () CC(Python 3 compatibility issues)  0x00000000005a9c18 in PyType_GenericAlloc () CC(Explicit tuples are not valid function parameters in Python 3)  0x00007fff7165a5bd in xla::PyBuffer::Make(std::shared_ptr, std::shared_ptr, std::shared_ptr) ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from ..core import JaxTuple)  0x00007fff71666228 in xla::PyClient::BufferFromPyval(pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics) ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from six.moves import xrange)  0x00007fff713f608d in pybind11::cpp_function::initialize, xla::PyClient, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v>(tensorflow::StatusOr (xla::PyClient::*)(pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&)::{lambda(xla::PyClient*, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics) CC(Python 3 compatibility issues)}, tensorflow::StatusOr, xla::PyClient*, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v>(pybind11::cpp_function::initialize, xla::PyClient, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::arg, pybind11::arg_v, pybind11::arg_v, pybind11::arg_v>(tensorflow::StatusOr (xla::PyClient::*)(pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&)::{lambda(xla::PyClient*, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics) CC(Python 3 compatibility issues)}&&, tensorflow::StatusOr (*)(xla::PyClient*, pybind11::handle, xla::PjRtDevice*, bool, xla::PjRtClient::HostBufferSemantics), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::arg const&, pybind11::arg_v const&, pybind11::arg_v const&, pybind11::arg_v const&)::{lambda(pybind11::detail::function_call&) CC(Undefined name: from ..core import JaxTuple)}::operator()(pybind11::detail::function_call) const ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Building on OSX with CUDA)  0x00007fff713edc7b in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Made a shim to handle configuration without having absl parse flags)  0x00000000005f3989 in PyCFunction_Call () CC(Quickish check)  0x00000000005f3e1e in _PyObject_MakeTpCall () CC(Quickish check)  0x000000000050b183 in ?? () CC(Adding quickstart notebook, and corresponding gitignore rules)  0x0000000000570035 in _PyEval_EvalFrameDefault () CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Split out `jax` and `jaxlib` packages) 0x000000000056b0ae in _PyEval_EvalFrameDefault () CC(Update the quickstart notebook.) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(Fixing logo size so resize is not required) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Add copyright notice to quickstart notebook.) 0x00000000005f3547 in PyObject_Call () CC(rename in_bdims, out_bdims > in_axes, out_axes) 0x000000000056c8cd in _PyEval_EvalFrameDefault () Type  for more, q to quit, c to continue without paging CC(Add wheelbuilding scripts) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Implement np.repeat for scalar repeats.) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Populate readme) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Notebook showing how to write gufuncs with vmap) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Fix link in gufuncs notebook) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(Typo) 0x000000000050aaa0 in ?? () CC(differention > differentiation) 0x000000000056c28c in _PyEval_EvalFrameDefault () CC(Typo, Python parens) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(attempt to centerjustify the jax logo in readme) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Barebones neural network and data loading example notebook) 0x00000000005f3547 in PyObject_Call () CC(fix symbolic zero handling in concat transpose) 0x000000000056c8cd in _PyEval_EvalFrameDefault () CC(Cloud TPU Support) 0x00000000005006d4 in ?? () CC(examples/datasets.py doesn’t work in python3) 0x0000000000510b02 in PyIter_Next () CC(Add support for `np.trace` ) 0x00007fff7146c6a6 in pybind11::iterator::advance() () from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Error on NaN?) 0x00007fff7160c4d5 in pybind11::object xla::PyTreeDef::UnflattenImpl(pybind11::iterable) const ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Bug in examples?) 0x00007fff7160c98d in xla::PyTreeDef::Unflatten(pybind11::iterable) const ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Fix the bug in classifier example, batching_test and README) 0x00007fff7160854b in pybind11::cpp_function::initialize(pybind11::object (xla::PyTreeDef::*)(pybind11::iterable) const, pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(xla::PyTreeDef const*, pybind11::iterable) CC(Python 3 compatibility issues)}, pybind11::object, xla::PyTreeDef const*, pybind11::iterable, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11::cpp_function::initialize(pybind11::object (xla::PyTreeDef::*)(pybind11::iterable) const, pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(xla::PyTreeDef const*, pybind11::iterable) CC(Python 3 compatibility issues)}&&, pybind11::object (*)(xla::PyTreeDef const*, pybind11::iterable), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&) CC(Undefined name: from ..core import JaxTuple)}::_FUN(pybind11::detail::function_call) () from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Broadcasting of size0 dimensions not implemented) 0x00007fff713edc7b in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()    from /home/yicheng/virtualenvs/orlb/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(minor spelling tweaks) 0x00000000005f3989 in PyCFunction_Call () CC(CUDA90 and py3 ) 0x00000000005f3e1e in _PyObject_MakeTpCall () CC(add dot_general batching rule) 0x000000000050b183 in ?? () CC(np.einsum support) 0x0000000000570035 in _PyEval_EvalFrameDefault () CC(Require protobuf 3.6.0 or later) 0x000000000056939a in _PyEval_EvalCodeWithName () Type  for more, q to quit, c to continue without paging CC(Hard crash when no compatible cuda devices found) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") 0x000000000056b0ae in _PyEval_EvalFrameDefault () CC(Fix support for arrays with size0 dimensions.) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(Set distinct_host_configuration=false in the bazel options.) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Open Source Contributions) 0x0000000000570035 in _PyEval_EvalFrameDefault () CC(np.linalg.inv support) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Feature request: export TF ops) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Update XLA and reenable numpy tests that failed on Mac) 0x000000000056939a in _PyEval_EvalCodeWithName () CC(jacrev and jacfwd usage example) 0x00000000005f6a13 in _PyFunction_Vectorcall () CC(Unimplemented: binary integer op 'power') 0x000000000056b0ae in _PyEval_EvalFrameDefault () CC(Update neural_network_and_data_loading.ipynb) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Update README.md) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(add docstrings for major public functions) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Scenarios to prefer over cupy) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(More informative error on trying to concatenate zerodimensional arrays) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Batching rules for pad and concatenate primitives not implemented) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(np.rot90 support) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Improving jax.scipy.stats) 0x00000000005f3547 in PyObject_Call () CC(v0.2 tasks) 0x000000000056c8cd in _PyEval_EvalFrameDefault () CC(Frequenty asked questions doc) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Autodiff cookbook) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Vmap cookbook) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Docstrings in api.py) 0x000000000056b1da in _PyEval_EvalFrameDefault () CC(Upload existing JAX talks to docs/ or talks/ folder) 0x00000000005f6836 in _PyFunction_Vectorcall () CC(Keras NumPy backend demo) 0x000000000050aa2c in ?? () CC(JAX philosophy, or ""culture and values"" doc) 0x00000000005f3547 in PyObject_Call () CC(Rename jaxlib to xlapy) 0x0000000000655a9c in ?? () CC(Conda installations) 0x0000000000675738 in ?? () CC(Open source tests) 0x00007ffff7da0609 in start_thread (arg=) at pthread_create.c:477 CC(Travis CI automated testing) 0x00007ffff7eda163 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95 ``` Is there any guess for why this happens in my case?",2022-06-11T15:50:58Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/11066
698,"以下是一个github上的jax下的一个issue, 标题是(Add an undocumented method on jit() functions to clear the function c…)， 内容是 (…ache. Some users were trying to use the internal method `f._cache_clear()` to clear the function cache associated with a `jit`. Unfortunately that method only clears the C++ cache, whereas to actually evict functions one must clear a Pythonlevel cache as well. Add a new `f.clear_cache()` method that clears both caches. Fixes https://github.com/google/jax/issues/10878 Somewhat related to issue https://github.com/google/jax/issues/10828 as well.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add an undocumented method on jit() functions to clear the function c…,"…ache. Some users were trying to use the internal method `f._cache_clear()` to clear the function cache associated with a `jit`. Unfortunately that method only clears the C++ cache, whereas to actually evict functions one must clear a Pythonlevel cache as well. Add a new `f.clear_cache()` method that clears both caches. Fixes https://github.com/google/jax/issues/10878 Somewhat related to issue https://github.com/google/jax/issues/10828 as well.",2022-06-11T01:43:23Z,pull ready,closed,3,0,https://github.com/jax-ml/jax/issues/11064
1053,"以下是一个github上的jax下的一个issue, 标题是(Feature request: Add support for Chebyshev and other polynomials)， 内容是 (I am working on a project that utilizes Chebyshev polynomials and might use other types of polynomials in the future, and I would like to leverage jit and auto differentiation through JAX. numpy supports Chebyshev and other polynomial types through the new polynomial package API, but this package has not been implemented in JAX. There was some brief discussion in CC(Unimplemented NumPy core functions) about whether it was even desirable to implement this interface in JAX, and I was wondering if there were any updates or a conclusion to this discussion. I have already started to implement the Chebyshev convenience class and convenience functions (chebadd, chebmul, etc.) in a branch for internal use, but merging them into the main branch would be great for longterm support and ease of access. Thanks,)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Feature request: Add support for Chebyshev and other polynomials,"I am working on a project that utilizes Chebyshev polynomials and might use other types of polynomials in the future, and I would like to leverage jit and auto differentiation through JAX. numpy supports Chebyshev and other polynomial types through the new polynomial package API, but this package has not been implemented in JAX. There was some brief discussion in CC(Unimplemented NumPy core functions) about whether it was even desirable to implement this interface in JAX, and I was wondering if there were any updates or a conclusion to this discussion. I have already started to implement the Chebyshev convenience class and convenience functions (chebadd, chebmul, etc.) in a branch for internal use, but merging them into the main branch would be great for longterm support and ease of access. Thanks,",2022-06-10T15:42:43Z,enhancement contributions welcome,open,0,7,https://github.com/jax-ml/jax/issues/11055,"In general, if something is in the NumPy API and it's useful, we welcome PRs! I don't know whether or not those particular APIs will be ergonomic or useful given the various constraints of JAX, but the best way to find that out would be to write a prototype, as you are doing. Let us know!","I think the Chebyshev code is ready for a PR, but I'm running into some issues with the tests. pytest n auto tests/ gets to about 7075% before one of the nodes crashes and is replaced. I can't see progress numbers after that, but the tests continue for a while longer before stalling forever or crashing. This happens both with my additions and with a clean copy of the main JAX branch. I'm working on an 8yearold laptop, so the hardware might be the problem. Any suggestions?","I think the best thing you can do at this point is narrow it down to a single test that fails, and then share instructions to reproduce and I can take a look. Crashing is never supposed to happen, so that's a bug.","Running all of the tests individually one by one worked fine, for both the main branch and for my modified branch, so I think the crashes and stalling must have been hardware issues on my end. I did get one failure in pjit_test.py for both versions of the library though, which reported that ""Some donated buffers were not usable"" and ""Donation is not implemented for cpu"". This might be a bug? I would have expected this test to be skipped if it wasn't supported on CPU only, but perhaps not.","If you have a way for me to reproduce the crash, even using many tests, I'd like to try. JAX should never crash. The buffer donation warning sounds like there's a test that should have been disabled but wasn't. Which test was it?","I've been unable to reproduce the crash, but the test that is failing is testLowerDonateArgnumsAvailable in pjit_test.py.",After the closure of CC(Add shape checks for lax.fft.) I finally got around to implementing my own package for working with polynomials in jax: https://github.com/f0uriest/orthax Mostly replicates the API of ``numpy.polynomial`` and plan on extending beyond that.
448,"以下是一个github上的jax下的一个issue, 标题是([x64] preserve weak types in promote_dtypes_inexact)， 内容是 (Why? Currently `jnp.sqrt(2.0)` returns a weaklytyped value, but `jnp.sqrt(2)` does not. This seems strange. ~I'm trying this change to see how many things it breaks.~ Edit: TGP is clean, I think this is good to go in!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[x64] preserve weak types in promote_dtypes_inexact,"Why? Currently `jnp.sqrt(2.0)` returns a weaklytyped value, but `jnp.sqrt(2)` does not. This seems strange. ~I'm trying this change to see how many things it breaks.~ Edit: TGP is clean, I think this is good to go in!",2022-06-09T21:10:13Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/11046,TGP passed
843,"以下是一个github上的jax下的一个issue, 标题是(`jax.numpy.interp` will crash)， 内容是 (`jax.numpy.interp` will crash ```python import jax def fn(arg_0, arg_1, arg_2, period):     return jax.numpy.interp(arg_0, arg_1, arg_2, period=period) arg_0 = jax.numpy.array(3.9354296, dtype=jax.numpy.float32) mykey = jax.random.PRNGKey(39933342) arg_1 = jax.random.uniform(mykey, [20], jax.numpy.float32, minval=2, maxval=8) mykey = jax.random.PRNGKey(45971415) mykey_ = jax.random.PRNGKey(59345104) arg_2 = jax.lax.complex(jax.random.uniform(mykey, [20], jax.numpy.float64, minval=16, maxval=128), jax.random.uniform(mykey_, [20], jax.numpy.float64, minval=16, maxval=128)) period = 0.59 fn(arg_0, arg_1, arg_2, period)  crash ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.numpy.interp` will crash,"`jax.numpy.interp` will crash ```python import jax def fn(arg_0, arg_1, arg_2, period):     return jax.numpy.interp(arg_0, arg_1, arg_2, period=period) arg_0 = jax.numpy.array(3.9354296, dtype=jax.numpy.float32) mykey = jax.random.PRNGKey(39933342) arg_1 = jax.random.uniform(mykey, [20], jax.numpy.float32, minval=2, maxval=8) mykey = jax.random.PRNGKey(45971415) mykey_ = jax.random.PRNGKey(59345104) arg_2 = jax.lax.complex(jax.random.uniform(mykey, [20], jax.numpy.float64, minval=16, maxval=128), jax.random.uniform(mykey_, [20], jax.numpy.float64, minval=16, maxval=128)) period = 0.59 fn(arg_0, arg_1, arg_2, period)  crash ```",2022-06-07T05:24:33Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/11008,Thanks for the report; I think this has already been fixed on the main branch as part of https://github.com/google/jax/pull/10931. Can you confirm?,I confirmed this no longer crashes at head. We'll probably make a new jax release soon that includes the fix.
368,"以下是一个github上的jax下的一个issue, 标题是(Update testReducerNoDtype and others to not run duplicate tests.)， 内容是 (ndarray.item() should return a python type (eg: float, complex, int) and thus many PYTHON_SCALAR_SHAPE types are duplicated.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update testReducerNoDtype and others to not run duplicate tests.,"ndarray.item() should return a python type (eg: float, complex, int) and thus many PYTHON_SCALAR_SHAPE types are duplicated.",2022-06-07T05:06:17Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/11006,e.g. maybe we could change it so that our shape enumerations look like `for shape in all_shapes(dtype)`. What do you think?,"The reason why this only affects the bfloat16 is that the test goes like so: v = input.item() v = np.array(v) ... return v There is a special case for bf16 (just these nan tests) that goes like so: v = input.item() v = np.array(v) v = np.array(v, dtype=float32) ... v = np.array(v, dtype=bfloat16) return v the item() change makes the type of blah.item() become float instead of bfloat16. "
1601,"以下是一个github上的jax下的一个issue, 标题是(Feature request: JAX implementation of scipy.special.jv)， 内容是 (Hi, I'm working on a research problem that involves computations of Bessel functions of the first kind, of multiple different integer orders, and I'm trying to leverage JAX for autodiff and fast computation on GPUs.  The specific function required by the equations I'm working with is implemented by scipy in scipy.special.jv. JAX's implementation of scipy.special exposes a limited amount of functionality associated with Bessel functions  the ones I could find are i0, i1, i0e, and i1e, but not any Bessel J.  A similar Bessel function related issue has been raised in the past, at https://github.com/google/jax/issues/2466, where someone requested a JAX implementation of scipy.special.iv. The accepted solution for that issue was to use the TensorFlow Probability library implementation of ive. Unfortunately, this solution does not seem to work in my case: TFP does not seem to have an implementation of jv, and I cannot convert between ive and jv since the TFP implementation of ive does not accept complex arguments as inputs. Thus I'd like to request that JAX (or TFP) implement scipy.special.jv. Alternatively, if someone could point me to a way I could translate one of the existing third party Bessel function implementations (eg. those provided by scipy) into a JAXcomposable jv compatible with jax.grad and jax.jit myself, I would appreciate it. Thanks,)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Feature request: JAX implementation of scipy.special.jv,"Hi, I'm working on a research problem that involves computations of Bessel functions of the first kind, of multiple different integer orders, and I'm trying to leverage JAX for autodiff and fast computation on GPUs.  The specific function required by the equations I'm working with is implemented by scipy in scipy.special.jv. JAX's implementation of scipy.special exposes a limited amount of functionality associated with Bessel functions  the ones I could find are i0, i1, i0e, and i1e, but not any Bessel J.  A similar Bessel function related issue has been raised in the past, at https://github.com/google/jax/issues/2466, where someone requested a JAX implementation of scipy.special.iv. The accepted solution for that issue was to use the TensorFlow Probability library implementation of ive. Unfortunately, this solution does not seem to work in my case: TFP does not seem to have an implementation of jv, and I cannot convert between ive and jv since the TFP implementation of ive does not accept complex arguments as inputs. Thus I'd like to request that JAX (or TFP) implement scipy.special.jv. Alternatively, if someone could point me to a way I could translate one of the existing third party Bessel function implementations (eg. those provided by scipy) into a JAXcomposable jv compatible with jax.grad and jax.jit myself, I would appreciate it. Thanks,",2022-06-06T22:09:03Z,enhancement P3 (no schedule),open,0,10,https://github.com/jax-ml/jax/issues/11002,"Maybe you could adapt an existing algorithm into a numbajittable function and see CC(Numba bridge) and/or https://github.com/josipd/jax/blob/master/jax/experimental/jambax.py to plug the function into a jax primitive (see also CC(Modified Bessel function of the second kind in jax.scipy.special)). Here is some code I am using to add some special functions but *without jit*, if it helps: ```python def makejaxufunc(ufunc, *derivs):      TODO use jax.something.standard_primitive     prim = core.Primitive(ufunc.__name__)     .wraps(ufunc)     def func(*args):         return prim.bind(*args)     .def_impl     def impl(*args):         return ufunc(*args)     .def_abstract_eval     def abstract_eval(*args):         shape = jnp.broadcast_shapes(*(x.shape for x in args))         dtype = jnp.result_type(*(x.dtype for x in args))         return core.ShapedArray(shape, dtype)     jvps = (         None if d is None         else lambda g, *args: d(*args) * g         for d in derivs     )     ad.defjvp(prim, *jvps)     batching.defbroadcasting(prim)     return func j0 = makejaxufunc(special.j0, lambda x: j1(x)) j1 = makejaxufunc(special.j1, lambda x: (j0(x)  jn(2, x)) / 2.0) jn = makejaxufunc(special.jn, None, lambda n, x: (jn(n  1, x)  jn(n + 1, x)) / 2.0) kv = makejaxufunc(special.kv, None, lambda v, z: kvp(v, z, 1)) kvp = makejaxufunc(special.kvp, None, lambda v, z, n: kvp(v, z, n + 1), None) ```","For being jittable, we may need to turn the recurrence relations into vectorization. Similar approach has been used for implementing the associated Legendre functions in jax.scipy.special.lpmn","Hi, is there any update on this? I need to use J1 and J2 (I do no need to get the gradient, btw) but I had no luck",I will give it a try. ,"Hi, would also like to see these implemented. In the meantime, a callback function to scipy seems good enough for me. It is jittable, differentiable  ( eg. ` jit(jacfwd(lambda x: jv(0.5, x)))(x))` works fine) and vmappable. The derivatives are implemented by taking the appropriate function combinations (from https://dlmf.nist.gov/10) EDIT: due to suggestion of , the function now uses pure_callback ```import jax.lax import jax.numpy as jnp import scipy.special from jax import custom_jvp, pure_callback, vmap  see https://github.com/google/jax/issues/11002 def generate_bessel(function):     """"""function is Jv, Yv, Hv_1,Hv_2""""""          def cv(v, x):         return pure_callback(             lambda vx: function(*vx),             x,             (v, x),             vectorized=True,         )     .defjvp     def cv_jvp(primals, tangents):         v, x = primals         dv, dx = tangents         primal_out = cv(v, x)          https://dlmf.nist.gov/10.6 formula 10.6.1         tangents_out = jax.lax.cond(             v == 0,             lambda: cv(v + 1, x),             lambda: 0.5 * (cv(v  1, x)  cv(v + 1, x)),         )         return primal_out, tangents_out * dx     return cv jv = generate_bessel(scipy.special.jv) yv = generate_bessel(scipy.special.yv) hankel1 = generate_bessel(scipy.special.hankel1) hankel2 = generate_bessel(scipy.special.hankel2) def generate_modified_bessel(function, sign):     """"""function is Kv and Iv""""""          def cv(v, x):         return pure_callback(             lambda vx: function(*vx),             x,             (v, x),             vectorized=True,         )     .defjvp     def cv_jvp(primals, tangents):         v, x = primals         dv, dx = tangents         primal_out = cv(v, x)          https://dlmf.nist.gov/10.6 formula 10.6.1         tangents_out = jax.lax.cond(             v == 0,             lambda: sign * cv(v + 1, x),             lambda: 0.5 * (cv(v  1, x) + cv(v + 1, x)),         )         return primal_out, tangents_out * dx     return cv kv = generate_modified_bessel(scipy.special.kv, sign=1) iv = generate_modified_bessel(scipy.special.iv, sign=+1) def spherical_bessel_genearator(f):     def g(v, x):         return f(v + 0.5, x) * jnp.sqrt(jnp.pi / (2 * x))     return g spherical_jv = spherical_bessel_genearator(jv) spherical_yv = spherical_bessel_genearator(yv) spherical_hankel1 = spherical_bessel_genearator(hankel1) spherical_hankel2 = spherical_bessel_genearator(hankel2) ``` For reference, the plots ``` import matplotlib.pyplot as plt x = jnp.linspace(0.0, 20.0, num=1000) for func, name in zip(     [jv, yv, iv, kv, spherical_jv, spherical_yv],     [""jv"", ""yv"", ""iv"", ""kv"", "" spherical_jv"", ""spherical_yv""], ):     plt.figure()     for i in range(5):         y = vmap(func, in_axes=(None, 0))(i, x)         plt.plot(x, y, label=i)     plt.ylim([1.1, 1.1])     plt.title(name)     plt.legend()     plt.draw()     plt.pause(0.001)      plt.show() print(""done"") ```",I would use `jax.pure_callback`. Is there a reason you are using `jax.experimental.host_callback` instead? `pure_callback` should work with `vmap`.,Assigning to  for further triage.,I am also interested here if there are any updates here :) ,"Since the spherical Hankel generates complex numbers, I am having a difficult time to take its derivative. Using what  shared, I wrote: ```python spherical_hankel1 = spherical_bessel_genearator(hankel1) def dhn(n,x):   real_func = lambda x: np.real(spherical_hankel1(n, x))   imag_func = lambda x: np.imag(spherical_hankel1(n, x))   diff=jax.jacfwd(real_func, argnums=0)(x)+1j*jax.jacfwd(imag_func, argnums=0)(x)   return diff dhn_vec=vmap(dhn, in_axes=(None, 0)) ``` and when printing: ``` krr=np.array([6.0]) print(dhn_vec(0, krr)) ``` I get: ``` XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: CpuCallback error: RuntimeError: Incorrect output dtype for return value 0: Expected: float32, Actual: complex64 ``` Any suggestions? Thanks for the edit Jake. Do you have any suggestions, ?","It looks like you'd need to update the `pure_callback` call to specify the appropriate dtype for complex input. You're telling it that it should be returning `float32`, but the function you're calling is returning `complex64`. https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html has some discussion of how to use `jax.pure_callback`"
4993,"以下是一个github上的jax下的一个issue, 标题是(Bump sphinx-autodoc-typehints from 1.11.1 to 1.18.2)， 内容是 (Bumps sphinxautodoctypehints from 1.11.1 to 1.18.2.  Release notes Sourced from sphinxautodoctypehints's releases.  1.18.2 What's Changed  [precommit.ci] precommit autoupdate by @​precommitci in toxdev/sphinxautodoctypehints CC(Indexing bug with multiple index arrays) Support and require nptyping 2.1.1 by @​gaborbernat in toxdev/sphinxautodoctypehints CC(fix isinstance check in indexing (fixes 227))  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.18.1...1.18.2 1.18.1 No release notes provided. 1.18.0 No release notes provided. 1.17.1 No release notes provided. typehints_use_rtype support and handle TypeError No release notes provided. 1.16.0 No release notes provided. 1.15.3 No release notes provided. 1.15.2 No release notes provided. 1.15.1 No release notes provided. 1.15.0 No release notes provided. 1.14.1 No release notes provided. Added document_defaults config option No release notes provided. Fix NewType is inserting a reference as first argument No release notes provided. Python 3.10 support and PEP563, drop 3.6 No release notes provided.    Changelog Sourced from sphinxautodoctypehints's changelog.  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked module import not working when used as guarded import  1.18.0  Support and require nptyping&gt;=2 Handle UnionType  1.17.1  Mark it as requiring nptyping&lt;2  1.17.0  Add typehints_use_rtype option Handles TypeError when getting source code via inspect  1.16.0  Add support for type subscriptions with multiple elements, where one or more elements are tuples; e.g., nptyping.NDArray[(Any, ...), nptyping.Float] Fix bug for arbitrary types accepting singleton subscriptions; e.g., nptyping.Float[64] Resolve forward references Expand and better handle TypeVar Add intershpinx reference link for ... to Ellipsis (as is just an alias)  1.15.3  Prevents reaching inner blocks that contains if TYPE_CHECKING  1.15.2  Log a warning instead of crashing when a type guard import fails to resolve When resolving type guard imports if the target module does not have source code (such is the case for Cextension modules) do nothing instead of crashing  1.15.1  Fix fully_qualified should be typehints_fully_qualified  1.15.0  Resolve type guard imports before evaluating annotations for objects Remove set_type_checking_flag flag as this is now done by default Fix crash when the inspect module returns an invalid python syntax source    ... (truncated)   Commits  7ee3891 Support and require nptyping 2.1.1 ( CC(fix isinstance check in indexing (fixes 227))) bff0765 [precommit.ci] precommit autoupdate ( CC(Indexing bug with multiple index arrays)) 73aa9b6 Fix mock imports on guarded imports ( CC(Test more Numpy ops for complex types.)) 4d5867d Handle UnionType ( CC(Batching rule for 'sort_key_val' not implemented)) 13ca2b4 [precommit.ci] precommit autoupdate ( CC(Inplace updating can lead to counterintuitive behavior)) def37f7 Support and require nptyping 2 ede082a Require nptyping&lt;2 ( CC(Failing lax_numpy_indexing_test.py tests on Python 3.7)) f9219b2 [precommit.ci] precommit autoupdate ( CC(jax.random.randint range must be valid)) a9b9023 Fix typos ( CC(add examples and docs for how to use lax.conv functions)) 1ef8488 Release 1.17.0 Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bump sphinx-autodoc-typehints from 1.11.1 to 1.18.2,"Bumps sphinxautodoctypehints from 1.11.1 to 1.18.2.  Release notes Sourced from sphinxautodoctypehints's releases.  1.18.2 What's Changed  [precommit.ci] precommit autoupdate by @​precommitci in toxdev/sphinxautodoctypehints CC(Indexing bug with multiple index arrays) Support and require nptyping 2.1.1 by @​gaborbernat in toxdev/sphinxautodoctypehints CC(fix isinstance check in indexing (fixes 227))  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.18.1...1.18.2 1.18.1 No release notes provided. 1.18.0 No release notes provided. 1.17.1 No release notes provided. typehints_use_rtype support and handle TypeError No release notes provided. 1.16.0 No release notes provided. 1.15.3 No release notes provided. 1.15.2 No release notes provided. 1.15.1 No release notes provided. 1.15.0 No release notes provided. 1.14.1 No release notes provided. Added document_defaults config option No release notes provided. Fix NewType is inserting a reference as first argument No release notes provided. Python 3.10 support and PEP563, drop 3.6 No release notes provided.    Changelog Sourced from sphinxautodoctypehints's changelog.  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked module import not working when used as guarded import  1.18.0  Support and require nptyping&gt;=2 Handle UnionType  1.17.1  Mark it as requiring nptyping&lt;2  1.17.0  Add typehints_use_rtype option Handles TypeError when getting source code via inspect  1.16.0  Add support for type subscriptions with multiple elements, where one or more elements are tuples; e.g., nptyping.NDArray[(Any, ...), nptyping.Float] Fix bug for arbitrary types accepting singleton subscriptions; e.g., nptyping.Float[64] Resolve forward references Expand and better handle TypeVar Add intershpinx reference link for ... to Ellipsis (as is just an alias)  1.15.3  Prevents reaching inner blocks that contains if TYPE_CHECKING  1.15.2  Log a warning instead of crashing when a type guard import fails to resolve When resolving type guard imports if the target module does not have source code (such is the case for Cextension modules) do nothing instead of crashing  1.15.1  Fix fully_qualified should be typehints_fully_qualified  1.15.0  Resolve type guard imports before evaluating annotations for objects Remove set_type_checking_flag flag as this is now done by default Fix crash when the inspect module returns an invalid python syntax source    ... (truncated)   Commits  7ee3891 Support and require nptyping 2.1.1 ( CC(fix isinstance check in indexing (fixes 227))) bff0765 [precommit.ci] precommit autoupdate ( CC(Indexing bug with multiple index arrays)) 73aa9b6 Fix mock imports on guarded imports ( CC(Test more Numpy ops for complex types.)) 4d5867d Handle UnionType ( CC(Batching rule for 'sort_key_val' not implemented)) 13ca2b4 [precommit.ci] precommit autoupdate ( CC(Inplace updating can lead to counterintuitive behavior)) def37f7 Support and require nptyping 2 ede082a Require nptyping&lt;2 ( CC(Failing lax_numpy_indexing_test.py tests on Python 3.7)) f9219b2 [precommit.ci] precommit autoupdate ( CC(jax.random.randint range must be valid)) a9b9023 Fix typos ( CC(add examples and docs for how to use lax.conv functions)) 1ef8488 Release 1.17.0 Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-06-06T17:11:35Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/10999, ignore,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
12459,"以下是一个github上的jax下的一个issue, 标题是(`rev` and `fwd` return different jacobian for some API)， 内容是 (`rev` and `fwd` return different jacobian for some API  `jax.lax.select` ```python import jax def fn(arg_1, arg_2):     pred = jax.numpy.array(4294967295, dtype=jax.numpy.uint32)     return jax.lax.select(pred, arg_1, arg_2) mykey = jax.random.PRNGKey(49762693) ontrue = jax.random.uniform(mykey, [], jax.numpy.float32, minval=128, maxval=8) mykey = jax.random.PRNGKey(989011) onfalse = jax.random.uniform(mykey, [], jax.numpy.float32, minval=2, maxval=64) print(ontrue) print(onfalse) print('res:') print(fn(ontrue.clone(), onfalse.clone())) res_rev = jax.jacrev(fn, (0, 1))(ontrue.clone(), onfalse.clone()) res_fwd = jax.jacfwd(fn, (0, 1))(ontrue.clone(), onfalse.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` result: ``` 111.64281 24.767237 res: 111.64281 reverse: (DeviceArray(0., dtype=float32), DeviceArray(0., dtype=float32)) forward: (DeviceArray(1., dtype=float32), DeviceArray(0., dtype=float32)) ```  `jax.lax.cumprod` ```python import jax def fn(arg_0):     reverse = True     return jax.lax.cumprod(arg_0, reverse=reverse) mykey = jax.random.PRNGKey(31644179) array = jax.random.uniform(mykey, [10], jax.numpy.float16, minval=8, maxval=0) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` result: ``` [7.1     5.21    4.133   5.117   2.625   0.08594 3.305   6.484  6.17    1.391  ] [ 3.2448e+04 4.5680e+03  8.7700e+02 2.1225e+02  4.1469e+01 1.5805e+01   1.8388e+02 5.5688e+01  8.5859e+00 1.3906e+00] reverse: [[4.5680e+03 6.2280e+03 7.8560e+03 6.3440e+03        inf        inf   9.8240e+03 5.0080e+03 5.2560e+03 2.3328e+04]  [ 0.0000e+00  8.7700e+02  1.1060e+03  8.9300e+02  1.7410e+03  5.3184e+04    1.3840e+03  7.0550e+02  7.4050e+02  3.2860e+03]  [ 0.0000e+00  0.0000e+00 2.1225e+02 1.7138e+02 3.3400e+02 1.0208e+04   2.6550e+02 1.3525e+02 1.4212e+02 6.3050e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  4.1469e+01  8.0875e+01  2.4700e+03    6.4312e+01  3.2750e+01  3.4406e+01  1.5262e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 1.5805e+01 4.8275e+02   1.2555e+01 6.3984e+00 6.7188e+00 2.9828e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  1.8388e+02    4.7852e+00  2.4375e+00  2.5605e+00  1.1359e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00   5.5688e+01 2.8375e+01 2.9797e+01 1.3225e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  8.5859e+00  9.0156e+00  4.0031e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  0.0000e+00 1.3906e+00 6.1719e+00]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  0.0000e+00  0.0000e+00  1.0000e+00]] forward: [[4.5680e+03 6.2280e+03 7.8520e+03 6.3400e+03 1.2360e+04        inf   9.8240e+03 5.0080e+03 5.2600e+03 2.3344e+04]  [ 0.0000e+00  8.7700e+02  1.1060e+03  8.9300e+02  1.7400e+03  5.3184e+04    1.3840e+03  7.0550e+02  7.4050e+02  3.2880e+03]  [ 0.0000e+00  0.0000e+00 2.1225e+02 1.7138e+02 3.3400e+02 1.0208e+04   2.6550e+02 1.3538e+02 1.4212e+02 6.3100e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  4.1469e+01  8.0875e+01  2.4700e+03    6.4312e+01  3.2750e+01  3.4406e+01  1.5262e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 1.5805e+01 4.8275e+02   1.2562e+01 6.4023e+00 6.7227e+00 2.9828e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  1.8388e+02    4.7852e+00  2.4375e+00  2.5605e+00  1.1367e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00   5.5688e+01 2.8375e+01 2.9797e+01 1.3225e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  8.5859e+00  9.0156e+00  4.0031e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  0.0000e+00 1.3906e+00 6.1719e+00]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  0.0000e+00  0.0000e+00  1.0000e+00]] ```  `jax.numpy.average` ```python import jax mykey = jax.random.PRNGKey(66613885) weight = jax.random.uniform(mykey, [1, 4], jax.numpy.bfloat16, minval=1, maxval=0) print('weight:') print(weight) def fn(arg_0):     return jax.numpy.average(arg_0, 1, weight) mykey = jax.random.PRNGKey(32581679) array = jax.random.uniform(mykey, [1, 4], jax.numpy.bfloat16, minval=64, maxval=8) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` result ``` weight: [[0.539062 0.796875 0.929688 0.015625]] [[49.5 46.5 58.5 32]] [52] reverse: [[[0.236328 0.347656 0.40625 0.00683594]]] forward: [[[0.236328 0.349609 0.408203 0.00683594]]] ```  `jax.numpy.corrcoef` ```python import jax def fn(arg_0):     rowvar = False     return jax.numpy.corrcoef(arg_0, rowvar=rowvar) mykey = jax.random.PRNGKey(81134584) array = jax.random.uniform(mykey, [5], jax.numpy.float16, minval=128, maxval=32) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` [ 90.8   95.     1.25 116.25  67.06] 1.0 reverse: [0.004265 0.005333  0.01855  0.01075   0.001787] forward: [0.004265 0.005333       nan 0.01074   0.001787] ```  `jax.numpy.cov` ```python import jax def fn(arg_0):     arg_1 = None     mykey = jax.random.PRNGKey(75273489)     fweights = jax.random.randint(mykey, [5], 64, 2, jax.numpy.int64)     mykey = jax.random.PRNGKey(76662243)     aweights = jax.random.uniform(mykey, [5], jax.numpy.bfloat16, minval=0, maxval=16)     rowvar = True     ddof = 3     bias = True     return jax.numpy.cov(arg_0, arg_1, fweights=fweights, aweights=aweights, rowvar=rowvar, ddof=ddof, bias=bias) mykey = jax.random.PRNGKey(80304800) array = jax.random.uniform(mykey, [5], jax.numpy.bfloat16, minval=2, maxval=128) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` [68 85.5 31.5 30.5 110.5] 324 reverse: [3.35938 6.4375 3.98438 0.373047 14.125] forward: [3.35938 6.4375 4.03125 0.373047 14.25] ```  `jax.lax.linalg.qr` ```python import jax def fn(arg_0):     full_matrices = False     return jax.lax.linalg.qr(arg_0, full_matrices=full_matrices) mykey = jax.random.PRNGKey(21862424) array = jax.random.uniform(mykey, [2, 2], jax.numpy.float32, minval=4, maxval=2) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` [[ 1.5485778  0.502125 ]  [2.966303  0.9675245]] (DeviceArray([[0.46278715,  0.8864694 ],              [ 0.8864694 ,  0.46278715]], dtype=float32), DeviceArray([[3.3461990e+00, 1.0900580e+00],              [ 0.0000000e+00, 2.6394129e03]], dtype=float32)) reverse: (DeviceArray([[[[2.3484200e01, 0.0000000e+00],                [1.2260080e01, 0.0000000e+00]],               [[1.2260080e01, 0.0000000e+00],                [6.4003207e02, 4.1191465e06]]],              [[[1.2260080e01, 0.0000000e+00],                [6.4004548e02, 0.0000000e+00]],               [[ 2.3484333e01, 4.1191465e06],                [ 1.2260080e01, 0.0000000e+00]]]], dtype=float32), DeviceArray([[[[4.6278715e01, 0.0000000e+00],                [ 8.8646942e01, 0.0000000e+00]],               [[ 6.9923524e04, 4.6278715e01],                [ 3.6503447e04,  8.8646942e01]]],              [[[0.0000000e+00, 0.0000000e+00],                [0.0000000e+00, 0.0000000e+00]],               [[2.8877634e01,  8.8646942e01],                [1.5075757e01,  4.6278715e01]]]], dtype=float32)) forward: (DeviceArray([[[[2.34842002e01,  0.00000000e+00],                [1.22600801e01,  0.00000000e+00]],               [[1.22604370e01,  0.00000000e+00],                [6.40042424e02,  1.45833037e05]]],              [[[1.22600794e01,  0.00000000e+00],                [6.40045404e02,  0.00000000e+00]],               [[ 2.34842256e01, 8.19233537e06],                [ 1.22604370e01,  0.00000000e+00]]]], dtype=float32), DeviceArray([[[[4.6278715e01,  0.0000000e+00],                [ 8.8646942e01,  0.0000000e+00]],               [[ 6.9923740e04, 4.6278715e01],                [ 3.6504347e04,  8.8646936e01]]],              [[[ 0.0000000e+00,  0.0000000e+00],                [ 0.0000000e+00,  0.0000000e+00]],               [[2.8877634e01,  8.8646936e01],                [1.5075757e01,  4.6278715e01]]]], dtype=float32)) ```  `jax.numpy.polydiv` ```python import jax def fn(m):     mykey = jax.random.PRNGKey(44731274)     p = jax.random.uniform(mykey, [6], jax.numpy.bfloat16, minval=256, maxval=128)     return jax.numpy.polydiv(p, m) mykey = jax.random.PRNGKey(28454225) array = jax.random.uniform(mykey, [1], jax.numpy.bfloat16, minval=2, maxval=0) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` [0.453125] (DeviceArray([22, 194, 266, 35.25, 163, 189], dtype=bfloat16), DeviceArray([0, 0, 0.5, 0, 0, 0.5], dtype=bfloat16)) reverse: (DeviceArray([[48.75],              [428],              [588],              [78],              [360],              [420]], dtype=bfloat16), DeviceArray([[0.125],              [1],              [0],              [0],              [0],              [1]], dtype=bfloat16)) forward: (DeviceArray([[48.75],              [428],              [588],              [78],              [360],              [420]], dtype=bfloat16), DeviceArray([[0.125],              [0],              [0],              [0],              [0],              [1]], dtype=bfloat16)) ```  `jax.numpy.sinc` ```python import jax def fn(arg_0):     return jax.numpy.sinc(arg_0) mykey = jax.random.PRNGKey(69423934) array = jax.random.uniform(mykey, [], jax.numpy.bfloat16, minval=2, maxval=0) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` 0.125 0.976562 reverse: 0.34375 forward: 0.375 ```  `jax.scipy.linalg.eigh` ```python import jax def fn(arg_0):     return jax.scipy.linalg.eigh(arg_0) mykey = jax.random.PRNGKey(30216818) array = jax.random.uniform(mykey, [5, 1], jax.numpy.float32, minval=16, maxval=4) res_rev = jax.jacrev(fn, (0))(arg_0) res_fwd = jax.jacfwd(fn, (0))(arg_0) ``` ``` Not equal to tolerance rtol=0.000125, atol=0.000125 Mismatched elements: 66 / 125 (52.8%) Max absolute difference: 0.02147339 Max relative difference: 2.7683024  x: array([[[[0.01861 ],          [0.00023 ],          [0.00029 ],...  y: array([[[[0.01861 ],          [0.00023 ],          [0.00029 ],... ```  `jax.scipy.sparse.linalg.cg` ```python import jax def fn(arg_0, arg_1):     atol = 0.0     return jax.scipy.sparse.linalg.cg(arg_0, arg_1, atol=atol) mykey = jax.random.PRNGKey(6634004) A = jax.random.uniform(mykey, [2, 2], jax.numpy.float32, minval=2, maxval=2) mykey = jax.random.PRNGKey(3054573) b = jax.random.uniform(mykey, [2], jax.numpy.float32, minval=0, maxval=1) res_rev = jax.jacrev(fn, (0, 1))(A.clone(), b.clone()) res_fwd = jax.jacfwd(fn, (0, 1))(A.clone(), b.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` reverse: ((DeviceArray([[[21.62046  ,   6.679416 ],               [ 15.982462 ,  4.9376154]],              [[37.647606 ,  11.6308365],               [18.672651 ,   5.768721 ]]], dtype=float32), DeviceArray([[3.406348 ,  2.5180697],              [5.931458 , 2.9419146]], dtype=float32)), None) forward: ((DeviceArray([[[21.620462 ,   6.6794186],               [37.64759  ,  11.630831 ]],              [[ 15.982458 ,  4.937614 ],               [18.672651 ,   5.768721 ]]], dtype=float32), DeviceArray([[3.406348 , 5.931455 ],              [ 2.5180697, 2.941915 ]], dtype=float32)), None) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`rev` and `fwd` return different jacobian for some API,"`rev` and `fwd` return different jacobian for some API  `jax.lax.select` ```python import jax def fn(arg_1, arg_2):     pred = jax.numpy.array(4294967295, dtype=jax.numpy.uint32)     return jax.lax.select(pred, arg_1, arg_2) mykey = jax.random.PRNGKey(49762693) ontrue = jax.random.uniform(mykey, [], jax.numpy.float32, minval=128, maxval=8) mykey = jax.random.PRNGKey(989011) onfalse = jax.random.uniform(mykey, [], jax.numpy.float32, minval=2, maxval=64) print(ontrue) print(onfalse) print('res:') print(fn(ontrue.clone(), onfalse.clone())) res_rev = jax.jacrev(fn, (0, 1))(ontrue.clone(), onfalse.clone()) res_fwd = jax.jacfwd(fn, (0, 1))(ontrue.clone(), onfalse.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` result: ``` 111.64281 24.767237 res: 111.64281 reverse: (DeviceArray(0., dtype=float32), DeviceArray(0., dtype=float32)) forward: (DeviceArray(1., dtype=float32), DeviceArray(0., dtype=float32)) ```  `jax.lax.cumprod` ```python import jax def fn(arg_0):     reverse = True     return jax.lax.cumprod(arg_0, reverse=reverse) mykey = jax.random.PRNGKey(31644179) array = jax.random.uniform(mykey, [10], jax.numpy.float16, minval=8, maxval=0) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` result: ``` [7.1     5.21    4.133   5.117   2.625   0.08594 3.305   6.484  6.17    1.391  ] [ 3.2448e+04 4.5680e+03  8.7700e+02 2.1225e+02  4.1469e+01 1.5805e+01   1.8388e+02 5.5688e+01  8.5859e+00 1.3906e+00] reverse: [[4.5680e+03 6.2280e+03 7.8560e+03 6.3440e+03        inf        inf   9.8240e+03 5.0080e+03 5.2560e+03 2.3328e+04]  [ 0.0000e+00  8.7700e+02  1.1060e+03  8.9300e+02  1.7410e+03  5.3184e+04    1.3840e+03  7.0550e+02  7.4050e+02  3.2860e+03]  [ 0.0000e+00  0.0000e+00 2.1225e+02 1.7138e+02 3.3400e+02 1.0208e+04   2.6550e+02 1.3525e+02 1.4212e+02 6.3050e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  4.1469e+01  8.0875e+01  2.4700e+03    6.4312e+01  3.2750e+01  3.4406e+01  1.5262e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 1.5805e+01 4.8275e+02   1.2555e+01 6.3984e+00 6.7188e+00 2.9828e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  1.8388e+02    4.7852e+00  2.4375e+00  2.5605e+00  1.1359e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00   5.5688e+01 2.8375e+01 2.9797e+01 1.3225e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  8.5859e+00  9.0156e+00  4.0031e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  0.0000e+00 1.3906e+00 6.1719e+00]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  0.0000e+00  0.0000e+00  1.0000e+00]] forward: [[4.5680e+03 6.2280e+03 7.8520e+03 6.3400e+03 1.2360e+04        inf   9.8240e+03 5.0080e+03 5.2600e+03 2.3344e+04]  [ 0.0000e+00  8.7700e+02  1.1060e+03  8.9300e+02  1.7400e+03  5.3184e+04    1.3840e+03  7.0550e+02  7.4050e+02  3.2880e+03]  [ 0.0000e+00  0.0000e+00 2.1225e+02 1.7138e+02 3.3400e+02 1.0208e+04   2.6550e+02 1.3538e+02 1.4212e+02 6.3100e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  4.1469e+01  8.0875e+01  2.4700e+03    6.4312e+01  3.2750e+01  3.4406e+01  1.5262e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00 1.5805e+01 4.8275e+02   1.2562e+01 6.4023e+00 6.7227e+00 2.9828e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  1.8388e+02    4.7852e+00  2.4375e+00  2.5605e+00  1.1367e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00   5.5688e+01 2.8375e+01 2.9797e+01 1.3225e+02]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  8.5859e+00  9.0156e+00  4.0031e+01]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  0.0000e+00 1.3906e+00 6.1719e+00]  [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00    0.0000e+00  0.0000e+00  0.0000e+00  1.0000e+00]] ```  `jax.numpy.average` ```python import jax mykey = jax.random.PRNGKey(66613885) weight = jax.random.uniform(mykey, [1, 4], jax.numpy.bfloat16, minval=1, maxval=0) print('weight:') print(weight) def fn(arg_0):     return jax.numpy.average(arg_0, 1, weight) mykey = jax.random.PRNGKey(32581679) array = jax.random.uniform(mykey, [1, 4], jax.numpy.bfloat16, minval=64, maxval=8) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` result ``` weight: [[0.539062 0.796875 0.929688 0.015625]] [[49.5 46.5 58.5 32]] [52] reverse: [[[0.236328 0.347656 0.40625 0.00683594]]] forward: [[[0.236328 0.349609 0.408203 0.00683594]]] ```  `jax.numpy.corrcoef` ```python import jax def fn(arg_0):     rowvar = False     return jax.numpy.corrcoef(arg_0, rowvar=rowvar) mykey = jax.random.PRNGKey(81134584) array = jax.random.uniform(mykey, [5], jax.numpy.float16, minval=128, maxval=32) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` [ 90.8   95.     1.25 116.25  67.06] 1.0 reverse: [0.004265 0.005333  0.01855  0.01075   0.001787] forward: [0.004265 0.005333       nan 0.01074   0.001787] ```  `jax.numpy.cov` ```python import jax def fn(arg_0):     arg_1 = None     mykey = jax.random.PRNGKey(75273489)     fweights = jax.random.randint(mykey, [5], 64, 2, jax.numpy.int64)     mykey = jax.random.PRNGKey(76662243)     aweights = jax.random.uniform(mykey, [5], jax.numpy.bfloat16, minval=0, maxval=16)     rowvar = True     ddof = 3     bias = True     return jax.numpy.cov(arg_0, arg_1, fweights=fweights, aweights=aweights, rowvar=rowvar, ddof=ddof, bias=bias) mykey = jax.random.PRNGKey(80304800) array = jax.random.uniform(mykey, [5], jax.numpy.bfloat16, minval=2, maxval=128) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` [68 85.5 31.5 30.5 110.5] 324 reverse: [3.35938 6.4375 3.98438 0.373047 14.125] forward: [3.35938 6.4375 4.03125 0.373047 14.25] ```  `jax.lax.linalg.qr` ```python import jax def fn(arg_0):     full_matrices = False     return jax.lax.linalg.qr(arg_0, full_matrices=full_matrices) mykey = jax.random.PRNGKey(21862424) array = jax.random.uniform(mykey, [2, 2], jax.numpy.float32, minval=4, maxval=2) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` [[ 1.5485778  0.502125 ]  [2.966303  0.9675245]] (DeviceArray([[0.46278715,  0.8864694 ],              [ 0.8864694 ,  0.46278715]], dtype=float32), DeviceArray([[3.3461990e+00, 1.0900580e+00],              [ 0.0000000e+00, 2.6394129e03]], dtype=float32)) reverse: (DeviceArray([[[[2.3484200e01, 0.0000000e+00],                [1.2260080e01, 0.0000000e+00]],               [[1.2260080e01, 0.0000000e+00],                [6.4003207e02, 4.1191465e06]]],              [[[1.2260080e01, 0.0000000e+00],                [6.4004548e02, 0.0000000e+00]],               [[ 2.3484333e01, 4.1191465e06],                [ 1.2260080e01, 0.0000000e+00]]]], dtype=float32), DeviceArray([[[[4.6278715e01, 0.0000000e+00],                [ 8.8646942e01, 0.0000000e+00]],               [[ 6.9923524e04, 4.6278715e01],                [ 3.6503447e04,  8.8646942e01]]],              [[[0.0000000e+00, 0.0000000e+00],                [0.0000000e+00, 0.0000000e+00]],               [[2.8877634e01,  8.8646942e01],                [1.5075757e01,  4.6278715e01]]]], dtype=float32)) forward: (DeviceArray([[[[2.34842002e01,  0.00000000e+00],                [1.22600801e01,  0.00000000e+00]],               [[1.22604370e01,  0.00000000e+00],                [6.40042424e02,  1.45833037e05]]],              [[[1.22600794e01,  0.00000000e+00],                [6.40045404e02,  0.00000000e+00]],               [[ 2.34842256e01, 8.19233537e06],                [ 1.22604370e01,  0.00000000e+00]]]], dtype=float32), DeviceArray([[[[4.6278715e01,  0.0000000e+00],                [ 8.8646942e01,  0.0000000e+00]],               [[ 6.9923740e04, 4.6278715e01],                [ 3.6504347e04,  8.8646936e01]]],              [[[ 0.0000000e+00,  0.0000000e+00],                [ 0.0000000e+00,  0.0000000e+00]],               [[2.8877634e01,  8.8646936e01],                [1.5075757e01,  4.6278715e01]]]], dtype=float32)) ```  `jax.numpy.polydiv` ```python import jax def fn(m):     mykey = jax.random.PRNGKey(44731274)     p = jax.random.uniform(mykey, [6], jax.numpy.bfloat16, minval=256, maxval=128)     return jax.numpy.polydiv(p, m) mykey = jax.random.PRNGKey(28454225) array = jax.random.uniform(mykey, [1], jax.numpy.bfloat16, minval=2, maxval=0) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` [0.453125] (DeviceArray([22, 194, 266, 35.25, 163, 189], dtype=bfloat16), DeviceArray([0, 0, 0.5, 0, 0, 0.5], dtype=bfloat16)) reverse: (DeviceArray([[48.75],              [428],              [588],              [78],              [360],              [420]], dtype=bfloat16), DeviceArray([[0.125],              [1],              [0],              [0],              [0],              [1]], dtype=bfloat16)) forward: (DeviceArray([[48.75],              [428],              [588],              [78],              [360],              [420]], dtype=bfloat16), DeviceArray([[0.125],              [0],              [0],              [0],              [0],              [1]], dtype=bfloat16)) ```  `jax.numpy.sinc` ```python import jax def fn(arg_0):     return jax.numpy.sinc(arg_0) mykey = jax.random.PRNGKey(69423934) array = jax.random.uniform(mykey, [], jax.numpy.bfloat16, minval=2, maxval=0) print(array) print(fn(array.clone())) res_rev = jax.jacrev(fn, (0))(array.clone()) res_fwd = jax.jacfwd(fn, (0))(array.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` 0.125 0.976562 reverse: 0.34375 forward: 0.375 ```  `jax.scipy.linalg.eigh` ```python import jax def fn(arg_0):     return jax.scipy.linalg.eigh(arg_0) mykey = jax.random.PRNGKey(30216818) array = jax.random.uniform(mykey, [5, 1], jax.numpy.float32, minval=16, maxval=4) res_rev = jax.jacrev(fn, (0))(arg_0) res_fwd = jax.jacfwd(fn, (0))(arg_0) ``` ``` Not equal to tolerance rtol=0.000125, atol=0.000125 Mismatched elements: 66 / 125 (52.8%) Max absolute difference: 0.02147339 Max relative difference: 2.7683024  x: array([[[[0.01861 ],          [0.00023 ],          [0.00029 ],...  y: array([[[[0.01861 ],          [0.00023 ],          [0.00029 ],... ```  `jax.scipy.sparse.linalg.cg` ```python import jax def fn(arg_0, arg_1):     atol = 0.0     return jax.scipy.sparse.linalg.cg(arg_0, arg_1, atol=atol) mykey = jax.random.PRNGKey(6634004) A = jax.random.uniform(mykey, [2, 2], jax.numpy.float32, minval=2, maxval=2) mykey = jax.random.PRNGKey(3054573) b = jax.random.uniform(mykey, [2], jax.numpy.float32, minval=0, maxval=1) res_rev = jax.jacrev(fn, (0, 1))(A.clone(), b.clone()) res_fwd = jax.jacfwd(fn, (0, 1))(A.clone(), b.clone()) print('reverse:') print(res_rev) print('') print('forward:') print(res_fwd) ``` ``` reverse: ((DeviceArray([[[21.62046  ,   6.679416 ],               [ 15.982462 ,  4.9376154]],              [[37.647606 ,  11.6308365],               [18.672651 ,   5.768721 ]]], dtype=float32), DeviceArray([[3.406348 ,  2.5180697],              [5.931458 , 2.9419146]], dtype=float32)), None) forward: ((DeviceArray([[[21.620462 ,   6.6794186],               [37.64759  ,  11.630831 ]],              [[ 15.982458 ,  4.937614 ],               [18.672651 ,   5.768721 ]]], dtype=float32), DeviceArray([[3.406348 , 5.931455 ],              [ 2.5180697, 2.941915 ]], dtype=float32)), None) ```",2022-06-06T14:08:22Z,bug,open,1,7,https://github.com/jax-ml/jax/issues/10993,"For `lax.select`, it turns out that `jit` will transfer `4294967295` with `uint32` as `False` ```python import jax def fn(arg_1, arg_2):     pred = jax.numpy.array(4294967295, dtype=jax.numpy.uint32)     return jax.lax.select(pred, arg_1, arg_2) ontrue = jax.numpy.array(111.64281, dtype=jax.numpy.float32) onfalse = jax.numpy.array(24.767237, dtype=jax.numpy.float32) print('res without jit: ', fn(ontrue.clone(), onfalse.clone())) print('res with jit: ', jax.jit(fn)(ontrue.clone(), onfalse.clone())) res_rev = jax.jacrev(fn, (0, 1))(ontrue.clone(), onfalse.clone()) res_fwd = jax.jacfwd(fn, (0, 1))(ontrue.clone(), onfalse.clone()) print('reverse jacobian:', res_rev) print('forward jacobian:', res_fwd) print(jax.make_jaxpr(fn)(ontrue, onfalse)) ``` ``` res without jit:  111.64281 res with jit:  24.767237 reverse jacobian: (DeviceArray(0., dtype=float32), DeviceArray(0., dtype=float32)) forward jacobian: (DeviceArray(1., dtype=float32), DeviceArray(0., dtype=float32)) { lambda ; a:f32[] b:f32[]. let c:f32[] = select_n 4294967295 b a in (c,) } ``` But it is still confusing that the second gradient of reverse mode is also zero.  I think this is so impactful that it can totally change the behavior of jax program, thus, what do you  think of this issue?","For the `jax.numpy.sinc`, it turns out that it is caused by the lowprecision dtype `bfloat16`. For example, ```python import jax def fn(arg_0):     return jax.numpy.sinc(arg_0) def test(dtype):     array = jax.numpy.array(0.125, dtype=dtype)     print(fn(array.clone()))     res_rev = jax.jacrev(fn, (0))(array.clone())     res_fwd = jax.jacfwd(fn, (0))(array.clone())     print('reverse: ', res_rev)     print('forward: ', res_fwd) test(jax.numpy.bfloat16) test(jax.numpy.float16) test(jax.numpy.float32) ``` ``` 0.976562 reverse:  0.34375 forward:  0.375 0.9746 reverse:  0.3987 forward:  0.3945 0.9744954 reverse:  0.40492725 forward:  0.404927 ``` The reverse and forward modes' jacobians are so different when the input has `bfloat16` dtype. I don't know why the reverse mode outputs a very inaccurate gradient for `bfloat16`, is there any way to mitigate the big difference?",">  I don't know why the reverse mode outputs a very inaccurate gradient for `bfloat16`, is there any way to mitigate the big difference? This is a consequence of the intended design of `bfloat16` – it has a very small mantissa (7 bits) which means each operation is only accurate to about 1 part in 128, and thus even small sequences of operations can accumulate very large float roundoff errors. This is a worthwhile tradeoff for speed in deep learning contexts, but you should generally not expect computations in bfloat16 to be numerically accurate.",How about the `lax.select` issue?,Seems like a bug. I'm not sure what's causing it.," I think it's miscompilation error, might be related to CC(Change lax.select_p to be an nary predicate, 'lax.select_n_p'. Change `lax.select()` to be a thin shim around the new nary version.) (introduced by ) and this comment. Consider this code: ```python import jax import jax.numpy as jnp from jax import lax def fn(a, b):   pred = jnp.array(2)   return lax.select(pred, a, b) def gn(a, b):   pred = True   return lax.select(pred, a, b) j1 = jax.make_jaxpr(jax.jacrev(fn, (0, 1)))(1., 2.) j2 = jax.make_jaxpr(jax.jacrev(gn, (0, 1)))(1., 2.) print((j1, j2)) ``` which produces ```ocaml ({ lambda ; a:f32[] b:f32[]. let     _:f32[] = select_n 2 b a      (**)     ...     k:bool[] = eq 2 0             (**)     l:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0.0     m:f32[1] = select_n k l j     n:bool[] = eq 2 1             (**)     ...   in (r, t) },  { lambda ; a:f32[] b:f32[]. let     _:f32[] = select_n True b a   (**)     ...     k:bool[] = eq True False      (**)     l:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0.0     m:f32[1] = select_n k l j     n:bool[] = eq True True       (**)     ...   in (r, t) }) ``` Lines marked with `(**)` should hint the cause of the problem. I think a potential fix would be converting `dtype` of `pred` to `np.bool_` first in `lax.select` as the following codes works just fine ```python def fn(a, b):   pred = jnp.array(2).astype(bool)   return lax.select(pred, a, b) ```",Thanks for your explanation!  
3680,"以下是一个github上的jax下的一个issue, 标题是(`value_and_grad` returns different value than direct invocation for some APIs)， 内容是 (`value_and_grad` returns different value than direct invocation for the APIs below:  `jax.numpy.arange` ```python import jax def fn(arg_0, arg_1, arg_2):     res = jax.numpy.arange(arg_0, arg_1, arg_2,)      print(res)     return res.sum() start = 1.2 stop = 4.8 step = 0.24 res1 = fn(start, stop, step) print(res1)  43.2 res2, _ = jax.value_and_grad(fn, (0,1,2))(start, stop, step) print(res2)  48.0 ```  `jax.numpy.trapz` ```python import jax def fn(arg_0, arg_1, dx):     axis = 1     return jax.numpy.trapz(arg_0, arg_1, dx=dx, axis=axis).sum() mykey = jax.random.PRNGKey(71431840) array1 = jax.random.uniform(mykey, [3, 10], jax.numpy.float16, minval=64, maxval=128) mykey = jax.random.PRNGKey(80930408) array2 = jax.random.uniform(mykey, [10], jax.numpy.bfloat16, minval=8, maxval=64) dx = 1.0 res1 = fn(array1.clone(), array2.clone(), dx) print(res1)  2578.787 res2, _ = jax.value_and_grad(fn, (0,1,2))(array1.clone(), array2.clone(), dx) print(res2)  2570.9902 ```  `jax.numpy.fft.fftfreq` ```python import jax def fn(arg_1):     arg_0 = 21     return jax.numpy.fft.fftfreq(arg_0, arg_1).sum() d = 6.25e05 res1 = fn(d) print(res1)  0.00048828125 res2, _ = jax.value_and_grad(fn, (0))(d) print(res2)  0.0005493164 ```  `jax.lax.dynamic_index_in_dim` ```python import jax results = dict() def fn(arg_0):     mykey = jax.random.PRNGKey(76009189)     index = jax.random.randint(mykey, [], 8, 3, jax.numpy.int32)     return jax.lax.dynamic_index_in_dim(arg_0, index, 0, 1).sum() mykey = jax.random.PRNGKey(29408463) array = jax.random.uniform(mykey, [5, 5], jax.numpy.float32, minval=32, maxval=128) res1 = fn(array.clone()) print(res1)  264.71423 res2, _ = jax.value_and_grad(fn, (0))(array.clone()) print(res2)  342.46118 ```  `jax.lax.dynamic_slice_in_dim` ```python import jax def fn(arg_0):     mykey = jax.random.PRNGKey(67830577)     arg_1 = jax.random.randint(mykey, [], 16, 33, jax.numpy.int32)     return jax.lax.dynamic_slice_in_dim(arg_0, arg_1, 3, axis=0).sum() mykey = jax.random.PRNGKey(9204993) array = jax.random.uniform(mykey, [6, 4], jax.numpy.bfloat16, minval=128, maxval=256) res1 = fn(array.clone()) print(res1)  808 res2, _ = jax.value_and_grad(fn, (0))(array.clone()) print(res2)  1264 ```  `jax.lax.dynamic_update_index_in_dim` ```python import jax results = dict() def fn(arg_0, arg_1):     mykey = jax.random.PRNGKey(47452882)     arg_2= jax.random.randint(mykey, [], 8, 5, jax.numpy.int32)     axis = 0     return jax.lax.dynamic_update_index_in_dim(arg_0, arg_1, arg_2, axis=0).sum() mykey = jax.random.PRNGKey(98152452) a1 = jax.random.uniform(mykey, [3], jax.numpy.float64, minval=0, maxval=128) mykey = jax.random.PRNGKey(54768506) a2 = jax.random.uniform(mykey, [], jax.numpy.float32, minval=128, maxval=32) res1 = fn(a1.clone(), a2.clone()) print(res1)  61.136593 res2, _ = jax.value_and_grad(fn, (0,1))(a1.clone(), a2.clone()) print(res2)  95.84935 ```  `jax.lax.dynamic_update_slice_in_dim` ```python import jax def fn(arg_0, arg_1):     return jax.lax.dynamic_update_slice_in_dim(arg_0, arg_1, 8, 0).sum() mykey = jax.random.PRNGKey(44698276) a1 = jax.random.uniform(mykey, [6, 3], jax.numpy.float32, minval=4, maxval=8) mykey = jax.random.PRNGKey(83852099) a2 = jax.random.uniform(mykey, [3, 3], jax.numpy.float32, minval=0, maxval=2) res1 = fn(a1.clone(), a2.clone()) print(res1)  15.913427 res2, _ = jax.value_and_grad(fn, (0,1))(a1.clone(), a2.clone()) print(res2)  30.159414 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`value_and_grad` returns different value than direct invocation for some APIs,"`value_and_grad` returns different value than direct invocation for the APIs below:  `jax.numpy.arange` ```python import jax def fn(arg_0, arg_1, arg_2):     res = jax.numpy.arange(arg_0, arg_1, arg_2,)      print(res)     return res.sum() start = 1.2 stop = 4.8 step = 0.24 res1 = fn(start, stop, step) print(res1)  43.2 res2, _ = jax.value_and_grad(fn, (0,1,2))(start, stop, step) print(res2)  48.0 ```  `jax.numpy.trapz` ```python import jax def fn(arg_0, arg_1, dx):     axis = 1     return jax.numpy.trapz(arg_0, arg_1, dx=dx, axis=axis).sum() mykey = jax.random.PRNGKey(71431840) array1 = jax.random.uniform(mykey, [3, 10], jax.numpy.float16, minval=64, maxval=128) mykey = jax.random.PRNGKey(80930408) array2 = jax.random.uniform(mykey, [10], jax.numpy.bfloat16, minval=8, maxval=64) dx = 1.0 res1 = fn(array1.clone(), array2.clone(), dx) print(res1)  2578.787 res2, _ = jax.value_and_grad(fn, (0,1,2))(array1.clone(), array2.clone(), dx) print(res2)  2570.9902 ```  `jax.numpy.fft.fftfreq` ```python import jax def fn(arg_1):     arg_0 = 21     return jax.numpy.fft.fftfreq(arg_0, arg_1).sum() d = 6.25e05 res1 = fn(d) print(res1)  0.00048828125 res2, _ = jax.value_and_grad(fn, (0))(d) print(res2)  0.0005493164 ```  `jax.lax.dynamic_index_in_dim` ```python import jax results = dict() def fn(arg_0):     mykey = jax.random.PRNGKey(76009189)     index = jax.random.randint(mykey, [], 8, 3, jax.numpy.int32)     return jax.lax.dynamic_index_in_dim(arg_0, index, 0, 1).sum() mykey = jax.random.PRNGKey(29408463) array = jax.random.uniform(mykey, [5, 5], jax.numpy.float32, minval=32, maxval=128) res1 = fn(array.clone()) print(res1)  264.71423 res2, _ = jax.value_and_grad(fn, (0))(array.clone()) print(res2)  342.46118 ```  `jax.lax.dynamic_slice_in_dim` ```python import jax def fn(arg_0):     mykey = jax.random.PRNGKey(67830577)     arg_1 = jax.random.randint(mykey, [], 16, 33, jax.numpy.int32)     return jax.lax.dynamic_slice_in_dim(arg_0, arg_1, 3, axis=0).sum() mykey = jax.random.PRNGKey(9204993) array = jax.random.uniform(mykey, [6, 4], jax.numpy.bfloat16, minval=128, maxval=256) res1 = fn(array.clone()) print(res1)  808 res2, _ = jax.value_and_grad(fn, (0))(array.clone()) print(res2)  1264 ```  `jax.lax.dynamic_update_index_in_dim` ```python import jax results = dict() def fn(arg_0, arg_1):     mykey = jax.random.PRNGKey(47452882)     arg_2= jax.random.randint(mykey, [], 8, 5, jax.numpy.int32)     axis = 0     return jax.lax.dynamic_update_index_in_dim(arg_0, arg_1, arg_2, axis=0).sum() mykey = jax.random.PRNGKey(98152452) a1 = jax.random.uniform(mykey, [3], jax.numpy.float64, minval=0, maxval=128) mykey = jax.random.PRNGKey(54768506) a2 = jax.random.uniform(mykey, [], jax.numpy.float32, minval=128, maxval=32) res1 = fn(a1.clone(), a2.clone()) print(res1)  61.136593 res2, _ = jax.value_and_grad(fn, (0,1))(a1.clone(), a2.clone()) print(res2)  95.84935 ```  `jax.lax.dynamic_update_slice_in_dim` ```python import jax def fn(arg_0, arg_1):     return jax.lax.dynamic_update_slice_in_dim(arg_0, arg_1, 8, 0).sum() mykey = jax.random.PRNGKey(44698276) a1 = jax.random.uniform(mykey, [6, 3], jax.numpy.float32, minval=4, maxval=8) mykey = jax.random.PRNGKey(83852099) a2 = jax.random.uniform(mykey, [3, 3], jax.numpy.float32, minval=0, maxval=2) res1 = fn(a1.clone(), a2.clone()) print(res1)  15.913427 res2, _ = jax.value_and_grad(fn, (0,1))(a1.clone(), a2.clone()) print(res2)  30.159414 ```",2022-06-04T13:08:37Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/10984,"issue about `jax.numpy.arange` and `jax.numpy.fft.fftfreq` stem from: ```python import numpy as np print(np.arange(1.2, stop=4.8, step=0.24, dtype=np.float32)[1])  4.56 print(np.arange(np.float32(1.2), stop=np.float32(4.8), step=np.float32(0.24), dtype=np.float32)[1])  4.8 ```  should we convert pythonfloat to float32 to unify it? But that will cause discrepancy between jnp and np.",Any update?,"For `jax.numpy.trapz`, I find that `jit(func)` will have different results than `func` (a higher precision output). For example, `fn()` has a similar behavior with `trapz` ```python import jax from functools import partial def fn(x):     y = jax.numpy.array([1, 2, 3, 4, 5], dtype=jax.numpy.float32)     x = jax.numpy.diff(x)     return (x * y[..., 1:]) array = jax.numpy.array([8, 32, 16, 8.875, 48], dtype=jax.numpy.bfloat16) res1 = fn(array.clone()) print(res1)  [ 48.  48.  28.5 195. ] res2 = jax.jit(fn)(array.clone()) print(res2)  [ 48.    48.    28.5   195.625] res3, _ = jax.linearize(fn, array.clone()) print(res3)  [ 48.  48.  28.5 195. ] ``` The `jit` output is different than others. It seems that `jit` + `diff` causes this difference that it could return a higher precision results than the original result. Because when `array` has `float32` dtype, it has the same output with `jit` version. So for the `trapz`, `linearize` has different value with direct invocation. ```python import jax def fn(arg):     array1 = jax.numpy.array([1, 2, 3, 4, 5], dtype=jax.numpy.float32)     return jax.numpy.trapz(array1, arg).sum() array = jax.numpy.array([8, 32, 16, 8.875, 48], dtype=jax.numpy.bfloat16) res1 = fn(array.clone()) print(res1)  167.125 res2, _ = jax.linearize(fn, array.clone()) print(res2)  166.5 ``` Hi , do you think this is a bug or expected behavior? I am doubt that why the `linearize` doesn't use the `jit` version","This is definitely a bug  we try to maintain invariance to JIT, and in this case the problem is that in eager mode inputs are being computed on the host at tracetime at different precision than they would be on device.","I think the issue here comes from the handling of Python scalar inputs, and that their dtypes are only canonicalized when passed through the grad transform. For example, the `arange` difference comes down to this: ```python import numpy as np                                                                                                                                                                                                                                                                                 args = (1.2, 4.8, 0.24) args_f32 = map(np.float32, args) print(np.arange(*args).sum())  43.2 print(np.arange(*args_f32).sum())  48.00000190734863 ```", CC(dynamic_slice: correctly handle negative start indices in autodiff) should fix the issues related to `dynamic_slice`
1734,"以下是一个github上的jax下的一个issue, 标题是(jaxlib on nvidia jetson nano? Is it possible?)， 内容是 (Hi! I can install jax but not jaxlib on debian stretch with python3.9.  What version do I need to get this to work? Is it even possible? Thanks for any help!! Dockerfile: ``` RUN echo ""Installing dependencies..."" && \     aptget update && aptget install y \     buildessential \     git WORKDIR /usr/src/app COPY requirements.txt . RUN python3 m pip install upgrade pip RUN python3 m pip install upgrade Pillow RUN pip3 install r requirements.txt RUN pip3 install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html COPY . . CMD sleep infinity ``` I get this error: ``` ERROR: Could not find a version that satisfies the requirement jaxlib (from versions: none) ERROR: No matching distribution found for jaxlib ``` ``` The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/usr/src/app/app.py"", line 8, in      import jax   File ""/usr/local/lib/python3.9/sitepackages/jax/__init__.py"", line 37, in      from . import config as _config_module   File ""/usr/local/lib/python3.9/sitepackages/jax/config.py"", line 18, in      from jax._src.config import config   File ""/usr/local/lib/python3.9/sitepackages/jax/_src/config.py"", line 27, in      from jax._src import lib   File ""/usr/local/lib/python3.9/sitepackages/jax/_src/lib/__init__.py"", line 39, in      raise ModuleNotFoundError( ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jaxinstallation for installation instructions. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jaxlib on nvidia jetson nano? Is it possible?,"Hi! I can install jax but not jaxlib on debian stretch with python3.9.  What version do I need to get this to work? Is it even possible? Thanks for any help!! Dockerfile: ``` RUN echo ""Installing dependencies..."" && \     aptget update && aptget install y \     buildessential \     git WORKDIR /usr/src/app COPY requirements.txt . RUN python3 m pip install upgrade pip RUN python3 m pip install upgrade Pillow RUN pip3 install r requirements.txt RUN pip3 install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html COPY . . CMD sleep infinity ``` I get this error: ``` ERROR: Could not find a version that satisfies the requirement jaxlib (from versions: none) ERROR: No matching distribution found for jaxlib ``` ``` The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/usr/src/app/app.py"", line 8, in      import jax   File ""/usr/local/lib/python3.9/sitepackages/jax/__init__.py"", line 37, in      from . import config as _config_module   File ""/usr/local/lib/python3.9/sitepackages/jax/config.py"", line 18, in      from jax._src.config import config   File ""/usr/local/lib/python3.9/sitepackages/jax/_src/config.py"", line 27, in      from jax._src import lib   File ""/usr/local/lib/python3.9/sitepackages/jax/_src/lib/__init__.py"", line 39, in      raise ModuleNotFoundError( ModuleNotFoundError: jax requires jaxlib to be installed. See https://github.com/google/jaxinstallation for installation instructions. ```",2022-06-04T00:23:08Z,bug,closed,2,1,https://github.com/jax-ml/jax/issues/10981,"I don't know of any reason in particular why Jetson Nano wouldn't work, but you would have to recompile `jaxlib` for `arm`. We don't ship that wheel configuration ourselves, not having access to any Linux ARM hardware to test the result on. It might be possible (and advisable!) to crosscompile on a beefier machine rather than trying to compile on the Jetson machine itself. Closing as a duplicate of https://github.com/google/jax/issues/7097 (Linux Aarch64 wheels)."
2039,"以下是一个github上的jax下的一个issue, 标题是(Segmentation fault when solving large systems of ODEs)， 内容是 (Hi there, I've encountered a bug that occurs when trying to solve large systems of ODEs.  I'm trying to code an implementation of the technique described in this paper for diagonalising large manybody quantum systems. At the heart of the technique is an array of size (N,N,N,N) which I pass to `odeint`, along with a function that computes the differential equation I'm trying to solve. I have working versions of this method using SciPy/NumPy/numba and PyTorch, however JAX on the GPU gives me the best performance. For sufficiently large values of N (approximately N>48), however, I'm encountering a segmentation fault on certain systems. This doesn't seem to be related to the amount of available memory, as the segfault occurs even on a system with 256GB of RAM. The code works for any value of N on my M1 Macbook Pro running Mac OS X 12.3.1 with Python 3.9.7, jax 0.3.7 and clang 13.1.6. (Despite the warning that JAX on ARM machines is experimental!) A colleague of mine has confirmed that it also works on a Linux cluster he has access to, running Debian Buster with gcc 11.2.  The code _does not work_ for N>48 on my Linux workstation running Debian 11 Bullseye, Python 3.9.7, jax 0.3.7 and CUDA 11.2, tested with gcc versions 10.2.1, 11.3 and 12.1. It runs for around 10 minutes before failing with a segmentation fault, on both CPU and GPU. The gbd backtrace points towards `xla::ShapeUtil::MakeValidatedShape(xla::PrimitiveType, absl::lts_20211102::Span) ()`. Full gdb backtrace here: gdb.txt A cutdown, selfcontained example code which exhibits the issue is available here: https://github.com/sjt48/JAX_test (It's a toy code, so it won't output anything sensible. It can be run with `python test.py N` where `N` is an integer.) Let me know if there's any further information that I can provide.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Segmentation fault when solving large systems of ODEs,"Hi there, I've encountered a bug that occurs when trying to solve large systems of ODEs.  I'm trying to code an implementation of the technique described in this paper for diagonalising large manybody quantum systems. At the heart of the technique is an array of size (N,N,N,N) which I pass to `odeint`, along with a function that computes the differential equation I'm trying to solve. I have working versions of this method using SciPy/NumPy/numba and PyTorch, however JAX on the GPU gives me the best performance. For sufficiently large values of N (approximately N>48), however, I'm encountering a segmentation fault on certain systems. This doesn't seem to be related to the amount of available memory, as the segfault occurs even on a system with 256GB of RAM. The code works for any value of N on my M1 Macbook Pro running Mac OS X 12.3.1 with Python 3.9.7, jax 0.3.7 and clang 13.1.6. (Despite the warning that JAX on ARM machines is experimental!) A colleague of mine has confirmed that it also works on a Linux cluster he has access to, running Debian Buster with gcc 11.2.  The code _does not work_ for N>48 on my Linux workstation running Debian 11 Bullseye, Python 3.9.7, jax 0.3.7 and CUDA 11.2, tested with gcc versions 10.2.1, 11.3 and 12.1. It runs for around 10 minutes before failing with a segmentation fault, on both CPU and GPU. The gbd backtrace points towards `xla::ShapeUtil::MakeValidatedShape(xla::PrimitiveType, absl::lts_20211102::Span) ()`. Full gdb backtrace here: gdb.txt A cutdown, selfcontained example code which exhibits the issue is available here: https://github.com/sjt48/JAX_test (It's a toy code, so it won't output anything sensible. It can be run with `python test.py N` where `N` is an integer.) Let me know if there's any further information that I can provide.",2022-06-02T17:06:58Z,bug,open,1,3,https://github.com/jax-ml/jax/issues/10953,"It won't help resolve this precise bug, but you could try Diffrax, which is a full library for differential equation solving in JAX. Depending on how things pan out for you, this may either be (a) a workaround, or (b) an improvement over `odeint` anyway. (Of course it's possible that the same issue might occur with both, which would at least tell us something.)","Thanks kidger, I might give that a shot. I was aware of Diffrax, but I figured I'd start with `odeint` first, thinking it would be simpler (although in the end `odeint` isn't really flexible enough for what I need in later parts of this calculation, and I had to use a bit of a hacky solution). I'll give Diffrax a try and see if I find any different behaviour.","Quick update on this: Diffrax also segfaults for large values of N. The error is slightly different, but the backtrace also goes back to a problem with `xla_extension.so`, so it looks like the problem is deeper than the choice of ODE solver."
893,"以下是一个github上的jax下的一个issue, 标题是(truncated_normal can't handle extreme bounds)， 内容是 (`jax.random.truncated_normal` produces deeply wrong results when you set the lower bound too high or the upper bound too low. For example, ```python jax.random.truncated_normal(     jax.random.PRNGKey(1), jnp.array([5., 5.25, 5.5]), 1e6, (3,)) ``` results in ``` DeviceArray([5.1665773e+00, 9.9999994e+05, 9.9999994e+05], dtype=float32) ``` The transition is suspiciously close to the point at which `erf` runs out of precision: ```python lax.erf(jnp.array([5., 5.25, 5.5]) / np.sqrt(2)) ``` gives ``` DeviceArray([0.9999994, 1.       , 1.       ], dtype=float32) ``` So this may be a fundamental limitation of the samplingbyinversion scheme used in `truncated_normal`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,truncated_normal can't handle extreme bounds,"`jax.random.truncated_normal` produces deeply wrong results when you set the lower bound too high or the upper bound too low. For example, ```python jax.random.truncated_normal(     jax.random.PRNGKey(1), jnp.array([5., 5.25, 5.5]), 1e6, (3,)) ``` results in ``` DeviceArray([5.1665773e+00, 9.9999994e+05, 9.9999994e+05], dtype=float32) ``` The transition is suspiciously close to the point at which `erf` runs out of precision: ```python lax.erf(jnp.array([5., 5.25, 5.5]) / np.sqrt(2)) ``` gives ``` DeviceArray([0.9999994, 1.       , 1.       ], dtype=float32) ``` So this may be a fundamental limitation of the samplingbyinversion scheme used in `truncated_normal`.",2022-06-02T15:16:24Z,,open,1,1,https://github.com/jax-ml/jax/issues/10951,"Thanks for the report, I'm taking a look"
1213,"以下是一个github上的jax下的一个issue, 标题是(Convolutions leak host memory on Colab GPU runtimes)， 内容是 (Example: ```python import jax import jax.numpy as jnp import flax.linen as nn import os, psutil memusage = lambda: psutil.Process(os.getpid()).memory_info().rss class CNN(nn.Module):     .compact     def __call__(self, x):         x = nn.Conv(features=32, kernel_size=(5, 5))(x)         x = nn.Conv(features=3, kernel_size=(5, 5))(x)         return x SIZE = 32 .jit def predict(params):     model_inputs = jnp.zeros((1, SIZE, SIZE, 3))     correction = jnp.squeeze(model.apply(params, model_inputs))     return correction model = CNN() batch = jnp.ones((1, SIZE, SIZE, 3))   (batch, height, width, channels) params = model.init(jax.random.PRNGKey(0), batch) for i in range(1000):     _ = predict(params)     if i % 100 == 0:         print(f""i={i} {memusage() / 1e6:.2f} MB"") ``` Output: ``` i=0 3721.99 MB i=100 3724.69 MB i=200 3727.12 MB i=300 3729.56 MB i=400 3731.99 MB i=500 3734.69 MB i=600 3737.12 MB i=700 3739.55 MB i=800 3742.26 MB i=900 3744.69 MB ``` Reproducer on Colab)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Convolutions leak host memory on Colab GPU runtimes,"Example: ```python import jax import jax.numpy as jnp import flax.linen as nn import os, psutil memusage = lambda: psutil.Process(os.getpid()).memory_info().rss class CNN(nn.Module):     .compact     def __call__(self, x):         x = nn.Conv(features=32, kernel_size=(5, 5))(x)         x = nn.Conv(features=3, kernel_size=(5, 5))(x)         return x SIZE = 32 .jit def predict(params):     model_inputs = jnp.zeros((1, SIZE, SIZE, 3))     correction = jnp.squeeze(model.apply(params, model_inputs))     return correction model = CNN() batch = jnp.ones((1, SIZE, SIZE, 3))   (batch, height, width, channels) params = model.init(jax.random.PRNGKey(0), batch) for i in range(1000):     _ = predict(params)     if i % 100 == 0:         print(f""i={i} {memusage() / 1e6:.2f} MB"") ``` Output: ``` i=0 3721.99 MB i=100 3724.69 MB i=200 3727.12 MB i=300 3729.56 MB i=400 3731.99 MB i=500 3734.69 MB i=600 3737.12 MB i=700 3739.55 MB i=800 3742.26 MB i=900 3744.69 MB ``` Reproducer on Colab",2022-06-02T11:27:52Z,bug,closed,1,5,https://github.com/jax-ml/jax/issues/10949,"Doesn't leak on my workstation, only on Colab, so probably a bug in Colab's cuDNN or so :man_shrugging:  Might still be worth investigating  I recon that a lot of people use JAX through Colab.","Colab is still on ubuntu18 and python3.7. I'm not sure if these are a problem, but it makes it hard to work on this. Here is the colab tracking bug for this: https://github.com/googlecolab/colabtools/issues/1880 .  I was unable to reproduce this on a cloud instance with a matching GPU. I can check back on this when they finish their update to 20.04.","I have ran into the same problem as well. I boiled down the example to (the most minimal I can get it to, with help from  from google/flax CC(Remove check comparing shift/axis and input dimensions in np.roll)): ```python import resource import jax import jax.numpy as jnp kernel = jnp.zeros((16,16,3,3)) bias = 0 x = jnp.zeros((1,16,32,32)) .jit def apply(k, b, x):     x = jax.lax.conv_general_dilated(x, k, (1, 1), ""SAME"")     x = jax.lax.conv_general_dilated(x, k, (1, 1), ""SAME"")     x += b     return x def mem_snapshot():     return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / (1024 ** 2) print(f""Before: {mem_snapshot():.6f}GB"") for i in range(5000):     if i % 1000 == 999:         print(f""Iter {i+1}: {mem_snapshot():.6f}GB"")     _ = apply(kernel, bias, x) print(f""After: {mem_snapshot():.6f}GB"") ``` Output: ``` Before: 4.486221GB Iter 1000: 4.502346GB Iter 2000: 4.517956GB Iter 3000: 4.533566GB Iter 4000: 4.549175GB Iter 5000: 4.564785GB After: 4.564785GB ``` Colab notebook to reproduce: https://colab.research.google.com/drive/1oWSDpYIDUgFfAe26XqsX9UfaPe_nF4d?usp=sharing Any help would be appreciated! 🚀  Things I tried that **doesn't** show a memory leak:  Passing `k` and `b` as variables via the environment / closure instead of as formal parameters.  Removing the bias, or any one of the two convolution operations.  Smaller number of kernel dimensions (e.g. 1 instead of 16).  Smaller kernel sizes (e.g. `(1, 1)` instead of `(3, 3)`.","Thanks for the minimized reproducer. I'm seeing leaks from inside cudnn::fusion::ConvBiasActPatternMatchingEngine. Looks like it is the same underlying bug as this: https://github.com/microsoft/onnxruntime/issues/9643 It is fixed in newer cudnn versions. Indeed, if I upgrade to 11.2 in colab, I get this: ``` !sudo aptget install libcudnn8=8.1.1.331+cuda11.2 cuda112 y allowchangeheldpackages ``` ``` Before: 0.756866GB Iter 1000: 2.323483GB Iter 2000: 2.323483GB Iter 3000: 2.323483GB Iter 4000: 2.323483GB Iter 5000: 2.323483GB After: 2.323483GB ```",Thanks for the fix !
329,"以下是一个github上的jax下的一个issue, 标题是([x64] make jnp.average compatible with strict promotion)， 内容是 (Part of https://github.com/google/jax/pull/10865 and https://github.com/google/jax/pull/10840.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[x64] make jnp.average compatible with strict promotion,Part of https://github.com/google/jax/pull/10865 and https://github.com/google/jax/pull/10840.,2022-06-01T21:26:06Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10936
3816,"以下是一个github上的jax下的一个issue, 标题是(`Assertion` and `TypeError: Value Zero` with `custom_linear_solve` in some cases)， 内容是 (I've been trying to get to the bottom of some weirdness I'm encountering when using a matrixfree linear solver with gradients. I've aggressively trimmed and simplified everything to make a small reproducible example (see below) of what I've been encounter. It's not impossible that I'm doing something nonsensical that happens to not raise an error in some cases, especially after all the simplifications, but I'm starting to suspect a bug on JAX's side. Any help understanding the cause of these errors would be greatly appreciated! ```python import numpy as np import jax import jax.numpy as jnp import jaxopt import optax D = 1 dimension of x F = 10 number of features B = 100 X = np.zeros((B, D)) y = np.zeros((B, D)) def linear_apply(params, x):     if x.ndim   Unfiltered Stacktrace  ```   UnfilteredStackTrace                      Traceback (most recent call last)   [](https://localhost:8080/) in ()        84                                     weirdness_case=case,   > 85                                     model_apply=linear_apply)        86     print(f""Running case {case} with `affine_fourier_apply`"")   [](https://localhost:8080/) in one_step_fit(params, weirdness_case, model_apply)        66     loss, grads = jax.value_and_grad(loss_fn)(   > 67         params, weirdness_case, model_apply)        68     updates, opt_state = optimizer.update(grads, opt_state)   /usr/local/lib/python3.7/distpackages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)       161     try:   > 162       return fun(*args, **kwargs)       163     except Exception as e:   /usr/local/lib/python3.7/distpackages/jax/_src/api.py in value_and_grad_f(*args, **kwargs)       986     tree_map(partial(_check_output_dtype_grad, holomorphic), ans)   > 987     g = vjp_py(lax_internal._one(ans))       988     g = g[0] if isinstance(argnums, int) else g   /usr/local/lib/python3.7/distpackages/jax/_src/tree_util.py in __call__(self, *args, **kw)       286   def __call__(self, *args, **kw):   > 287     return self.fun(*args, **kw)       288    /usr/local/lib/python3.7/distpackages/jax/_src/api.py in _vjp_pullback_wrapper(cotangent_dtypes, cotangent_shapes, io_tree, fun, py_args)      2349           f""{ct_shape}."")   > 2350   ans = fun(*args)      2351   return tree_unflatten(out_tree, ans)   /usr/local/lib/python3.7/distpackages/jax/_src/tree_util.py in __call__(self, *args, **kw)       286   def __call__(self, *args, **kw):   > 287     return self.fun(*args, **kw)       288    /usr/local/lib/python3.7/distpackages/jax/interpreters/ad.py in unbound_vjp(pvals, jaxpr, consts, *cts)       136     dummy_args = [UndefinedPrimal(v.aval) for v in jaxpr.invars]   > 137     arg_cts = backward_pass(jaxpr, reduce_axes, True, consts, dummy_args, cts)       138     return map(instantiate_zeros, arg_cts)   /usr/local/lib/python3.7/distpackages/jax/interpreters/ad.py in backward_pass(jaxpr, reduce_axes, transform_stack, consts, primals_in, cotangents_in)       239           cts_out = get_primitive_transpose(eqn.primitive)(   > 240               cts_in, *invals, **eqn.params)       241         cts_out = [Zero(v.aval) for v in eqn.invars] if cts_out is Zero else cts_out   /usr/local/lib/python3.7/distpackages/jax/_src/lax/control_flow.py in _linear_solve_transpose_rule(cotangent, const_lengths, jaxprs, *primals)      2539   x_cotangent, _ = split_list(cotangent, [len(b)])   > 2540   assert all(ad.is_undefined_primal(x) for x in b)      2541   cotangent_b_full = linear_solve_p.bind(   UnfilteredStackTrace: AssertionError   ``` )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`Assertion` and `TypeError: Value Zero` with `custom_linear_solve` in some cases,"I've been trying to get to the bottom of some weirdness I'm encountering when using a matrixfree linear solver with gradients. I've aggressively trimmed and simplified everything to make a small reproducible example (see below) of what I've been encounter. It's not impossible that I'm doing something nonsensical that happens to not raise an error in some cases, especially after all the simplifications, but I'm starting to suspect a bug on JAX's side. Any help understanding the cause of these errors would be greatly appreciated! ```python import numpy as np import jax import jax.numpy as jnp import jaxopt import optax D = 1 dimension of x F = 10 number of features B = 100 X = np.zeros((B, D)) y = np.zeros((B, D)) def linear_apply(params, x):     if x.ndim   Unfiltered Stacktrace  ```   UnfilteredStackTrace                      Traceback (most recent call last)   [](https://localhost:8080/) in ()        84                                     weirdness_case=case,   > 85                                     model_apply=linear_apply)        86     print(f""Running case {case} with `affine_fourier_apply`"")   [](https://localhost:8080/) in one_step_fit(params, weirdness_case, model_apply)        66     loss, grads = jax.value_and_grad(loss_fn)(   > 67         params, weirdness_case, model_apply)        68     updates, opt_state = optimizer.update(grads, opt_state)   /usr/local/lib/python3.7/distpackages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)       161     try:   > 162       return fun(*args, **kwargs)       163     except Exception as e:   /usr/local/lib/python3.7/distpackages/jax/_src/api.py in value_and_grad_f(*args, **kwargs)       986     tree_map(partial(_check_output_dtype_grad, holomorphic), ans)   > 987     g = vjp_py(lax_internal._one(ans))       988     g = g[0] if isinstance(argnums, int) else g   /usr/local/lib/python3.7/distpackages/jax/_src/tree_util.py in __call__(self, *args, **kw)       286   def __call__(self, *args, **kw):   > 287     return self.fun(*args, **kw)       288    /usr/local/lib/python3.7/distpackages/jax/_src/api.py in _vjp_pullback_wrapper(cotangent_dtypes, cotangent_shapes, io_tree, fun, py_args)      2349           f""{ct_shape}."")   > 2350   ans = fun(*args)      2351   return tree_unflatten(out_tree, ans)   /usr/local/lib/python3.7/distpackages/jax/_src/tree_util.py in __call__(self, *args, **kw)       286   def __call__(self, *args, **kw):   > 287     return self.fun(*args, **kw)       288    /usr/local/lib/python3.7/distpackages/jax/interpreters/ad.py in unbound_vjp(pvals, jaxpr, consts, *cts)       136     dummy_args = [UndefinedPrimal(v.aval) for v in jaxpr.invars]   > 137     arg_cts = backward_pass(jaxpr, reduce_axes, True, consts, dummy_args, cts)       138     return map(instantiate_zeros, arg_cts)   /usr/local/lib/python3.7/distpackages/jax/interpreters/ad.py in backward_pass(jaxpr, reduce_axes, transform_stack, consts, primals_in, cotangents_in)       239           cts_out = get_primitive_transpose(eqn.primitive)(   > 240               cts_in, *invals, **eqn.params)       241         cts_out = [Zero(v.aval) for v in eqn.invars] if cts_out is Zero else cts_out   /usr/local/lib/python3.7/distpackages/jax/_src/lax/control_flow.py in _linear_solve_transpose_rule(cotangent, const_lengths, jaxprs, *primals)      2539   x_cotangent, _ = split_list(cotangent, [len(b)])   > 2540   assert all(ad.is_undefined_primal(x) for x in b)      2541   cotangent_b_full = linear_solve_p.bind(   UnfilteredStackTrace: AssertionError   ``` ",2022-06-01T19:09:33Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/10929, Would you have some time to check this out?
3273,"以下是一个github上的jax下的一个issue, 标题是(Add odeint max step size optional argument)， 内容是 (It'd be nice if `odeint` had an optional argument analogous to scipy `solve_ivp`'s `max_step` argument, which controls the maximum step size. I'll copy a code example below, but I'm running into an issue with `odeint` in which it steps over a feature in the ODE `rhs` after a longish period for which `rhs=0.` One minor issue is that `max_step` unfortunately has a naming collision with `odeint`'s optional arg `mxstep`, which is an integer controlling the total number of iterations `odeint` can take before exiting, so this name may not be usable without changing `mxstep` which may not be desirable. Aside from choosing the argument name, I think implementing this is nearly trivial: just need to add the argument, and I believe change line 199 to take the minimum of the max step size and the computed optimal step. The example I post below could be rolled into a unittest validating this behaviour (perhaps with comparison directly with scipy). Thoughts? Given how simple it is I'm happy to implement this myself, but would definitely defer to the core JAX developers for what argument name to use (or for any insight into why this might be an issue with other parts of `odeint`/JAX control flow).  Motivating example ``` import jax jax.config.update(""jax_enable_x64"", True) jax.config.update('jax_platform_name', 'cpu') import jax.numpy as jnp from jax.experimental.ode import odeint dt = jnp.array(1.) k = 5 def deriv(t):     t = jnp.array(t)     return jnp.piecewise(         t,         [t = k * dt) & (t <= (k + 2) * dt)],         [lambda s: jnp.array(1.), lambda s: jnp.array(1.), lambda s: jnp.array(0.)]     ) def rhs(y, t):     return deriv(t) odeint(     func=rhs,      y0=jnp.array(0.),      t=jnp.array([0., k*dt, 2*k*dt]),     atol=1e8, rtol=1e8 ) ``` Using `odeint`, the above code is essentially integrating a function which is `1.` over the interval `[0., 2*dt]`, `1.` over the interval `[k*dt, (k+1)*dt]`, and zero elsewhere. If integrated over an interval containing both nonzero intervals, the correct result should be `0.`. However , the above code outputs: ``` DeviceArray([0.        , 1.99999565, 1.99999565], dtype=float64) ``` and changing the tolerance doesn't change this (it just yields better or worse approximations of `[0., 2., 2.]`) The analogous code in scipy using `solve_ivp` with `method='RK45'` yields the same results, but with `solve_ivp` this can be corrected by setting `max_step=dt`. Shortening the intermediate `0.` interval by either reducing `dt` or `k` both solve the problem as well, so I'm pretty confident it's an issue of step size overshoot.  Slightly more context The above is a minimal example, but the similarities to the situation in which I'm using `odeint` and the above are:   I don't know ahead of time if a period of `0.` will be present or for how long.   I do however have something analogous to `dt`  basically a natural time scale over which to restrict the step so that features aren't missed, but that also isn't so short that it will totally mess with the efficiency of the solver.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add odeint max step size optional argument,"It'd be nice if `odeint` had an optional argument analogous to scipy `solve_ivp`'s `max_step` argument, which controls the maximum step size. I'll copy a code example below, but I'm running into an issue with `odeint` in which it steps over a feature in the ODE `rhs` after a longish period for which `rhs=0.` One minor issue is that `max_step` unfortunately has a naming collision with `odeint`'s optional arg `mxstep`, which is an integer controlling the total number of iterations `odeint` can take before exiting, so this name may not be usable without changing `mxstep` which may not be desirable. Aside from choosing the argument name, I think implementing this is nearly trivial: just need to add the argument, and I believe change line 199 to take the minimum of the max step size and the computed optimal step. The example I post below could be rolled into a unittest validating this behaviour (perhaps with comparison directly with scipy). Thoughts? Given how simple it is I'm happy to implement this myself, but would definitely defer to the core JAX developers for what argument name to use (or for any insight into why this might be an issue with other parts of `odeint`/JAX control flow).  Motivating example ``` import jax jax.config.update(""jax_enable_x64"", True) jax.config.update('jax_platform_name', 'cpu') import jax.numpy as jnp from jax.experimental.ode import odeint dt = jnp.array(1.) k = 5 def deriv(t):     t = jnp.array(t)     return jnp.piecewise(         t,         [t = k * dt) & (t <= (k + 2) * dt)],         [lambda s: jnp.array(1.), lambda s: jnp.array(1.), lambda s: jnp.array(0.)]     ) def rhs(y, t):     return deriv(t) odeint(     func=rhs,      y0=jnp.array(0.),      t=jnp.array([0., k*dt, 2*k*dt]),     atol=1e8, rtol=1e8 ) ``` Using `odeint`, the above code is essentially integrating a function which is `1.` over the interval `[0., 2*dt]`, `1.` over the interval `[k*dt, (k+1)*dt]`, and zero elsewhere. If integrated over an interval containing both nonzero intervals, the correct result should be `0.`. However , the above code outputs: ``` DeviceArray([0.        , 1.99999565, 1.99999565], dtype=float64) ``` and changing the tolerance doesn't change this (it just yields better or worse approximations of `[0., 2., 2.]`) The analogous code in scipy using `solve_ivp` with `method='RK45'` yields the same results, but with `solve_ivp` this can be corrected by setting `max_step=dt`. Shortening the intermediate `0.` interval by either reducing `dt` or `k` both solve the problem as well, so I'm pretty confident it's an issue of step size overshoot.  Slightly more context The above is a minimal example, but the similarities to the situation in which I'm using `odeint` and the above are:   I don't know ahead of time if a period of `0.` will be present or for how long.   I do however have something analogous to `dt`  basically a natural time scale over which to restrict the step so that features aren't missed, but that also isn't so short that it will totally mess with the efficiency of the solver.",2022-06-01T16:22:43Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/10922,"Have a look at Diffrax, which supports this as: ```python diffeqsolve(..., stepsize_controller=PIDController(..., dtmax=...)) ```",kidger we are actually in the process of adding `diffrax` as a dependency to the project in which this came up :) .  In any case it would still be a nice feature to have in `odeint`
10375,"以下是一个github上的jax下的一个issue, 标题是(`bincount` will fail with `value_and_grads` if `length=False`)， 内容是 (Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import jax def fn(weights):     mykey = jax.random.PRNGKey(95208508)     x = jax.random.randint(mykey, [10], 1, 3, jax.numpy.int8)     length = False     return jax.numpy.bincount(x, weights, minlength=20, length=length).sum() mykey = jax.random.PRNGKey(65116306) array = jax.random.uniform(mykey, [10], jax.numpy.float32, minval=16, maxval=4) weights = array.clone() print(fn(weights))  0.0 weights = array.clone() jax.value_and_grad(fn, (0))(arg_1)  TypeError: Slice size at index 0 in gather op is out of range, must be within [0, 0 + 1), got 1. ```  [x] If applicable, include full error messages/tracebacks. ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_module_as_main(***failed resolving arguments***)     196         sys.argv[0] = mod_spec.origin > 197     return _run_code(code, main_globals, None,     198                      ""__main__"", mod_spec) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_code(***failed resolving arguments***)      86                        __spec__ = mod_spec) > 87     exec(code, run_globals)      88     return run_globals ~/Library/Python/3.9/lib/python/sitepackages/ipykernel_launcher.py in       15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() ~/Library/Python/3.9/lib/python/sitepackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     844         app.initialize(argv) > 845         app.start()     846  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     618             try: > 619                 self.io_loop.start()     620             except KeyboardInterrupt: ~/Library/Python/3.9/lib/python/sitepackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     198             asyncio.set_event_loop(self.asyncio_loop) > 199             self.asyncio_loop.run_forever()     200         finally: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in run_forever(***failed resolving arguments***)     600             while True: > 601                 self._run_once()     602                 if self._stopping: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1904             else: > 1905                 handle._run()    1906         handle = None   Needed to break cycles when an exception occurs. /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py in _run(***failed resolving arguments***)      79         try: > 80             self._context.run(self._callback, *self._args)      81         except (SystemExit, KeyboardInterrupt): ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in (***failed resolving arguments***)     687             future.add_done_callback( > 688                 lambda f: self._run_callback(functools.partial(callback, future))     689             ) ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in _run_callback(***failed resolving arguments***)     740         try: > 741             ret = callback()     742             if ret is not None: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in inner(***failed resolving arguments***)     813                 f = None   noqa: F841 > 814                 self.ctx_run(self.run)     815  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in run(***failed resolving arguments***)     774                     else: > 775                         yielded = self.gen.send(value)     776  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in process_one(***failed resolving arguments***)     357                 return None > 358         yield gen.maybe_future(dispatch(*args))     359  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     260             try: > 261                 yield gen.maybe_future(handler(stream, idents, msg))     262             except Exception: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     535         reply_content = yield gen.maybe_future( > 536             self.do_execute(     537                 code, silent, store_history, ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     301                  letting shell dispatch to loop runners > 302                 res = shell.run_cell(code, store_history=store_history, silent=silent)     303         finally: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     538         self._last_traceback = None > 539         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     540  ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2897         try: > 2898             result = self._run_cell(    2899                 raw_cell, store_history, silent, shell_futures) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in _run_cell(***failed resolving arguments***)    2943         try: > 2944             return runner(coro)    2945         except BaseException as e: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/async_helpers.py in _pseudo_sync_runner(***failed resolving arguments***)      67     try: > 68         coro.send(None)      69     except StopIteration as exc: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell_async(***failed resolving arguments***)    3168  > 3169                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3170                        interactivity=interactivity, compiler=compiler, result=result) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    3360                         asy = compare(code) > 3361                     if (await self.run_code(code, result,  async_=asy)):    3362                         return True ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    3440                 else: > 3441                     exec(code_obj, self.user_global_ns, self.user_ns)    3442             finally:  in       15 weights = array.clone() > 16 jax.value_and_grad(fn, (0))(arg_1)  in fn(***failed resolving arguments***)       6     length = False > 7     return jax.numpy.bincount(x, weights, minlength=20, length=length).sum()       8  /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py in bincount(***failed resolving arguments***)    1036     raise ValueError(""shape of weights must match shape of x."") > 1037   return zeros(length, _dtype(weights)).at[clip(x, 0)].add(weights)    1038  /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py in add(***failed resolving arguments***)    4873     """""" > 4874     return scatter._scatter_update(self.array, self.index, values,    4875                                    lax.scatter_add, /usr/local/lib/python3.9/sitepackages/jax/_src/ops/scatter.py in _scatter_update(***failed resolving arguments***)      69   treedef, static_idx, dynamic_idx = jnp._split_index_for_jit(idx, x.shape) > 70   return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,      71                        indices_are_sorted, unique_indices, mode, /usr/local/lib/python3.9/sitepackages/jax/_src/ops/scatter.py in _scatter_impl(***failed resolving arguments***)     108   ) > 109   out = scatter_op(     110     x, indexer.gather_indices, y, dnums, JaxStackTraceBeforeTransformation: TypeError: Slice size at index 0 in gather op is out of range, must be within [0, 0 + 1), got 1. The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last)  in       14       15 weights = array.clone() > 16 jax.value_and_grad(fn, (0))(arg_1)     [... skipping hidden 30 frame] /usr/local/lib/python3.9/sitepackages/jax/_src/lax/slicing.py in _gather_shape_rule(operand, indices, dimension_numbers, slice_sizes, unique_indices, indices_are_sorted, mode, fill_value)    1143     if not (core.greater_equal_dim(slice_size, 0) and    1144             core.greater_equal_dim(corresponding_input_size, slice_size)): > 1145       raise TypeError(f""Slice size at index {i} in gather op is out of range, ""    1146                       f""must be within [0, {corresponding_input_size} + 1), ""    1147                       f""got {slice_size}."") TypeError: Slice size at index 0 in gather op is out of range, must be within [0, 0 + 1), got 1. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`bincount` will fail with `value_and_grads` if `length=False`,"Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import jax def fn(weights):     mykey = jax.random.PRNGKey(95208508)     x = jax.random.randint(mykey, [10], 1, 3, jax.numpy.int8)     length = False     return jax.numpy.bincount(x, weights, minlength=20, length=length).sum() mykey = jax.random.PRNGKey(65116306) array = jax.random.uniform(mykey, [10], jax.numpy.float32, minval=16, maxval=4) weights = array.clone() print(fn(weights))  0.0 weights = array.clone() jax.value_and_grad(fn, (0))(arg_1)  TypeError: Slice size at index 0 in gather op is out of range, must be within [0, 0 + 1), got 1. ```  [x] If applicable, include full error messages/tracebacks. ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_module_as_main(***failed resolving arguments***)     196         sys.argv[0] = mod_spec.origin > 197     return _run_code(code, main_globals, None,     198                      ""__main__"", mod_spec) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_code(***failed resolving arguments***)      86                        __spec__ = mod_spec) > 87     exec(code, run_globals)      88     return run_globals ~/Library/Python/3.9/lib/python/sitepackages/ipykernel_launcher.py in       15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() ~/Library/Python/3.9/lib/python/sitepackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     844         app.initialize(argv) > 845         app.start()     846  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     618             try: > 619                 self.io_loop.start()     620             except KeyboardInterrupt: ~/Library/Python/3.9/lib/python/sitepackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     198             asyncio.set_event_loop(self.asyncio_loop) > 199             self.asyncio_loop.run_forever()     200         finally: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in run_forever(***failed resolving arguments***)     600             while True: > 601                 self._run_once()     602                 if self._stopping: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1904             else: > 1905                 handle._run()    1906         handle = None   Needed to break cycles when an exception occurs. /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py in _run(***failed resolving arguments***)      79         try: > 80             self._context.run(self._callback, *self._args)      81         except (SystemExit, KeyboardInterrupt): ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in (***failed resolving arguments***)     687             future.add_done_callback( > 688                 lambda f: self._run_callback(functools.partial(callback, future))     689             ) ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in _run_callback(***failed resolving arguments***)     740         try: > 741             ret = callback()     742             if ret is not None: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in inner(***failed resolving arguments***)     813                 f = None   noqa: F841 > 814                 self.ctx_run(self.run)     815  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in run(***failed resolving arguments***)     774                     else: > 775                         yielded = self.gen.send(value)     776  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in process_one(***failed resolving arguments***)     357                 return None > 358         yield gen.maybe_future(dispatch(*args))     359  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     260             try: > 261                 yield gen.maybe_future(handler(stream, idents, msg))     262             except Exception: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     535         reply_content = yield gen.maybe_future( > 536             self.do_execute(     537                 code, silent, store_history, ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     301                  letting shell dispatch to loop runners > 302                 res = shell.run_cell(code, store_history=store_history, silent=silent)     303         finally: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     538         self._last_traceback = None > 539         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     540  ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2897         try: > 2898             result = self._run_cell(    2899                 raw_cell, store_history, silent, shell_futures) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in _run_cell(***failed resolving arguments***)    2943         try: > 2944             return runner(coro)    2945         except BaseException as e: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/async_helpers.py in _pseudo_sync_runner(***failed resolving arguments***)      67     try: > 68         coro.send(None)      69     except StopIteration as exc: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell_async(***failed resolving arguments***)    3168  > 3169                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3170                        interactivity=interactivity, compiler=compiler, result=result) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    3360                         asy = compare(code) > 3361                     if (await self.run_code(code, result,  async_=asy)):    3362                         return True ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    3440                 else: > 3441                     exec(code_obj, self.user_global_ns, self.user_ns)    3442             finally:  in       15 weights = array.clone() > 16 jax.value_and_grad(fn, (0))(arg_1)  in fn(***failed resolving arguments***)       6     length = False > 7     return jax.numpy.bincount(x, weights, minlength=20, length=length).sum()       8  /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py in bincount(***failed resolving arguments***)    1036     raise ValueError(""shape of weights must match shape of x."") > 1037   return zeros(length, _dtype(weights)).at[clip(x, 0)].add(weights)    1038  /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py in add(***failed resolving arguments***)    4873     """""" > 4874     return scatter._scatter_update(self.array, self.index, values,    4875                                    lax.scatter_add, /usr/local/lib/python3.9/sitepackages/jax/_src/ops/scatter.py in _scatter_update(***failed resolving arguments***)      69   treedef, static_idx, dynamic_idx = jnp._split_index_for_jit(idx, x.shape) > 70   return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,      71                        indices_are_sorted, unique_indices, mode, /usr/local/lib/python3.9/sitepackages/jax/_src/ops/scatter.py in _scatter_impl(***failed resolving arguments***)     108   ) > 109   out = scatter_op(     110     x, indexer.gather_indices, y, dnums, JaxStackTraceBeforeTransformation: TypeError: Slice size at index 0 in gather op is out of range, must be within [0, 0 + 1), got 1. The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last)  in       14       15 weights = array.clone() > 16 jax.value_and_grad(fn, (0))(arg_1)     [... skipping hidden 30 frame] /usr/local/lib/python3.9/sitepackages/jax/_src/lax/slicing.py in _gather_shape_rule(operand, indices, dimension_numbers, slice_sizes, unique_indices, indices_are_sorted, mode, fill_value)    1143     if not (core.greater_equal_dim(slice_size, 0) and    1144             core.greater_equal_dim(corresponding_input_size, slice_size)): > 1145       raise TypeError(f""Slice size at index {i} in gather op is out of range, ""    1146                       f""must be within [0, {corresponding_input_size} + 1), ""    1147                       f""got {slice_size}."") TypeError: Slice size at index 0 in gather op is out of range, must be within [0, 0 + 1), got 1. ```",2022-06-01T15:18:25Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/10920,Thanks for the report  closing as a duplicate of CC(jvp rules don't validate static args in the same way as their primitives)
11114,"以下是一个github上的jax下的一个issue, 标题是(`average` will succeed if `axis=False` but fail when using `value_and_grads`)， 内容是 (Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: `average` will succeed if `axis=False` but fail when using `value_and_grads` ```python import jax def fn(arg_0):     return jax.numpy.average(arg_0, axis=False).sum() mykey = jax.random.PRNGKey(92127318) array = jax.random.uniform(mykey, [2, 3, 4], jax.numpy.float32, minval=0, maxval=2) arg_0 = array.clone() print(fn(arg_0))  12.970601 arg_0 = array.clone() jax.value_and_grad(fn, (0))(arg_0)  ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 3 ```  [x] If applicable, include full error messages/tracebacks. ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_module_as_main(***failed resolving arguments***)     196         sys.argv[0] = mod_spec.origin > 197     return _run_code(code, main_globals, None,     198                      ""__main__"", mod_spec) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_code(***failed resolving arguments***)      86                        __spec__ = mod_spec) > 87     exec(code, run_globals)      88     return run_globals ~/Library/Python/3.9/lib/python/sitepackages/ipykernel_launcher.py in       15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() ~/Library/Python/3.9/lib/python/sitepackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     844         app.initialize(argv) > 845         app.start()     846  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     618             try: > 619                 self.io_loop.start()     620             except KeyboardInterrupt: ~/Library/Python/3.9/lib/python/sitepackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     198             asyncio.set_event_loop(self.asyncio_loop) > 199             self.asyncio_loop.run_forever()     200         finally: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in run_forever(***failed resolving arguments***)     600             while True: > 601                 self._run_once()     602                 if self._stopping: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1904             else: > 1905                 handle._run()    1906         handle = None   Needed to break cycles when an exception occurs. /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py in _run(***failed resolving arguments***)      79         try: > 80             self._context.run(self._callback, *self._args)      81         except (SystemExit, KeyboardInterrupt): ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in (***failed resolving arguments***)     687             future.add_done_callback( > 688                 lambda f: self._run_callback(functools.partial(callback, future))     689             ) ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in _run_callback(***failed resolving arguments***)     740         try: > 741             ret = callback()     742             if ret is not None: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in inner(***failed resolving arguments***)     813                 f = None   noqa: F841 > 814                 self.ctx_run(self.run)     815  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in run(***failed resolving arguments***)     774                     else: > 775                         yielded = self.gen.send(value)     776  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in process_one(***failed resolving arguments***)     357                 return None > 358         yield gen.maybe_future(dispatch(*args))     359  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     260             try: > 261                 yield gen.maybe_future(handler(stream, idents, msg))     262             except Exception: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     535         reply_content = yield gen.maybe_future( > 536             self.do_execute(     537                 code, silent, store_history, ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     301                  letting shell dispatch to loop runners > 302                 res = shell.run_cell(code, store_history=store_history, silent=silent)     303         finally: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     538         self._last_traceback = None > 539         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     540  ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2897         try: > 2898             result = self._run_cell(    2899                 raw_cell, store_history, silent, shell_futures) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in _run_cell(***failed resolving arguments***)    2943         try: > 2944             return runner(coro)    2945         except BaseException as e: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/async_helpers.py in _pseudo_sync_runner(***failed resolving arguments***)      67     try: > 68         coro.send(None)      69     except StopIteration as exc: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell_async(***failed resolving arguments***)    3168  > 3169                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3170                        interactivity=interactivity, compiler=compiler, result=result) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    3360                         asy = compare(code) > 3361                     if (await self.run_code(code, result,  async_=asy)):    3362                         return True ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    3440                 else: > 3441                     exec(code_obj, self.user_global_ns, self.user_ns)    3442             finally:  in       17     arg_0 = arg_0_array.clone() > 18     results['res_2'], _ = jax.value_and_grad(fn, (0))(arg_0)      19 except Exception as e:  in fn(***failed resolving arguments***)       5     returned = False > 6     return jax.numpy.average(arg_0, arg_1, returned=returned).sum()       7 try: /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in average(***failed resolving arguments***)     284             returned=False): > 285   return _average(a, _ensure_optional_axes(axis), weights, returned)     286  /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _average(***failed resolving arguments***)     292   if weights is None:  Treat all weights as 1 > 293     avg = mean(a, axis=axis)     294     if axis is None: /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in mean(***failed resolving arguments***)     251          out=None, keepdims=False, *, where=None): > 252   return _mean(a, _ensure_optional_axes(axis), dtype, out, keepdims,     253                where=where) /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _mean(***failed resolving arguments***)     278   return lax.div( > 279       sum(a, axis, dtype=dtype, keepdims=keepdims, where=where),     280       lax.convert_element_type(normalizer, dtype)) /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in sum(***failed resolving arguments***)     164         out=None, keepdims=None, initial=None, where=None): > 165   return _reduce_sum(a, axis=_ensure_optional_axes(axis), dtype=dtype, out=out,     166                      keepdims=keepdims, initial=initial, where=where) /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _reduce_sum(***failed resolving arguments***)     156                 dtype=None, out=None, keepdims=None, initial=None, where=None): > 157   return _reduction(a, ""sum"", np.sum, lax.add, 0,     158                     bool_op=lax.bitwise_or, upcast_f16_for_computation=True, /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _reduction(***failed resolving arguments***)      97   else: > 98     result = lax.reduce(a, init_val, op, dims)      99   if initial is not None: JaxStackTraceBeforeTransformation: ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 3 The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: ValueError                                Traceback (most recent call last)  in       11       12 arg_0 = array.clone() > 13 jax.value_and_grad(fn, (0))(arg_0)     [... skipping hidden 37 frame]  in delete(*args, **kwargs) /usr/local/lib/python3.9/sitepackages/numpy/lib/function_base.py in delete(arr, obj, axis)    4470         if obj.dtype == bool:    4471             if obj.shape != (N,): > 4472                 raise ValueError('boolean array argument obj to delete '    4473                                  'must be one dimensional and match the axis '    4474                                  'length of {}'.format(N)) ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 3 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`average` will succeed if `axis=False` but fail when using `value_and_grads`,"Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: `average` will succeed if `axis=False` but fail when using `value_and_grads` ```python import jax def fn(arg_0):     return jax.numpy.average(arg_0, axis=False).sum() mykey = jax.random.PRNGKey(92127318) array = jax.random.uniform(mykey, [2, 3, 4], jax.numpy.float32, minval=0, maxval=2) arg_0 = array.clone() print(fn(arg_0))  12.970601 arg_0 = array.clone() jax.value_and_grad(fn, (0))(arg_0)  ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 3 ```  [x] If applicable, include full error messages/tracebacks. ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_module_as_main(***failed resolving arguments***)     196         sys.argv[0] = mod_spec.origin > 197     return _run_code(code, main_globals, None,     198                      ""__main__"", mod_spec) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_code(***failed resolving arguments***)      86                        __spec__ = mod_spec) > 87     exec(code, run_globals)      88     return run_globals ~/Library/Python/3.9/lib/python/sitepackages/ipykernel_launcher.py in       15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() ~/Library/Python/3.9/lib/python/sitepackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     844         app.initialize(argv) > 845         app.start()     846  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     618             try: > 619                 self.io_loop.start()     620             except KeyboardInterrupt: ~/Library/Python/3.9/lib/python/sitepackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     198             asyncio.set_event_loop(self.asyncio_loop) > 199             self.asyncio_loop.run_forever()     200         finally: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in run_forever(***failed resolving arguments***)     600             while True: > 601                 self._run_once()     602                 if self._stopping: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1904             else: > 1905                 handle._run()    1906         handle = None   Needed to break cycles when an exception occurs. /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py in _run(***failed resolving arguments***)      79         try: > 80             self._context.run(self._callback, *self._args)      81         except (SystemExit, KeyboardInterrupt): ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in (***failed resolving arguments***)     687             future.add_done_callback( > 688                 lambda f: self._run_callback(functools.partial(callback, future))     689             ) ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in _run_callback(***failed resolving arguments***)     740         try: > 741             ret = callback()     742             if ret is not None: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in inner(***failed resolving arguments***)     813                 f = None   noqa: F841 > 814                 self.ctx_run(self.run)     815  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in run(***failed resolving arguments***)     774                     else: > 775                         yielded = self.gen.send(value)     776  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in process_one(***failed resolving arguments***)     357                 return None > 358         yield gen.maybe_future(dispatch(*args))     359  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     260             try: > 261                 yield gen.maybe_future(handler(stream, idents, msg))     262             except Exception: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     535         reply_content = yield gen.maybe_future( > 536             self.do_execute(     537                 code, silent, store_history, ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     301                  letting shell dispatch to loop runners > 302                 res = shell.run_cell(code, store_history=store_history, silent=silent)     303         finally: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     538         self._last_traceback = None > 539         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     540  ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2897         try: > 2898             result = self._run_cell(    2899                 raw_cell, store_history, silent, shell_futures) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in _run_cell(***failed resolving arguments***)    2943         try: > 2944             return runner(coro)    2945         except BaseException as e: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/async_helpers.py in _pseudo_sync_runner(***failed resolving arguments***)      67     try: > 68         coro.send(None)      69     except StopIteration as exc: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell_async(***failed resolving arguments***)    3168  > 3169                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3170                        interactivity=interactivity, compiler=compiler, result=result) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    3360                         asy = compare(code) > 3361                     if (await self.run_code(code, result,  async_=asy)):    3362                         return True ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    3440                 else: > 3441                     exec(code_obj, self.user_global_ns, self.user_ns)    3442             finally:  in       17     arg_0 = arg_0_array.clone() > 18     results['res_2'], _ = jax.value_and_grad(fn, (0))(arg_0)      19 except Exception as e:  in fn(***failed resolving arguments***)       5     returned = False > 6     return jax.numpy.average(arg_0, arg_1, returned=returned).sum()       7 try: /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in average(***failed resolving arguments***)     284             returned=False): > 285   return _average(a, _ensure_optional_axes(axis), weights, returned)     286  /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _average(***failed resolving arguments***)     292   if weights is None:  Treat all weights as 1 > 293     avg = mean(a, axis=axis)     294     if axis is None: /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in mean(***failed resolving arguments***)     251          out=None, keepdims=False, *, where=None): > 252   return _mean(a, _ensure_optional_axes(axis), dtype, out, keepdims,     253                where=where) /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _mean(***failed resolving arguments***)     278   return lax.div( > 279       sum(a, axis, dtype=dtype, keepdims=keepdims, where=where),     280       lax.convert_element_type(normalizer, dtype)) /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in sum(***failed resolving arguments***)     164         out=None, keepdims=None, initial=None, where=None): > 165   return _reduce_sum(a, axis=_ensure_optional_axes(axis), dtype=dtype, out=out,     166                      keepdims=keepdims, initial=initial, where=where) /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _reduce_sum(***failed resolving arguments***)     156                 dtype=None, out=None, keepdims=None, initial=None, where=None): > 157   return _reduction(a, ""sum"", np.sum, lax.add, 0,     158                     bool_op=lax.bitwise_or, upcast_f16_for_computation=True, /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _reduction(***failed resolving arguments***)      97   else: > 98     result = lax.reduce(a, init_val, op, dims)      99   if initial is not None: JaxStackTraceBeforeTransformation: ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 3 The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: ValueError                                Traceback (most recent call last)  in       11       12 arg_0 = array.clone() > 13 jax.value_and_grad(fn, (0))(arg_0)     [... skipping hidden 37 frame]  in delete(*args, **kwargs) /usr/local/lib/python3.9/sitepackages/numpy/lib/function_base.py in delete(arr, obj, axis)    4470         if obj.dtype == bool:    4471             if obj.shape != (N,): > 4472                 raise ValueError('boolean array argument obj to delete '    4473                                  'must be one dimensional and match the axis '    4474                                  'length of {}'.format(N)) ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 3 ```",2022-06-01T15:13:06Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/10919,Thanks for the report  closing as a duplicate of CC(jvp rules don't validate static args in the same way as their primitives)
10316,"以下是一个github上的jax下的一个issue, 标题是(`softmax` will fail when using `value_and_grads`)， 内容是 (Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: `softmax` will fail when using `value_and_grads` but succeed when calling it directly ```python import jax def fn(arg_0):     return jax.nn.softmax(arg_0, axis=False).sum() mykey = jax.random.PRNGKey(5386231) array = jax.random.uniform(mykey, [10, 5], jax.numpy.float32, minval=64, maxval=0) arg_0 = array.clone() print(fn(arg_0))  5 arg_0 = array.clone() jax.value_and_grad(fn, (0))(arg_0)  ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 2 ```  [x] If applicable, include full error messages/tracebacks. ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_module_as_main(***failed resolving arguments***)     196         sys.argv[0] = mod_spec.origin > 197     return _run_code(code, main_globals, None,     198                      ""__main__"", mod_spec) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_code(***failed resolving arguments***)      86                        __spec__ = mod_spec) > 87     exec(code, run_globals)      88     return run_globals ~/Library/Python/3.9/lib/python/sitepackages/ipykernel_launcher.py in       15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() ~/Library/Python/3.9/lib/python/sitepackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     844         app.initialize(argv) > 845         app.start()     846  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     618             try: > 619                 self.io_loop.start()     620             except KeyboardInterrupt: ~/Library/Python/3.9/lib/python/sitepackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     198             asyncio.set_event_loop(self.asyncio_loop) > 199             self.asyncio_loop.run_forever()     200         finally: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in run_forever(***failed resolving arguments***)     600             while True: > 601                 self._run_once()     602                 if self._stopping: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1904             else: > 1905                 handle._run()    1906         handle = None   Needed to break cycles when an exception occurs. /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py in _run(***failed resolving arguments***)      79         try: > 80             self._context.run(self._callback, *self._args)      81         except (SystemExit, KeyboardInterrupt): ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in (***failed resolving arguments***)     687             future.add_done_callback( > 688                 lambda f: self._run_callback(functools.partial(callback, future))     689             ) ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in _run_callback(***failed resolving arguments***)     740         try: > 741             ret = callback()     742             if ret is not None: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in inner(***failed resolving arguments***)     813                 f = None   noqa: F841 > 814                 self.ctx_run(self.run)     815  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in run(***failed resolving arguments***)     774                     else: > 775                         yielded = self.gen.send(value)     776  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in process_one(***failed resolving arguments***)     357                 return None > 358         yield gen.maybe_future(dispatch(*args))     359  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     260             try: > 261                 yield gen.maybe_future(handler(stream, idents, msg))     262             except Exception: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     535         reply_content = yield gen.maybe_future( > 536             self.do_execute(     537                 code, silent, store_history, ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     301                  letting shell dispatch to loop runners > 302                 res = shell.run_cell(code, store_history=store_history, silent=silent)     303         finally: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     538         self._last_traceback = None > 539         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     540  ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2897         try: > 2898             result = self._run_cell(    2899                 raw_cell, store_history, silent, shell_futures) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in _run_cell(***failed resolving arguments***)    2943         try: > 2944             return runner(coro)    2945         except BaseException as e: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/async_helpers.py in _pseudo_sync_runner(***failed resolving arguments***)      67     try: > 68         coro.send(None)      69     except StopIteration as exc: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell_async(***failed resolving arguments***)    3168  > 3169                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3170                        interactivity=interactivity, compiler=compiler, result=result) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    3360                         asy = compare(code) > 3361                     if (await self.run_code(code, result,  async_=asy)):    3362                         return True ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    3440                 else: > 3441                     exec(code_obj, self.user_global_ns, self.user_ns)    3442             finally:  in       16     arg_0 = arg_0_array.clone() > 17     results['res_2'], _ = jax.value_and_grad(fn, (0))(arg_0)      18 except Exception as e:  in fn(***failed resolving arguments***)       4     axis = False > 5     return jax.nn.softmax(arg_0, axis=axis).sum()       6 try: /usr/local/lib/python3.9/sitepackages/jax/_src/nn/functions.py in softmax(***failed resolving arguments***)     333   unnormalized = jnp.exp(x  lax.stop_gradient(x_max)) > 334   return unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)     335  /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in sum(***failed resolving arguments***)     164         out=None, keepdims=None, initial=None, where=None): > 165   return _reduce_sum(a, axis=_ensure_optional_axes(axis), dtype=dtype, out=out,     166                      keepdims=keepdims, initial=initial, where=where) /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _reduce_sum(***failed resolving arguments***)     156                 dtype=None, out=None, keepdims=None, initial=None, where=None): > 157   return _reduction(a, ""sum"", np.sum, lax.add, 0,     158                     bool_op=lax.bitwise_or, upcast_f16_for_computation=True, /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _reduction(***failed resolving arguments***)      97   else: > 98     result = lax.reduce(a, init_val, op, dims)      99   if initial is not None: JaxStackTraceBeforeTransformation: ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 2 The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: ValueError                                Traceback (most recent call last)  in       11       12 arg_0 = array.clone() > 13 jax.value_and_grad(fn, (0))(arg_0)     [... skipping hidden 23 frame]  in delete(*args, **kwargs) /usr/local/lib/python3.9/sitepackages/numpy/lib/function_base.py in delete(arr, obj, axis)    4470         if obj.dtype == bool:    4471             if obj.shape != (N,): > 4472                 raise ValueError('boolean array argument obj to delete '    4473                                  'must be one dimensional and match the axis '    4474                                  'length of {}'.format(N)) ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 2 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`softmax` will fail when using `value_and_grads`,"Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: `softmax` will fail when using `value_and_grads` but succeed when calling it directly ```python import jax def fn(arg_0):     return jax.nn.softmax(arg_0, axis=False).sum() mykey = jax.random.PRNGKey(5386231) array = jax.random.uniform(mykey, [10, 5], jax.numpy.float32, minval=64, maxval=0) arg_0 = array.clone() print(fn(arg_0))  5 arg_0 = array.clone() jax.value_and_grad(fn, (0))(arg_0)  ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 2 ```  [x] If applicable, include full error messages/tracebacks. ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_module_as_main(***failed resolving arguments***)     196         sys.argv[0] = mod_spec.origin > 197     return _run_code(code, main_globals, None,     198                      ""__main__"", mod_spec) /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py in _run_code(***failed resolving arguments***)      86                        __spec__ = mod_spec) > 87     exec(code, run_globals)      88     return run_globals ~/Library/Python/3.9/lib/python/sitepackages/ipykernel_launcher.py in       15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() ~/Library/Python/3.9/lib/python/sitepackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     844         app.initialize(argv) > 845         app.start()     846  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     618             try: > 619                 self.io_loop.start()     620             except KeyboardInterrupt: ~/Library/Python/3.9/lib/python/sitepackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     198             asyncio.set_event_loop(self.asyncio_loop) > 199             self.asyncio_loop.run_forever()     200         finally: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in run_forever(***failed resolving arguments***)     600             while True: > 601                 self._run_once()     602                 if self._stopping: /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1904             else: > 1905                 handle._run()    1906         handle = None   Needed to break cycles when an exception occurs. /usr/local/Cellar/python.9/3.9.12_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py in _run(***failed resolving arguments***)      79         try: > 80             self._context.run(self._callback, *self._args)      81         except (SystemExit, KeyboardInterrupt): ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in (***failed resolving arguments***)     687             future.add_done_callback( > 688                 lambda f: self._run_callback(functools.partial(callback, future))     689             ) ~/Library/Python/3.9/lib/python/sitepackages/tornado/ioloop.py in _run_callback(***failed resolving arguments***)     740         try: > 741             ret = callback()     742             if ret is not None: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in inner(***failed resolving arguments***)     813                 f = None   noqa: F841 > 814                 self.ctx_run(self.run)     815  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in run(***failed resolving arguments***)     774                     else: > 775                         yielded = self.gen.send(value)     776  ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in process_one(***failed resolving arguments***)     357                 return None > 358         yield gen.maybe_future(dispatch(*args))     359  ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     260             try: > 261                 yield gen.maybe_future(handler(stream, idents, msg))     262             except Exception: ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     535         reply_content = yield gen.maybe_future( > 536             self.do_execute(     537                 code, silent, store_history, ~/Library/Python/3.9/lib/python/sitepackages/tornado/gen.py in wrapper(***failed resolving arguments***)     233                 try: > 234                     yielded = ctx_run(next, result)     235                 except (StopIteration, Return) as e: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     301                  letting shell dispatch to loop runners > 302                 res = shell.run_cell(code, store_history=store_history, silent=silent)     303         finally: ~/Library/Python/3.9/lib/python/sitepackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     538         self._last_traceback = None > 539         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     540  ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2897         try: > 2898             result = self._run_cell(    2899                 raw_cell, store_history, silent, shell_futures) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in _run_cell(***failed resolving arguments***)    2943         try: > 2944             return runner(coro)    2945         except BaseException as e: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/async_helpers.py in _pseudo_sync_runner(***failed resolving arguments***)      67     try: > 68         coro.send(None)      69     except StopIteration as exc: ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_cell_async(***failed resolving arguments***)    3168  > 3169                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3170                        interactivity=interactivity, compiler=compiler, result=result) ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    3360                         asy = compare(code) > 3361                     if (await self.run_code(code, result,  async_=asy)):    3362                         return True ~/Library/Python/3.9/lib/python/sitepackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    3440                 else: > 3441                     exec(code_obj, self.user_global_ns, self.user_ns)    3442             finally:  in       16     arg_0 = arg_0_array.clone() > 17     results['res_2'], _ = jax.value_and_grad(fn, (0))(arg_0)      18 except Exception as e:  in fn(***failed resolving arguments***)       4     axis = False > 5     return jax.nn.softmax(arg_0, axis=axis).sum()       6 try: /usr/local/lib/python3.9/sitepackages/jax/_src/nn/functions.py in softmax(***failed resolving arguments***)     333   unnormalized = jnp.exp(x  lax.stop_gradient(x_max)) > 334   return unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)     335  /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in sum(***failed resolving arguments***)     164         out=None, keepdims=None, initial=None, where=None): > 165   return _reduce_sum(a, axis=_ensure_optional_axes(axis), dtype=dtype, out=out,     166                      keepdims=keepdims, initial=initial, where=where) /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _reduce_sum(***failed resolving arguments***)     156                 dtype=None, out=None, keepdims=None, initial=None, where=None): > 157   return _reduction(a, ""sum"", np.sum, lax.add, 0,     158                     bool_op=lax.bitwise_or, upcast_f16_for_computation=True, /usr/local/lib/python3.9/sitepackages/jax/_src/numpy/reductions.py in _reduction(***failed resolving arguments***)      97   else: > 98     result = lax.reduce(a, init_val, op, dims)      99   if initial is not None: JaxStackTraceBeforeTransformation: ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 2 The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: ValueError                                Traceback (most recent call last)  in       11       12 arg_0 = array.clone() > 13 jax.value_and_grad(fn, (0))(arg_0)     [... skipping hidden 23 frame]  in delete(*args, **kwargs) /usr/local/lib/python3.9/sitepackages/numpy/lib/function_base.py in delete(arr, obj, axis)    4470         if obj.dtype == bool:    4471             if obj.shape != (N,): > 4472                 raise ValueError('boolean array argument obj to delete '    4473                                  'must be one dimensional and match the axis '    4474                                  'length of {}'.format(N)) ValueError: boolean array argument obj to delete must be one dimensional and match the axis length of 2 ```",2022-06-01T15:08:55Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/10918,"The `axis` argument should be an integer or sequence of integers, as mentioned in the docs. The fact that `axis=False`, an invalid input, has inconsistent behavior, is not something we check for, and I think that can probably be chalkedup to user error in this case. What do you think?","In my opinion, I expect that the behavior of `value_and_grads` should output the same with direct call. So, maybe they should have the same error checking mechanism. I am not quite sure.",Thanks  closing as a duplicate of CC(jvp rules don't validate static args in the same way as their primitives)
10666,"以下是一个github上的jax下的一个issue, 标题是(jvp rules don't validate static args in the same way as their primitives)， 内容是 (Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: `cummax, cummin, cumprod` will succeed when using `value_and_grads` but fail if directly call ```python import jax def fn(arg_0):     axis = 1     return jax.lax.cummax(arg_0, axis=axis).sum() mykey = jax.random.PRNGKey(14830444) array = jax.random.uniform(mykey, [2, 3], jax.numpy.float32, minval=0, maxval=128) try:     arg_0 = array.clone()     res1 = fn(arg_0) except Exception as e:     print(e) arg_0 = array.clone() res2, _ = jax.value_and_grad(fn, (0))(arg_0) print(res2)  axis 1 is out of bounds for array of shape (2, 3)  676.8106 ``` ```python import jax def fn(arg_0):     axis = 1     return jax.lax.cummin(arg_0, axis=axis).sum() mykey = jax.random.PRNGKey(14830444) array = jax.random.uniform(mykey, [2, 3], jax.numpy.float32, minval=0, maxval=128) try:     arg_0 = array.clone()     res1 = fn(arg_0) except Exception as e:     print(e) arg_0 = array.clone() res2, _ = jax.value_and_grad(fn, (0))(arg_0) print(res2)  axis 1 is out of bounds for array of shape (2, 3)  549.35925 ``` ```python import jax def fn(arg_0):     axis = 1     return jax.lax.cumprod(arg_0, axis=axis).sum() mykey = jax.random.PRNGKey(14830444) array = jax.random.uniform(mykey, [2, 3], jax.numpy.float32, minval=0, maxval=128) try:     arg_0 = array.clone()     res1 = fn(arg_0) except Exception as e:     print(e) arg_0 = array.clone() res2, _ = jax.value_and_grad(fn, (0))(arg_0) print(res2)  axis 1 is out of bounds for array of shape (2, 3)  1533018.6 ```  [x] If applicable, include full error messages/tracebacks. ```  ValueError                                Traceback (most recent call last)  in       15       16 arg_0 = array.clone() > 17 res1 = fn(arg_0)  in fn(arg_0)       3 def fn(arg_0):       4     axis = 1 > 5     return jax.lax.cummin(arg_0, axis=axis).sum()       6        7 mykey = jax.random.PRNGKey(14830444) /usr/local/lib/python3.9/sitepackages/jax/_src/lax/control_flow.py in cummin(operand, axis, reverse)    2869 def cummin(operand: Array, axis: int = 0, reverse: bool = False) > Array:    2870   """"""Computes a cumulative minimum along `axis`."""""" > 2871   return cummin_p.bind(operand, axis=int(axis), reverse=bool(reverse))    2872     2873 def _cumred_shape_rule(x, *, axis: int, reverse: bool): /usr/local/lib/python3.9/sitepackages/jax/core.py in bind(self, *args, **params)     321     assert (not config.jax_enable_checks or     322             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 323     return self.bind_with_trace(find_top_trace(args), args, params)     324      325   def bind_with_trace(self, trace, args, params): /usr/local/lib/python3.9/sitepackages/jax/core.py in bind_with_trace(self, trace, args, params)     324      325   def bind_with_trace(self, trace, args, params): > 326     out = trace.process_primitive(self, map(trace.full_raise, args), params)     327     return map(full_lower, out) if self.multiple_results else full_lower(out)     328  /usr/local/lib/python3.9/sitepackages/jax/core.py in process_primitive(self, primitive, tracers, params)     673      674   def process_primitive(self, primitive, tracers, params): > 675     return primitive.impl(*tracers, **params)     676      677   def process_call(self, primitive, f, tracers, params): /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in apply_primitive(prim, *args, **params)      96 def apply_primitive(prim, *args, **params):      97   """"""Impl rule that compiles and runs a single primitive 'prim' using XLA."""""" > 98   compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),      99                                         **params)     100   return compiled_fun(*args) /usr/local/lib/python3.9/sitepackages/jax/_src/util.py in wrapper(*args, **kwargs)     217         return f(*args, **kwargs)     218       else: > 219         return cached(config._trace_context(), *args, **kwargs)     220      221     wrapper.cache_clear = cached.cache_clear /usr/local/lib/python3.9/sitepackages/jax/_src/util.py in cached(_, *args, **kwargs)     210     .lru_cache(max_size)     211     def cached(_, *args, **kwargs): > 212       return f(*args, **kwargs)     213      214     .wraps(f) /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in xla_primitive_callable(prim, *arg_specs, **params)     146     else:     147       return out, > 148   compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), device, None,     149                                     prim.name, donated_invars, False, *arg_specs)     150   if not prim.multiple_results: /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in _xla_callable_uncached(fun, device, backend, name, donated_invars, keep_unused, *arg_specs)     228 def _xla_callable_uncached(fun: lu.WrappedFun, device, backend, name,     229                            donated_invars, keep_unused, *arg_specs): > 230   return lower_xla_callable(fun, device, backend, name, donated_invars, False,     231                             keep_unused, *arg_specs).compile().unsafe_call     232  /usr/local/lib/python3.9/sitepackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     204   def wrapper(*args, **kwargs):     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper     208   return wrapper /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in lower_xla_callable(fun, device, backend, name, donated_invars, always_lower, keep_unused, *arg_specs)     270   with log_elapsed_time(f""Finished tracing + transforming {fun.__name__} ""     271                         ""for jit in {elapsed_time} sec""): > 272     jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(     273         fun, abstract_args, pe.debug_info_final(fun, ""jit""), which_explicit)     274   if any(isinstance(c, core.Tracer) for c in consts): /usr/local/lib/python3.9/sitepackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     204   def wrapper(*args, **kwargs):     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper     208   return wrapper /usr/local/lib/python3.9/sitepackages/jax/interpreters/partial_eval.py in trace_to_jaxpr_final(fun, in_avals, debug_info, keep_inputs)    1891     main.jaxpr_stack = ()   type: ignore    1892     with core.new_sublevel(): > 1893       jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    1894         fun, main, in_avals, keep_inputs=keep_inputs)    1895     del fun, main /usr/local/lib/python3.9/sitepackages/jax/interpreters/partial_eval.py in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs)    1863     in_tracers = _input_type_to_tracers(trace, in_avals)    1864     in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 1865     ans = fun.call_wrapped(*in_tracers_)    1866     out_tracers = map(trace.full_raise, ans)    1867     jaxpr, consts = frame.to_jaxpr(out_tracers) /usr/local/lib/python3.9/sitepackages/jax/linear_util.py in call_wrapped(self, *args, **kwargs)     166      167     try: > 168       ans = self.f(*args, **dict(self.params, **kwargs))     169     except:     170        Some transformations yield from inside context managers, so we have to /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in prim_fun(*args)     141   device = _device_from_arg_devices(arg_devices)     142   def prim_fun(*args): > 143     out = prim.bind(*args, **params)     144     if prim.multiple_results:     145       return out /usr/local/lib/python3.9/sitepackages/jax/core.py in bind(self, *args, **params)     321     assert (not config.jax_enable_checks or     322             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 323     return self.bind_with_trace(find_top_trace(args), args, params)     324      325   def bind_with_trace(self, trace, args, params): /usr/local/lib/python3.9/sitepackages/jax/core.py in bind_with_trace(self, trace, args, params)     324      325   def bind_with_trace(self, trace, args, params): > 326     out = trace.process_primitive(self, map(trace.full_raise, args), params)     327     return map(full_lower, out) if self.multiple_results else full_lower(out)     328  /usr/local/lib/python3.9/sitepackages/jax/interpreters/partial_eval.py in process_primitive(self, primitive, tracers, params)    1558     if primitive in custom_staging_rules:    1559       return custom_staging_rulesprimitive > 1560     return self.default_process_primitive(primitive, tracers, params)    1561     1562   def default_process_primitive(self, primitive, tracers, params): /usr/local/lib/python3.9/sitepackages/jax/interpreters/partial_eval.py in default_process_primitive(self, primitive, tracers, params)    1562   def default_process_primitive(self, primitive, tracers, params):    1563     avals = [t.aval for t in tracers] > 1564     out_avals, effects = primitive.abstract_eval(*avals, **params)    1565     out_avals = [out_avals] if not primitive.multiple_results else out_avals    1566     source_info = source_info_util.current() /usr/local/lib/python3.9/sitepackages/jax/core.py in abstract_eval_(*args, **kwargs)     357 def _effect_free_abstract_eval(abstract_eval):     358   def abstract_eval_(*args, **kwargs): > 359     return abstract_eval(*args, **kwargs), no_effects     360   return abstract_eval_     361  /usr/local/lib/python3.9/sitepackages/jax/_src/lax/utils.py in standard_abstract_eval(prim, shape_rule, dtype_rule, weak_type_rule, named_shape_rule, *avals, **kwargs)      64     return core.ConcreteArray(out.dtype, out, weak_type=weak_type)      65   elif least_specialized is core.ShapedArray: > 66     return core.ShapedArray(shape_rule(*avals, **kwargs),      67                             dtype_rule(*avals, **kwargs), weak_type=weak_type,      68                             named_shape=named_shape_rule(*avals, **kwargs)) /usr/local/lib/python3.9/sitepackages/jax/_src/lax/control_flow.py in _cumred_shape_rule(x, axis, reverse)    2873 def _cumred_shape_rule(x, *, axis: int, reverse: bool):    2874   if axis = x.ndim: > 2875     raise ValueError(    2876         ""axis {} is out of bounds for array of shape {}"".format(axis, x.shape))    2877   return x.shape ValueError: axis 1 is out of bounds for array of shape (2, 3) ``` version: 0.3.13)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jvp rules don't validate static args in the same way as their primitives,"Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: `cummax, cummin, cumprod` will succeed when using `value_and_grads` but fail if directly call ```python import jax def fn(arg_0):     axis = 1     return jax.lax.cummax(arg_0, axis=axis).sum() mykey = jax.random.PRNGKey(14830444) array = jax.random.uniform(mykey, [2, 3], jax.numpy.float32, minval=0, maxval=128) try:     arg_0 = array.clone()     res1 = fn(arg_0) except Exception as e:     print(e) arg_0 = array.clone() res2, _ = jax.value_and_grad(fn, (0))(arg_0) print(res2)  axis 1 is out of bounds for array of shape (2, 3)  676.8106 ``` ```python import jax def fn(arg_0):     axis = 1     return jax.lax.cummin(arg_0, axis=axis).sum() mykey = jax.random.PRNGKey(14830444) array = jax.random.uniform(mykey, [2, 3], jax.numpy.float32, minval=0, maxval=128) try:     arg_0 = array.clone()     res1 = fn(arg_0) except Exception as e:     print(e) arg_0 = array.clone() res2, _ = jax.value_and_grad(fn, (0))(arg_0) print(res2)  axis 1 is out of bounds for array of shape (2, 3)  549.35925 ``` ```python import jax def fn(arg_0):     axis = 1     return jax.lax.cumprod(arg_0, axis=axis).sum() mykey = jax.random.PRNGKey(14830444) array = jax.random.uniform(mykey, [2, 3], jax.numpy.float32, minval=0, maxval=128) try:     arg_0 = array.clone()     res1 = fn(arg_0) except Exception as e:     print(e) arg_0 = array.clone() res2, _ = jax.value_and_grad(fn, (0))(arg_0) print(res2)  axis 1 is out of bounds for array of shape (2, 3)  1533018.6 ```  [x] If applicable, include full error messages/tracebacks. ```  ValueError                                Traceback (most recent call last)  in       15       16 arg_0 = array.clone() > 17 res1 = fn(arg_0)  in fn(arg_0)       3 def fn(arg_0):       4     axis = 1 > 5     return jax.lax.cummin(arg_0, axis=axis).sum()       6        7 mykey = jax.random.PRNGKey(14830444) /usr/local/lib/python3.9/sitepackages/jax/_src/lax/control_flow.py in cummin(operand, axis, reverse)    2869 def cummin(operand: Array, axis: int = 0, reverse: bool = False) > Array:    2870   """"""Computes a cumulative minimum along `axis`."""""" > 2871   return cummin_p.bind(operand, axis=int(axis), reverse=bool(reverse))    2872     2873 def _cumred_shape_rule(x, *, axis: int, reverse: bool): /usr/local/lib/python3.9/sitepackages/jax/core.py in bind(self, *args, **params)     321     assert (not config.jax_enable_checks or     322             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 323     return self.bind_with_trace(find_top_trace(args), args, params)     324      325   def bind_with_trace(self, trace, args, params): /usr/local/lib/python3.9/sitepackages/jax/core.py in bind_with_trace(self, trace, args, params)     324      325   def bind_with_trace(self, trace, args, params): > 326     out = trace.process_primitive(self, map(trace.full_raise, args), params)     327     return map(full_lower, out) if self.multiple_results else full_lower(out)     328  /usr/local/lib/python3.9/sitepackages/jax/core.py in process_primitive(self, primitive, tracers, params)     673      674   def process_primitive(self, primitive, tracers, params): > 675     return primitive.impl(*tracers, **params)     676      677   def process_call(self, primitive, f, tracers, params): /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in apply_primitive(prim, *args, **params)      96 def apply_primitive(prim, *args, **params):      97   """"""Impl rule that compiles and runs a single primitive 'prim' using XLA."""""" > 98   compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),      99                                         **params)     100   return compiled_fun(*args) /usr/local/lib/python3.9/sitepackages/jax/_src/util.py in wrapper(*args, **kwargs)     217         return f(*args, **kwargs)     218       else: > 219         return cached(config._trace_context(), *args, **kwargs)     220      221     wrapper.cache_clear = cached.cache_clear /usr/local/lib/python3.9/sitepackages/jax/_src/util.py in cached(_, *args, **kwargs)     210     .lru_cache(max_size)     211     def cached(_, *args, **kwargs): > 212       return f(*args, **kwargs)     213      214     .wraps(f) /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in xla_primitive_callable(prim, *arg_specs, **params)     146     else:     147       return out, > 148   compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), device, None,     149                                     prim.name, donated_invars, False, *arg_specs)     150   if not prim.multiple_results: /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in _xla_callable_uncached(fun, device, backend, name, donated_invars, keep_unused, *arg_specs)     228 def _xla_callable_uncached(fun: lu.WrappedFun, device, backend, name,     229                            donated_invars, keep_unused, *arg_specs): > 230   return lower_xla_callable(fun, device, backend, name, donated_invars, False,     231                             keep_unused, *arg_specs).compile().unsafe_call     232  /usr/local/lib/python3.9/sitepackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     204   def wrapper(*args, **kwargs):     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper     208   return wrapper /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in lower_xla_callable(fun, device, backend, name, donated_invars, always_lower, keep_unused, *arg_specs)     270   with log_elapsed_time(f""Finished tracing + transforming {fun.__name__} ""     271                         ""for jit in {elapsed_time} sec""): > 272     jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(     273         fun, abstract_args, pe.debug_info_final(fun, ""jit""), which_explicit)     274   if any(isinstance(c, core.Tracer) for c in consts): /usr/local/lib/python3.9/sitepackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     204   def wrapper(*args, **kwargs):     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper     208   return wrapper /usr/local/lib/python3.9/sitepackages/jax/interpreters/partial_eval.py in trace_to_jaxpr_final(fun, in_avals, debug_info, keep_inputs)    1891     main.jaxpr_stack = ()   type: ignore    1892     with core.new_sublevel(): > 1893       jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    1894         fun, main, in_avals, keep_inputs=keep_inputs)    1895     del fun, main /usr/local/lib/python3.9/sitepackages/jax/interpreters/partial_eval.py in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs)    1863     in_tracers = _input_type_to_tracers(trace, in_avals)    1864     in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 1865     ans = fun.call_wrapped(*in_tracers_)    1866     out_tracers = map(trace.full_raise, ans)    1867     jaxpr, consts = frame.to_jaxpr(out_tracers) /usr/local/lib/python3.9/sitepackages/jax/linear_util.py in call_wrapped(self, *args, **kwargs)     166      167     try: > 168       ans = self.f(*args, **dict(self.params, **kwargs))     169     except:     170        Some transformations yield from inside context managers, so we have to /usr/local/lib/python3.9/sitepackages/jax/_src/dispatch.py in prim_fun(*args)     141   device = _device_from_arg_devices(arg_devices)     142   def prim_fun(*args): > 143     out = prim.bind(*args, **params)     144     if prim.multiple_results:     145       return out /usr/local/lib/python3.9/sitepackages/jax/core.py in bind(self, *args, **params)     321     assert (not config.jax_enable_checks or     322             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 323     return self.bind_with_trace(find_top_trace(args), args, params)     324      325   def bind_with_trace(self, trace, args, params): /usr/local/lib/python3.9/sitepackages/jax/core.py in bind_with_trace(self, trace, args, params)     324      325   def bind_with_trace(self, trace, args, params): > 326     out = trace.process_primitive(self, map(trace.full_raise, args), params)     327     return map(full_lower, out) if self.multiple_results else full_lower(out)     328  /usr/local/lib/python3.9/sitepackages/jax/interpreters/partial_eval.py in process_primitive(self, primitive, tracers, params)    1558     if primitive in custom_staging_rules:    1559       return custom_staging_rulesprimitive > 1560     return self.default_process_primitive(primitive, tracers, params)    1561     1562   def default_process_primitive(self, primitive, tracers, params): /usr/local/lib/python3.9/sitepackages/jax/interpreters/partial_eval.py in default_process_primitive(self, primitive, tracers, params)    1562   def default_process_primitive(self, primitive, tracers, params):    1563     avals = [t.aval for t in tracers] > 1564     out_avals, effects = primitive.abstract_eval(*avals, **params)    1565     out_avals = [out_avals] if not primitive.multiple_results else out_avals    1566     source_info = source_info_util.current() /usr/local/lib/python3.9/sitepackages/jax/core.py in abstract_eval_(*args, **kwargs)     357 def _effect_free_abstract_eval(abstract_eval):     358   def abstract_eval_(*args, **kwargs): > 359     return abstract_eval(*args, **kwargs), no_effects     360   return abstract_eval_     361  /usr/local/lib/python3.9/sitepackages/jax/_src/lax/utils.py in standard_abstract_eval(prim, shape_rule, dtype_rule, weak_type_rule, named_shape_rule, *avals, **kwargs)      64     return core.ConcreteArray(out.dtype, out, weak_type=weak_type)      65   elif least_specialized is core.ShapedArray: > 66     return core.ShapedArray(shape_rule(*avals, **kwargs),      67                             dtype_rule(*avals, **kwargs), weak_type=weak_type,      68                             named_shape=named_shape_rule(*avals, **kwargs)) /usr/local/lib/python3.9/sitepackages/jax/_src/lax/control_flow.py in _cumred_shape_rule(x, axis, reverse)    2873 def _cumred_shape_rule(x, *, axis: int, reverse: bool):    2874   if axis = x.ndim: > 2875     raise ValueError(    2876         ""axis {} is out of bounds for array of shape {}"".format(axis, x.shape))    2877   return x.shape ValueError: axis 1 is out of bounds for array of shape (2, 3) ``` version: 0.3.13",2022-06-01T14:27:28Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/10916,"Thanks for the report – in general `jax.lax` functions are quite restrictive on their inputs, and don't allow negative axes. It appears that the axis is somehow not being validated in the case of the autodiff rule for these functions – the correct behavior would be to raise an error there as well.","I changed the title, because I think this (and dup'd issues) stem from a more fundamental issue: static input validation is not consistent across transformed primitives. One way we might address this comprehensively is to (1) make sure that all validation happens within abstract evaluation rules (I think this is *mostly* true currently), and (2) make sure that those abstract evaluation rules are called in the dispatch patch for these transforms (this is not true currently) The cost here would be some amount of tracetime overhead, but I think it would be better than a piecemeal solution. Any thoughts?",I am not sure which way is better. But there are ~20 APIs suffer from such issue.,I'm going to assign to  for now because he may have more context on this.,"> in general `jax.lax` functions are quite restrictive on their inputs, and don't allow negative axes. What's the reason for this? Is it documented somewhere? ```python3 import jax x = jax.numpy.arange(10) jax.lax.cummax(x, x.ndim  1) jax.lax.cummax(x, 1)  ValueError: axis 1 is out of bounds for array of shape (10,) ```","The reason for this is that `jax.lax` are basically Python wrappers of XLA operations, and XLA does not generally allow negative axes specifications.","  Thanks. Could the error message perhaps be modified as follows, for clarity? ```python3 if axis = x.ndim:     raise ValueError(f""axis {axis} is out of bounds for array of shape {x.shape}"") else:     return x.shape ```"
411,"以下是一个github上的jax下的一个issue, 标题是(Remove experimental warning from Mac ARM wheels.)， 内容是 (Enough JAX developers now have Mac ARM machines that we can be reasonably confident of noticing problems, even if we don't yet have CI coverage. Issue CC(Provide wheels for macOS ARM))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Remove experimental warning from Mac ARM wheels.,"Enough JAX developers now have Mac ARM machines that we can be reasonably confident of noticing problems, even if we don't yet have CI coverage. Issue CC(Provide wheels for macOS ARM)",2022-06-01T00:58:08Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/10910,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
494,"以下是一个github上的jax下的一个issue, 标题是(Store profiler server as a global variable and add a `stop_server` function)， 内容是 (Currently, the user needs to store around the return value of `jax.profiler.start_server` or the server will immediately be destroyed. This change stores the server in a global variable and exposes a way of destroying it via `stop_server`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Store profiler server as a global variable and add a `stop_server` function,"Currently, the user needs to store around the return value of `jax.profiler.start_server` or the server will immediately be destroyed. This change stores the server in a global variable and exposes a way of destroying it via `stop_server`.",2022-05-31T22:45:43Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/10906,Thank you for the review!
1881,"以下是一个github上的jax下的一个issue, 标题是(Wrapping a slow Python function in an asynchronous DeviceArray)， 内容是 (I have a slow Python computation that produces a pytree (in the motivating case, it loads weights over a network).  I know the shapes and dtypes in advance, and would like to be able to wrap up my function as a `jnp.DeviceArray` so that it can be used as if it was a normal array by further jax computation.  Assuming for the moment that instead of a pytree our function returns a single array, the code would look like ```python3 def async_wrap(f, *, shape, dtype):   """"""Produces a jnp.DeviceArray of given shape and dtype whose value if f().   `f()` is run on a separate thread, so that this function returns immediately.   If `f()` can't be converted to an array of the given shape and dtype, an exception is thrown.   """"""   ... def slow():   time.sleep(10)   Note that this releases the GIL   return jnp.arange(4, dtype=np.int32) .jit def square(x):   return x * x def main():    Returns quickly   x = async_wrap(slow, shape=(4,), dtype=np.int32)    Add more jax computation on top of x.    The compilation of square is overlapped with slow().    Returns either immediately or once compilation completes (I forget what happens normally)   y = square(x)    Blocks for about 10 seconds, then prints roughly [0, 1, 4, 9]   print(y) ``` Naively, I'd expect that `async_wrap` (which is a terrible name) could mostly reuse existing asynchrony machinery inside Jax, but I'm not confident of that. The actual `async_wrap` interface would want to support pytrees.  Ignoring the number of threads created, the pytree version is implementable on top of the single array version, but we probably don't want to ignore the number of threads created.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Wrapping a slow Python function in an asynchronous DeviceArray,"I have a slow Python computation that produces a pytree (in the motivating case, it loads weights over a network).  I know the shapes and dtypes in advance, and would like to be able to wrap up my function as a `jnp.DeviceArray` so that it can be used as if it was a normal array by further jax computation.  Assuming for the moment that instead of a pytree our function returns a single array, the code would look like ```python3 def async_wrap(f, *, shape, dtype):   """"""Produces a jnp.DeviceArray of given shape and dtype whose value if f().   `f()` is run on a separate thread, so that this function returns immediately.   If `f()` can't be converted to an array of the given shape and dtype, an exception is thrown.   """"""   ... def slow():   time.sleep(10)   Note that this releases the GIL   return jnp.arange(4, dtype=np.int32) .jit def square(x):   return x * x def main():    Returns quickly   x = async_wrap(slow, shape=(4,), dtype=np.int32)    Add more jax computation on top of x.    The compilation of square is overlapped with slow().    Returns either immediately or once compilation completes (I forget what happens normally)   y = square(x)    Blocks for about 10 seconds, then prints roughly [0, 1, 4, 9]   print(y) ``` Naively, I'd expect that `async_wrap` (which is a terrible name) could mostly reuse existing asynchrony machinery inside Jax, but I'm not confident of that. The actual `async_wrap` interface would want to support pytrees.  Ignoring the number of threads created, the pytree version is implementable on top of the single array version, but we probably don't want to ignore the number of threads created.",2022-05-31T20:00:38Z,enhancement,open,1,6,https://github.com/jax-ml/jax/issues/10897,"I should say that I am confused whether I should want to pass something like `concurrent.futures.Future` or pass a function and have Jax handle the threading.  The former requires me (as user of Jax) to have a `concurrent.futures.Executor` which is annoying, so if it's reasonable for Jax to manage things that would be my weak preference.","My understanding is that the asyncronous dispatch used by JAX is within the C++ layer, and so there's not any direct access to this from Pythonlevel async constructs, but someone might be able to tell me I'm wrong 😁 "," That's not an obvious blocker, as long as you can pass a general `std::function` to that C++ layer.","I'd find it useful to factor out discussion of async execution mechanisms from the request to support a futurebacked jax array. Imagine 's original `async_wrap` is rephrased in terms of Pythonlevel async objects (e.g. from `concurrent.futures`). We can ignore how jax does its async dispatch internally for now. This issue then essentially becomes a request to support a notion of a jax array backed by (i) a futurepromised jax device array plus (ii) some type information known about it upfront (`shape`, `dtype`, etc.). This is already a nontrivial and interesting request. Then we can bring in considerations for who manages the async execution and what options are available for doing so. That includes whether jax ought to reuse its internal async machinery from dispatch (if JAX manages the async execution) or whether jax wants to expose those internals to python users (if user manages async). We might have some hunches about this already, but it seems secondary to the request above. Designing a futurebacked jax array that works with `concurrent.futures`—or, say, an arbitrary thunk—for now seems like the right first step to me.", Should we synchronize some of the offline discussion into this bug thread?  I'm also not sure what the correct path forward is based on that discussion.,"The offthread discussion suggested defining a thunkbacked jax array, without saying the word ""future"" at first. Specifically, think of a python object (call it `LazyDeviceArray`) with the same interface as jax's (say `DeviceArray`), but whose construction takes a function: ``` thunk :: () > DeviceArray ``` An instance of `LazyDeviceArray` carries the information needed to correspond to a jax type (typically shape and dtype). The moment it is involved in an operation that requires its data, it defers to `thunk()` (say, also checking that `thunk`'s output indeed matches the shape/dtype info it carries). The idea is that it might be easy enough to back `thunk` by a future array, whether that's done using `concurrent.futures` or something else. Intuitively, this seems doable: kick off the asynchronous computation prior to constructing the `LazyDeviceArray`, and supply a `thunk` that blocks until the future array is ready. I suspect that `concurrent` or `asyncio` might help make a driving example, although I understand we may not want to build on those further than that. Also, whether this is a proposed addition to jax, and whether jax ought to offers utilities for futures alongside it, are questions for later (my first guess is ""no"" on the latter but we have more to learn first)."
9336,"以下是一个github上的jax下的一个issue, 标题是(Build failure on macOS M1)， 内容是 (M1 wheel builds are broken at HEAD again, but nothing too serious this time: ``` ➜ python build/build.py      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: /opt/homebrew/bin/bazel Bazel version: 5.1.1 Python binary path: /Users/nicholasjunge/Workspaces/python/jax/venv/bin/python Python version: 3.9 NumPy version: 1.22.3 MKLDNN enabled: yes Target CPU: arm64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... /opt/homebrew/bin/bazel run verbose_failures=true config=mkl_open_source_only :build_wheel  output_path=/Users/nicholasjunge/Workspaces/python/jax/dist cpu=arm64 Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //tensorflow/compiler/xla/python:enable_gpu=false //tensorflow/compiler/xla/python:enable_tpu=false INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python action_env=PYENV_ROOT python_path=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/e4b355cf794b4df50a8d8150c7f44fe76c8a12d5.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading: Loading: 1 packages loaded Analyzing: target //build:build_wheel (2 packages loaded, 0 targets configured) WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Analyzing: target //build:build_wheel (226 packages loaded, 14251 targets configured) INFO: Analyzed target //build:build_wheel (227 packages loaded, 15127 targets configured). INFO: Found 1 target... [0 / 31] [Prepa] Expanding template build/build_wheel [578 / 3,068] Compiling llvm/lib/MC/MCParser/MasmParser.cpp; 2s local ... (10 actions, 9 running) ERROR: /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/llvmproject/mlir/BUILD.bazel:3153:11: Compiling mlir/lib/Support/Timing.cpp failed: (Aborted): wrapped_clang_pp failed: error executing command   (cd /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/execroot/__main__ && \   exec env  \     APPLE_SDK_PLATFORM=MacOSX \     APPLE_SDK_VERSION_OVERRIDE=12.3 \     PATH=/Users/nicholasjunge/Workspaces/python/jax/venv/bin:/opt/homebrew/Caskroom/googlecloudsdk/latest/googlecloudsdk/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin:/opt/homebrew/opt/fzf/bin \     XCODE_VERSION_OVERRIDE=13.4.0.13F17a \     ZERO_AR_DATE=1 \   external/local_config_cc/wrapped_clang_pp 'D_FORTIFY_SOURCE=1' fstackprotector fcolordiagnostics Wall Wthreadsafety Wselfassign fnoomitframepointer g0 O2 DNDEBUG 'DNS_BLOCK_ASSERTIONS=1' 'std=c++11' 'DEBUG_PREFIX_MAP_PWD=.' iquote external/llvmproject iquote bazelout/darwin_arm64opt/bin/external/llvmproject iquote external/llvm_terminfo iquote bazelout/darwin_arm64opt/bin/external/llvm_terminfo iquote external/llvm_zlib iquote bazelout/darwin_arm64opt/bin/external/llvm_zlib isystem external/llvmproject/mlir/include isystem bazelout/darwin_arm64opt/bin/external/llvmproject/mlir/include isystem external/llvmproject/llvm/include isystem bazelout/darwin_arm64opt/bin/external/llvmproject/llvm/include MD MF bazelout/darwin_arm64opt/bin/external/llvmproject/mlir/_objs/Support/Timing.d 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' 'DHAVE_MACH_MACH_H=1' 'DHAVE_MALLOC_MALLOC_H=1' 'DHAVE_MALLOC_ZONE_STATISTICS=1' 'DHAVE_PROC_PID_RUSAGE=1' 'DHAVE_UNW_ADD_DYNAMIC_FDE=1' 'DLLVM_NATIVE_ARCH=""AArch64""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeAArch64AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeAArch64AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeAArch64Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeAArch64Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeAArch64TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeAArch64TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeAArch64TargetMCA' 'DLLVM_HOST_TRIPLE=""arm64appledarwin""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""arm64appledarwin""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS DBLAKE3_NO_AVX2 DBLAKE3_NO_AVX512 DBLAKE3_NO_SSE2 DBLAKE3_NO_SSE41 'DBLAKE3_USE_NEON=0' 'frandomseed=bazelout/darwin_arm64opt/bin/external/llvmproject/mlir/_objs/Support/Timing.o' isysroot __BAZEL_XCODE_SDKROOT__ F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/MacOSX.platform/Developer/Library/Frameworks 'mmacosxversionmin=10.9' nocanonicalprefixes pthread 'fvisibility=hidden' Wnosigncompare 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' 'std=c++17' nocanonicalprefixes Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' target arm64applemacosx c external/llvmproject/mlir/lib/Support/Timing.cpp o bazelout/darwin_arm64opt/bin/external/llvmproject/mlir/_objs/Support/Timing.o)  Configuration: 32b36e8e2b8768da789a893fb050b01470cd5e09932078831a2959b0fe5ef2b0  Execution platform: //:platform In file included from external/llvmproject/mlir/lib/Support/Timing.cpp:24: external/llvmproject/llvm/include/llvm/Support/RWMutex.h:98:8: error: 'shared_mutex' is unavailable: introduced in macOS 10.12   std::shared_mutex impl;        ^ /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.3.sdk/usr/include/c++/v1/shared_mutex:180:58: note: 'shared_mutex' has been explicitly marked unavailable here class _LIBCPP_TYPE_VIS _LIBCPP_AVAILABILITY_SHARED_MUTEX shared_mutex                                                          ^ 1 error generated. Error in child process '/usr/bin/xcrun'. 1 Target //build:build_wheel failed to build INFO: Elapsed time: 149.959s, Critical Path: 5.80s INFO: 611 processes: 355 internal, 256 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully b'' Traceback (most recent call last):   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 528, in      main()   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 523, in main     shell(command)   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/opt/homebrew/Cellar/python.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/opt/homebrew/Cellar/python.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['/opt/homebrew/bin/bazel', 'run', 'verbose_failures=true', 'config=mkl_open_source_only', ':build_wheel', '', 'output_path=/Users/nicholasjunge/Workspaces/python/jax/dist', 'cpu=arm64']' returned nonzero exit status 1. ``` The message is very helpful, and indeed, setting the minimum macOS version in the `.bazelrc` to 10.12 fixes the build.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Build failure on macOS M1,"M1 wheel builds are broken at HEAD again, but nothing too serious this time: ``` ➜ python build/build.py      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: /opt/homebrew/bin/bazel Bazel version: 5.1.1 Python binary path: /Users/nicholasjunge/Workspaces/python/jax/venv/bin/python Python version: 3.9 NumPy version: 1.22.3 MKLDNN enabled: yes Target CPU: arm64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... /opt/homebrew/bin/bazel run verbose_failures=true config=mkl_open_source_only :build_wheel  output_path=/Users/nicholasjunge/Workspaces/python/jax/dist cpu=arm64 Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //tensorflow/compiler/xla/python:enable_gpu=false //tensorflow/compiler/xla/python:enable_tpu=false INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python action_env=PYENV_ROOT python_path=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/e4b355cf794b4df50a8d8150c7f44fe76c8a12d5.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading: Loading: 1 packages loaded Analyzing: target //build:build_wheel (2 packages loaded, 0 targets configured) WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Analyzing: target //build:build_wheel (226 packages loaded, 14251 targets configured) INFO: Analyzed target //build:build_wheel (227 packages loaded, 15127 targets configured). INFO: Found 1 target... [0 / 31] [Prepa] Expanding template build/build_wheel [578 / 3,068] Compiling llvm/lib/MC/MCParser/MasmParser.cpp; 2s local ... (10 actions, 9 running) ERROR: /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/llvmproject/mlir/BUILD.bazel:3153:11: Compiling mlir/lib/Support/Timing.cpp failed: (Aborted): wrapped_clang_pp failed: error executing command   (cd /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/execroot/__main__ && \   exec env  \     APPLE_SDK_PLATFORM=MacOSX \     APPLE_SDK_VERSION_OVERRIDE=12.3 \     PATH=/Users/nicholasjunge/Workspaces/python/jax/venv/bin:/opt/homebrew/Caskroom/googlecloudsdk/latest/googlecloudsdk/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin:/opt/homebrew/opt/fzf/bin \     XCODE_VERSION_OVERRIDE=13.4.0.13F17a \     ZERO_AR_DATE=1 \   external/local_config_cc/wrapped_clang_pp 'D_FORTIFY_SOURCE=1' fstackprotector fcolordiagnostics Wall Wthreadsafety Wselfassign fnoomitframepointer g0 O2 DNDEBUG 'DNS_BLOCK_ASSERTIONS=1' 'std=c++11' 'DEBUG_PREFIX_MAP_PWD=.' iquote external/llvmproject iquote bazelout/darwin_arm64opt/bin/external/llvmproject iquote external/llvm_terminfo iquote bazelout/darwin_arm64opt/bin/external/llvm_terminfo iquote external/llvm_zlib iquote bazelout/darwin_arm64opt/bin/external/llvm_zlib isystem external/llvmproject/mlir/include isystem bazelout/darwin_arm64opt/bin/external/llvmproject/mlir/include isystem external/llvmproject/llvm/include isystem bazelout/darwin_arm64opt/bin/external/llvmproject/llvm/include MD MF bazelout/darwin_arm64opt/bin/external/llvmproject/mlir/_objs/Support/Timing.d 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' 'DHAVE_MACH_MACH_H=1' 'DHAVE_MALLOC_MALLOC_H=1' 'DHAVE_MALLOC_ZONE_STATISTICS=1' 'DHAVE_PROC_PID_RUSAGE=1' 'DHAVE_UNW_ADD_DYNAMIC_FDE=1' 'DLLVM_NATIVE_ARCH=""AArch64""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeAArch64AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeAArch64AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeAArch64Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeAArch64Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeAArch64TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeAArch64TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeAArch64TargetMCA' 'DLLVM_HOST_TRIPLE=""arm64appledarwin""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""arm64appledarwin""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS DBLAKE3_NO_AVX2 DBLAKE3_NO_AVX512 DBLAKE3_NO_SSE2 DBLAKE3_NO_SSE41 'DBLAKE3_USE_NEON=0' 'frandomseed=bazelout/darwin_arm64opt/bin/external/llvmproject/mlir/_objs/Support/Timing.o' isysroot __BAZEL_XCODE_SDKROOT__ F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/MacOSX.platform/Developer/Library/Frameworks 'mmacosxversionmin=10.9' nocanonicalprefixes pthread 'fvisibility=hidden' Wnosigncompare 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' 'std=c++17' nocanonicalprefixes Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' target arm64applemacosx c external/llvmproject/mlir/lib/Support/Timing.cpp o bazelout/darwin_arm64opt/bin/external/llvmproject/mlir/_objs/Support/Timing.o)  Configuration: 32b36e8e2b8768da789a893fb050b01470cd5e09932078831a2959b0fe5ef2b0  Execution platform: //:platform In file included from external/llvmproject/mlir/lib/Support/Timing.cpp:24: external/llvmproject/llvm/include/llvm/Support/RWMutex.h:98:8: error: 'shared_mutex' is unavailable: introduced in macOS 10.12   std::shared_mutex impl;        ^ /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.3.sdk/usr/include/c++/v1/shared_mutex:180:58: note: 'shared_mutex' has been explicitly marked unavailable here class _LIBCPP_TYPE_VIS _LIBCPP_AVAILABILITY_SHARED_MUTEX shared_mutex                                                          ^ 1 error generated. Error in child process '/usr/bin/xcrun'. 1 Target //build:build_wheel failed to build INFO: Elapsed time: 149.959s, Critical Path: 5.80s INFO: 611 processes: 355 internal, 256 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully b'' Traceback (most recent call last):   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 528, in      main()   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 523, in main     shell(command)   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/opt/homebrew/Cellar/python.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/opt/homebrew/Cellar/python.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['/opt/homebrew/bin/bazel', 'run', 'verbose_failures=true', 'config=mkl_open_source_only', ':build_wheel', '', 'output_path=/Users/nicholasjunge/Workspaces/python/jax/dist', 'cpu=arm64']' returned nonzero exit status 1. ``` The message is very helpful, and indeed, setting the minimum macOS version in the `.bazelrc` to 10.12 fixes the build.",2022-05-31T12:06:41Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/10884
616,"以下是一个github上的jax下的一个issue, 标题是(adding/testing Google Chat notification workflows.)， 内容是 (Trying to test out some Google chat notification workflows. Three workflows here, one that should notify on a new cut release `releasenotification.yml`, one that should trigger on any pull request (this isn't meant to persist but to be used to test on a more active trigger, eventually should be removed) as well as one that attempts to trigger any time a main branch check suite fails.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,adding/testing Google Chat notification workflows.,"Trying to test out some Google chat notification workflows. Three workflows here, one that should notify on a new cut release `releasenotification.yml`, one that should trigger on any pull request (this isn't meant to persist but to be used to test on a more active trigger, eventually should be removed) as well as one that attempts to trigger any time a main branch check suite fails.",2022-05-27T16:23:21Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10853
291,"以下是一个github上的jax下的一个issue, 标题是([mesh_utils] Avoid relying on process-tiled device order)， 内容是 ([mesh_utils] Avoid relying on processtiled device order)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[mesh_utils] Avoid relying on process-tiled device order,[mesh_utils] Avoid relying on processtiled device order,2022-05-27T00:52:19Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/10848,Note that this change only affects TPU v4; the heuristics used by other generations still need to be replaced (WIP because I'm doing that as part of a bigger rewrite).
4180,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Add example demonstrating JAX --> TFLite with shape polymorphism)， 内容是 (It is quite common to use shape polymorphism in TFLite (for instance when processing audio), yet our mnist example does not demonstrate this, and it seems that the conversion path we recommend there also doesn't allow this. This is quite confusing for users. TFLite provides two relevant ways of converting from TF to TFLite: * `from_concrete_function()` * `from_saved_model()` We currently use `from_concrete_function()` in our example, but this path has a number of disadvantages: * It is not clear how to get shape polymorphism working. I didn't get it to work, and I doubt whether it is even possible. * Using an interpreter in this case is quite messy. It requires setting tensors, and one often runs into problems with dangling references. * We call `from_concrete_function()` without a `trackable_obj`, and this actually deprecated by TFLite (code]. Below is some experimental code for how I managed to convert a model with shape polymorphism to TFLite. My recommendation is to modify the code below and add it to the MNIST example, and modify it so we have a polymorphic dimension as well (e.g., the batch dimension). ```py  jax2tf conversion variables = module.init(...)  First dimension is polymorphic. tf_fn = jax2tf.convert(     module.apply,     polymorphic_shapes=[None, ""(b, 64000)""],     enable_xla=False)  Create SavedModel   This is from saved_model_lib.py class _ReusableSavedModelWrapper(tf.train.Checkpoint):   def __init__(self, tf_graph, param_vars):     super().__init__()      Implement the interface from https://www.tensorflow.org/hub/reusable_saved_models     self.variables = tf.nest.flatten(param_vars)     self.trainable_variables = [v for v in self.variables if v.trainable]      If you intend to prescribe regularization terms for users of the model,      add them as .functions with no inputs to this list. Else drop this.     self.regularization_losses = []     self.__call__ = tf_graph  Create tf.Variables for the parameters. If you want more useful variable  names, you can use `tree.map_structure_with_path` from the `dmtree` package param_vars = tf.nest.map_structure(   lambda param: tf.Variable(param, trainable=False),   variables) tf_graph = tf.function(lambda inputs: tf_fn(param_vars, inputs),                        autograph=False,                        jit_compile=True)  Also provide this to the SavedModel. input_signatures = [tf.TensorSpec(shape=(None, 64000), dtype=tf.float32)] signatures = {}  This signature is needed for TensorFlow Serving use. signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \   tf_graph.get_concrete_function(input_signatures[0]) for input_signature in input_signatures[1:]:    If there are more signatures, trace and cache a TF function for each one   tf_graph.get_concrete_function(input_signature) wrapper = _ReusableSavedModelWrapper(tf_graph, param_vars) saved_model_dir = './saved_model' tf.saved_model.save(wrapper, saved_model_dir, signatures=signatures)  Create TFLite model  Build a TFLite converter based on the TF graph. converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,   enable TensorFlow Lite ops.     tf.lite.OpsSet.SELECT_TF_OPS   enable TensorFlow ops. ]  Convert from the TF graph to TFLite. tflite_model = converter.convert() with gfile.Open(save_path, 'wb') as f:   f.write(tflite_model)  Compare outputs using TFLite interpreter  Print the signatures from the converted model interpreter = tf.lite.Interpreter(model_content=tflite_model) signatures = interpreter.get_signature_list() print(signatures)  {'serving_default': {'inputs': ['inputs'], 'outputs': ['output_0']}} interpreter_apply = interpreter.get_signature_runner('serving_default')  Verify outputs are the same. tf_result = interpreter_apply(inputs=inputs) jax_result = jax_fn(random_data) np.testing.assert_allclose(tf_result['output_0'], jax_result) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Add example demonstrating JAX --> TFLite with shape polymorphism,"It is quite common to use shape polymorphism in TFLite (for instance when processing audio), yet our mnist example does not demonstrate this, and it seems that the conversion path we recommend there also doesn't allow this. This is quite confusing for users. TFLite provides two relevant ways of converting from TF to TFLite: * `from_concrete_function()` * `from_saved_model()` We currently use `from_concrete_function()` in our example, but this path has a number of disadvantages: * It is not clear how to get shape polymorphism working. I didn't get it to work, and I doubt whether it is even possible. * Using an interpreter in this case is quite messy. It requires setting tensors, and one often runs into problems with dangling references. * We call `from_concrete_function()` without a `trackable_obj`, and this actually deprecated by TFLite (code]. Below is some experimental code for how I managed to convert a model with shape polymorphism to TFLite. My recommendation is to modify the code below and add it to the MNIST example, and modify it so we have a polymorphic dimension as well (e.g., the batch dimension). ```py  jax2tf conversion variables = module.init(...)  First dimension is polymorphic. tf_fn = jax2tf.convert(     module.apply,     polymorphic_shapes=[None, ""(b, 64000)""],     enable_xla=False)  Create SavedModel   This is from saved_model_lib.py class _ReusableSavedModelWrapper(tf.train.Checkpoint):   def __init__(self, tf_graph, param_vars):     super().__init__()      Implement the interface from https://www.tensorflow.org/hub/reusable_saved_models     self.variables = tf.nest.flatten(param_vars)     self.trainable_variables = [v for v in self.variables if v.trainable]      If you intend to prescribe regularization terms for users of the model,      add them as .functions with no inputs to this list. Else drop this.     self.regularization_losses = []     self.__call__ = tf_graph  Create tf.Variables for the parameters. If you want more useful variable  names, you can use `tree.map_structure_with_path` from the `dmtree` package param_vars = tf.nest.map_structure(   lambda param: tf.Variable(param, trainable=False),   variables) tf_graph = tf.function(lambda inputs: tf_fn(param_vars, inputs),                        autograph=False,                        jit_compile=True)  Also provide this to the SavedModel. input_signatures = [tf.TensorSpec(shape=(None, 64000), dtype=tf.float32)] signatures = {}  This signature is needed for TensorFlow Serving use. signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \   tf_graph.get_concrete_function(input_signatures[0]) for input_signature in input_signatures[1:]:    If there are more signatures, trace and cache a TF function for each one   tf_graph.get_concrete_function(input_signature) wrapper = _ReusableSavedModelWrapper(tf_graph, param_vars) saved_model_dir = './saved_model' tf.saved_model.save(wrapper, saved_model_dir, signatures=signatures)  Create TFLite model  Build a TFLite converter based on the TF graph. converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) converter.target_spec.supported_ops = [     tf.lite.OpsSet.TFLITE_BUILTINS,   enable TensorFlow Lite ops.     tf.lite.OpsSet.SELECT_TF_OPS   enable TensorFlow ops. ]  Convert from the TF graph to TFLite. tflite_model = converter.convert() with gfile.Open(save_path, 'wb') as f:   f.write(tflite_model)  Compare outputs using TFLite interpreter  Print the signatures from the converted model interpreter = tf.lite.Interpreter(model_content=tflite_model) signatures = interpreter.get_signature_list() print(signatures)  {'serving_default': {'inputs': ['inputs'], 'outputs': ['output_0']}} interpreter_apply = interpreter.get_signature_runner('serving_default')  Verify outputs are the same. tf_result = interpreter_apply(inputs=inputs) jax_result = jax_fn(random_data) np.testing.assert_allclose(tf_result['output_0'], jax_result) ```",2022-05-25T13:57:05Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/10821,"Closing this due to inactivity and I likely won't get to this, and it seems overly complex to require users to use a savedmodel."
1758,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Converting jax.grad(ViT) with enable_xla=False fails)， 内容是 (Converting a ViT model with `enable_xla=False` fails if we wrap the apply function with `jax.grad`. Without wrapping it with `jax.grad` things work fine: ```py import jax from jax import random import jax.numpy as jnp from jax.experimental import jax2tf import tensorflow as tf from vit_jax import models import numpy as np model_name = 'LiTB16B' lit_model = models.get_model(model_name) lit_variables = lit_model.load_variables() def apply(x):   logits, _, _ = lit_model.apply(lit_variables, images=x)   return jnp.sum(logits) def convert_and_compare(jax_fn, rtol=1e5):   x = random.uniform(random.PRNGKey(0), (1, 224, 224, 3))   tf_fn = jax2tf.convert(jax_fn, enable_xla=False)   np.testing.assert_allclose(jax_fn(x), tf_fn(x), rtol=rtol) convert_and_compare(apply)   Passes ``` But when we wrap `apply` with `jax.grad` it fails: ```py convert_and_compare(jax.grad(apply)) ``` ``` AssertionError:  Not equal to tolerance rtol=1e07, atol=0 Mismatched elements: 150528 / 150528 (100%) Max absolute difference: 0.16114585 Max relative difference: inf  x: array([[[[0.005736, 0.002271,  0.002803],          [0.009796, 0.007272,  0.000364],          [0.00644 , 0.002177,  0.004559],...  y: array([[[[0., 0., 0.],          [0., 0., 0.],          [0., 0., 0.],... ``` To address this we should first find the problematic op in ViT and then see where the bug is in our conversion code in impl_no_xla.py. Related issue: CC([jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False)  FYI:   )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Converting jax.grad(ViT) with enable_xla=False fails,"Converting a ViT model with `enable_xla=False` fails if we wrap the apply function with `jax.grad`. Without wrapping it with `jax.grad` things work fine: ```py import jax from jax import random import jax.numpy as jnp from jax.experimental import jax2tf import tensorflow as tf from vit_jax import models import numpy as np model_name = 'LiTB16B' lit_model = models.get_model(model_name) lit_variables = lit_model.load_variables() def apply(x):   logits, _, _ = lit_model.apply(lit_variables, images=x)   return jnp.sum(logits) def convert_and_compare(jax_fn, rtol=1e5):   x = random.uniform(random.PRNGKey(0), (1, 224, 224, 3))   tf_fn = jax2tf.convert(jax_fn, enable_xla=False)   np.testing.assert_allclose(jax_fn(x), tf_fn(x), rtol=rtol) convert_and_compare(apply)   Passes ``` But when we wrap `apply` with `jax.grad` it fails: ```py convert_and_compare(jax.grad(apply)) ``` ``` AssertionError:  Not equal to tolerance rtol=1e07, atol=0 Mismatched elements: 150528 / 150528 (100%) Max absolute difference: 0.16114585 Max relative difference: inf  x: array([[[[0.005736, 0.002271,  0.002803],          [0.009796, 0.007272,  0.000364],          [0.00644 , 0.002177,  0.004559],...  y: array([[[[0., 0., 0.],          [0., 0., 0.],          [0., 0., 0.],... ``` To address this we should first find the problematic op in ViT and then see where the bug is in our conversion code in impl_no_xla.py. Related issue: CC([jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False)  FYI:   ",2022-05-25T11:50:58Z,bug,open,1,0,https://github.com/jax-ml/jax/issues/10819
2871,"以下是一个github上的jax下的一个issue, 标题是(""Very slow compile"" of dm-haiku training step)， 内容是 (Please:  [X] Check for duplicate issues.  [X] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import itertools import jax import jax.numpy as jnp import haiku as hk import optax Model = hk.nets.ResNet152 def dataloader(batch_size, *, key):     while True:          Binary classification         X_key, Y_key = jax.random.split(key)         X = jax.random.normal(X_key, (batch_size, 64, 64, 3))         Y = jax.random.randint(Y_key, (batch_size,), 0, 2)         yield X, Y def train():     batch_size = 32     learning_rate = 0.001     def forward(input, is_training):         net = Model(num_classes=2)         output = net(input, is_training)         return output     def learner_fn(input, ground_truth):         logits = forward(input, True)         labels = jax.nn.one_hot(ground_truth, 2)         return jnp.mean(optax.softmax_cross_entropy(logits, labels))     learner_fn_t = hk.transform_with_state(learner_fn)     learner_fn_t = hk.without_apply_rng(learner_fn_t)     .jit     def train_step(params, state, opt_state, X, y):         (loss, state), grads = jax.value_and_grad(learner_fn_t.apply, has_aux=True)(params, state, X, y)         updates, opt_state = optimizer.update(grads, opt_state, params)         params = optax.apply_updates(params, updates)         return params, state, opt_state, loss     rng = jax.random.PRNGKey(42)     loader_rng, rng = jax.random.split(rng)     loader = dataloader(batch_size, key=loader_rng)     X, y = next(loader)     params, state = learner_fn_t.init(rng, X, y)     optimizer = optax.adam(learning_rate)     opt_state = optimizer.init(params)      Training     for X, y in itertools.islice(loader, 50):         params, state, opt_state, loss = train_step(params, state, opt_state, X, y)         print(loss) if __name__ == '__main__':     train() ```  [X] If applicable, include full error messages/tracebacks. ``` $ XLA_FLAGS=xla_dump_to=/tmp/slow_operation_dump pipenv run python3 slow_operation.py WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 20220521 01:56:39.688109: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] ******************************** Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. Compiling module jit_train_step.261 ******************************** ``` Here is the content of `/tmp/slow_operation_dump`: slow_operation_dump.zip. Additionally, here is the Pipenv lock files to help you reproduce my development environment on macOS 12.3.1: Pipfile.zip)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"""Very slow compile"" of dm-haiku training step","Please:  [X] Check for duplicate issues.  [X] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import itertools import jax import jax.numpy as jnp import haiku as hk import optax Model = hk.nets.ResNet152 def dataloader(batch_size, *, key):     while True:          Binary classification         X_key, Y_key = jax.random.split(key)         X = jax.random.normal(X_key, (batch_size, 64, 64, 3))         Y = jax.random.randint(Y_key, (batch_size,), 0, 2)         yield X, Y def train():     batch_size = 32     learning_rate = 0.001     def forward(input, is_training):         net = Model(num_classes=2)         output = net(input, is_training)         return output     def learner_fn(input, ground_truth):         logits = forward(input, True)         labels = jax.nn.one_hot(ground_truth, 2)         return jnp.mean(optax.softmax_cross_entropy(logits, labels))     learner_fn_t = hk.transform_with_state(learner_fn)     learner_fn_t = hk.without_apply_rng(learner_fn_t)     .jit     def train_step(params, state, opt_state, X, y):         (loss, state), grads = jax.value_and_grad(learner_fn_t.apply, has_aux=True)(params, state, X, y)         updates, opt_state = optimizer.update(grads, opt_state, params)         params = optax.apply_updates(params, updates)         return params, state, opt_state, loss     rng = jax.random.PRNGKey(42)     loader_rng, rng = jax.random.split(rng)     loader = dataloader(batch_size, key=loader_rng)     X, y = next(loader)     params, state = learner_fn_t.init(rng, X, y)     optimizer = optax.adam(learning_rate)     opt_state = optimizer.init(params)      Training     for X, y in itertools.islice(loader, 50):         params, state, opt_state, loss = train_step(params, state, opt_state, X, y)         print(loss) if __name__ == '__main__':     train() ```  [X] If applicable, include full error messages/tracebacks. ``` $ XLA_FLAGS=xla_dump_to=/tmp/slow_operation_dump pipenv run python3 slow_operation.py WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 20220521 01:56:39.688109: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] ******************************** Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. Compiling module jit_train_step.261 ******************************** ``` Here is the content of `/tmp/slow_operation_dump`: slow_operation_dump.zip. Additionally, here is the Pipenv lock files to help you reproduce my development environment on macOS 12.3.1: Pipfile.zip",2022-05-21T07:03:06Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/10790,"Okay, after waiting for ~10 minutes the JIT compilation appears to be successful, and the loss is dropping rapidly! However, the backpropagation is very slow (each iteration takes several seconds), so I suspect there are still some problems. ```python $ XLA_FLAGS=xla_dump_to=/tmp/slow_operation_dump pipenv run python3 slow_operation.py WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 20220521 01:56:39.688109: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] ******************************** Very slow compile?  If you want to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. Compiling module jit_train_step.261 ******************************** 0.6931475 0.657604 0.4421207 0.19778039 0.046985622 0.01008308 0.002680283 0.00088729034 0.00035099505 0.00015979768 8.119534e05 4.4985038e05 2.6750724e05 1.6849244e05 1.1138508e05 7.696395e06 5.513401e06 4.071727e06 3.0957071e06 2.4139822e06 1.9259712e06 1.5795204e06 1.3075752e06 1.1026847e06 9.499482e07 8.232885e07 7.338816e07 6.4820006e07 5.8114495e07 5.3271623e07 4.8801274e07 4.507599e07 4.1350705e07 3.9115534e07 3.6135302e07 3.4645188e07 3.2037485e07 3.1292427e07 2.9802314e07 2.8312198e07 2.7567143e07 2.6822084e07 2.6449555e07 2.5704497e07 2.5331968e07 2.421438e07 2.3096796e07 2.3096796e07 2.2351738e07 2.1979208e07 ``` Three additional files appeared in `/tmp/slow_operation_dump`, but somehow I cannot upload them to GitHub directly. Let me know if they are important, and I can set up a Google Drive link for you. ``` $ diff rq /tmp/slow_operation_dump /tmp/slow_operation_dump_old Only in /tmp/slow_operation_dump/: module_0282.jit_train_step.261.irwithoptnoconst.ll Only in /tmp/slow_operation_dump/: module_0282.jit_train_step.261.irwithopt.ll Only in /tmp/slow_operation_dump/: module_0282.jit_train_step.261.o ```","`ResNet152` is a fairly compute intensive model (forward pass is ~12GFLOPs) so I'm not surprised that the step time to train it in full precision on CPU is in the order of a second or two. You would probably be better of training this on a GPU or TPU (or using a smaller ResNet model like ResNet50)? The XLA:GPU and XLA:TPU compilers are also better optimized (vs. XLA:CPU) and so compilation will also likely be quicker. Taking your code and running it on a GPU Colab instance it compiles in ~2 mins and the step time (including printing the loss each step) is 200ms. If you make the loss printing less frequent your step time would likely be a lot faster. https://colab.research.google.com/gist/tomhennigan/79ecdf274ad4c36a21a64cd4a18a4042/googlejax10790.ipynb Note that I did refactor your code a little, to try and ensure JAX operations were wrapped into three big jits (one to generate dataset batches, one to generate all initial state, and the other to compute a training step).","Thanks for the reply, . I wonder why we should JIT the `initial_state` function, though. This function is called once, so I'm not sure if it's worth the compilation overhead.","The way JAX works is that when you execute a single op eagerly (e.g. `a + b`) JAX will just in time compile a program to add two tensors of the given shape/dtype. So If you don't `jit` your `initial_state` function JAX will actually just in time compile hundreds of small kernels anyway, one for each op. I would suspect that compilation of 100s of small programs vs. compilation of one program with 100s of ops would take a similar amount of time. The larger program may be a bit slower to compile (there are some compiler passes that scale nonlinearly in the number of ops in the program) but I would guess the timings would be pretty close. Your thinking is definitely correct, that it is far more important to `jit` the `train_step` function. There are two other reasons why jit compiling initial_state might be preferable: 1. **Fragmentation**  if you don't jit compile initial_state, then JAX needs to allocate and deallocate every tensor in your initial_state program eagerly. If you imagine memory as a list of values, this means your parameters and temporaries would be allocated like so: `[param1, temp,, .., param2, temp, ..]`. When the `initial_state` function exits all the temporaries will be deallocated, so you are left with fragmentation: `[param1, , param2, , ...]`. If you JIT compile, JAX allocates space for all outputs first, followed by space for all temporaries, so your memory should be less fragmented: `[param1, param2, , ..]`. (The details of memory allocation may vary between JAX backends and I am not an expert here, but this is my understanding of how it typically works. Fragmentation also might not be something you care about if you have plenty of GPU memory available to execute your programs, so you may only want to do this optimisation when you need it.)  3. **Future upgrades**  We find that JAX users want to use >1 GPU, and the simplest way today to do that is `pmap` and there you would need to `pmap` your `initial_state` function (to replicate your parameters across multiple GPUs).","Thank you, . That is extremely helpful!"
336,"以下是一个github上的jax下的一个issue, 标题是([sparse] Trace BCOO `indices_sorted` in sparsifying zero_preserving_unary_ops.)， 内容是 ([sparse] Trace BCOO `indices_sorted` in sparsifying zero_preserving_unary_ops.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] Trace BCOO `indices_sorted` in sparsifying zero_preserving_unary_ops.,[sparse] Trace BCOO `indices_sorted` in sparsifying zero_preserving_unary_ops.,2022-05-20T21:07:33Z,,closed,0,1,https://github.com/jax-ml/jax/issues/10784, 
1555,"以下是一个github上的jax下的一个issue, 标题是(vit imagenet training precision issues with amd w6800 gpu + rocm 5.0)， 内容是 (per discussion in CC(Add support for other GPUs (than NVIDIA)), breaking this out into a standalone issue. ``` pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html  rollback to old scenic git clone https://github.com/googleresearch/scenic.git cd scenic git reset hard 791ffdd5d6e8fcb2b410ad2fb8c89701ce833c67 pip install . ``` The local python install will need a tweak at line 688 in ~/.local/lib/python3.8/sitepackages/scenic/model_lib/base_models/model_utils.py ``` import functools .partial(jax.vmap, in_axes=[0, 0], out_axes=0) ``` After that, set the batch size to 128 in scenic/projects/baselines/configs/imagenet/imagenet_vit_config.py ``` python3 scenic/main.py  config=scenic/projects/baselines/configs/imagenet/imagenet_vit_config.py workdir=/OUTPUT_DIR ``` Per discussion in that thread, Nvidia/TPU devices converge to the same result at 100 epochs, whereas the AMD device (a w6800 with rocm 5.0) converges to a lower number (~80% accuracy of the tpu/nvidia baseline).  Per my notes, had similar difference with rocm 4.5 and the same GPU last year. I am using this specific version of scenic in order to use the tfdataset 5.1.0 version of imagenet (the dataset handlers were reworked in February).  Going to work on updating to the latest scenic soon.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,vit imagenet training precision issues with amd w6800 gpu + rocm 5.0,"per discussion in CC(Add support for other GPUs (than NVIDIA)), breaking this out into a standalone issue. ``` pip install ""jax[tpu]>=0.2.16"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html  rollback to old scenic git clone https://github.com/googleresearch/scenic.git cd scenic git reset hard 791ffdd5d6e8fcb2b410ad2fb8c89701ce833c67 pip install . ``` The local python install will need a tweak at line 688 in ~/.local/lib/python3.8/sitepackages/scenic/model_lib/base_models/model_utils.py ``` import functools .partial(jax.vmap, in_axes=[0, 0], out_axes=0) ``` After that, set the batch size to 128 in scenic/projects/baselines/configs/imagenet/imagenet_vit_config.py ``` python3 scenic/main.py  config=scenic/projects/baselines/configs/imagenet/imagenet_vit_config.py workdir=/OUTPUT_DIR ``` Per discussion in that thread, Nvidia/TPU devices converge to the same result at 100 epochs, whereas the AMD device (a w6800 with rocm 5.0) converges to a lower number (~80% accuracy of the tpu/nvidia baseline).  Per my notes, had similar difference with rocm 4.5 and the same GPU last year. I am using this specific version of scenic in order to use the tfdataset 5.1.0 version of imagenet (the dataset handlers were reworked in February).  Going to work on updating to the latest scenic soon.",2022-05-19T03:38:08Z,bug needs info AMD GPU,closed,0,4,https://github.com/jax-ml/jax/issues/10761,"amd  On the assumption that this is a numerical bug, I think the obvious debugging step would be to compare the numerical outputs of a single training step and see if they diverge significantly between AMD GPU and a reference implementation. If so, I would try to minimize the computation to a small reproducer. (I unfortunately cannot help you with this myself, since I don't have access to the necessary hardware.)","I have pulled scenic forward to 86a01bc062f63e5495f7e632edc478e3facd497e and am running ablations there.  I cannot use the ROCm jax build script, see CC(build rocm JAX failed).  Will try again with a more recent jax when I get a chance.","ROCm 5.2 working, will try again after https://github.com/googleresearch/scenic/issues/460 is resolved!",will try to reproduce with 5.7 or later!
1448,"以下是一个github上的jax下的一个issue, 标题是(Using ```grad``` on ```vmap``` on ```map``` on function containing ```sinc``` results in error)， 内容是 (Hi, I ran into an error while trying to take the ```grad``` of a ```vmap``` on a ```map``` on a function, which contains ```sinc```.  The following code reproduces the error: ``` import jax import jax.numpy as jnp N_batch = 5 N_elem = 3 batch_data = jnp.reshape(jnp.arange(N_batch*N_elem),(N_batch,N_elem)) def fail(x):     return jnp.sinc(x) def mapfail(x):     return jax.lax.map(fail, x) vmapmapfail = jax.vmap(mapfail) def loss(param):     pos = param*batch_data     desc = vmapmapfail(pos)     return jnp.mean(desc) jax.value_and_grad(loss)(0.1) ```  Error: ```JaxStackTraceBeforeTransformation: TypeError: broadcast_in_dim broadcast_dimensions must have length equal to operand ndim; got broadcast_dimensions () for operand ndim 1.```  Jax version: 0.3.13 I suspect this is related to the custom derivatives of the ```_sinc_maclaurin```, since commenting out both ```(custom_jvp, nondiff_argnums=(0,))``` and ```.defjvp``` in the jax source ""solves"" the problem. It is also specific to ```map```, since using a vmap instead results in no error.  Ps.: A colleague of mine also reminded me, that the documentation of sinc, does not describe the x=0 behaviour correctly. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Using ```grad``` on ```vmap``` on ```map``` on function containing ```sinc``` results in error,"Hi, I ran into an error while trying to take the ```grad``` of a ```vmap``` on a ```map``` on a function, which contains ```sinc```.  The following code reproduces the error: ``` import jax import jax.numpy as jnp N_batch = 5 N_elem = 3 batch_data = jnp.reshape(jnp.arange(N_batch*N_elem),(N_batch,N_elem)) def fail(x):     return jnp.sinc(x) def mapfail(x):     return jax.lax.map(fail, x) vmapmapfail = jax.vmap(mapfail) def loss(param):     pos = param*batch_data     desc = vmapmapfail(pos)     return jnp.mean(desc) jax.value_and_grad(loss)(0.1) ```  Error: ```JaxStackTraceBeforeTransformation: TypeError: broadcast_in_dim broadcast_dimensions must have length equal to operand ndim; got broadcast_dimensions () for operand ndim 1.```  Jax version: 0.3.13 I suspect this is related to the custom derivatives of the ```_sinc_maclaurin```, since commenting out both ```(custom_jvp, nondiff_argnums=(0,))``` and ```.defjvp``` in the jax source ""solves"" the problem. It is also specific to ```map```, since using a vmap instead results in no error.  Ps.: A colleague of mine also reminded me, that the documentation of sinc, does not describe the x=0 behaviour correctly. ",2022-05-18T10:28:24Z,bug,closed,0,11,https://github.com/jax-ml/jax/issues/10750,Thanks for the report. I'll look into the issue – Can you say more about the issue you see in the `sinc` documentation?,"I am afraid I am the nitpicking colleague (unexpectedly) mentioned in the report :). My comment was actually more closely related to the part of the docstring inherited from numpy, specifically the sentence ""the sinc function is `sin(pi*x)/(pi*x)`"". Since the analytic character of sinc is important both for its applications and for the actual implementation at x=0, I would suggest defining it, for instance, as the limit when t>x of `sin(pi*t)/(pi*t)`. Thank you for your work on this fantastic project!","Thanks  as you said, that part of the docstring is drawn directly from `numpy.sinc`, so the tweak would have to be contributed to the numpy package itself.","This is a fantastic bug! Thanks for raising it. (And thanks for the tip about the docstring too.) I think the best fix may be to add a `full_like` primitive. (Alternatives include: defensively batching more outputs of `custom_jvp` functions, or probing custom JVP rules' vmap data dependence, but neither of those seem as good.) A quick fix, which is basically the same thing, is to replace these lines with ```python if k % 2:   return x * 0 else:   return x * 0 + lax.full_like(x, (1) ** (k // 2) / (k + 1)) ``` The issue has to do with data dependence, and an assumption baked into `custom_jvp` about how the inputoutput data dependence of a custom JVP rule relates to the inputoutput data dependence of the function for which it is the rule. That is, if we have `f = custom_jvp(f_orig)` and `f.defjvp(f_jvp)`, our batching rule for `custom_jvp` roughly assumes that for any `in_axes: Tuple[Optional[int], ...]` we have `vmap_out_axes(f_orig, in_axes) == vmap_out_axes(lambda xs: jax.jvp(f, xs, xs)[1], in_axes))`, which in turn roughly means that the inputoutput data dependence of `f_jvp` looks like the inputoutput dependence of `f_orig`. But for `sinc` that's not the case: the `_sinc_maclaurin` function has no data dependence on its input, while in its JVP rule the tangent output has a data dependence on the tangent input (which it must, for linearity to hold). What resulted was ultimately a type error: we'd get a `custom_jvp_call_jaxpr` application with an `f32[]` output (and downstream operations, like `broadcast_in_dim`, which were set up for that `f32[]` output), but when differentiated we'd get an `f32[5]` primal (and `f32[5]` tangent), which was then typeincompatible with downstream applications. There's a bit more going on here which was necessary to exhibit this bug; in particular, `scan` (i.e. `lax.map`) was necessary because it causes the JVP rule to be run in a later pass, rather than the JVP happening ""on the fly"" where we could've noticed the output of the JVP rule was batched. That's why replacing the `lax.map` application with a `jnp.stack([jnp.sinc(x_) for x_ in x])` did not exhibit the bug. (Sorry, this paragraph is probably even more insidebaseball than the preceding paragraph...) This analysis directly leads to the two of the possible solutions briefly mentioned above: 1. if we had a `full_like` primitive then we could write `f` so as to model the same data dependence as `f_jvp` here; 2. if we defensively assume more stuff will come out batched in the `custom_jvp_call_jaxpr` vmap rule, we won't get this issue of JVPs looking batched where primals aren't. I kind of like the first approach but I'm not sure. I might land the quick fix in the meantime, since multiplying by zero isn't so bad (and it encodes the data dependence we want to maintain).","If we don't want to require users' `custom_jvp` functions to respect the data dependence invariant mentioned above, then we should go with solution 2, i.e. defensively batching more outputs. But that might have issues with `jacfwd` where we don't want primals to be batched but we do want tangents to be batched...","Say we have ```python f = jax.custom_jvp(f_orig) f.defjvp(f_jvp) ``` A sufficient requirement for correctness here, which the original implementation of `sinc` violated (because `_sinc_maclaurin` was specializing its output to a particular input _point_) is that for all `x` we have ```python f_jvp(x, t)[1] ≈ f_orig(x + t)  f_orig(x) ``` where `≈` means close as functions of `t` for all `t` in some region around 0. In particular, we want the derivatives to be the same at `t=0`. This requirement is a property on the pair (`f_orig`, `f_jvp`). But the original (`_sinc_maclaurin`, `_sinc_maclaurin_jvp`) violated this property because `_sinc_maclaurin` was a constant function while `lambda t: _sinc_maclaurin_jvp(x, t)[1]` was by necessity a nonzero linear function in `t`. I like this statement of the requirement because it makes clear that we're only requiring that (`f_orig`, `f_jvp`) satisfy the defining characteristic of a JVP. But it's only sufficient and not necessary; the weaker necessary condition we require here is about data dependence, i.e. that `lambda t: f_jvp(x, t)[1]` not have any extra inputoutput data dependencies which `f_orig` does not have. (More precisely we need this condition on ""vmap data dependencies"", whatever that means.) We should probably spell out this requirement on `custom_jvp` rules, and perhaps even have a cookbook recipe for a test. But it's quite an edge case: we only ran into it here because `_sinc_maclaurin` is only applied when its input is zero (under a `jnp.where`), leading us to write it as a constant function. But usually one doesn't write a `custom_jvp` of a constant function! Moreover, it's only an issue when combining both `vmap` and `scan` (or other control flow) on top.","Thank you for the quick fix, and also for the detailed explanation. With those changes it works perfectly. And thanks again for your work on jax!","Just curious, why `lax.full_like(x, constant)` doesn't model the data dependency correctly? Shouldn't it be equivalent to `x * 0 + constant`?"," great question, as always. As a Python function we would say `lax.full_like(x, constant)` has a data dependence on `x`. But as you know, JAX doesn't ""see"" Python functions directly: instead JAX _specializes_ Python functions while tracing. In this case, `jit` specializes on the shape and dtype of `x`, so these two are indistinguishable from one another: ```python x = jnp.ones(3, dtype='float32')  def f(x):   return lax.full_like(x, 0) f(x)  def g(x):   return lax.full((3,), lax.convert_element_type(0, 'float32')) g(x) ``` Notice that `g`, which is a shapeanddtypespecialized version of `f`, has no data dependence on its input! If instead `full_like` were a JAX primitive, then instead of tracetimespecializing away this Python data dependence, we would preserve the data dependence in the ""JAX view"" of things. That would give us a more natural way to express this particular code. (Indeed, because multiplication and addition are JAX primitives, that's why the `x * 0 + constant` spelling of the same function isn't dropping the data dependence.) But there would still be different spellings that would be problematic: `jnp.zeros(x.shape, x.dtype)` wouldn't be equivalent, for example. I chalk this up to the complexity of JAX being a staged system: we're kind of programming and metaprogramming at the same time, in that our Python expressions sometimes correspond fairly directly to jaxpr expressions but sometimes they act more like metaprogramming macros. WDYT?"," Thanks for nice explanation! Now I know why ""I think the best fix may be to add a `full_like` primitive."". Since `vmap` operate on shapes, could it become easier to model data dependence(i.e. IIUC, preserve BatchTracer) by ""propagating"" tracer to shape for `BatchTracer.shape`? Hmmm. ```python def f(x):     y = lax.full_like(x, 0)     print(x)     print(y)     return y jax.vmap(f)(jnp.ones((3,3)))  Tracedwith with    val = DeviceArray([[1., 1., 1.],               [1., 1., 1.],               [1., 1., 1.]], dtype=float32)    batch_dim = 0  [0. 0. 0.]  y  expect y is  Tracedwith with    val = DeviceArray([[0., 0., 0.],               [0., 0., 0.],               [0., 0., 0.]], dtype=float32)    batch_dim = 0  like y = x * 0 + 0 ```","Fixed by CC(quick fix for 10750, add checks and todo)!"
2640,"以下是一个github上的jax下的一个issue, 标题是(Suspected bug in jax.lax.conv_general_dilated_patches)， 内容是 (Trying to implement the solution here but ran into the following bug: ```import jax import jax.numpy as jnp dev = jax.local_devices()[0] ftmap_fn = jax.device_put(jnp.ones((1, 384, 384, 256), dtype=jnp.bfloat16), device=dev) W = 5 stride, padding = (4, 4), (W // 2, W // 2) jax.lax.conv_general_dilated_patches(ftmap_fn, (5, 5), stride, padding).shape ``` but i found the following error: ``` ... File ~/.local/lib/python3.8/sitepackages/jax/_src/dispatch.py:609, in XlaCompiledComputation.from_xla_computation(name, xla_computation, nreps, device, backend, tuple_args, in_avals, out_avals, kept_var_idx)     606 options.parameter_is_tupled_arguments = tuple_args     607 with log_elapsed_time(f""Finished XLA compilation of {name} ""     608                       ""in {elapsed_time} sec""): > 609   compiled = compile_or_get_cached(backend, xla_computation, options)     610 buffer_counts = (None if len(out_avals) == 1 else     611                  [aval_to_num_buffers(aval) for aval in out_avals])     612 execute = _execute_compiled if nreps == 1 else _execute_replicated File ~/.local/lib/python3.8/sitepackages/jax/_src/dispatch.py:578, in compile_or_get_cached(backend, computation, compile_options)     575   ir_str = (computation if isinstance(computation, str)     576             else computation.as_hlo_text())     577   _dump_ir_to_file(module_name, ir_str) > 578 return backend_compile(backend, computation, compile_options) File ~/.local/lib/python3.8/sitepackages/jax/_src/profiler.py:206, in annotate_function..wrapper(*args, **kwargs)     203 (func)     204 def wrapper(*args, **kwargs):     205   with TraceAnnotation(name, **decorator_kwargs): > 206     return func(*args, **kwargs)     207   return wrapper File ~/.local/lib/python3.8/sitepackages/jax/_src/dispatch.py:532, in backend_compile(backend, built_c, options)     528 .annotate_function     529 def backend_compile(backend, built_c, options):     530    we use a separate function call to ensure that XLA compilation appears     531    separately in Python profiling results > 532   return backend.compile(built_c, compile_options=options) RuntimeError: UNKNOWN: :4:130: error: expected '[' ```  The error looks very cryptic cannot figure out what it means. Is it an XLA / Jax bug? Thanks! Environment: TPUVM v38 with software version tf.2.8.0 ``` jax==0.3.5 jaxlib==0.3.5 libtpunightly==0.1.dev20220407 cloudtpuclient==0.10 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Suspected bug in jax.lax.conv_general_dilated_patches,"Trying to implement the solution here but ran into the following bug: ```import jax import jax.numpy as jnp dev = jax.local_devices()[0] ftmap_fn = jax.device_put(jnp.ones((1, 384, 384, 256), dtype=jnp.bfloat16), device=dev) W = 5 stride, padding = (4, 4), (W // 2, W // 2) jax.lax.conv_general_dilated_patches(ftmap_fn, (5, 5), stride, padding).shape ``` but i found the following error: ``` ... File ~/.local/lib/python3.8/sitepackages/jax/_src/dispatch.py:609, in XlaCompiledComputation.from_xla_computation(name, xla_computation, nreps, device, backend, tuple_args, in_avals, out_avals, kept_var_idx)     606 options.parameter_is_tupled_arguments = tuple_args     607 with log_elapsed_time(f""Finished XLA compilation of {name} ""     608                       ""in {elapsed_time} sec""): > 609   compiled = compile_or_get_cached(backend, xla_computation, options)     610 buffer_counts = (None if len(out_avals) == 1 else     611                  [aval_to_num_buffers(aval) for aval in out_avals])     612 execute = _execute_compiled if nreps == 1 else _execute_replicated File ~/.local/lib/python3.8/sitepackages/jax/_src/dispatch.py:578, in compile_or_get_cached(backend, computation, compile_options)     575   ir_str = (computation if isinstance(computation, str)     576             else computation.as_hlo_text())     577   _dump_ir_to_file(module_name, ir_str) > 578 return backend_compile(backend, computation, compile_options) File ~/.local/lib/python3.8/sitepackages/jax/_src/profiler.py:206, in annotate_function..wrapper(*args, **kwargs)     203 (func)     204 def wrapper(*args, **kwargs):     205   with TraceAnnotation(name, **decorator_kwargs): > 206     return func(*args, **kwargs)     207   return wrapper File ~/.local/lib/python3.8/sitepackages/jax/_src/dispatch.py:532, in backend_compile(backend, built_c, options)     528 .annotate_function     529 def backend_compile(backend, built_c, options):     530    we use a separate function call to ensure that XLA compilation appears     531    separately in Python profiling results > 532   return backend.compile(built_c, compile_options=options) RuntimeError: UNKNOWN: :4:130: error: expected '[' ```  The error looks very cryptic cannot figure out what it means. Is it an XLA / Jax bug? Thanks! Environment: TPUVM v38 with software version tf.2.8.0 ``` jax==0.3.5 jaxlib==0.3.5 libtpunightly==0.1.dev20220407 cloudtpuclient==0.10 ```",2022-05-16T18:52:29Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/10729,"The issue is that `padding` should be a list of pairs, one for each dimension. e.g., this works: ``` stride, padding = (4, 4), [(W // 2, W // 2), (W // 2, W //2)] ``` We are apparently missing some input validation that would have caught this sooner. I'll fix that!"
2952,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Error when converting function wrapped in jax.grad to SavedModel format)， 内容是 (When trying to convert models in CC([jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False), an issue was uncovered with converting to SavedModel format. Copying 's remark from there: ""The users needs to be aware that their input signature has changed to something flat, and ideally they need to name it in a way that makes sense. They just define the model and a way to pass args to it, which can be tricky when you’re using it in a number of platforms/languages. There could be a utility function left around though, which people are advised to use which wraps f and inputs."" And from : ""would the most simple example code to add to the jax2tf readme be something like this?"" ```py def your_fn(nested_data):    function that you want to convert to TensorFlow with jax2tf nested_data =  add an input data sample here flat_data = jax.tree_map(tf.constant, jax.tree_flatten(nested_data)[0]) flat_names = list(flatten(nested_data)) _, nested_data_structure = jax.tree_flatten(nested_data) def your_fn_flat(flat_data):   nested_data = jax.tree_unflatten(nested_data_structure, nested_data)   return your_fn(nested_data) your_fn_flat_tf = jax2tf.convert(your_fn_flat, enable_xla=False) my_model = tf.Module() my_model.f = tf.function(your_fn_flat_tf, autograph=False, jit_compile=True, input_signature=[   jax.tree_map(lambda x, name: tf.TensorSpec(x.shape, x.dtype, name=name), flat_data, flat_names), ]) model_name = 'your_fn_flat' tf.saved_model.save(my_model, model_name,  options=tf.saved_model.SaveOptions(experimental_custom_gradients=False), signatures=my_model.f.get_concrete_function()) ``` I think one problem with that is in cases like this: `list(flatten({'foo':{'bar':1}, 'foo_bar':2}))`, which returns `['foo_bar']`. It's also just kind of complicated, and I'm concerned that it would be hard for many people to use even if that were put in the jax2tf readme, especially if they have multiple inputs, and perhaps also need to use _variables. But maybe most users of jax2tf won't be as much of a noob as me? I'm not sure. I'm also wondering if the `signatures=my_model.f.get_concrete_function()` bit is necessary in `tf.saved_model.save`, since we've already defined input_signature during the tf.function creation? Strangely, if I add it to the CLIP notebook that I linked above, then I get this error during `tf.saved_model.save`: ``` ValueError: Got a nonTensor value  for key 'output_0' in the output of the function __inference_score_tf_grad_95495 used to generate the SavedModel signature 'serving_default'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Error when converting function wrapped in jax.grad to SavedModel format,"When trying to convert models in CC([jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False), an issue was uncovered with converting to SavedModel format. Copying 's remark from there: ""The users needs to be aware that their input signature has changed to something flat, and ideally they need to name it in a way that makes sense. They just define the model and a way to pass args to it, which can be tricky when you’re using it in a number of platforms/languages. There could be a utility function left around though, which people are advised to use which wraps f and inputs."" And from : ""would the most simple example code to add to the jax2tf readme be something like this?"" ```py def your_fn(nested_data):    function that you want to convert to TensorFlow with jax2tf nested_data =  add an input data sample here flat_data = jax.tree_map(tf.constant, jax.tree_flatten(nested_data)[0]) flat_names = list(flatten(nested_data)) _, nested_data_structure = jax.tree_flatten(nested_data) def your_fn_flat(flat_data):   nested_data = jax.tree_unflatten(nested_data_structure, nested_data)   return your_fn(nested_data) your_fn_flat_tf = jax2tf.convert(your_fn_flat, enable_xla=False) my_model = tf.Module() my_model.f = tf.function(your_fn_flat_tf, autograph=False, jit_compile=True, input_signature=[   jax.tree_map(lambda x, name: tf.TensorSpec(x.shape, x.dtype, name=name), flat_data, flat_names), ]) model_name = 'your_fn_flat' tf.saved_model.save(my_model, model_name,  options=tf.saved_model.SaveOptions(experimental_custom_gradients=False), signatures=my_model.f.get_concrete_function()) ``` I think one problem with that is in cases like this: `list(flatten({'foo':{'bar':1}, 'foo_bar':2}))`, which returns `['foo_bar']`. It's also just kind of complicated, and I'm concerned that it would be hard for many people to use even if that were put in the jax2tf readme, especially if they have multiple inputs, and perhaps also need to use _variables. But maybe most users of jax2tf won't be as much of a noob as me? I'm not sure. I'm also wondering if the `signatures=my_model.f.get_concrete_function()` bit is necessary in `tf.saved_model.save`, since we've already defined input_signature during the tf.function creation? Strangely, if I add it to the CLIP notebook that I linked above, then I get this error during `tf.saved_model.save`: ``` ValueError: Got a nonTensor value  for key 'output_0' in the output of the function __inference_score_tf_grad_95495 used to generate the SavedModel signature 'serving_default'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor. ```",2022-05-16T13:30:44Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/10725," I have tried isolating your issue from CC([jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False), but could you help me by providing a somewhat minimal example of when you encounter the issue and what error message you got exactly?","I think might be a bug/issue with `tf.lite.TFLiteConverter.from_saved_model` and `tf.lite.TFLiteConverter.from_concrete_function` rather than `jax2tf`, but I'm not sure. The error occurs when trying to convert a `SavedModel` (that was created via `jax2tf` + `tf.saved_model.save(...)`) to tflite. Originally it seemed like it had something to do with nesting of the inputs to the function that was saved, but here's a minimal replication of the error without any nesting of the function's input parameters: https://colab.research.google.com/drive/1Te2uHCFOdZgRTN4VsAeIGjASK80E4im The exact error shown in the above notebook is ""ValueError: Only support at least one signature key."" A potentially related problem is that if we use `jax.grad(score)` in the above example instead of GradientTape1, then there are no errors, but the tflite file is only 19kb: https://colab.research.google.com/drive/1H3PwVAgPxjv6ldJB9k4gc4pakCW8Scoy The only difference between the former and the latter Colab notebooks is that I've used `jax.grad` in the latter instead of TF's `GradientTape` to get the gradient of the `score` function. So I'd really have expected the same ""no input node"" error in the latter too  not sure what's going on there, but it might be a clue. It initially seemed like this could be workedaround by using your suggestion of using `tf.lite.TFLiteConverter.from_concrete_function` instead of `tf.lite.TFLiteConverter.from_saved_model`, but `from_concrete_function` produces a ~500kb `.tflite` file (see your notebook  note that it's converting LiT, whereas the previous two Colabs that I linked are converting CLIP  I'm guessing `from_concrete_function`, like `from_saved_model`, would produce a ~19kb file if used on CLIP). Also, note that Oliver did manage to get CLIP gradients working in tflite with `tf.lite.TFLiteConverter.experimental_from_jax`: https://colab.research.google.com/drive/1elQpDL1ysaOMncMnnuHEalnh4HAAeC53  A relevant comment from : >* jax2tf involves mapping jaxpr of the program to tensorflow ops, saving a saved model then loading and passing it with tflite converter. When enable_xla=False is used (which for tflite it's not required) this involves mapping to high level tensorflow ops. > >* tflite.experimental_from_jax uses the HLO itself, and doesn't save any intermediate files. > >While both are perfectly valid, the second seems like less possible pain points. So I'm not sure if this is a problem with `jax2tf` or TensorFlow, but if this issue is kept open, the title should probably be changed to something vaguely like ""Small tflite file (only input and output nodes) or *no* input node when converting function wrapped in jax.grad to tflite via jax2tf"". 1 I previously had to use `GradientTape` because of the lack of scatter add support, which Oliver's PR ( CC([jax2tf] add rudimentary scatter support for enable_xla=False)) seems to have fixed.","Hi , I believe the small filesize is due to the fact that there is a bug in the jax2tf conversion of `jax.grad` with `enable_xla=False`, which causes all outputs to be zero, so TFLite optimizes this by removing the params (which are simply unused constants then). (see CC([jax2tf] Converting jax.grad(ViT) with enable_xla=False fails)). I would argue that when converting to TFLite you should really not have to create a `SavedModel` at all. If you define your apply function by closing over your model's parameters, they will be stored as constants in the TF graph, and TFLite will treat them as constants as well, which seems fine for you (you don't want to do any training it seems).  do you think there is anything else problematic with converting JAX to SavedModel? I was under the impression that you believed we need to do some tree unflattening, and I am trying to understand when this is the case, or whether this was a red herring. Thanks in advance!","Closing this since it isn't clear there is a problem with SavedModel, and as I said before the recommended way of converting a JAX function to TFLite is either using `tf.lite.experimental_from_jax` or using `tf.lite.from_concrete_function`.   please reopen if you are still encountering problems here, or let me know if you think I misunderstood soemthing!"
1056,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: module 'jaxlib.xla_extension' has no attribute 'PmapFunction')， 内容是 (I am getting the above error after installing the latest version of jax and trying to import it for some computations: ``` !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html ``` ``` /usr/local/lib/python3.7/distpackages/jax/_src/lax/linalg.py in ()     977       lu_pivots_to_permutation_p,     978       partial(_lu_pivots_to_permutation_gpu_lowering, > 979               gpu_linalg.cuda_lu_pivots_to_permutation),     980       platform='cuda')     981   mlir.register_lowering( AttributeError: module 'jaxlib.cuda_linalg' has no attribute 'cuda_lu_pivots_to_permutation' ``` It would be helpful if someone could please look into this ASAP. This is a new error I received when running my code within the past hour and had no problem with this earlier. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AttributeError: module 'jaxlib.xla_extension' has no attribute 'PmapFunction',"I am getting the above error after installing the latest version of jax and trying to import it for some computations: ``` !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html ``` ``` /usr/local/lib/python3.7/distpackages/jax/_src/lax/linalg.py in ()     977       lu_pivots_to_permutation_p,     978       partial(_lu_pivots_to_permutation_gpu_lowering, > 979               gpu_linalg.cuda_lu_pivots_to_permutation),     980       platform='cuda')     981   mlir.register_lowering( AttributeError: module 'jaxlib.cuda_linalg' has no attribute 'cuda_lu_pivots_to_permutation' ``` It would be helpful if someone could please look into this ASAP. This is a new error I received when running my code within the past hour and had no problem with this earlier. ",2022-05-15T19:59:47Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/10717, please check your latest commit,Which version were you running with previously? Any latest commit would not affect you since you are installing an already built version.,"> Which version were you running with previously? >  > Any latest commit would not affect you since you are installing an already built version. This fetches the latest version right? ``` !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html ``` How do I specify the previous version while installing?","I downgraded using this which I found on StackOverflow: ``` !pip install upgrade jax==0.3.10 jaxlib==0.3.10+cuda11.cudnn82 f https://storage.googleapis.com/jaxreleases/jax_releases.html ``` It would be helpful if this could be mentioned on the README page as it becomes difficult to revert back to older versions with only the general installation command that is provided, when newer versions are released and some functionalities suddenly do not work. ",We'll release with the fix again! Sorry for the breakage.,No worries!,We released jax 0.3.12: https://pypi.org/project/jax/0.3.12/ with the fix.,"Ok, thanks for addressing this. "
4880,"以下是一个github上的jax下的一个issue, 标题是(MHLO doesn't contain same transformations as HLO)， 内容是 (More a question than a bug about whether this behaviour is desired. Currently, the MHLO from compiler_ir is taking the jaxpr and writing it in the MHLO dialect. It doesn't seem to have the simplification passes that HLO does. See a gather below, the HLO and compiled HLO both transform the gather into a slice operation while MHLO does not.  ```python import jax from jax import numpy as jnp x = jnp.zeros((2,)) f = lambda x: x.at[0].get() jaxpr = jax.make_jaxpr(f)(x) l = jax.jit(f).lower(x) mhlo = l.compiler_ir('mhlo') hlo = l.compiler_ir('hlo').as_hlo_text() c = l.compile() compiled_hlo = c.compiler_ir()[0].to_string() print("" jaxpr"") display(jaxpr) print(' MHLO\n', mhlo) print(' HLO\n', hlo) print(' Compiled HLO\n', compiled_hlo)  Output  jaxpr { lambda ; a:f32[2]. let     b:bool[] = lt 0 0     c:i32[] = add 0 2     d:i32[] = select_n b 0 c     e:i32[] = convert_element_type[new_dtype=int32 weak_type=False] d     f:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] e     g:f32[] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(1,)       unique_indices=True     ] a f   in (g,) }  MHLO  module .2 {   func public (%arg0: tensor) > tensor {     %0 = mhlo.constant dense : tensor     %1 = mhlo.constant dense : tensor     %2 = ""mhlo.compare""(%0, %1) {compare_type = mhlo, comparison_direction = mhlo} : (tensor, tensor) > tensor     %3 = mhlo.constant dense : tensor     %4 = mhlo.constant dense : tensor     %5 = mhlo.add %3, %4 : tensor     %6 = mhlo.constant dense : tensor     %7 = ""mhlo.select""(%2, %5, %6) : (tensor, tensor, tensor) > tensor     %8 = mhlo.convert %7 : tensor     %9 = ""mhlo.broadcast_in_dim""(%8) {broadcast_dimensions = dense : tensor} : (tensor) > tensor     %10 = ""mhlo.gather""(%arg0, %9) {dimension_numbers = mhlo.gather, indices_are_sorted = true, slice_sizes = dense : tensor} : (tensor, tensor) > tensor     return %10 : tensor   } }  HLO  HloModule jit__lambda_.2 ENTRY main.4 {   Arg_0.1 = f32[2]{0} parameter(0)   slice.2 = f32[1]{0} slice(Arg_0.1), slice={[0:1]}   ROOT reshape.3 = f32[] reshape(slice.2) }  Compiled HLO  HloModule jit__lambda_.2 %fused_computation (param_0.1: f32[2]) > f32[] {   %param_0.1 = f32[2]{0} parameter(0)   %slice.0 = f32[1]{0} slice(f32[2]{0} %param_0.1), slice={[0:1]}, metadata={op_name=""jit()/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)) slice_sizes=(1,) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]"" source_file="""" source_line=4}   ROOT %reshape.0 = f32[] reshape(f32[1]{0} %slice.0), metadata={op_name=""jit()/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)) slice_sizes=(1,) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]"" source_file="""" source_line=4} } ENTRY %main.4 (Arg_0.1: f32[2]) > f32[] {   %Arg_0.1 = f32[2]{0} parameter(0)   ROOT %fusion = f32[] fusion(f32[2]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit()/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)) slice_sizes=(1,) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]"" source_file="""" source_line=4} } ``` One could produce the MHLO simplified from the xla computation as below rather than parsing the jaxpr ```python from jax._src.lib.mlir import ir from jax._src.lib import xla_client as xc from jax.interpreters import mlir context = mlir.make_ir_context() xla_comp = l._xla_computation() module_str = xc._xla.mlir.xla_computation_to_mlir_module(xla_comp) m = ir.Module.parse(module_str, context=context) print(' MHLO converted from underlying xla computation\n', m)  Output  MHLO converted from HLO  module .6 {   func (%arg0: tensor) > tensor {     %0 = ""mhlo.slice""(%arg0) {limit_indices = dense : tensor, start_indices = dense : tensor, strides = dense : tensor} : (tensor) > tensor     %1 = ""mhlo.reshape""(%0) : (tensor) > tensor     return %1 : tensor   } } ``` Note: Currently tflite's experimental_from_jax converter uses this path, generating the xla computation which it then deals with the MHLO repr of I think the grand plan is to eliminate HLO and MHLO to be all of it. This check here could be used to be sure all the great optimisations on HLO are transferred to MHLO.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,MHLO doesn't contain same transformations as HLO,"More a question than a bug about whether this behaviour is desired. Currently, the MHLO from compiler_ir is taking the jaxpr and writing it in the MHLO dialect. It doesn't seem to have the simplification passes that HLO does. See a gather below, the HLO and compiled HLO both transform the gather into a slice operation while MHLO does not.  ```python import jax from jax import numpy as jnp x = jnp.zeros((2,)) f = lambda x: x.at[0].get() jaxpr = jax.make_jaxpr(f)(x) l = jax.jit(f).lower(x) mhlo = l.compiler_ir('mhlo') hlo = l.compiler_ir('hlo').as_hlo_text() c = l.compile() compiled_hlo = c.compiler_ir()[0].to_string() print("" jaxpr"") display(jaxpr) print(' MHLO\n', mhlo) print(' HLO\n', hlo) print(' Compiled HLO\n', compiled_hlo)  Output  jaxpr { lambda ; a:f32[2]. let     b:bool[] = lt 0 0     c:i32[] = add 0 2     d:i32[] = select_n b 0 c     e:i32[] = convert_element_type[new_dtype=int32 weak_type=False] d     f:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] e     g:f32[] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(1,)       unique_indices=True     ] a f   in (g,) }  MHLO  module .2 {   func public (%arg0: tensor) > tensor {     %0 = mhlo.constant dense : tensor     %1 = mhlo.constant dense : tensor     %2 = ""mhlo.compare""(%0, %1) {compare_type = mhlo, comparison_direction = mhlo} : (tensor, tensor) > tensor     %3 = mhlo.constant dense : tensor     %4 = mhlo.constant dense : tensor     %5 = mhlo.add %3, %4 : tensor     %6 = mhlo.constant dense : tensor     %7 = ""mhlo.select""(%2, %5, %6) : (tensor, tensor, tensor) > tensor     %8 = mhlo.convert %7 : tensor     %9 = ""mhlo.broadcast_in_dim""(%8) {broadcast_dimensions = dense : tensor} : (tensor) > tensor     %10 = ""mhlo.gather""(%arg0, %9) {dimension_numbers = mhlo.gather, indices_are_sorted = true, slice_sizes = dense : tensor} : (tensor, tensor) > tensor     return %10 : tensor   } }  HLO  HloModule jit__lambda_.2 ENTRY main.4 {   Arg_0.1 = f32[2]{0} parameter(0)   slice.2 = f32[1]{0} slice(Arg_0.1), slice={[0:1]}   ROOT reshape.3 = f32[] reshape(slice.2) }  Compiled HLO  HloModule jit__lambda_.2 %fused_computation (param_0.1: f32[2]) > f32[] {   %param_0.1 = f32[2]{0} parameter(0)   %slice.0 = f32[1]{0} slice(f32[2]{0} %param_0.1), slice={[0:1]}, metadata={op_name=""jit()/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)) slice_sizes=(1,) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]"" source_file="""" source_line=4}   ROOT %reshape.0 = f32[] reshape(f32[1]{0} %slice.0), metadata={op_name=""jit()/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)) slice_sizes=(1,) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]"" source_file="""" source_line=4} } ENTRY %main.4 (Arg_0.1: f32[2]) > f32[] {   %Arg_0.1 = f32[2]{0} parameter(0)   ROOT %fusion = f32[] fusion(f32[2]{0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit()/jit(main)/gather[dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)) slice_sizes=(1,) unique_indices=True indices_are_sorted=True mode=GatherScatterMode.PROMISE_IN_BOUNDS fill_value=None]"" source_file="""" source_line=4} } ``` One could produce the MHLO simplified from the xla computation as below rather than parsing the jaxpr ```python from jax._src.lib.mlir import ir from jax._src.lib import xla_client as xc from jax.interpreters import mlir context = mlir.make_ir_context() xla_comp = l._xla_computation() module_str = xc._xla.mlir.xla_computation_to_mlir_module(xla_comp) m = ir.Module.parse(module_str, context=context) print(' MHLO converted from underlying xla computation\n', m)  Output  MHLO converted from HLO  module .6 {   func (%arg0: tensor) > tensor {     %0 = ""mhlo.slice""(%arg0) {limit_indices = dense : tensor, start_indices = dense : tensor, strides = dense : tensor} : (tensor) > tensor     %1 = ""mhlo.reshape""(%0) : (tensor) > tensor     return %1 : tensor   } } ``` Note: Currently tflite's experimental_from_jax converter uses this path, generating the xla computation which it then deals with the MHLO repr of I think the grand plan is to eliminate HLO and MHLO to be all of it. This check here could be used to be sure all the great optimisations on HLO are transferred to MHLO.",2022-05-15T18:46:22Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/10715,"The compilation flow of JAX looks like this, roughly: Python > jaxpr > MHLO > optimized MHLO > HLO > optimized HLO. `mhlo` in your script above corresponds to ""MHLO"" above, i.e., before any optimization has been done. `hlo` in your script corresponds to ""HLO"". We still go via MHLO and MHLO optimizations to get here! And any optimization that has been done was done by an MHLO rewrite: the conversion to HLO is almost mechanical beyond those rewrites. `compiled_hlo` corresponds to ""optimized HLO"", which includes many other rewrites that XLA does during compilation. Note that it's entirely expected that optimizations haven't yet been done on `mhlo`! We do a handful of optimizations on the MHLO before converting it to HLO: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/xla/pjrt/mlir_to_hlo.cc;drc=74f728173a6a3b013f428668e1269b775c0a1e64;l=37 The key rewrite is `canonicalize`, which applies a number of simplifications, I believe including the ones you are noting. I'd suggest for the `tflite` conversion probably should be applying MHLO rewrites to simplify the `gather` also. Hope that helps!","With https://github.com/google/jax/pull/10723 you can run the canonicalizer from Python, which might help if you want to play around with this: ``` In [3]: import jax, jax.lax as lax, jaxlib.mlir.passmanager as passmanager, jaxlib.mlir.transforms as transforms In [4]: m  = jax.jit(lambda x: x + lax.add(2, 3)).lower(7).compiler_ir() WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) In [5]: print(m) module .0 {   func public (%arg0: tensor) > tensor {     %0 = mhlo.constant dense : tensor     %1 = mhlo.constant dense : tensor     %2 = mhlo.add %0, %1 : tensor     %3 = mhlo.add %arg0, %2 : tensor     return %3 : tensor   } } In [6]: with m.context: passmanager.PassManager.parse(""canonicalize"").run(m) In [7]: print(m) module .0 {   func public (%arg0: tensor) > tensor {     %0 = mhlo.constant dense : tensor     %1 = mhlo.add %arg0, %0 : tensor     return %1 : tensor   } } ```"
6031,"以下是一个github上的jax下的一个issue, 标题是(add core.closed_call_p)， 内容是 (This PR adds a variant of core.call_p called core.closed_call_p. The only difference is that in its 'jaxpr form' its call_jaxpr parameter is a core.ClosedJaxpr rather than a core.Jaxpr. Some background: * `core.call_p` is the most vanilla call primitive possible: unlike, say, `xla_call_p` (the primitive underlying `jax.jit`), its impl rule isn't to compile an XLA computation, but instead it's just to interpret its jaxpr (staying in Python, using `core.eval_jaxpr`). Correspondingly, it doesn't need to raise the abstraction level of its arguments. It's basically a model for other ""finalstyle"" call primitives, each of which is interesting in precisely how it deviates from `core.call_p` (e.g. `xla_call_p`'s impl rule stages out for compilation; `remat_call_p` has a special partial evaluation rule; `custom_jvp_call_p` has a special JVP rule; etc). Historically it    was the first call primitive we introduced, just to test the system; `core.call_p` is not really used anywhere. * `core.ClosedJaxpr` is a data type which would be better named as `PartiallyAppliedJaxpr`. When we form jaxprs, they usually get paired with ""constants"" (e.g. `trace_to_jaxpr_nounits` and `trace_to_jaxpr_dynamic` output a list of constants), which are values that are not arguments and that we don't want to turn into literals (e.g. because we want to deduplicate them, or even just avoid inlining them in prettyprints). In some cases, these ""constants"" can be `core.Tracer`s, like when we form the jaxprs for `jax.lax.scan` and the body function closes over some `Tracer`; when that's possible, because `Tracer`s have to be handled with `core.Primitive.bind`, we  typically just convert them to arguments (via `pe.convert_constvars_jaxpr`). But in other cases the constants that come out can't be `Tracer`s (e.g. in the JVP rule of an initialstyle primitive, when we run `ad.jvp_jaxpr`, we can get new constants out which can't be `Tracer`s and must be raw array values). That's when `core.ClosedJaxpr` comes in handy: it lets us pair a `jaxpr` with some array constants so that the caller, e.g. a JVP rule for an initialstyle higherorder primitive, doesn't need to deal with handling new constant values and their input binders. In other words, primitives which are parameterized by `ClosedJaxpr`s can have simpler rules, especially jaxprtojaxpr rules, since those rules don't need to worry about handling new constants/binders introduced by the rule. On that last point, when working on CC([newremat] add scan rule for new remat) we ran into a situation where * the current signature for ""custompolicy partial eval rules"" didn't allow a custom partial evaluation rule to introduce new constants (because such rules just get to output a pair of `Optional[JaxprEqn]`s and have no output for ""new constants for the caller to handle appropriately""); * but to perform an optimization, namely hoisting loopinvariant residual computations out of a `scan` body, we might need such a rule to introduce multiple equations as well as new constants. To proceed, there were at least two options: 1. make the signature for custompolicy partial evaluation rules even more complex (to support outputting multiple equations, new variable names being introduced, new constants, etc) 2. just use a call primitive to handle the ""multiple equations with new variables"" problem, and as long as it was a call primitive with a `ClosedJaxpr` it would handle the constants problem too. I chose the second approach, which led to this PR. For simplicity, we could delete `core.call_p` in favor of this `core.closed_call_p`; after all, the former is not used at all. Going further, we might want to make all higherorder primitives (i.e. even the finalstyle ones, not just the initial style ones as at present) take `ClosedJaxpr`s rather than `Jaxpr`s; futher still, at    that point we could deduplicate `Jaxpr` and `ClosedJaxpr` so that we only have one such type. Those simplifications sound reasonable, but they're out of scope for this PR. Here I just want to land a change for enabling the new `remat` implementation with `scan` inside! Finally, some notes on the changes here. Finalstyle primitives (like the new `closed_call_p`) have two forms, with different parameters: the 'bind form' used during tracing which takes a Python callable as a parameter representing the function to be called (really a `linear_util.WrappedFun`), and the 'jaxpr form' which appears in  a jaxpr which itself takes a `Jaxpr` (or after this PR alternatively a `ClosedJaxpr`). Since we're introducing a primitive which is like `core.call_p` except that it takes a `ClosedJaxpr` parameter, we need to * update places where the bindform primitive is converted to the jaxprform primitive (i.e. `JaxprTrace.process_call` and `DynamicJaxprTrace.process_call` in partial_eval.py, both of which can be handled by using the existing ""call param updater"" hook) to actually produce a `ClosedJaxpr` parameter; * update places where the jaxprform is converted to the bindform (namely `ClosedCallPrimitive.get_bind_params` in core.py) * update rules which consume the jaxprform to handle the `ClosedJaxpr` parameter (namely the MLIR lowering rule in mlir.py, the transpose rule in ad.py, the typecheck rule in core.py, the DCE rule in partial_eval.py, and (once it exists for any calls) the forwarding rule in partial_eval.py); note that we do _not_ need to update    rules which consume the bind form (e.g. `JVPTrace.process_call` or `BatchTrace.process_call`) since the bind forms of `call_p` and `closed_call_p` are identical; * update core_test.py to cover the new call primitive. Only the secondtolast bullet seems burdensome. That would be mitigated by moving to make all call primitives take `ClosedJaxpr` parameters, which I _think_ was already a good idea. But again that's out of scope!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,add core.closed_call_p,"This PR adds a variant of core.call_p called core.closed_call_p. The only difference is that in its 'jaxpr form' its call_jaxpr parameter is a core.ClosedJaxpr rather than a core.Jaxpr. Some background: * `core.call_p` is the most vanilla call primitive possible: unlike, say, `xla_call_p` (the primitive underlying `jax.jit`), its impl rule isn't to compile an XLA computation, but instead it's just to interpret its jaxpr (staying in Python, using `core.eval_jaxpr`). Correspondingly, it doesn't need to raise the abstraction level of its arguments. It's basically a model for other ""finalstyle"" call primitives, each of which is interesting in precisely how it deviates from `core.call_p` (e.g. `xla_call_p`'s impl rule stages out for compilation; `remat_call_p` has a special partial evaluation rule; `custom_jvp_call_p` has a special JVP rule; etc). Historically it    was the first call primitive we introduced, just to test the system; `core.call_p` is not really used anywhere. * `core.ClosedJaxpr` is a data type which would be better named as `PartiallyAppliedJaxpr`. When we form jaxprs, they usually get paired with ""constants"" (e.g. `trace_to_jaxpr_nounits` and `trace_to_jaxpr_dynamic` output a list of constants), which are values that are not arguments and that we don't want to turn into literals (e.g. because we want to deduplicate them, or even just avoid inlining them in prettyprints). In some cases, these ""constants"" can be `core.Tracer`s, like when we form the jaxprs for `jax.lax.scan` and the body function closes over some `Tracer`; when that's possible, because `Tracer`s have to be handled with `core.Primitive.bind`, we  typically just convert them to arguments (via `pe.convert_constvars_jaxpr`). But in other cases the constants that come out can't be `Tracer`s (e.g. in the JVP rule of an initialstyle primitive, when we run `ad.jvp_jaxpr`, we can get new constants out which can't be `Tracer`s and must be raw array values). That's when `core.ClosedJaxpr` comes in handy: it lets us pair a `jaxpr` with some array constants so that the caller, e.g. a JVP rule for an initialstyle higherorder primitive, doesn't need to deal with handling new constant values and their input binders. In other words, primitives which are parameterized by `ClosedJaxpr`s can have simpler rules, especially jaxprtojaxpr rules, since those rules don't need to worry about handling new constants/binders introduced by the rule. On that last point, when working on CC([newremat] add scan rule for new remat) we ran into a situation where * the current signature for ""custompolicy partial eval rules"" didn't allow a custom partial evaluation rule to introduce new constants (because such rules just get to output a pair of `Optional[JaxprEqn]`s and have no output for ""new constants for the caller to handle appropriately""); * but to perform an optimization, namely hoisting loopinvariant residual computations out of a `scan` body, we might need such a rule to introduce multiple equations as well as new constants. To proceed, there were at least two options: 1. make the signature for custompolicy partial evaluation rules even more complex (to support outputting multiple equations, new variable names being introduced, new constants, etc) 2. just use a call primitive to handle the ""multiple equations with new variables"" problem, and as long as it was a call primitive with a `ClosedJaxpr` it would handle the constants problem too. I chose the second approach, which led to this PR. For simplicity, we could delete `core.call_p` in favor of this `core.closed_call_p`; after all, the former is not used at all. Going further, we might want to make all higherorder primitives (i.e. even the finalstyle ones, not just the initial style ones as at present) take `ClosedJaxpr`s rather than `Jaxpr`s; futher still, at    that point we could deduplicate `Jaxpr` and `ClosedJaxpr` so that we only have one such type. Those simplifications sound reasonable, but they're out of scope for this PR. Here I just want to land a change for enabling the new `remat` implementation with `scan` inside! Finally, some notes on the changes here. Finalstyle primitives (like the new `closed_call_p`) have two forms, with different parameters: the 'bind form' used during tracing which takes a Python callable as a parameter representing the function to be called (really a `linear_util.WrappedFun`), and the 'jaxpr form' which appears in  a jaxpr which itself takes a `Jaxpr` (or after this PR alternatively a `ClosedJaxpr`). Since we're introducing a primitive which is like `core.call_p` except that it takes a `ClosedJaxpr` parameter, we need to * update places where the bindform primitive is converted to the jaxprform primitive (i.e. `JaxprTrace.process_call` and `DynamicJaxprTrace.process_call` in partial_eval.py, both of which can be handled by using the existing ""call param updater"" hook) to actually produce a `ClosedJaxpr` parameter; * update places where the jaxprform is converted to the bindform (namely `ClosedCallPrimitive.get_bind_params` in core.py) * update rules which consume the jaxprform to handle the `ClosedJaxpr` parameter (namely the MLIR lowering rule in mlir.py, the transpose rule in ad.py, the typecheck rule in core.py, the DCE rule in partial_eval.py, and (once it exists for any calls) the forwarding rule in partial_eval.py); note that we do _not_ need to update    rules which consume the bind form (e.g. `JVPTrace.process_call` or `BatchTrace.process_call`) since the bind forms of `call_p` and `closed_call_p` are identical; * update core_test.py to cover the new call primitive. Only the secondtolast bullet seems burdensome. That would be mitigated by moving to make all call primitives take `ClosedJaxpr` parameters, which I _think_ was already a good idea. But again that's out of scope!",2022-05-14T18:07:01Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10711
28938,"以下是一个github上的jax下的一个issue, 标题是(jaxlib v.0.3.10 build failure on linux)， 内容是 (Please:  [X] Check for duplicate issues.  [X] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: I've been trying to build jaxlib v.0.3.10 from source on a new cluster with cuda 11.3.1 and cudnn 8.2.1 but it seems  to be that the bazel builder is having issues with fetching `llvmraw` as its returning an input/output error.  Operating system RHEL 8  CUDA version 11.3.1  cuDNN version 8.2.1.32  Python version 3.9.5  compiler GCC 10.3.0 ```bash $ git clone https://github.com/google/jax $ cd jax $ python build/build.py enable_cuda enable_nccl cuda_path='/usr/local/easybuild/software/CUDA/11.3.1/' cudnn_path='/usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1/' cuda_version='11.3' cudnn_version='8.2.1' target_cpu_features='native' ```  [X] If applicable, include full error messages/tracebacks. ```bash      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: ./bazel5.1.1linuxx86_64 Bazel version: 5.1.1 Python binary path: /usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/bin/python Python version: 3.9 NumPy version: 1.21.0 MKLDNN enabled: yes Target CPU: x86_64 Target CPU features: native CUDA enabled: yes CUDA toolkit path: /usr/local/easybuild/software/CUDA/11.3.1/ CUDNN library path: /usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1/ CUDA version: 11.3 CUDNN version: 8.2.1 NCCL enabled: yes TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel5.1.1linuxx86_64 run verbose_failures=true config=native_arch_posix config=mkl_open_source_only config=cuda :build_wheel  output_path=/home/oxg34/user_modules/jax_0.3.10/jax/dist cpu=x86_64 Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /home/oxg34/user_modules/jax_0.3.10/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/bin/python action_env=PYENV_ROOT python_path=/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/bin/python action_env CUDA_TOOLKIT_PATH=/usr/local/easybuild/software/CUDA/11.3.1/ action_env CUDNN_INSTALL_PATH=/usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1/ action_env TF_CUDA_PATHS=/usr/local/easybuild/software/CUDA/11.3.1/,/usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1/ action_env TF_CUDA_VERSION=11.3 action_env TF_CUDNN_VERSION=8.2.1 distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:native_arch_posix in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: copt=march=native host_copt=march=native INFO: Found applicable config definition build:mkl_open_source_only in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true INFO: Found applicable config definition build:linux in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: config=posix copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error write (Input/output error) INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error write (Input/output error) INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error write (Input/output error) INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error write (Input/output error) b'' Traceback (most recent call last):   File ""/home/oxg34/user_modules/jax_0.3.10/jax/build/build.py"", line 528, in      main()   File ""/home/oxg34/user_modules/jax_0.3.10/jax/build/build.py"", line 523, in main     shell(command)   File ""/home/oxg34/user_modules/jax_0.3.10/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.1linuxx86_64', 'run', 'verbose_failures=true', 'config=native_arch_posix', 'config=mkl_open_source_only', 'config=cuda', ':build_wheel', '', 'output_path=/home/oxg34/user_modules/jax_0.3.10/jax/dist', 'cpu=x86_64']' returned nonzero exit status 2. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jaxlib v.0.3.10 build failure on linux,"Please:  [X] Check for duplicate issues.  [X] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: I've been trying to build jaxlib v.0.3.10 from source on a new cluster with cuda 11.3.1 and cudnn 8.2.1 but it seems  to be that the bazel builder is having issues with fetching `llvmraw` as its returning an input/output error.  Operating system RHEL 8  CUDA version 11.3.1  cuDNN version 8.2.1.32  Python version 3.9.5  compiler GCC 10.3.0 ```bash $ git clone https://github.com/google/jax $ cd jax $ python build/build.py enable_cuda enable_nccl cuda_path='/usr/local/easybuild/software/CUDA/11.3.1/' cudnn_path='/usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1/' cuda_version='11.3' cudnn_version='8.2.1' target_cpu_features='native' ```  [X] If applicable, include full error messages/tracebacks. ```bash      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: ./bazel5.1.1linuxx86_64 Bazel version: 5.1.1 Python binary path: /usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/bin/python Python version: 3.9 NumPy version: 1.21.0 MKLDNN enabled: yes Target CPU: x86_64 Target CPU features: native CUDA enabled: yes CUDA toolkit path: /usr/local/easybuild/software/CUDA/11.3.1/ CUDNN library path: /usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1/ CUDA version: 11.3 CUDNN version: 8.2.1 NCCL enabled: yes TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel5.1.1linuxx86_64 run verbose_failures=true config=native_arch_posix config=mkl_open_source_only config=cuda :build_wheel  output_path=/home/oxg34/user_modules/jax_0.3.10/jax/dist cpu=x86_64 Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /home/oxg34/user_modules/jax_0.3.10/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/bin/python action_env=PYENV_ROOT python_path=/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/bin/python action_env CUDA_TOOLKIT_PATH=/usr/local/easybuild/software/CUDA/11.3.1/ action_env CUDNN_INSTALL_PATH=/usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1/ action_env TF_CUDA_PATHS=/usr/local/easybuild/software/CUDA/11.3.1/,/usr/local/easybuild/software/cuDNN/8.2.1.32CUDA11.3.1/ action_env TF_CUDA_VERSION=11.3 action_env TF_CUDNN_VERSION=8.2.1 distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:native_arch_posix in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: copt=march=native host_copt=march=native INFO: Found applicable config definition build:mkl_open_source_only in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true INFO: Found applicable config definition build:linux in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: config=posix copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error write (Input/output error) INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error write (Input/output error) INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error write (Input/output error) INFO: Repository llvmraw instantiated at:   /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: in    /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/tensorflow/workspace3.bzl:32:9: in workspace   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:128:21: in tf_http_archive Repository rule _tf_http_archive defined at:   /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl:81:35: in  ERROR: An error occurred during the fetch of repository 'llvmraw':    Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error ERROR: /home/oxg34/user_modules/jax_0.3.10/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: Traceback (most recent call last): 	File ""/home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/org_tensorflow/third_party/repo.bzl"", line 64, column 33, in _tf_http_archive_impl 		ctx.download_and_extract( Error in download_and_extract: java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error INFO: Found applicable config definition build:cuda in file /home/oxg34/user_modules/jax_0.3.10/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true write (Input/output error) ERROR: //:enable_cuda :: Error loading option //:enable_cuda: no such package 'raw//utils/bazel': java.io.IOException: Error extracting /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076/e2ed3fd71e08ac50ca326c79f31247e7e4a16b7b.tar.gz to /home/oxg34/.cache/bazel/_bazel_oxg34/6b511a965134ca496153611be50e71ba/external/llvmraw/temp11546401468023270076: Input/output error write (Input/output error) b'' Traceback (most recent call last):   File ""/home/oxg34/user_modules/jax_0.3.10/jax/build/build.py"", line 528, in      main()   File ""/home/oxg34/user_modules/jax_0.3.10/jax/build/build.py"", line 523, in main     shell(command)   File ""/home/oxg34/user_modules/jax_0.3.10/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/usr/local/easybuild/software/Python/3.9.5GCCcore10.3.0/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.1linuxx86_64', 'run', 'verbose_failures=true', 'config=native_arch_posix', 'config=mkl_open_source_only', 'config=cuda', ':build_wheel', '', 'output_path=/home/oxg34/user_modules/jax_0.3.10/jax/dist', 'cpu=x86_64']' returned nonzero exit status 2. ```",2022-05-13T19:28:27Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/10699,"I was able to build successfully using a different compiler. Turns out it was `GCC v10.3.0` causing the issue, when I used `Clang v10.1.0` built against `GCC v9.3.0` the build finished successfully.  If others are running into similar issues with GCC, maybe it would be a good idea to update the docs about building from source?", could you take a look?,Yeah we were also hitting this. But now its fixed at Head. Try bumping up the TF commit in the WORKSPACE file and build again.  I’ll also send a PR to update that file at head. ,"Will try that and try building with GCC v10.3.0 and/or clang built against GCC v10.3.0. Just as a test, i ran into the same issue when trying to build tensorflow from source with GCC v10.3.0. ",I updated the WORSPACE file. Please reopen if its still not fixed.,Haven't tried with GCC yet but TF at head was building with 10.3.0. Thanks for all the help.
1594,"以下是一个github上的jax下的一个issue, 标题是(`--config=cuda` overrides explicit options, instead of augmenting them)， 内容是 (Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import jax.numpy as jnp print(jnp.arange(10))  [0 1 2 3 4 5 6 7 8 9] ```  [x] If applicable, include full error messages/tracebacks.  I am trying to build cudaenabled jaxlib on condaforge. I am running into a problem where the `config=cuda` flag replaces the custom toolchain. I understand this is outside the scope here, but getting cudaenabled jax on condaforge seems to be a very valuable contribution, and I hope you can help us achieve that. We do build cudaenabled tensorflow, and we follow our method almost exactly. The option `config=cuda` works fine in our tensorflow builds. ``` WARNING: option 'config=cuda' (source command line options) was expanded and now overrides the explicit option crosstool_top=//bazel_toolchain:toolchain with crosstool_top=//crosstool:toolchain ``` Once the above happens, the local_config_cuda parameters essentially cannot see any of the system libraries and everything  breaks. I haven't looked too deeply into this yet, so bringing it here first in case you have a quick answer/shortcut. Corresponding PR in condaforge: https://github.com/condaforge/jaxlibfeedstock/pull/97 Tagging  who's been involved in issues related to the builds before. Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"`--config=cuda` overrides explicit options, instead of augmenting them","Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import jax.numpy as jnp print(jnp.arange(10))  [0 1 2 3 4 5 6 7 8 9] ```  [x] If applicable, include full error messages/tracebacks.  I am trying to build cudaenabled jaxlib on condaforge. I am running into a problem where the `config=cuda` flag replaces the custom toolchain. I understand this is outside the scope here, but getting cudaenabled jax on condaforge seems to be a very valuable contribution, and I hope you can help us achieve that. We do build cudaenabled tensorflow, and we follow our method almost exactly. The option `config=cuda` works fine in our tensorflow builds. ``` WARNING: option 'config=cuda' (source command line options) was expanded and now overrides the explicit option crosstool_top=//bazel_toolchain:toolchain with crosstool_top=//crosstool:toolchain ``` Once the above happens, the local_config_cuda parameters essentially cannot see any of the system libraries and everything  breaks. I haven't looked too deeply into this yet, so bringing it here first in case you have a quick answer/shortcut. Corresponding PR in condaforge: https://github.com/condaforge/jaxlibfeedstock/pull/97 Tagging  who's been involved in issues related to the builds before. Thanks!",2022-05-13T17:58:57Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/10696,"I believe I have managed to patch it successfully, but I will keep this open for now. I will update the condaforge PR with details. The relevant patch looks like this (+ removing the additional command line `config=cuda`). I have no idea why triggering these options with `config=cuda` was resulting in problems for us, but here it is...  ```diff diff git a/.bazelrc b/.bazelrc index 147bf0f6b..23dafdf76 100644  a/.bazelrc +++ b/.bazelrc @@ 52,7 +52,7 @@ build:native_arch_posix host_copt=march=native  build:mkl_open_source_only define=tensorflow_mkldnn_contraction_kernel=1 build:cuda repo_env TF_NEED_CUDA=1 +build repo_env TF_NEED_CUDA=1 build:cuda action_env TF_CUDA_COMPUTE_CAPABILITIES=""3.5,5.2,6.0,7.0,8.0"" +build action_env TF_CUDA_COMPUTE_CAPABILITIES=""sm_35,sm_50,sm_60,sm_62,sm_70,sm_72,sm_75,sm_80,sm_86,compute_86"" build:cuda crosstool_top=//crosstool:toolchain +build crosstool_top=//crosstool:toolchain build:cuda //:enable_cuda +build //:enable_cuda build:cuda define=xla_python_enable_gpu=true +build define=xla_python_enable_gpu=true ```", could you take a look?,What command did you use to build jaxlib?,"something like: ```  python build/build.py  bazel_options=crosstool_top=//custom_toolchain:toolchain bazel_options=cpu enable_cuda  ``` More verbosely, ```   ${PYTHON} build/build.py target_cpu_features default enable_mkl_dnn ${CUSTOM_BAZEL_OPTIONS} bazel_options=cpu bazel_options=${TARGET_CPU} bazel_options=""local_cpu_resources=${CPU_COUNT}"" ${CUDA_ARGS:} ``` with the various expansions: ```shell CUSTOM_BAZEL_OPTIONS=""bazel_options=crosstool_top=//custom_toolchain:toolchain"" CUSTOM_BAZEL_OPTIONS=""${CUSTOM_BAZEL_OPTIONS} bazel_options=logging=6 bazel_options=verbose_failures bazel_options=toolchain_resolution_debug bazel_options=define=PREFIX=${PREFIX} bazel_options=define=PROTOBUF_INCLUDE_PATH=${PREFIX}/include"" export TF_CUDA_COMPUTE_CAPABILITIES=sm_35,sm_50,sm_60,sm_62,sm_70,sm_72,sm_75,sm_80,sm_86,compute_86 CUDA_ARGS=""enable_cuda \                enable_nccl \                cuda_path=$CUDA_HOME \                cudnn_path=$PREFIX   \                cuda_compute_capabilities=$TF_CUDA_COMPUTE_CAPABILITIES \                cuda_version=$TF_CUDA_VERSION \                cudnn_version=$TF_CUDNN_VERSION"" ``` see recipe/build.sh in the above PR: https://github.com/condaforge/jaxlibfeedstock/blob/009b2fe3f774570476da47fac0479d05c9ded67a/recipe/build.sh","Here's how I see it:  `enable_cuda` triggers `config=cuda` in build.py  `config=cuda` is passed as commandline arg to bazel  in bazelrc, `config=cuda` sets a toolchain for the nvcc compilation  that in turn results in this: `WARNING: option 'config=cuda' (source command line options) was expanded and now overrides the explicit option crosstool_top=//bazel_toolchain:toolchain with crosstool_top=//crosstool:toolchain`  this override results in our custom_toolchain getting ignored completely (thus missing system libs, etc.)","The breaking point imo is that `crosstool_top=//crosstool:toolchain` should be augmented (since it is only for the nvcc compilation anyway) and not allowed to completely override what the user wants. I have no idea if this is something that's set here or elsewhere, but it is worth looking into, especially visavis tensorflow build. If you look at my patch, I simply assigned that value without the subsection ""cuda"", so it must be something in bazelrc setup that's causing it to go aggressive and override other settings. I tried looking at the bazelrc from tf, and it looked pretty similar to the one here, so I am not really sure what is causing this...","The arguments to `bazel` are ordering sensitive. I'm wonder if the fix would be to reorder the arguments here: https://github.com/google/jax/blob/7d4d15e260e24c54f9fbc1685a45d910edb61e43/build/build.pyL493 so that the userprovided `bazel_options` override `config=cuda` (i.e., come later), rather than the other way around?","> so that the userprovided bazel_options override config=cuda (i.e., come later), rather than the other way around? I attempted a version of this and it didn't work, but there is a good chance I did it badly/wrongly... I will try to do it more explicitly and report back soon",Another thing you can try is to add things at the end to `.jax_configure.bazelrc` (Make sure you are adding your custom things at the end of the file  not the python file but the .jax_configure.bazelrc file.),"I am not going to pursue a fix for this anytime soon. If I run into it again, I will open a new issue. Thanks everyone."
4847,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Expand gathers and scatters to slices and dynamic slices where possible)， 内容是 (When looking into CC([jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False). They'd hit a conversion problem because of a scatteradd in their jaxpr. The root cause comes from handy numpy indexing: https://github.com/googleresearch/vision_transformer/blob/main/vit_jax/models_vit.py, line 283 ```python  x = x[:, 0] ``` This is traced to a gather. The grad of a gather is a scatteradd. This can be slower than dynamic_slice on some accelerators and has patchy support in both hardware and converters due to the complex syntax.  I have rewritten some functions into lax.dynamic_slice to get performance boosts before, it'd be better to avoid the far uglier code it generates. I've had similar issues in scatter where to do `x.at[0].set(y)` I would `concat(y, slice(x,1,n))` to avoid poor scatter implementations. I've written an exit path which catches 'single' gathers into the numpy indexing.  This can be applied to gathers and scatters in general, is there any simple way to do an optimisation sweep on the jax trace at the end? This would avoid me having to cover all the entry routes into gather/scatter. Though I see that this means the user loses the fine grained control over if they want a gather/scatter instead of dynamic slice or not due to their own reasons. Note:  I know that (nearly) all (commonly used) gather/scatters are just loops around: slice array  calculate update  apply update to slice. I would be solving just for loops of length 1. Though it could be handy occassionally to convert short loops as a fix in special cases JAX should not officially support it. Also, I'm only considering unique indices at the moment. ```python import jax import jax.numpy as jnp from jax.lax import GatherScatterMode  simplify jax_pr's def wrap(f):     def _wrapped(*args, **kwargs):         with jax.ensure_compile_time_eval():             return f(*args, **kwargs)     return _wrapped def check(f, x, eval=True):     wrap_fn = wrap if eval else lambda f: f     pr = lambda f: jax.make_jaxpr(wrap_fn(f))     print(f'val\n{pr(f)(x)}\ngrad\n{pr(jax.grad(f))(x)}') x = jnp.arange(6, dtype=float).reshape(2,3) f = lambda x: x[:,0].max() check(f, x)  Output (without pull request) val { lambda a:i32[1]; b:f32[2,3]. let     c:f32[2] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(1,), start_index_map=(1,))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(2, 1)       unique_indices=True     ] b a     d:f32[] = reduce_max[axes=(0,)] c   in (d,) } grad { lambda a:i32[1] b:f32[2,3]; c:f32[2,3]. let     d:f32[2] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(1,), start_index_map=(1,))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.CLIP       slice_sizes=(2, 1)       unique_indices=True     ] c a     e:f32[] = reduce_max[axes=(0,)] d     f:f32[1] = reshape[dimensions=None new_sizes=(1,)] e     g:bool[2] = eq d f     h:f32[2] = convert_element_type[new_dtype=float32 weak_type=False] g     i:f32[] = reduce_sum[axes=(0,)] h     j:f32[] = div 1.0 i     k:f32[2] = broadcast_in_dim[broadcast_dimensions=() shape=(2,)] j     l:f32[2] = mul k h     m:f32[2] = reduce_sum[axes=()] l     n:f32[2,3] = scatteradd[       dimension_numbers=ScatterDimensionNumbers(update_window_dims=(0,), inserted_window_dims=(1,), scatter_dims_to_operand_dims=(1,))       indices_are_sorted=True       mode=GatherScatterMode.CLIP       unique_indices=True       update_consts=()       update_jaxpr={ lambda ; o:f32[] p:f32[]. let q:f32[] = add o p in (q,) }     ] b a m   in (n,) } ``` Output with pull request ```python val { lambda ; a:f32[2,3]. let     b:f32[2,1] = dynamic_slice[slice_sizes=(2, 1)] a 0 0     c:f32[2] = squeeze[dimensions=(1,)] b     d:f32[] = reduce_max[axes=(0,)] c   in (d,) } grad { lambda a:f32[2,3]; b:f32[2,3]. let     c:f32[2,1] = dynamic_slice[slice_sizes=(2, 1)] b 0 0     d:f32[2] = squeeze[dimensions=(1,)] c     e:f32[] = reduce_max[axes=(0,)] d     f:f32[1] = reshape[dimensions=None new_sizes=(1,)] e     g:bool[2] = eq d f     h:f32[2] = convert_element_type[new_dtype=float32 weak_type=False] g     i:f32[] = reduce_sum[axes=(0,)] h     j:f32[] = div 1.0 i     k:f32[2] = broadcast_in_dim[broadcast_dimensions=() shape=(2,)] j     l:f32[2] = mul k h     m:f32[2] = reduce_sum[axes=()] l     n:f32[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] m     o:f32[2,3] = dynamic_update_slice a n 0 0   in (o,) } ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,[jax2tf] Expand gathers and scatters to slices and dynamic slices where possible,"When looking into CC([jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False). They'd hit a conversion problem because of a scatteradd in their jaxpr. The root cause comes from handy numpy indexing: https://github.com/googleresearch/vision_transformer/blob/main/vit_jax/models_vit.py, line 283 ```python  x = x[:, 0] ``` This is traced to a gather. The grad of a gather is a scatteradd. This can be slower than dynamic_slice on some accelerators and has patchy support in both hardware and converters due to the complex syntax.  I have rewritten some functions into lax.dynamic_slice to get performance boosts before, it'd be better to avoid the far uglier code it generates. I've had similar issues in scatter where to do `x.at[0].set(y)` I would `concat(y, slice(x,1,n))` to avoid poor scatter implementations. I've written an exit path which catches 'single' gathers into the numpy indexing.  This can be applied to gathers and scatters in general, is there any simple way to do an optimisation sweep on the jax trace at the end? This would avoid me having to cover all the entry routes into gather/scatter. Though I see that this means the user loses the fine grained control over if they want a gather/scatter instead of dynamic slice or not due to their own reasons. Note:  I know that (nearly) all (commonly used) gather/scatters are just loops around: slice array  calculate update  apply update to slice. I would be solving just for loops of length 1. Though it could be handy occassionally to convert short loops as a fix in special cases JAX should not officially support it. Also, I'm only considering unique indices at the moment. ```python import jax import jax.numpy as jnp from jax.lax import GatherScatterMode  simplify jax_pr's def wrap(f):     def _wrapped(*args, **kwargs):         with jax.ensure_compile_time_eval():             return f(*args, **kwargs)     return _wrapped def check(f, x, eval=True):     wrap_fn = wrap if eval else lambda f: f     pr = lambda f: jax.make_jaxpr(wrap_fn(f))     print(f'val\n{pr(f)(x)}\ngrad\n{pr(jax.grad(f))(x)}') x = jnp.arange(6, dtype=float).reshape(2,3) f = lambda x: x[:,0].max() check(f, x)  Output (without pull request) val { lambda a:i32[1]; b:f32[2,3]. let     c:f32[2] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(1,), start_index_map=(1,))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(2, 1)       unique_indices=True     ] b a     d:f32[] = reduce_max[axes=(0,)] c   in (d,) } grad { lambda a:i32[1] b:f32[2,3]; c:f32[2,3]. let     d:f32[2] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(1,), start_index_map=(1,))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.CLIP       slice_sizes=(2, 1)       unique_indices=True     ] c a     e:f32[] = reduce_max[axes=(0,)] d     f:f32[1] = reshape[dimensions=None new_sizes=(1,)] e     g:bool[2] = eq d f     h:f32[2] = convert_element_type[new_dtype=float32 weak_type=False] g     i:f32[] = reduce_sum[axes=(0,)] h     j:f32[] = div 1.0 i     k:f32[2] = broadcast_in_dim[broadcast_dimensions=() shape=(2,)] j     l:f32[2] = mul k h     m:f32[2] = reduce_sum[axes=()] l     n:f32[2,3] = scatteradd[       dimension_numbers=ScatterDimensionNumbers(update_window_dims=(0,), inserted_window_dims=(1,), scatter_dims_to_operand_dims=(1,))       indices_are_sorted=True       mode=GatherScatterMode.CLIP       unique_indices=True       update_consts=()       update_jaxpr={ lambda ; o:f32[] p:f32[]. let q:f32[] = add o p in (q,) }     ] b a m   in (n,) } ``` Output with pull request ```python val { lambda ; a:f32[2,3]. let     b:f32[2,1] = dynamic_slice[slice_sizes=(2, 1)] a 0 0     c:f32[2] = squeeze[dimensions=(1,)] b     d:f32[] = reduce_max[axes=(0,)] c   in (d,) } grad { lambda a:f32[2,3]; b:f32[2,3]. let     c:f32[2,1] = dynamic_slice[slice_sizes=(2, 1)] b 0 0     d:f32[2] = squeeze[dimensions=(1,)] c     e:f32[] = reduce_max[axes=(0,)] d     f:f32[1] = reshape[dimensions=None new_sizes=(1,)] e     g:bool[2] = eq d f     h:f32[2] = convert_element_type[new_dtype=float32 weak_type=False] g     i:f32[] = reduce_sum[axes=(0,)] h     j:f32[] = div 1.0 i     k:f32[2] = broadcast_in_dim[broadcast_dimensions=() shape=(2,)] j     l:f32[2] = mul k h     m:f32[2] = reduce_sum[axes=()] l     n:f32[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] m     o:f32[2,3] = dynamic_update_slice a n 0 0   in (o,) } ```",2022-05-12T19:45:49Z,,closed,2,8,https://github.com/jax-ml/jax/issues/10682,"Related discussion around the inefficiency of differentiating gathers:  https://github.com/google/jax/issues/6872 On the whole I think this sort of change is probably a good idea, given the major potential performance gains. That said, I appreciate the simplicity of lowering indexing to a single XLA operation. I have not thought about the relative merits of lowering to `lax.dynamic_slice` vs `lax.slice`.","I would need to hit:  numpy indexing  .at[] indexing + ops  autobatching rules (I’ve noted a dynamic slice vmaps to a gather  will show jaxpr later) While I like the idea of ‘fixing’ later on, I think just hitting these convenience cases like x[0] would get 99% of cases so we get pretty code and optimal performance.   I will add a check if concrete array to catch slice opportunities where available over dynamic_slice.","Could this be done in the MLIR lowering? That would cover the use cases you want, I believe.  ",I don't remember for sure but I think there are cases that are hard to turn from general Gathers back to DynamicSlices once they've been lowered from NumPystyle indexing to (M)HLO Gather (and similar for scatters). So +1 to adding something like this optimization! Maybe it wouldn't be hard to handle tuple of ints and slices too?,"> I don't remember for sure but I think there are cases that are hard to turn from general Gathers back to DynamicSlices once they've been lowered from NumPystyle indexing to (M)HLO Gather (and similar for scatters). So +1 to adding something like this optimization! Maybe it wouldn't be hard to handle tuple of ints and slices too? Currently handles x[:,None,3:5,2] I think it handles reversing the dim order because the outer of the parser deals with it. I haven’t looked at strides. Slice supports strides, but dynamic slice does not.  I think x[(0,1)] already gets converted by the handy functions around into x[0,1] by the time the code sees it, cases when someone does x[((0,1),)] however are not. Thank you for the speedy responses, it really helps to know it’s of interest, I will work on it further.","The jax2tf lowering issue is the most compelling reason I can see for doing this change. For compilation via XLA, my argument has always been ""XLA can do this"" and we should optimize for simpler Python code. But I don't feel that strongly about it."," Abbreviated view Gather/Scatter may have poor support/poor implementations on some hardware backends/converters e.g. jax2tf no XLA which is commonly used to export to ONNX or tflite.  Looking at some ways Jax can be used:  This PR reimplements the scatter/gather passes at the jaxpr level. This is not a good general idea as in some cases it will backfire and lead to poor performance, but would help jax2tf. As for tflite, tflite.experimental_from_jax should just apply the optimisation pass to simplify scatters (I will add this as a separate issue).  I think this pull request should only apply to jax2tf (I'll make the necessary code changes) and only when `enable_xla=False` is enabled. This flag suggests the exit point will not be XLA compiled and hence these simplifications improve support and probably performance.","This is now a companion PR request to CC([jax2tf] add rudimentary scatter support for enable_xla=False). But instead of mapping scatter to tfscatter ops as the other PR does, this one expands them into slices.  It covers a subset of the cases that CC([jax2tf] add rudimentary scatter support for enable_xla=False) does so I'm aiming to land both. I've also placed the expanded gather, which may be replicating some of the rules already implemented."
542,"以下是一个github上的jax下的一个issue, 标题是(Make CUDA install instructions copy-pasteablee)， 内容是 (The command after the pip command always annoys me because (at least on zsh) leads to an error if I copypaste that line... ``` ➜ pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html  ciao ERROR: Invalid requirement: '' ``` This PR simply moves the comment on the line above.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Make CUDA install instructions copy-pasteablee,"The command after the pip command always annoys me because (at least on zsh) leads to an error if I copypaste that line... ``` ➜ pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html  ciao ERROR: Invalid requirement: '' ``` This PR simply moves the comment on the line above.",2022-05-11T15:57:56Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10664
1248,"以下是一个github上的jax下的一个issue, 标题是(DeviceArray: Improve support for copy, deepcopy, and pickle)， 内容是 (Currently deepcopying or pickling of jax DeviceArrays is implemented by forwarding `arr.__reduce__` to the ndarray value, meaning that the copied/unpickled result is a normal numpy array, and calling `copy.copy` is a noop. This change implements custom `__copy__`, `__deepcopy__`, and `__reduce__` methods in order to properly copy and/or serialize DeviceArray objects in both traced and untraced contexts. For `__reduce__` (i.e. pickle), deserialization is done on the default device – I'm not sure whether it would make sense to try to persist the device; in any case it's not straightforward because `jaxlib.xla_extension.Device` is not serializable. Further, because deserialization may happen in a different runtime than the one where serialization took place, it's not clear how the nondefault device would be identified. Addresses CC(deepcopy of jax.numpy array generates regular numpy array) Note that pickling of bfloat16 arrays is still broken due to CC(bfloat16 arrays are incompatible with pickle))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"DeviceArray: Improve support for copy, deepcopy, and pickle","Currently deepcopying or pickling of jax DeviceArrays is implemented by forwarding `arr.__reduce__` to the ndarray value, meaning that the copied/unpickled result is a normal numpy array, and calling `copy.copy` is a noop. This change implements custom `__copy__`, `__deepcopy__`, and `__reduce__` methods in order to properly copy and/or serialize DeviceArray objects in both traced and untraced contexts. For `__reduce__` (i.e. pickle), deserialization is done on the default device – I'm not sure whether it would make sense to try to persist the device; in any case it's not straightforward because `jaxlib.xla_extension.Device` is not serializable. Further, because deserialization may happen in a different runtime than the one where serialization took place, it's not clear how the nondefault device would be identified. Addresses CC(deepcopy of jax.numpy array generates regular numpy array) Note that pickling of bfloat16 arrays is still broken due to CC(bfloat16 arrays are incompatible with pickle)",2022-05-10T22:36:34Z,pull ready,closed,0,4,https://github.com/jax-ml/jax/issues/10659,pulling in for more testing  not sure whether there will be unintended sideeffects.,Turns out there are a number of downstream users implicitly relying on pickling and/or deepcopy converting device arrays to numpy arrays. Perhaps we can start with a deprecation warning.,I'm working on fixing internal breakages; probably will not be able to merge this for a week or two. But requesting review new because I think this will be close to the final state of the PR. Thanks!,Ready for a review: PTAL
327,"以下是一个github上的jax下的一个issue, 标题是([linalg] Extend svd test coverage to input of zero and near-zero elements.)， 内容是 ([linalg] Extend svd test coverage to input of zero and nearzero elements.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[linalg] Extend svd test coverage to input of zero and near-zero elements.,[linalg] Extend svd test coverage to input of zero and nearzero elements.,2022-05-10T06:04:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/10644
1520,"以下是一个github上的jax下的一个issue, 标题是(minor display issue in rendered docs)， 内容是 (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Split the `print(...)` statements into different cells in the notebooks so that they're better rendered in .md. If not split, they are usually rendered on separate cells. For example, the following cell ```python  Set a step size for finite differences calculations eps = 1e4  Check b_grad with scalar finite differences b_grad_numerical = (loss(W, b + eps / 2.)  loss(W, b  eps / 2.)) / eps print('b_grad_numerical', b_grad_numerical) print('b_grad_autodiff', grad(loss, 1)(W, b))  Check W_grad with finite differences in a random direction key, subkey = random.split(key) vec = random.normal(subkey, W.shape) unitvec = vec / jnp.sqrt(jnp.vdot(vec, vec)) W_grad_numerical = (loss(W + eps / 2. * unitvec, b)  loss(W  eps / 2. * unitvec, b)) / eps print('W_dirderiv_numerical', W_grad_numerical) print('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec)) ``` gets rendered like ``` b_grad_numerical ``` ```  0.29325485 b_grad_autodiff 0.29227245 ``` ``` W_dirderiv_numerical 0.2002716 W_dirderiv_autodiff 0.19909117 ``` It is cumbersome to separate the print calls, but without a better fix in the underlying rendering, this is likely the best solution for now... )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,minor display issue in rendered docs,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Split the `print(...)` statements into different cells in the notebooks so that they're better rendered in .md. If not split, they are usually rendered on separate cells. For example, the following cell ```python  Set a step size for finite differences calculations eps = 1e4  Check b_grad with scalar finite differences b_grad_numerical = (loss(W, b + eps / 2.)  loss(W, b  eps / 2.)) / eps print('b_grad_numerical', b_grad_numerical) print('b_grad_autodiff', grad(loss, 1)(W, b))  Check W_grad with finite differences in a random direction key, subkey = random.split(key) vec = random.normal(subkey, W.shape) unitvec = vec / jnp.sqrt(jnp.vdot(vec, vec)) W_grad_numerical = (loss(W + eps / 2. * unitvec, b)  loss(W  eps / 2. * unitvec, b)) / eps print('W_dirderiv_numerical', W_grad_numerical) print('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec)) ``` gets rendered like ``` b_grad_numerical ``` ```  0.29325485 b_grad_autodiff 0.29227245 ``` ``` W_dirderiv_numerical 0.2002716 W_dirderiv_autodiff 0.19909117 ``` It is cumbersome to separate the print calls, but without a better fix in the underlying rendering, this is likely the best solution for now... ",2022-05-09T16:32:44Z,documentation contributions welcome P2 (eventual),closed,0,2,https://github.com/jax-ml/jax/issues/10633,"I first noticed this issue when I was trying to figure out this part of the docs: ```python  reverseoverreverse, only works for single arguments def hvp_revrev(f, primals, tangents):   x, = primals   v, = tangents   return grad(lambda x: jnp.vdot(grad(f)(x), v))(x) print(""Forward over reverse"") %timeit n10 r3 hvp(f, (X,), (V,)) print(""Reverse over forward"") %timeit n10 r3 hvp_revfwd(f, (X,), (V,)) print(""Reverse over reverse"") %timeit n10 r3 hvp_revrev(f, (X,), (V,)) print(""Naive full Hessian materialization"") %timeit n10 r3 jnp.tensordot(hessian(f)(X), V, 2) ``` ``` Forward over reverse 4.29 ms ± 153 µs per loop (mean ± std. dev. of 3 runs, 10 loops each) Reverse over forward ``` ``` 8.47 ms ± 4.63 ms per loop (mean ± std. dev. of 3 runs, 10 loops each) Reverse over reverse ``` ``` 12.5 ms ± 7.41 ms per loop (mean ± std. dev. of 3 runs, 10 loops each) Naive full Hessian materialization ``` ``` 47.2 ms ± 546 µs per loop (mean ± std. dev. of 3 runs, 10 loops each) ```","beautifully addressed by  in CC(DOC: fix docs generator to group notebook code output), closing :) "
3505,"以下是一个github上的jax下的一个issue, 标题是(PR 10453 breaks negative-valued `argnums` in `jax.grad)， 内容是 (https://github.com/google/jax/pull/10453 breaks the ability to use negativevalues (a la negative indexing) for the argnums arg in `jax.grad`, resulting in a TypeError about positional arguments. Here is a minimal repro (yay!) that mirrors the way I use this in practice. ```python import jax def f(x, y):   return x.sum() * y.sum() g = jax.grad(f, argnums=1) x = jax.random.normal(jax.random.PRNGKey(0), (16, 16)) y = jax.random.normal(jax.random.PRNGKey(1), (16, 16)) g(x, y) ``` yields this stack trace: ```  UnfilteredStackTrace                      Traceback (most recent call last) []() in ()       8 y = jax.random.normal(jax.random.PRNGKey(1), (16, 16)) > 9 g(x, y) 10 frames [jax/_src/traceback_util.py]() in reraise_with_filtered_traceback(*args, **kwargs)     161     try: > 162       return fun(*args, **kwargs)     163     except Exception as e: [jax/_src/api.py]() in grad_f(*args, **kwargs)     904   def grad_f(*args, **kwargs): > 905     _, g = value_and_grad_f(*args, **kwargs)     906     return g [jax/_src/traceback_util.py]() in reraise_with_filtered_traceback(*args, **kwargs)     161     try: > 162       return fun(*args, **kwargs)     163     except Exception as e: [jax/_src/api.py]() in value_and_grad_f(*args, **kwargs)     980     if not has_aux: > 981       ans, vjp_py = _vjp(f_partial, *dyn_args, reduce_axes=reduce_axes)     982     else: [jax/_src/api.py]() in _vjp(fun, has_aux, reduce_axes, *primals)    2442     flat_fun, out_tree = flatten_fun_nokwargs(fun, in_tree) > 2443     out_primal, out_vjp = ad.vjp(    2444         flat_fun, primals_flat, reduce_axes=reduce_axes) [jax/interpreters/ad.py]() in vjp(traceable, primals, has_aux, reduce_axes)     129   if not has_aux: > 130     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)     131   else: [jax/interpreters/ad.py]() in linearize(traceable, *primals, **kwargs)     118   jvpfun_flat, out_tree = flatten_fun(jvpfun, in_tree) > 119   jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)     120   out_primals_pvals, out_tangents_pvals = tree_unflatten(out_tree(), out_pvals) [/jax/_src/profiler.py]() in wrapper(*args, **kwargs)     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper [jax/interpreters/partial_eval.py]() in trace_to_jaxpr_nounits(fun, pvals, instantiate)     607     fun = trace_to_subjaxpr_nounits(fun, main, instantiate) > 608     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)     609     assert not env [jax/linear_util.py]() in call_wrapped(self, *args, **kwargs)     167     try: > 168       ans = self.f(*args, **dict(self.params, **kwargs))     169     except: UnfilteredStackTrace: TypeError: f() takes 2 positional arguments but 3 were given The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last) []() in ()       7 x = jax.random.normal(jax.random.PRNGKey(0), (16, 16))       8 y = jax.random.normal(jax.random.PRNGKey(1), (16, 16)) > 9 g(x, y) **TypeError: f() takes 2 positional arguments but 3 were given** ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,PR 10453 breaks negative-valued `argnums` in `jax.grad,"https://github.com/google/jax/pull/10453 breaks the ability to use negativevalues (a la negative indexing) for the argnums arg in `jax.grad`, resulting in a TypeError about positional arguments. Here is a minimal repro (yay!) that mirrors the way I use this in practice. ```python import jax def f(x, y):   return x.sum() * y.sum() g = jax.grad(f, argnums=1) x = jax.random.normal(jax.random.PRNGKey(0), (16, 16)) y = jax.random.normal(jax.random.PRNGKey(1), (16, 16)) g(x, y) ``` yields this stack trace: ```  UnfilteredStackTrace                      Traceback (most recent call last) []() in ()       8 y = jax.random.normal(jax.random.PRNGKey(1), (16, 16)) > 9 g(x, y) 10 frames [jax/_src/traceback_util.py]() in reraise_with_filtered_traceback(*args, **kwargs)     161     try: > 162       return fun(*args, **kwargs)     163     except Exception as e: [jax/_src/api.py]() in grad_f(*args, **kwargs)     904   def grad_f(*args, **kwargs): > 905     _, g = value_and_grad_f(*args, **kwargs)     906     return g [jax/_src/traceback_util.py]() in reraise_with_filtered_traceback(*args, **kwargs)     161     try: > 162       return fun(*args, **kwargs)     163     except Exception as e: [jax/_src/api.py]() in value_and_grad_f(*args, **kwargs)     980     if not has_aux: > 981       ans, vjp_py = _vjp(f_partial, *dyn_args, reduce_axes=reduce_axes)     982     else: [jax/_src/api.py]() in _vjp(fun, has_aux, reduce_axes, *primals)    2442     flat_fun, out_tree = flatten_fun_nokwargs(fun, in_tree) > 2443     out_primal, out_vjp = ad.vjp(    2444         flat_fun, primals_flat, reduce_axes=reduce_axes) [jax/interpreters/ad.py]() in vjp(traceable, primals, has_aux, reduce_axes)     129   if not has_aux: > 130     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)     131   else: [jax/interpreters/ad.py]() in linearize(traceable, *primals, **kwargs)     118   jvpfun_flat, out_tree = flatten_fun(jvpfun, in_tree) > 119   jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)     120   out_primals_pvals, out_tangents_pvals = tree_unflatten(out_tree(), out_pvals) [/jax/_src/profiler.py]() in wrapper(*args, **kwargs)     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper [jax/interpreters/partial_eval.py]() in trace_to_jaxpr_nounits(fun, pvals, instantiate)     607     fun = trace_to_subjaxpr_nounits(fun, main, instantiate) > 608     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)     609     assert not env [jax/linear_util.py]() in call_wrapped(self, *args, **kwargs)     167     try: > 168       ans = self.f(*args, **dict(self.params, **kwargs))     169     except: UnfilteredStackTrace: TypeError: f() takes 2 positional arguments but 3 were given The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last) []() in ()       7 x = jax.random.normal(jax.random.PRNGKey(0), (16, 16))       8 y = jax.random.normal(jax.random.PRNGKey(1), (16, 16)) > 9 g(x, y) **TypeError: f() takes 2 positional arguments but 3 were given** ```",2022-05-09T14:54:48Z,bug better_errors,closed,0,4,https://github.com/jax-ml/jax/issues/10630,"Thanks for the report – I'm not sure negative indices for argnums were ever intentionally supported; at least, we never mention this in the docs or cover it in tests (let me know if I'm mistaken on that) My inclination here is to ""fix"" this by adding assertions that `argnums` must be positive – what do you think? Paging  for input here."," I'm working on argument 'annotation' validation for `jax.jit` now (with the intention of expanding to other places where argument annotation is used later, including `grad`).  has kindly provided feedback over in CC(Additional input validation for transformations)  I started a discussion issue CC([Proposal] Consistent `argnums` and `argnames` parameters for transformations) that outlines how I think the annotation feature could be improved and made more consistent across different functions. If there is appetite for negative `argnum` support, I'd be happy to add that into the work I hope to do as part of CC([Proposal] Consistent `argnums` and `argnames` parameters for transformations) ","I assigned Matt, who authored CC([removeunits] remove units from argnums/argnames_partial), so that we have an assignee. Additionally it seems that  may also be interested in contributing here as well!","This was fixed by  in CC(fix grad(..., argnums=1), regressed in 10453)"
301,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Update JAX limitations)， 内容是 (JAX has made progress in coverage of primitives on TPU. This PR updates those limitations.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Update JAX limitations,JAX has made progress in coverage of primitives on TPU. This PR updates those limitations.,2022-05-09T12:47:31Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10628
4461,"以下是一个github上的jax下的一个issue, 标题是([Proposal] Consistent `argnums` and `argnames` parameters for transformations)， 内容是 (Hey JAX team, I have been trying to wrap my head around 'argument annotation` in JAX for a bit in the hopes of finding a more intuitive/consistent implementation, which has lead me to the big block of text below. I would be super keen to hear your thoughts as I try to dive deeper into the inner workings of JAX. Lately there have been a number of issues requesting improvements to `*_argnums` and `*_argnames` parameters used in transformations in addition to other ergonomics improvements related to declaring which function arguments should be annotated with a given property. I figured it might be helpful to make an overarching issue with the end goal of having a consistent, ergonomic way of specifying these parameters. Managing argument 'annotations' in transformations has definitely been one of the more frustrating experiences of learning JAX (which is otherwise entirely amazing, of course) Related issues:  CC(Add `jax.jit(donate_argnames=...)`)  CC(`jit` input validation can lead to silently dynamic variables) ( CC(Additional input validation for transformations))  CC(Add static_argnames to pjit and lower)  CC([Proposal] Define static arguments using typing annotations)   CC(static_broadcasted_argnames for pmap?)  CC(make_jaxpr does not interact well with static arguments)  CC(kwargs sometimes cannot work well with jit)  CC(xmap doesn't preserve static argnums)  `jax.jit` correctly implements `static_argnames` even for cases with keywordonly arguments, which would suggest that it should be possible to add `argnames` equivalents to any function that currently only implements `argnums`. An easier but less robust fix could be to map `argnames` to `argnums` using `inspect` (see discussion: CC(kwargs sometimes cannot work well with jit)). This would likely not work for keywordonly arguments (though it might for things like `donate_arg...`?)  Current shortcomings Currently even the most robust implementation of the 'argument annotation' mechanism behaves in a somewhat counterintuitive way (although this is suggested in the fine print of the docstring, if one reads it with sufficient care): ```python def f(a, /, b, *, c):     print(a, b, c) jf = jit(f, static_argnames=(""a"", ""b"", ""c"")) jf(1, 2, c=3) > Tracedwith 2 3  Expected: 1 2 3 jf2 = jit(f, static_argnames=(""b"", ""c""), static_argnums=(0,)) jf2(1, 2, c=3) > 1 Tracedwith 3  Expected: 1 2 3 jf2(1, b=2, c=3) > 1 2 3  As expected jf3 = jit(f, static_argnums=(0, 1, 2)) jf3(1, 2, c=3) > 1 2 Tracedwith  Expected: 1 2 3 jf3(1, b=2, c=3) > 1 2 Tracedwith  Expected: 1 2 3 ``` The fact that we have one instance where we are able to get the expected result gives hope that a solution should be possible by inspecting the function and arguments and modifying `static_argnums` and `static_argnames` accordingly – or perhaps a better solution exists? Ideally we would want to avoid inspecting the arguments at calltime. I have started toying with validation of `static_argnums` and `static_argnames` in CC(Additional input validation for transformations)  Goals My suggestion would be that a solution that fixes the inconsistencies above (or in the worst case documents them thoroughly) is found for `jax.jit`. Once that is done, it would be great to see `*_argnames` and keywordarg support added to other functions:  `jax.experiment.pjit`  `jax.pmap`  `jax.value_and_grad`  `jax.custom_vjp`  `jax.custom_jvp`  `jax.hessian`  `jax.jacrev`  `jax.jacfwd`  `jax.grad` Additionally CC([Proposal] Define static arguments using typing annotations) can be explored (could live in `jax.experimental.annotations`, if there is any interest for this feature at all)  Progress  [ ] Get feedback and decide on: (this issue)     [ ] Interface (potential changes in function signatures for argument annotations)     [ ] Behaviour   [ ] Document interface and behaviour (initial PR: CC([WIP] Document argument annotations))  [ ] Make tests and ensure consistency for functions    [ ] `jax.jit` (PR: CC(Better `argnums`/`argnames` inference))    [ ] `jax.experiment.pjit`    [ ] `jax.pmap`    [ ] `jax.value_and_grad`    [ ] `jax.custom_vjp`    [ ] `jax.custom_jvp`    [ ] `jax.hessian`    [ ] `jax.jacrev`    [ ] `jax.jacfwd`    [ ] `jax.grad`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Proposal] Consistent `argnums` and `argnames` parameters for transformations,"Hey JAX team, I have been trying to wrap my head around 'argument annotation` in JAX for a bit in the hopes of finding a more intuitive/consistent implementation, which has lead me to the big block of text below. I would be super keen to hear your thoughts as I try to dive deeper into the inner workings of JAX. Lately there have been a number of issues requesting improvements to `*_argnums` and `*_argnames` parameters used in transformations in addition to other ergonomics improvements related to declaring which function arguments should be annotated with a given property. I figured it might be helpful to make an overarching issue with the end goal of having a consistent, ergonomic way of specifying these parameters. Managing argument 'annotations' in transformations has definitely been one of the more frustrating experiences of learning JAX (which is otherwise entirely amazing, of course) Related issues:  CC(Add `jax.jit(donate_argnames=...)`)  CC(`jit` input validation can lead to silently dynamic variables) ( CC(Additional input validation for transformations))  CC(Add static_argnames to pjit and lower)  CC([Proposal] Define static arguments using typing annotations)   CC(static_broadcasted_argnames for pmap?)  CC(make_jaxpr does not interact well with static arguments)  CC(kwargs sometimes cannot work well with jit)  CC(xmap doesn't preserve static argnums)  `jax.jit` correctly implements `static_argnames` even for cases with keywordonly arguments, which would suggest that it should be possible to add `argnames` equivalents to any function that currently only implements `argnums`. An easier but less robust fix could be to map `argnames` to `argnums` using `inspect` (see discussion: CC(kwargs sometimes cannot work well with jit)). This would likely not work for keywordonly arguments (though it might for things like `donate_arg...`?)  Current shortcomings Currently even the most robust implementation of the 'argument annotation' mechanism behaves in a somewhat counterintuitive way (although this is suggested in the fine print of the docstring, if one reads it with sufficient care): ```python def f(a, /, b, *, c):     print(a, b, c) jf = jit(f, static_argnames=(""a"", ""b"", ""c"")) jf(1, 2, c=3) > Tracedwith 2 3  Expected: 1 2 3 jf2 = jit(f, static_argnames=(""b"", ""c""), static_argnums=(0,)) jf2(1, 2, c=3) > 1 Tracedwith 3  Expected: 1 2 3 jf2(1, b=2, c=3) > 1 2 3  As expected jf3 = jit(f, static_argnums=(0, 1, 2)) jf3(1, 2, c=3) > 1 2 Tracedwith  Expected: 1 2 3 jf3(1, b=2, c=3) > 1 2 Tracedwith  Expected: 1 2 3 ``` The fact that we have one instance where we are able to get the expected result gives hope that a solution should be possible by inspecting the function and arguments and modifying `static_argnums` and `static_argnames` accordingly – or perhaps a better solution exists? Ideally we would want to avoid inspecting the arguments at calltime. I have started toying with validation of `static_argnums` and `static_argnames` in CC(Additional input validation for transformations)  Goals My suggestion would be that a solution that fixes the inconsistencies above (or in the worst case documents them thoroughly) is found for `jax.jit`. Once that is done, it would be great to see `*_argnames` and keywordarg support added to other functions:  `jax.experiment.pjit`  `jax.pmap`  `jax.value_and_grad`  `jax.custom_vjp`  `jax.custom_jvp`  `jax.hessian`  `jax.jacrev`  `jax.jacfwd`  `jax.grad` Additionally CC([Proposal] Define static arguments using typing annotations) can be explored (could live in `jax.experimental.annotations`, if there is any interest for this feature at all)  Progress  [ ] Get feedback and decide on: (this issue)     [ ] Interface (potential changes in function signatures for argument annotations)     [ ] Behaviour   [ ] Document interface and behaviour (initial PR: CC([WIP] Document argument annotations))  [ ] Make tests and ensure consistency for functions    [ ] `jax.jit` (PR: CC(Better `argnums`/`argnames` inference))    [ ] `jax.experiment.pjit`    [ ] `jax.pmap`    [ ] `jax.value_and_grad`    [ ] `jax.custom_vjp`    [ ] `jax.custom_jvp`    [ ] `jax.hessian`    [ ] `jax.jacrev`    [ ] `jax.jacfwd`    [ ] `jax.grad`",2022-05-06T23:43:43Z,enhancement,open,4,20,https://github.com/jax-ml/jax/issues/10614,"This is totally doable using `inspect.signature`. For example: ```python sig = inspect.signature(f) sig = ...   replace all defaults with False; elided for space static_args = tuple(True if i in static_argnums else False for i in range(static_argnums)) static_kwargs = {k: True for k in static_argnames} bound = sig.bind_partial(*static_args, **static_kwargs) bound.apply_defaults() static_args = bound.static_args static_kwargs = bound.static_kwargs ``` which canonicalises args and kwargs based on the signature of the function. Then if necessary do the same thing to the actual `args` and `kwargs` passed at runtime, and match them up. (This is exactly how Equinox handles `filter_{jit,grad,vmap,pmap,value_and_grad}`.)","kidger I think a native JAX solution might even be able to just lean on the existing `argnames_partial`/`argnums_partial` functions. This way the only changes needed would be to generate the extra entries in `argnums` and `argnames` at the beginning of methods like `jit`, `grad`, ... The lookup logic would then also work for `nodiff` and `donate` type arguments.","From CC(fix grad(..., argnums=1), regressed in 10453) it appears that negative `argnums` should also be made valid for `static_argnums`, in which case I think that convention should be carried across to `nodiff`, `argnums`, etc. This will require some addition work on CC(Additional input validation for transformations). Could  weigh in?","Supporting negative indices everywhere SGTM. (Sorry, haven't had time to reply to the broader proposal in more detail...)","kidger It seems that your example has some typos, and `bind_partial` need `static_args` and `static_kwargs` doesn't overlap, so `static_args = tuple(True if i in static_argnums else False for i in range(static_argnums))` seems wrong(and `range(static_argnums)` should be a typo I think). I'll have a look at Equinox.  Okay it seems that Equinox have a different API interface, so it doesn't have this problem.","If `static_args` and `static_kwargs` overlap then that's a user error, analogous to `def f(a) ... f(1, a=2)`. Indeed it should be `range(len(static_argnums)). This was typed out without testing. If in doubt use the Equinox version, that definitely works ;) Equinox provides a superset of the interface being considered here  it also handles mapping over PyTrees, filter functions, auxiliary outputs, etc.","kidger It is not a user error since `static_args = tuple(True if i in static_argnums else False for i in range(static_argnums))` generate a full set args. And I doubt that it should be `range(max(static_argnums))`. And `static_args, static_kwargs = bound.args, bound.kwargs` doesn't meet the need of JAX, since `bound.kwargs` only contains keyworld only arguments and var kwargs.","Ah! I see what you're saying. Sorry, yes, being a bit slow today. JAX uses an indexbased way of selecting arguments and I was thinking of a maskbased way. The basic principle holds, but you're right that the parsing would be a bit more involved.","I have tried to implement it leveraging `bind_partial`, and finally I found that it might be better to manually parse it using `inspect`, similar to what JAX currently do.","My trial: ```python neg_argnums = tuple(argnum for argnum in argnums if argnum = 0) sentinel = object() args = tuple(None if i in argnums_set else sentinel for i in range(max(argnums_set))) kwargs = {k: None for k in argnames} sig = inspect.signature(fun) ba = inspect.BoundArguments(sig, sig.bind_partial(*args).arguments | sig.bind_partial(**kwargs)) args = ba.args kwargs = ba.kwargs  JAX need POSITIONAL_OR_KEYWORD, KEYWORD_ONLY and VAR_KEYWORD  but ba.kwargs only contains KEYWORD_ONLY and VAR_KEYWORD ```","Idea  Interface discussion: Don't use `*_argnums` and `*_argnames` at all, just a `*_args` parameter of type `Sequence[str | int]` where integers are taken as positions and strings are taken as argument names. This is not only more succinct, but also allows us to maintain full backwards compatibility: `argnums` and `argnames` would continue to work as they currently do, but would give rise to a deprecation warning for a few versions before being removed (potentially until JAX 1.0, but preferably sooner). Using the container class approach as proposed (proposal not finished) in CC([WIP, Proposal] Improve argument annotations interface) would enable relatively painless support of `argnames`+`argnums` and `args` for the period of deprecation.", I would love to hear some more thoughts from the JAX team. Could I get you to ping people that might be interested in this?  was really helpful in getting CC(Additional input validation for transformations) merged.,"  I just wanted to flag this issue again, it would be great to make progress on this to improve JAX usability. The approach of just having e.g. `static_args` that takes `Sequence[str | int]` seems nice, regardless of whether it's implemented via https://github.com/google/jax/pull/10746 or something easier. Background: The current behavior in JAX is somewhat broken, where static_argnums cannot be passed as kwargs and static_argnames cannot be passed positionally. Moreover, counting argnums has a good amount of mental overhead, especially when using function transformations that changes the function signature, whereas argnames isn't supported everywhere, e.g. in pmap."," Actually if you only pass `static_argnums` or only pass `static_argnames`, static arguments can be passed as kwargs and can be passed positionally. However, if you pass `static_argnums` and `static_argnames`, the behavior is what you describe. I try to unify this behavior in CC(normalize/rearrange (kw)args by signature in jit) "," Thank you for highlighting this. Even after having spent a good bit of time with this particular part of the JAX source, the behaviour still manages to confuse me from time to time. CC([WIP, Proposal] Improve argument annotations interface) is intended as more of a rough sketch, but I think having an immutable dataclass object and passing that around might be a good option. Having had a very cursory look at the code, I think in most places I would be able to figure out how to expand the code to accept named arguments as well (many places have `argnums` but no equivalent `argnames` parameter).",(I just wanted to note that looking at this is still on my radar but I haven't had time to do so between travel and other higher priorities. Sorry for the delay.),"> Actually if you only pass static_argnums or only pass static_argnames, static arguments can be passed as kwargs and can be passed positionally. Is this true for all functions? It's not what I've been seeing with jit or pmap. Another failure case right now is d default arguments. If I mark an input with default value as static, it raises an error if the value isn't pissed in. This should all be pretty easy to do with the `inspect` module by canonicalizing the function signature.",I second the proposal of https://github.com/google/jax/issues/10614issuecomment1132238808 and https://github.com/google/jax/issues/10614issuecomment1145151133 to use `*_args : Sequence[int | str]`. Treat each element as an argument number if it's an `int` and argument name if it's a `str`.,"Any update on this ? Not being able to use `kwargs` in `jax.grad` makes the API very brittle (very easy to get arguments order wrong, especially when updating the function signature) and force bad programming practices.","`jax.grad` does support passing through keyword arguments  just not as a differentiated argument. Generally speaking I'd recommend against using `jax.grad(argnums=...)`  instead just pack all of your differentiated quantities into a tuple that is passed through the first argument. This helps to avoid the issue you're describing. In any case, I don't believe there are any plans to change the API for `jax.grad`. "
5022,"以下是一个github上的jax下的一个issue, 标题是(Importing `jax.numpy` leads to Partially Initialised module error )， 内容是 (Please:  [x] Check for duplicate issues. I am running `JAX` on a `Fedora 35` system, with `CUDA 11.6`, `CuDNN 8.2`, `Driver version 510.60.02` I installed `CuDNN` based on the `RHEL8` instructions [here, since `Fedora 35` doesn't seem to officially get the builds for it] I installed `JAX` by following the instructions present in the README.md file. On doing that, and simply running the import command, I get the following error ```python import jax.numpy as jnp ``` ```python AttributeError                            Traceback (most recent call last) Input In [8], in ()       2 import nibabel as nib       3 import numpy as np  > 4 import jax.numpy as jnp       6 class NIIHandler():       8     def __init__(self, TRAIN_DATASET_PATH): File ~/.local/lib/python3.10/sitepackages/jax/__init__.py:58, in       38 from jax._src.config import (      39   config as config,      40   enable_checks as enable_checks,    (...)      55   transfer_guard_device_to_host as transfer_guard_device_to_host,      56 )      57 from .core import eval_context as ensure_compile_time_eval > 58 from jax._src.api import (      59   ad,   TODO(phawkins): update users to avoid this.      60   block_until_ready,      61   checkpoint as checkpoint,      62   checkpoint_policies as checkpoint_policies,      63   closure_convert as closure_convert,      64   curry,   TODO(phawkins): update users to avoid this.      65   custom_gradient as custom_gradient,      66   custom_jvp as custom_jvp,      67   custom_vjp as custom_vjp,      68   default_backend as default_backend,      69   device_count as device_count,      70   device_get as device_get,      71   device_put as device_put,      72   device_put_sharded as device_put_sharded,      73   device_put_replicated as device_put_replicated,      74   devices as devices,      75   disable_jit as disable_jit,      76   eval_shape as eval_shape,      77   flatten_fun_nokwargs,   TODO(phawkins): update users to avoid this.      78   float0 as float0,      79   grad as grad,      80   hessian as hessian,      81   host_count as host_count,      82   host_id as host_id,      83   host_ids as host_ids,      84   jacobian as jacobian,      85   jacfwd as jacfwd,      86   jacrev as jacrev,      87   jit as jit,      88   jvp as jvp,      89   local_device_count as local_device_count,      90   local_devices as local_devices,      91   linearize as linearize,      92   linear_transpose as linear_transpose,      93   make_jaxpr as make_jaxpr,      94   mask as mask,      95   named_call as named_call,      96   pmap as pmap,      97   process_count as process_count,      98   process_index as process_index,      99   pxla,   TODO(phawkins): update users to avoid this.     100   remat as remat,     101   shapecheck as shapecheck,     102   ShapedArray as ShapedArray,     103   ShapeDtypeStruct as ShapeDtypeStruct,     104    TODO(phawkins): hide tree* functions from jax, update callers to use     105    jax.tree_util.     106   treedef_is_leaf,     107   tree_flatten,     108   tree_leaves,     109   tree_map,     110   tree_multimap,     111   tree_structure,     112   tree_transpose,     113   tree_unflatten,     114   value_and_grad as value_and_grad,     115   vjp as vjp,     116   vmap as vmap,     117   xla,   TODO(phawkins): update users to avoid this.     118   xla_computation as xla_computation,     119 )     120 from jax.experimental.maps import soft_pmap as soft_pmap     121 from jax.version import __version__ as __version__ File ~/.local/lib/python3.10/sitepackages/jax/_src/api.py:61, in       55 from jax._src import traceback_util      56 from jax._src.api_util import (      57     flatten_fun, apply_flat_fun, flatten_fun_nokwargs, flatten_fun_nokwargs2,      58     argnums_partial, argnums_partial_except, flatten_axes, donation_vector,      59     rebase_donate_argnums, _ensure_index, _ensure_index_tuple,      60     shaped_abstractify, _ensure_str_tuple, argnames_partial_except) > 61 from jax._src.lax import lax as lax_internal      62 from jax._src.lib import jax_jit      63 from jax._src.lib import xla_bridge as xb File ~/.local/lib/python3.10/sitepackages/jax/_src/lax/lax.py:1653, in     1651 tan_p = standard_unop(_float | _complex, 'tan')    1652 ad.defjvp2(tan_p, lambda g, ans, x: mul(g, _const(x, 1) + square(ans))) > 1653 if jax._src.lib.mlir_api_version >= 11:    1654   mlir.register_lowering(tan_p, partial(_nary_lower_mhlo, chlo.TanOp))    1655 else: AttributeError: partially initialized module 'jax' has no attribute '_src' (most likely due to a circular import) ``` I am extremely new to `JAX`, so please do let me know if there is something else I should be trying instead. Attaching my `nvidiasmi` and `nvcc  version` results below. !image !image Thank you very much!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Importing `jax.numpy` leads to Partially Initialised module error ,"Please:  [x] Check for duplicate issues. I am running `JAX` on a `Fedora 35` system, with `CUDA 11.6`, `CuDNN 8.2`, `Driver version 510.60.02` I installed `CuDNN` based on the `RHEL8` instructions [here, since `Fedora 35` doesn't seem to officially get the builds for it] I installed `JAX` by following the instructions present in the README.md file. On doing that, and simply running the import command, I get the following error ```python import jax.numpy as jnp ``` ```python AttributeError                            Traceback (most recent call last) Input In [8], in ()       2 import nibabel as nib       3 import numpy as np  > 4 import jax.numpy as jnp       6 class NIIHandler():       8     def __init__(self, TRAIN_DATASET_PATH): File ~/.local/lib/python3.10/sitepackages/jax/__init__.py:58, in       38 from jax._src.config import (      39   config as config,      40   enable_checks as enable_checks,    (...)      55   transfer_guard_device_to_host as transfer_guard_device_to_host,      56 )      57 from .core import eval_context as ensure_compile_time_eval > 58 from jax._src.api import (      59   ad,   TODO(phawkins): update users to avoid this.      60   block_until_ready,      61   checkpoint as checkpoint,      62   checkpoint_policies as checkpoint_policies,      63   closure_convert as closure_convert,      64   curry,   TODO(phawkins): update users to avoid this.      65   custom_gradient as custom_gradient,      66   custom_jvp as custom_jvp,      67   custom_vjp as custom_vjp,      68   default_backend as default_backend,      69   device_count as device_count,      70   device_get as device_get,      71   device_put as device_put,      72   device_put_sharded as device_put_sharded,      73   device_put_replicated as device_put_replicated,      74   devices as devices,      75   disable_jit as disable_jit,      76   eval_shape as eval_shape,      77   flatten_fun_nokwargs,   TODO(phawkins): update users to avoid this.      78   float0 as float0,      79   grad as grad,      80   hessian as hessian,      81   host_count as host_count,      82   host_id as host_id,      83   host_ids as host_ids,      84   jacobian as jacobian,      85   jacfwd as jacfwd,      86   jacrev as jacrev,      87   jit as jit,      88   jvp as jvp,      89   local_device_count as local_device_count,      90   local_devices as local_devices,      91   linearize as linearize,      92   linear_transpose as linear_transpose,      93   make_jaxpr as make_jaxpr,      94   mask as mask,      95   named_call as named_call,      96   pmap as pmap,      97   process_count as process_count,      98   process_index as process_index,      99   pxla,   TODO(phawkins): update users to avoid this.     100   remat as remat,     101   shapecheck as shapecheck,     102   ShapedArray as ShapedArray,     103   ShapeDtypeStruct as ShapeDtypeStruct,     104    TODO(phawkins): hide tree* functions from jax, update callers to use     105    jax.tree_util.     106   treedef_is_leaf,     107   tree_flatten,     108   tree_leaves,     109   tree_map,     110   tree_multimap,     111   tree_structure,     112   tree_transpose,     113   tree_unflatten,     114   value_and_grad as value_and_grad,     115   vjp as vjp,     116   vmap as vmap,     117   xla,   TODO(phawkins): update users to avoid this.     118   xla_computation as xla_computation,     119 )     120 from jax.experimental.maps import soft_pmap as soft_pmap     121 from jax.version import __version__ as __version__ File ~/.local/lib/python3.10/sitepackages/jax/_src/api.py:61, in       55 from jax._src import traceback_util      56 from jax._src.api_util import (      57     flatten_fun, apply_flat_fun, flatten_fun_nokwargs, flatten_fun_nokwargs2,      58     argnums_partial, argnums_partial_except, flatten_axes, donation_vector,      59     rebase_donate_argnums, _ensure_index, _ensure_index_tuple,      60     shaped_abstractify, _ensure_str_tuple, argnames_partial_except) > 61 from jax._src.lax import lax as lax_internal      62 from jax._src.lib import jax_jit      63 from jax._src.lib import xla_bridge as xb File ~/.local/lib/python3.10/sitepackages/jax/_src/lax/lax.py:1653, in     1651 tan_p = standard_unop(_float | _complex, 'tan')    1652 ad.defjvp2(tan_p, lambda g, ans, x: mul(g, _const(x, 1) + square(ans))) > 1653 if jax._src.lib.mlir_api_version >= 11:    1654   mlir.register_lowering(tan_p, partial(_nary_lower_mhlo, chlo.TanOp))    1655 else: AttributeError: partially initialized module 'jax' has no attribute '_src' (most likely due to a circular import) ``` I am extremely new to `JAX`, so please do let me know if there is something else I should be trying instead. Attaching my `nvidiasmi` and `nvcc  version` results below. !image !image Thank you very much!",2022-05-06T18:31:59Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/10605,"Thanks for the report  I suspect this has to do with version skew between the `jax` and `jaxlib` packages. Can you tell us which versions of jax and jaxlib you have installed? For example, if you use `pip` to install packages you can run: ``` $ python m pip list | grep jax ``` using the appropriate Python executable.","Right, I tried that, this is the result I get !image","Thanks for the additional information. I was not able to reproduce this on an Ubuntu system with jax 0.3.10 and jaxlib 0.3.10, running Python 3.10. Can you try to isolate this error further (your traceback shows that this import is happening within the context of other code – perhaps some interaction between packages is at fault?)? Without a local reproduction, it's hard to debug.","Hi! I am in the same situation. I left my jax version attached !image I get that same error just entering the python command line and executing a jax import, like in the next screenshot !image","It looks like you have a local file in your directory named `jax.py`, so when you run `import jax` it is loading this file instead of the jax package. I'd suggest renaming this file; for example: ``` $ mv jax.py my_jax_script.py ```","> It looks like you have a local file in your directory named `jax.py`, so when you run `import jax` it is loading this file instead of the jax package. I'd suggest renaming this file; for example: >  > ``` > $ mv jax.py my_jax_script.py > ``` What a fail, you were totally right","Hi I got the same error when import jax, like the screenshot:    thank you very much for your advice!"
2716,"以下是一个github上的jax下的一个issue, 标题是(Additional input validation for transformations)， 内容是 (Adds additional input validation for `*_argnames` and `*_argnums` arguments to `jax.jit`. Previously `static_argnums` and `static_argnames` could easily lead to silently dynamic arguments. In fact, two such cases were found in the `jax` source code using the new test coverage (c477ce3). Additional test coverage has been added. Includes a small refactoring in `api.py` which introduces `_jit` to reduce code duplication (this leads to a number of small changes in various tests). The refactoring has the additional benefit of making experimentation with CC([Proposal] Define static arguments using typing annotations) easier. For discussion see issue: CC(`jit` input validation can lead to silently dynamic variables) Other places that could use additional validation (just add plumbing to validation logic introduced by this PR)  `jax.experiment.pjit`  `jax.pmap`  `jax.value_and_grad`  `jax.custom_vjp`  `jax.custom_jvp`  `jax.hessian`  `jax.jacrev`  `jax.jacfwd`  `jax.grad` I will add validation for the functions mentioned above in a separate PR once/if this one has been merged.  Handling of `*args` and `**kwargs` In cases where variable positional and/or keyword arguments are used, we (rightly) assume that the function will know how to deal with these and thus it is safe to assume that for `*args` any `argnum` is valid. Similarly for `**kwargs` any `argname` is valid. Importantly we still do some validation where possible. As an example, consider: ```python def f(a, /, b, *, c): ... jit(f, static_argnames=(""a"",))    This will fail since we know that `a` must be positional jit(f, static_argnums=(2,))   This will fail since `c` must be keyword ```  Additional improvements `argnums` could be patched to contain positionalonly arguments given as `static_argnames` using `inspect`. Currently: ```python def f(a, /, b, *, c):     print(a, b, c) ff = jit(f, static_argnames=(""a"", ""b"", ""c""))   with validation disabled ff(1, 2, c=3) > Tracedwith 2 3  Expected: 1 2 3 ff2 = jit(f, static_argnames=(""b"", ""c""), static_argnums=(0,)) ff2(1, 2, c=3) > 1 Tracedwith 3  Expected 1 2 3 ``` This is due to the way `_infer_argnums_and_argnames` works, which might be worthwhile to change in a separate PR. See further discussion in CC([Proposal] Consistent `argnums` and `argnames` parameters for transformations) Fix: CC(`jit` input validation can lead to silently dynamic variables) Fix: CC(Proposal  jax.jit static_argnums and static_argnames raises warning if parameter doesn't exist))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Additional input validation for transformations,"Adds additional input validation for `*_argnames` and `*_argnums` arguments to `jax.jit`. Previously `static_argnums` and `static_argnames` could easily lead to silently dynamic arguments. In fact, two such cases were found in the `jax` source code using the new test coverage (c477ce3). Additional test coverage has been added. Includes a small refactoring in `api.py` which introduces `_jit` to reduce code duplication (this leads to a number of small changes in various tests). The refactoring has the additional benefit of making experimentation with CC([Proposal] Define static arguments using typing annotations) easier. For discussion see issue: CC(`jit` input validation can lead to silently dynamic variables) Other places that could use additional validation (just add plumbing to validation logic introduced by this PR)  `jax.experiment.pjit`  `jax.pmap`  `jax.value_and_grad`  `jax.custom_vjp`  `jax.custom_jvp`  `jax.hessian`  `jax.jacrev`  `jax.jacfwd`  `jax.grad` I will add validation for the functions mentioned above in a separate PR once/if this one has been merged.  Handling of `*args` and `**kwargs` In cases where variable positional and/or keyword arguments are used, we (rightly) assume that the function will know how to deal with these and thus it is safe to assume that for `*args` any `argnum` is valid. Similarly for `**kwargs` any `argname` is valid. Importantly we still do some validation where possible. As an example, consider: ```python def f(a, /, b, *, c): ... jit(f, static_argnames=(""a"",))    This will fail since we know that `a` must be positional jit(f, static_argnums=(2,))   This will fail since `c` must be keyword ```  Additional improvements `argnums` could be patched to contain positionalonly arguments given as `static_argnames` using `inspect`. Currently: ```python def f(a, /, b, *, c):     print(a, b, c) ff = jit(f, static_argnames=(""a"", ""b"", ""c""))   with validation disabled ff(1, 2, c=3) > Tracedwith 2 3  Expected: 1 2 3 ff2 = jit(f, static_argnames=(""b"", ""c""), static_argnums=(0,)) ff2(1, 2, c=3) > 1 Tracedwith 3  Expected 1 2 3 ``` This is due to the way `_infer_argnums_and_argnames` works, which might be worthwhile to change in a separate PR. See further discussion in CC([Proposal] Consistent `argnums` and `argnames` parameters for transformations) Fix: CC(`jit` input validation can lead to silently dynamic variables) Fix: CC(Proposal  jax.jit static_argnums and static_argnames raises warning if parameter doesn't exist)",2022-05-06T17:27:06Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/10603," changes since last time are primarily support for negative `argnums` as per CC(fix grad(..., argnums=1), regressed in 10453)","an idea: If we do `arg` and `kwargs` inference using inspect tools following https://github.com/google/jax/issues/10614issuecomment1122688252 (there are some bugs in that comment), we might only need to catch error from bind? But this might need a breaking change...","> only If I understand correctly, that would fall outside the scope of this PR. Further, it only addresses cases where bind is used (static) and not other argument annotations (donate). This PR strictly deals with argument annotation validation at transformation time. I believe (but can't recall, and am traveling atm) that the binding process happens when the transformed function is called, not when the transformation is performed. I am working on another PR that builds on top of this one which goes a little further, but this PR aims to be compatible and uncontroversial. ","I don't mean that the bind for call, I mean that if we use `Signature.bind_partial` to inference `argnums` and `argnames`. Okay I find it is hard to use `Signature.bind_partial` in current JAX API interface...",Implemented the requested changes. Outstanding nit: https://github.com/google/jax/pull/10603pullrequestreview976051804 Let me know what you think ,Squashed to one commit and fixed a test. Ready for review/merge
533,"以下是一个github上的jax下的一个issue, 标题是(remat abstraction provenance)， 内容是 (Applying `remat` to a function can raise the abstraction level of its arguments. There's an easy recourse (e.g. use `partial`) but without a good error message it can be hard to figure out what's going wrong. In some cases we have good provenance information for abstraction error messages, but I think `remat` is lacking it.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,remat abstraction provenance,"Applying `remat` to a function can raise the abstraction level of its arguments. There's an easy recourse (e.g. use `partial`) but without a good error message it can be hard to figure out what's going wrong. In some cases we have good provenance information for abstraction error messages, but I think `remat` is lacking it.",2022-05-06T15:54:09Z,better_errors,closed,0,1,https://github.com/jax-ml/jax/issues/10602,"We fixed this with the new remat! ```python import jax .remat def f(x):   if x > 0: return 5.   return 2. f(3.) ``` ``` Traceback (most recent call last):   File ""/usr/local/google/home/mattjj/packages/jax/10602.py"", line 8, in      f(3.) jax.errors.ConcretizationTypeError: Attempted boolean conversion of traced array with shape bool[]. The error occurred while tracing the function f at /usr/local/google/home/mattjj/packages/jax/10602.py:3 for checkpoint. This concrete value was not available in Python because it depends on the value of the argument x. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerBoolConversionError Consider using the `static_argnums` parameter for `jax.remat` or `jax.checkpoint`. See the `jax.checkpoint` docstring and its example involving `static_argnums`: https://jax.readthedocs.io/en/latest/_autosummary/jax.checkpoint.html ```"
282,"以下是一个github上的jax下的一个issue, 标题是([mhlo] Add result type inference for mhlo.alltoall.)， 内容是 ([mhlo] Add result type inference for mhlo.alltoall.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[mhlo] Add result type inference for mhlo.alltoall.,[mhlo] Add result type inference for mhlo.alltoall.,2022-05-06T02:40:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/10597
1176,"以下是一个github上的jax下的一个issue, 标题是(tape.gradients returned in wrong order when using jax2tf+tf.Module)， 内容是 (Originally reported in deepmind/dmhaiku CC(fix lax_numpy array/asarray performance bug). A minimal reproducer can be found in this Colab: https://colab.research.google.com/gist/tomhennigan/144e75ca79627cfd0edd98113a90e1c0/jax2tf.ipynb. I've found that you can ""fix"" the issue by removing `tf.Module` as the base class of `JaxModule`. My hypothesis for why this fixes things: `tf.Module` inherits from `Checkpointable`. This means that (despite what the code makes it look like) `self._params` will not actually be a `dict` instance, it will be some `DictWrapper` thing that Checkpointable uses to track mutations. I suspect the issue is that somewhere in JAX or TF we are relying on the iteration order of dicts and it differs between `dict` and `DictWrapper` (not 100% sure on the name). This wasn't previously an issue (the notebook ran when it was checked in in Nov 2020) so I am not sure if something has changed in JAX or TF.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,tape.gradients returned in wrong order when using jax2tf+tf.Module,"Originally reported in deepmind/dmhaiku CC(fix lax_numpy array/asarray performance bug). A minimal reproducer can be found in this Colab: https://colab.research.google.com/gist/tomhennigan/144e75ca79627cfd0edd98113a90e1c0/jax2tf.ipynb. I've found that you can ""fix"" the issue by removing `tf.Module` as the base class of `JaxModule`. My hypothesis for why this fixes things: `tf.Module` inherits from `Checkpointable`. This means that (despite what the code makes it look like) `self._params` will not actually be a `dict` instance, it will be some `DictWrapper` thing that Checkpointable uses to track mutations. I suspect the issue is that somewhere in JAX or TF we are relying on the iteration order of dicts and it differs between `dict` and `DictWrapper` (not 100% sure on the name). This wasn't previously an issue (the notebook ran when it was checked in in Nov 2020) so I am not sure if something has changed in JAX or TF.",2022-05-05T09:08:00Z,bug P1 (soon),closed,0,1,https://github.com/jax-ml/jax/issues/10586,The problem comes from the fact that `jax.tree_util.flatten` behaves somewhat differently than `tf.nest.flatten` on a `DictWrapper`. I will think if there is a clean solution or simply we should give an error in that case.  The same would happen if the argument contains a type for which either JAX or TensorFlow register custom pytree handlers. In that case probably the best thing to do is to give an error. 
1796,"以下是一个github上的jax下的一个issue, 标题是([new-remat] add scan rule for new remat)， 内容是 (TODO: * [x] extensivetointensive residuals optimization * [x] ~forwarding optimization~ turns out it's not needed with new remat, b/c new remat doesn't pass inputs through jaxpr_known as residuals! * [x] make sure we don't forward numpy.ndarrays though (i.e. make the fix analogous to CC(make scan fwd raw extensive inputs as DeviceArray)) **NOTE:** I only halffinished this, see this TODO. * [x] get lots of coverage by adapting existing tests * [x] merge CC(add more scan dce rule tests, fix issues) (this PR includes changes from that PR) * [x] merge CC(make core_test.py pass with core.call) (this PR includes changes from that PR) * [x] merge CC(improve partial_eval_jaxpr_custom) (this PR includes changes from that PR) * [x] merge CC(add core.closed_call_p) (this PR includes changes from that PR) * [x] write new tests * [ ] [eventually, probably not this PR] deduplicate with `_scan_partial_eval` (the policyless tracersintracersout analogue of the new `_scan_partial_eval_custom` function) The main idea is that `_scan_partial_eval_custom` generalizes partial evaluation so that what gets staged out doesn't necessarily correspond to what is unknown. That is, in standard partial evaluation, we unzip into known and unknown parts, and the unknown parts get staged into a jaxpr. But with more general partial evaluation, parameterized by a policy, what gets staged out can in general be different from what's unknown: known things can get staged out too. The rule signature, accordingly, has distinct listofbool inputs and outputs to indicate knownness and stagedness.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[new-remat] add scan rule for new remat,"TODO: * [x] extensivetointensive residuals optimization * [x] ~forwarding optimization~ turns out it's not needed with new remat, b/c new remat doesn't pass inputs through jaxpr_known as residuals! * [x] make sure we don't forward numpy.ndarrays though (i.e. make the fix analogous to CC(make scan fwd raw extensive inputs as DeviceArray)) **NOTE:** I only halffinished this, see this TODO. * [x] get lots of coverage by adapting existing tests * [x] merge CC(add more scan dce rule tests, fix issues) (this PR includes changes from that PR) * [x] merge CC(make core_test.py pass with core.call) (this PR includes changes from that PR) * [x] merge CC(improve partial_eval_jaxpr_custom) (this PR includes changes from that PR) * [x] merge CC(add core.closed_call_p) (this PR includes changes from that PR) * [x] write new tests * [ ] [eventually, probably not this PR] deduplicate with `_scan_partial_eval` (the policyless tracersintracersout analogue of the new `_scan_partial_eval_custom` function) The main idea is that `_scan_partial_eval_custom` generalizes partial evaluation so that what gets staged out doesn't necessarily correspond to what is unknown. That is, in standard partial evaluation, we unzip into known and unknown parts, and the unknown parts get staged into a jaxpr. But with more general partial evaluation, parameterized by a policy, what gets staged out can in general be different from what's unknown: known things can get staged out too. The rule signature, accordingly, has distinct listofbool inputs and outputs to indicate knownness and stagedness.",2022-05-04T22:25:17Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10576
690,"以下是一个github上的jax下的一个issue, 标题是([sparse] add bcoo_update_layout utility)， 内容是 (This is a more general utility that will replace things like `bcoo_add_batch_dim` and `_unbatch_bcoo`. `bcoo_update_layout` takes an arbitrary `BCOO` matrix and returns the same matrix represented with a different `n_batch`, `n_sparse`, and `n_dense` in its representation. In some cases such an update can result in very inefficient storage; for these cases the default is to raise a `SparseEfficiencyError`, but this can be silenced using the `on_inefficient` argument.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[sparse] add bcoo_update_layout utility,"This is a more general utility that will replace things like `bcoo_add_batch_dim` and `_unbatch_bcoo`. `bcoo_update_layout` takes an arbitrary `BCOO` matrix and returns the same matrix represented with a different `n_batch`, `n_sparse`, and `n_dense` in its representation. In some cases such an update can result in very inefficient storage; for these cases the default is to raise a `SparseEfficiencyError`, but this can be silenced using the `on_inefficient` argument.",2022-05-04T18:01:52Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10566
5994,"以下是一个github上的jax下的一个issue, 标题是(Bump myst-nb from 0.13.2 to 0.14.0)， 内容是 (Bumps mystnb from 0.13.2 to 0.14.0.  Release notes Sourced from mystnb's releases.  v0.14.0 What's Changed  DOCS: Fix typo: Controling &gt; Controlling by @​psychemedia in executablebooks/MySTNB CC(fix nan handling in pow jvp (fixes 347)) ✨♻️‼️ Rewrite mystnb by @​chrisjsewell in executablebooks/MySTNB CC(MAML Gradients + Convnets don't work) 👌 IMPROVE: Add image options to glue:figure by @​chrisjsewell in executablebooks/MySTNB CC(improve loop construct docs, remove foreach_loop) ✨ NEW: Add nb_execution_raise_on_error config by @​chrisjsewell in executablebooks/MySTNB CC(Implement np.roll (70).) 📚👌‼️: Improve documentation and usability by @​chrisjsewell in executablebooks/MySTNB CC(Remove obsolete workarounds for bugs that seem fixed.) 👌 IMPROVE: Update ANSI CSS colors by @​saulshanabrook in executablebooks/MySTNB CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.) 🔧 MAINTAIN: Readd ipykernel as dependency by @​chrisjsewell in executablebooks/MySTNB CC(Add some doc strings to lax primitives.) 🔧 MAINTAIN: Split sphinx extension into separate module by @​chrisjsewell in executablebooks/MySTNB CC(Use a regular import to add jax.__version__ rather than exec() trickery.) ✨ NEW: Add nb_kernel_rgx_aliases option by @​chrisjsewell in executablebooks/MySTNB CC(Add transpose rule for sub_p ) 📚 DOCS: Document more config options by @​chrisjsewell in executablebooks/MySTNB CC(Update XLA release) 🚀 RELEASE: v0.14.0 by @​chrisjsewell in executablebooks/MySTNB CC(Noisy CUDA_ERROR_INVALID_VALUE warning for 0element tensors outside  )  New Contributors  @​psychemedia made their first contribution in executablebooks/MySTNB CC(fix nan handling in pow jvp (fixes 347)) @​saulshanabrook made their first contribution in executablebooks/MySTNB CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.)  Full Changelog: https://github.com/executablebooks/MySTNB/compare/v0.13.2...v0.14.0    Changelog Sourced from mystnb's changelog.  v0.14.0  20220427 Full changelog This release encompasses a major rewrite of the entire library and its documentation, primarily in  CC(MAML Gradients + Convnets don't work) and  CC(Remove obsolete workarounds for bugs that seem fixed.). Breaking Changes ‼️ Configuration A number of configuration option names have been changed, such that they now share the nb_ prefix. Most of the deprecated names will be autoconverted at the start of the build, emitting a warning such as: WARNING: 'jupyter_execute_notebooks' is deprecated for 'nb_execution_mode' [mystnb.config]  nb_render_priority has been removed and replaced by nb_mime_priority_overrides, which has a different format and is more flexible. See Outputs MIME priority for more information. As per the changes in myst_parser, the dollarmath syntax extension is no longer included by default. To readd this extension, ensure that it is specified in your conf.py: myst_enable_extensions = [&quot;dollarmath&quot;]. For celllevel configuration the toplevel key render has now been deprecated for mystnb. For example, replace: ```{codecell}  render:   image:     width: 200px  ... ```  with: ```{codecell}  mystnb:   image:     width: 200px  ... ```  render will currently still be read, if present, and will issue a [mystnb.cell_metadata_key] warning.   ... (truncated)   Commits  1521fd7 🚀 RELEASE: v0.14.0 ( CC(Noisy CUDA_ERROR_INVALID_VALUE warning for 0element tensors outside  )) b75d11d 📚 DOCS: Document more config options ( CC(Update XLA release)) 7e6111b ✨ NEW: Add nb_kernel_rgx_aliases option ( CC(Add transpose rule for sub_p )) 090d58b 🔧 MAINTAIN: Split sphinx extension into separate module ( CC(Use a regular import to add jax.__version__ rather than exec() trickery.)) 3c7fec7 🔧 MAINTAIN: Readd ipykernel as dependency ( CC(Add some doc strings to lax primitives.)) e17e738 👌 IMPROVE: Update ANSI CSS colors ( CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.)) d639806 📚 DOCS: Add missing basics section cfd5b9e 📚👌‼️: Improve documentation and usability ( CC(Remove obsolete workarounds for bugs that seem fixed.)) ba22f95 ✨ NEW: Add nb_execution_raise_on_error config ( CC(Implement np.roll (70).)) edb0daf 👌 IMPROVE: Add image options to glue:figure ( CC(improve loop construct docs, remove foreach_loop)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Bump myst-nb from 0.13.2 to 0.14.0,"Bumps mystnb from 0.13.2 to 0.14.0.  Release notes Sourced from mystnb's releases.  v0.14.0 What's Changed  DOCS: Fix typo: Controling &gt; Controlling by @​psychemedia in executablebooks/MySTNB CC(fix nan handling in pow jvp (fixes 347)) ✨♻️‼️ Rewrite mystnb by @​chrisjsewell in executablebooks/MySTNB CC(MAML Gradients + Convnets don't work) 👌 IMPROVE: Add image options to glue:figure by @​chrisjsewell in executablebooks/MySTNB CC(improve loop construct docs, remove foreach_loop) ✨ NEW: Add nb_execution_raise_on_error config by @​chrisjsewell in executablebooks/MySTNB CC(Implement np.roll (70).) 📚👌‼️: Improve documentation and usability by @​chrisjsewell in executablebooks/MySTNB CC(Remove obsolete workarounds for bugs that seem fixed.) 👌 IMPROVE: Update ANSI CSS colors by @​saulshanabrook in executablebooks/MySTNB CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.) 🔧 MAINTAIN: Readd ipykernel as dependency by @​chrisjsewell in executablebooks/MySTNB CC(Add some doc strings to lax primitives.) 🔧 MAINTAIN: Split sphinx extension into separate module by @​chrisjsewell in executablebooks/MySTNB CC(Use a regular import to add jax.__version__ rather than exec() trickery.) ✨ NEW: Add nb_kernel_rgx_aliases option by @​chrisjsewell in executablebooks/MySTNB CC(Add transpose rule for sub_p ) 📚 DOCS: Document more config options by @​chrisjsewell in executablebooks/MySTNB CC(Update XLA release) 🚀 RELEASE: v0.14.0 by @​chrisjsewell in executablebooks/MySTNB CC(Noisy CUDA_ERROR_INVALID_VALUE warning for 0element tensors outside  )  New Contributors  @​psychemedia made their first contribution in executablebooks/MySTNB CC(fix nan handling in pow jvp (fixes 347)) @​saulshanabrook made their first contribution in executablebooks/MySTNB CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.)  Full Changelog: https://github.com/executablebooks/MySTNB/compare/v0.13.2...v0.14.0    Changelog Sourced from mystnb's changelog.  v0.14.0  20220427 Full changelog This release encompasses a major rewrite of the entire library and its documentation, primarily in  CC(MAML Gradients + Convnets don't work) and  CC(Remove obsolete workarounds for bugs that seem fixed.). Breaking Changes ‼️ Configuration A number of configuration option names have been changed, such that they now share the nb_ prefix. Most of the deprecated names will be autoconverted at the start of the build, emitting a warning such as: WARNING: 'jupyter_execute_notebooks' is deprecated for 'nb_execution_mode' [mystnb.config]  nb_render_priority has been removed and replaced by nb_mime_priority_overrides, which has a different format and is more flexible. See Outputs MIME priority for more information. As per the changes in myst_parser, the dollarmath syntax extension is no longer included by default. To readd this extension, ensure that it is specified in your conf.py: myst_enable_extensions = [&quot;dollarmath&quot;]. For celllevel configuration the toplevel key render has now been deprecated for mystnb. For example, replace: ```{codecell}  render:   image:     width: 200px  ... ```  with: ```{codecell}  mystnb:   image:     width: 200px  ... ```  render will currently still be read, if present, and will issue a [mystnb.cell_metadata_key] warning.   ... (truncated)   Commits  1521fd7 🚀 RELEASE: v0.14.0 ( CC(Noisy CUDA_ERROR_INVALID_VALUE warning for 0element tensors outside  )) b75d11d 📚 DOCS: Document more config options ( CC(Update XLA release)) 7e6111b ✨ NEW: Add nb_kernel_rgx_aliases option ( CC(Add transpose rule for sub_p )) 090d58b 🔧 MAINTAIN: Split sphinx extension into separate module ( CC(Use a regular import to add jax.__version__ rather than exec() trickery.)) 3c7fec7 🔧 MAINTAIN: Readd ipykernel as dependency ( CC(Add some doc strings to lax primitives.)) e17e738 👌 IMPROVE: Update ANSI CSS colors ( CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.)) d639806 📚 DOCS: Add missing basics section cfd5b9e 📚👌‼️: Improve documentation and usability ( CC(Remove obsolete workarounds for bugs that seem fixed.)) ba22f95 ✨ NEW: Add nb_execution_raise_on_error config ( CC(Implement np.roll (70).)) edb0daf 👌 IMPROVE: Add image options to glue:figure ( CC(improve loop construct docs, remove foreach_loop)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-05-02T17:20:21Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/10523," ignore I attempted this on Friday in CC(docs: pin mystnb to 0.13.2); it's going to take some more work to actually make it happen, because of some unfortunate interactions between mystnb and jupytext. See the top comment in that PR for details.","OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
891,"以下是一个github上的jax下的一个issue, 标题是(improve error when vmapped array passed as axes argument)， 内容是 (```python from jax import vmap, pmap import jax.numpy as jnp x = jnp.array(jnp.ones((3, 2, 3, 4, 5))) y = jnp.array(jnp.ones((3, 2, 3, 4, 5))) print(pmap(vmap(lambda x, y: jnp.sum(x, y), in_axes = (1,1), out_axes = (1)), in_axes = (0, 0), out_axes = 0)(x,y)) ``` ``` ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Tracedwith with   val = Tracedwith   batch_dim = 1 The axis argument must be known statically. This Tracer was created on line ~mattjj/packages/jax/halley.py:7 () ``` Maybe we can say more here, e.g. mentioning the function name `jnp.sum` and saying more about _why_ it's not ""known statically"".)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,improve error when vmapped array passed as axes argument,"```python from jax import vmap, pmap import jax.numpy as jnp x = jnp.array(jnp.ones((3, 2, 3, 4, 5))) y = jnp.array(jnp.ones((3, 2, 3, 4, 5))) print(pmap(vmap(lambda x, y: jnp.sum(x, y), in_axes = (1,1), out_axes = (1)), in_axes = (0, 0), out_axes = 0)(x,y)) ``` ``` ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Tracedwith with   val = Tracedwith   batch_dim = 1 The axis argument must be known statically. This Tracer was created on line ~mattjj/packages/jax/halley.py:7 () ``` Maybe we can say more here, e.g. mentioning the function name `jnp.sum` and saying more about _why_ it's not ""known statically"".",2022-04-29T15:47:08Z,better_errors,open,0,2,https://github.com/jax-ml/jax/issues/10493, I can take this up. Can you please provide bit more context on how the errors should be?,"Hmmm, this error message can be triggered by a much simpler and runnable w/o vmap example... ```python from jax import vmap import jax.numpy as jnp x = jnp.ones((3, 2, 2)) y = jnp.array([[0, 1]] * 3) print(jnp.sum(x[0], y[0])) print(vmap(lambda x, y: jnp.sum(x, y))(x,y)) ```"
2039,"以下是一个github上的jax下的一个issue, 标题是([remove-units] don't use abstract_unit for dropvar avals)， 内容是 (Partial evaluation using `JaxprTrace` constructs a jaxpr by first, during tracing, building a kind of bipartite graph between `JaxprTracer`s and `Recipe`s. The `JaxprTracer`s have references to the `Recipe` which constructs them, and the common `JaxprEqnRecipe`s in turn have references to the `JaxprTracer`s corresponding to both their inputs and outputs. More precisely, to avoid strong reference cycles, the `JaxprEqnRecipe`s hold weak references to their output `JaxprTracer`s. That provides a memory optimization: when user code drops a reference to a `JaxprTracer`, e.g. by a variable going out of scope, its memory can be reclaimed just like a user may expect from Python's usual reference counting operational semantics, as if there were no AD going on. Before this PR, that led to a funny typing issue: while unused `JaxprTracer`s could go out of scope before the trace completed, we still might have variables in the jaxpr being formed which correspond to those `JaxprTracer`s, and yet since we relied on the `JaxprTracer` instance to get the aval for use as a type annotation in the jaxpr, we wouldn't be able to annotate the variable's type! We used a catchall fallback: we used a special variable `DropVar` (prettyprinted like `_`) and, since we didn't have a type available, used an `AbstractUnit` type, prettyprinted like `*`. So the jaxpr would get letbinders which prettyprinted as `_:*`. This had a funny downstream effect on autodiff: transpose rules could get `ad_util.Zero(core.abstract_unit)` instances passed in as their cotangent arguments, despite that not matching the type of the tangent output! Here we fixed the issue simply by tracking the output type separately on the `JaxprEqnRecipe`, rather than relying on reading it from the weakly referenced `JaxprTracer` instances.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[remove-units] don't use abstract_unit for dropvar avals,"Partial evaluation using `JaxprTrace` constructs a jaxpr by first, during tracing, building a kind of bipartite graph between `JaxprTracer`s and `Recipe`s. The `JaxprTracer`s have references to the `Recipe` which constructs them, and the common `JaxprEqnRecipe`s in turn have references to the `JaxprTracer`s corresponding to both their inputs and outputs. More precisely, to avoid strong reference cycles, the `JaxprEqnRecipe`s hold weak references to their output `JaxprTracer`s. That provides a memory optimization: when user code drops a reference to a `JaxprTracer`, e.g. by a variable going out of scope, its memory can be reclaimed just like a user may expect from Python's usual reference counting operational semantics, as if there were no AD going on. Before this PR, that led to a funny typing issue: while unused `JaxprTracer`s could go out of scope before the trace completed, we still might have variables in the jaxpr being formed which correspond to those `JaxprTracer`s, and yet since we relied on the `JaxprTracer` instance to get the aval for use as a type annotation in the jaxpr, we wouldn't be able to annotate the variable's type! We used a catchall fallback: we used a special variable `DropVar` (prettyprinted like `_`) and, since we didn't have a type available, used an `AbstractUnit` type, prettyprinted like `*`. So the jaxpr would get letbinders which prettyprinted as `_:*`. This had a funny downstream effect on autodiff: transpose rules could get `ad_util.Zero(core.abstract_unit)` instances passed in as their cotangent arguments, despite that not matching the type of the tangent output! Here we fixed the issue simply by tracking the output type separately on the `JaxprEqnRecipe`, rather than relying on reading it from the weakly referenced `JaxprTracer` instances.",2022-04-29T05:52:09Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10489
338,"以下是一个github上的jax下的一个issue, 标题是([sparse] display n_batch & n_dense in BCOO repr)， 内容是 (I've found it inconvenient recently that this info is not immediately available when displaying sparse objects.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] display n_batch & n_dense in BCOO repr,I've found it inconvenient recently that this info is not immediately available when displaying sparse objects.,2022-04-28T23:49:14Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10488
3203,"以下是一个github上的jax下的一个issue, 标题是([Proposal] Define static arguments using typing annotations)， 内容是 (I am really enjoying `jax`, but have found the ergonomics of declaring function arguments as static for use in `jit`, `pjit`, and `pmap` to be a little cumbersome and errorprone when refactoring. In many (but not necessarily all) cases it is useful and intuitive to declare some arguments as static at the point of function declaration rather than later when mapping or jitting the function. This contrasts axismapping, which is always most intuitively done at the point of transforming the function using `xmap`, `vmap`, or `pmap`.  Using PEP 593 (Python 3.9+, backported in `typing_extensions`) it should be possible to declare static arguments in the function signature.  Example We have some function, which has both dynamic and static variables. Here `c_func` is ""embarrasingly static"" in that we strictly require it to be static at compiletime. ```python def make_hypercube(a, b, c: float, c_func: Callable[[float], float]): 	arr = jnp.empty((3, 3)) 	 Some other implementation details here 	arr += a 	arr *= b 	arr **= c_func(c) 	... 	return arr ``` Currently to `pmap` over `a` and `b`, one would do: ```python pmake_hypercube = pmap(make_hypercube, in_axes=(0, 0, None, None), static_broadcasted_argnums=(3,)) ``` This is not ideal in that changing the signature of `make_hypercube` would require us to change our transformation, bearing in mind that these may be in very different parts of the code. Additionally, when functions have many arguments, changing the `argnums` tuple during refactoring or experimentation becomes needlessly cumbersome and errorprone. I would propose adding a PEP 593 annotation, which would complement (not replace) the current method of marking arguments as static. In this case our implementation becomes: ```python from jax import pmap, Static def make_hypercube(a, b, c: float, c_func: Static[Callable[[float], float]]): 	arr = jnp.empty((3, 3)) 	 Some other implementation details here 	arr += a 	arr *= b 	arr **= c_func(c) 	... 	return arr pmake_hypercube = pmap(make_hypercube, in_axes=(0, 0, None, None)) ```  Implementation While the PEP and Python documentation go into more details on this, such an annotation would be implemented as: ```python from typing import Annotated, TypeVar,, get_type_hints, get_args T = TypeVar(""T"") StaticAnnotation = object() Static = Annotated[T, StaticAnnotation]  def f(a: str, b: Static[dict[str, float]]):     ... def is_static(func, arg) > bool:     return StaticAnnotation in get_args(get_type_hints(func, include_extras=True)[arg]) print(is_static(f, ""a"")) > False print(is_static(f, ""b"")) > True ```  Other usecases A similar situation might apply to nondifferentiable arguments: https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.htmlhandlingnondifferentiablearguments Which might have an annotation `NoDiff`.  TL;DR Introduce `Static` annotation to specify static arguments in function signatures. Should work with `jit`, `pjit`, `pmap`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Proposal] Define static arguments using typing annotations,"I am really enjoying `jax`, but have found the ergonomics of declaring function arguments as static for use in `jit`, `pjit`, and `pmap` to be a little cumbersome and errorprone when refactoring. In many (but not necessarily all) cases it is useful and intuitive to declare some arguments as static at the point of function declaration rather than later when mapping or jitting the function. This contrasts axismapping, which is always most intuitively done at the point of transforming the function using `xmap`, `vmap`, or `pmap`.  Using PEP 593 (Python 3.9+, backported in `typing_extensions`) it should be possible to declare static arguments in the function signature.  Example We have some function, which has both dynamic and static variables. Here `c_func` is ""embarrasingly static"" in that we strictly require it to be static at compiletime. ```python def make_hypercube(a, b, c: float, c_func: Callable[[float], float]): 	arr = jnp.empty((3, 3)) 	 Some other implementation details here 	arr += a 	arr *= b 	arr **= c_func(c) 	... 	return arr ``` Currently to `pmap` over `a` and `b`, one would do: ```python pmake_hypercube = pmap(make_hypercube, in_axes=(0, 0, None, None), static_broadcasted_argnums=(3,)) ``` This is not ideal in that changing the signature of `make_hypercube` would require us to change our transformation, bearing in mind that these may be in very different parts of the code. Additionally, when functions have many arguments, changing the `argnums` tuple during refactoring or experimentation becomes needlessly cumbersome and errorprone. I would propose adding a PEP 593 annotation, which would complement (not replace) the current method of marking arguments as static. In this case our implementation becomes: ```python from jax import pmap, Static def make_hypercube(a, b, c: float, c_func: Static[Callable[[float], float]]): 	arr = jnp.empty((3, 3)) 	 Some other implementation details here 	arr += a 	arr *= b 	arr **= c_func(c) 	... 	return arr pmake_hypercube = pmap(make_hypercube, in_axes=(0, 0, None, None)) ```  Implementation While the PEP and Python documentation go into more details on this, such an annotation would be implemented as: ```python from typing import Annotated, TypeVar,, get_type_hints, get_args T = TypeVar(""T"") StaticAnnotation = object() Static = Annotated[T, StaticAnnotation]  def f(a: str, b: Static[dict[str, float]]):     ... def is_static(func, arg) > bool:     return StaticAnnotation in get_args(get_type_hints(func, include_extras=True)[arg]) print(is_static(f, ""a"")) > False print(is_static(f, ""b"")) > True ```  Other usecases A similar situation might apply to nondifferentiable arguments: https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.htmlhandlingnondifferentiablearguments Which might have an annotation `NoDiff`.  TL;DR Introduce `Static` annotation to specify static arguments in function signatures. Should work with `jit`, `pjit`, `pmap`.",2022-04-28T13:35:38Z,enhancement,open,10,13,https://github.com/jax-ml/jax/issues/10476,"I would be happy to do the implementation and documentation for this if I get a few pointers in the right direction. `typing_extensions` is already a dependency, so there shouldn't be any additional dependencies needed. Other libraries have seen great success with adopting the new type hinting language features (`pydantic`, `fastapi`, ...). It should be noted that the `Annotated` 'passthrough'style typehints haven't yet seen much adoption, but I think for cases like this one they provide the most Pythonic experience. If the user does not wish to use typehinting, but still use the `Static` annotation, one can simply bind the generic to `Any` with: ```python def func(a, b: Static[Any]): 	...  Or to reduce verbosity S = Static[Any] def func(a, b: S): 	... ```","It's not exactly the same, but the Equinox project offers something very similar to what you're requesting. Equinox offers a few ""filtered transformations"" that take a rule for determining how the arguments should be partitioned, and then uses that instead of the manuallyspecified approach used at the moment. (Equinox also implements some neural networks etc, but that's an unrelated part of the library.) The main difference to your approach is that the partitioning is handled using the types of the arguments actually provided, rather than the types provided via the annotation. (And if you really want I think it should probably be possible to pass a `filter_spec` that does the annotationlike behaviour, so you can have that too.) `filter_{jit,grad,value_and_grad,custom_vjp}` currently exist; the next release will also include `filter_{vmap,pmap}`.  Side notes. 1. Equinox's approach is actually more powerful: it allows for splitting each individual argument into dynamic/static/etc. pieces, rather than each entire argument having to be the same choice of dynamic/static/etc. This withinargument splitting comes up quite naturally in quite a lot of contexts, e.g. we might store the data for an MLP as `[[weight0, bias0], activation, [weight1, bias1]]`, in which the weights and biases are JAX arrays but the activation is some Python callable. It would then be neatest to pass all of this data as a single argument. 2. Your example `c_func` need not be ""embarrasingly static"". It's perfectly possible to have callable dynamic objects. Going back to Equinox: try passing `c_func` as an instance of `equinox.nn.Linear`. This is because it's possible to have custom PyTrees can (a) contain only dynamic JAX arrays and (b) can implement a `__call__` method.","I really like the look of Equinox, which seems to have a lot of improvements to the ergonomics of JAX – `eqx.Module` seems similar in spirit to the idea of a dataclass that is also automatically a PyTree, which seems to be a highly requested feature (cf.: CC(Adding a pytree registration function for dataclasses.), CC( decorator for classes), CC(Automatically treat dataclasses as pytrees)) Some of these discussions predate wider adoption of PEP 593. I think my attempt at trying to coin a phrase might need some elaboration – ""embarrasingly static"" was intended to refer to arguments that are __always__ statically compiled such as a python function or any other object that cannot be replaced by a `Tracer` and jitted. Since these attributes are necessarily static in all cases, they can be deduced as static when defining the function. An important exception to this could be a function that takes an argument, `arr`, that can be either a numpy or JAX array. In this case we might want to specify that `arr` is static at the point of calling `jax.jit` in the case where `arr` is a numpy array.  The problem In native JAX we currently need to carry around a 'schema' that we can use to define which arguments are static. This schema is `static_argnums` in the case of `jax.jit` and `broadcasted_static_argnums` in the case of `jax.pmap`. It is not very ergonomic to type out at all and is prone to breakage when refactoring.  The solution by Equinox (as I understand it, please correct) Using Equinox this situation is improved by either:  Filter spec Making a custom `filter_spec` and using filtered transformations `filter_jit`, `filter_pmap`. This solution still requires creation a schemalike object and passing it to the filtered transformation. It is arguably more ergonomic.  Using `is_array` filter spec The default filter spec turns all nonarrays into static arguments. This is essentially exactly what I wanted, but I don't really like the fact that it 'hides' from the user which arguments are statically compiled and which are traced. Since it is done using a runtime instance check, it won't fail loudly in cases where an argument inferred to be static is passed in a place where the user thinks it is dynamic. This could silently lead to excessive recompilation.  The solution using annotations  Static arguments must still be explicitly marked as such  Ergonomic and composable  Can be implemented while preserving existing `jit` and `pmap` function signatures  Works alongside existing logic  Still loudly fails when nonJAX types are passed to arguments marked as static",Sounds Great!❤️,"Can you use `static_argnames` instead of `static_argnums` to help with refactoring? I've been using Equinox and really like it so far, it has some decent default settings for `filter_jit` and `filter_grad` so 90% of the time I don't to worry about the `filter_spec` stuff.","Folks here may be interested in some changes we're proposing for Equinox. Namely, to simplify the `filter_jit ` API to just `filter_jit(fn, donate)`, with all JAX/NumPy arrays dynamically traced and everything else held static. (And the `donate` flag handling donation be behaviour, of course.) This is in recognition of the fact that all arrays *have* to be treated dynamically, and most Python types *have* to be treated statically, so really the only room for nondefault behaviour (i.e. treating `bool`/`int`/`float`/`complex` dynamically) can be handled just by wrapping these types with `jnp.asarray` yourself. The PR for this is here: https://github.com/patrickkidger/equinox/pull/235 The code is here: https://github.com/patrickkidger/equinox/blob/ee7f91a1c422abc0e502c6dd467045e79c1717bc/equinox/jit.pyL87 I'd welcome any feedback on this change.","kidger The proposed changes seem like it would make things easier to use... was interested in how this would handle `typing.NamedTuple`, I'm guessing those would be treated as static by default? Would the `filter_spec` stuff still be required for making `equniox.Module` array attributes static? I have an `equinox.Module` that has a`default_value` attribute that I need to always keep static despite being a `jax.Array` type. Currently I'm using the `filter_spec` functionality to do this. With the new API changes would this functionality remain the same?","Regarding `NamedTuple`: as this a pytree node, then it will be unpacked, and each of its elements treated individually. Regarding static `Array`s: this should be impossible right now. Static inputs must be hashable, and `jax.Array`s aren't hashable. So I'm not sure what you're currently doing here.","Here are some more details. I have a voxel map class. ``` class VoxelMap(equinox.Module):     """"""A sparse voxel map.     Attributes:         resolution: Voxel width [meters]         indices: Dense 3D array relating voxel index to values. Populated voxels are set to             integers in the range [0, num_populated_voxels), unpopulated are set to 1         values: Dense 2D array of voxel values (num_populated_voxels, num_elements_per_voxel)         default_value: Default voxel value (num_elements_per_voxel)     """"""     resolution: float     indices: jax.Array     values: jax.Array     default_value: jax.Array ``` The `equinox.filter_grad` function updates everything properly by default except for the `default_value` attribute. I can't just set its type to `float` because it needs to handle more than one element sometimes. Currently I'm using `filter_spec` functionality to ignore this attribute but is a bit complicated to read. I wouldn't mind baking it into the class somehow, are there other options to consider? Maybe baking it into the class using a decorator?","Ah, we're (partly) talking past each other. The proposed change is to `filter_jit`. The function you're using is `filter_grad`. Right now that's staying the same. However  you're right that the filtering argument to `filter_grad` is ugly! So first of all, it is actually possible to avoid using it, and bake this into the class: ```python class Buffer(eqx.Module):     array: jax.Array     def get(self):         return lax.stop_gradient(self.array) class VoxelMap(eqx.Module):     ...     default_value: Buffer     def __init__(self, ..., default_value):         self.default_value = Buffer(default_value)     def __call__(self, ...):         default_value = self.default_value.get()         ... ``` As it happens, we are also contemplating removing the argument of `equinox.filter_grad`. (We just haven't done it yet.) Rationale:  It's not commonly used, and this way we get to simplify its API.  99% of its usecases are more easily handled using the above pattern. And if you really really need exactly the same behaviour, then you can still manually split things up using `eqx.partition`. If you are one of the relatively few users using the filtering argument of `eqx.filter_grad`, then I'd welcome feedback on that proposed change as well!","Ah, thanks `lax.stop_gradient` seems like the best solution.","At one point I was also thinking about this feature, but after more discussion internally it seems like an implementation based on annotations would be errorprone and hard to put guard rails around. The issue is that you could have other decorators that are misbehaved or perform arbitrary transformations on the arguments that make it so Jax can't inspect the annotations. For wellbehaved decorators that use `functools.wraps` and `typing.ParamSpec` things would work out nicely, but in general there could be lots of silent failure cases and there's no way to detect a misbehaved decorator.","  I admittedly haven't messed around enough with PEP 593 to have a feel for how often decorators mangle the annotations, but I understand if Jax doesn't want to introduce what would admittedly be hardtodebug cases by introducing this syntax. I can't gauge whether a warning in the documentation might be deemed sufficient to go ahead with a proposal like this anyway – the usecase matches the one mentioned in the PEP very well, so presumably there is some enthusiasm for syntax like this being supported in the Python community. A large amount of the 'frustration' or fiddliness around annotating arguments in Jax currently could also be alleviated by CC([Proposal] Consistent `argnums` and `argnames` parameters for transformations), which is independent of this proposal. "
1349,"以下是一个github上的jax下的一个issue, 标题是(Change the default scatter mode to FILL_OR_DROP.)， 内容是 (Change the default scatter mode to FILL_OR_DROP. This is a reasonably safe change, because it has no effect on the forward pass of a computation: the default behavior (PROMISE_IN_BOUNDS) also drops outofbounds scatters. This change does however affect the transpose (gradient) of a scatter with outofbounds indices: the gradient of a PROMISE_IN_BOUNDS scatter is a PROMISE_IN_BOUNDS gather, and a PROMISE_IN_BOUNDS gather clips outofbounds indices into range. This is not mathematically correct: a dropped scatter index does not contribute to the primal output, and so its transpose should yield a zero cotangent. After this change, the gradient of a default scatter is a gather with a fill value of 0: i.e., the indices that were dropped do not make gradient contributions, which is mathematically correct. Separately, I am working towards switching outofbounds gather() operations to also have FILL_OR_DROP semantics, although that change is more disruptive because a number of users have outofbounds indices in their gather()s. Issues: https://github.com/google/jax/issues/278 https://github.com/google/jax/issues/9839)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Change the default scatter mode to FILL_OR_DROP.,"Change the default scatter mode to FILL_OR_DROP. This is a reasonably safe change, because it has no effect on the forward pass of a computation: the default behavior (PROMISE_IN_BOUNDS) also drops outofbounds scatters. This change does however affect the transpose (gradient) of a scatter with outofbounds indices: the gradient of a PROMISE_IN_BOUNDS scatter is a PROMISE_IN_BOUNDS gather, and a PROMISE_IN_BOUNDS gather clips outofbounds indices into range. This is not mathematically correct: a dropped scatter index does not contribute to the primal output, and so its transpose should yield a zero cotangent. After this change, the gradient of a default scatter is a gather with a fill value of 0: i.e., the indices that were dropped do not make gradient contributions, which is mathematically correct. Separately, I am working towards switching outofbounds gather() operations to also have FILL_OR_DROP semantics, although that change is more disruptive because a number of users have outofbounds indices in their gather()s. Issues: https://github.com/google/jax/issues/278 https://github.com/google/jax/issues/9839",2022-04-27T14:27:39Z,,closed,0,0,https://github.com/jax-ml/jax/issues/10463
5762,"以下是一个github上的jax下的一个issue, 标题是(NCCL error when trying xmap on 4 T4 GPUs)， 内容是 (Hi,  First, thank you for making this library available, I love it! I tried to reproduce the example from the xmap documentation (https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html) on a VM with 4 T4 GPUs. I am using conda and the requirements used are: ``` name: jaxenv channels:    defaults    condaforge dependencies:    python=3.8    numpy==1.19.5    pandas==1.2.5    six==1.15.0    pip=21.0.1    pip:        findlinks https://storage.googleapis.com/jaxreleases/jax_releases.html        jax==0.3.2        optax==0.0.9        dmhaiku==0.0.6        regex==2022.1.18        tokenizers==0.11.5        tensorflow==2.6.0        neptuneclient==0.14.3        biopython==1.79        jaxlib==0.3.0+cuda11.cudnn82        tqdm==4.56.0` ``` I launched this snippet of code (taken from the documentation): ``` import jax import jax.numpy as jnp import numpy as np from jax import lax from jax.experimental.maps import Mesh, xmap from jax.nn import one_hot, relu from jax.scipy.special import logsumexp def named_predict(w1: jnp.ndarray, w2: jnp.ndarray, image: jnp.ndarray) > jnp.ndarray:     hidden = relu(lax.pdot(image, w1, ""inputs""))     logits = lax.pdot(hidden, w2, ""hidden"")     return logits  logsumexp(logits, ""classes"") def named_loss(     w1: jnp.ndarray, w2: jnp.ndarray, images: jnp.ndarray, labels: jnp.ndarray ) > jnp.ndarray:     predictions = named_predict(w1, w2, images)     num_classes = lax.psum(1, ""classes"")     targets = one_hot(labels, num_classes, axis=""classes"")     losses = lax.psum(targets * predictions, ""classes"")     return lax.pmean(losses, ""batch"") if __name__ == ""__main__"":      Start script     devices = jax.local_devices()     print(f""Start test, detected devices: {devices}"")      Generate dummy data     w1 = jnp.zeros((784, 512))     w2 = jnp.zeros((512, 10))     images = jnp.zeros((128, 784))     labels = jnp.zeros(128, dtype=jnp.int32)      Prepare xmapped function     in_axes = [         [""inputs"", ""hidden"", ...],         [""hidden"", ""classes"", ...],         [""batch"", ""inputs"", ...],         [""batch"", ...],     ]     loss = xmap(         named_loss,         in_axes=in_axes,         out_axes=[...],         axis_resources={""batch"": ""x"", ""hidden"": ""y""},     )      Prepare devices mesh     num_devices = len(devices)     assert num_devices >= 2 and num_devices % 2 == 0     devices = np.array(jax.local_devices()).reshape((2, num_devices // 2))     print(f""Device mesh: {devices}"")      Run meshed computations     with Mesh(devices, (""x"", ""y"")):         print(""Loss computed on mesh: "", loss(w1, w2, images, labels))     print(""End of test."") ``` I got the following output: ``` Start test, detected devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)] /opt/conda/envs/jaxenv/lib/python3.8/sitepackages/jax/experimental/maps.py:492: UserWarning: xmap is an experimental feature and probably has bugs!   warn(""xmap is an experimental feature and probably has bugs!"") Device mesh: [[GpuDevice(id=0, process_index=0) GpuDevice(id=1, process_index=0)]  [GpuDevice(id=2, process_index=0) GpuDevice(id=3, process_index=0)]] 20220427 12:30:21.693000: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2124] Execution of replica 2 failed: INTERNAL: external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:326: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled system error 20220427 12:30:21.693490: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2124] Execution of replica 3 failed: INTERNAL: external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:326: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled system error 20220427 12:30:31.317787: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck: 20220427 12:30:31.317860: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck: 20220427 12:30:31.693344: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2264] Replicated computation launch failed, but not all replicas terminated. Aborting process to work around deadlock. Failure message (there may have been multiple failures, see the error log for all failures):  external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:326: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled system error Aborted (core dumped) ``` If I change the version of Jax from 0.3.2 to 0.3.0 (without changing anything else), then I obtain a different error: ``` Start test, detected devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)] /opt/conda/envs/jaxenv/lib/python3.8/sitepackages/jax/experimental/maps.py:544: UserWarning: xmap is an experimental feature and probably has bugs!   warn(""xmap is an experimental feature and probably has bugs!"") Device mesh: [[GpuDevice(id=0, process_index=0) GpuDevice(id=1, process_index=0)]  [GpuDevice(id=2, process_index=0) GpuDevice(id=3, process_index=0)]] Traceback (most recent call last):   File ""scripts/xmap_test.py"", line 62, in      with Mesh(devices, (""x"", ""y"")): AttributeError: __enter__ ``` Please, do not hesitate if you need more information. Thanks a lot!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,NCCL error when trying xmap on 4 T4 GPUs,"Hi,  First, thank you for making this library available, I love it! I tried to reproduce the example from the xmap documentation (https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html) on a VM with 4 T4 GPUs. I am using conda and the requirements used are: ``` name: jaxenv channels:    defaults    condaforge dependencies:    python=3.8    numpy==1.19.5    pandas==1.2.5    six==1.15.0    pip=21.0.1    pip:        findlinks https://storage.googleapis.com/jaxreleases/jax_releases.html        jax==0.3.2        optax==0.0.9        dmhaiku==0.0.6        regex==2022.1.18        tokenizers==0.11.5        tensorflow==2.6.0        neptuneclient==0.14.3        biopython==1.79        jaxlib==0.3.0+cuda11.cudnn82        tqdm==4.56.0` ``` I launched this snippet of code (taken from the documentation): ``` import jax import jax.numpy as jnp import numpy as np from jax import lax from jax.experimental.maps import Mesh, xmap from jax.nn import one_hot, relu from jax.scipy.special import logsumexp def named_predict(w1: jnp.ndarray, w2: jnp.ndarray, image: jnp.ndarray) > jnp.ndarray:     hidden = relu(lax.pdot(image, w1, ""inputs""))     logits = lax.pdot(hidden, w2, ""hidden"")     return logits  logsumexp(logits, ""classes"") def named_loss(     w1: jnp.ndarray, w2: jnp.ndarray, images: jnp.ndarray, labels: jnp.ndarray ) > jnp.ndarray:     predictions = named_predict(w1, w2, images)     num_classes = lax.psum(1, ""classes"")     targets = one_hot(labels, num_classes, axis=""classes"")     losses = lax.psum(targets * predictions, ""classes"")     return lax.pmean(losses, ""batch"") if __name__ == ""__main__"":      Start script     devices = jax.local_devices()     print(f""Start test, detected devices: {devices}"")      Generate dummy data     w1 = jnp.zeros((784, 512))     w2 = jnp.zeros((512, 10))     images = jnp.zeros((128, 784))     labels = jnp.zeros(128, dtype=jnp.int32)      Prepare xmapped function     in_axes = [         [""inputs"", ""hidden"", ...],         [""hidden"", ""classes"", ...],         [""batch"", ""inputs"", ...],         [""batch"", ...],     ]     loss = xmap(         named_loss,         in_axes=in_axes,         out_axes=[...],         axis_resources={""batch"": ""x"", ""hidden"": ""y""},     )      Prepare devices mesh     num_devices = len(devices)     assert num_devices >= 2 and num_devices % 2 == 0     devices = np.array(jax.local_devices()).reshape((2, num_devices // 2))     print(f""Device mesh: {devices}"")      Run meshed computations     with Mesh(devices, (""x"", ""y"")):         print(""Loss computed on mesh: "", loss(w1, w2, images, labels))     print(""End of test."") ``` I got the following output: ``` Start test, detected devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)] /opt/conda/envs/jaxenv/lib/python3.8/sitepackages/jax/experimental/maps.py:492: UserWarning: xmap is an experimental feature and probably has bugs!   warn(""xmap is an experimental feature and probably has bugs!"") Device mesh: [[GpuDevice(id=0, process_index=0) GpuDevice(id=1, process_index=0)]  [GpuDevice(id=2, process_index=0) GpuDevice(id=3, process_index=0)]] 20220427 12:30:21.693000: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2124] Execution of replica 2 failed: INTERNAL: external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:326: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled system error 20220427 12:30:21.693490: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2124] Execution of replica 3 failed: INTERNAL: external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:326: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled system error 20220427 12:30:31.317787: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck: 20220427 12:30:31.317860: E external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:144] This thread has been waiting for 10s and may be stuck: 20220427 12:30:31.693344: F external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2264] Replicated computation launch failed, but not all replicas terminated. Aborting process to work around deadlock. Failure message (there may have been multiple failures, see the error log for all failures):  external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:326: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled system error Aborted (core dumped) ``` If I change the version of Jax from 0.3.2 to 0.3.0 (without changing anything else), then I obtain a different error: ``` Start test, detected devices: [GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)] /opt/conda/envs/jaxenv/lib/python3.8/sitepackages/jax/experimental/maps.py:544: UserWarning: xmap is an experimental feature and probably has bugs!   warn(""xmap is an experimental feature and probably has bugs!"") Device mesh: [[GpuDevice(id=0, process_index=0) GpuDevice(id=1, process_index=0)]  [GpuDevice(id=2, process_index=0) GpuDevice(id=3, process_index=0)]] Traceback (most recent call last):   File ""scripts/xmap_test.py"", line 62, in      with Mesh(devices, (""x"", ""y"")): AttributeError: __enter__ ``` Please, do not hesitate if you need more information. Thanks a lot!",2022-04-27T12:42:55Z,bug P2 (eventual) NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/10461,"Hmm. This might be tough for us to figure out, given it's happening inside one of NVidia's libraries (NCCL). One hypothesis: perhaps we are low on memory?  It's worth ruling out, at least. https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html Try setting `XLA_PYTHON_CLIENT_ALLOCATOR=platform`, which is slow, but let's see if it fixes the problem."," I ran into a similar problem as yours and I later found out that there was a mismatch between the CUDA version the driver was using (11.4) and the CUDA libraries I had loaded (11.1). I was experimenting with `pjit` and got the following error: ``` 20220629 12:36:12.905032: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2129] Execution of replica 0 failed: INTERNAL: external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:245:  NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled cuda error                                                                                                                                                     20220629 12:36:12.907134: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2129] Execution of replica 0 failed: INTERNAL: external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:245:  NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled cuda error                                                                                                                                                     20220629 12:36:12.907630: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2129] Execution of replica 0 failed: INTERNAL: external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:245:  NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled cuda error                                                                                                                                                     20220629 12:36:12.907832: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2129] Execution of replica 0 failed: INTERNAL: external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:245:  NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled cuda error                                                                                                                                                     Traceback (most recent call last):                                                                                                                                                                                                                File ""/petrobr/parceirosbr/brcluster/gustavo.leite/research/examples/jax/partitioning.py"", line 25, in                                                                                                                                    output = f(M, M)                                                                                                                                                                                                                              File ""/petrobr/parceirosbr/brcluster/gustavo.leite/research/examples/venv/lib/python3.9/sitepackages/jax/experimental/pjit.py"", line 355, in wrapped                                                                                             out = pjit_p.bind(*args_flat, **params)                                                                                                                                                                                                       File ""/petrobr/parceirosbr/brcluster/gustavo.leite/research/examples/venv/lib/python3.9/sitepackages/jax/core.py"", line 327, in bind                                                                                                             return self.bind_with_trace(find_top_trace(args), args, params)                                                                                                                                                                               File ""/petrobr/parceirosbr/brcluster/gustavo.leite/research/examples/venv/lib/python3.9/sitepackages/jax/core.py"", line 330, in bind_with_trace                                                                                                  out = trace.process_primitive(self, map(trace.full_raise, args), params)                                                                                                                                                                      File ""/petrobr/parceirosbr/brcluster/gustavo.leite/research/examples/venv/lib/python3.9/sitepackages/jax/core.py"", line 680, in process_primitive                                                                                                return primitive.impl(*tracers, **params)                                                                                                                                                                                                     File ""/petrobr/parceirosbr/brcluster/gustavo.leite/research/examples/venv/lib/python3.9/sitepackages/jax/experimental/pjit.py"", line 722, in _pjit_call_impl                                                                                     return compiled.unsafe_call(*args)                                                                                                                                                                                                            File ""/petrobr/parceirosbr/brcluster/gustavo.leite/research/examples/venv/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 312, in wrapper                                                                                                 return func(*args, **kwargs)                                                                                                                                                                                                                  File ""/petrobr/parceirosbr/brcluster/gustavo.leite/research/examples/venv/lib/python3.9/sitepackages/jax/interpreters/pxla.py"", line 1687, in __call__                                                                                           out_bufs = self.xla_executable.execute_sharded_on_local_devices(input_bufs)                                                                                                                                                                 jaxlib.xla_extension.XlaRuntimeError: INTERNAL: external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nccl_utils.cc:245: NCCL operation ncclCommInitRank(comm.get(), nranks, id, rank) failed: unhandled cuda error: while running replic a 0 and partition 0 of a replicated computation (other replicas may have failed as well). ``` In my case, I work on a HPC cluster and I can load different versions of CUDA via environment modules. The fix was as simple as loading CUDA 11.4 via: `module load cuda/11.4`. I am not sure if this is your case, but I hope it helps.  Click here to see the example code that produced the error. ```python !/usr/bin/env python3 import jax import jax.numpy as jnp import numpy as np from jax.experimental.pjit import pjit from jax.experimental.maps import Mesh from jax.experimental import PartitionSpec  Create device mesh mesh = Mesh(np.array(jax.devices()).reshape((2, 2)), (""x"", ""y""))  Create matrix and vector M = jnp.eye(8) v = jnp.arange(8).reshape((8, 1)) spec = PartitionSpec(""x"", ""y"") f = pjit(jnp.dot,          in_axis_resources=(spec, None),          out_axis_resources=spec) with mesh:     output = f(M, M) print(output) ``` ","Given there was no followup and we can't reproduce this, I'm going to declare the issue stale. (Please retry with an up to date jax/jaxlib, if it's still a problem.)"
708,"以下是一个github上的jax下的一个issue, 标题是([sparse] bcoo_broadcast_in_dim: default to adding leading batch dimensions)， 内容是 (Why? Because in this situation batch dimensions always yield a more compact representation, and converting from batch to sparse dimensions is far cheaper than converting from sparse to batch dimensions. This, along with a sparse lowering rule for `lax.concatenate_p`, will allow us to support clean concatenation & stacking of sparse arrays. I'm a bit concerned that this might break an existing usecase, so I need to run some extra tests before merging.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] bcoo_broadcast_in_dim: default to adding leading batch dimensions,"Why? Because in this situation batch dimensions always yield a more compact representation, and converting from batch to sparse dimensions is far cheaper than converting from sparse to batch dimensions. This, along with a sparse lowering rule for `lax.concatenate_p`, will allow us to support clean concatenation & stacking of sparse arrays. I'm a bit concerned that this might break an existing usecase, so I need to run some extra tests before merging.",2022-04-26T19:26:33Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/10454
424,"以下是一个github上的jax下的一个issue, 标题是(lax.linalg.qr: allow jvp when m == n and full_matrices=True)， 内容是 (The JVP is fine in this case because the computation is the same as for `full_matrices=False`. Addresses CC(Autodiff with `numpy.linalg.qr` differs from autodiff with `scipy.linalg.qr`))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,lax.linalg.qr: allow jvp when m == n and full_matrices=True,The JVP is fine in this case because the computation is the same as for `full_matrices=False`. Addresses CC(Autodiff with `numpy.linalg.qr` differs from autodiff with `scipy.linalg.qr`),2022-04-26T16:41:56Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10451
2882,"以下是一个github上的jax下的一个issue, 标题是(Autodiff with `numpy.linalg.qr` differs from autodiff with `scipy.linalg.qr`)， 内容是 (Hi all!  I am a little confused by the different implementations of QR decompositions in `jax.scipy.linalg` und `jax.numpy.linalg`. If called with specific inputs, one admits a JVP, and the other one does not. It is not unlikely that I missed something, but I suspect the reason is the following. Both `jax.scipy.linalg` and `jax.numpy.linalg` point to `jax.lax.linalg.qr`, but differently: if one uses for example `mode=""r""`, then the numpy version leads to `full_matrix=False` https://github.com/google/jax/blob/04b6f15cdbf26163aae1ca73cd3422e6e1912df9/jax/_src/numpy/linalg.pyL481 and the scipy version to `full_matrix=True` https://github.com/google/jax/blob/04b6f15cdbf26163aae1ca73cd3422e6e1912df9/jax/_src/scipy/linalg.pyL171 (Admittedly, it is probably hard to see from the permalinks... ) This is an issue for me, because the Jacobianvector product of `jax.lax.linalg.qr` raises an error whenever `full_matrices` is True https://github.com/google/jax/blob/04b6f15cdbf26163aae1ca73cd3422e6e1912df9/jax/_src/lax/linalg.pyL1176 and thus I have to be really careful with Jacobians in my code, which does rely quite heavily on QRdecompositions.  Minimal working example: ```python import jax import jax.numpy as jnp import jax.scipy as jsp key = jax.random.PRNGKey(seed=2) key, subkey = jax.random.split(key) x = jax.random.normal(key, shape=(2, 2)) dx = jax.random.normal(subkey, shape=(2, 2))  The values coincide r_np = jnp.linalg.qr(x, mode=""r"") r_sp = jsp.linalg.qr(x, mode=""r"") assert jnp.allclose(r_np, r_sp)  The numpy jvp seems to work, at least kind of... _, dr = jax.jvp(lambda s: jnp.linalg.qr(s, mode=""r""), (x,), (dx,)) dt = 1e6 dr_ = (jnp.linalg.qr(x + dt * dx, mode=""r"")  jnp.linalg.qr(x, mode=""r"")) / dt assert jnp.allclose(dr, dr_, atol=1e3, rtol=1e3) , (drdr_)   with tol=1e5 it does not work, but at least there are values...  The scipy jvp fails:  NotImplementedError: Unimplemented case of QR decomposition derivative _ = jax.jvp(lambda s: jsp.linalg.qr(s, mode=""r""), (x,), (dx,)) ``` From the snippet, it is not 100% clear to me whether the JVP is even correct, given that I had to reduce the precision in the second assertion. But the results are also too similar for something to be drastically broken, so it should be fine?! There have been some potentially related issues regarding QR decompositions and their differentiability:  CC(Gradient of QR fails)  CC(Error differentiating QR decomposition)   CC(Vmapped QR on GPU is not parallelized) (only vaguely related, but it does mention QR decompositions!) Am I missing something? Thanks for your help! Let me know if you need more info from me :))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Autodiff with `numpy.linalg.qr` differs from autodiff with `scipy.linalg.qr`,"Hi all!  I am a little confused by the different implementations of QR decompositions in `jax.scipy.linalg` und `jax.numpy.linalg`. If called with specific inputs, one admits a JVP, and the other one does not. It is not unlikely that I missed something, but I suspect the reason is the following. Both `jax.scipy.linalg` and `jax.numpy.linalg` point to `jax.lax.linalg.qr`, but differently: if one uses for example `mode=""r""`, then the numpy version leads to `full_matrix=False` https://github.com/google/jax/blob/04b6f15cdbf26163aae1ca73cd3422e6e1912df9/jax/_src/numpy/linalg.pyL481 and the scipy version to `full_matrix=True` https://github.com/google/jax/blob/04b6f15cdbf26163aae1ca73cd3422e6e1912df9/jax/_src/scipy/linalg.pyL171 (Admittedly, it is probably hard to see from the permalinks... ) This is an issue for me, because the Jacobianvector product of `jax.lax.linalg.qr` raises an error whenever `full_matrices` is True https://github.com/google/jax/blob/04b6f15cdbf26163aae1ca73cd3422e6e1912df9/jax/_src/lax/linalg.pyL1176 and thus I have to be really careful with Jacobians in my code, which does rely quite heavily on QRdecompositions.  Minimal working example: ```python import jax import jax.numpy as jnp import jax.scipy as jsp key = jax.random.PRNGKey(seed=2) key, subkey = jax.random.split(key) x = jax.random.normal(key, shape=(2, 2)) dx = jax.random.normal(subkey, shape=(2, 2))  The values coincide r_np = jnp.linalg.qr(x, mode=""r"") r_sp = jsp.linalg.qr(x, mode=""r"") assert jnp.allclose(r_np, r_sp)  The numpy jvp seems to work, at least kind of... _, dr = jax.jvp(lambda s: jnp.linalg.qr(s, mode=""r""), (x,), (dx,)) dt = 1e6 dr_ = (jnp.linalg.qr(x + dt * dx, mode=""r"")  jnp.linalg.qr(x, mode=""r"")) / dt assert jnp.allclose(dr, dr_, atol=1e3, rtol=1e3) , (drdr_)   with tol=1e5 it does not work, but at least there are values...  The scipy jvp fails:  NotImplementedError: Unimplemented case of QR decomposition derivative _ = jax.jvp(lambda s: jsp.linalg.qr(s, mode=""r""), (x,), (dx,)) ``` From the snippet, it is not 100% clear to me whether the JVP is even correct, given that I had to reduce the precision in the second assertion. But the results are also too similar for something to be drastically broken, so it should be fine?! There have been some potentially related issues regarding QR decompositions and their differentiability:  CC(Gradient of QR fails)  CC(Error differentiating QR decomposition)   CC(Vmapped QR on GPU is not parallelized) (only vaguely related, but it does mention QR decompositions!) Am I missing something? Thanks for your help! Let me know if you need more info from me :)",2022-04-26T15:44:59Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/10450,"Hi  thanks for the question. First of all, the difference in the `jax.scipy.qr` and `jax.numpy.qr` defaults comes from the APIs they are wrapping. Scipy defaults to `mode='full'` (doc) while numpy defaults to `mode='reduced'` (doc) I'm not entirely familiar with the qr JVP rule, but my understanding is that the reason it is undefined for the full matrix output is becuse the gradient itself is mathematically illdefined in this case: full output will construct additional bases spanning the remaining vector space, and there are infinitely many valid bases for these and thus how they change with respect to a perturbation of the input is illdefined. It may be that we are too strict about erroring in the jvp on `full_matrices=True` even when the shapes of the inputs mean that the result is equivalen to `full_matrices=False`, and thus it should be differentiable. Finally, I suspect the reason your JVP comparison requires such loose tolerance is probably not because of inaccuracy of the automatic gradient, but because of inaccuracy in the finite difference gradient to which you're comparing it.","Hi, thanks for the immediate answer! My question is less about the defaults or why there are errors for specific QRdecompositionmodes, but more about why both APIs point to _different_ versions of `jax.lax.linalg.qr` when called with `mode=""full""` (or `mode=""r""`, as in the example above). I understand that different QR formats may have different differentiability assumptions, but that seems to be different, or am I missing something? I apologise if this wasn't clear from my initial question. > Finally, I suspect the reason your JVP comparison requires such loose tolerance is probably not because of inaccuracy of the automatic gradient, but because of inaccuracy in the finitedifference gradient to which you're comparing it. True, that could be the case. However. it is not resolved with different `dt` values (I was a bit too lazy to try more sophisticated schemes, to be perfectly honest). I don't think it is a major part of this issue, though :) ","I see  thanks for the clarification. I believe the behavior of the `numpy.linalg.qr` and `scipy.linalg.qr` functions match that of the corresponding numpy/scipy implementation, which is the intent. In particular, the original numpy implementation has `mode='full'` and `mode='reduced'` return the same thing, though the behavior is marked as deprecated: https://github.com/numpy/numpy/blob/65a701fd40d5ab3b772131daf45679d6ecf3d721/numpy/linalg/linalg.pyL911L917 I can't say I understand the reason for this, but the `jax.numpy.linalg.qr` result does seem consistent with the `numpy.linalg.qr` result in this case. If you find any case where the behavior of `jax.numpy.linalg.qr` or `jax.scipy.linalg.qr` differs from that of the corresponding `numpy`/`scipy` routines, then it is a bug that we should address. But differences between `mode` conventions used within `numpy` and `scipy` are not something we can control in JAX.","It looks like there is one inconsistency: `scipy.linalg.qr` with `mode=""r""` returns a length1 tuple containing the resulting array, where `jax.scipy.linalg.qr` just returns the array result itself. We should address this.","Oh wow, it seems that I haven't noticed that scipy's mode=r and numpy's mode=r return differently shaped arrays! Thank you for clarifying this :)  From my perspective, the issue can be closed now :) ",Thanks!
423,"以下是一个github上的jax下的一个issue, 标题是(Add an option when lowering to not remove unused arguments.)， 内容是 (Add an option when lowering to not remove unused arguments. This way, code using the output xla executable does not need to also drop the unused arguments, simplifying downstream code.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add an option when lowering to not remove unused arguments.,"Add an option when lowering to not remove unused arguments. This way, code using the output xla executable does not need to also drop the unused arguments, simplifying downstream code.",2022-04-26T12:31:42Z,,closed,0,0,https://github.com/jax-ml/jax/issues/10449
1242,"以下是一个github上的jax下的一个issue, 标题是(Revert: https://github.com/google/jax/pull/10221 (2nd revert))， 内容是 (Revert: https://github.com/google/jax/pull/10221 (2nd revert) Prefer jnp.tile over concatenate. jnp.tile generates a jaxpr like the following: ``` { lambda ; a:i32[720192]. let     b:i32[1,720192] = reshape[dimensions=None new_sizes=(1, 720192)] a     c:i32[720192] = squeeze[dimensions=(0,)] b     d:i32[2,720192] = broadcast_in_dim[       broadcast_dimensions=(1,)       shape=(2, 720192)     ] c     e:i32[1440384] = reshape[dimensions=None new_sizes=(1440384,)] d   in (e,) } ``` whereas lax.concatenate generates the following jaxpr: ``` { lambda ; a:i32[720192]. let     b:i32[1440384] = concatenate[dimension=0] a a   in (b,) } ``` It seems the TPU compiler isn't doing as good a job with laying out memory for the formulation with `jnp.tile`. `reshape` in particular can be difficult for it to handle well, and it's best to avoid it when possible. Since the benefit was marginal (a simpler jaxpr... but is it? Really?) and the cost is real (a user's model broke), we should revert this change.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Revert: https://github.com/google/jax/pull/10221 (2nd revert),"Revert: https://github.com/google/jax/pull/10221 (2nd revert) Prefer jnp.tile over concatenate. jnp.tile generates a jaxpr like the following: ``` { lambda ; a:i32[720192]. let     b:i32[1,720192] = reshape[dimensions=None new_sizes=(1, 720192)] a     c:i32[720192] = squeeze[dimensions=(0,)] b     d:i32[2,720192] = broadcast_in_dim[       broadcast_dimensions=(1,)       shape=(2, 720192)     ] c     e:i32[1440384] = reshape[dimensions=None new_sizes=(1440384,)] d   in (e,) } ``` whereas lax.concatenate generates the following jaxpr: ``` { lambda ; a:i32[720192]. let     b:i32[1440384] = concatenate[dimension=0] a a   in (b,) } ``` It seems the TPU compiler isn't doing as good a job with laying out memory for the formulation with `jnp.tile`. `reshape` in particular can be difficult for it to handle well, and it's best to avoid it when possible. Since the benefit was marginal (a simpler jaxpr... but is it? Really?) and the cost is real (a user's model broke), we should revert this change.",2022-04-25T14:20:55Z,,closed,0,2,https://github.com/jax-ml/jax/issues/10437,cc/  ,Thanks for the heads up. Reverting makes sense. Sorry for the extra work 👍 
2814,"以下是一个github上的jax下的一个issue, 标题是(Improved docs for (default) matmul precision)， 内容是 (1. It is somewhat unintuitive that the default matmul precision is bfloat16 on TPU, especially for users coming from PyTorch/GPU where the default precision is float32. Information regarding the default matrix multiplication precision on TPUs is extremely difficult to find. There is a short section on the README.md within the cloud TPU Colab folder of the JAX repo: https://github.com/google/jax/tree/main/cloud_tpu_colabsbfloat16dtype However, this is somewhat unclear, as it references 'MXUs' without any explanation of what this abbreviation means, and only highlights how the default precision can be changed manually on a opbyop basis by setting `precision=jax.lax.Precision.XXX`. This gives the impression that in order to change the TPU precision to float32, one must insert the keyword argument `precision=jax.lax.Precision.HIGHEST` for every `jax.numpy` operation in one's script.  2. It is difficult to find **how** the default precision can be changed. Performing matmul operations in the default bfloat16 precision can lead to undesirable results. At Hugging Face, we're constantly running into problems with the default fastspeed low precision TPU default, as shown here for example: https://github.com/huggingface/transformers/issues/15754  In the case of changing the default matmul precision, the docs do make mention to the default matmul precision context manager: https://jax.readthedocs.io/en/latest/_autosummary/jax.default_matmul_precision.html However, they do not explicitly state how one can use this context manager to change the default matmul precision (for instance with an example). It's hard to know from the docs that you have to write your code under the context manager as follows: ```python with jax.default_matmul_precision('float32'):    or 'bfloat16' for lowest   ... = foo(...) ``` The docs also brush over three additional methods for changing the default matmul precision, highlighted brilliantly in this PR: https://github.com/google/jax/pull/6143issue836396575 These three methods require no change to one's actual script, just the inclusion of a shell/command line flag or a JAX config change, and are arguably much easier to use and less obtrusive. It would be great if the default matmul precisions for CPU/GPU/TPU were documented, along with what bfloat16, tensorfloat16, float32 precision actually mean for matmul precision in terms of number of passes. It would also be super helpful if all four methods for manipulating the default precision were added to the docs with short examples on how to use them, as done in the aforementioned PR.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Improved docs for (default) matmul precision,"1. It is somewhat unintuitive that the default matmul precision is bfloat16 on TPU, especially for users coming from PyTorch/GPU where the default precision is float32. Information regarding the default matrix multiplication precision on TPUs is extremely difficult to find. There is a short section on the README.md within the cloud TPU Colab folder of the JAX repo: https://github.com/google/jax/tree/main/cloud_tpu_colabsbfloat16dtype However, this is somewhat unclear, as it references 'MXUs' without any explanation of what this abbreviation means, and only highlights how the default precision can be changed manually on a opbyop basis by setting `precision=jax.lax.Precision.XXX`. This gives the impression that in order to change the TPU precision to float32, one must insert the keyword argument `precision=jax.lax.Precision.HIGHEST` for every `jax.numpy` operation in one's script.  2. It is difficult to find **how** the default precision can be changed. Performing matmul operations in the default bfloat16 precision can lead to undesirable results. At Hugging Face, we're constantly running into problems with the default fastspeed low precision TPU default, as shown here for example: https://github.com/huggingface/transformers/issues/15754  In the case of changing the default matmul precision, the docs do make mention to the default matmul precision context manager: https://jax.readthedocs.io/en/latest/_autosummary/jax.default_matmul_precision.html However, they do not explicitly state how one can use this context manager to change the default matmul precision (for instance with an example). It's hard to know from the docs that you have to write your code under the context manager as follows: ```python with jax.default_matmul_precision('float32'):    or 'bfloat16' for lowest   ... = foo(...) ``` The docs also brush over three additional methods for changing the default matmul precision, highlighted brilliantly in this PR: https://github.com/google/jax/pull/6143issue836396575 These three methods require no change to one's actual script, just the inclusion of a shell/command line flag or a JAX config change, and are arguably much easier to use and less obtrusive. It would be great if the default matmul precisions for CPU/GPU/TPU were documented, along with what bfloat16, tensorfloat16, float32 precision actually mean for matmul precision in terms of number of passes. It would also be super helpful if all four methods for manipulating the default precision were added to the docs with short examples on how to use them, as done in the aforementioned PR.",2022-04-22T10:55:55Z,enhancement documentation,open,11,1,https://github.com/jax-ml/jax/issues/10413,Note that default precision for matrixmatrix multiplication is actually now tensorfloat32 on recent Nvidia GPUs: https://github.com/google/jax/issues/14022
2551,"以下是一个github上的jax下的一个issue, 标题是(loops.Scope() arguments to iterate over )， 内容是 (The issue here is the parameters one can mention to the `range` function in Python which is normally: `range(start, stop, step)`.  The example below works  ``` with loops.Scope() as s:     s.arr = np.zeros(shape = (10, 10), dtype = np.float64)     for i in s.range(x.shape[0]):         for j in s.range(x.shape[0]):             print(i,j) ``` whereas if one changes the `start` value of `j` to be `i`:  ``` with loops.Scope() as s:     s.arr = np.zeros(shape = (10, 10), dtype = np.float64)     for i in s.range(x.shape[0]):         for j in s.range(i, x.shape[0]):             print(i,j) ``` we get the following error:  ```  ConcretizationTypeError                   Traceback (most recent call last) /home/zchandani/q_infinity/jax_comparisons.ipynb Cell 10' in ()       3[ s.arr = np.zeros(shape = (10, 10), dtype = np.float64)       ]()5[ for i in s.range(x.shape[0]): > ]()7[     for j in s.range(i, x.shape[0]):       ]()8[         print(i,j) File ~/.cache/pypoetry/virtualenvs/qinfinityO1jTU4hCpy3.9/lib/python3.9/sitepackages/jax/experimental/loops.py:201, in Scope.range(self, first, second, third)     ]()199[ step = 1     ]()200[ if second is not None: > ]()201[   start = int(first)     ]()202[   stop = int(second)     ]()203[ else: File ~/.cache/pypoetry/virtualenvs/qinfinityO1jTU4hCpy3.9/lib/python3.9/sitepackages/jax/core.py:583, in Tracer.__int__(self) > ]()583[ def __int__(self): return self.aval._int(self) File ~/.cache/pypoetry/virtualenvs/qinfinityO1jTU4hCpy3.9/lib/python3.9/sitepackages/jax/core.py:1123, in concretization_function_error..error(self, arg)    ]()1122[ def error(self, arg): > ]()1123[   raise ConcretizationTypeError(arg, fname_context) ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced The problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.ConcretizationTypeError]() ``` A workaround for this is:  ``` with loops.Scope() as s:     s.arr = np.zeros(shape = (10, 10), dtype = np.float64)     u = 1     for i in s.range(x.shape[0]):         u+=1         for j in s.range(u, x.shape[0]):             print(i,j) ``` but curious to see if there is a better way of doing this... Thanks. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,loops.Scope() arguments to iterate over ,"The issue here is the parameters one can mention to the `range` function in Python which is normally: `range(start, stop, step)`.  The example below works  ``` with loops.Scope() as s:     s.arr = np.zeros(shape = (10, 10), dtype = np.float64)     for i in s.range(x.shape[0]):         for j in s.range(x.shape[0]):             print(i,j) ``` whereas if one changes the `start` value of `j` to be `i`:  ``` with loops.Scope() as s:     s.arr = np.zeros(shape = (10, 10), dtype = np.float64)     for i in s.range(x.shape[0]):         for j in s.range(i, x.shape[0]):             print(i,j) ``` we get the following error:  ```  ConcretizationTypeError                   Traceback (most recent call last) /home/zchandani/q_infinity/jax_comparisons.ipynb Cell 10' in ()       3[ s.arr = np.zeros(shape = (10, 10), dtype = np.float64)       ]()5[ for i in s.range(x.shape[0]): > ]()7[     for j in s.range(i, x.shape[0]):       ]()8[         print(i,j) File ~/.cache/pypoetry/virtualenvs/qinfinityO1jTU4hCpy3.9/lib/python3.9/sitepackages/jax/experimental/loops.py:201, in Scope.range(self, first, second, third)     ]()199[ step = 1     ]()200[ if second is not None: > ]()201[   start = int(first)     ]()202[   stop = int(second)     ]()203[ else: File ~/.cache/pypoetry/virtualenvs/qinfinityO1jTU4hCpy3.9/lib/python3.9/sitepackages/jax/core.py:583, in Tracer.__int__(self) > ]()583[ def __int__(self): return self.aval._int(self) File ~/.cache/pypoetry/virtualenvs/qinfinityO1jTU4hCpy3.9/lib/python3.9/sitepackages/jax/core.py:1123, in concretization_function_error..error(self, arg)    ]()1122[ def error(self, arg): > ]()1123[   raise ConcretizationTypeError(arg, fname_context) ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Traced The problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.ConcretizationTypeError]() ``` A workaround for this is:  ``` with loops.Scope() as s:     s.arr = np.zeros(shape = (10, 10), dtype = np.float64)     u = 1     for i in s.range(x.shape[0]):         u+=1         for j in s.range(u, x.shape[0]):             print(i,j) ``` but curious to see if there is a better way of doing this... Thanks. ",2022-04-21T12:33:19Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/10400,`jax.experimental.loops` is being deprecated https://github.com/google/jax/blob/main/CHANGELOG.mdjax036april122022 You can use `jax.lax.fori_loop` instead.
410,"以下是一个github上的jax下的一个issue, 标题是(vmap + array-indexing is very slow on TPU)， 内容是 (I spotted this bug when I was running a benchmark script written by myself in https://github.com/google/flax/issues/2051issuecomment1103801386 (please see the script in the issue comment). )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,vmap + array-indexing is very slow on TPU,I spotted this bug when I was running a benchmark script written by myself in https://github.com/google/flax/issues/2051issuecomment1103801386 (please see the script in the issue comment). ,2022-04-20T11:44:21Z,enhancement performance XLA TPU,open,1,2,https://github.com/jax-ml/jax/issues/10378,"Thanks, I think this is a missing optimization in XLA. (Filed Google bug b/229839204) It may be hard for them to fix, so I'd definitely suggest using the non`vmap` formulation as a workaround!",This is still an issue. I have found that indexing arrays is magnitudes slower on a TPU than on a CPU. Is there any update to the missing XLA optimisation?
276,"以下是一个github上的jax下的一个issue, 标题是([linalg] Adds `full_matrices` option to TPU SVD.)， 内容是 ([linalg] Adds `full_matrices` option to TPU SVD.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[linalg] Adds `full_matrices` option to TPU SVD.,[linalg] Adds `full_matrices` option to TPU SVD.,2022-04-19T20:52:53Z,,closed,0,1,https://github.com/jax-ml/jax/issues/10370,  
366,"以下是一个github上的jax下的一个issue, 标题是([fix: typing] Explicitly rexport `KeyArray` in `jax.random` )， 内容是 (Explicitly reexports PRNGKeyArray as KeyArray in accordance with PEP 484 See also: https://github.com/python/mypy/issues/11706)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[fix: typing] Explicitly rexport `KeyArray` in `jax.random` ,Explicitly reexports PRNGKeyArray as KeyArray in accordance with PEP 484 See also: https://github.com/python/mypy/issues/11706,2022-04-19T16:54:51Z,pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/10362,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). For more information, open the CLA check for this pull request.","Tiny fix that improves compatibility with strict linters (mypy, pylint)",Thanks!
515,"以下是一个github上的jax下的一个issue, 标题是(Prevent negative output shapes in shape inference for reduce_window.)， 内容是 (The example in CC(Lowlevel error in forwardmode AD of `lax.reduce_window_sum_p`) yields a negative shape in a jaxpr, which doesn't go well. Clamp the size of the output when the dilated window is larger than the input. Fixes https://github.com/google/jax/issues/10315)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Prevent negative output shapes in shape inference for reduce_window.,"The example in CC(Lowlevel error in forwardmode AD of `lax.reduce_window_sum_p`) yields a negative shape in a jaxpr, which doesn't go well. Clamp the size of the output when the dilated window is larger than the input. Fixes https://github.com/google/jax/issues/10315",2022-04-19T02:26:02Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10351
4012,"以下是一个github上的jax下的一个issue, 标题是(Bump sphinx-autodoc-typehints from 1.11.1 to 1.18.1)， 内容是 (Bumps sphinxautodoctypehints from 1.11.1 to 1.18.1.  Changelog Sourced from sphinxautodoctypehints's changelog.  1.18.1  Fix mocked module import not working when used as guarded import  1.18.0  Support and require nptyping&gt;=2 Handle UnionType  1.17.1  Mark it as requiring nptyping&lt;2  1.17.0  Add typehints_use_rtype option Handles TypeError when getting source code via inspect  1.16.0  Add support for type subscriptions with multiple elements, where one or more elements are tuples; e.g., nptyping.NDArray[(Any, ...), nptyping.Float] Fix bug for arbitrary types accepting singleton subscriptions; e.g., nptyping.Float[64] Resolve forward references Expand and better handle TypeVar Add intershpinx reference link for ... to Ellipsis (as is just an alias)  1.15.3  Prevents reaching inner blocks that contains if TYPE_CHECKING  1.15.2  Log a warning instead of crashing when a type guard import fails to resolve When resolving type guard imports if the target module does not have source code (such is the case for Cextension modules) do nothing instead of crashing  1.15.1  Fix fully_qualified should be typehints_fully_qualified  1.15.0  Resolve type guard imports before evaluating annotations for objects Remove set_type_checking_flag flag as this is now done by default Fix crash when the inspect module returns an invalid python syntax source Made formatting function configurable using the option typehints_formatter  1.14.1   ... (truncated)   Commits  73aa9b6 Fix mock imports on guarded imports ( CC(Test more Numpy ops for complex types.)) 4d5867d Handle UnionType ( CC(Batching rule for 'sort_key_val' not implemented)) 13ca2b4 [precommit.ci] precommit autoupdate ( CC(Inplace updating can lead to counterintuitive behavior)) def37f7 Support and require nptyping 2 ede082a Require nptyping&lt;2 ( CC(Failing lax_numpy_indexing_test.py tests on Python 3.7)) f9219b2 [precommit.ci] precommit autoupdate ( CC(jax.random.randint range must be valid)) a9b9023 Fix typos ( CC(add examples and docs for how to use lax.conv functions)) 1ef8488 Release 1.17.0 aa345ca Add typehints_use_rtype option ( CC(Update XLA release to fix build problem.)) a022d1a inspect.getsource can raise TypeError ( CC(Update XLA release to include XLA Gather and Scatter Python bindings..)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bump sphinx-autodoc-typehints from 1.11.1 to 1.18.1,"Bumps sphinxautodoctypehints from 1.11.1 to 1.18.1.  Changelog Sourced from sphinxautodoctypehints's changelog.  1.18.1  Fix mocked module import not working when used as guarded import  1.18.0  Support and require nptyping&gt;=2 Handle UnionType  1.17.1  Mark it as requiring nptyping&lt;2  1.17.0  Add typehints_use_rtype option Handles TypeError when getting source code via inspect  1.16.0  Add support for type subscriptions with multiple elements, where one or more elements are tuples; e.g., nptyping.NDArray[(Any, ...), nptyping.Float] Fix bug for arbitrary types accepting singleton subscriptions; e.g., nptyping.Float[64] Resolve forward references Expand and better handle TypeVar Add intershpinx reference link for ... to Ellipsis (as is just an alias)  1.15.3  Prevents reaching inner blocks that contains if TYPE_CHECKING  1.15.2  Log a warning instead of crashing when a type guard import fails to resolve When resolving type guard imports if the target module does not have source code (such is the case for Cextension modules) do nothing instead of crashing  1.15.1  Fix fully_qualified should be typehints_fully_qualified  1.15.0  Resolve type guard imports before evaluating annotations for objects Remove set_type_checking_flag flag as this is now done by default Fix crash when the inspect module returns an invalid python syntax source Made formatting function configurable using the option typehints_formatter  1.14.1   ... (truncated)   Commits  73aa9b6 Fix mock imports on guarded imports ( CC(Test more Numpy ops for complex types.)) 4d5867d Handle UnionType ( CC(Batching rule for 'sort_key_val' not implemented)) 13ca2b4 [precommit.ci] precommit autoupdate ( CC(Inplace updating can lead to counterintuitive behavior)) def37f7 Support and require nptyping 2 ede082a Require nptyping&lt;2 ( CC(Failing lax_numpy_indexing_test.py tests on Python 3.7)) f9219b2 [precommit.ci] precommit autoupdate ( CC(jax.random.randint range must be valid)) a9b9023 Fix typos ( CC(add examples and docs for how to use lax.conv functions)) 1ef8488 Release 1.17.0 aa345ca Add typehints_use_rtype option ( CC(Update XLA release to fix build problem.)) a022d1a inspect.getsource can raise TypeError ( CC(Update XLA release to include XLA Gather and Scatter Python bindings..)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-04-18T17:09:00Z,dependencies python,closed,0,3,https://github.com/jax-ml/jax/issues/10340, ignore, ignore,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1702,"以下是一个github上的jax下的一个issue, 标题是(`TracerArrayConversionError` with numpy in the function for autodiff)， 内容是 (I am trying the autodiff functionality in `JAX`.  In a function to be taken derivatives, I have to use `jax.numpy` to call some math operations such as `exp` instead of using the one from `numpy`.  Otherwise it will give error messages below.  ``` import jax import jax.numpy as jnp import numpy as np ``` ``` def model_np(W):     comp1 = np.exp(W[0])     comp2 = np.exp(np.exp(W[1]))     comp3 = np.exp(W[2])     comp4 = np.exp(np.exp(W[3]))     return comp1 * comp2 + comp3 * comp4 val = np.array([0.69, 0.69, 1.6, 1.6]) model_grad = jax.jit(jax.jacfwd(model_np, argnums=0)) model_grad(val) ``` ``` [TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Tracedwith with   primal = Tracedwith   tangent = Tracedwith with     val = Tracedwith     batch_dim = 0]() ``` ``` def model_jnp(W):     comp1 = jnp.exp(W[0])     comp2 = jnp.exp(jnp.exp(W[1]))     comp3 = jnp.exp(W[2])     comp4 = jnp.exp(jnp.exp(W[3]))     return comp1 * comp2 + comp3 * comp4 val = np.array([0.69, 0.69, 1.6, 1.6]) model_grad = jax.jit(jax.jacfwd(model_jnp, argnums=0)) model_grad(val) DeviceArray([ 0.2715211 , 0.5413358 ,  0.1649857 , 0.03331004], dtype=float32) ``` The function calling autodiff is using `numpy` in all other places, and there is no issue to calculate other results if `model_np` changes to use`jax.numpy` and others using `numpy`, therefore, I am not sure it is a bug or implemented intentionally. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`TracerArrayConversionError` with numpy in the function for autodiff,"I am trying the autodiff functionality in `JAX`.  In a function to be taken derivatives, I have to use `jax.numpy` to call some math operations such as `exp` instead of using the one from `numpy`.  Otherwise it will give error messages below.  ``` import jax import jax.numpy as jnp import numpy as np ``` ``` def model_np(W):     comp1 = np.exp(W[0])     comp2 = np.exp(np.exp(W[1]))     comp3 = np.exp(W[2])     comp4 = np.exp(np.exp(W[3]))     return comp1 * comp2 + comp3 * comp4 val = np.array([0.69, 0.69, 1.6, 1.6]) model_grad = jax.jit(jax.jacfwd(model_np, argnums=0)) model_grad(val) ``` ``` [TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Tracedwith with   primal = Tracedwith   tangent = Tracedwith with     val = Tracedwith     batch_dim = 0]() ``` ``` def model_jnp(W):     comp1 = jnp.exp(W[0])     comp2 = jnp.exp(jnp.exp(W[1]))     comp3 = jnp.exp(W[2])     comp4 = jnp.exp(jnp.exp(W[3]))     return comp1 * comp2 + comp3 * comp4 val = np.array([0.69, 0.69, 1.6, 1.6]) model_grad = jax.jit(jax.jacfwd(model_jnp, argnums=0)) model_grad(val) DeviceArray([ 0.2715211 , 0.5413358 ,  0.1649857 , 0.03331004], dtype=float32) ``` The function calling autodiff is using `numpy` in all other places, and there is no issue to calculate other results if `model_np` changes to use`jax.numpy` and others using `numpy`, therefore, I am not sure it is a bug or implemented intentionally. ",2022-04-17T13:50:02Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/10330,"Yes, that is correct: JAX transforms are only compatible with JAX functions, not NumPy functions, so you should use `jax.numpy` instead of `numpy` if you wish to use `jax.jit`, `jax.jacfwd`, and other function transformations.","> Yes, that is correct: JAX transforms are only compatible with JAX functions, not NumPy functions, so you should use `jax.numpy` instead of `numpy` if you wish to use `jax.jit`, `jax.jacfwd`, and other function transformations. Since the input does not necessarily need `jax.numpy.array` as in my example, but the function needs `jax.numpy`, wouldn't it be a bit inconsistent? ","Ah, I see  the important piece here is that transformations (like `jit` and `jacfwd`) act on functions – so if you want to transform a function it must be implemented in terms of JAX functions. For convenience, if a JAX function receives a numpy array as an argument, it will implicitly convert it to a JAX array if necessary."
584,"以下是一个github上的jax下的一个issue, 标题是(Cannot change `jax_platform_name` on the fly)， 内容是 (I am using `jax.config.update('jax_platform_name', 'cpu')` to change the device placement. ```python import jax import jax.numpy as np a = np.zeros((128, 128)) print(a.device())   TPU jax.config.update('jax_platform_name', 'cpu') a = np.zeros((128, 128)) print(a.device())   TPU, expected CPU ``` Related: CC(Context manager to control placement of jax arrays))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot change `jax_platform_name` on the fly,"I am using `jax.config.update('jax_platform_name', 'cpu')` to change the device placement. ```python import jax import jax.numpy as np a = np.zeros((128, 128)) print(a.device())   TPU jax.config.update('jax_platform_name', 'cpu') a = np.zeros((128, 128)) print(a.device())   TPU, expected CPU ``` Related: CC(Context manager to control placement of jax arrays)",2022-04-16T04:45:55Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/10320,"`jax_platform_name` isn't intended to be set at run time. I suspect what you actually want to control is the default device placement, for which  was adding a contextmanagerlike API. (I don't believe it landed yet, though.)", did https://github.com/google/jax/pull/9118 fix this?,"Yes, you should be able to use `jax.default_device` to do this more safely. It doesn't support a platform name (yet?), so you have to use: `with jax.default_device(jax.devices(""cpu"")[0])`",Let's consider this solved! Docs on `jax.default_device` here.," Hi, I am very interested by the feature you developped. I am still struggling to use it. Would you mind explaining how it works please? Here is the behavior I don't understand: ```Python import jax print(jax.numpy.ones(3).device()) jax.default_device(jax.devices(""cpu"")[0]) print(jax.numpy.ones(3).device())  ``` Output ```Bash gpu:0 gpu:0 ``` I would expect: ```Bash gpu:0 TFRT_CPU_0 ``` I am using jax==0.3.22 and jaxlib==0.3.22+cuda11.cudnn82. Using __device_put__, the behavior is the one that I expect: ```Python import jax a = jax.numpy.ones(3) print(a.device()) a = jax.device_put(a, jax.devices(""cpu"")[0]) print(a.device()) ``` Output ```Bash gpu:0 TFRT_CPU_0 ```",  The correct way is: ```python import jax import jax.numpy as jnp print(jnp.ones(3).device()) with jax.default_device(jax.devices('cpu')[0]):     print(jnp.ones(3).device())  ```," Amazing, thanks :)"
4413,"以下是一个github上的jax下的一个issue, 标题是(Generic Protocol for transformed function)， 内容是 (With `ParamSpec` we can completely annotate `jit` signature. Thus type checking can be performed with transformed function. (and `lower`) ```python T = TypeVar(""T"") P = ParamSpec(""P"") def jit(fun: Callable[P, T], ...) > stages.Wrapped[P, T]:   ...  stages.py T_co = TypeVar(""T_co"", covariant=True) P_contra = ParamSpec(""P_contra"", contravariant=True) class Wrapped(Protocol[P_contra, T_co]):   def __call__(self, *args: P_contra.args, **kwargs: P_contra.kwargs) > T_co:     ...   def lower(self, *args: P_contra.args, **kwargs: P_contra.kwargs) > Lowered:     ... ``` Just like CC(generic protocol for compilation wrappers) , and it works almost perfectly with Visual Studio Code + pylance, signature(including parameters name) of function is fully preserved. However, `mypy` currently doesn't handle `partial` + functiontransform correctly. https://github.com/python/mypy/issues/12593 We can wait `mypy` fix this bug, or make some workaround.  Some workaround option:  make `jit` a polymorphism decorator + decorator factory. toy example ```python from typing import TypeVar, Callable, overload from typing_extensions import ParamSpec, Protocol, Literal T_co = TypeVar('T_co', covariant=True) T = TypeVar('T') P = ParamSpec('P') class Wrapped(Protocol[P, T_co]):   def __call__(self, *args: P.args, **kwargs: P.kwargs) > T_co:     pass  def jit(f: Callable[P, T], *, a: bool) > Wrapped[P, T]:     ...  def jit(f: Literal[None] = None, *, a: bool) > Callable[[Callable[P, T]], Wrapped[P, T]]:     ... def jit(f=None, *, a):     if f is None:         return lambda f: jit(f, a=a)      original code here     return f (a=True) def f(x: int, y: int):     return x + y f(0, 1) mypy ok f(0, 'a')  mypy error  Argument 2 to ""__call__"" of ""Wrapped"" has incompatible type ""str""; expected ""int"" ``` This break ""Our general philosophy is to make the JAX core API as simple and explicit as possible""    https://github.com/google/jax/issues/184issuecomment582029342 I personally like this option, but it breaks the philosophy of JAX.  add a dedicated decorator factory function and use it internally toy example ```python from typing import TypeVar, Callable from typing_extensions import ParamSpec, Protocol T_co = TypeVar('T_co', covariant=True) T = TypeVar('T') P = ParamSpec('P') class Wrapped(Protocol[P, T_co]):   def __call__(self, *args: P.args, **kwargs: P.kwargs) > T_co:     pass def jit(f: Callable[P, T], *, a: bool) > Wrapped[P, T]:      original code here     return f def _make_jit(*, a: bool) > Callable[[Callable[P, T]], Wrapped[P, T]]:     return lambda f: jit(f, a=a)  type: ignore  a mypy bug fixed in mypy0.950 (a=True) def f(x: int, y: int):     return x + y f(0, 0)  mypy ok f(0, 'a')  mypy error  Argument 2 to ""__call__"" of ""Wrapped"" has incompatible type ""str""; expected ""int"" ``` I think this option is satisfactory for everyone. (also can be a public API) In addtion, we can write a function that can create decorator factory function without losing type information, but current(even the next release) `mypy` has bug in this case as well. https://github.com/python/mypy/issues/12595 ```python from typing import TypeVar, Callable from typing_extensions import Concatenate, ParamSpec, Protocol T_co = TypeVar('T_co', covariant=True) T = TypeVar('T') P = ParamSpec('P') class Wrapped(Protocol[P, T_co]):   def __call__(self, *args: P.args, **kwargs: P.kwargs) > T_co:     pass def jit(f: Callable[P, T], *, a: bool) > Wrapped[P, T]:      original code here     return f _T = TypeVar('_T') _P = ParamSpec('_P') def _make_factory(     transform: Callable[Concatenate[Callable[P, T], _P], _T]  type: ignore      mypy can't recognize Concatenate, mypy0.950 has fixed it ) > Callable[_P, Callable[[Callable[P, T]], _T]]:     return lambda *args, **kwargs: lambda f: transform(f, *args, **kwargs) _make_jit: Callable[..., Callable[[Callable[P, T]], Wrapped[P, T]]] = _make_factory(jit)  mypy need a type annotation, but this annotation will erase signature of jit > bad for vscode  mypy0.950 has not fixed it (a=True) def f(x: int, y: int):     return x + y f(0, 0)  mypy ok f(0, 'a')  mypy error Argument 2 to ""__call__"" of ""Wrapped"" has incompatible type ""str""; expected ""int"" ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Generic Protocol for transformed function,"With `ParamSpec` we can completely annotate `jit` signature. Thus type checking can be performed with transformed function. (and `lower`) ```python T = TypeVar(""T"") P = ParamSpec(""P"") def jit(fun: Callable[P, T], ...) > stages.Wrapped[P, T]:   ...  stages.py T_co = TypeVar(""T_co"", covariant=True) P_contra = ParamSpec(""P_contra"", contravariant=True) class Wrapped(Protocol[P_contra, T_co]):   def __call__(self, *args: P_contra.args, **kwargs: P_contra.kwargs) > T_co:     ...   def lower(self, *args: P_contra.args, **kwargs: P_contra.kwargs) > Lowered:     ... ``` Just like CC(generic protocol for compilation wrappers) , and it works almost perfectly with Visual Studio Code + pylance, signature(including parameters name) of function is fully preserved. However, `mypy` currently doesn't handle `partial` + functiontransform correctly. https://github.com/python/mypy/issues/12593 We can wait `mypy` fix this bug, or make some workaround.  Some workaround option:  make `jit` a polymorphism decorator + decorator factory. toy example ```python from typing import TypeVar, Callable, overload from typing_extensions import ParamSpec, Protocol, Literal T_co = TypeVar('T_co', covariant=True) T = TypeVar('T') P = ParamSpec('P') class Wrapped(Protocol[P, T_co]):   def __call__(self, *args: P.args, **kwargs: P.kwargs) > T_co:     pass  def jit(f: Callable[P, T], *, a: bool) > Wrapped[P, T]:     ...  def jit(f: Literal[None] = None, *, a: bool) > Callable[[Callable[P, T]], Wrapped[P, T]]:     ... def jit(f=None, *, a):     if f is None:         return lambda f: jit(f, a=a)      original code here     return f (a=True) def f(x: int, y: int):     return x + y f(0, 1) mypy ok f(0, 'a')  mypy error  Argument 2 to ""__call__"" of ""Wrapped"" has incompatible type ""str""; expected ""int"" ``` This break ""Our general philosophy is to make the JAX core API as simple and explicit as possible""    https://github.com/google/jax/issues/184issuecomment582029342 I personally like this option, but it breaks the philosophy of JAX.  add a dedicated decorator factory function and use it internally toy example ```python from typing import TypeVar, Callable from typing_extensions import ParamSpec, Protocol T_co = TypeVar('T_co', covariant=True) T = TypeVar('T') P = ParamSpec('P') class Wrapped(Protocol[P, T_co]):   def __call__(self, *args: P.args, **kwargs: P.kwargs) > T_co:     pass def jit(f: Callable[P, T], *, a: bool) > Wrapped[P, T]:      original code here     return f def _make_jit(*, a: bool) > Callable[[Callable[P, T]], Wrapped[P, T]]:     return lambda f: jit(f, a=a)  type: ignore  a mypy bug fixed in mypy0.950 (a=True) def f(x: int, y: int):     return x + y f(0, 0)  mypy ok f(0, 'a')  mypy error  Argument 2 to ""__call__"" of ""Wrapped"" has incompatible type ""str""; expected ""int"" ``` I think this option is satisfactory for everyone. (also can be a public API) In addtion, we can write a function that can create decorator factory function without losing type information, but current(even the next release) `mypy` has bug in this case as well. https://github.com/python/mypy/issues/12595 ```python from typing import TypeVar, Callable from typing_extensions import Concatenate, ParamSpec, Protocol T_co = TypeVar('T_co', covariant=True) T = TypeVar('T') P = ParamSpec('P') class Wrapped(Protocol[P, T_co]):   def __call__(self, *args: P.args, **kwargs: P.kwargs) > T_co:     pass def jit(f: Callable[P, T], *, a: bool) > Wrapped[P, T]:      original code here     return f _T = TypeVar('_T') _P = ParamSpec('_P') def _make_factory(     transform: Callable[Concatenate[Callable[P, T], _P], _T]  type: ignore      mypy can't recognize Concatenate, mypy0.950 has fixed it ) > Callable[_P, Callable[[Callable[P, T]], _T]]:     return lambda *args, **kwargs: lambda f: transform(f, *args, **kwargs) _make_jit: Callable[..., Callable[[Callable[P, T]], Wrapped[P, T]]] = _make_factory(jit)  mypy need a type annotation, but this annotation will erase signature of jit > bad for vscode  mypy0.950 has not fixed it (a=True) def f(x: int, y: int):     return x + y f(0, 0)  mypy ok f(0, 'a')  mypy error Argument 2 to ""__call__"" of ""Wrapped"" has incompatible type ""str""; expected ""int"" ```",2022-04-15T18:56:48Z,enhancement,open,1,10,https://github.com/jax-ml/jax/issues/10311,    Could you please give some advice?,"I commented in CC(Attempt to add Generic Wrapped) about option 2 (""dedicated factory function""), but it possibly applies to option 1 (""polymorphic jit"") as well. I think we have a rather strict constraint to maintain: keep `jit` as it is (no double meanings based on how it is called), and allow for `partial(jit, ...)`. We have many downstream dependencies on `jit` at this point (many of which take `partial`), and possibly many more to come, so we want to keep this as simple and explicit as possible. These are both clever approaches and I'm learning simply by reading them. Thanks for thinking about this. Are there any other approaches you can think of? Following your comment here, does the issue with mypy+`partial` seem temporary? If so, can we make this work for `jit` without `partial` for now, without trying to work around mypy's `partial` bug, and anticipate a future fix?","Yes, I think the `mypy` issue is temporary. Do you have any idea about ""make this work fof `jit` without `partial`""? Can we make the `jit` with `partial` be specially typed?","My suggestion is that we pretend the mypy issue is already fixed, so we do not try to work around it (for now). In that case, what would be the best approach to annotation?","There is something surprising: as the last case of https://github.com/python/mypy/issues/12593 ,`mypy` can't handle `partial` + `jit` with typevar annotated `fun` correctly. But IIRC, we use this annotation a few month ago.","In that case, maybe we can annotate `jit` returntype as `Union[Wrapped[P, T], Wrapped]`, and annotated `_make_jit` returntype as `Wrapped[P, T]`. As a result, any downstream codebase won't be broken, and we can do typechecking for jitted JAX functions with `_make_jit` instead of `partial`. And users using vscode can enjoy a nice jitted function signature hint.","When the mypy bug is fixed, would we be able to remove `_make_jit` and only annotate `jit`'s return type as `Wrapped[P, T]`?","Yes. But I think it is not bad to have `_make_jit` `mypy` won't report an error if `partial` pass wrong argument. ```python def f(x: int, y: int):   return x + y g = partial(f, z=0) h = partial(f, 'a')  no error with mypy ```","Ah, I find that `mypy` support for `partial` is too weak... ```python def jit(     fun: Callable[P, T],     *,     static_argnums: Union[int, Iterable[int], None] = None,     static_argnames: Union[str, Iterable[str], None] = None,     device: Optional[xc.Device] = None,     backend: Optional[str] = None,     donate_argnums: Union[int, Iterable[int]] = (),     inline: bool = False,     abstracted_axes: Optional[Any] = None,   ) > Union[stages.GenericWrapped[P, T], stages.Wrapped]: ``` ```python class GenericWrapped(Protocol[P, T_co]):      def __call__(self, *args: P.args, **kwargs: P.kwargs) > T_co:     """"""Executes the wrapped function, lowering and compiling as needed.""""""      def __call__(self, *args: Any, **kwargs: Any) > T_co: ...  without this mypy will expect P.args      def lower(self, *args: P.args, **kwargs: P.kwargs) > Lowered:     """"""Lower this function for the given arguments.     A lowered function is staged out of Python and translated to a     compiler's input language, possibly in a backenddependent     manner. It is ready for compilation but not yet compiled.     Returns:       A ``Lowered`` instance representing the lowering.     """"""      def lower(self, *args: Any, **kwargs: Any) > Lowered: ... class Wrapped(Protocol):   def __call__(self, *args: Any, **kwargs: Any) > Any: ...   def lower(self, *args: Any, **kwargs: Any) > Lowered: ... ``` And ```python from jax import jit from functools import partial (jit) def f(x: float, y: float):     return x + y x: float = f(0.0, 0.0) ``` ``` error: Incompatible types in assignment (expression has type ""Union[T, Any]"", variable has type ""float"")  [assignment] ``` 😭 This actually is caused by that `mypy` will erase the parameter specification of `func` for `partial`, only retain the return type. So the `TypeVar` inside `func` is erased.","Due to the `mypy`‘s limitations, we can only leave a nongeneric `jit` here, while using `_make_jit` internally. Or we can expose `_make_jit` as a public API for people who want to preserve function signature with JIT."
422,"以下是一个github上的jax下的一个issue, 标题是(Fix some straggling name stack bugs)， 内容是 (This PR fixes some straggling namestack issues. JAX tests will now all pass when name stack is enabled. What's left to do is fix nonJAX usages of the namedcall primitive (e.g. Haiku's jaxprmunging utilities))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Fix some straggling name stack bugs,This PR fixes some straggling namestack issues. JAX tests will now all pass when name stack is enabled. What's left to do is fix nonJAX usages of the namedcall primitive (e.g. Haiku's jaxprmunging utilities),2022-04-14T22:34:28Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10296
2742,"以下是一个github上的jax下的一个issue, 标题是(ImportError: cannot import name 'index' from 'jax.ops')， 内容是 (I'm trying to work through the tutorial here: http://secondearths.sakura.ne.jp/exojax/tutorials/optimize_spectrum_JAXopt.html And when I get to this block of code, I get an error. I'm using a GPU on google colab and I've confirmed that it's running.  ```python from exojax.spec.lpf import xsmatrix from exojax.spec.exomol import gamma_exomol from exojax.spec.hitran import SijT, doppler_sigma, gamma_natural, gamma_hitran from exojax.spec.hitrancia import read_cia, logacia from exojax.spec.rtransfer import rtrun, dtauM, dtauCIA, nugrid from exojax.spec import planck, response from exojax.spec.lpf import xsvector from exojax.spec import molinfo from exojax.utils.constants import RJ, pc, Rs, c ``` pip show jax outputs: Name: jax Version: 0.3.4 Summary: Differentiate, compile, and transform Numpy code. Homepage: https://github.com/google/jax Author: JAX team Authoremail: jaxdev.com License: Apache2.0 Location: /usr/local/lib/python3.7/distpackages Requires: typingextensions, scipy, abslpy, opteinsum, numpy Requiredby: numpyro, jaxopt, exojax Full error messages/tracebacks:  ImportError                               Traceback (most recent call last) [](https://localhost:8080/) in ()       4 from exojax.spec.hitrancia import read_cia, logacia       5 from exojax.spec.rtransfer import rtrun, dtauM, dtauCIA, nugrid > 6 from exojax.spec import planck, response       7 from exojax.spec.lpf import xsvector       8 from exojax.spec import molinfo /usr/local/lib/python3.7/distpackages/exojax/spec/__init__.py in ()      15 )      16  > 17 from exojax.spec.autospec import (      18     AutoXS,      19     AutoRT, /usr/local/lib/python3.7/distpackages/exojax/spec/autospec.py in ()       1 """"""Automatic Opacity and Spectrum Generator.""""""       2 import time > 3 from exojax.spec import defmol, defcia, moldb, contdb, planck, molinfo, lpf, dit, modit, initspec, response       4 from exojax.spec.opacity import xsection       5 from exojax.spec.hitran import SijT, doppler_sigma,  gamma_natural, gamma_hitran, normalized_doppler_sigma /usr/local/lib/python3.7/distpackages/exojax/spec/dit.py in ()      10 from jax.lax import scan      11 from exojax.spec.ditkernel import fold_voigt_kernel > 12 from jax.ops import index as joi      13 from exojax.spec.atomll import padding_2Darray_for_each_atom      14 from exojax.spec.rtransfer import dtauM ImportError: cannot import name 'index' from 'jax.ops' (/usr/local/lib/python3.7/distpackages/jax/ops/__init__.py) I'm not sure how to resolve this issue.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,ImportError: cannot import name 'index' from 'jax.ops',"I'm trying to work through the tutorial here: http://secondearths.sakura.ne.jp/exojax/tutorials/optimize_spectrum_JAXopt.html And when I get to this block of code, I get an error. I'm using a GPU on google colab and I've confirmed that it's running.  ```python from exojax.spec.lpf import xsmatrix from exojax.spec.exomol import gamma_exomol from exojax.spec.hitran import SijT, doppler_sigma, gamma_natural, gamma_hitran from exojax.spec.hitrancia import read_cia, logacia from exojax.spec.rtransfer import rtrun, dtauM, dtauCIA, nugrid from exojax.spec import planck, response from exojax.spec.lpf import xsvector from exojax.spec import molinfo from exojax.utils.constants import RJ, pc, Rs, c ``` pip show jax outputs: Name: jax Version: 0.3.4 Summary: Differentiate, compile, and transform Numpy code. Homepage: https://github.com/google/jax Author: JAX team Authoremail: jaxdev.com License: Apache2.0 Location: /usr/local/lib/python3.7/distpackages Requires: typingextensions, scipy, abslpy, opteinsum, numpy Requiredby: numpyro, jaxopt, exojax Full error messages/tracebacks:  ImportError                               Traceback (most recent call last) [](https://localhost:8080/) in ()       4 from exojax.spec.hitrancia import read_cia, logacia       5 from exojax.spec.rtransfer import rtrun, dtauM, dtauCIA, nugrid > 6 from exojax.spec import planck, response       7 from exojax.spec.lpf import xsvector       8 from exojax.spec import molinfo /usr/local/lib/python3.7/distpackages/exojax/spec/__init__.py in ()      15 )      16  > 17 from exojax.spec.autospec import (      18     AutoXS,      19     AutoRT, /usr/local/lib/python3.7/distpackages/exojax/spec/autospec.py in ()       1 """"""Automatic Opacity and Spectrum Generator.""""""       2 import time > 3 from exojax.spec import defmol, defcia, moldb, contdb, planck, molinfo, lpf, dit, modit, initspec, response       4 from exojax.spec.opacity import xsection       5 from exojax.spec.hitran import SijT, doppler_sigma,  gamma_natural, gamma_hitran, normalized_doppler_sigma /usr/local/lib/python3.7/distpackages/exojax/spec/dit.py in ()      10 from jax.lax import scan      11 from exojax.spec.ditkernel import fold_voigt_kernel > 12 from jax.ops import index as joi      13 from exojax.spec.atomll import padding_2Darray_for_each_atom      14 from exojax.spec.rtransfer import dtauM ImportError: cannot import name 'index' from 'jax.ops' (/usr/local/lib/python3.7/distpackages/jax/ops/__init__.py) I'm not sure how to resolve this issue.",2022-04-14T20:46:53Z,,closed,0,1,https://github.com/jax-ml/jax/issues/10293,"Thanks for the report! This was deprecated in JAX version 0.2.22 and removed in version 0.3.2 (see https://github.com/google/jax/blob/main/CHANGELOG.mdjax032march162022) Instead of `jax.ops.index`, we recommend `jnp.index_exp` (which is essentially identical). If you're depending on another project that is attempting to import this, you'll have to downgrade to JAX 0.3.1 or older until the package using it can be updated."
2045,"以下是一个github上的jax下的一个issue, 标题是(Low-level error when calling `lax.convert_element_type_p` with uncanocinalized `new_dtype`)， 内容是 (Tiny repro: https://colab.research.google.com/gist/icml2022anon/a368de2bbbd7ed6a6b4ea12a6966683c/jax_type_error.ipynb ```python from jax import numpy as np, lax, jacfwd a = np.ones((1,), dtype=np.float16) f = lambda x: lax.convert_element_type_p.bind(x, new_dtype=np.float64, weak_type=False) jacfwd(f)(a) ``` causes ```python WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)  UnfilteredStackTrace                      Traceback (most recent call last)  in ()       5  > 6 jacfwd(f)(a) 27 frames UnfilteredStackTrace: RuntimeError: UNKNOWN: :0: error: type of return operand 0 ('tensor') doesn't match function result type ('tensor') in function  :0: note: see current operation: ""func.return""(%0) : (tensor) > () The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: RuntimeError                              Traceback (most recent call last) /usr/local/lib/python3.7/distpackages/jax/_src/numpy/lax_numpy.py in transpose(a, axes)     494   _check_arraylike(""transpose"", a)     495   axes = np.arange(ndim(a))[::1] if axes is None else axes > 496   return lax.transpose(a, axes)     497      498  RuntimeError: UNKNOWN: :0: error: type of return operand 0 ('tensor') doesn't match function result type ('tensor') in function  :0: note: see current operation: ""func.return""(%0) : (tensor) > () ``` As  told me in the chat, this can be fixed via 1. call lax.convert_element_type (or even lax._convert_element_type if you really need to control weak_type) rather than bind 2. pass new_dtype=jax.dtypes.canonicalize_dtype(np.float64) 3. enable the x64 flag but is still considered a bug.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,Low-level error when calling `lax.convert_element_type_p` with uncanocinalized `new_dtype`,"Tiny repro: https://colab.research.google.com/gist/icml2022anon/a368de2bbbd7ed6a6b4ea12a6966683c/jax_type_error.ipynb ```python from jax import numpy as np, lax, jacfwd a = np.ones((1,), dtype=np.float16) f = lambda x: lax.convert_element_type_p.bind(x, new_dtype=np.float64, weak_type=False) jacfwd(f)(a) ``` causes ```python WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)  UnfilteredStackTrace                      Traceback (most recent call last)  in ()       5  > 6 jacfwd(f)(a) 27 frames UnfilteredStackTrace: RuntimeError: UNKNOWN: :0: error: type of return operand 0 ('tensor') doesn't match function result type ('tensor') in function  :0: note: see current operation: ""func.return""(%0) : (tensor) > () The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: RuntimeError                              Traceback (most recent call last) /usr/local/lib/python3.7/distpackages/jax/_src/numpy/lax_numpy.py in transpose(a, axes)     494   _check_arraylike(""transpose"", a)     495   axes = np.arange(ndim(a))[::1] if axes is None else axes > 496   return lax.transpose(a, axes)     497      498  RuntimeError: UNKNOWN: :0: error: type of return operand 0 ('tensor') doesn't match function result type ('tensor') in function  :0: note: see current operation: ""func.return""(%0) : (tensor) > () ``` As  told me in the chat, this can be fixed via 1. call lax.convert_element_type (or even lax._convert_element_type if you really need to control weak_type) rather than bind 2. pass new_dtype=jax.dtypes.canonicalize_dtype(np.float64) 3. enable the x64 flag but is still considered a bug.",2022-04-14T00:06:37Z,,closed,0,3,https://github.com/jax-ml/jax/issues/10273,"Thanks for the report! Here's a slightly simpler repo: ```python import jax.numpy as jnp from jax import lax y = lax.convert_element_type_p.bind(jnp.ones((2, 3)), new_dtype=jnp.float64, weak_type=False) z = y.T ``` The issue here is that calling the primitive directly sidesteps around the X64 flag, so that `y` has type `float64`. Then the dtype rule for `lax.transpose` will return the wrong type, meaning the aval and the buffer do not match. You can achieve the same thing this way: ```python with jax.experimental.enable_x64():   y = jnp.ones((2, 3), dtype='float64') z = y.T ``` Essentially any standard lax primitives which uses `_input_dtype` for its dtype rule is going to have this same problem when faced with a 64bit input while the X64 flag is set to False. Incidentally, it's exactly this kind of difficulty with the x64 context manager that's prevented us from making it a supported API, and is further convincing us that the whole X64 mode idea should be removed.",This is similar in spirit to the errors reported in https://github.com/google/jax/issues/5982 You've just found a clever way to enable X64 mode without the context manager 😁 ,"Thanks for opening this issue, Roman. After thinking about this, I think we should consider it intended behavior for internal APIs. It is a sharp edge related to x64, but fortunately not one in public APIs!"
719,"以下是一个github上的jax下的一个issue, 标题是(add default values to config context managers)， 内容是 (I don't know about you but I found this really annoying: ```python import jax with jax.debug_nans():   ... ``` ``` TypeError                                 Traceback (most recent call last)  in  > 1 with jax.debug_nans():       2       3     ...       4 ... TypeError: __call__() missing 1 required positional argument: 'new_val' ``` Oh right, I'm supposed to write `with jax.debug_nans(True)`... What if we used _default arguments_, at least for booleanvalued config options? Behold, this PR!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,add default values to config context managers,"I don't know about you but I found this really annoying: ```python import jax with jax.debug_nans():   ... ``` ``` TypeError                                 Traceback (most recent call last)  in  > 1 with jax.debug_nans():       2       3     ...       4 ... TypeError: __call__() missing 1 required positional argument: 'new_val' ``` Oh right, I'm supposed to write `with jax.debug_nans(True)`... What if we used _default arguments_, at least for booleanvalued config options? Behold, this PR!",2022-04-12T22:13:14Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10254
15145,"以下是一个github上的jax下的一个issue, 标题是([GPU] jax.scipy.linalg.sqrtm --> TypeError: _schur_translation_rule() got an unexpected keyword argument 'select_callable')， 内容是 ( [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: When run on colab w/ GPU runtime the following code (notebook), ``` import jax import numpy as np print(jax.__version__) jax.scipy.linalg.sqrtm(np.random.uniform(size=(8,8))) ``` results in,  [x] Include stack trace ``` 0.3.4  JaxStackTraceBeforeTransformation         Traceback (most recent call last) /usr/lib/python3.7/runpy.py in _run_module_as_main(***failed resolving arguments***)     192     return _run_code(code, main_globals, None, > 193                      ""__main__"", mod_spec)     194  53 frames /usr/lib/python3.7/runpy.py in _run_code(***failed resolving arguments***)      84                        __spec__ = mod_spec) > 85     exec(code, run_globals)      86     return run_globals /usr/local/lib/python3.7/distpackages/ipykernel_launcher.py in ()      15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() /usr/local/lib/python3.7/distpackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     845         app.initialize(argv) > 846         app.start()     847  /usr/local/lib/python3.7/distpackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     498         try: > 499             self.io_loop.start()     500         except KeyboardInterrupt: /usr/local/lib/python3.7/distpackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     131             asyncio.set_event_loop(self.asyncio_loop) > 132             self.asyncio_loop.run_forever()     133         finally: /usr/lib/python3.7/asyncio/base_events.py in run_forever(***failed resolving arguments***)     540             while True: > 541                 self._run_once()     542                 if self._stopping: /usr/lib/python3.7/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1785             else: > 1786                 handle._run()    1787         handle = None   Needed to break cycles when an exception occurs. /usr/lib/python3.7/asyncio/events.py in _run(***failed resolving arguments***)      87         try: > 88             self._context.run(self._callback, *self._args)      89         except Exception as exc: /usr/local/lib/python3.7/distpackages/tornado/platform/asyncio.py in _handle_events(***failed resolving arguments***)     121         fileobj, handler_func = self.handlers[fd] > 122         handler_func(fileobj, events)     123  /usr/local/lib/python3.7/distpackages/tornado/stack_context.py in null_wrapper(***failed resolving arguments***)     299                 _state.contexts = cap_contexts[0] > 300                 return fn(*args, **kwargs)     301             finally: /usr/local/lib/python3.7/distpackages/zmq/eventloop/zmqstream.py in _handle_events(***failed resolving arguments***)     451             if zmq_events & zmq.POLLIN and self.receiving(): > 452                 self._handle_recv()     453                 if not self.socket: /usr/local/lib/python3.7/distpackages/zmq/eventloop/zmqstream.py in _handle_recv(***failed resolving arguments***)     480                 callback = self._recv_callback > 481                 self._run_callback(callback, msg)     482  /usr/local/lib/python3.7/distpackages/zmq/eventloop/zmqstream.py in _run_callback(***failed resolving arguments***)     430              inside our blanket exception handler rather than outside. > 431             callback(*args, **kwargs)     432         except Exception: /usr/local/lib/python3.7/distpackages/tornado/stack_context.py in null_wrapper(***failed resolving arguments***)     299                 _state.contexts = cap_contexts[0] > 300                 return fn(*args, **kwargs)     301             finally: /usr/local/lib/python3.7/distpackages/ipykernel/kernelbase.py in dispatcher(***failed resolving arguments***)     282             def dispatcher(msg): > 283                 return self.dispatch_shell(stream, msg)     284             return dispatcher /usr/local/lib/python3.7/distpackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     232             try: > 233                 handler(stream, idents, msg)     234             except Exception: /usr/local/lib/python3.7/distpackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     398         reply_content = self.do_execute(code, silent, store_history, > 399                                         user_expressions, allow_stdin)     400  /usr/local/lib/python3.7/distpackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     207         try: > 208             res = shell.run_cell(code, store_history=store_history, silent=silent)     209         finally: /usr/local/lib/python3.7/distpackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     536         self._last_traceback = None > 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     538  /usr/local/lib/python3.7/distpackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2717                 has_raised = self.run_ast_nodes(code_ast.body, cell_name, > 2718                    interactivity=interactivity, compiler=compiler, result=result)    2719  /usr/local/lib/python3.7/distpackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    2827                 code = compiler(mod, cell_name, ""single"") > 2828                 if self.run_code(code, result):    2829                     return True /usr/local/lib/python3.7/distpackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    2881                 rprint('Running code', repr(code_obj))  dbg > 2882                 exec(code_obj, self.user_global_ns, self.user_ns)    2883             finally: [](https://localhost:8080/) in ()       5  > 6 jax.scipy.linalg.sqrtm(np.random.uniform(size=(16,16))) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in sqrtm(***failed resolving arguments***)     657       raise NotImplementedError(""Blocked version is not implemented yet."") > 658   return _sqrtm(A) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in _sqrtm(***failed resolving arguments***)     637 def _sqrtm(A): > 638   T, Z = schur(A, output='complex')     639   sqrt_T = _sqrtm_triu(T) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in schur(***failed resolving arguments***)     116       ""Expected 'output' to be either 'real' or 'complex', got output={}."".format(output)) > 117   return _schur(a, output)     118  /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in _schur(***failed resolving arguments***)     109     a = a.astype(jnp.result_type(a.dtype, 0j)) > 110   return lax_linalg.schur(a)     111  JaxStackTraceBeforeTransformation: TypeError: _schur_translation_rule() got an unexpected keyword argument 'select_callable' The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: UnfilteredStackTrace                      Traceback (most recent call last) [](https://localhost:8080/) in ()       5  > 6 jax.scipy.linalg.sqrtm(np.random.uniform(size=(16,16))) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in sqrtm(A, blocksize)     657       raise NotImplementedError(""Blocked version is not implemented yet."") > 658   return _sqrtm(A) /usr/local/lib/python3.7/distpackages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     161     try: > 162       return fun(*args, **kwargs)     163     except Exception as e: /usr/local/lib/python3.7/distpackages/jax/_src/api.py in cache_miss(*args, **kwargs)     434         device=device, backend=backend, name=flat_fun.__name__, > 435         donated_invars=donated_invars, inline=inline)     436     out_pytree_def = out_tree() /usr/local/lib/python3.7/distpackages/jax/core.py in bind(self, fun, *args, **params)    1708   def bind(self, fun, *args, **params): > 1709     return call_bind(self, fun, *args, **params)    1710  /usr/local/lib/python3.7/distpackages/jax/core.py in call_bind(primitive, fun, *args, **params)    1720   tracers = map(top_trace.full_raise, args) > 1721   outs = top_trace.process_call(primitive, fun, tracers, params)    1722   return map(full_lower, apply_todos(env_trace_todo(), outs)) /usr/local/lib/python3.7/distpackages/jax/core.py in process_call(self, primitive, f, tracers, params)     613   def process_call(self, primitive, f, tracers, params): > 614     return primitive.impl(f, *tracers, **params)     615   process_map = process_call /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in _xla_call_impl(***failed resolving arguments***)     142   compiled_fun = _xla_callable(fun, device, backend, name, donated_invars, > 143                                *unsafe_map(arg_spec, args))     144   try: /usr/local/lib/python3.7/distpackages/jax/linear_util.py in memoized_fun(fun, *args)     271     else: > 272       ans = call(fun, *args)     273       cache[key] = (ans, fun.stores) /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in _xla_callable_uncached(fun, device, backend, name, donated_invars, *arg_specs)     169   return lower_xla_callable(fun, device, backend, name, donated_invars, > 170                             *arg_specs).compile().unsafe_call     171  /usr/local/lib/python3.7/distpackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in lower_xla_callable(fun, device, backend, name, donated_invars, *arg_specs)     259         module_name, closed_jaxpr, backend.platform, > 260         mlir.ReplicaAxisContext(axis_env), name_stack, donated_invars)     261   else: /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in lower_jaxpr_to_module(module_name, jaxpr, platform, axis_context, name_stack, donated_args, replicated_args, arg_shardings, result_shardings)     493         arg_shardings=arg_shardings, result_shardings=result_shardings, > 494         input_output_aliases=input_output_aliases)     495  /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in lower_jaxpr_to_fun(ctx, name, jaxpr, public, replace_units_with_dummy, replace_tokens_with_dummy, replicated_args, arg_shardings, result_shardings, input_output_aliases)     636                              jaxpr.jaxpr, map(ir_constants, jaxpr.consts), > 637                              *args)     638     outs = [] /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in jaxpr_subcomp(ctx, jaxpr, consts, *args)     722       ans = rule(rule_ctx, *map(_unwrap_singleton_ir_values, in_nodes), > 723                  **eqn.params)     724  /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in _xla_call_lower(***failed resolving arguments***)     783                         backend, ctx.module_context, ctx.avals_in, ctx.avals_out, > 784                         *args)     785  /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in _call_lowering(fn_name, stack_name, call_jaxpr, backend, ctx, avals_in, avals_out, *args)     771   symbol_name = lower_jaxpr_to_fun(sub_ctx, fn_name, > 772                                    core.ClosedJaxpr(call_jaxpr, ())).name.value     773   call = func_dialect.CallOp(flat_output_types, /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in lower_jaxpr_to_fun(ctx, name, jaxpr, public, replace_units_with_dummy, replace_tokens_with_dummy, replicated_args, arg_shardings, result_shardings, input_output_aliases)     636                              jaxpr.jaxpr, map(ir_constants, jaxpr.consts), > 637                              *args)     638     outs = [] /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in jaxpr_subcomp(ctx, jaxpr, consts, *args)     722       ans = rule(rule_ctx, *map(_unwrap_singleton_ir_values, in_nodes), > 723                  **eqn.params)     724  /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in cached_lowering(ctx, *args, **params)     935     if func is None: > 936       func = _emit_lowering_rule_as_fun(partial(f, **params), ctx)     937       ctx.module_context.cached_primitive_lowerings[key] = func /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in _emit_lowering_rule_as_fun(lowering_rule, ctx)     665                                       map(len, input_types)) > 666     outs = lowering_rule(ctx, *_unwrap_singleton_ir_values(unflattened_args))     667     func_dialect.ReturnOp(util.flatten(map(wrap_singleton_ir_values, outs))) /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in fallback(ctx, *args, **params)     952     xla_computation = xla.primitive_subcomputation( > 953         module_ctx.platform, module_ctx.axis_env, prim, *ctx.avals_in, **params)     954     submodule_str = xc._xla.mlir.xla_computation_to_mlir_module(xla_computation) /usr/local/lib/python3.7/distpackages/jax/interpreters/xla.py in primitive_subcomputation(platform, axis_env, prim, *avals, **params)     445                            name_stack=new_name_stack()) > 446   ans = f(ctx.replace(builder=c), avals, None, *xla_args, **params)     447   if prim.multiple_results: /usr/local/lib/python3.7/distpackages/jax/interpreters/xla.py in f_new(ctx, avals_in, avals_out, *xla_args, **params)    1035       return jaxpr_subcomp(ctx, jaxpr, _xla_consts(ctx.builder, consts), > 1036                            *xla_args)    1037     return f_new /usr/local/lib/python3.7/distpackages/jax/interpreters/xla.py in jaxpr_subcomp(ctx, jaxpr, consts, *args)     611       ans = rule(eqn_ctx, map(aval, eqn.invars), map(aval, eqn.outvars), > 612                  *in_nodes, **eqn.params)     613  UnfilteredStackTrace: TypeError: _schur_translation_rule() got an unexpected keyword argument 'select_callable' The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last) [](https://localhost:8080/) in ()       4 print(jax.__version__)       5  > 6 jax.scipy.linalg.sqrtm(np.random.uniform(size=(16,16))) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in sqrtm(A, blocksize)     656   if blocksize > 1:     657       raise NotImplementedError(""Blocked version is not implemented yet."") > 658   return _sqrtm(A) TypeError: _schur_translation_rule() got an unexpected keyword argument 'select_callable' ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[GPU] jax.scipy.linalg.sqrtm --> TypeError: _schur_translation_rule() got an unexpected keyword argument 'select_callable'," [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: When run on colab w/ GPU runtime the following code (notebook), ``` import jax import numpy as np print(jax.__version__) jax.scipy.linalg.sqrtm(np.random.uniform(size=(8,8))) ``` results in,  [x] Include stack trace ``` 0.3.4  JaxStackTraceBeforeTransformation         Traceback (most recent call last) /usr/lib/python3.7/runpy.py in _run_module_as_main(***failed resolving arguments***)     192     return _run_code(code, main_globals, None, > 193                      ""__main__"", mod_spec)     194  53 frames /usr/lib/python3.7/runpy.py in _run_code(***failed resolving arguments***)      84                        __spec__ = mod_spec) > 85     exec(code, run_globals)      86     return run_globals /usr/local/lib/python3.7/distpackages/ipykernel_launcher.py in ()      15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() /usr/local/lib/python3.7/distpackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     845         app.initialize(argv) > 846         app.start()     847  /usr/local/lib/python3.7/distpackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     498         try: > 499             self.io_loop.start()     500         except KeyboardInterrupt: /usr/local/lib/python3.7/distpackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     131             asyncio.set_event_loop(self.asyncio_loop) > 132             self.asyncio_loop.run_forever()     133         finally: /usr/lib/python3.7/asyncio/base_events.py in run_forever(***failed resolving arguments***)     540             while True: > 541                 self._run_once()     542                 if self._stopping: /usr/lib/python3.7/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1785             else: > 1786                 handle._run()    1787         handle = None   Needed to break cycles when an exception occurs. /usr/lib/python3.7/asyncio/events.py in _run(***failed resolving arguments***)      87         try: > 88             self._context.run(self._callback, *self._args)      89         except Exception as exc: /usr/local/lib/python3.7/distpackages/tornado/platform/asyncio.py in _handle_events(***failed resolving arguments***)     121         fileobj, handler_func = self.handlers[fd] > 122         handler_func(fileobj, events)     123  /usr/local/lib/python3.7/distpackages/tornado/stack_context.py in null_wrapper(***failed resolving arguments***)     299                 _state.contexts = cap_contexts[0] > 300                 return fn(*args, **kwargs)     301             finally: /usr/local/lib/python3.7/distpackages/zmq/eventloop/zmqstream.py in _handle_events(***failed resolving arguments***)     451             if zmq_events & zmq.POLLIN and self.receiving(): > 452                 self._handle_recv()     453                 if not self.socket: /usr/local/lib/python3.7/distpackages/zmq/eventloop/zmqstream.py in _handle_recv(***failed resolving arguments***)     480                 callback = self._recv_callback > 481                 self._run_callback(callback, msg)     482  /usr/local/lib/python3.7/distpackages/zmq/eventloop/zmqstream.py in _run_callback(***failed resolving arguments***)     430              inside our blanket exception handler rather than outside. > 431             callback(*args, **kwargs)     432         except Exception: /usr/local/lib/python3.7/distpackages/tornado/stack_context.py in null_wrapper(***failed resolving arguments***)     299                 _state.contexts = cap_contexts[0] > 300                 return fn(*args, **kwargs)     301             finally: /usr/local/lib/python3.7/distpackages/ipykernel/kernelbase.py in dispatcher(***failed resolving arguments***)     282             def dispatcher(msg): > 283                 return self.dispatch_shell(stream, msg)     284             return dispatcher /usr/local/lib/python3.7/distpackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     232             try: > 233                 handler(stream, idents, msg)     234             except Exception: /usr/local/lib/python3.7/distpackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     398         reply_content = self.do_execute(code, silent, store_history, > 399                                         user_expressions, allow_stdin)     400  /usr/local/lib/python3.7/distpackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     207         try: > 208             res = shell.run_cell(code, store_history=store_history, silent=silent)     209         finally: /usr/local/lib/python3.7/distpackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     536         self._last_traceback = None > 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     538  /usr/local/lib/python3.7/distpackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2717                 has_raised = self.run_ast_nodes(code_ast.body, cell_name, > 2718                    interactivity=interactivity, compiler=compiler, result=result)    2719  /usr/local/lib/python3.7/distpackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    2827                 code = compiler(mod, cell_name, ""single"") > 2828                 if self.run_code(code, result):    2829                     return True /usr/local/lib/python3.7/distpackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    2881                 rprint('Running code', repr(code_obj))  dbg > 2882                 exec(code_obj, self.user_global_ns, self.user_ns)    2883             finally: [](https://localhost:8080/) in ()       5  > 6 jax.scipy.linalg.sqrtm(np.random.uniform(size=(16,16))) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in sqrtm(***failed resolving arguments***)     657       raise NotImplementedError(""Blocked version is not implemented yet."") > 658   return _sqrtm(A) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in _sqrtm(***failed resolving arguments***)     637 def _sqrtm(A): > 638   T, Z = schur(A, output='complex')     639   sqrt_T = _sqrtm_triu(T) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in schur(***failed resolving arguments***)     116       ""Expected 'output' to be either 'real' or 'complex', got output={}."".format(output)) > 117   return _schur(a, output)     118  /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in _schur(***failed resolving arguments***)     109     a = a.astype(jnp.result_type(a.dtype, 0j)) > 110   return lax_linalg.schur(a)     111  JaxStackTraceBeforeTransformation: TypeError: _schur_translation_rule() got an unexpected keyword argument 'select_callable' The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: UnfilteredStackTrace                      Traceback (most recent call last) [](https://localhost:8080/) in ()       5  > 6 jax.scipy.linalg.sqrtm(np.random.uniform(size=(16,16))) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in sqrtm(A, blocksize)     657       raise NotImplementedError(""Blocked version is not implemented yet."") > 658   return _sqrtm(A) /usr/local/lib/python3.7/distpackages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     161     try: > 162       return fun(*args, **kwargs)     163     except Exception as e: /usr/local/lib/python3.7/distpackages/jax/_src/api.py in cache_miss(*args, **kwargs)     434         device=device, backend=backend, name=flat_fun.__name__, > 435         donated_invars=donated_invars, inline=inline)     436     out_pytree_def = out_tree() /usr/local/lib/python3.7/distpackages/jax/core.py in bind(self, fun, *args, **params)    1708   def bind(self, fun, *args, **params): > 1709     return call_bind(self, fun, *args, **params)    1710  /usr/local/lib/python3.7/distpackages/jax/core.py in call_bind(primitive, fun, *args, **params)    1720   tracers = map(top_trace.full_raise, args) > 1721   outs = top_trace.process_call(primitive, fun, tracers, params)    1722   return map(full_lower, apply_todos(env_trace_todo(), outs)) /usr/local/lib/python3.7/distpackages/jax/core.py in process_call(self, primitive, f, tracers, params)     613   def process_call(self, primitive, f, tracers, params): > 614     return primitive.impl(f, *tracers, **params)     615   process_map = process_call /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in _xla_call_impl(***failed resolving arguments***)     142   compiled_fun = _xla_callable(fun, device, backend, name, donated_invars, > 143                                *unsafe_map(arg_spec, args))     144   try: /usr/local/lib/python3.7/distpackages/jax/linear_util.py in memoized_fun(fun, *args)     271     else: > 272       ans = call(fun, *args)     273       cache[key] = (ans, fun.stores) /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in _xla_callable_uncached(fun, device, backend, name, donated_invars, *arg_specs)     169   return lower_xla_callable(fun, device, backend, name, donated_invars, > 170                             *arg_specs).compile().unsafe_call     171  /usr/local/lib/python3.7/distpackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in lower_xla_callable(fun, device, backend, name, donated_invars, *arg_specs)     259         module_name, closed_jaxpr, backend.platform, > 260         mlir.ReplicaAxisContext(axis_env), name_stack, donated_invars)     261   else: /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in lower_jaxpr_to_module(module_name, jaxpr, platform, axis_context, name_stack, donated_args, replicated_args, arg_shardings, result_shardings)     493         arg_shardings=arg_shardings, result_shardings=result_shardings, > 494         input_output_aliases=input_output_aliases)     495  /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in lower_jaxpr_to_fun(ctx, name, jaxpr, public, replace_units_with_dummy, replace_tokens_with_dummy, replicated_args, arg_shardings, result_shardings, input_output_aliases)     636                              jaxpr.jaxpr, map(ir_constants, jaxpr.consts), > 637                              *args)     638     outs = [] /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in jaxpr_subcomp(ctx, jaxpr, consts, *args)     722       ans = rule(rule_ctx, *map(_unwrap_singleton_ir_values, in_nodes), > 723                  **eqn.params)     724  /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in _xla_call_lower(***failed resolving arguments***)     783                         backend, ctx.module_context, ctx.avals_in, ctx.avals_out, > 784                         *args)     785  /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in _call_lowering(fn_name, stack_name, call_jaxpr, backend, ctx, avals_in, avals_out, *args)     771   symbol_name = lower_jaxpr_to_fun(sub_ctx, fn_name, > 772                                    core.ClosedJaxpr(call_jaxpr, ())).name.value     773   call = func_dialect.CallOp(flat_output_types, /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in lower_jaxpr_to_fun(ctx, name, jaxpr, public, replace_units_with_dummy, replace_tokens_with_dummy, replicated_args, arg_shardings, result_shardings, input_output_aliases)     636                              jaxpr.jaxpr, map(ir_constants, jaxpr.consts), > 637                              *args)     638     outs = [] /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in jaxpr_subcomp(ctx, jaxpr, consts, *args)     722       ans = rule(rule_ctx, *map(_unwrap_singleton_ir_values, in_nodes), > 723                  **eqn.params)     724  /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in cached_lowering(ctx, *args, **params)     935     if func is None: > 936       func = _emit_lowering_rule_as_fun(partial(f, **params), ctx)     937       ctx.module_context.cached_primitive_lowerings[key] = func /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in _emit_lowering_rule_as_fun(lowering_rule, ctx)     665                                       map(len, input_types)) > 666     outs = lowering_rule(ctx, *_unwrap_singleton_ir_values(unflattened_args))     667     func_dialect.ReturnOp(util.flatten(map(wrap_singleton_ir_values, outs))) /usr/local/lib/python3.7/distpackages/jax/interpreters/mlir.py in fallback(ctx, *args, **params)     952     xla_computation = xla.primitive_subcomputation( > 953         module_ctx.platform, module_ctx.axis_env, prim, *ctx.avals_in, **params)     954     submodule_str = xc._xla.mlir.xla_computation_to_mlir_module(xla_computation) /usr/local/lib/python3.7/distpackages/jax/interpreters/xla.py in primitive_subcomputation(platform, axis_env, prim, *avals, **params)     445                            name_stack=new_name_stack()) > 446   ans = f(ctx.replace(builder=c), avals, None, *xla_args, **params)     447   if prim.multiple_results: /usr/local/lib/python3.7/distpackages/jax/interpreters/xla.py in f_new(ctx, avals_in, avals_out, *xla_args, **params)    1035       return jaxpr_subcomp(ctx, jaxpr, _xla_consts(ctx.builder, consts), > 1036                            *xla_args)    1037     return f_new /usr/local/lib/python3.7/distpackages/jax/interpreters/xla.py in jaxpr_subcomp(ctx, jaxpr, consts, *args)     611       ans = rule(eqn_ctx, map(aval, eqn.invars), map(aval, eqn.outvars), > 612                  *in_nodes, **eqn.params)     613  UnfilteredStackTrace: TypeError: _schur_translation_rule() got an unexpected keyword argument 'select_callable' The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last) [](https://localhost:8080/) in ()       4 print(jax.__version__)       5  > 6 jax.scipy.linalg.sqrtm(np.random.uniform(size=(16,16))) /usr/local/lib/python3.7/distpackages/jax/_src/scipy/linalg.py in sqrtm(A, blocksize)     656   if blocksize > 1:     657       raise NotImplementedError(""Blocked version is not implemented yet."") > 658   return _sqrtm(A) TypeError: _schur_translation_rule() got an unexpected keyword argument 'select_callable' ```",2022-04-09T06:55:38Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/10209,"Thanks for the report! `schur` is not implemented on GPU, so the fix in CC(Fix arguments to schur translation rule) will result in a more clear `NotImplementedError` in place of the `TypeError`: https://github.com/google/jax/blob/28842151c6030a951bc389b771a3dcd3d4ca74a7/jax/_src/lax/linalg.pyL1819L1822"
2299,"以下是一个github上的jax下的一个issue, 标题是(Big performance discrepancy between JAX and TensorFlow with in-place updates)， 内容是 (Hi,  I am trying to understand what's causing JAX to perform 1001000x slower compared to TensorFlow on the following computation. JAX version: ```  def f_jax(p0, p1):     n, _ = p0.shape     P = jnp.concatenate([p0[None], p1[None], jnp.empty([n  2, n, n])])     def body(P, i):         X = P[i  1] @ P[i  2]         X /= X.sum(1, keepdims=True)         return P.at[i].set(X), None     P, _ = jax.lax.scan(body, P, jnp.arange(2, n))     return P[1, 0, 0] df_jax = jit(grad(f_jax)) ``` Here is an equivalent implementation in TF: ``` .function def f_tf(p0, p1):     n, _ = p0.shape     P = (         tf.TensorArray(tf.float32, size=n, infer_shape=True, clear_after_read=False)         .write(0, p0)         .write(1, p1)     )     def body(P, i):         X = P.read(i  1) @ P.read(i  2)         X /= tf.reduce_sum(X, axis=1, keepdims=True)         return P.write(i, X)     _, P = tf.while_loop(lambda i, P: i < n, lambda i, P: (i + 1, body(P, i)), (2, P))     return P.read(n  1)[0, 0] .function def df_tf(p0, p1):     with tf.GradientTape() as g:         g.watch(p0)         y = f_tf(p0, p1)     return g.gradient(y, p0) ``` We have noticed that as the input size `n` increases, JAX (specifically the gradient computation) is 23 orders slower than TF (note log scale): !bench (This is after accounting for compilation time and async dispatch.) Of course, the implementation shown above does not need inplace updates at all; it's my attempt to distill our problem down to an MRE. Rewritten more sanely as  ```  def f_jax(p0, p1):     n, _ = p0.shape     def body(tup, _):         X1, X0 = tup         X2 = X1 @ X0         X2 /= X2.sum(1, keepdims=True)         return (X2, X1), None     (P, _), _ = jax.lax.scan(body, (p1, p0), None, length=n  2)     return P[0, 0] ``` and equivalently in TF, the difference persists but is less dramatic: !bench2 (Our actual use case, which is running the sumproduct algorithm on a tree, does not have any such simplification, so we are stuck with inplace updates.) Any ideas what could be going on? Thanks! ())请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Big performance discrepancy between JAX and TensorFlow with in-place updates,"Hi,  I am trying to understand what's causing JAX to perform 1001000x slower compared to TensorFlow on the following computation. JAX version: ```  def f_jax(p0, p1):     n, _ = p0.shape     P = jnp.concatenate([p0[None], p1[None], jnp.empty([n  2, n, n])])     def body(P, i):         X = P[i  1] @ P[i  2]         X /= X.sum(1, keepdims=True)         return P.at[i].set(X), None     P, _ = jax.lax.scan(body, P, jnp.arange(2, n))     return P[1, 0, 0] df_jax = jit(grad(f_jax)) ``` Here is an equivalent implementation in TF: ``` .function def f_tf(p0, p1):     n, _ = p0.shape     P = (         tf.TensorArray(tf.float32, size=n, infer_shape=True, clear_after_read=False)         .write(0, p0)         .write(1, p1)     )     def body(P, i):         X = P.read(i  1) @ P.read(i  2)         X /= tf.reduce_sum(X, axis=1, keepdims=True)         return P.write(i, X)     _, P = tf.while_loop(lambda i, P: i < n, lambda i, P: (i + 1, body(P, i)), (2, P))     return P.read(n  1)[0, 0] .function def df_tf(p0, p1):     with tf.GradientTape() as g:         g.watch(p0)         y = f_tf(p0, p1)     return g.gradient(y, p0) ``` We have noticed that as the input size `n` increases, JAX (specifically the gradient computation) is 23 orders slower than TF (note log scale): !bench (This is after accounting for compilation time and async dispatch.) Of course, the implementation shown above does not need inplace updates at all; it's my attempt to distill our problem down to an MRE. Rewritten more sanely as  ```  def f_jax(p0, p1):     n, _ = p0.shape     def body(tup, _):         X1, X0 = tup         X2 = X1 @ X0         X2 /= X2.sum(1, keepdims=True)         return (X2, X1), None     (P, _), _ = jax.lax.scan(body, (p1, p0), None, length=n  2)     return P[0, 0] ``` and equivalently in TF, the difference persists but is less dramatic: !bench2 (Our actual use case, which is running the sumproduct algorithm on a tree, does not have any such simplification, so we are stuck with inplace updates.) Any ideas what could be going on? Thanks! ()",2022-04-08T18:00:41Z,bug,open,2,20,https://github.com/jax-ml/jax/issues/10197,Thanks for the question! Let's figure this out. What backend is this on (CPU/GPU/TPU)?,"Can you share your code for creating the arguments `p0` and `p1`, perhaps as a function of `n`? (An endtoend runnable script would be helpful.)","Thanks for the quick reply! So far I have only tried on CPU. I will try on GPU presently and report back. Here is the whole script.  ``` from functools import partial import jax import jax.numpy as jnp from jax import jit, grad import tensorflow as tf from timeit import default_timer as timer import numpy as np import logging logging.getLogger('tensorflow').setLevel(logging.ERROR)  def f_jax(p0, p1):     n, _ = p0.shape     P = jnp.concatenate([p0[None], p1[None], jnp.empty([n  2, n, n])])     def body(P, i):         X = P[i  1] @ P[i  2]         X /= X.sum(1, keepdims=True)         return P.at[i].set(X), None     P, _ = jax.lax.scan(body, P, jnp.arange(2, n))     return P[1, 0, 0] df_jax = jit(grad(f_jax)) .function def f_tf(p0, p1):     n, _ = p0.shape     P = (         tf.TensorArray(tf.float32, size=n, infer_shape=True, clear_after_read=False)         .write(0, p0)         .write(1, p1)     )     def body(P, i):         X = P.read(i  1) @ P.read(i  2)         X /= tf.reduce_sum(X, axis=1, keepdims=True)         return P.write(i, X)     _, P = tf.while_loop(lambda i, P: i < n, lambda i, P: (i + 1, body(P, i)), (2, P))     return P.read(n  1)[0, 0] .function def df_tf(p0, p1):     with tf.GradientTape() as g:         g.watch(p0)         y = f_tf(p0, p1)     return g.gradient(y, p0) records = [] def record(f, n, x, c, t):     x = float(x)     records.append(locals())     print(f'{f}({n=})={x} {c=} {t=}') for n in range(100, 500, 50):     P = np.random.rand(2, n, n).astype(np.float32)     P /= P.sum(2, keepdims=True)     p0, p1 = P     print(f""\n\n***** {n=} *****"")     start = timer()     _ = f_jax(p0, p1).block_until_ready()     end = timer()     c = end  start     start = timer()     x = f_jax(p0, p1).block_until_ready()     end = timer()     t = end  start     record('f_jax', n, x, c, t)     start = timer()     _ = f_tf(p0, p1)     end = timer()     c = end  start     start = timer()     x = f_tf(p0, p1)     end = timer()     t = end  start     record('f_tf', n, x, c, t)     start = timer()     _ = df_jax(p0, p1).block_until_ready()     end = timer()     c = end  start     start = timer()     x = jnp.linalg.norm(df_jax(p0, p1).block_until_ready().reshape(1))     end = timer()     t = end  start     record('df_jax', n, x, c, t)     start = timer()     _ = df_tf(p0, p1)     end = timer()     c = end  start     start = timer()     x = np.linalg.norm(np.reshape(df_tf(p0, p1), 1))     end = timer()     t = end  start     record('df_tf', n, x, c, t) import pandas as pd df = pd.DataFrame.from_records(records) p = df.pivot(index='n', columns='f', values='t').plot.line() p.set_yscale('log') p.set_ylabel('running time (s)') p.figure.savefig('bench.pdf') ```","I think there may be an AD issue here (perhaps in addition to others): because we're not using the scannedover inputs and outputs, and are instead just using the loop carry, JAX's AD must make some defensive copies and store a lot of extra information. We should try rewriting to use the scannedover inputs/outputs directly. I can give that a shot.","Ah, just noticed your rewrite (""Rewritten more sanely as"") avoids the AD issue I mentioned. But I'm unclear on why your real use case can't admit a similar optimization. Is it because there's no linear scan order and instead access into the array is more 'random' (depending on the tree structure)? Can we avoid using a `scan` at all? What backend do you care about (CPU vs GPU)?","> Is it because there's no linear scan order and instead access into the array is more 'random' (depending on the tree structure)?  Yes, exactlyfor a tree you have to scan over a postordering of the nodes, which is not linear in general. For avoiding the scan, I don't see how except by using a regular Python `for` loop which gets unrolled. I tried this, but the compilation times were in the tensofminutes range on the larger end. Re: GPU vs. CPU, I'd say we interested in getting this to work on both at this point. We develop methods for analyzing evolutionary data, and both GPU and CPU are commonly used depending on the application. We'd certainly take whichever is easier to fix as a start though :)  and  may have more to add.","(Edit: we also tried `lax.fori_loop`, but it suffers from the same problems as `scan`.)","This is a really interesting use case, and I think it's showing us a weakness in JAX (specifically `scan`) that we must improve.  _If_ this turns out to be an AD+scan issue as I currently suspect, then I think the best option for the near future, if at all feasible, is not to use `scan`, and for us to try to get the compilation times down in other ways. In the longer term we may need to generalize JAX's loop constructs so that we can have nonsequential reads/writes in a loop body while maintaining sparse/efficient AD. To decide whether this is an AD+scan issue, i'm running the script with the scan fully unrolled. So far it certainly seems to ~solve~ mitigate the AD execution time issue: !image","> Re: GPU vs. CPU, I'd say we interested in getting this to work on both at this point. We develop methods for analyzing evolutionary data, and both GPU and CPU are commonly used depending on the application. We'd certainly take whichever is easier to fix as a start though :)  and  may have more to add. Yes, completely agreeing with  here both are now commonly used. Perhaps further discussion would be merited after this simplified example is running quickly.","Re: alternatives to `scan`, one other thought I had is to write the entire computation as a (giant) `einsum` and then see if `opt_einsum` could work its magic. (I suspect the answer is no because the optimal contraction path is exactly the postorder traversal, so it would be equivalent to unrolling the for loop, but hope springs eternal.)","I'd been focusing my attention on the examples with AD, but I guess the performance difference of `f_jax` vs `f_tf` shown in the plots is interesting on its own... I think those must show some XLA issues (rather than JAX ones).","Actually, rerunning the original benchmark script on CPU, I got a somewhat different plot: !image This certainly makes it look more like an AD issue, since `f_jax` and `f_tf` look the same (unless I screwed something up).","Sorry about that! I realized as I was running the GPU benchmark just now that I had forgotten to disable the GPU for TF in the first set of plots I sent. So it was really comparing JAXonCPU to TFonGPU. Here are the correct plots for CPU vs. CPU and GPU vs. GPU on my machine. GPU performance is a lot closer between the two.  CPU: !benchcpu GPU: !benchgpu Also, after I installed the CUDA version of jaxlib just now, compiles began taking longer, and I started getting a new warning message: ``` 20220408 15:39:08.079551: E external/org_tensorflow/tensorflow/compiler/xla/service/slow_operation_alarm.cc:61] Constant folding an instruction is takin g > 2s:   %dynamicupdateslice.21 = f32[450,450,450]{2,1,0} dynamicupdateslice(f32[450,450,450]{2,1,0} %broadcast.4, f32[1,1,1]{2,1,0} %constant.347, s32[] %co nstant.348, s32[] %constant.350, s32[] %constant.352), metadata={op_name=""jit(f_jax)/jit(main)/jit(transpose(jvp(f_jax)))/jit(jit_transpose(jvp(f_jax)))/s catteradd[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0, 1, 2), scatter_dims_to_operand_dims= (0, 1, 2)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.PROMISE_IN_BOUNDS]"" source_file=""./mre2.py"" source_line=40} This isn't necessarily a bug; constantfolding is inherently a tradeoff between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time. If you'd like to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. ```",No worries at all! I think this narrows it down to the hunch that this is a JAX scan+AD issue (and on the flip side a compilation time issue forcing us to use scan). Does that sound right to you? (I'm context switching between tasks so I may be missing some things.),Yes I agree with that assessment.,"Here's an unrolled version and the raw data up to `n=350`: !image ``` ***** n=100 ***** f_jax(n=100)=0.010338895954191685 c=13.903323588019703 t=0.00370277400361374 f_tf(n=100)=0.010338901542127132 c=0.8613433229911607 t=0.009604775987099856 df_jax(n=100)=0.005899609997868538 c=60.08326904202113 t=0.07732683801441453 df_tf(n=100)=0.005899606738239527 c=0.2325137360021472 t=0.022367377998307347 ***** n=150 ***** f_jax(n=150)=0.006485469173640013 c=34.875870921008755 t=0.014937857980839908 f_tf(n=150)=0.0064854552038013935 c=0.0584060609980952 t=0.016561672993702814 df_jax(n=150)=0.0038804737851023674 c=170.71420154100633 t=0.22453490100451745 df_tf(n=150)=0.00388047331944108 c=0.23616197099909186 t=0.05086089699761942 ***** n=200 ***** f_jax(n=200)=0.004887463059276342 c=65.07420958002331 t=0.03230343101313338 f_tf(n=200)=0.004887455143034458 c=0.0741558290028479 t=0.03332922499976121 df_jax(n=200)=0.0030319192446768284 c=320.42920567598776 t=0.6313395249890164 df_tf(n=200)=0.0030319183133542538 c=0.2939827009977307 t=0.10080308798933402 ***** n=250 ***** f_jax(n=250)=0.004011679906398058 c=131.0552428800147 t=0.07588186502107419 f_tf(n=250)=0.004011671058833599 c=0.1462114229798317 t=0.05330039601540193 df_jax(n=250)=0.002302329521626234 c=715.0989373530028 t=1.800481797021348 df_tf(n=250)=0.0023023297544568777 c=0.43415209601516835 t=0.2324468480073847 ***** n=300 ***** f_jax(n=300)=0.00330487173050642 c=172.14162274301634 t=0.14097746700281277 f_tf(n=300)=0.003304873825982213 c=0.18904318899149075 t=0.1117071449989453 df_jax(n=300)=0.0019429094390943646 c=850.5360104879946 t=2.872852647997206 df_tf(n=300)=0.0019429087406024337 c=0.4660322629788425 t=0.29649784197681583 ***** n=350 ***** f_jax(n=350)=0.0027687139809131622 c=238.86592792198644 t=0.2026489340059925 f_tf(n=350)=0.0027687170077115297 c=0.20865108599537052 t=0.14300706400536 df_jax(n=350)=0.001680677873082459 c=1256.9225249080046 t=5.858040032006102 df_tf(n=350)=0.00168067857157439 c=0.9184109750203788 t=0.6154126079927664 ``` `df_jax` is still slow! So even without `scan` we're missing inplace opportunities, maybe? Hrm... I did this unrolling by just putting a `with jax.disable_jit()` around the call to `scan`, which may not be ideal.","It may be related, I'm experiencing slow compilation time by using AD+scan on CPU. The application is maximum likelihood optimization where the loglikelihood is computed with the Kalman filter. I can post a reproducible example if there is an interest.","To narrow things down a bit. Here's a simplified MWE, which clearly demonstrates O(n^2) asymptotics for an O(n) program. Every time the problem size doubles, the runtime increases by a factor of 4. Moreover, it's actually possible to induce this without even using `jax.grad`. I've included a MWE for this case too. (Although this is a bit of a funny one, the program is naively O(n^2), it's just that it can clearly be optimised to O(n). Maybe this one is unfair, or maybe it's a MWE for the optimisation we'd like the backward pass of the first example to be doing?) So I'm pretty sure the problem is indeed a lack of copy elision. (c.f. also CC(Feature request: disabling/detecting outofplace `.at[].set()` updates.)) Notably, this was on the CPU. When I try running on the GPU I get much muddier results  not clearly O(n) but not clearly O(n^2) either. I think this might be backenddependent. ```python import functools as ft import jax import jax.lax as lax import jax.numpy as jnp import jax.random as jr import timeit .partial(jax.jit, static_argnums=1) .grad def f(p0, n):     P = jnp.concatenate([p0[None], jnp.empty([n  1, 10, 10])])     def body(P, i):         return P.at[i].set(P[i  1]), None     P, _ = lax.scan(body, P, jnp.arange(2, n))     return jnp.sum(P[1]) .partial(jax.jit, static_argnums=1) def g(p0, n):     P = jnp.concatenate([p0[None], jnp.empty([n  1, 10, 10])])     def body(P, i):          Condition is always true         out = jnp.where(i < n, P.at[i].set(P[i  1]), P)         return out, None     P, _ = lax.scan(body, P, jnp.arange(2, n))     return jnp.sum(P[1]) def time_fn(fn, name):     p0 = jr.normal(jr.PRNGKey(0), (10, 10))     for n in (100, 200, 400, 800, 1600, 3200, 6400):         elapsed = min(timeit.repeat(lambda: fn(p0, n), number=1, repeat=10))         print(name, n, elapsed) time_fn(f, ""f"") time_fn(g, ""g"")  f 100 0.0004602999997587176  f 200 0.002000699999371136  f 400 0.008667000000059488  f 800 0.036140100000011444  f 1600 0.17022369999995135  f 3200 0.6104193000001032  f 6400 2.8481895000004442  g 100 0.00020359999962238362  g 200 0.0008628999994471087  g 400 0.0038144000000102096  g 800 0.018358399999669928  g 1600 0.13079819999984466  g 3200 0.5259796000000279  g 6400 1.8100587999997515 ```","This might a little off topic as it does not isolate index updates, but also in this (nonminimal, but real usecase) example compile time is a bottleneck not only limited to index ops: https://colab.research.google.com/drive/1GkMqlniPnvSqWdamX_waNJF9Tc5ZmoMu?usp=sharing `scan` and the python for loop are in a tradeoff between compile time and runtime:  `scan` is faster to compile, slower to execute  the python `for` is slow ti compile, fast to execute","Alright, the latest Equinox release (v0.10.8) provides a way to workaround this  use `equinox.internal.scan` instead of `lax.scan`! Here's an example: ```python import functools as ft import equinox.internal as eqxi import jax import jax.lax as lax import jax.numpy as jnp import jax.random as jr import timeit .partial(jax.jit, static_argnums=1) .grad def f(p0, n):     P = jnp.concatenate([p0[None], jnp.empty([n  1, 10, 10])])     def body(P, i):         return P.at[i].set(P[i  1]), None     def buffers(P):         return P     P, _ = eqxi.scan(body, P, jnp.arange(2, n), buffers=buffers, kind=""checkpointed"", checkpoints=""all"")     return jnp.sum(P[1]) p0 = jr.normal(jr.PRNGKey(0), (10, 10)) for n in (100, 200, 400, 800, 1600, 3200, 6400):     elapsed = min(timeit.repeat(lambda: f(p0, n), number=1, repeat=10))     print(n, elapsed)  linear scaling!  100 3.25329999668611e05  200 5.7779000144364545e05  400 0.00011023599995496625  800 0.00021904000004724367  1600 0.0004653779999443941  3200 0.0009436909999749332  6400 0.001919317000101728 ``` This uses the ""buffers"" functionality provided by `eqxi.scan`. Given `scan(f, init, ..., buffers)`, then everything returned by `buffers(init)` will be a ""buffer"" instead of a normal JAX array.  Buffers support *only* reading (`buf[some_index]`) and writing (`buf.at[some_index].set(some_value)`). You can just read from the whole buffer before doing anything else with the value, of course, but note the next point:  You are only allowed to read from a location you have previously written to. (Something like `0 * buf[index_not_set_yet]` is fine  you just need to have no ""mathematical dependency"" on places you haven't written to yet; a ""computational dependency"" is fine.)  You are only allowed to write to the same location *once*, both before the buffer is passed in to the scan, and during the scan. Something like `buf.at[index].set(...).at[index].set(...)` is not allowed. On to the small print:  The last two bullet points are completely unchecked: if you violate these assumptions you will silently get incorrect gradients.  If you instead have a body function with multiple reads, e.g. `P.at[i].set(P[i  2] + P[i  1])`, then we still get quadratic scaling. This is due to an XLA bug. (Googlers: this is tracked as b/288798733.)"
2165,"以下是一个github上的jax下的一个issue, 标题是(JAX profiler is not working for TPU)， 内容是 ( Versions Python 3.10 ``` jax                          0.3.4 jaxlib                       0.3.2 libtpunightly               0.1.dev20220315 tensorboard                  2.8.0 tensorboarddataserver      0.6.1 tensorboardpluginprofile   2.8.0 tensorboardpluginwit       1.8.1 tensorflow                   2.8.0 tensorflowiogcsfilesystem 0.24.0 ``` Tensorflow is built from source with TPU support, following the official guide. This is because the original wheel in TPU VM is for Python 3.8, not Python 3.10. ```sh sudo mkdir p /usr/local/lib/bazel/bin sudo wget P /usr/local/lib/bazel/bin https://github.com/bazelbuild/bazel/releases/download/4.2.1/bazel4.2.1linuxx86_64 sudo chmod +x /usr/local/lib/bazel/bin/bazel4.2.1linuxx86_64 git clone b v2.8.0 depth=1 https://github.com/tensorflow/tensorflow.git cd tensorflow python3.10 m virtualenv ~/.venv310 . ~/.venv310/bin/activate pip install U ""jax[tpu]"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html python configure.py   interactive bazel build config=tpu //tensorflow/tools/pip_package:build_pip_package ./bazelbin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg pip install /tmp/tensorflow_pkg/tensorflow*.whl ```  Issue Profiling a JAX program in this way: ```python import jax jax.profiler.start_trace(log_dir='/tmp/jaxprofiler') ... jax.profiler.stop_trace() ``` Open Tensorboard. In the `overview_page`, there is no information. ![](https://userimages.githubusercontent.com/68557794/162389289463ffcbe55714976b0ae3279c031bb52.png) In `memory_profile`, it says 'There is no memory profile to display because there were no memory activity data in the captured duration'. ![](https://userimages.githubusercontent.com/68557794/162390693d095c5c3c24b40d79e8ba598bb0e045e.png) In `trace_viewer`, there is only information about CPU, but no information about TPU. ![](https://userimages.githubusercontent.com/68557794/16238960384ab8ace88934c2588840ddd5c6bdc07.png))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX profiler is not working for TPU," Versions Python 3.10 ``` jax                          0.3.4 jaxlib                       0.3.2 libtpunightly               0.1.dev20220315 tensorboard                  2.8.0 tensorboarddataserver      0.6.1 tensorboardpluginprofile   2.8.0 tensorboardpluginwit       1.8.1 tensorflow                   2.8.0 tensorflowiogcsfilesystem 0.24.0 ``` Tensorflow is built from source with TPU support, following the official guide. This is because the original wheel in TPU VM is for Python 3.8, not Python 3.10. ```sh sudo mkdir p /usr/local/lib/bazel/bin sudo wget P /usr/local/lib/bazel/bin https://github.com/bazelbuild/bazel/releases/download/4.2.1/bazel4.2.1linuxx86_64 sudo chmod +x /usr/local/lib/bazel/bin/bazel4.2.1linuxx86_64 git clone b v2.8.0 depth=1 https://github.com/tensorflow/tensorflow.git cd tensorflow python3.10 m virtualenv ~/.venv310 . ~/.venv310/bin/activate pip install U ""jax[tpu]"" f https://storage.googleapis.com/jaxreleases/libtpu_releases.html python configure.py   interactive bazel build config=tpu //tensorflow/tools/pip_package:build_pip_package ./bazelbin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg pip install /tmp/tensorflow_pkg/tensorflow*.whl ```  Issue Profiling a JAX program in this way: ```python import jax jax.profiler.start_trace(log_dir='/tmp/jaxprofiler') ... jax.profiler.stop_trace() ``` Open Tensorboard. In the `overview_page`, there is no information. ![](https://userimages.githubusercontent.com/68557794/162389289463ffcbe55714976b0ae3279c031bb52.png) In `memory_profile`, it says 'There is no memory profile to display because there were no memory activity data in the captured duration'. ![](https://userimages.githubusercontent.com/68557794/162390693d095c5c3c24b40d79e8ba598bb0e045e.png) In `trace_viewer`, there is only information about CPU, but no information about TPU. ![](https://userimages.githubusercontent.com/68557794/16238960384ab8ace88934c2588840ddd5c6bdc07.png)",2022-04-08T07:49:54Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/10191,"It seems that the profiler works for the example program, but not for my training script. As my training script is too long to post here, I will close this issue first."
808,"以下是一个github上的jax下的一个issue, 标题是([sparse] allow specifying axis type in bcoo_broadcast_in_dim)， 内容是 (Currently `B[np.newaxis, :]` will add a sparse axis when possible, and a batch axis otherwise. We should add the ability to specify this. Possible API: ```python B[sparse.newaxis, :]   add a sparse axis if possible, batch otherwise (same as current np.newaxis behavior) B[sparse.newaxis.sparse, :]   add a sparse axis, error if not possible B[sparse.newaxis.batch, :]   add a batch axis, error if not possible B[..., sparse.newaxis]   add a sparse axis if possible, dense otherwise B[..., sparse.newaxis.dense]   add a dense axis if possible, error if not possible ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] allow specifying axis type in bcoo_broadcast_in_dim,"Currently `B[np.newaxis, :]` will add a sparse axis when possible, and a batch axis otherwise. We should add the ability to specify this. Possible API: ```python B[sparse.newaxis, :]   add a sparse axis if possible, batch otherwise (same as current np.newaxis behavior) B[sparse.newaxis.sparse, :]   add a sparse axis, error if not possible B[sparse.newaxis.batch, :]   add a batch axis, error if not possible B[..., sparse.newaxis]   add a sparse axis if possible, dense otherwise B[..., sparse.newaxis.dense]   add a dense axis if possible, error if not possible ```",2022-04-06T18:09:48Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/10168,Related: CC([sparse] bcoo_broadcast_in_dim: default to adding leading batch dimensions),I'm going to close this because I don't think we'll be adding this feature.
10597,"以下是一个github上的jax下的一个issue, 标题是(build rocm JAX failed)， 内容是 (Please:  [ ] Check for duplicate issues.  [ ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: I use singularity to build an image and follow the README.md using the command like below. ```shell ./build_rocm.sh ```  [ ] If applicable, include full error messages/tracebacks. these are all the outputs. ``` Singularity> ./build_rocm.sh + ROCM_TF_FORK_REPO=https://github.com/ROCmSoftwarePlatform/tensorflowupstream + ROCM_TF_FORK_BRANCH=developupstream + ROCM_PATH=/opt/rocm/ + python3 ../build.py enable_rocm rocm_path=/opt/rocm/ bazel_options=override_repository=org_tensorflow=/tmp/tensorflowupstream      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: ./bazel5.1.0linuxx86_64 Bazel version: 5.1.0 Python binary path: /opt/conda/bin/python3 Python version: 3.9 NumPy version: 1.20.3 MKLDNN enabled: yes Target CPU: x86_64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: yes ROCm toolkit path: /opt/rocm/ ROCm amdgpu targets: gfx900,gfx906,gfx908,gfx90a,gfx1030 Building XLA and installing it in the jaxlib source tree... ./bazel5.1.0linuxx86_64 run verbose_failures=true override_repository=org_tensorflow=/tmp/tensorflowupstream config=avx_posix config=mkl_open_source_only config=rocm :build_wheel  output_path=/home/code/jax/build/rocm/dist cpu=x86_64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /home/code/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /home/code/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /home/code/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/opt/conda/bin/python3 action_env=PYENV_ROOT python_path=/opt/conda/bin/python3 action_env ROCM_PATH=/opt/rocm/ distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /home/code/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /home/code/jax/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /home/code/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:rocm in file /home/code/jax/.bazelrc: crosstool_top=//crosstool:toolchain define=using_rocm=true define=using_rocm_hipcc=true define=xla_python_enable_gpu=true repo_env TF_NEED_ROCM=1 action_env TF_ROCM_AMDGPU_TARGETS=gfx900,gfx906,gfx908 INFO: Found applicable config definition build:rocm in file /home/code/jax/.jax_configure.bazelrc: action_env TF_ROCM_AMDGPU_TARGETS=gfx900,gfx906,gfx908,gfx90a,gfx1030 INFO: Found applicable config definition build:linux in file /home/code/jax/.bazelrc: config=posix copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /home/code/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 Loading: Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/2123408fb43a5c4afdf87dafd67117d9c0ff70cd.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading: 0 packages loaded Loading: 0 packages loaded INFO: Repository local_config_rocm instantiated at:   /home/code/jax/WORKSPACE:31:14: in    /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/tensorflow/workspace2.bzl:869:19: in workspace   /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/tensorflow/workspace2.bzl:101:19: in _tf_toolchains Repository rule rocm_configure defined at:   /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl:843:33: in  ERROR: An error occurred during the fetch of repository 'local_config_rocm':    Traceback (most recent call last):         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 824, column 38, in _rocm_autoconf_impl                 _create_local_rocm_repository(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 550, column 35, in _create_local_rocm_repository                 rocm_config = _get_rocm_config(repository_ctx, bash_bin, find_rocm_config_script)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 398, column 30, in _get_rocm_config                 config = find_rocm_config(repository_ctx, find_rocm_config_script)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 376, column 41, in find_rocm_config                 exec_result = _exec_find_rocm_config(repository_ctx, script_path)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 372, column 19, in _exec_find_rocm_config                 return execute(repository_ctx, [python_bin, ""c"", decompress_and_execute_cmd])         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed ERROR: ROCm version file not found in ['include/rocmcore/rocm_version.h', 'include/rocm_version.h'] ERROR: /home/code/jax/WORKSPACE:31:14: fetching rocm_configure rule //external:local_config_rocm: Traceback (most recent call last):         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 824, column 38, in _rocm_autoconf_impl                 _create_local_rocm_repository(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 550, column 35, in _create_local_rocm_repository                 rocm_config = _get_rocm_config(repository_ctx, bash_bin, find_rocm_config_script)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 398, column 30, in _get_rocm_config                 config = find_rocm_config(repository_ctx, find_rocm_config_script)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 376, column 41, in find_rocm_config                 exec_result = _exec_find_rocm_config(repository_ctx, script_path)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 372, column 19, in _exec_find_rocm_config                 return execute(repository_ctx, [python_bin, ""c"", decompress_and_execute_cmd])         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed ERROR: ROCm version file not found in ['include/rocmcore/rocm_version.h', 'include/rocm_version.h'] INFO: Repository bazel_skylib instantiated at:   /home/code/jax/WORKSPACE:28:14: in    /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/tensorflow/workspace3.bzl:21:17: in workspace Repository rule http_archive defined at:   /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/bazel_tools/tools/build_defs/repo/http.bzl:353:31: in  ERROR: Skipping ':build_wheel': no such package '//rocm': Repository command failed ERROR: ROCm version file not found in ['include/rocmcore/rocm_version.h', 'include/rocm_version.h'] WARNING: Target pattern parsing failed. ERROR: no such package '//rocm': Repository command failed ERROR: ROCm version file not found in ['include/rocmcore/rocm_version.h', 'include/rocm_version.h'] INFO: Elapsed time: 77.870s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded) ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully (0 packages loaded) b'' Traceback (most recent call last):   File ""/home/code/jax/build/rocm/../build.py"", line 527, in      main()   File ""/home/code/jax/build/rocm/../build.py"", line 522, in main     shell(command)   File ""/home/code/jax/build/rocm/../build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/opt/conda/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/opt/conda/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.0linuxx86_64', 'run', 'verbose_failures=true', 'override_repository=org_tensorflow=/tmp/tensorflowupstream', 'config=avx_posix', 'config=mkl_open_source_only', 'config=rocm', ':build_wheel', '', 'output_path=/home/code/jax/build/rocm/dist', 'cpu=x86_64']' returned nonzero exit status 1. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,build rocm JAX failed,"Please:  [ ] Check for duplicate issues.  [ ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: I use singularity to build an image and follow the README.md using the command like below. ```shell ./build_rocm.sh ```  [ ] If applicable, include full error messages/tracebacks. these are all the outputs. ``` Singularity> ./build_rocm.sh + ROCM_TF_FORK_REPO=https://github.com/ROCmSoftwarePlatform/tensorflowupstream + ROCM_TF_FORK_BRANCH=developupstream + ROCM_PATH=/opt/rocm/ + python3 ../build.py enable_rocm rocm_path=/opt/rocm/ bazel_options=override_repository=org_tensorflow=/tmp/tensorflowupstream      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: ./bazel5.1.0linuxx86_64 Bazel version: 5.1.0 Python binary path: /opt/conda/bin/python3 Python version: 3.9 NumPy version: 1.20.3 MKLDNN enabled: yes Target CPU: x86_64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: yes ROCm toolkit path: /opt/rocm/ ROCm amdgpu targets: gfx900,gfx906,gfx908,gfx90a,gfx1030 Building XLA and installing it in the jaxlib source tree... ./bazel5.1.0linuxx86_64 run verbose_failures=true override_repository=org_tensorflow=/tmp/tensorflowupstream config=avx_posix config=mkl_open_source_only config=rocm :build_wheel  output_path=/home/code/jax/build/rocm/dist cpu=x86_64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /home/code/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /home/code/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /home/code/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/opt/conda/bin/python3 action_env=PYENV_ROOT python_path=/opt/conda/bin/python3 action_env ROCM_PATH=/opt/rocm/ distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /home/code/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /home/code/jax/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /home/code/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:rocm in file /home/code/jax/.bazelrc: crosstool_top=//crosstool:toolchain define=using_rocm=true define=using_rocm_hipcc=true define=xla_python_enable_gpu=true repo_env TF_NEED_ROCM=1 action_env TF_ROCM_AMDGPU_TARGETS=gfx900,gfx906,gfx908 INFO: Found applicable config definition build:rocm in file /home/code/jax/.jax_configure.bazelrc: action_env TF_ROCM_AMDGPU_TARGETS=gfx900,gfx906,gfx908,gfx90a,gfx1030 INFO: Found applicable config definition build:linux in file /home/code/jax/.bazelrc: config=posix copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /home/code/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 Loading: Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/2123408fb43a5c4afdf87dafd67117d9c0ff70cd.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading: 0 packages loaded Loading: 0 packages loaded INFO: Repository local_config_rocm instantiated at:   /home/code/jax/WORKSPACE:31:14: in    /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/tensorflow/workspace2.bzl:869:19: in workspace   /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/tensorflow/workspace2.bzl:101:19: in _tf_toolchains Repository rule rocm_configure defined at:   /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl:843:33: in  ERROR: An error occurred during the fetch of repository 'local_config_rocm':    Traceback (most recent call last):         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 824, column 38, in _rocm_autoconf_impl                 _create_local_rocm_repository(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 550, column 35, in _create_local_rocm_repository                 rocm_config = _get_rocm_config(repository_ctx, bash_bin, find_rocm_config_script)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 398, column 30, in _get_rocm_config                 config = find_rocm_config(repository_ctx, find_rocm_config_script)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 376, column 41, in find_rocm_config                 exec_result = _exec_find_rocm_config(repository_ctx, script_path)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 372, column 19, in _exec_find_rocm_config                 return execute(repository_ctx, [python_bin, ""c"", decompress_and_execute_cmd])         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed ERROR: ROCm version file not found in ['include/rocmcore/rocm_version.h', 'include/rocm_version.h'] ERROR: /home/code/jax/WORKSPACE:31:14: fetching rocm_configure rule //external:local_config_rocm: Traceback (most recent call last):         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 824, column 38, in _rocm_autoconf_impl                 _create_local_rocm_repository(repository_ctx)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 550, column 35, in _create_local_rocm_repository                 rocm_config = _get_rocm_config(repository_ctx, bash_bin, find_rocm_config_script)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 398, column 30, in _get_rocm_config                 config = find_rocm_config(repository_ctx, find_rocm_config_script)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 376, column 41, in find_rocm_config                 exec_result = _exec_find_rocm_config(repository_ctx, script_path)         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/gpus/rocm_configure.bzl"", line 372, column 19, in _exec_find_rocm_config                 return execute(repository_ctx, [python_bin, ""c"", decompress_and_execute_cmd])         File ""/root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute                 fail( Error in fail: Repository command failed ERROR: ROCm version file not found in ['include/rocmcore/rocm_version.h', 'include/rocm_version.h'] INFO: Repository bazel_skylib instantiated at:   /home/code/jax/WORKSPACE:28:14: in    /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/org_tensorflow/tensorflow/workspace3.bzl:21:17: in workspace Repository rule http_archive defined at:   /root/.cache/bazel/_bazel_root/7777a22c05c38dcb5674638712ab6fae/external/bazel_tools/tools/build_defs/repo/http.bzl:353:31: in  ERROR: Skipping ':build_wheel': no such package '//rocm': Repository command failed ERROR: ROCm version file not found in ['include/rocmcore/rocm_version.h', 'include/rocm_version.h'] WARNING: Target pattern parsing failed. ERROR: no such package '//rocm': Repository command failed ERROR: ROCm version file not found in ['include/rocmcore/rocm_version.h', 'include/rocm_version.h'] INFO: Elapsed time: 77.870s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded) ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully (0 packages loaded) b'' Traceback (most recent call last):   File ""/home/code/jax/build/rocm/../build.py"", line 527, in      main()   File ""/home/code/jax/build/rocm/../build.py"", line 522, in main     shell(command)   File ""/home/code/jax/build/rocm/../build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/opt/conda/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/opt/conda/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.0linuxx86_64', 'run', 'verbose_failures=true', 'override_repository=org_tensorflow=/tmp/tensorflowupstream', 'config=avx_posix', 'config=mkl_open_source_only', 'config=rocm', ':build_wheel', '', 'output_path=/home/code/jax/build/rocm/dist', 'cpu=x86_64']' returned nonzero exit status 1. ```",2022-04-06T13:40:08Z,bug contributions welcome,closed,0,5,https://github.com/jax-ml/jax/issues/10162,amd ,"I installed rocm 5.1.3, and have a similar failure during the build process as well: ``` Downloading bazel from: https://github.com/bazelbuild/bazel/releases/download/5.1.1/bazel5.1.1 linuxx86_64                                                                                   Bazel binary path: ./bazel5.1.1linuxx86_64                                                   Bazel version: 5.1.1                                                                            Python binary path: /usr/bin/python3                                                            Python version: 3.9                                                                             NumPy version: 1.19.5                                                                           MKLDNN enabled: yes                            Target CPU: x86_64      Target CPU features: release                    CUDA enabled: no        TPU enabled: no         ROCm enabled: yes       ROCm toolkit path: /opt/rocm5.1.0              ROCm amdgpu targets: gfx900,gfx906,gfx908,gfx90a,gfx1030   Building XLA and installing it in the jaxlib source tree... ./bazel5.1.1linuxx86_64 run verbose_failures=true override_repository=org_tensorflow=/tmp/tensorflowupstream config=avx_posix config=mkl_open_source_only config=rocm :build_wheel  output_path=/workspace/dist cpu=x86_64 b'' Traceback (most recent call last):   File ""/workspace/./build/build.py"", line 528, in      main()   File ""/workspace/./build/build.py"", line 523, in main     shell(command)   File ""/workspace/./build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/usr/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/usr/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.1linuxx86_64', 'run', 'verbose_failures=true', 'override_repository=org_tensorflow=/tmp/tensorflowupstream', 'config=avx_posix', 'config=mkl_open_source_only', 'config=rocm', ':build_wheel', '', 'output_path=/workspace/dist', 'cpu=x86_64']' returned nonzero exit status 2. ```", We are aware of the issue and are working on a fix for it.,The OP did not have ROCm installed in their image.,I'm guessing this issue is fixed!
1365,"以下是一个github上的jax下的一个issue, 标题是(unsigned int behaviour retains negative indices fix)， 内容是 (I was trying to avoid the extra cost of the `add` and `select_n` thats present in promise_in_bounds, clip and fill so I went to `u32`. Looking at the jaxpr it does the compare still and then converts to `i32`. This seems wrong. This relates to https://github.com/google/jax/issues/10063, I would also be okay with the approach 'unsigned should not exist' and add a mode for 'promise_in_bounds_positive'. ```python import jax import jax.numpy as jnp indices = jnp.array([0,3,4]).astype('uint32') x = jnp.arange(5) print(jax.make_jaxpr(x.__getitem__)(indices))  Output: { lambda a:i32[5]; b:u32[3]. let     c:bool[3] = lt b 0     d:u32[3] = add b 5     e:u32[3] = select_n c b d     f:i32[3] = convert_element_type[new_dtype=int32 weak_type=False] e     g:i32[3,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(3, 1)] f     h:i32[3] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))       fill_value=None       indices_are_sorted=False       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(1,)       unique_indices=False     ] a g   in (h,) } ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,unsigned int behaviour retains negative indices fix,"I was trying to avoid the extra cost of the `add` and `select_n` thats present in promise_in_bounds, clip and fill so I went to `u32`. Looking at the jaxpr it does the compare still and then converts to `i32`. This seems wrong. This relates to https://github.com/google/jax/issues/10063, I would also be okay with the approach 'unsigned should not exist' and add a mode for 'promise_in_bounds_positive'. ```python import jax import jax.numpy as jnp indices = jnp.array([0,3,4]).astype('uint32') x = jnp.arange(5) print(jax.make_jaxpr(x.__getitem__)(indices))  Output: { lambda a:i32[5]; b:u32[3]. let     c:bool[3] = lt b 0     d:u32[3] = add b 5     e:u32[3] = select_n c b d     f:i32[3] = convert_element_type[new_dtype=int32 weak_type=False] e     g:i32[3,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(3, 1)] f     h:i32[3] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,))       fill_value=None       indices_are_sorted=False       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(1,)       unique_indices=False     ] a g   in (h,) } ```",2022-04-05T13:13:59Z,enhancement P2 (eventual),closed,0,4,https://github.com/jax-ml/jax/issues/10142,Thanks for raising this  we could definitely specialcase the negative indices check on the dtype.,"There’s two things: 1  Negative index catching 2  Conversion to i32 for gather  Even when (1) is removed you still will pay a performance penalty for (2). Given JAX already has special int rules for out of bounds indexing the encoding of even more rules as an extra dtype u32 means you now have to deal with both dtype and bounds mode cases. Adding an extra mode solves both things and keeps the whole conversation related to just gather mode not dtype too, uint32 could be deprecated as even if it does have a marginal gain of increases range that increases range is lost and confusing when it’s forcibly converted to i32 for the ops.","We could probably avoid conversion to signed int as well, though there are some corner cases that need to be handled. I don't think this needs an extra indexing mode to account for this.",Original issue closed. The idea of an unsigned int accepting gather would be a separate issue which I don’t intend to file. Thank you.
8353,"以下是一个github上的jax下的一个issue, 标题是(Update pillow requirement from <9.1.0,>=8.3.1 to >=8.3.1,<9.2.0)， 内容是 (Updates the requirements on pillow to permit the latest version.  Release notes Sourced from pillow's releases.  9.1.0 https://pillow.readthedocs.io/en/stable/releasenotes/9.1.0.html Changes  Add support for multiple component transformation to JPEG2000  CC(add support for scipy.stats.poisson.cdf) [@​scaramallion] Fix loading FriBiDi on Alpine  CC(fix convert_element_type on large Python int inputs) [@​nulano] Added setting for converting GIF P frames to RGB  CC(未找到相关数据) [@​radarhere] Allow 1 mode images to be inverted  CC(aheadoftime compilation) [@​radarhere] Raise ValueError when trying to save empty JPEG  CC(未找到相关数据) [@​radarhere] Always save TIFF with contiguous planar configuration  CC(未找到相关数据) [@​radarhere] Connected discontiguous polygon corners  CC(Initial commit of seventh JAX101 notebook) [@​radarhere] Corrected memory allocation  CC(DOC: add SPMD to the glossary) [@​radarhere] Ensure Tkinter hook is activated for getimage()  CC(Suppress gRPC log spam on Cloud TPU.) [@​radarhere] Fixed comparison warnings  CC(未找到相关数据) [@​radarhere] Use screencapture arguments to crop on macOS  CC(未找到相关数据) [@​radarhere] Do not mark L mode JPEG as 1 bit in PDF  CC(未找到相关数据) [@​radarhere] Fixed saving L mode GIF with background as WebP  CC(未找到相关数据) [@​radarhere] Do not expand GIF during n_frames  CC(未找到相关数据) [@​radarhere] Added support for reading I;16R TIFF images  CC(use c++ jit dispatch path for opbyop execution) [@​radarhere] If an error occurs after creating a file, remove the file  CC('TypeError: iteration over a 0d array' when using vmap) [@​radarhere] decode() should return 1 when finished  CC(make constant handlers follow type mro) [@​radarhere] Fixed calling DisplayViewer or XVViewer without a title  CC(generalize ravel_pytree to handle int types) [@​radarhere] Retain RGBA transparency when saving multiple GIF frames  CC(Update README, etc. for jaxlib 0.1.64 release) [@​radarhere] Save additional ICO frames with other bit depths if supplied  CC(jax.lax.dynamic_slice casts uint indices to a too narrow signed type) [@​radarhere] Handle EXIF data truncated to just the header  CC(Make sublevel weakreferable, and enable the leak checker on sublevels.) [@​radarhere] Added support for reading BMP images with RLE8 compression  CC(未找到相关数据) [@​radarhere] Support Python distributions where _tkinter is compiled in  CC(reduce_window gradient crashes with general padding) [@​lukegb] Added WebP default duration of zero when saving  CC(未找到相关数据) [@​radarhere] Added support for PPM arbitrary maxval  CC(未找到相关数据) [@​radarhere] Removed unused variable  CC(grad(): improve error for traced argnums) [@​radarhere] libwebp 1.2.2 fixed endian bugs  CC(Automatically initialize Cloud TPU topology env vars if running on a Cloud TPU VM.) [@​radarhere] Added BigTIFF reading  CC([jax2tf] Added instructions for using OSS TensorFlow model server.) [@​radarhere] GIF seek performance improvements  CC(Add helpful message when `import jaxlib` fails.) [@​radarhere] Removed load_prepare nearly identical to ImageFile load_prepare  CC([host_callback] Enable configuring the outfeed receiver buffer size) [@​radarhere] When converting, clip I;16 to be unsigned, not signed  CC(unify configuration state handling) [@​radarhere] Check if self.im is not None  CC(Annotate scatter and random) [@​radarhere] Fixed loading L mode GIF with transparency  CC(Fix test failure with NumPy 1.20.) [@​radarhere] Ensure cleanup() is called for PyEncoders  CC(betainc batching rule is not quite correct) [@​radarhere] Various _accept changes  CC(CI: fix broken packageoverrides check) [@​radarhere] Improved handling of PPM header  CC(Add a mnist example that runs a flax model on TFLite using jax2tf.) [@​Piolie] Reset size when seeking away from &quot;Large Thumbnail&quot; MPO frame  CC(Internal rollback) [@​radarhere] Replace requirements.txt with extras  CC(未找到相关数据) [@​hugovk] Added PyEncoder and support BLP saving  CC(WIP: experiment with CodeCov integration) [@​radarhere] Handle TGA images with packets that cross scan lines  CC(Add extras_require to setup.py) [@​radarhere] Added FITS reading  CC([JAX] Add a .weak_type attribute to C++ array objects.) [@​radarhere] Added rawmode argument to Image.getpalette()  CC(未找到相关数据) [@​radarhere] Fixed BUFR, GRIB and HDF5 stub saving  CC(0.2.9 fails: ModuleNotFoundError: No module named 'jaxlib') [@​radarhere] Changed quantize default dither to FLOYDSTEINBERG  CC(BUG: fix jnp.result_type for noncanonical weak types) [@​radarhere]    ... (truncated)   Changelog Sourced from pillow's changelog.  9.1.0 (20220401)   Add support for multiple component transformation to JPEG2000  CC(add support for scipy.stats.poisson.cdf) [scaramallion, radarhere, hugovk]   Fix loading FriBiDi on Alpine  CC(fix convert_element_type on large Python int inputs) [nulano]   Added setting for converting GIF P frames to RGB  CC(未找到相关数据) [radarhere]   Allow 1 mode images to be inverted  CC(aheadoftime compilation) [radarhere]   Raise ValueError when trying to save empty JPEG  CC(未找到相关数据) [radarhere]   Always save TIFF with contiguous planar configuration  CC(未找到相关数据) [radarhere]   Connected discontiguous polygon corners  CC(Initial commit of seventh JAX101 notebook) [radarhere]   Ensure Tkinter hook is activated for getimage()  CC(Suppress gRPC log spam on Cloud TPU.) [radarhere]   Use screencapture arguments to crop on macOS  CC(未找到相关数据) [radarhere]   Do not mark L mode JPEG as 1 bit in PDF  CC(未找到相关数据) [radarhere]   Added support for reading I;16R TIFF images  CC(use c++ jit dispatch path for opbyop execution) [radarhere]   If an error occurs after creating a file, remove the file  CC('TypeError: iteration over a 0d array' when using vmap) [radarhere]   Fixed calling DisplayViewer or XVViewer without a title  CC(generalize ravel_pytree to handle int types) [radarhere]   Retain RGBA transparency when saving multiple GIF frames  CC(Update README, etc. for jaxlib 0.1.64 release) [radarhere]   Save additional ICO frames with other bit depths if supplied  CC(jax.lax.dynamic_slice casts uint indices to a too narrow signed type) [radarhere]   Handle EXIF data truncated to just the header  CC(Make sublevel weakreferable, and enable the leak checker on sublevels.) [radarhere]     ... (truncated)   Commits  5d07022 Update CHANGES.rst [ci skip] 1e0bc4a 9.1.0 version bump 0606f02 Merge pull request  CC(JAX 101: add pytrees notebook) from hugovk/releasenotes 703f54c Merge pull request  CC(add support for scipy.stats.poisson.cdf) from scaramallion/devj2k 4181f49 Wording 78c389e Merge pull request  CC(DOC: add note about return type in jnp.linalg.eig) from radarhere/freetype 081c3ba Updated freetype to 2.12 31e08c8 Wording e0de5b6 Document mct and no_jp2 options for saving JPEG 2000 698f529 Parametrized test Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Update pillow requirement from <9.1.0,>=8.3.1 to >=8.3.1,<9.2.0","Updates the requirements on pillow to permit the latest version.  Release notes Sourced from pillow's releases.  9.1.0 https://pillow.readthedocs.io/en/stable/releasenotes/9.1.0.html Changes  Add support for multiple component transformation to JPEG2000  CC(add support for scipy.stats.poisson.cdf) [@​scaramallion] Fix loading FriBiDi on Alpine  CC(fix convert_element_type on large Python int inputs) [@​nulano] Added setting for converting GIF P frames to RGB  CC(未找到相关数据) [@​radarhere] Allow 1 mode images to be inverted  CC(aheadoftime compilation) [@​radarhere] Raise ValueError when trying to save empty JPEG  CC(未找到相关数据) [@​radarhere] Always save TIFF with contiguous planar configuration  CC(未找到相关数据) [@​radarhere] Connected discontiguous polygon corners  CC(Initial commit of seventh JAX101 notebook) [@​radarhere] Corrected memory allocation  CC(DOC: add SPMD to the glossary) [@​radarhere] Ensure Tkinter hook is activated for getimage()  CC(Suppress gRPC log spam on Cloud TPU.) [@​radarhere] Fixed comparison warnings  CC(未找到相关数据) [@​radarhere] Use screencapture arguments to crop on macOS  CC(未找到相关数据) [@​radarhere] Do not mark L mode JPEG as 1 bit in PDF  CC(未找到相关数据) [@​radarhere] Fixed saving L mode GIF with background as WebP  CC(未找到相关数据) [@​radarhere] Do not expand GIF during n_frames  CC(未找到相关数据) [@​radarhere] Added support for reading I;16R TIFF images  CC(use c++ jit dispatch path for opbyop execution) [@​radarhere] If an error occurs after creating a file, remove the file  CC('TypeError: iteration over a 0d array' when using vmap) [@​radarhere] decode() should return 1 when finished  CC(make constant handlers follow type mro) [@​radarhere] Fixed calling DisplayViewer or XVViewer without a title  CC(generalize ravel_pytree to handle int types) [@​radarhere] Retain RGBA transparency when saving multiple GIF frames  CC(Update README, etc. for jaxlib 0.1.64 release) [@​radarhere] Save additional ICO frames with other bit depths if supplied  CC(jax.lax.dynamic_slice casts uint indices to a too narrow signed type) [@​radarhere] Handle EXIF data truncated to just the header  CC(Make sublevel weakreferable, and enable the leak checker on sublevels.) [@​radarhere] Added support for reading BMP images with RLE8 compression  CC(未找到相关数据) [@​radarhere] Support Python distributions where _tkinter is compiled in  CC(reduce_window gradient crashes with general padding) [@​lukegb] Added WebP default duration of zero when saving  CC(未找到相关数据) [@​radarhere] Added support for PPM arbitrary maxval  CC(未找到相关数据) [@​radarhere] Removed unused variable  CC(grad(): improve error for traced argnums) [@​radarhere] libwebp 1.2.2 fixed endian bugs  CC(Automatically initialize Cloud TPU topology env vars if running on a Cloud TPU VM.) [@​radarhere] Added BigTIFF reading  CC([jax2tf] Added instructions for using OSS TensorFlow model server.) [@​radarhere] GIF seek performance improvements  CC(Add helpful message when `import jaxlib` fails.) [@​radarhere] Removed load_prepare nearly identical to ImageFile load_prepare  CC([host_callback] Enable configuring the outfeed receiver buffer size) [@​radarhere] When converting, clip I;16 to be unsigned, not signed  CC(unify configuration state handling) [@​radarhere] Check if self.im is not None  CC(Annotate scatter and random) [@​radarhere] Fixed loading L mode GIF with transparency  CC(Fix test failure with NumPy 1.20.) [@​radarhere] Ensure cleanup() is called for PyEncoders  CC(betainc batching rule is not quite correct) [@​radarhere] Various _accept changes  CC(CI: fix broken packageoverrides check) [@​radarhere] Improved handling of PPM header  CC(Add a mnist example that runs a flax model on TFLite using jax2tf.) [@​Piolie] Reset size when seeking away from &quot;Large Thumbnail&quot; MPO frame  CC(Internal rollback) [@​radarhere] Replace requirements.txt with extras  CC(未找到相关数据) [@​hugovk] Added PyEncoder and support BLP saving  CC(WIP: experiment with CodeCov integration) [@​radarhere] Handle TGA images with packets that cross scan lines  CC(Add extras_require to setup.py) [@​radarhere] Added FITS reading  CC([JAX] Add a .weak_type attribute to C++ array objects.) [@​radarhere] Added rawmode argument to Image.getpalette()  CC(未找到相关数据) [@​radarhere] Fixed BUFR, GRIB and HDF5 stub saving  CC(0.2.9 fails: ModuleNotFoundError: No module named 'jaxlib') [@​radarhere] Changed quantize default dither to FLOYDSTEINBERG  CC(BUG: fix jnp.result_type for noncanonical weak types) [@​radarhere]    ... (truncated)   Changelog Sourced from pillow's changelog.  9.1.0 (20220401)   Add support for multiple component transformation to JPEG2000  CC(add support for scipy.stats.poisson.cdf) [scaramallion, radarhere, hugovk]   Fix loading FriBiDi on Alpine  CC(fix convert_element_type on large Python int inputs) [nulano]   Added setting for converting GIF P frames to RGB  CC(未找到相关数据) [radarhere]   Allow 1 mode images to be inverted  CC(aheadoftime compilation) [radarhere]   Raise ValueError when trying to save empty JPEG  CC(未找到相关数据) [radarhere]   Always save TIFF with contiguous planar configuration  CC(未找到相关数据) [radarhere]   Connected discontiguous polygon corners  CC(Initial commit of seventh JAX101 notebook) [radarhere]   Ensure Tkinter hook is activated for getimage()  CC(Suppress gRPC log spam on Cloud TPU.) [radarhere]   Use screencapture arguments to crop on macOS  CC(未找到相关数据) [radarhere]   Do not mark L mode JPEG as 1 bit in PDF  CC(未找到相关数据) [radarhere]   Added support for reading I;16R TIFF images  CC(use c++ jit dispatch path for opbyop execution) [radarhere]   If an error occurs after creating a file, remove the file  CC('TypeError: iteration over a 0d array' when using vmap) [radarhere]   Fixed calling DisplayViewer or XVViewer without a title  CC(generalize ravel_pytree to handle int types) [radarhere]   Retain RGBA transparency when saving multiple GIF frames  CC(Update README, etc. for jaxlib 0.1.64 release) [radarhere]   Save additional ICO frames with other bit depths if supplied  CC(jax.lax.dynamic_slice casts uint indices to a too narrow signed type) [radarhere]   Handle EXIF data truncated to just the header  CC(Make sublevel weakreferable, and enable the leak checker on sublevels.) [radarhere]     ... (truncated)   Commits  5d07022 Update CHANGES.rst [ci skip] 1e0bc4a 9.1.0 version bump 0606f02 Merge pull request  CC(JAX 101: add pytrees notebook) from hugovk/releasenotes 703f54c Merge pull request  CC(add support for scipy.stats.poisson.cdf) from scaramallion/devj2k 4181f49 Wording 78c389e Merge pull request  CC(DOC: add note about return type in jnp.linalg.eig) from radarhere/freetype 081c3ba Updated freetype to 2.12 31e08c8 Wording e0de5b6 Document mct and no_jp2 options for saving JPEG 2000 698f529 Parametrized test Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-04-04T17:13:55Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/10136, ignore We can't do this upgrade because the current Keras release uses deprecated APIs,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
13594,"以下是一个github上的jax下的一个issue, 标题是(Build failures at HEAD on Apple ARM64)， 内容是 ( [x] If applicable, include full error messages/tracebacks. Current top of main does not build on my machine (Apple M1 Pro, macOS 12.3.1): ``` jax on  main [⇡$!] via jax  ➜ python build/build.py                                  _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ b'\x1b[31mERROR: The project you\'re trying to build requires Bazel 5.1.0 (specified in /Users/nicholasjunge/Workspaces/python/jax/.bazelversion), but it wasn\'t found in /opt/homebrew/Cellar/bazel/5.0.0/libexec/bin.\x1b[0m\n\nBazel binaries for all official releases can be downloaded from here:\n  https://github.com/bazelbuild/bazel/releases\n\nYou can download the required version directly using this command:\n  (cd ""/opt/homebrew/Cellar/bazel/5.0.0/libexec/bin"" && curl fLO https://releases.bazel.build/5.1.0/release/bazel5.1.0darwinarm64 && chmod +x bazel5.1.0darwinarm64)\n' Bazel binary path: ./bazel5.1.0darwinarm64 Bazel version: 5.1.0 Python binary path: /Users/nicholasjunge/Workspaces/python/jax/venv/bin/python Python version: 3.9 NumPy version: 1.22.3 MKLDNN enabled: yes Target CPU: arm64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel5.1.0darwinarm64 run verbose_failures=true toolchain_resolution_debug=//tools/cpp:toolchain_type config=mkl_open_source_only :build_wheel  output_path=/Users/nicholasjunge/Workspaces/python/jax/dist cpu=arm64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python action_env=PYENV_ROOT python_path=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 Loading:  Loading: 0 packages loaded INFO: Build option distinct_host_configuration has changed, discarding analysis cache. Analyzing: target //build:build_wheel (0 packages loaded, 0 targets configured) DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 0400"" DEBUG: Repository io_bazel_rules_docker instantiated at:   /Users/nicholasjunge/Workspaces/python/jax/WORKSPACE:37:14: in    /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/org_tensorflow/tensorflow/workspace0.bzl:107:34: in workspace   /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories Repository rule git_repository defined at:   /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in  INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerarmeabiv7a; mismatching values: arm INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerarmeabiv7a; mismatching values: arm INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_arm64; mismatching values: arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_arm64; mismatching values: arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_arm64e; mismatching values: arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_arm64e; mismatching values: arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_x86_64; mismatching values: x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_x86_64; mismatching values: x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64e; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64e; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_armv7; mismatching values: ios, armv7 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_armv7; mismatching values: ios, armv7 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_i386; mismatching values: ios, i386 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_i386; mismatching values: ios, i386 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_sim_arm64; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_sim_arm64; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_arm64; mismatching values: tvos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_arm64; mismatching values: tvos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_sim_arm64; mismatching values: tvos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_sim_arm64; mismatching values: tvos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_x86_64; mismatching values: tvos, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_x86_64; mismatching values: tvos, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64; mismatching values: watchos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64; mismatching values: watchos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64_32; mismatching values: watchos, arm64_32 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64_32; mismatching values: watchos, arm64_32 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_armv7k; mismatching values: watchos, armv7k INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_armv7k; mismatching values: watchos, armv7k INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_i386; mismatching values: watchos, i386 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_i386; mismatching values: watchos, i386 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_x86_64; mismatching values: watchos, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_x86_64; mismatching values: watchos, x86_64 INFO: ToolchainResolution:   Type //tools/cpp:toolchain_type: target platform //:host: No toolchains found. ERROR: /Users/nicholasjunge/Workspaces/python/jax/build/BUILD.bazel:25:10: While resolving toolchains for target //build:build_wheel: No matching toolchains found for types //tools/cpp:toolchain_type. Maybe incompatible_use_cc_configure_from_rules_cc has been flipped and there is no default C++ toolchain added in the WORKSPACE file? See https://github.com/bazelbuild/bazel/issues/10134 for details and migration instructions. ERROR: Analysis of target '//build:build_wheel' failed; build aborted:  INFO: Elapsed time: 0.093s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded, 253 targets configured) ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully (0 packages loaded, 253 targets configured) b'' Traceback (most recent call last):   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 527, in      main()   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 522, in main     shell(command)   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/opt/homebrew/Cellar/python.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/opt/homebrew/Cellar/python.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.0darwinarm64', 'run', 'verbose_failures=true', 'toolchain_resolution_debug=//tools/cpp:toolchain_type', 'config=mkl_open_source_only', ':build_wheel', '', 'output_path=/Users/nicholasjunge/Workspaces/python/jax/dist', 'cpu=arm64']' returned nonzero exit status 1. ``` Some notes / observations: * Apparently, toolchain resolution does not work properly for downloaded versions of Bazel; currently, the latest Homebrewsupplied Bazel is still v5.0.0. * Editing the `.bazelversion` (and, as of recently, `build.py`) to use Bazel 5.0.0 also fails, with a different error (it complains about a `win_ver_file` setting in Tensorflow's `BUILD` file, which I guess was not supported with v5.0.0). * The above output was generated by injecting the `toolchain_resolution_debug=//tools/cpp:toolchain_type` flag into the build args. Somehow, all candidate toolchains are rejected, but I don't know why. * I switched over to a local Tensorflow repo on my machine (as mentioned in the WORKSPACE file). Apparently in that setting, Bazel uses whatever ref is checked out there (currently, that is `master` for me, at or near current HEAD). So I pulled in some of the recent changes in Tensorflow's build config. ``` ➜ git log commit e1bbbf55cd28abe60175a5cf106c4c2c9c9e044f (HEAD > main, upstream/main) Merge: 359b614b c72d8f6b Author: jax authors  Date:   Sun Apr 3 12:11:03 2022 0700     Merge pull request CC(remove string annotations from core.py) from mattjj:nostringannotations     PiperOriginRevId: 439174012 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Build failures at HEAD on Apple ARM64," [x] If applicable, include full error messages/tracebacks. Current top of main does not build on my machine (Apple M1 Pro, macOS 12.3.1): ``` jax on  main [⇡$!] via jax  ➜ python build/build.py                                  _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ b'\x1b[31mERROR: The project you\'re trying to build requires Bazel 5.1.0 (specified in /Users/nicholasjunge/Workspaces/python/jax/.bazelversion), but it wasn\'t found in /opt/homebrew/Cellar/bazel/5.0.0/libexec/bin.\x1b[0m\n\nBazel binaries for all official releases can be downloaded from here:\n  https://github.com/bazelbuild/bazel/releases\n\nYou can download the required version directly using this command:\n  (cd ""/opt/homebrew/Cellar/bazel/5.0.0/libexec/bin"" && curl fLO https://releases.bazel.build/5.1.0/release/bazel5.1.0darwinarm64 && chmod +x bazel5.1.0darwinarm64)\n' Bazel binary path: ./bazel5.1.0darwinarm64 Bazel version: 5.1.0 Python binary path: /Users/nicholasjunge/Workspaces/python/jax/venv/bin/python Python version: 3.9 NumPy version: 1.22.3 MKLDNN enabled: yes Target CPU: arm64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel5.1.0darwinarm64 run verbose_failures=true toolchain_resolution_debug=//tools/cpp:toolchain_type config=mkl_open_source_only :build_wheel  output_path=/Users/nicholasjunge/Workspaces/python/jax/dist cpu=arm64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python action_env=PYENV_ROOT python_path=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 Loading:  Loading: 0 packages loaded INFO: Build option distinct_host_configuration has changed, discarding analysis cache. Analyzing: target //build:build_wheel (0 packages loaded, 0 targets configured) DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 0400"" DEBUG: Repository io_bazel_rules_docker instantiated at:   /Users/nicholasjunge/Workspaces/python/jax/WORKSPACE:37:14: in    /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/org_tensorflow/tensorflow/workspace0.bzl:107:34: in workspace   /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories Repository rule git_repository defined at:   /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in  INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerarmeabiv7a; mismatching values: arm INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerarmeabiv7a; mismatching values: arm INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_arm64; mismatching values: arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_arm64; mismatching values: arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_arm64e; mismatching values: arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_arm64e; mismatching values: arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_x86_64; mismatching values: x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_x86_64; mismatching values: x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64e; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64e; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_armv7; mismatching values: ios, armv7 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_armv7; mismatching values: ios, armv7 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_i386; mismatching values: ios, i386 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_i386; mismatching values: ios, i386 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_sim_arm64; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_sim_arm64; mismatching values: ios, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_arm64; mismatching values: tvos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_arm64; mismatching values: tvos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_sim_arm64; mismatching values: tvos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_sim_arm64; mismatching values: tvos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_x86_64; mismatching values: tvos, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_x86_64; mismatching values: tvos, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64; mismatching values: watchos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64; mismatching values: watchos, arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64_32; mismatching values: watchos, arm64_32 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64_32; mismatching values: watchos, arm64_32 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_armv7k; mismatching values: watchos, armv7k INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_armv7k; mismatching values: watchos, armv7k INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_i386; mismatching values: watchos, i386 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_i386; mismatching values: watchos, i386 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_x86_64; mismatching values: watchos, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_x86_64; mismatching values: watchos, x86_64 INFO: ToolchainResolution:   Type //tools/cpp:toolchain_type: target platform //:host: No toolchains found. ERROR: /Users/nicholasjunge/Workspaces/python/jax/build/BUILD.bazel:25:10: While resolving toolchains for target //build:build_wheel: No matching toolchains found for types //tools/cpp:toolchain_type. Maybe incompatible_use_cc_configure_from_rules_cc has been flipped and there is no default C++ toolchain added in the WORKSPACE file? See https://github.com/bazelbuild/bazel/issues/10134 for details and migration instructions. ERROR: Analysis of target '//build:build_wheel' failed; build aborted:  INFO: Elapsed time: 0.093s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded, 253 targets configured) ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully (0 packages loaded, 253 targets configured) b'' Traceback (most recent call last):   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 527, in      main()   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 522, in main     shell(command)   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/opt/homebrew/Cellar/python.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/opt/homebrew/Cellar/python.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.0darwinarm64', 'run', 'verbose_failures=true', 'toolchain_resolution_debug=//tools/cpp:toolchain_type', 'config=mkl_open_source_only', ':build_wheel', '', 'output_path=/Users/nicholasjunge/Workspaces/python/jax/dist', 'cpu=arm64']' returned nonzero exit status 1. ``` Some notes / observations: * Apparently, toolchain resolution does not work properly for downloaded versions of Bazel; currently, the latest Homebrewsupplied Bazel is still v5.0.0. * Editing the `.bazelversion` (and, as of recently, `build.py`) to use Bazel 5.0.0 also fails, with a different error (it complains about a `win_ver_file` setting in Tensorflow's `BUILD` file, which I guess was not supported with v5.0.0). * The above output was generated by injecting the `toolchain_resolution_debug=//tools/cpp:toolchain_type` flag into the build args. Somehow, all candidate toolchains are rejected, but I don't know why. * I switched over to a local Tensorflow repo on my machine (as mentioned in the WORKSPACE file). Apparently in that setting, Bazel uses whatever ref is checked out there (currently, that is `master` for me, at or near current HEAD). So I pulled in some of the recent changes in Tensorflow's build config. ``` ➜ git log commit e1bbbf55cd28abe60175a5cf106c4c2c9c9e044f (HEAD > main, upstream/main) Merge: 359b614b c72d8f6b Author: jax authors  Date:   Sun Apr 3 12:11:03 2022 0700     Merge pull request CC(remove string annotations from core.py) from mattjj:nostringannotations     PiperOriginRevId: 439174012 ```",2022-04-04T13:44:26Z,bug,closed,0,17,https://github.com/jax-ml/jax/issues/10132,"To clarify: does it build with a newer TF tree from the WORKSPACE? i.e., is this just a case of us using a newer TF snapshot?","Just pulled in `master` again in my local Tensorflow fork, no changes. I think I am bitten by two different things here  first is Bazel 5.1.0 being downloaded locally and failing to resolve toolchains properly (if these are related, I cannot really say, due to limited Bazel experience), second is that with my Bazel 5.0.0 on PATH, I am getting errors from Tensorflow's BUILD.",5.1.0 is now required because of other changes in the TF tree's BUILD files. So we need to figure out what to do to get Bazel 5.1.0 building successfully on ARM. (I don't have access to such a machine myself.),"FYI, the `v5.0.0` attempt: ``` jax on  main [⇡$!] via jax took 4s  ➜ python build/build.py      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: /opt/homebrew/bin/bazel Bazel version: 5.0.0 Python binary path: /Users/nicholasjunge/Workspaces/python/jax/venv/bin/python Python version: 3.9 NumPy version: 1.22.3 MKLDNN enabled: yes Target CPU: arm64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... /opt/homebrew/bin/bazel run verbose_failures=true toolchain_resolution_debug=//tools/cpp:toolchain_type config=mkl_open_source_only :build_wheel  output_path=/Users/nicholasjunge/Workspaces/python/jax/dist cpu=arm64 Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python action_env=PYENV_ROOT python_path=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 Loading:  Loading: 0 packages loaded Loading: 0 packages loaded Analyzing: target //build:build_wheel (1 packages loaded, 0 targets configured) Analyzing: target //build:build_wheel (34 packages loaded, 12 targets configured) DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 0400"" DEBUG: Repository io_bazel_rules_docker instantiated at:   /Users/nicholasjunge/Workspaces/python/jax/WORKSPACE:37:14: in    /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/org_tensorflow/tensorflow/workspace0.bzl:107:34: in workspace   /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories Repository rule git_repository defined at:   /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in  INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerarmeabiv7a; mismatching values: arm INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerarmeabiv7a; mismatching values: arm INFO: ToolchainResolution:   Type //tools/cpp:toolchain_type: target platform //:host: execution //:platform: Selected toolchain //:cccompilerdarwin_arm64 INFO: ToolchainResolution:   Type //tools/cpp:toolchain_type: target platform //:host: execution //:host: Selected toolchain //:cccompilerdarwin_arm64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_x86_64; mismatching values: x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerdarwin_x86_64; mismatching values: x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64e; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_arm64e; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_armv7; mismatching values: ios, arm INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_armv7; mismatching values: ios, arm INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_i386; mismatching values: ios, x86_32 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_i386; mismatching values: ios, x86_32 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_sim_arm64; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_sim_arm64; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerios_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_arm64; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_arm64; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilertvos_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64_32; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_arm64_32; mismatching values: ios INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_armv7k; mismatching values: ios, arm INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_armv7k; mismatching values: ios, arm INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_i386; mismatching values: ios, x86_32 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_i386; mismatching values: ios, x86_32 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution:     Type //tools/cpp:toolchain_type: target platform //:host: Rejected toolchain //:cccompilerwatchos_x86_64; mismatching values: ios, x86_64 INFO: ToolchainResolution: Target platform //:host: Selected execution platform //:platform, type //tools/cpp:toolchain_type > toolchain //:cccompilerdarwin_arm64, type //tools/python:toolchain_type > toolchain //:py_runtime_pair ERROR: /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/org_tensorflow/tensorflow/BUILD:1004:21: //tensorflow:libtensorflow.so.2.10.0_st: no such attribute 'shared_lib_name' in 'cc_shared_library' rule ERROR: /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/org_tensorflow/tensorflow/BUILD:1004:21: //tensorflow:libtensorflow.so.2.10.0_st: no such attribute 'win_def_file' in 'cc_shared_library' rule ERROR: /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/org_tensorflow/tensorflow/BUILD:1004:21: //tensorflow:libtensorflow.2.10.0.dylib_st: no such attribute 'shared_lib_name' in 'cc_shared_library' rule ERROR: /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/org_tensorflow/tensorflow/BUILD:1004:21: //tensorflow:libtensorflow.2.10.0.dylib_st: no such attribute 'win_def_file' in 'cc_shared_library' rule ERROR: /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/org_tensorflow/tensorflow/BUILD:1004:21: //tensorflow:tensorflow.dll_st: no such attribute 'shared_lib_name' in 'cc_shared_library' rule ERROR: /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/org_tensorflow/tensorflow/BUILD:1004:21: //tensorflow:tensorflow.dll_st: no such attribute 'win_def_file' in 'cc_shared_library' rule ERROR: /Users/nicholasjunge/Workspaces/python/jax/build/BUILD.bazel:25:10: errors encountered resolving select() keys for //build:build_wheel ERROR: Analysis of target '//build:build_wheel' failed; build aborted:  INFO: Elapsed time: 5.142s INFO: 0 processes. FAILED: Build did NOT complete successfully (41 packages loaded, 242 targets configured) ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully (41 packages loaded, 242 targets configured) b'' Traceback (most recent call last):   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 527, in      main()   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 522, in main     shell(command)   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/opt/homebrew/Cellar/python.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/opt/homebrew/Cellar/python.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['/opt/homebrew/bin/bazel', 'run', 'verbose_failures=true', 'toolchain_resolution_debug=//tools/cpp:toolchain_type', 'config=mkl_open_source_only', ':build_wheel', '', 'output_path=/Users/nicholasjunge/Workspaces/python/jax/dist', 'cpu=arm64']' returned nonzero exit status 1. ```",I wonder whether https://github.com/bazelbuild/bazel/pull/14995 (which is mentioned in the Bazel 5.1.0 release notes) is relevant.,"Okay, I can take this on if you want, as it is my personal machine. Do you have some insights on toolchain resolution off the top of your head that may be useful for debugging? I assume this will be resolved once Homebrew updates the Bazel bottle. Still it might be worth investigating how to enable macOS builds with downloaded intree Bazel snapshots. (Otherwise, this functionality can't really be considered working on macOS ARM64 in my opinion)","No, I don't think Homebrew updates will fix this. My guess is this is some sort of legitimate Mac/ARM/Bazel 5.1.0 problem. The released Bazel should work without any homebrew stuff.","Fair point. I went digging into the Bazel cache and found that apparently the host was autodetected to have constraints  ``` //local_config_platform/constraints.bzl  DO NOT EDIT: automatically generated constraints list for local_config_platform  Autodetected host platform constraints. HOST_CONSTRAINTS = [   '//cpu:aarch64',   '//os:osx', ] ``` which would explain all of the toolchain mismatches. Something apparently went wrong when setting the host platform constraints, can I specifically ask Bazel to build for a `darwin_arm64` platform somehow, which Bazel 5.1 should understand (given the PR you just linked)?",I'd guess that comes from: https://github.com/bazelbuild/bazel/blob/e2853223f429ee30731c1015f83baed1570fcbe6/src/main/java/com/google/devtools/build/lib/bazel/repository/LocalConfigPlatformFunction.javaL117 I'm wondering if we should just report this to upstream Bazel and ask them.,"More problems: After removing the entire cache folder (to rule out cache conflicts between 5.0.0 and 5.1.0) and running with Bazel 5.1.0, I get a different error this time: ``` jax on  main [⇡$!] via jax took 6s  ➜ python build/build.py      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ b'\x1b[31mERROR: The project you\'re trying to build requires Bazel 5.1.0 (specified in /Users/nicholasjunge/Workspaces/python/jax/.bazelversion), but it wasn\'t found in /opt/homebrew/Cellar/bazel/5.0.0/libexec/bin.\x1b[0m\n\nBazel binaries for all official releases can be downloaded from here:\n  https://github.com/bazelbuild/bazel/releases\n\nYou can download the required version directly using this command:\n  (cd ""/opt/homebrew/Cellar/bazel/5.0.0/libexec/bin"" && curl fLO https://releases.bazel.build/5.1.0/release/bazel5.1.0darwinarm64 && chmod +x bazel5.1.0darwinarm64)\n' Bazel binary path: ./bazel5.1.0darwinarm64 Bazel version: 5.1.0 Python binary path: /Users/nicholasjunge/Workspaces/python/jax/venv/bin/python Python version: 3.9 NumPy version: 1.22.3 MKLDNN enabled: yes Target CPU: arm64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel5.1.0darwinarm64 run verbose_failures=true subcommands toolchain_resolution_debug=//tools/cpp:toolchain_type subcommands config=mkl_open_source_only :build_wheel  output_path=/Users/nicholasjunge/Workspaces/python/jax/dist cpu=arm64 Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /Users/nicholasjunge/Workspaces/python/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python action_env=PYENV_ROOT python_path=/Users/nicholasjunge/Workspaces/python/jax/venv/bin/python distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/nicholasjunge/Workspaces/python/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 Loading:  Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/2123408fb43a5c4afdf87dafd67117d9c0ff70cd.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/5197d2791f908815134ad48d7b966de2d8c47eeb.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found ERROR: /Users/nicholasjunge/Workspaces/python/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: java.io.IOException: /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/llvmraw (Directory not empty) ERROR: no such package 'raw//utils/bazel': /private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/llvmraw (Directory not empty) INFO: Elapsed time: 35.139s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded) ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully (0 packages loaded) b'' Traceback (most recent call last):   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 530, in      main()   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 525, in main     shell(command)   File ""/Users/nicholasjunge/Workspaces/python/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/opt/homebrew/Cellar/python.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/opt/homebrew/Cellar/python.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.0darwinarm64', 'run', 'verbose_failures=true', 'subcommands', 'toolchain_resolution_debug=//tools/cpp:toolchain_type', 'subcommands', 'config=mkl_open_source_only', ':build_wheel', '', 'output_path=/Users/nicholasjunge/Workspaces/python/jax/dist', 'cpu=arm64']' returned nonzero exit status 1. ``` This occurs inside the `tf_workspace3` function, which leads me to believe that Tensorflow itself will not build at HEAD on macOS ARM64, either. Do you happen to have a snapshot that might work, but already requires Bazel 5.1.0? (I would have posted you the new constraints file, but Bazel crashed with the above before writing it out.)","I filed https://github.com/bazelbuild/bazel/issues/15175, and referenced this thread. I hope I presented the issue in a reasonable way given our conversation  if you feel like anything is wrong about what I wrote there, please let me know, so I can correct it. Thanks!","Hey , your first suggestion was absolutely correct. Reverting https://github.com/bazelbuild/bazel/pull/14995 in a local Bazel source build makes `jaxlib` builds work again on my machine. The problem is (in my opinion) that OpenJDK recognizes the M1 line of processors as `aarch64`, which causes the whole toolchain mismatch cascade I observed, because all of the `aarch64` constraint flags were overwritten to be `arm64` in that PR.","By the way, the local repo approach finally worked when I checked out the same commit SHA as the HTTP repo snapshot above it in the WORKSPACE. How can I check whether a given TensorFlow commit will work for a Jaxlib build? I went through numerous errors with Bazel and LLVM dependencies yesterday evening on the way."," Normally, most TF versions work. I'm wondering if something went wrong with cached state on your machine. `bazel clean` maybe?","Could be. TensorFlow master does not build. I manually removed the whole bazel repository cache, but that didn't change anything. ``` Loading:  Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/51d8f5b0d9a5b2fc09cda867a4c8823c1221702d.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded Loading: 0 packages loaded WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/302fe7b3c40f7b949f3bebb74997bef9bf74d59f.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found ERROR: /Users/nicholasjunge/Workspaces/python/jax/WORKSPACE:28:14: fetching _tf_http_archive rule //external:llvmraw: java.io.IOException: unlinkat (/private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/llvmraw/llvm/test) (Directory not empty) ERROR: no such package 'raw//utils/bazel': unlinkat (/private/var/tmp/_bazel_nicholasjunge/270a4a78734ae0f3124fa7265b8a65ef/external/llvmraw/llvm/test) (Directory not empty) INFO: Elapsed time: 31.806s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded) ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully (0 packages loaded) ``` EDIT: Pulled again, this time it builds. No idea what's going on, but in case someone on Apple ARM64 wants to build jaxlib, TensorFlow @ ed9b4612299de07881ede418fc7fbf809e6911e8 apparently works.","Quick update : Thanks to the platforms update in TFRT, jaxlin with the new pinned TensorFlow archive builds fine even without the platforms repo pin. I also retested with my local fork, and TensorFlow @ 3f4c773f94a4936c5749764b7bfcc2a567deef0e (even newer than the currently pinned TF) builds without the platforms, too. So you can drop the explicit platforms import in JAX's workspace again, if you prefer."," Thanks, I'll revert that change."
557,"以下是一个github上的jax下的一个issue, 标题是(Fix race condition for weakref destructor by catching rare exceptions.)， 内容是 (Note that the tests don't directly reproduce the error because it is a rare bug and it requires injecting GC calls in the right spot, but they provide reasonable coverage of the weakref part of the code. It wasn't even easy to trigger with multiple threads. Fixes: https://github.com/google/jax/issues/10107)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Fix race condition for weakref destructor by catching rare exceptions.,"Note that the tests don't directly reproduce the error because it is a rare bug and it requires injecting GC calls in the right spot, but they provide reasonable coverage of the weakref part of the code. It wasn't even easy to trigger with multiple threads. Fixes: https://github.com/google/jax/issues/10107",2022-03-31T21:18:25Z,pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/10110,"I investigated how to implement `LockWithWorkQueue` in python, and found it is hard to make it perfect. ```python from collections import namedtuple import functools from queue import Empty, SimpleQueue import threading from typing import Any, Callable import weakref class LockWithWorkQueue:   def __init__(self):     self._block = threading.Lock()     self._queue = SimpleQueue()   def __enter__(self):     return self._block.acquire()   def __exit__(self):      we assume __enter__ and __exit__ must be paired      thus there is no race with __enter__ or other __exit__     while True:       try:         work = self._queue.get_nowait()       except Empty:         break       else:         work()      if control flow is interrupted here in the same thread by call_locked      there will be intermediate state that _block is unlocked while _queue is not empty      that should be not too bad     self._block.release()   def call_locked(self, work: Callable[[], Any]):     if self._block.acquire(blocking=False):        caller should ensure control flow won't be interrupted here in the same thread        or possible interrupter in the same thread won't acquire self._block        otherwise there will be deadlock        gc won't be triggered when invoking weakref callbacks        https://github.com/python/cpython/pull/2695       try:         work()       finally:         self._block.release()     else:       self._queue.put_nowait(work) CacheInfo = namedtuple(""CacheInfo"", [""hits"", ""misses"", ""maxsize"", ""currsize""]) def weakref_lru_cache(call: Callable, maxsize=2048):   """"""   Least recently used cache decorator with weakref support.   The cache will take a weakref to the first argument of the wrapped function   and strong refs to all subsequent operations. In all other respects it should   behave similar to `functools.lru_cache`.   """"""   cache = {}   hits = misses = 0   lock = LockWithWorkQueue()   def remove_key(args, kwargs, weak_arg):     k = (weak_arg, args, kwargs)     lock.call_locked(lambda: cache.pop(k, None))   def wrapped(weak_arg, *args, **kwargs):     nonlocal hits, misses     kwargs_key = tuple(kwargs.items())     k = (weakref.ref(weak_arg,         functools.partial(remove_key, args, kwargs_key)),         args, kwargs_key)     with lock:       if k in cache:         hits += 1         result = cache[k]          del and reinsert to bump key in the insertion order.         del cache[k]         cache[k] = result         return result       misses += 1     result = call(weak_arg, *args, **kwargs)     with lock:       cache[k] = result       while len(cache) > maxsize:         del cache[next(iter(cache))]     return result   def cache_info():     with lock:       return CacheInfo(hits, misses, maxsize, len(cache))   def cache_clear():     nonlocal hits, misses     with lock:       hits = misses = 0       cache.clear()   wrapped.cache_info = cache_info   wrapped.cache_clear = cache_clear   return wrapped ```","After further investigating, it seems that moving `next(iter(cache))` code to C (i.e. call `tp_iter` and `tp_iternext` in c) cannot prevent gc callback , since `PyObject_GC_New` can still trigger gc, and calling stack of `tp_iter` on `dict` will contain `dict_iter` > `dictiter_new` > `PyObject_GC_New`(macro), and it SHOULD do that since `iter(some_dict)` should hold a reference to `some_dict`. Moreover, since `__del__` and weakref callback can execute arbitrary code, while `Py_DECREF` that could dealloc the object is widely used in C extension, C code is not automatically immune to reentrant. `__eq__` used by dict access can execute arbitrary code as well.(this could be avoided by adding an identity hash shim, use identity match of wrapped object as its `__eq__`). (BTW, `dict.popitem()` will allocate a tuple, so it is not ""atomic"" as well.) Special care was taken in https://github.com/python/cpython/pull/11623 for `lru_cache`. But there is still something wrong in current implementation of `lru_cache`, if I understand correctly. https://github.com/python/cpython/blob/6cbb57f62d345d7a5d6aeb1b3b5d37a845344d5e/Modules/_functoolsmodule.cL975L976 said The `__eq__` method call is never made for the deletion (dict access CC(Undefined name: from ..core import JaxTuple)) because it is an identity match. But as https://github.com/python/cpython/blob/6cbb57f62d345d7a5d6aeb1b3b5d37a845344d5e/Objects/dictobject.cL2207L2224 and https://github.com/python/cpython/blob/6cbb57f62d345d7a5d6aeb1b3b5d37a845344d5e/Objects/dictobject.cL970L989 show, if there is hash collision, only the first inserted key can avoid `__eq__` call. In ""conclusion"", it is not easy to make operation ""atomic"". I think `nonreentrant Lock + background thread` or `nonreentrant Lock + deferred callback queue` or ignoring error are reasonable options.","Not mutating cache dict is satisfactory to me. WDYT? ```python import functools import weakref class _ResultWrapper:   def __init__(self, data):     self.data = data   def clean(self):     self.data = None def weakref_lru_cache(max_size=4096):   def wrap(f):     .lru_cache(max_size)     def cached(_, weak_arg_ref, *args, **kwargs):       return _ResultWrapper(f(weak_arg_ref(), *args, **kwargs))     .wraps(f)     def wrapper(weak_arg, *args, **kwargs):       assert type(weak_arg).__hash__ is object.__hash__       assert type(weak_arg).__eq__ is object.__eq__       tctx = None       weak_arg_ref = weakref.ref(weak_arg, lambda _: result_wrapper.clean())       result_wrapper = cached(tctx, weak_arg_ref, *args, **kwargs)        weakref callback won't be triggered before return       return result_wrapper.data     wrapper.cache_clear = cached.cache_clear     wrapper.cache_info = cached.cache_info     return wrapper   return wrap ```"
316,"以下是一个github上的jax下的一个issue, 标题是(Fixed a typo in the return type of _array_ir_types)， 内容是 (There could be other typing issues in that module, but I will address them separately.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fixed a typo in the return type of _array_ir_types,"There could be other typing issues in that module, but I will address them separately.",2022-03-30T15:32:12Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/10087
1445,"以下是一个github上的jax下的一个issue, 标题是(eval_shape(pjit(f)) doesn't match pjit(f).shape with GlobalDeviceArray in multi-host settings)， 内容是 ( Steps to reproduce: ```python import functools from absl import app import jax from jax.experimental import maps from jax.experimental.pjit import PartitionSpec from jax.experimental.pjit import pjit import numpy as np jax.config.update('jax_parallel_functions_output_gda', True) .partial(     pjit,     in_axis_resources=(),     out_axis_resources=PartitionSpec('x'), ) def f():   return jax.numpy.zeros([32, 10]) def main(argv):   del argv   Unused   with maps.Mesh(np.asarray(jax.devices()), axis_names=['x']):     print(f().shape)     print(jax.eval_shape(f)) if __name__ == '__main__':   app.run(main) ```  Expected output: ``` (32, 10) ShapeDtypeStruct(shape=(32, 10), dtype=float32) ```  Observed output: ``` (32, 10) ShapeDtypeStruct(shape=(16, 10), dtype=float32) ``` More generally, it would be nice if `eval_shape()` could recognise `GlobalDeviceArray` and produce a subtype of `ShapeDtypeStruct` conveying not just global shape but also the shard index and replica information derived from the corresponding `PartitionSpec`. Otherwise it is currently very hard to work out what shape a `GlobalDeviceArray` is going to have and what shape its shards will have.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,eval_shape(pjit(f)) doesn't match pjit(f).shape with GlobalDeviceArray in multi-host settings," Steps to reproduce: ```python import functools from absl import app import jax from jax.experimental import maps from jax.experimental.pjit import PartitionSpec from jax.experimental.pjit import pjit import numpy as np jax.config.update('jax_parallel_functions_output_gda', True) .partial(     pjit,     in_axis_resources=(),     out_axis_resources=PartitionSpec('x'), ) def f():   return jax.numpy.zeros([32, 10]) def main(argv):   del argv   Unused   with maps.Mesh(np.asarray(jax.devices()), axis_names=['x']):     print(f().shape)     print(jax.eval_shape(f)) if __name__ == '__main__':   app.run(main) ```  Expected output: ``` (32, 10) ShapeDtypeStruct(shape=(32, 10), dtype=float32) ```  Observed output: ``` (32, 10) ShapeDtypeStruct(shape=(16, 10), dtype=float32) ``` More generally, it would be nice if `eval_shape()` could recognise `GlobalDeviceArray` and produce a subtype of `ShapeDtypeStruct` conveying not just global shape but also the shard index and replica information derived from the corresponding `PartitionSpec`. Otherwise it is currently very hard to work out what shape a `GlobalDeviceArray` is going to have and what shape its shards will have.",2022-03-30T13:13:43Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/10084,"Thanks for the bug! I am looking into it. > it is currently very hard to work out what shape a GlobalDeviceArray is going to have and what shape its shards will have. For this to unblock you, you can use https://github.com/google/jax/blob/main/jax/experimental/global_device_array.pyL86L89. That is also a noflop calculation.","Thank you for the suggestion about `get_shard_indices_replica_ids()`. But that still requires duplicating the logic that works out which `PartitionSpec` applies to each leaf of the tree, and then applying it to give the mesh axes."
4268,"以下是一个github上的jax下的一个issue, 标题是(*** WARNING *** You are using ptxas 9.2.148)， 内容是 (Hi! I am trying to install jax, however I always get the following warning after running jax code. For example, after running  ```python  from jax import grad import jax.numpy as jnp def tanh(x):   Define a function   y = jnp.exp(2.0 * x)   return (1.0  y) / (1.0 + y) grad_tanh = grad(tanh)   Obtain its gradient function print(grad_tanh(1.0))    Evaluate it at x = 1.0  prints 0.4199743 ```` I get the following : ```W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:58.854617: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:58.911476: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:58.957363: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:59.055427: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:59.122488: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:59.168795: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:59.233470: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. ``` I have have made sure all drivers/CUDA version/cuDNN version are as expected. I am working on **Debian 10** and have **cudnn 8.3.3+cuda11.6**. And Tensorflow does appear to recognize the existence of three GPUs: ``` >> python c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')] ``` I also have the right version of jaxlib installed: ```` >> pip freeze | grep jaxlib jaxlib==0.3.2+cuda11.cudnn82 ```` Any ideas about how to solve this?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,*** WARNING *** You are using ptxas 9.2.148,"Hi! I am trying to install jax, however I always get the following warning after running jax code. For example, after running  ```python  from jax import grad import jax.numpy as jnp def tanh(x):   Define a function   y = jnp.exp(2.0 * x)   return (1.0  y) / (1.0 + y) grad_tanh = grad(tanh)   Obtain its gradient function print(grad_tanh(1.0))    Evaluate it at x = 1.0  prints 0.4199743 ```` I get the following : ```W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:58.854617: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:58.911476: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:58.957363: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:59.055427: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:59.122488: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:59.168795: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220330 15:06:59.233470: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 9.2.148, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. ``` I have have made sure all drivers/CUDA version/cuDNN version are as expected. I am working on **Debian 10** and have **cudnn 8.3.3+cuda11.6**. And Tensorflow does appear to recognize the existence of three GPUs: ``` >> python c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')] ``` I also have the right version of jaxlib installed: ```` >> pip freeze | grep jaxlib jaxlib==0.3.2+cuda11.cudnn82 ```` Any ideas about how to solve this?",2022-03-30T13:10:08Z,bug needs info,closed,0,3,https://github.com/jax-ml/jax/issues/10083,Which version of `ptxas` is in your path? Run `ptxas version` ?,"```` >> ptxas version ptxas: NVIDIA (R) Ptx optimizing assembler Copyright (c) 20052018 NVIDIA Corporation Built on Tue_Jun_12_23:06:09_CDT_2018 Cuda compilation tools, release 9.2, V9.2.148 ````","Well, there you are. You have an old `ptxas` in your PATH, which JAX is finding instead of the one you probably intended. `which ptxas` will tell you where that copy is. I'd suggest either removing it or changing your `PATH` to point to the more recent version of `ptxas` in your CUDA 11.6 installation."
12874,"以下是一个github上的jax下的一个issue, 标题是(ImportError: initialization failed)， 内容是 ( [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: Here's how to reproduce (Ubuntu 20.04, Intel Xeon 1650 CPU): ```bash  Create a fresh virtual env conda create n venv python=3.9 conda activate venv  Install `jax` from conda conda install jax  Since I wanted GPU support, next line only removes `jaxlib`, not other deps conda remove jaxlib force  Now I install GPUcompatible `jaxlib` pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html ``` Now, just enter Python REPL, and import `jax`: ```python >>> import jax ```  [x] If applicable, include full error messages/tracebacks. ```python Traceback (most recent call last):   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/scipy/linalg/__init__.py"", line 210, in      from ._matfuncs import *   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/scipy/linalg/_matfuncs.py"", line 21, in      from ._matfuncs_sqrtm import sqrtm   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/scipy/linalg/_matfuncs_sqrtm.py"", line 24, in      from ._matfuncs_sqrtm_triu import within_block_loop ImportError: /usr/lib/x86_64linuxgnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by /home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/scipy/linalg/_matfuncs_sqrtm_triu.cpython39x86_64linuxgnu.so) The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jax/__init__.py"", line 37, in      from jax import config as _config_module   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jax/config.py"", line 18, in      from jax._src.config import config   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jax/_src/config.py"", line 27, in      from jax._src import lib   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jax/_src/lib/__init__.py"", line 101, in      import jaxlib.lapack as lapack   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jaxlib/lapack.py"", line 21, in      from . import _lapack ImportError: initialization failed ``` The reason that in the script above I initially install `jax` from `conda` although it's not GPUcompatible is because I want the packages to mostly come from `condaforge` (for compatibility reasons) and only if that's not possible, get them from `pypi`. In this case, I only want to install GPUcompatible `jaxlib` from PyPI and the rest from `condaforge`. If it helps debugging the issue, I tried getting all dependencies from `pypi` (essentially installing `jax` from `pypi` right off the bat after creating a new virtual env), and it works fine. so there must be some package from `condaforge` that causes the `ImportError`. To further illustrate the issue, here's the output of `conda list` when everything comes from `pypi`: ```bash  Name                    Version                   Build  Channel _libgcc_mutex             0.1                 conda_forge    condaforge _openmp_mutex             4.5                       1_gnu    condaforge abslpy                   1.0.0                    pypi_0    pypi bzip2                     1.0.8                h7f98852_4    condaforge cacertificates           2021.10.8            ha878542_0    condaforge flatbuffers               2.0                      pypi_0    pypi jax                       0.3.4                    pypi_0    pypi jaxlib                    0.3.2+cuda11.cudnn82          pypi_0    pypi ld_impl_linux64          2.36.1               hea4e1c9_2    condaforge libffi                    3.4.2                h7f98852_5    condaforge libgccng                 11.2.0              h1d223b6_14    condaforge libgomp                   11.2.0              h1d223b6_14    condaforge libnsl                    2.0.0                h7f98852_0    condaforge libuuid                   2.32.1            h7f98852_1000    condaforge libzlib                   1.2.11            h166bdaf_1014    condaforge ncurses                   6.3                  h9c3ff4c_0    condaforge numpy                     1.22.3                   pypi_0    pypi openssl                   3.0.2                h166bdaf_1    condaforge opteinsum                3.3.0                    pypi_0    pypi pip                       22.0.4             pyhd8ed1ab_0    condaforge python                    3.9.12          h2660328_1_cpython    condaforge python_abi                3.9                      2_cp39    condaforge readline                  8.1                  h46c0cb4_0    condaforge scipy                     1.8.0                    pypi_0    pypi setuptools                61.2.0           py39hf3d152e_1    condaforge six                       1.16.0                   pypi_0    pypi sqlite                    3.37.1               h4ff8645_0    condaforge tk                        8.6.12               h27826a3_0    condaforge typingextensions         4.1.1                    pypi_0    pypi tzdata                    2022a                h191b570_0    condaforge wheel                     0.37.1             pyhd8ed1ab_0    condaforge xz                        5.2.5                h516909a_1    condaforge zlib                      1.2.11            h166bdaf_1014    condaforge ``` and here's its output when only `jaxlib` is coming from `pypi`: ```bash  Name                    Version                   Build  Channel _libgcc_mutex             0.1                 conda_forge    condaforge _openmp_mutex             4.5                       1_gnu    condaforge abseilcpp                20211102.0           h27087fc_1    condaforge abslpy                   1.0.0              pyhd8ed1ab_0    condaforge bzip2                     1.0.8                h7f98852_4    condaforge cares                    1.18.1               h7f98852_0    condaforge cacertificates           2021.10.8            ha878542_0    condaforge grpccpp                  1.44.0               h1021d7f_1    condaforge jax                       0.3.4              pyhd8ed1ab_0    condaforge jaxlib                    0.3.2+cuda11.cudnn82          pypi_0    pypi ld_impl_linux64          2.36.1               hea4e1c9_2    condaforge libblas                   3.9.0           13_linux64_openblas    condaforge libcblas                  3.9.0           13_linux64_openblas    condaforge libffi                    3.4.2                h7f98852_5    condaforge libgccng                 11.2.0              h1d223b6_14    condaforge libgfortranng            11.2.0              h69a702a_14    condaforge libgfortran5              11.2.0              h5c6108e_14    condaforge libgomp                   11.2.0              h1d223b6_14    condaforge liblapack                 3.9.0           13_linux64_openblas    condaforge libnsl                    2.0.0                h7f98852_0    condaforge libopenblas               0.3.18          pthreads_h8fe5266_0    condaforge libprotobuf               3.19.4               h780b84a_0    condaforge libstdcxxng              11.2.0              he4da1e4_14    condaforge libuuid                   2.32.1            h7f98852_1000    condaforge libzlib                   1.2.11            h166bdaf_1014    condaforge ncurses                   6.3                  h9c3ff4c_0    condaforge numpy                     1.22.3           py39h18676bf_0    condaforge openssl                   3.0.2                h166bdaf_1    condaforge opt_einsum                3.3.0              pyhd8ed1ab_1    condaforge pip                       22.0.4             pyhd8ed1ab_0    condaforge python                    3.9.12          h2660328_1_cpython    condaforge pythonflatbuffers        2.0                pyhd8ed1ab_0    condaforge python_abi                3.9                      2_cp39    condaforge re2                       2022.02.01           h9c3ff4c_0    condaforge readline                  8.1                  h46c0cb4_0    condaforge scipy                     1.8.0            py39hee8e79c_1    condaforge setuptools                61.2.0           py39hf3d152e_1    condaforge six                       1.16.0             pyh6c4a22f_0    condaforge sqlite                    3.37.1               h4ff8645_0    condaforge tk                        8.6.12               h27826a3_0    condaforge typing_extensions         4.1.1              pyha770c72_0    condaforge tzdata                    2022a                h191b570_0    condaforge wheel                     0.37.1             pyhd8ed1ab_0    condaforge xz                        5.2.5                h516909a_1    condaforge zlib                      1.2.11            h166bdaf_1014    condaforge ``` and here's the diff between the two: ```bash   Name                    Version                   Build  Channel  _libgcc_mutex             0.1                 conda_forge    condaforge  _openmp_mutex             4.5                       1_gnu    condaforge abseilcpp                20211102.0           h27087fc_1    condaforge abslpy                   1.0.0              pyhd8ed1ab_0    condaforge +abslpy                   1.0.0                    pypi_0    pypi  bzip2                     1.0.8                h7f98852_4    condaforge cares                    1.18.1               h7f98852_0    condaforge  cacertificates           2021.10.8            ha878542_0    condaforge grpccpp                  1.44.0               h1021d7f_1    condaforge jax                       0.3.4              pyhd8ed1ab_0    condaforge +flatbuffers               2.0                      pypi_0    pypi +jax                       0.3.4                    pypi_0    pypi  jaxlib                    0.3.2+cuda11.cudnn82          pypi_0    pypi  ld_impl_linux64          2.36.1               hea4e1c9_2    condaforge libblas                   3.9.0           13_linux64_openblas    condaforge libcblas                  3.9.0           13_linux64_openblas    condaforge  libffi                    3.4.2                h7f98852_5    condaforge  libgccng                 11.2.0              h1d223b6_14    condaforge libgfortranng            11.2.0              h69a702a_14    condaforge libgfortran5              11.2.0              h5c6108e_14    condaforge  libgomp                   11.2.0              h1d223b6_14    condaforge liblapack                 3.9.0           13_linux64_openblas    condaforge  libnsl                    2.0.0                h7f98852_0    condaforge libopenblas               0.3.18          pthreads_h8fe5266_0    condaforge libprotobuf               3.19.4               h780b84a_0    condaforge libstdcxxng              11.2.0              he4da1e4_14    condaforge  libuuid                   2.32.1            h7f98852_1000    condaforge  libzlib                   1.2.11            h166bdaf_1014    condaforge  ncurses                   6.3                  h9c3ff4c_0    condaforge numpy                     1.22.3           py39h18676bf_0    condaforge +numpy                     1.22.3                   pypi_0    pypi  openssl                   3.0.2                h166bdaf_1    condaforge opt_einsum                3.3.0              pyhd8ed1ab_1    condaforge +opteinsum                3.3.0                    pypi_0    pypi  pip                       22.0.4             pyhd8ed1ab_0    condaforge  python                    3.9.12          h2660328_1_cpython    condaforge pythonflatbuffers        2.0                pyhd8ed1ab_0    condaforge  python_abi                3.9                      2_cp39    condaforge re2                       2022.02.01           h9c3ff4c_0    condaforge  readline                  8.1                  h46c0cb4_0    condaforge scipy                     1.8.0            py39hee8e79c_1    condaforge +scipy                     1.8.0                    pypi_0    pypi  setuptools                61.2.0           py39hf3d152e_1    condaforge six                       1.16.0             pyh6c4a22f_0    condaforge +six                       1.16.0                   pypi_0    pypi  sqlite                    3.37.1               h4ff8645_0    condaforge  tk                        8.6.12               h27826a3_0    condaforge typing_extensions         4.1.1              pyha770c72_0    condaforge +typingextensions         4.1.1                    pypi_0    pypi  tzdata                    2022a                h191b570_0    condaforge  wheel                     0.37.1             pyhd8ed1ab_0    condaforge  xz                        5.2.5                h516909a_1    condaforge ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,ImportError: initialization failed," [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: Here's how to reproduce (Ubuntu 20.04, Intel Xeon 1650 CPU): ```bash  Create a fresh virtual env conda create n venv python=3.9 conda activate venv  Install `jax` from conda conda install jax  Since I wanted GPU support, next line only removes `jaxlib`, not other deps conda remove jaxlib force  Now I install GPUcompatible `jaxlib` pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html ``` Now, just enter Python REPL, and import `jax`: ```python >>> import jax ```  [x] If applicable, include full error messages/tracebacks. ```python Traceback (most recent call last):   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/scipy/linalg/__init__.py"", line 210, in      from ._matfuncs import *   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/scipy/linalg/_matfuncs.py"", line 21, in      from ._matfuncs_sqrtm import sqrtm   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/scipy/linalg/_matfuncs_sqrtm.py"", line 24, in      from ._matfuncs_sqrtm_triu import within_block_loop ImportError: /usr/lib/x86_64linuxgnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by /home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/scipy/linalg/_matfuncs_sqrtm_triu.cpython39x86_64linuxgnu.so) The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jax/__init__.py"", line 37, in      from jax import config as _config_module   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jax/config.py"", line 18, in      from jax._src.config import config   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jax/_src/config.py"", line 27, in      from jax._src import lib   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jax/_src/lib/__init__.py"", line 101, in      import jaxlib.lapack as lapack   File ""/home/anaconda/mambaforge/envs/gpu/lib/python3.9/sitepackages/jaxlib/lapack.py"", line 21, in      from . import _lapack ImportError: initialization failed ``` The reason that in the script above I initially install `jax` from `conda` although it's not GPUcompatible is because I want the packages to mostly come from `condaforge` (for compatibility reasons) and only if that's not possible, get them from `pypi`. In this case, I only want to install GPUcompatible `jaxlib` from PyPI and the rest from `condaforge`. If it helps debugging the issue, I tried getting all dependencies from `pypi` (essentially installing `jax` from `pypi` right off the bat after creating a new virtual env), and it works fine. so there must be some package from `condaforge` that causes the `ImportError`. To further illustrate the issue, here's the output of `conda list` when everything comes from `pypi`: ```bash  Name                    Version                   Build  Channel _libgcc_mutex             0.1                 conda_forge    condaforge _openmp_mutex             4.5                       1_gnu    condaforge abslpy                   1.0.0                    pypi_0    pypi bzip2                     1.0.8                h7f98852_4    condaforge cacertificates           2021.10.8            ha878542_0    condaforge flatbuffers               2.0                      pypi_0    pypi jax                       0.3.4                    pypi_0    pypi jaxlib                    0.3.2+cuda11.cudnn82          pypi_0    pypi ld_impl_linux64          2.36.1               hea4e1c9_2    condaforge libffi                    3.4.2                h7f98852_5    condaforge libgccng                 11.2.0              h1d223b6_14    condaforge libgomp                   11.2.0              h1d223b6_14    condaforge libnsl                    2.0.0                h7f98852_0    condaforge libuuid                   2.32.1            h7f98852_1000    condaforge libzlib                   1.2.11            h166bdaf_1014    condaforge ncurses                   6.3                  h9c3ff4c_0    condaforge numpy                     1.22.3                   pypi_0    pypi openssl                   3.0.2                h166bdaf_1    condaforge opteinsum                3.3.0                    pypi_0    pypi pip                       22.0.4             pyhd8ed1ab_0    condaforge python                    3.9.12          h2660328_1_cpython    condaforge python_abi                3.9                      2_cp39    condaforge readline                  8.1                  h46c0cb4_0    condaforge scipy                     1.8.0                    pypi_0    pypi setuptools                61.2.0           py39hf3d152e_1    condaforge six                       1.16.0                   pypi_0    pypi sqlite                    3.37.1               h4ff8645_0    condaforge tk                        8.6.12               h27826a3_0    condaforge typingextensions         4.1.1                    pypi_0    pypi tzdata                    2022a                h191b570_0    condaforge wheel                     0.37.1             pyhd8ed1ab_0    condaforge xz                        5.2.5                h516909a_1    condaforge zlib                      1.2.11            h166bdaf_1014    condaforge ``` and here's its output when only `jaxlib` is coming from `pypi`: ```bash  Name                    Version                   Build  Channel _libgcc_mutex             0.1                 conda_forge    condaforge _openmp_mutex             4.5                       1_gnu    condaforge abseilcpp                20211102.0           h27087fc_1    condaforge abslpy                   1.0.0              pyhd8ed1ab_0    condaforge bzip2                     1.0.8                h7f98852_4    condaforge cares                    1.18.1               h7f98852_0    condaforge cacertificates           2021.10.8            ha878542_0    condaforge grpccpp                  1.44.0               h1021d7f_1    condaforge jax                       0.3.4              pyhd8ed1ab_0    condaforge jaxlib                    0.3.2+cuda11.cudnn82          pypi_0    pypi ld_impl_linux64          2.36.1               hea4e1c9_2    condaforge libblas                   3.9.0           13_linux64_openblas    condaforge libcblas                  3.9.0           13_linux64_openblas    condaforge libffi                    3.4.2                h7f98852_5    condaforge libgccng                 11.2.0              h1d223b6_14    condaforge libgfortranng            11.2.0              h69a702a_14    condaforge libgfortran5              11.2.0              h5c6108e_14    condaforge libgomp                   11.2.0              h1d223b6_14    condaforge liblapack                 3.9.0           13_linux64_openblas    condaforge libnsl                    2.0.0                h7f98852_0    condaforge libopenblas               0.3.18          pthreads_h8fe5266_0    condaforge libprotobuf               3.19.4               h780b84a_0    condaforge libstdcxxng              11.2.0              he4da1e4_14    condaforge libuuid                   2.32.1            h7f98852_1000    condaforge libzlib                   1.2.11            h166bdaf_1014    condaforge ncurses                   6.3                  h9c3ff4c_0    condaforge numpy                     1.22.3           py39h18676bf_0    condaforge openssl                   3.0.2                h166bdaf_1    condaforge opt_einsum                3.3.0              pyhd8ed1ab_1    condaforge pip                       22.0.4             pyhd8ed1ab_0    condaforge python                    3.9.12          h2660328_1_cpython    condaforge pythonflatbuffers        2.0                pyhd8ed1ab_0    condaforge python_abi                3.9                      2_cp39    condaforge re2                       2022.02.01           h9c3ff4c_0    condaforge readline                  8.1                  h46c0cb4_0    condaforge scipy                     1.8.0            py39hee8e79c_1    condaforge setuptools                61.2.0           py39hf3d152e_1    condaforge six                       1.16.0             pyh6c4a22f_0    condaforge sqlite                    3.37.1               h4ff8645_0    condaforge tk                        8.6.12               h27826a3_0    condaforge typing_extensions         4.1.1              pyha770c72_0    condaforge tzdata                    2022a                h191b570_0    condaforge wheel                     0.37.1             pyhd8ed1ab_0    condaforge xz                        5.2.5                h516909a_1    condaforge zlib                      1.2.11            h166bdaf_1014    condaforge ``` and here's the diff between the two: ```bash   Name                    Version                   Build  Channel  _libgcc_mutex             0.1                 conda_forge    condaforge  _openmp_mutex             4.5                       1_gnu    condaforge abseilcpp                20211102.0           h27087fc_1    condaforge abslpy                   1.0.0              pyhd8ed1ab_0    condaforge +abslpy                   1.0.0                    pypi_0    pypi  bzip2                     1.0.8                h7f98852_4    condaforge cares                    1.18.1               h7f98852_0    condaforge  cacertificates           2021.10.8            ha878542_0    condaforge grpccpp                  1.44.0               h1021d7f_1    condaforge jax                       0.3.4              pyhd8ed1ab_0    condaforge +flatbuffers               2.0                      pypi_0    pypi +jax                       0.3.4                    pypi_0    pypi  jaxlib                    0.3.2+cuda11.cudnn82          pypi_0    pypi  ld_impl_linux64          2.36.1               hea4e1c9_2    condaforge libblas                   3.9.0           13_linux64_openblas    condaforge libcblas                  3.9.0           13_linux64_openblas    condaforge  libffi                    3.4.2                h7f98852_5    condaforge  libgccng                 11.2.0              h1d223b6_14    condaforge libgfortranng            11.2.0              h69a702a_14    condaforge libgfortran5              11.2.0              h5c6108e_14    condaforge  libgomp                   11.2.0              h1d223b6_14    condaforge liblapack                 3.9.0           13_linux64_openblas    condaforge  libnsl                    2.0.0                h7f98852_0    condaforge libopenblas               0.3.18          pthreads_h8fe5266_0    condaforge libprotobuf               3.19.4               h780b84a_0    condaforge libstdcxxng              11.2.0              he4da1e4_14    condaforge  libuuid                   2.32.1            h7f98852_1000    condaforge  libzlib                   1.2.11            h166bdaf_1014    condaforge  ncurses                   6.3                  h9c3ff4c_0    condaforge numpy                     1.22.3           py39h18676bf_0    condaforge +numpy                     1.22.3                   pypi_0    pypi  openssl                   3.0.2                h166bdaf_1    condaforge opt_einsum                3.3.0              pyhd8ed1ab_1    condaforge +opteinsum                3.3.0                    pypi_0    pypi  pip                       22.0.4             pyhd8ed1ab_0    condaforge  python                    3.9.12          h2660328_1_cpython    condaforge pythonflatbuffers        2.0                pyhd8ed1ab_0    condaforge  python_abi                3.9                      2_cp39    condaforge re2                       2022.02.01           h9c3ff4c_0    condaforge  readline                  8.1                  h46c0cb4_0    condaforge scipy                     1.8.0            py39hee8e79c_1    condaforge +scipy                     1.8.0                    pypi_0    pypi  setuptools                61.2.0           py39hf3d152e_1    condaforge six                       1.16.0             pyh6c4a22f_0    condaforge +six                       1.16.0                   pypi_0    pypi  sqlite                    3.37.1               h4ff8645_0    condaforge  tk                        8.6.12               h27826a3_0    condaforge typing_extensions         4.1.1              pyha770c72_0    condaforge +typingextensions         4.1.1                    pypi_0    pypi  tzdata                    2022a                h191b570_0    condaforge  wheel                     0.37.1             pyhd8ed1ab_0    condaforge  xz                        5.2.5                h516909a_1    condaforge ```",2022-03-29T19:19:41Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/10071,"From the traceback, it appears the error happens on `import scipy.linalg`. Can you check whether the error occurs on `import scipy.linalg` if you do not install JAX? If so, it may be better to report the error elsewhere.",I think this might be the issue described here: https://github.com/google/jax/issues/9218issuecomment1016949739 Did you activate your conda environment?,"Thank you for your quick reply. > From the traceback, it appears the error happens on `import scipy.linalg`. Can you check whether the error occurs on `import scipy.linalg` if you do not install JAX? If so, it may be better to report the error elsewhere. `import scipy.linalg` works fine even after I install `jax`. > I think this might be the issue described here:  CC(problems installing JAX on a GCP deep learning VM with GPU) (comment) >  > Did you activate your conda environment? Yes, I did activate the environment. However, the link you referenced contained a crucial piece of information! So, it turns out, if I import `scipy.linalg` before importing `jax`, then the problem is gone! So, the workaround is to run `import scipy.linalg` before `import jax`.  Do you think this can be added to `jax`'s `__init__`? **Update 1**: Interestingly, when I import `jax` first, and it fails, then I can't even import `scipy.linalg`, but the other way around is OK.","sadeghi No, this is a bug in your environment (well, conda's environment, I guess), not in JAX. You have two C++ libraries and you need to arrange that only Conda's version is used."," Oh, I see. Based on the output of `conda list` I posted, do you know which ones might be conflicting? I'm not very familiar with C libraries. On another note, do you know if there's any ongoing effort to publish GPUcompatible `jax` on `condaforge` so it can all be installed in one go? Thank yo so much!","The issue is described in https://github.com/google/jax/issues/9218issuecomment1016949739 fairly completely. You most likely have libstdc++ in /lib (or similar) as well as in your conda installation. When using conda, you *must* arrange that conda's libstdc++ is the one found first, or you will get bugs like the one you have. I don't personally use conda, but my belief is that the way to do this is by activating a conda virtual environment. If that isn't happening, I would ask the conda folks. (You can probably do this manually by setting `LD_LIBRARY_PATH` to point to Conda's library path, but I would think that Conda should be doing this for you.) (We don't distribute Conda binaries ourselves, the condaforge binaries are community supported.) (There's no action for us to take here, so I'm closing the bug.)"
1317,"以下是一个github上的jax下的一个issue, 标题是(Jax doesn't scale correctly on a TPU Pod.)， 内容是 (Hello, Is there any kind of changes that need to be done on the Flax/Jax code to make it run on a TPU pod rather than a single TPU node? for example the T5 example on HuggingFace: https://github.com/huggingface/transformers/blob/main/examples/flax/languagemodeling/run_t5_mlm_flax.py Runs normally on a single TPU Pod, but if we need to run it on a TPU Pod, do we need to make any kind of changes to the code? or we just need to run the same script across all workers and it will communicate with each other: gcloud alpha compute tpus tpuvm ssh tpuvm zone=uscentral2b worker=all command ""python myscript.py"" The reason for this question is that several users who have access to TPU V4 have a big issue. The training script for hugging face runs at the same speed and global batch size in a single node or a pod. For example. running flax t5 in a TPU V464 pod reduces the per device batch size to 4 and makes the global batch size to 128, while In a single node it increases to 32 per device and makes the global batch size to 128. The training speed is exactly the same in a single node or a pod.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Jax doesn't scale correctly on a TPU Pod.,"Hello, Is there any kind of changes that need to be done on the Flax/Jax code to make it run on a TPU pod rather than a single TPU node? for example the T5 example on HuggingFace: https://github.com/huggingface/transformers/blob/main/examples/flax/languagemodeling/run_t5_mlm_flax.py Runs normally on a single TPU Pod, but if we need to run it on a TPU Pod, do we need to make any kind of changes to the code? or we just need to run the same script across all workers and it will communicate with each other: gcloud alpha compute tpus tpuvm ssh tpuvm zone=uscentral2b worker=all command ""python myscript.py"" The reason for this question is that several users who have access to TPU V4 have a big issue. The training script for hugging face runs at the same speed and global batch size in a single node or a pod. For example. running flax t5 in a TPU V464 pod reduces the per device batch size to 4 and makes the global batch size to 128, while In a single node it increases to 32 per device and makes the global batch size to 128. The training speed is exactly the same in a single node or a pod.",2022-03-29T07:35:54Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/10057,We have figured out the problem in Flax discussion 👍  https://github.com/google/flax/discussions/2017
4348,"以下是一个github上的jax下的一个issue, 标题是(cuDNN error when using scan and 3D convolution)， 内容是 (I am trying to recurrently apply a neural network to a tensor. I am using Flax and the `flax.linen.scan` transform which is a modified `jax.lax.scan`. I get an error related to cuDNN.  Environment specs  python 3.9.7  jax 0.2.24  jaxlib 0.1.73+cuda11.cudnn805  flax 0.4.0  CUDA 11.1 and cuDNN 8.1.1  Nvidia V100 32GB GPU  Code to reproduce ```python import jax import jax.numpy as jnp               from flax import linen as nn            from functools import partial class CNN(nn.Module):     .compact     def __call__(self, x, *args):         conv = partial(nn.Conv, features=3, kernel_size=(3, 3, 3))         x = conv()(x)         x = nn.relu(x)  Commenting this line makes the error go away         return x, x class CNNScan(nn.Module):     .compact     def __call__(self, x):         rollout = nn.scan(             CNN,             variable_broadcast='params',             split_rngs={'params': False},             length=8,             out_axes=1         )         _, out = rollout()(x, None)         return out rng = jax.random.PRNGKey(0) cnn = CNNScan() init_cnn = cnn.init(rng, jnp.ones([1, 64, 64, 64, 3])) ```  Error message ``` File /home/nagabhushana/anaconda3/envs/ddp_v2/lib/python3.9/sitepackages/flax/core/axes_scan.py:144, in scan..scan_fn(broadcast_in, init, *args)     141   out_flat.append(const)     142 broadcast_in, constants_out = jax.tree_unflatten(out_tree(), out_flat) > 144 c, ys = lax.scan(body_fn, init, xs, length=length, reverse=reverse)     145 ys = jax.tree_multimap(transpose_from_front, out_axes, ys)     146 ys = jax.tree_multimap(lambda ax, const, y: (const if ax is broadcast else y),     147                        out_axes, constants_out, ys) ... skipping hidden 13 frame File /home/nagabhushana/anaconda3/envs/ddp_v2/lib/python3.9/sitepackages/jax/interpreters/xla.py:369, in backend_compile(backend, built_c, options)     366 def backend_compile(backend, built_c, options):     367    we use a separate function call to ensure that XLA compilation appears     368    separately in Python profiling results > 369   return backend.compile(built_c, compile_options=options) RuntimeError: UNKNOWN: Failed to determine best cudnn convolution algorithm: INTERNAL: All algorithms tried for %cudnnconvbiasactivation = (f32[1,64,64,64,3]{3,2,1,4,0}, u8[0]{0}) customcall(f32[1,64,64,64,3]{3,2,1,4,0} %gettupleelement.8, f32[3,3,3,3,3]{2,1,0,3,4} %gettupleelement.5, f32[3]{0} %gettupleelement.6), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=b012f_012io>b012f, custom_call_target=""__cudnn$convBiasActivationForward"", metadata={op_type=""conv_general_dilated"" op_name=""jit(scan)/body/conv_general_dilated[\n  batch_group_count=1\n  dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3))\n  feature_group_count=1\n  lhs_dilation=(1, 1, 1)\n  lhs_shape=(1, 64, 64, 64, 3)\n  padding=((1, 1), (1, 1), (1, 1))\n  precision=None\n  preferred_element_type=None\n  rhs_dilation=(1, 1, 1)\n  rhs_shape=(3, 3, 3, 3, 3)\n  window_strides=(1, 1, 1)\n]"" source_file=""/home/nagabhushana/anaconda3/envs/ddp_v2/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=305}, backend_config=""{\""algorithm\"":\""0\"",\""tensor_ops_enabled\"":false,\""conv_result_scale\"":1,\""activation_mode\"":\""2\"",\""side_input_scale\"":0}"" failed. Falling back to default algorithm.  Convolution performance may be suboptimal.  To ignore this failure and try to use a fallback algorithm, use XLA_FLAGS=xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning. ```  Observations 1. The error only occurs when recurrently applying the CNN using `scan`, i.e, the CNN on it's own runs fine. 2. The code runs successfully after commenting out the `nn.relu` activation function. 3. The code runs successfully if I use 2D convolutions instead of 3D convolutions, by changing `kernel_size` to `(3, 3)` and the input tensor shape to `[1, 64, 64, 3]`. 4. The code runs successfully if I disable JIT compilation, using `config.update('jax_disable_jit', True)`. I'd be grateful if someone can help me solve this issue. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,cuDNN error when using scan and 3D convolution,"I am trying to recurrently apply a neural network to a tensor. I am using Flax and the `flax.linen.scan` transform which is a modified `jax.lax.scan`. I get an error related to cuDNN.  Environment specs  python 3.9.7  jax 0.2.24  jaxlib 0.1.73+cuda11.cudnn805  flax 0.4.0  CUDA 11.1 and cuDNN 8.1.1  Nvidia V100 32GB GPU  Code to reproduce ```python import jax import jax.numpy as jnp               from flax import linen as nn            from functools import partial class CNN(nn.Module):     .compact     def __call__(self, x, *args):         conv = partial(nn.Conv, features=3, kernel_size=(3, 3, 3))         x = conv()(x)         x = nn.relu(x)  Commenting this line makes the error go away         return x, x class CNNScan(nn.Module):     .compact     def __call__(self, x):         rollout = nn.scan(             CNN,             variable_broadcast='params',             split_rngs={'params': False},             length=8,             out_axes=1         )         _, out = rollout()(x, None)         return out rng = jax.random.PRNGKey(0) cnn = CNNScan() init_cnn = cnn.init(rng, jnp.ones([1, 64, 64, 64, 3])) ```  Error message ``` File /home/nagabhushana/anaconda3/envs/ddp_v2/lib/python3.9/sitepackages/flax/core/axes_scan.py:144, in scan..scan_fn(broadcast_in, init, *args)     141   out_flat.append(const)     142 broadcast_in, constants_out = jax.tree_unflatten(out_tree(), out_flat) > 144 c, ys = lax.scan(body_fn, init, xs, length=length, reverse=reverse)     145 ys = jax.tree_multimap(transpose_from_front, out_axes, ys)     146 ys = jax.tree_multimap(lambda ax, const, y: (const if ax is broadcast else y),     147                        out_axes, constants_out, ys) ... skipping hidden 13 frame File /home/nagabhushana/anaconda3/envs/ddp_v2/lib/python3.9/sitepackages/jax/interpreters/xla.py:369, in backend_compile(backend, built_c, options)     366 def backend_compile(backend, built_c, options):     367    we use a separate function call to ensure that XLA compilation appears     368    separately in Python profiling results > 369   return backend.compile(built_c, compile_options=options) RuntimeError: UNKNOWN: Failed to determine best cudnn convolution algorithm: INTERNAL: All algorithms tried for %cudnnconvbiasactivation = (f32[1,64,64,64,3]{3,2,1,4,0}, u8[0]{0}) customcall(f32[1,64,64,64,3]{3,2,1,4,0} %gettupleelement.8, f32[3,3,3,3,3]{2,1,0,3,4} %gettupleelement.5, f32[3]{0} %gettupleelement.6), window={size=3x3x3 pad=1_1x1_1x1_1}, dim_labels=b012f_012io>b012f, custom_call_target=""__cudnn$convBiasActivationForward"", metadata={op_type=""conv_general_dilated"" op_name=""jit(scan)/body/conv_general_dilated[\n  batch_group_count=1\n  dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 4, 1, 2, 3), rhs_spec=(4, 3, 0, 1, 2), out_spec=(0, 4, 1, 2, 3))\n  feature_group_count=1\n  lhs_dilation=(1, 1, 1)\n  lhs_shape=(1, 64, 64, 64, 3)\n  padding=((1, 1), (1, 1), (1, 1))\n  precision=None\n  preferred_element_type=None\n  rhs_dilation=(1, 1, 1)\n  rhs_shape=(3, 3, 3, 3, 3)\n  window_strides=(1, 1, 1)\n]"" source_file=""/home/nagabhushana/anaconda3/envs/ddp_v2/lib/python3.9/sitepackages/flax/linen/linear.py"" source_line=305}, backend_config=""{\""algorithm\"":\""0\"",\""tensor_ops_enabled\"":false,\""conv_result_scale\"":1,\""activation_mode\"":\""2\"",\""side_input_scale\"":0}"" failed. Falling back to default algorithm.  Convolution performance may be suboptimal.  To ignore this failure and try to use a fallback algorithm, use XLA_FLAGS=xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning. ```  Observations 1. The error only occurs when recurrently applying the CNN using `scan`, i.e, the CNN on it's own runs fine. 2. The code runs successfully after commenting out the `nn.relu` activation function. 3. The code runs successfully if I use 2D convolutions instead of 3D convolutions, by changing `kernel_size` to `(3, 3)` and the input tensor shape to `[1, 64, 64, 3]`. 4. The code runs successfully if I disable JIT compilation, using `config.update('jax_disable_jit', True)`. I'd be grateful if someone can help me solve this issue. ",2022-03-29T06:45:29Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/10056,"I think this is already fixed in the current JAX release. I was able to reproduce your problem with `jax==0.2.24` and `jaxlib==0.1.73+cuda11.cudnn805`, but not with the current release (`jax==0.3.4`, `jaxlib==0.3.2+cuda11.cudnn805`). Please use an up to date JAX release?"
980,"以下是一个github上的jax下的一个issue, 标题是(jax svd is exceptionally slow on TPUs)， 内容是 (The runtime is much slower than on cpu and also slower than `tf`. ```python from time import time from functools import partial import jax import jax.numpy as jnp run_svd = jax.jit(partial(jnp.linalg.svd, full_matrices=False)) mat = jax.random.normal(jax.random.PRNGKey(2), (128, 64)) jax.block_until_ready(run_svd(mat)) start = time() jax.block_until_ready(run_svd(mat)) print(time()  start) ``` For me, this prints out around ~0.35 seconds on a tpu v3. This tf code takes around 0.0015 which seems much more reasonable: ```python import tensorflow as tf tf_mat = tf.random.normal((128, 64)) tf.linalg.svd(tf_mat, full_matrices=False) start = time() s, u, v = tf.linalg.svd(tf_mat, full_matrices=False) print(time()  start) ``` The `hermitian=True` case is fine.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,jax svd is exceptionally slow on TPUs,"The runtime is much slower than on cpu and also slower than `tf`. ```python from time import time from functools import partial import jax import jax.numpy as jnp run_svd = jax.jit(partial(jnp.linalg.svd, full_matrices=False)) mat = jax.random.normal(jax.random.PRNGKey(2), (128, 64)) jax.block_until_ready(run_svd(mat)) start = time() jax.block_until_ready(run_svd(mat)) print(time()  start) ``` For me, this prints out around ~0.35 seconds on a tpu v3. This tf code takes around 0.0015 which seems much more reasonable: ```python import tensorflow as tf tf_mat = tf.random.normal((128, 64)) tf.linalg.svd(tf_mat, full_matrices=False) start = time() s, u, v = tf.linalg.svd(tf_mat, full_matrices=False) print(time()  start) ``` The `hermitian=True` case is fine.",2022-03-28T22:20:01Z,enhancement P2 (eventual),closed,0,3,https://github.com/jax-ml/jax/issues/10052,"Yes, this is a known issue. The TPU SVD implementation is quite naive at the moment: it's a nonparallel Jacobi solver that performs terribly on accelerators. Symmetric eigendecomposition (the `hermitian` case) is much more optimized, especially for small matrices. There's an open PR (https://github.com/google/jax/pull/9681) that replaces it with an approach based on the polar decomposition that hopefully we should be able to merge soon.",The PR on qdwhsvd solver (https://github.com/google/jax/pull/10473) is ready to merge. Hope this will address your concern on TPU performance. ,"I forgot to update this issue. This issue should be fixed in JAX 0.3.14! The SVD implementation on TPU is now a much faster implementation based on the QDWH algorithm for computing the polar decomposition. For your specific example here, I measure it at about 600us on TPUv3 at head. (I suspect the TF code in your first example is computing the SVD on CPU, by the way.)"
1168,"以下是一个github上的jax下的一个issue, 标题是(jax2tf fails with static_arguments)， 内容是 (This fails with indecipherable error messages: ``` import jax  from jax.experimental import jax2tf  import numpy as jnp   import numpy as np  import tensorflow as tf    def f(x):   return x + 2   f = jax.jit(f, static_argnums=0) f_tf = tf.function(jax2tf.convert(f))  f_tf.get_concrete_function(tf.TensorSpec(shape=(), dtype=np.int32))  ValueError: Nonhashable static arguments are not supported. An error occured during a call to 'f_jax' while trying to hash an object of type , Tracedwith with       val =        _aval = ShapedArray(int32[]). The error was:     TypeError: unhashable type: 'TensorFlowTracer' ``` It would be good to support static argument conversion. But whilst this is support is not there, this limitation of `jax2tf` should be documented (there is nothing in the docs) and giving a reasonable error message (its easy to check on the compiled object whether there are static argnums/names via `f.__getstate__()['static_argnames']`).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax2tf fails with static_arguments,"This fails with indecipherable error messages: ``` import jax  from jax.experimental import jax2tf  import numpy as jnp   import numpy as np  import tensorflow as tf    def f(x):   return x + 2   f = jax.jit(f, static_argnums=0) f_tf = tf.function(jax2tf.convert(f))  f_tf.get_concrete_function(tf.TensorSpec(shape=(), dtype=np.int32))  ValueError: Nonhashable static arguments are not supported. An error occured during a call to 'f_jax' while trying to hash an object of type , Tracedwith with       val =        _aval = ShapedArray(int32[]). The error was:     TypeError: unhashable type: 'TensorFlowTracer' ``` It would be good to support static argument conversion. But whilst this is support is not there, this limitation of `jax2tf` should be documented (there is nothing in the docs) and giving a reasonable error message (its easy to check on the compiled object whether there are static argnums/names via `f.__getstate__()['static_argnames']`).",2022-03-28T11:28:10Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/10044
731,"以下是一个github上的jax下的一个issue, 标题是(Batch tracers strip weak type)， 内容是 (```python from jax import vmap import jax.numpy as jnp x = jnp.arange(1, dtype='uint8')   stronglytyped uint8 y = jnp.ravel(1)   weaklytyped int32 print(jnp.add(x, y).dtype)  uint8 print(vmap(jnp.add)(x, y).dtype)  int32 ``` This is causing the test failure in CC(jax.numpy: add ufuncstyle APIs for several jnp functions) Edit: showing this more directly: ```python def print_avals(y):   print(f""{y.aval=}"") print_avals(y)  y.aval=ShapedArray(int32[1], weak_type=True) vmap(print_avals)(y)  y.aval=ShapedArray(int32[]) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Batch tracers strip weak type,"```python from jax import vmap import jax.numpy as jnp x = jnp.arange(1, dtype='uint8')   stronglytyped uint8 y = jnp.ravel(1)   weaklytyped int32 print(jnp.add(x, y).dtype)  uint8 print(vmap(jnp.add)(x, y).dtype)  int32 ``` This is causing the test failure in CC(jax.numpy: add ufuncstyle APIs for several jnp functions) Edit: showing this more directly: ```python def print_avals(y):   print(f""{y.aval=}"") print_avals(y)  y.aval=ShapedArray(int32[1], weak_type=True) vmap(print_avals)(y)  y.aval=ShapedArray(int32[]) ```",2022-03-24T17:37:08Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/10025
1314,"以下是一个github上的jax下的一个issue, 标题是(Cannot Share GPUs between processes when using JAX with SLURM)， 内容是 (It's possible that this is a limitation of SLURM rather than JAX, but I think it might be worth starting a discussion about this here to get to the bottom of it.  I'm trying to launch scripts using JAX with SLURM. Particularly, I want to launch multiple processes that use JAX and have them share a single allocated GPU. The issue I'm running into is that one process will be able to detect the GPU while the rest will not. The remaining processes will default to using CPU after printing the following warning:  ``` 20220323 20:23:46.386347: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:205] unable to create  StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to  cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal ```  It's worth noting that running `nvidiasmi` from within each process succeeds and shows statistics for the GPU I am attempting to share.  Does anyone have an idea why this `CUDA_ERROR_INVALID_DEVICE` error might be arising in my context?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot Share GPUs between processes when using JAX with SLURM,"It's possible that this is a limitation of SLURM rather than JAX, but I think it might be worth starting a discussion about this here to get to the bottom of it.  I'm trying to launch scripts using JAX with SLURM. Particularly, I want to launch multiple processes that use JAX and have them share a single allocated GPU. The issue I'm running into is that one process will be able to detect the GPU while the rest will not. The remaining processes will default to using CPU after printing the following warning:  ``` 20220323 20:23:46.386347: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:205] unable to create  StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to  cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal ```  It's worth noting that running `nvidiasmi` from within each process succeeds and shows statistics for the GPU I am attempting to share.  Does anyone have an idea why this `CUDA_ERROR_INVALID_DEVICE` error might be arising in my context?",2022-03-24T00:29:57Z,bug P3 (no schedule) needs info NVIDIA GPU,open,0,12,https://github.com/jax-ml/jax/issues/10021,"Can you try setting the env var `XLA_PYTHON_CLIENT_PREALLOCATE=false`? See https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html for more information and similar options. I think the issue might be that the first jax process is preallocating most of the GPU memory, causing other processes to OOM (sometimes OOMs show up in confusing ways). ","I'm not sure what's going on here, but I'm wondering what exactly SLURM does to expose a particular GPU to a task. Does it set `CUDA_VISIBLE_DEVICES`? Something else? Perhaps it sets some other environment variables? Since I have no access to or experience with SLURM I'd want to see if we can reproduce the problem without using SLURM. I wonder if we can replicate what SLURM does to the task in isolation, without using SLURM? That would help us replicate the problem and allow us to debug it.", were you able to resolve this?,Similar issue. Waiting for a solution... I cannot run multiple GPUs in a GPU.,"Here's a minimum reproducable example, where the second GPU node will not be able to bind to the GPU: ``` import os import time import jax import launchpad as lp import tensorflow as tf from absl import app from launchpad.nodes.python.local_multi_processing import PythonProcess class DeviceTester:     def test(self):         print(os.environ)         print(""Backends: "", jax.default_backend())         print(""Devices: "", jax.devices())     def run(self):         time.sleep(5)         lp.stop() def _build_test_node():     return lp.CourierNode(DeviceTester) def main(_):      Test independent of Launchpad.     print(""\n\nLocal"")     print(os.environ)     test = DeviceTester()     test.test()      Test GPU accessibility on Launchpad.     print(""\n\nLaunchpad"")     program = lp.Program(name=""experiment"")     handles = []     with program.group(""test_cpu""):         handles.append(program.add_node(_build_test_node()))     with program.group(""test_gpu""):         handles.append(program.add_node(_build_test_node()))         handles.append(program.add_node(_build_test_node()))     lp.launch(         program,         launch_type=lp.LaunchType.LOCAL_MULTI_PROCESSING,         terminal=""current_terminal"",         local_resources={             ""test_cpu"": PythonProcess(                 env={                     ""CUDA_VISIBLE_DEVICES"": """",                     ""JAX_PLATFORM_NAME"": ""cpu"",                 }             ),             ""test_gpu"": PythonProcess(                 env={                     ""CUDA_VISIBLE_DEVICES"": ""0"",                     ""XLA_PYTHON_CLIENT_MEM_FRACTION"": "".2"",                     ""XLA_PYTHON_CLIENT_PREALLOCATE"": ""false"",                     ""JAX_PLATFORM_NAME"": ""gpu"",                 }             ),         },     )     for handle in handles:         handle.dereference().test() if __name__ == ""__main__"":      Provide access to jax_backend_target and jax_xla_backend flags.     jax.config.config_with_absl()      Binary should use CPU     jax.config.update(""jax_platform_name"", ""cpu"")     tf.config.experimental.set_visible_devices([], ""GPU"")     app.run(main) ``` ``` The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""launchpad_gpu_test.py"", line 80, in      app.run(main)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""launchpad_gpu_test.py"", line 70, in main     handle.dereference().test()   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/courier/python/client.py"", line 52, in inner_function     raise translate_status(e.status) from e pybind11_abseil.status.StatusNotOk: Python exception was raised on the server: Traceback (most recent call last):   File ""launchpad_gpu_test.py"", line 14, in test     print(""Backends: "", jax.default_backend())   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 490, in default_backend     return get_backend(None).platform   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 427, in get_backend     return _get_backend_uncached(platform)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 413, in _get_backend_uncached     platform = canonicalize_platform(platform)   File ""/home/mxsmith/.conda/envs/model38/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py"", line 294, in canonicalize_platform     raise RuntimeError(f""Unknown backend: '{platform}' requested, but no "" RuntimeError: Unknown backend: 'gpu' requested, but no platforms that are instances of gpu are present. Platforms present are: interpreter,cpu ``` Both GPU nodes have the same env variables, which are: ``` [test_gpu/1] environ({'CONDA_SHLVL': '2', 'LD_LIBRARY_PATH': '/home/mxsmith/.conda/envs/model38/lib', 'LS_COLORS': 'no=00:di=34;01:tw=34;01:ow=34;01:fi=00:ln=00:pi=00:so=00:bd=00:cd=00:or=00:mi=00:ex=00:*.sh=31:*.exe=31:*.bat=31', 'CONDA_EXE': '/sw/pkgs/arc/python3.9anaconda/2021.11/bin/conda', 'SRUN_DEBUG': '3', 'SLURM_STEP_ID': '0', 'SLURM_STEP_GPUS': '0', 'SLURM_NODEID': '0', 'SLURM_TASK_PID': '827752', 'HTTP_PROXY': 'http://proxy.arcts.umich.edu:3128/', 'SSH_CONNECTION': '141.211.21.82 51992 141.211.192.38 22', 'SLURM_PRIO_PROCESS': '0', 'SLURM_CPU_BIND_VERBOSE': 'quiet', 'IEX_TOKEN': 'pk_6cd11420a64b4d5a856ac31281418f38', 'LANG': 'en_US.UTF8', 'SLURM_SUBMIT_DIR': '/home/mxsmith', 'HISTCONTROL': 'ignoreboth:erasedups', 'HOSTNAME': 'gl1520.arcts.umich.edu', 'OLDPWD': '/home/mxsmith', 'SLURM_STEPID': '0', 'SLURM_SRUN_COMM_HOST': '141.211.192.38', 'EDITOR': 'emacs', 'SLURM_DISTRIBUTION': 'cyclic', 'ROCR_VISIBLE_DEVICES': '0', 'CONDA_PREFIX': '/home/mxsmith/.conda/envs/model38', 'SQUEUE_FORMAT': '%.18i %.9P %40j %.8u %.2t %.10M %.20R', 'SLURM_PROCID': '0', 'SLURM_JOB_GID': '99464869', 'SLURM_CPU_BIND': 'quiet,mask_cpu:0x00000001', 'SLURMD_NODENAME': 'gl1520', 'GIT_EDITOR': 'emacs', 'SLURM_TASKS_PER_NODE': '1', 'S_COLORS': 'auto', '_CE_M': '', 'XLA_PYTHON_CLIENT_PREALLOCATE': 'false', 'TF2_BEHAVIOR': '1', 'XDG_SESSION_ID': '11093', 'SLURM_NNODES': '1', 'USER': 'mxsmith', 'SLURM_LAUNCH_NODE_IPADDR': '141.211.192.38', 'CONDA_PREFIX_1': '/sw/pkgs/arc/python3.9anaconda/2021.11', 'SLURM_STEP_TASKS_PER_NODE': '1', 'MATPLOTLIBRC': '/home/mxsmith/profile/matplotlib', 'FTP_PROXY': 'http://proxy.arcts.umich.edu:3128/', 'PWD': '/home/mxsmith/projects', 'SSH_ASKPASS': '/usr/libexec/openssh/gnomesshaskpass', 'SLURM_JOB_NODELIST': 'gl1520', 'HOME': '/home/mxsmith', 'SLURM_CLUSTER_NAME': 'greatlakes', 'CONDA_PYTHON_EXE': '/sw/pkgs/arc/python3.9anaconda/2021.11/bin/python', 'SLURM_NODELIST': 'gl1520', 'SLURM_GPUS_ON_NODE': '1', 'SSH_CLIENT': '141.211.21.82 51992 22', 'LMOD_VERSION': '8.6.14', 'SLURM_NTASKS': '1', 'TMUX': '/tmp/tmux99464869/default,827956,0', 'rsync_proxy': 'proxy.arcts.umich.edu:3128', 'SLURM_UMASK': '0002', 'https_proxy': 'http://proxy.arcts.umich.edu:3128/', 'KRB5CCNAME': 'FILE:/tmp/krb5cc_99464869_TngfOC', 'TF_CPP_MIN_LOG_LEVEL': '1', 'SLURM_JOB_CPUS_PER_NODE': '4', 'BASH_ENV': '/sw/lmod/lmod/init/bash', 'XDG_DATA_DIRS': '/home/mxsmith/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share', 'AUTOJUMP_ERROR_PATH': '/home/mxsmith/.local/share/autojump/errors.log', 'SLURM_TOPOLOGY_ADDR': 'gl1520', 'http_proxy': 'http://proxy.arcts.umich.edu:3128/', '_CE_CONDA': '', 'SLURM_WORKING_CLUSTER': 'greatlakes:glctld:6817:9472:109', 'SLURM_STEP_NODELIST': 'gl1520', 'SLURM_JOB_NAME': 'bash', 'SLURM_SRUN_COMM_PORT': '60207', 'TMPDIR': '/tmp', 'LMOD_sys': 'Linux', 'SLURM_JOBID': '45908601', 'JAX_PLATFORM_NAME': 'gpu', 'SLURM_CONF': '/var/spool/slurmd.spool/confcache/slurm.conf', 'LMOD_AVAIL_STYLE': 'grouped', 'no_proxy': 'localhost,127.0.0.1,.localdomain,.umich.edu', 'LMOD_ROOT': '/sw/lmod', 'SLURM_JOB_QOS': 'normal', 'SLURM_TOPOLOGY_ADDR_PATTERN': 'node', 'CONDA_PROMPT_MODIFIER': '(model38) ', 'SSH_TTY': '/dev/pts/80', 'NO_PROXY': 'localhost,127.0.0.1,.localdomain,.umich.edu', 'MAIL': '/var/spool/mail/mxsmith', 'HTTPS_PROXY': 'http://proxy.arcts.umich.edu:3128/', 'SLURM_CPUS_ON_NODE': '4', 'XLA_PYTHON_CLIENT_MEM_FRACTION': '.2', 'VISUAL': 'emacs', 'SLURM_JOB_NUM_NODES': '1', 'AUTOJUMP_SOURCED': '1', 'SHELL': '/bin/bash', 'TERM': 'xterm256color', 'SLURM_JOB_UID': '99464869', 'SLURM_JOB_PARTITION': 'spgpu', 'SLURM_PTY_WIN_ROW': '49', 'SLURM_CPU_BIND_LIST': '0x00000001', 'SLURM_JOB_USER': 'mxsmith', 'CUDA_VISIBLE_DEVICES': '0', 'SLURM_PTY_WIN_COL': '97', 'TMUX_PANE': '%3', 'SLURM_NPROCS': '1', 'SHLVL': '3', 'SLURM_SUBMIT_HOST': 'gllogin1.arcts.umich.edu', 'SLURM_JOB_ACCOUNT': 'wellman0', 'MANPATH': '/sw/lmod/lmod/share/man:/usr/local/share/man:/usr/share/man:/opt/ddn/ime/share/man:/opt/ddn/ime/share/man:/opt/slurm/share/man/:/opt/TurboVNC/man/:/opt/ddn/ime/share/man:/opt/ddn/ime/share/man', 'SLURM_STEP_LAUNCHER_PORT': '60207', 'MODULEPATH': '/sw/lmod/lmod/modulefiles/Core:/sw/modules/Core:/sw/modules/Collections', 'SLURM_PTY_PORT': '60206', 'SLURM_GTIDS': '0', 'LOGNAME': 'mxsmith', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/99464869/bus', 'XDG_RUNTIME_DIR': '/run/user/99464869', 'MODULEPATH_ROOT': '/sw/modules', 'LMOD_PACKAGE_PATH': '/sw/lmod', 'PATH': '/home/mxsmith/software/bin:/home/mxsmith/software/bin:/home/mxsmith/.conda/envs/model38/bin:/sw/pkgs/arc/python3.9anaconda/2021.11/condabin:/home/mxsmith/software/bin:/opt/TurboVNC/bin:/opt/slurm/bin:/opt/slurm/sbin:/sw/pkgs/arc/usertools/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/lpp/mmfs/bin:/opt/ddn/ime/bin:/home/mxsmith/anaconda/bin:/home/mxsmith/software/gambit15.1.1:/home/mxsmith/.local/bin:/home/mxsmith/bin:/opt/ddn/ime/bin:/home/mxsmith/anaconda/bin:/home/mxsmith/software/gambit15.1.1:/opt/ddn/ime/bin:/home/mxsmith/anaconda/bin:/home/mxsmith/software/gambit15.1.1:/home/mxsmith/.local/bin:/home/mxsmith/bin', 'SLURM_JOB_ID': '45908601', 'SLURM_CPU_BIND_TYPE': 'mask_cpu:', 'SLURM_STEP_NUM_TASKS': '1', 'MODULESHOME': '/sw/lmod/lmod', 'CONDA_DEFAULT_ENV': 'model38', 'LMOD_SETTARG_FULL_SUPPORT': 'no', 'HISTSIZE': '1000', 'LMOD_PKG': '/sw/lmod/lmod', 'CLUSTER_NAME': 'greatlakes', 'SLURM_STEP_NUM_NODES': '1', 'ftp_proxy': 'http://proxy.arcts.umich.edu:3128/', 'RSYNC_PROXY': 'proxy.arcts.umich.edu:3128', 'LMOD_CMD': '/sw/lmod/lmod/libexec/lmod', 'SLURM_LOCALID': '0', 'GPU_DEVICE_ORDINAL': '0', 'LESSOPEN': '/usr/bin/lesspipe.sh %s', 'LMOD_DIR': '/sw/lmod/lmod/libexec', 'BASH_FUNC_module%%': '() {  local __lmod_my_status;\n local __lmod_sh_dbg;\n if [ z ""${LMOD_SH_DBG_ON+x}"" ]; then\n case ""$"" in \n *v*x*)\n __lmod_sh_dbg=\'vx\'\n ;;\n *v*)\n __lmod_sh_dbg=\'v\'\n ;;\n *x*)\n __lmod_sh_dbg=\'x\'\n ;;\n esac;\n fi;\n if [ n ""${__lmod_sh_dbg:}"" ]; then\n set +$__lmod_sh_dbg;\n echo ""Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod\'s output"" 1>&2;\n fi;\n eval ""$($LMOD_CMD bash ""$@"")"" && eval $(${LMOD_SETTARG_CMD::} s sh);\n __lmod_my_status=$?;\n if [ n ""${__lmod_sh_dbg:}"" ]; then\n echo ""Shell debugging restarted"" 1>&2;\n set $__lmod_sh_dbg;\n fi;\n return $__lmod_my_status\n}', 'BASH_FUNC_ml%%': '() {  eval ""$($LMOD_DIR/ml_cmd ""$@"")""\n}'}) ``` ``` tensorflow==2.8.3 jax @ file:///home/conda/feedstock_root/build_artifacts/jax_1671027717961/work jaxlib==0.4.1+cuda11.cudnn86 dmlaunchpad==0.5.2p ```","If you add this to the nodes you can see that cuda backend is missing from the platform processing ``` from jax._src.lib import xla_bridge print(""Backends: "", xla_bridge.backends()) ``` ``` [test_gpu/0] Backends:  {'interpreter': , 'cpu': , 'cuda': } [test_gpu/1] Backends:  {'interpreter': , 'cpu': } ``` If you also add: ``` from jax.config import config print(config.jax_platforms) ``` They're both `None`, prompting all of the backend factories to be run. ``` [test_gpu/1] {'interpreter': (, 100), 'cpu': (functools.partial(, use_tfrt=True), 0), 'tpu_driver': (, 100), 'cuda': (functools.partial(, platform_name='cuda', visible_devices_flag='jax_cuda_visible_devices'), 200), 'rocm': (functools.partial(, platform_name='rocm', visible_devices_flag='jax_rocm_visible_devices'), 200), 'tpu': (functools.partial(, timer_secs=60.0), 300), 'plugin': (, 400)} [test_gpu/0] {'interpreter': (, 100), 'cpu': (functools.partial(, use_tfrt=True), 0), 'tpu_driver': (, 100), 'cuda': (functools.partial(, platform_name='cuda', visible_devices_flag='jax_cuda_visible_devices'), 200), 'rocm': (functools.partial(, platform_name='rocm', visible_devices_flag='jax_rocm_visible_devices'), 200), 'tpu': (functools.partial(, timer_secs=60.0), 300), 'plugin': (, 400)} ```","If you further print out: ``` from jax._src.config import flags FLAGS = flags.FLAGS print(FLAGS.jax_cuda_visible_devices) from jax._src import distributed print(distributed.global_state.client) print(distributed.global_state.service) print(distributed.global_state.process_id) ``` Both have the same settings: ``` [test_gpu/1] all [test_gpu/1] None [test_gpu/1] None [test_gpu/1] 0 [test_gpu/0] all [test_gpu/0] None [test_gpu/0] None [test_gpu/0] 0 ``` As far as I can tell, all of the system's settings are the same during the handoff to XLA.","The original issue was about sharing one GPU by multiple job on SLURM. Is this your case? It was clear. If it isn't the case, can you open a new issue?  SLURM can be configured in many ways to control the GPU. Some old config was doing it via CUDA_VISIBLE_DEVICES. More recent config will do it via cgroups. Using cgroups is better as it enforce the rules, while using env var, the end user can overwrite them.","I guess I read ""Particularly, I want to launch multiple processes that use JAX and have them share a single allocated GPU. "" incorrectly then. I'll open a different issue.","For the original issue, the issue is that SLURM prevent that by default. This is a normal SLURM behavior. Otherwise, other users could just allocate on your GPUs and use its compute. This could crash your own jobs or kill them. I'm not a SLURM master, so maybe there is a cleaner way to do this. But you could do what you want by starting 1 SLURM job per GPU. But that jobs is a bash script that dispatch multiple jobs on the GPU is has access to.  what do you think of that solution?","I believe I hit the same issue here (a search did not raise this issue until now). The solution that worked for me was to: * set XLA_PYTHON_CLIENT_MEM_FRACTION to avoid having processes ask for more memory than available, * do not _call_ a Jax function before calling `jax.distributed.initialize`, * pass the `device_id` manually to `jax.distributed.initialize` (`jax.distributed.initialize(local_device_ids=[device_id])`). Note that there _is_ an issue with JAX asking for non existing device number (I point to the problematic line in my issue).", Can you create a bug specific for your last sentence so that it doesn't get lost? Thanks for the report!
1561,"以下是一个github上的jax下的一个issue, 标题是(Memory Leak when create DeviceArray in python loop(without JIT)?)， 内容是 (Okay, it is just a mistake. Since `bs = jax.lib.xla_bridge.get_backend().live_buffers()` will reference the arrays.  Original issue: ```python import gc import os import psutil import jax print(jax.__version__) print(jax.default_backend()) for _ in range(5):     A = jax.numpy.ones(1000 ** 2)     gc.collect()     bs = jax.lib.xla_bridge.get_backend().live_buffers()     if jax.default_backend() == 'cpu':         print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)     print('', len(bs))     for b in bs:         print(b.size) ``` On GPU: ```python 0.3.4 gpu  1 1000000  2 1000000 1000000  3 1000000 1000000 1000000  4 1000000 1000000 1000000 1000000  5 1000000 1000000 1000000 1000000 1000000 ``` On CPU: ```python 0.3.4 20220323 14:09:26.021477: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) cpu 229.5  1 1000000 233.34765625  2 1000000 1000000 237.15234375  3 1000000 1000000 1000000 241.0078125  4 1000000 1000000 1000000 1000000 244.5546875  5 1000000 1000000 1000000 1000000 1000000 ``` If I use `A = jax.numpy.ones(1000 ** 3)`, it will run into OOM in 4 iterations on a 16G V100.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Memory Leak when create DeviceArray in python loop(without JIT)?,"Okay, it is just a mistake. Since `bs = jax.lib.xla_bridge.get_backend().live_buffers()` will reference the arrays.  Original issue: ```python import gc import os import psutil import jax print(jax.__version__) print(jax.default_backend()) for _ in range(5):     A = jax.numpy.ones(1000 ** 2)     gc.collect()     bs = jax.lib.xla_bridge.get_backend().live_buffers()     if jax.default_backend() == 'cpu':         print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)     print('', len(bs))     for b in bs:         print(b.size) ``` On GPU: ```python 0.3.4 gpu  1 1000000  2 1000000 1000000  3 1000000 1000000 1000000  4 1000000 1000000 1000000 1000000  5 1000000 1000000 1000000 1000000 1000000 ``` On CPU: ```python 0.3.4 20220323 14:09:26.021477: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) cpu 229.5  1 1000000 233.34765625  2 1000000 1000000 237.15234375  3 1000000 1000000 1000000 241.0078125  4 1000000 1000000 1000000 1000000 244.5546875  5 1000000 1000000 1000000 1000000 1000000 ``` If I use `A = jax.numpy.ones(1000 ** 3)`, it will run into OOM in 4 iterations on a 16G V100.",2022-03-23T14:10:36Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/10007,"If I explicitly delete reference, there will be no memory leak ```python import os import psutil import jax print(jax.__version__) print(jax.default_backend()) for _ in range(5):     A = jax.numpy.ones(1000 ** 2)     del A     bs = jax.lib.xla_bridge.get_backend().live_buffers()     if jax.default_backend() == 'cpu':         print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)     print('', len(bs))     for b in bs:         print(b.size) ``` However, IIUC python assignment semantic, rebind an already bound name to another value should decrease reference count for the object previously bound to the name.", I would appreciate it if you could let me know whether you/JAXteam have seen this issue., Could you maybe leave this issue opened? Because I think this is still a present bug of JAX. I'm curious what references those DeviceArray instances. I as well will try troubleshoothing this in my next free cycle.,"Sorry, I missed this the first time around. I think your assessment in the edited comment is correct: `bs` holds references to the live buffers, meaning that when the name is rebound they do not have their reference count reduced to zero. Note also that in Python `del X` does not guarantee that the memory associated wtih `X` will be deleted; if there are no other references to `X`, the refcount will go to zero, but it won't actually be deleted until the garbage collector runs. You can force garbage collection by running `import gc; gc.collect()`","IIUC, in Python we need `gc` only when there are cyclic references, i.e. the reference count never reduced to zero. Reference counting in Python is just like `shared_ptr` in c++."
1484,"以下是一个github上的jax下的一个issue, 标题是(Downcasting from fp to uint formats diverges from numpy behavior)， 内容是 (`jax.numpy` differs from `numpy` in the way it treats downcasting from floatingpoint formats to unsigned integer formats (and possibly with other combinations). Consider the following: ```python import numpy as np import jax.numpy as jnp i32 = np.array([255, 1, 256, 255, 257], dtype=np.int32) f32 = np.array([255, 1, 256, 255, 257], dtype=np.float32) print(f'                 {i32.astype(np.uint8) = }') print(f'{jnp.array(i32).astype(np.uint8) = }') print() print(f'                 {f32.astype(np.uint8) = }') print(f'{jnp.array(f32).astype(np.uint8) = }')                   i32.astype(np.uint8) = array([  1, 255,   0, 255,   1], dtype=uint8)  jnp.array(i32).astype(np.uint8) = DeviceArray([  1, 255,   0, 255,   1], dtype=uint8)                    f32.astype(np.uint8) = array([  1, 255,   0, 255,   1], dtype=uint8)  jnp.array(f32).astype(np.uint8) = DeviceArray([  0,   0, 255, 255, 255], dtype=uint8) ``` My expectation was that the last line would match the others, but JAX is clipping rather than wrapping around like the `numpy`  behavior in the penultimate line.  I looked for this in JAX  The Sharp Bits, but didn't turn up anything.   If this behavior is not a bug, it would be helpful to document it there. Thanks in advance!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Downcasting from fp to uint formats diverges from numpy behavior,"`jax.numpy` differs from `numpy` in the way it treats downcasting from floatingpoint formats to unsigned integer formats (and possibly with other combinations). Consider the following: ```python import numpy as np import jax.numpy as jnp i32 = np.array([255, 1, 256, 255, 257], dtype=np.int32) f32 = np.array([255, 1, 256, 255, 257], dtype=np.float32) print(f'                 {i32.astype(np.uint8) = }') print(f'{jnp.array(i32).astype(np.uint8) = }') print() print(f'                 {f32.astype(np.uint8) = }') print(f'{jnp.array(f32).astype(np.uint8) = }')                   i32.astype(np.uint8) = array([  1, 255,   0, 255,   1], dtype=uint8)  jnp.array(i32).astype(np.uint8) = DeviceArray([  1, 255,   0, 255,   1], dtype=uint8)                    f32.astype(np.uint8) = array([  1, 255,   0, 255,   1], dtype=uint8)  jnp.array(f32).astype(np.uint8) = DeviceArray([  0,   0, 255, 255, 255], dtype=uint8) ``` My expectation was that the last line would match the others, but JAX is clipping rather than wrapping around like the `numpy`  behavior in the penultimate line.  I looked for this in JAX  The Sharp Bits, but didn't turn up anything.   If this behavior is not a bug, it would be helpful to document it there. Thanks in advance!",2022-03-23T12:52:45Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/10005,"You may refer to: https://github.com/google/jax/blob/e0d3946e341903bb5cedfe605618b06438f9fe1d/jax/_src/numpy/lax_numpy.pyL4882 => `DeviceArray.astype` is implemented in `_astype` https://github.com/google/jax/blob/e0d3946e341903bb5cedfe605618b06438f9fe1d/jax/_src/numpy/lax_numpy.pyL4343L4347 => `_astype` use `lax.convert_element_type` https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.convert_element_type.html => `lax.convert_element_type` wrap XLA ConvertElementType https://www.tensorflow.org/xla/operation_semanticsconvertelementtype > Similar to an elementwise static_cast in C++ > The precise floattoint and visaversa conversions are currently unspecified, but may become additional arguments to the convert operation in the future. Not all possible conversions have been implemented for all targets. It seems that the behavior is implementation defined.","Thanks, .   For future reference, this is on GPU: ```python print(f'{jax.devices()[0].device_kind = }') print(f'{jax.__version__ = }, {jaxlib.__version__ = }')  jax.devices()[0].device_kind = 'Tesla V100DGXS32GB'  jax.__version__ = '0.3.4', jaxlib.__version__ = '0.3.2' ```",Thanks for bringing this up. I've clarified the behavior in the docstring for `astype` in https://github.com/google/jax/pull/10009,"More often than not I do conversions using this form: `jnp.uint8(jnp.arange(300, dtype='<f4'))`.  Is there a good way to document the unexpected numpy divergence in that case?  Or is there some sort of warning behavior that can be set to protect against this footgun?","What type of documentation or warning would have been most helpful to you when you first hit this issue? For example, we could raise a warning on every floattoint or inttofloat cast that potentially diverges from NumPy's behavior, but I think that would cause so much noise that it wouldn't be helpful. We could disable those warnings by default and provide a flag to enable them if desired, but users won't know to enable the flag until they've already hit the unexpected behavior. We could document this behavior more than we've already done, but let's be honest: users don't read documentation about things like this 😁  Do you have other ideas about how we could best surface this?","For me the sequence in getting to be comfortable with JAX went something like this: 1.  Read all over the place that it is essentially a wicked fast dropin replacement for numpy on the GPU. 2. Experiment with it and quickly discover that I couldn't modify arrays inplace. 3. Go read the documentation and, in particular, try to absorb all the teachings in JAX  The Sharp Bits. So, when my JAX code started behaving differently than numpy in the sense that downcasting now cared about whether the source array was float or integer type, I just assumed that I had missed some sharp bits and I went back to study that carefully again.  I filed this bug when I didn't find anything about implementationdependent casting behavior, because I think of the Sharp Bits document as being the onestop shop for the ways in which JAX isn't just a dropin replacement for numpy. I agree that warnings would probably be too noisy, and it would probably be a lot of work to implement a JAXwide ""be pedantic"" setting for warnings, although it would have saved me in this case.  An entry in JAX  The Sharp Bits would've probably also saved me since I would've mentally logged that casting was a differentfromnumpy danger zone. Thanks for the very helpful response, by the way.  I think the support that the team gives to JAX users is outstanding.",Thanks for the suggestions: CC(Sharp bits: add miscellaneous divergences from numpy) adds a section to the Sharp Bits documentation for these kinds of miscellaneous differences. Please let us know if you can think of other things that should be listed there!,Thanks again!
295,"以下是一个github上的jax下的一个issue, 标题是(protocols for the lowering and executable underlying `Lowered` and `Compiled`)， 内容是 ((aheadoftime lowering and compilation))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,protocols for the lowering and executable underlying `Lowered` and `Compiled`,(aheadoftime lowering and compilation),2022-03-23T02:10:05Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9998
3345,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: 'Unit' object has no attribute 'shape' while tracing function. What does it mean??)， 内容是 ( Discussed in https://github.com/google/jax/discussions/9979  Originally posted by **RadostW** March 21, 2022 Inside a nested scan when using `jax.lax.cond` I get error message that is hard to understand. Original code was very complex, sadly minimal example I could make is still quite complex. Bizzarly, when changing small details code runs fine (see commented lines).  Working fine: `x * d_www[0,0,0]  Ok` `x * f_www(x)[0,0,0,0]  Ok` Throwing hard to understand error `jnp.array([1.0]) * f_www(x)[0, 0, 0, 0] * d_www[0, 0, 0]   AttributeError` Which makes me think that both `d_www` creation and `f_www` creation work fine. I've found no similar issues when searching. ~~R ```python import jax.numpy as jnp import jax def upper(y):     return jnp.exp(y) def lower(y):     return jnp.exp(2 * y) def branch_fun(q):     x = q[0]     ret = jax.lax.cond(x > 1.0, upper, lower, x)     return jnp.array([[ret]]) def L_w(f):     def wrapped(x):         return jax.numpy.tensordot(jax.jacobian(f)(x), branch_fun(x), axes=1)     return wrapped def solve_many():     t0 = 0.0     w0 = jax.numpy.zeros(1)     id_ = lambda x: x     f_www = L_w(L_w(L_w(id_)))     def step(         x,         d_www,     ):         new_x = (              x * d_www[0,0,0]  Ok              x * f_www(x)[0,0,0,0]  Ok             jnp.array([1.0]) * f_www(x)[0, 0, 0, 0] * d_www[0, 0, 0]   AttributeError              x * f_www(x)[0,0,0,0] * d_www[0,0,0]  AttributeError              contract_all(f_www(x), d_www)  AttributeError              f_www(x)[:,0,0,0]  AttributeError         )         return new_x     key = jax.random.PRNGKey(0)     def scan_func(x, y):         xp = step(x, y)         return (xp, xp)     def chunk_function(x, y):         z = jax.lax.scan(scan_func, jnp.array([0.1]), y)[0]         return z, z     def get_solution_fragment(starting_state, key):         wiener_integrals = jax.random.normal(key, shape=(4, 1, 1, 1))         last_state, solution_values = jax.lax.scan(             chunk_function,             starting_state,             jnp.reshape(wiener_integrals, (2, 2, 1, 1, 1)),         )         return (last_state, last_state)     .vmap     def get_solution(key):         _, chunked_solution = jax.lax.scan(             lambda state, key: get_solution_fragment(state, key),             (jnp.array([0.1])),             jax.random.split(key, 1),         )         return chunked_solution     keys = jax.random.split(key, 1)     solutions = get_solution(keys)     return solutions def test_branched_coefficients():     trajectories = solve_many() if __name__ == ""__main__"":     trajectories = solve_many() ``` ``` The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test_branched_coefficients.py"", line 91, in      trajectories = solve_many()   File ""test_branched_coefficients.py"", line 81, in solve_many     solutions = get_solution(keys)   File ""test_branched_coefficients.py"", line 72, in get_solution     _, chunked_solution = jax.lax.scan( AttributeError: 'Unit' object has no attribute 'shape' ``` )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,AttributeError: 'Unit' object has no attribute 'shape' while tracing function. What does it mean??," Discussed in https://github.com/google/jax/discussions/9979  Originally posted by **RadostW** March 21, 2022 Inside a nested scan when using `jax.lax.cond` I get error message that is hard to understand. Original code was very complex, sadly minimal example I could make is still quite complex. Bizzarly, when changing small details code runs fine (see commented lines).  Working fine: `x * d_www[0,0,0]  Ok` `x * f_www(x)[0,0,0,0]  Ok` Throwing hard to understand error `jnp.array([1.0]) * f_www(x)[0, 0, 0, 0] * d_www[0, 0, 0]   AttributeError` Which makes me think that both `d_www` creation and `f_www` creation work fine. I've found no similar issues when searching. ~~R ```python import jax.numpy as jnp import jax def upper(y):     return jnp.exp(y) def lower(y):     return jnp.exp(2 * y) def branch_fun(q):     x = q[0]     ret = jax.lax.cond(x > 1.0, upper, lower, x)     return jnp.array([[ret]]) def L_w(f):     def wrapped(x):         return jax.numpy.tensordot(jax.jacobian(f)(x), branch_fun(x), axes=1)     return wrapped def solve_many():     t0 = 0.0     w0 = jax.numpy.zeros(1)     id_ = lambda x: x     f_www = L_w(L_w(L_w(id_)))     def step(         x,         d_www,     ):         new_x = (              x * d_www[0,0,0]  Ok              x * f_www(x)[0,0,0,0]  Ok             jnp.array([1.0]) * f_www(x)[0, 0, 0, 0] * d_www[0, 0, 0]   AttributeError              x * f_www(x)[0,0,0,0] * d_www[0,0,0]  AttributeError              contract_all(f_www(x), d_www)  AttributeError              f_www(x)[:,0,0,0]  AttributeError         )         return new_x     key = jax.random.PRNGKey(0)     def scan_func(x, y):         xp = step(x, y)         return (xp, xp)     def chunk_function(x, y):         z = jax.lax.scan(scan_func, jnp.array([0.1]), y)[0]         return z, z     def get_solution_fragment(starting_state, key):         wiener_integrals = jax.random.normal(key, shape=(4, 1, 1, 1))         last_state, solution_values = jax.lax.scan(             chunk_function,             starting_state,             jnp.reshape(wiener_integrals, (2, 2, 1, 1, 1)),         )         return (last_state, last_state)     .vmap     def get_solution(key):         _, chunked_solution = jax.lax.scan(             lambda state, key: get_solution_fragment(state, key),             (jnp.array([0.1])),             jax.random.split(key, 1),         )         return chunked_solution     keys = jax.random.split(key, 1)     solutions = get_solution(keys)     return solutions def test_branched_coefficients():     trajectories = solve_many() if __name__ == ""__main__"":     trajectories = solve_many() ``` ``` The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test_branched_coefficients.py"", line 91, in      trajectories = solve_many()   File ""test_branched_coefficients.py"", line 81, in solve_many     solutions = get_solution(keys)   File ""test_branched_coefficients.py"", line 72, in get_solution     _, chunked_solution = jax.lax.scan( AttributeError: 'Unit' object has no attribute 'shape' ``` ",2022-03-21T19:30:08Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/9985,"minimal repro: ```python import jax.numpy as jnp import jax id_f = lambda x: x def b(x):     return jax.lax.cond(x[0], id_f, id_f, x)  jnp.where(x[0], id_f(x), id_f(x)) is ok f = jax.jacrev(lambda x: b(b(x)))  jacfwd is ok def repro():     jax.vmap(f)(jnp.ones((1,1)))  without vmap is ok if __name__ == ""__main__"":     with jax.disable_jit():         repro() ```"
4223,"以下是一个github上的jax下的一个issue, 标题是(Bump actions/cache from 2 to 3)， 内容是 (Bumps actions/cache from 2 to 3.  Release notes Sourced from actions/cache's releases.  v3.0.0   This change adds a minimum runner version(node12 &gt; node16), which can break users using an outofdate/fork of the runner. This would be most commonly affecting users on GHES 3.3 or before, as those runners do not support node16 actions and they can use actions from github.com via github connect or manually copying the repo to their GHES instance.   Few dependencies and cache action usage examples have also been updated.   v2.1.7 Support 10GB cache upload using the latest version 1.0.8 of /cache  v2.1.6  Catch unhandled &quot;bad file descriptor&quot; errors that sometimes occurs when the cache server returns nonsuccessful response (actions/cache CC(python performance improvements))  v2.1.5  Fix permissions error seen when extracting caches with GNU tar that were previously created using BSD tar (actions/cache CC(add docstring for `jax.linearize`))  v2.1.4  Make caching more verbose  CC(jacfwd through while_loop) Use GNU tar on macOS if available  CC(add adagrad optimizer)  v2.1.3  Upgrades /core to v1.2.6 for CVE202015228. This action was not using the affected methods. Fix error handling in uploadChunk where 400level errors were not being detected and handled correctly  v2.1.2  Adds input to limit the chunk upload size, useful for selfhosted runners with slower upload speeds Noop when executing on GHES  v2.1.1  Update /cache package to v1.0.2 which allows cache action to use posix format when taring files.  v2.1.0  Replaces the httpclient with the Azure Storage SDK for NodeJS when downloading cache content from Azure.  This should help improve download performance and reliability as the SDK downloads files in 4 MB chunks, which can be parallelized and retried independently Display download progress and speed     Commits  4b0cf6c Merge pull request  CC(fix travis by running `pytest n 1` instead of `n 2`) from actions/users/ashwinsangem/bump_major_version 60c606a Update licensed files b6e9a91 Revert &quot;Updated to the latest version.&quot; c842503 Updated to the latest version. 2b7da2a Bumped up to a major version. deae296 Merge pull request  CC(Add Differentially Private SGD example) from magnetikonline/fixgolangwindowsexample c7c46bc Merge pull request  CC(Implement np.linalg.eig on CPU.) from duxtland/main 6535c5f Regenerated examples.md TOC 3fdafa4 Update GitHub Actions status badge markdown in README.md 341e6d7 Merge branch 'actions:main' into fixgolangwindowsexample Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bump actions/cache from 2 to 3,"Bumps actions/cache from 2 to 3.  Release notes Sourced from actions/cache's releases.  v3.0.0   This change adds a minimum runner version(node12 &gt; node16), which can break users using an outofdate/fork of the runner. This would be most commonly affecting users on GHES 3.3 or before, as those runners do not support node16 actions and they can use actions from github.com via github connect or manually copying the repo to their GHES instance.   Few dependencies and cache action usage examples have also been updated.   v2.1.7 Support 10GB cache upload using the latest version 1.0.8 of /cache  v2.1.6  Catch unhandled &quot;bad file descriptor&quot; errors that sometimes occurs when the cache server returns nonsuccessful response (actions/cache CC(python performance improvements))  v2.1.5  Fix permissions error seen when extracting caches with GNU tar that were previously created using BSD tar (actions/cache CC(add docstring for `jax.linearize`))  v2.1.4  Make caching more verbose  CC(jacfwd through while_loop) Use GNU tar on macOS if available  CC(add adagrad optimizer)  v2.1.3  Upgrades /core to v1.2.6 for CVE202015228. This action was not using the affected methods. Fix error handling in uploadChunk where 400level errors were not being detected and handled correctly  v2.1.2  Adds input to limit the chunk upload size, useful for selfhosted runners with slower upload speeds Noop when executing on GHES  v2.1.1  Update /cache package to v1.0.2 which allows cache action to use posix format when taring files.  v2.1.0  Replaces the httpclient with the Azure Storage SDK for NodeJS when downloading cache content from Azure.  This should help improve download performance and reliability as the SDK downloads files in 4 MB chunks, which can be parallelized and retried independently Display download progress and speed     Commits  4b0cf6c Merge pull request  CC(fix travis by running `pytest n 1` instead of `n 2`) from actions/users/ashwinsangem/bump_major_version 60c606a Update licensed files b6e9a91 Revert &quot;Updated to the latest version.&quot; c842503 Updated to the latest version. 2b7da2a Bumped up to a major version. deae296 Merge pull request  CC(Add Differentially Private SGD example) from magnetikonline/fixgolangwindowsexample c7c46bc Merge pull request  CC(Implement np.linalg.eig on CPU.) from duxtland/main 6535c5f Regenerated examples.md TOC 3fdafa4 Update GitHub Actions status badge markdown in README.md 341e6d7 Merge branch 'actions:main' into fixgolangwindowsexample Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2022-03-21T17:13:19Z,pull ready dependencies github_actions,closed,0,0,https://github.com/jax-ml/jax/issues/9982
1252,"以下是一个github上的jax下的一个issue, 标题是(TPU cannot do simple arithmetic!)， 内容是 (I am trying to do simple matrix multiplication on TPU, but it gives a wrong result: ```python import jax.numpy as np import numpy as onp  On CPU x = onp.array([[0.3744, 0.1656],                [0.4707, 0.1663]]) y = onp.array([[0.3946, 0.1186],                [0.1569, 0.3145]]) z = onp.dot(x, y)  On TPU x_ = np.asarray(x) y_ = np.asarray(y) z_ = np.dot(x_, y_) print('JAX device:', x_.device())  Compare print('CPU result:', z) print('TPU result:', z_) assert np.allclose(z, z_) ``` Output: ``` JAX device: TPU_0(process=0,(0,0,0,0)) CPU result: [[0.17372088 0.09648504]  [0.21183069 0.10812637]] TPU result: [[0.17405128 0.09669876]  [0.21180916 0.10805416]] Traceback (most recent call last):   File ""/home/ayaka/main.py"", line 21, in      assert np.allclose(z, z_) AssertionError ``` Manual calculation: 0.3744 * 0.3946 + 0.1656 * 0.1569 = 0.13732088 So the result on CPU is correct, while the result on TPU is wrong. Library versions: ```sh jax                  0.3.4 jaxlib               0.3.2 libtpunightly       0.1.dev20220315 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,TPU cannot do simple arithmetic!,"I am trying to do simple matrix multiplication on TPU, but it gives a wrong result: ```python import jax.numpy as np import numpy as onp  On CPU x = onp.array([[0.3744, 0.1656],                [0.4707, 0.1663]]) y = onp.array([[0.3946, 0.1186],                [0.1569, 0.3145]]) z = onp.dot(x, y)  On TPU x_ = np.asarray(x) y_ = np.asarray(y) z_ = np.dot(x_, y_) print('JAX device:', x_.device())  Compare print('CPU result:', z) print('TPU result:', z_) assert np.allclose(z, z_) ``` Output: ``` JAX device: TPU_0(process=0,(0,0,0,0)) CPU result: [[0.17372088 0.09648504]  [0.21183069 0.10812637]] TPU result: [[0.17405128 0.09669876]  [0.21180916 0.10805416]] Traceback (most recent call last):   File ""/home/ayaka/main.py"", line 21, in      assert np.allclose(z, z_) AssertionError ``` Manual calculation: 0.3744 * 0.3946 + 0.1656 * 0.1569 = 0.13732088 So the result on CPU is correct, while the result on TPU is wrong. Library versions: ```sh jax                  0.3.4 jaxlib               0.3.2 libtpunightly       0.1.dev20220315 ```",2022-03-21T07:41:24Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/9973,"There are two ""sharp bits"" to be aware of here. **1. float64**  Your NumPy example is running with f64 inputs/outputs but the JAX version is f32.  See https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmldouble64bitprecision **2. Compute precision**  XLA supports computing dot product in various precisions. JAX's default precision is optimised for performance and thus has relatively low precision (see CC(🔪 Remaining Sharp Bit TODOs 🔪)). This is typically OK for NN training, but can be the wrong choice for other applications. You can adjust the precision globally using: ```python jax.config.update('jax_default_matmul_precision', jax.lax.Precision.HIGHEST) ``` Or you can do so on a per operation basis: ```python c = jnp.dot(a, b, precision=jax.lax.Precision.HIGHEST) ``` Here is a copy of your example using `HIGHEST` precision on TPU (with f32 inputs) and getting a very close result compared to CPU: https://colab.research.google.com/gist/tomhennigan/69844409e46fd71267acf7479e2ba7f4/exampleofchangingdefaultprecisioninjax.ipynb","Thank you for the explanation! I am still wondering: 1\. Are there any research indicating low precision will not affect the model performance? As deep learning models are growing larger and larger, I am thinking that different precision may result to totally different output after many layers of operations. 2\. How much training time can I save when using lower precision?","> Are there any research indicating low precision will not affect the model performance? As deep learning models are growing larger and larger, I am thinking that different precision may result to totally different output after many layers of operations. It is important to note that while the default precision is low it is deterministic, so if you train a model in low precision and do inference on that trained model in low precision you should get the expected answer. For very large models it is typical to drop the precision, because you need to compute so many floating point operations to train these models that the improvement in performance very significant on training time. For Gopher (a large language model from DeepMind) we talk about low precision training (even lower than f32 defaults in JAX) with bfloat16 in Appendix C.2 of our paper https://arxiv.org/pdf/2112.11446.pdf. > How much training time can I save when using lower precision? Typically accelerators have special hardware (""tensor cores"") for half precision (e.g. bf16, f16) compute and you can expect computations to run somewhere between 510x faster than full precision f32 computations. JAX's default precision for f32 dot product means the actual computation is done in bf16 on the TPU, so the performance improvement is significant vs. `Precision.HIGH` or `Precision.HIGHEST`.",Thank you for the detailed explanation!,"I think we can close this issue, but please let me know if not!"
790,"以下是一个github上的jax下的一个issue, 标题是(track input argument information in one tree at each AOT stage)， 内容是 (Both `Lowered` and `Compiled` carry information about input arguments for which the underlying computation was lowered (namely avals, donation bits, and the input pytree structure today). This change rearranges some internals so that all of this information is held together in a single pytree of structs. Doing so simplifies the fields of both stage classes and helps ensure the input argument properties are consistent with one another (e.g. now they must share a consistent pytree structure by definition). (aheadoftime lowering and compilation))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,track input argument information in one tree at each AOT stage,"Both `Lowered` and `Compiled` carry information about input arguments for which the underlying computation was lowered (namely avals, donation bits, and the input pytree structure today). This change rearranges some internals so that all of this information is held together in a single pytree of structs. Doing so simplifies the fields of both stage classes and helps ensure the input argument properties are consistent with one another (e.g. now they must share a consistent pytree structure by definition). (aheadoftime lowering and compilation)",2022-03-19T04:47:51Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/9967,"I'm leaning on existing test coverage (e.g. these) to ensure that the change in this internal representation doesn't affect how we lower, execute, or guard against incorrect invocations."
3915,"以下是一个github上的jax下的一个issue, 标题是(bug with jit, custom_vjp and reduce_window )， 内容是 (I have `jaxlib==0.3.2` and `jax=0.3.3` and the following code crash on my CPU (might also crash on other devices) ```python import jax import jax.numpy as jnp import math from jax import lax def _index_max_norm(input, strides):     norms = jnp.sum(input**2, axis=1)     shape = input.shape[:1]     assert len(shape) == len(strides)     idxs = jnp.arange(math.prod(shape)).reshape(shape)     def g(a, b):         an, ai = a         bn, bi = b         which = an >= bn         return (jnp.where(which, an, bn), jnp.where(which, ai, bi))     _, idxs = lax.reduce_window(         (norms, idxs),         (jnp.inf, 1),         g,         window_dimensions=strides,         window_strides=strides,         padding=((0, 0),) * len(strides),     )     return idxs .custom_vjp def norm_maxpool(input, strides):     idxs = _index_max_norm(input, strides)     return input.reshape(math.prod(input.shape[:1]), input.shape[1])[idxs] def norm_maxpool_fwd(input, strides):     idxs = _index_max_norm(input, strides)     output = input.reshape(math.prod(input.shape[:1]), input.shape[1])[idxs]     return output, (idxs, input.shape) def norm_maxpool_bwd(residuals, grad):     idxs, shape = residuals     idxs = idxs.flatten()     grad = jnp.zeros((math.prod(shape[:1]),) + (grad.shape[1],)).at[idxs].add(grad.reshape(idxs.shape[0], 1))     grad = grad.reshape(shape)     return (grad, None) norm_maxpool.defvjp(norm_maxpool_fwd, norm_maxpool_bwd) x = jnp.array([     [[1.0, 2], [3, 4]],     [[5, 6], [7, 8]],     [[9, 10], [11, 12]],     [[9, 10], [1, 12]], ]) jax.jit(lambda x: norm_maxpool(x, (2, 2)))(x) ``` ```  TypeError                                 Traceback (most recent call last)     [... skipping hidden 1 frame] ~/base/lib/python3.9/sitepackages/jax/core.py in canonicalize_shape(shape, context)    1594     pass > 1595   raise _invalid_shape_error(shape, context)    1596  TypeError: Shapes must be 1D sequences of concrete values of integer type, got [Tracedwith  Tracedwith]. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last) /tmp/ipykernel_158588/13487716.py in       57 ])      58  > 59 jax.jit(lambda x: norm_maxpool(x, (2, 2)))(x)     [... skipping hidden 14 frame] /tmp/ipykernel_158588/13487716.py in (x)      57 ])      58  > 59 jax.jit(lambda x: norm_maxpool(x, (2, 2)))(x)     [... skipping hidden 6 frame] /tmp/ipykernel_158588/13487716.py in norm_maxpool(input, strides)      30 .custom_vjp      31 def norm_maxpool(input, strides): > 32     idxs = _index_max_norm(input, strides)      33     return input.reshape(math.prod(input.shape[:1]), input.shape[1])[idxs]      34  /tmp/ipykernel_158588/13487716.py in _index_max_norm(input, strides)      17         return (jnp.where(which, an, bn), jnp.where(which, ai, bi))      18  > 19     _, idxs = lax.reduce_window(      20         (norms, idxs),      21         (jnp.inf, 1),     [... skipping hidden 7 frame] ~/base/lib/python3.9/sitepackages/jax/_src/lax/lax.py in _check_shapelike(fun_name, arg_name, obj, non_zero_shape)    4554   except TypeError as err:    4555     msg = ""{} {} must have every element be an integer type, got {}."" > 4556     raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj)))) from err    4557   lower_bound, bound_error = (    4558       (1, ""strictly positive"") if non_zero_shape else (0, ""nonnegative"")) TypeError: reduce_window window_dimensions must have every element be an integer type, got (, ). ``` Surprisingly, if I do one of the following it works:  remove `jit`  remove `custom_vjp`  add `grad`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"bug with jit, custom_vjp and reduce_window ","I have `jaxlib==0.3.2` and `jax=0.3.3` and the following code crash on my CPU (might also crash on other devices) ```python import jax import jax.numpy as jnp import math from jax import lax def _index_max_norm(input, strides):     norms = jnp.sum(input**2, axis=1)     shape = input.shape[:1]     assert len(shape) == len(strides)     idxs = jnp.arange(math.prod(shape)).reshape(shape)     def g(a, b):         an, ai = a         bn, bi = b         which = an >= bn         return (jnp.where(which, an, bn), jnp.where(which, ai, bi))     _, idxs = lax.reduce_window(         (norms, idxs),         (jnp.inf, 1),         g,         window_dimensions=strides,         window_strides=strides,         padding=((0, 0),) * len(strides),     )     return idxs .custom_vjp def norm_maxpool(input, strides):     idxs = _index_max_norm(input, strides)     return input.reshape(math.prod(input.shape[:1]), input.shape[1])[idxs] def norm_maxpool_fwd(input, strides):     idxs = _index_max_norm(input, strides)     output = input.reshape(math.prod(input.shape[:1]), input.shape[1])[idxs]     return output, (idxs, input.shape) def norm_maxpool_bwd(residuals, grad):     idxs, shape = residuals     idxs = idxs.flatten()     grad = jnp.zeros((math.prod(shape[:1]),) + (grad.shape[1],)).at[idxs].add(grad.reshape(idxs.shape[0], 1))     grad = grad.reshape(shape)     return (grad, None) norm_maxpool.defvjp(norm_maxpool_fwd, norm_maxpool_bwd) x = jnp.array([     [[1.0, 2], [3, 4]],     [[5, 6], [7, 8]],     [[9, 10], [11, 12]],     [[9, 10], [1, 12]], ]) jax.jit(lambda x: norm_maxpool(x, (2, 2)))(x) ``` ```  TypeError                                 Traceback (most recent call last)     [... skipping hidden 1 frame] ~/base/lib/python3.9/sitepackages/jax/core.py in canonicalize_shape(shape, context)    1594     pass > 1595   raise _invalid_shape_error(shape, context)    1596  TypeError: Shapes must be 1D sequences of concrete values of integer type, got [Tracedwith  Tracedwith]. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last) /tmp/ipykernel_158588/13487716.py in       57 ])      58  > 59 jax.jit(lambda x: norm_maxpool(x, (2, 2)))(x)     [... skipping hidden 14 frame] /tmp/ipykernel_158588/13487716.py in (x)      57 ])      58  > 59 jax.jit(lambda x: norm_maxpool(x, (2, 2)))(x)     [... skipping hidden 6 frame] /tmp/ipykernel_158588/13487716.py in norm_maxpool(input, strides)      30 .custom_vjp      31 def norm_maxpool(input, strides): > 32     idxs = _index_max_norm(input, strides)      33     return input.reshape(math.prod(input.shape[:1]), input.shape[1])[idxs]      34  /tmp/ipykernel_158588/13487716.py in _index_max_norm(input, strides)      17         return (jnp.where(which, an, bn), jnp.where(which, ai, bi))      18  > 19     _, idxs = lax.reduce_window(      20         (norms, idxs),      21         (jnp.inf, 1),     [... skipping hidden 7 frame] ~/base/lib/python3.9/sitepackages/jax/_src/lax/lax.py in _check_shapelike(fun_name, arg_name, obj, non_zero_shape)    4554   except TypeError as err:    4555     msg = ""{} {} must have every element be an integer type, got {}."" > 4556     raise TypeError(msg.format(fun_name, arg_name, tuple(map(type, obj)))) from err    4557   lower_bound, bound_error = (    4558       (1, ""strictly positive"") if non_zero_shape else (0, ""nonnegative"")) TypeError: reduce_window window_dimensions must have every element be an integer type, got (, ). ``` Surprisingly, if I do one of the following it works:  remove `jit`  remove `custom_vjp`  add `grad`",2022-03-18T20:13:38Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/9961,Might be related to https://github.com/google/jax/discussions/9818discussioncomment2328818 but not sure,"Ok, I forget that variadic reduce_window isn't implemented on GPU. But use `jit` with `backend='cpu'` still encounter same error. set empty CUDA_VISIBLE_DEVICES to fall back to CPU still encounter same error.  On `jaxlib==0.3.2` and `jax==0.3.4` and GPU, I encounter a different error with your repro code: (folder names are replaced with ""playground"") ``` File ""/home/jiacheng/playground/play.py"", line 19, in _index_max_norm     _, idxs = lax.reduce_window(   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/_src/lax/windowed_reductions.py"", line 94, in reduce_window     out_flat = reduce_window_p.bind(   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/core.py"", line 286, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/core.py"", line 289, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/interpreters/partial_eval.py"", line 1485, in process_primitive     return self.default_process_primitive(primitive, tracers, params)   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/interpreters/partial_eval.py"", line 1489, in default_process_primitive     out_avals = primitive.abstract_eval(*avals, **params)   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/_src/lax/windowed_reductions.py"", line 259, in _reduce_window_abstract_eval_rule     out_shape = _common_reduce_window_shape_rule(   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/_src/lax/windowed_reductions.py"", line 425, in _common_reduce_window_shape_rule     lax._check_shapelike(""reduce_window"", ""window_dimensions"", window_dimensions,   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 4577, in _check_shapelike     obj_arr = np.array(obj)   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/core.py"", line 470, in __array__     raise TracerArrayConversionError(self)   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/_src/errors.py"", line 322, in __init__     f""the JAX Tracer object {tracer}{tracer._origin_msg()}"")   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/interpreters/partial_eval.py"", line 1258, in _origin_msg     f""of {dbg.arg_info(invar_pos)}."")   File ""/home/jiacheng/conda/envs/playground/lib/python3.10/sitepackages/jax/interpreters/partial_eval.py"", line 1703, in arg_info_pytree     for i in flat_pos: dummy_args[i] = True jax._src.traceback_util.UnfilteredStackTrace: IndexError: list assignment index out of range ```"
581,"以下是一个github上的jax下的一个issue, 标题是(Add small and big matmul to api_benchmarks.)， 内容是 (Add small and big matmul to api_benchmarks. name                                  cpu/op  jit_small_matmul                      2.96µs ± 2%  jit_big_matmul                        22.1µs ±21%  name                                  time/op                    jit_small_matmul                      2.96µs ± 2%   jit_big_matmul                        22.7µs ±21%)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add small and big matmul to api_benchmarks.,Add small and big matmul to api_benchmarks. name                                  cpu/op  jit_small_matmul                      2.96µs ± 2%  jit_big_matmul                        22.1µs ±21%  name                                  time/op                    jit_small_matmul                      2.96µs ± 2%   jit_big_matmul                        22.7µs ±21%,2022-03-17T21:23:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9945
1656,"以下是一个github上的jax下的一个issue, 标题是(bug in jax.nn.glu)， 内容是 (There is an issue with `jax.nn.glu`  in the latest release https://github.com/google/jax/blob/728e4fd3fad334e551dcd1a13b47b8d51aae5a9f/jax/_src/nn/functions.pyL261 Here the `static_argnames` should be `axis`. Right now  calling `glu`  results in a `TracerIntegerConversionError` ```python import jax import jax.numpy as jnp jax.nn.glu(jnp.ones((1, 4)), axis=1) ``` full error messages/tracebacks. ``` TracerIntegerConversionError              Traceback (most recent call last) Input In [8], in  > 1 jax.nn.glu(jnp.ones((1, 4)), 1)     [... skipping hidden 14 frame] File ~/transformersenv/lib/python3.8/sitepackages/jax/_src/nn/functions.py:269, in glu(x, axis)     261 (jax.jit, static_argnames=(""glu"",))     262 def glu(x: Array, axis: int = 1) > Array:     263   """"""Gated linear unit activation function.     264      265   Args:     266     x : input array     267     axis: the axis along which the split should be computed (default: 1)     268   """""" > 269   size = x.shape[axis]     270   assert size % 2 == 0, ""axis size must be divisible by 2""     271   x1, x2 = jnp.split(x, 2, axis) File ~/transformersenv/lib/python3.8/sitepackages/jax/core.py:473, in Tracer.__index__(self)     472 def __index__(self): > 473   raise TracerIntegerConversionError(self) TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object Tracedwith See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerIntegerConversionError ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,bug in jax.nn.glu,"There is an issue with `jax.nn.glu`  in the latest release https://github.com/google/jax/blob/728e4fd3fad334e551dcd1a13b47b8d51aae5a9f/jax/_src/nn/functions.pyL261 Here the `static_argnames` should be `axis`. Right now  calling `glu`  results in a `TracerIntegerConversionError` ```python import jax import jax.numpy as jnp jax.nn.glu(jnp.ones((1, 4)), axis=1) ``` full error messages/tracebacks. ``` TracerIntegerConversionError              Traceback (most recent call last) Input In [8], in  > 1 jax.nn.glu(jnp.ones((1, 4)), 1)     [... skipping hidden 14 frame] File ~/transformersenv/lib/python3.8/sitepackages/jax/_src/nn/functions.py:269, in glu(x, axis)     261 (jax.jit, static_argnames=(""glu"",))     262 def glu(x: Array, axis: int = 1) > Array:     263   """"""Gated linear unit activation function.     264      265   Args:     266     x : input array     267     axis: the axis along which the split should be computed (default: 1)     268   """""" > 269   size = x.shape[axis]     270   assert size % 2 == 0, ""axis size must be divisible by 2""     271   x1, x2 = jnp.split(x, 2, axis) File ~/transformersenv/lib/python3.8/sitepackages/jax/core.py:473, in Tracer.__index__(self)     472 def __index__(self): > 473   raise TracerIntegerConversionError(self) TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object Tracedwith See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerIntegerConversionError ```",2022-03-17T18:31:40Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/9941,Thanks for the report  it looks like a typo. The fact that this made it into a release makes me worried that we don't have good test coverage for the function. I'll make a fix,Do you think you can do a patch release for this one as we have a couple of Flax/JAX models in Transformers failing to due this on master at the moment and are also looking into doing another release soon.,Sorry for the breakage. We'll do a pypi release as soon as the PR gets merged.,We uploaded jax==0.3.3 to pypi with this fix. Google colab servers might not get updated until tomorrow night though.,Let me know if anything is still broken about this!,Thanks a lot for helping so quickly!
3453,"以下是一个github上的jax下的一个issue, 标题是(segmentation fault when runing unit test )， 内容是 (Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```bach python3  tests/lax_numpy_test.py num_generated_cases=5 ```  [x] If applicable, include full error messages/tracebacks. [ RUN      ] LaxBackedNumpyTests.testOpIscomplex_bfloat16[3,4] Fatal Python error: Segmentation fault Current thread 0x00007f19ad5eb740 (most recent call first):   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 537 in backend_compile   File ""/data00/home/xxx/jax/jax/_src/profiler.py"", line 206 in wrapper   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 583 in compile_or_get_cached   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 614 in from_xla_computation   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 529 in compile   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 170 in _xla_callable_uncached   File ""/data00/home/xxx/jax/jax/linear_util.py"", line 272 in memoized_fun   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 143 in _xla_call_impl   File ""/data00/home/xxx/jax/jax/core.py"", line 614 in process_call   File ""/data00/home/xxx/jax/jax/core.py"", line 1720 in call_bind   File ""/data00/home/xxx/jax/jax/core.py"", line 1708 in bind   File ""/data00/home/xxx/jax/jax/_src/api.py"", line 435 in cache_miss   File ""/data00/home/xxx/jax/jax/_src/traceback_util.py"", line 162 in reraise_with_filtered_traceback   File ""/data00/home/xxx/jax/jax/_src/test_util.py"", line 1072 in _CheckAgainstNumpy   File ""tests/lax_numpy_test.py"", line 589 in testOp   File ""/usr/lib/python3.7/contextlib.py"", line 74 in inner   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/parameterized.py"", line 314 in bound_param_test   File ""/usr/lib/python3.7/unittest/case.py"", line 615 in run   File ""/usr/lib/python3.7/unittest/case.py"", line 663 in __call__   File ""/usr/lib/python3.7/unittest/suite.py"", line 122 in run   File ""/usr/lib/python3.7/unittest/suite.py"", line 84 in __call__   File ""/usr/lib/python3.7/unittest/suite.py"", line 122 in run   File ""/usr/lib/python3.7/unittest/suite.py"", line 84 in __call__   File ""/usr/lib/python3.7/unittest/runner.py"", line 176 in run   File ""/home/yizhengjiao/.local/lib/python3.7/sitepackages/absl/testing/_pretty_print_reporter.py"", line 86 in run   File ""/usr/lib/python3.7/unittest/main.py"", line 271 in runTests   File ""/usr/lib/python3.7/unittest/main.py"", line 101 in __init__   File ""/home/yizhengjiao/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2569 in run_tests   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2164 in main_function   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/app.py"", line 258 in _run_main   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/app.py"", line 312 in run   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2166 in _run_in_app   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2049 in main   File ""tests/lax_numpy_test.py"", line 6317 in  Segmentation fault (core dumped))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,segmentation fault when runing unit test ,"Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```bach python3  tests/lax_numpy_test.py num_generated_cases=5 ```  [x] If applicable, include full error messages/tracebacks. [ RUN      ] LaxBackedNumpyTests.testOpIscomplex_bfloat16[3,4] Fatal Python error: Segmentation fault Current thread 0x00007f19ad5eb740 (most recent call first):   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 537 in backend_compile   File ""/data00/home/xxx/jax/jax/_src/profiler.py"", line 206 in wrapper   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 583 in compile_or_get_cached   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 614 in from_xla_computation   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 529 in compile   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 170 in _xla_callable_uncached   File ""/data00/home/xxx/jax/jax/linear_util.py"", line 272 in memoized_fun   File ""/data00/home/xxx/jax/jax/_src/dispatch.py"", line 143 in _xla_call_impl   File ""/data00/home/xxx/jax/jax/core.py"", line 614 in process_call   File ""/data00/home/xxx/jax/jax/core.py"", line 1720 in call_bind   File ""/data00/home/xxx/jax/jax/core.py"", line 1708 in bind   File ""/data00/home/xxx/jax/jax/_src/api.py"", line 435 in cache_miss   File ""/data00/home/xxx/jax/jax/_src/traceback_util.py"", line 162 in reraise_with_filtered_traceback   File ""/data00/home/xxx/jax/jax/_src/test_util.py"", line 1072 in _CheckAgainstNumpy   File ""tests/lax_numpy_test.py"", line 589 in testOp   File ""/usr/lib/python3.7/contextlib.py"", line 74 in inner   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/parameterized.py"", line 314 in bound_param_test   File ""/usr/lib/python3.7/unittest/case.py"", line 615 in run   File ""/usr/lib/python3.7/unittest/case.py"", line 663 in __call__   File ""/usr/lib/python3.7/unittest/suite.py"", line 122 in run   File ""/usr/lib/python3.7/unittest/suite.py"", line 84 in __call__   File ""/usr/lib/python3.7/unittest/suite.py"", line 122 in run   File ""/usr/lib/python3.7/unittest/suite.py"", line 84 in __call__   File ""/usr/lib/python3.7/unittest/runner.py"", line 176 in run   File ""/home/yizhengjiao/.local/lib/python3.7/sitepackages/absl/testing/_pretty_print_reporter.py"", line 86 in run   File ""/usr/lib/python3.7/unittest/main.py"", line 271 in runTests   File ""/usr/lib/python3.7/unittest/main.py"", line 101 in __init__   File ""/home/yizhengjiao/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2569 in run_tests   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2164 in main_function   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/app.py"", line 258 in _run_main   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/app.py"", line 312 in run   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2166 in _run_in_app   File ""/home/xxx/.local/lib/python3.7/sitepackages/absl/testing/absltest.py"", line 2049 in main   File ""tests/lax_numpy_test.py"", line 6317 in  Segmentation fault (core dumped)",2022-03-17T03:05:29Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/9929,"I am also seeing a segmentation fault in backend_compile using python 3.10, but only when running the code across multiple processes. This doesn't occur with Jax v0.2.28 and jaxlib v0.1.76, but does with the most recent release and the couple other versions I tried among the v0.3 releases.","I tried reproducing this at head but it works for me. Closing because I""m going to guess the bug is fixed. If you can still reproduce it, please reopen with instructions on how I can reproduce it!"
398,"以下是一个github上的jax下的一个issue, 标题是(revise internal helper _is_perturbed to _maybe_perturbed, fix logic)， 内容是 (kidger FYI (and feel free to leave a review!) fixes CC([Incorrect gradients] `_is_perturbed` produces the wrong value when used inside `lax.scan` etc.))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"revise internal helper _is_perturbed to _maybe_perturbed, fix logic",kidger FYI (and feel free to leave a review!) fixes CC([Incorrect gradients] `_is_perturbed` produces the wrong value when used inside `lax.scan` etc.),2022-03-16T22:48:06Z,kokoro:force-run pull ready,closed,0,3,https://github.com/jax-ml/jax/issues/9923,"As a fix for correctness this LGTM! Since JAX sees a whole program then in principle it should be able to do this precisely without heuristics though. Correctness is good but correctness and efficiency is better. I think I can see how this should be doable for all practical use cases. The fix is to adjust `custom_jvp` and `custom_vjp` and deprecate `closure_convert`. AFAIK it's only ever needed around these anyway. We clearly can't have a nonheuristic `closure_convert` that works in arbitrary contexts, as we need to defer binary classification until autodiff time and I don't think there's any way to encode things appropriately in the jaxprs before then. But since we only ever (a) use it around custom autodiff, and (b) only ever use the hoisted constants in one way (as args back into the provided function), then we can handle everything at once just by pushing the whole lot into the custom autodiff operations. Have a look at Equinox's `filter_custom_vjp`. This currently uses similar floatingpoint heuristics (on an explicit arg rather than an implicit arg but that's not important) as it's constrained by its use of `jac.custom_vjp`, but it demonstrates an API in which nondifferentiability/differentiablility is a detail that can be hidden from the end user. (Dw, I recognise this is a lot of work, and invites similar Equinoxfilteringstyle changes elsewhere, and that something like this is undoubtedly low priority. My commentary here is merely aspirational.)","Thanks for taking a look kidger ! > Since JAX sees a whole program then in principle it should be able to do this precisely without heuristics though. > We clearly can't have a nonheuristic closure_convert that works in arbitrary contexts, as we need to defer binary classification until autodiff time and I don't think there's any way to encode things appropriately in the jaxprs before then. I may not be following but aren't these two statements contradictions? (""should be able to do this precisely without heuristics"" and ""can't have a nonheuristic closure_convert"") What's ""this"" mean in the first sentence? Actually it's not quite true that JAX sees the whole program. Someone could always use `jax.make_jaxpr` to stage out a function, at which point the `_maybe_perturbed` function would be called, and later when interpreting that jaxpr differentiation may or may not be involved. For that reason we simply can't know for sure if differentiation is going to eventually be applied. (If we rule out the use of `jax.make_jaxpr` and jaxpr interpreters, and only think about e.g. higherorder primitives like `jax.lax.scan`, then indeed I believe we could have a complete and sound check.) > The fix is to adjust custom_jvp and custom_vjp and deprecate closure_convert. > Have a look at Equinox's filter_custom_vjp. Could you say more about the fix you have in mind, and/or provide some code links? I can't tell what you mean :) I looked at the docstring for `filter_custom_vjp` but since it also seems to work on heuristics (as you say) I don't see what the diff is.","> I may not be following but aren't these two statements contradictions? (""should be able to do this precisely without heuristics"" and ""can't have a nonheuristic closure_convert"") What's ""this"" mean in the first sentence? Right, sorry for being unclear. ""should be able to do this precisely without heuristics"" > 10,000 foot view that JAX should be able to figure out which arrays participate in autodiff, as it sees the whole program. [Yes it does! See my next point.] ""can't have a nonheuristic closure_convert"" is then digging into the implementation details of how that might happen; at this point I'm making the comment that `closure_convert` is a bad abstraction because what is or isn't participating in autodiff isn't known at the time that it is parsed. As you say: > Someone could always use jax.make_jaxpr to stage out a function, at which point the _maybe_perturbed function would be called, and later when interpreting that jaxpr differentiation may or may not be involved. which is precisely my point as well. `_maybe_perturbed` is being called at the wrong time! So I agree with you ;) > Could you say more about the fix you have in mind, and/or provide some code links? I can't tell what you mean :) >  > I looked at the docstring for filter_custom_vjp but since it also seems to work on heuristics (as you say) I don't see what the diff is. So the problem is that `_maybe_perturbed` is being called at the wrong time. It's being called at firsttracetime; it should be called at autodifftracetime. Given the ubiquity of `closure_convert` around custom autodiff, I'm proposing that filtering differentiable from nondifferentiable be something that's handled inside `jax.custom_jvp` and `jax.custom_vjp`, rather than having `closure_convert` as a standalone function. [Side note! `closure_convert` only filters differentiable from nondifferentiable for _implicit_ args  but around custom autodiff we need to the same thing for _explicit_ args as well. Once again `closure_convert` isn't quite the right abstraction; using just the facilities of JAX we'd have to define a new function closing over the explicit args and then `closure_convert` _that_ in order to filter all arguments to the function.] The implementation of `filter_custom_vjp` indeed has an implementation that depends on heuristics. I'm offering it just as an example of an API that doesn't have to depend on heuristics. Imagine `filter_custom_vjp` was actually a primitive that got preserved in the jaxpr across transformations (like `custom_vjp` already is). We could then use it to perform filtering at autodiff time rather than firsttracetime. By the way, notice how the word ""filter"" keeps coming up in the above discussion. Filtering is just a really useful thing to be doing around transformations, and so much so that Equinox is built around this idea. (Along with the parameterisedfunctionsaspytrees idea.)"
863,"以下是一个github上的jax下的一个issue, 标题是(add random.loggamma and improve dirichlet & beta implementation)， 内容是 (Fixes CC(Distribution of jax.random.dirichlet samples is incorrect for small values of the concentration parameters) This roughly follows the strategy of TFP, which computes the underlying gamma distribution in normal space and the boost offset in log space (see https://github.com/tensorflow/probability/blob/dc5b6b094cfc1f18f949e120c3491acb2e2af6eb/tensorflow_probability/python/distributions/gamma.pyL453L459). Tested by patchingin initial tests from CC(random: add values test cases for various distributions) to ensure that noncornercase gamma, beta, and dirichlet outputs remain unchanged by this reimplementation.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,add random.loggamma and improve dirichlet & beta implementation,"Fixes CC(Distribution of jax.random.dirichlet samples is incorrect for small values of the concentration parameters) This roughly follows the strategy of TFP, which computes the underlying gamma distribution in normal space and the boost offset in log space (see https://github.com/tensorflow/probability/blob/dc5b6b094cfc1f18f949e120c3491acb2e2af6eb/tensorflow_probability/python/distributions/gamma.pyL453L459). Tested by patchingin initial tests from CC(random: add values test cases for various distributions) to ensure that noncornercase gamma, beta, and dirichlet outputs remain unchanged by this reimplementation.",2022-03-16T00:19:08Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/9906,I did a bit of cleanup and improved the grad implementation to avoid infinities. PTAL!
5487,"以下是一个github上的jax下的一个issue, 标题是(tf.shape doesnt eval to ConcreteArray in conversion)， 内容是 (Tensorflow probability can be used in JAX. In the conversion if `tf.shape` is used in a JIT this is propagated in the tracing as a shaped array (`Tracedwith `) not materialised to a concrete array. I've placed the issue here, though I see it sits on the tfjax boundary. I don't know how the jax 'substrate' is working here, if it's using jax2tf. tensorflow probability is the nightly version: '0.17.0dev20220314', `pip install Uq tfpnightly[jax]` ```python from tensorflow_probability.python.internal.backend import jax as tf import tensorflow_probability as tfp; tfp = tfp.substrates.jax import numpy as np import jax tfd = tfp.distributions  Run 100 AIS chains in parallel num_chains = 100 dims = 20 dtype = np.float32 key = jax.random.PRNGKey(0) proposal = tfd.MultivariateNormalDiag(    loc=tf.zeros([dims], dtype=dtype)) target = tfd.TransformedDistribution(   distribution=tfd.Sample(       tfd.Gamma(concentration=dtype(2), rate=dtype(3)),       sample_shape=[dims]),   bijector=tfp.bijectors.Invert(tfp.bijectors.Exp()))  All good, not jitted chains_state, ais_weights, kernels_results = (     tfp.mcmc.sample_annealed_importance_chain(         num_steps=1000,         proposal_log_prob_fn=proposal.log_prob,         target_log_prob_fn=target.log_prob,         current_state=proposal.sample(num_chains, seed=key),         make_kernel_fn=lambda tlp_fn: tfp.mcmc.HamiltonianMonteCarlo(           target_log_prob_fn=tlp_fn,           step_size=0.2,           num_leapfrog_steps=2),         seed=key     ) ) from functools import partial  update = partial(     tfp.mcmc.sample_annealed_importance_chain,      num_steps=1000,     proposal_log_prob_fn=proposal.log_prob,     target_log_prob_fn=target.log_prob,     make_kernel_fn=lambda tlp_fn: tfp.mcmc.HamiltonianMonteCarlo(       target_log_prob_fn=tlp_fn,       step_size=0.2,       num_leapfrog_steps=2), )  Now update takes just current_state and seed current_state=proposal.sample(num_chains, seed=key), jax.jit(update)(seed=key, current_state=current_state) ``` Traceback: ```python  TypeError                                 Traceback (most recent call last)  in        1 current_state=proposal.sample(num_chains, seed=key), > 2 jax.jit(update)(seed=key, current_state=current_state)     [... skipping hidden 14 frame] /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/substrates/jax/mcmc/sample_annealed_importance.py in sample_annealed_importance_chain(num_steps, proposal_log_prob_fn, target_log_prob_fn, current_state, make_kernel_fn, parallel_iterations, seed, name)     271       )     272  > 273     previous_kernel_results = _bootstrap_results(current_state)     274     inner_results = previous_kernel_results.inner_results     275     mh_results = _find_inner_mh_results(inner_results) /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/substrates/jax/mcmc/sample_annealed_importance.py in _bootstrap_results(init_state)     260       dtype = dtype_util.as_numpy_dtype(convex_combined_log_prob.dtype)     261       shape = tf.shape(convex_combined_log_prob) > 262       proposal_log_prob = tf.fill(shape, dtype(np.nan),     263                                   name='bootstrap_proposal_log_prob')     264       target_log_prob = tf.fill(shape, dtype(np.nan), /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/python/internal/backend/jax/_utils.py in wrap(***failed resolving arguments***)      60   def wrap(wrapped, instance, args, kwargs):      61     del instance, wrapped > 62     return new_fn(*args, **kwargs)      63   return wrap(original_fn)   pylint: disable=novalueforparameter      64  /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/python/internal/backend/jax/numpy_array.py in (dims, value, name)     392 fill = utils.copy_docstring(     393     'tf.fill', > 394     lambda dims, value, name=None: np.full(dims, ops.convert_to_tensor(value)))     395      396 gather = utils.copy_docstring( /opt/conda/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py in full(shape, fill_value, dtype)    3696   if ndim(fill_value) == 0:    3697     shape = (shape,) if ndim(shape) == 0 else shape > 3698     return lax.full(shape, fill_value, dtype)    3699   else:    3700     return broadcast_to(asarray(fill_value, dtype=dtype), shape)     [... skipping hidden 1 frame] /opt/conda/lib/python3.8/sitepackages/jax/core.py in canonicalize_shape(shape, context)    1567   except TypeError:    1568     pass > 1569   raise _invalid_shape_error(shape, context)    1570     1571 def canonicalize_dim(d: DimSize, context: str="""") > DimSize: TypeError: Shapes must be 1D sequences of concrete values of integer type, got Tracedwith. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. ``` The relevant bit is  ```python /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/substrates/jax/mcmc/sample_annealed_importance.py in _bootstrap_results(init_state)     260       dtype = dtype_util.as_numpy_dtype(convex_combined_log_prob.dtype)     261       shape = tf.shape(convex_combined_log_prob) > 262       proposal_log_prob = tf.fill(shape, dtype(np.nan),     263                                   name='bootstrap_proposal_log_prob') ``` Line 261 contains the `tf.shape` which is still ShapedArray.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,tf.shape doesnt eval to ConcreteArray in conversion,"Tensorflow probability can be used in JAX. In the conversion if `tf.shape` is used in a JIT this is propagated in the tracing as a shaped array (`Tracedwith `) not materialised to a concrete array. I've placed the issue here, though I see it sits on the tfjax boundary. I don't know how the jax 'substrate' is working here, if it's using jax2tf. tensorflow probability is the nightly version: '0.17.0dev20220314', `pip install Uq tfpnightly[jax]` ```python from tensorflow_probability.python.internal.backend import jax as tf import tensorflow_probability as tfp; tfp = tfp.substrates.jax import numpy as np import jax tfd = tfp.distributions  Run 100 AIS chains in parallel num_chains = 100 dims = 20 dtype = np.float32 key = jax.random.PRNGKey(0) proposal = tfd.MultivariateNormalDiag(    loc=tf.zeros([dims], dtype=dtype)) target = tfd.TransformedDistribution(   distribution=tfd.Sample(       tfd.Gamma(concentration=dtype(2), rate=dtype(3)),       sample_shape=[dims]),   bijector=tfp.bijectors.Invert(tfp.bijectors.Exp()))  All good, not jitted chains_state, ais_weights, kernels_results = (     tfp.mcmc.sample_annealed_importance_chain(         num_steps=1000,         proposal_log_prob_fn=proposal.log_prob,         target_log_prob_fn=target.log_prob,         current_state=proposal.sample(num_chains, seed=key),         make_kernel_fn=lambda tlp_fn: tfp.mcmc.HamiltonianMonteCarlo(           target_log_prob_fn=tlp_fn,           step_size=0.2,           num_leapfrog_steps=2),         seed=key     ) ) from functools import partial  update = partial(     tfp.mcmc.sample_annealed_importance_chain,      num_steps=1000,     proposal_log_prob_fn=proposal.log_prob,     target_log_prob_fn=target.log_prob,     make_kernel_fn=lambda tlp_fn: tfp.mcmc.HamiltonianMonteCarlo(       target_log_prob_fn=tlp_fn,       step_size=0.2,       num_leapfrog_steps=2), )  Now update takes just current_state and seed current_state=proposal.sample(num_chains, seed=key), jax.jit(update)(seed=key, current_state=current_state) ``` Traceback: ```python  TypeError                                 Traceback (most recent call last)  in        1 current_state=proposal.sample(num_chains, seed=key), > 2 jax.jit(update)(seed=key, current_state=current_state)     [... skipping hidden 14 frame] /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/substrates/jax/mcmc/sample_annealed_importance.py in sample_annealed_importance_chain(num_steps, proposal_log_prob_fn, target_log_prob_fn, current_state, make_kernel_fn, parallel_iterations, seed, name)     271       )     272  > 273     previous_kernel_results = _bootstrap_results(current_state)     274     inner_results = previous_kernel_results.inner_results     275     mh_results = _find_inner_mh_results(inner_results) /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/substrates/jax/mcmc/sample_annealed_importance.py in _bootstrap_results(init_state)     260       dtype = dtype_util.as_numpy_dtype(convex_combined_log_prob.dtype)     261       shape = tf.shape(convex_combined_log_prob) > 262       proposal_log_prob = tf.fill(shape, dtype(np.nan),     263                                   name='bootstrap_proposal_log_prob')     264       target_log_prob = tf.fill(shape, dtype(np.nan), /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/python/internal/backend/jax/_utils.py in wrap(***failed resolving arguments***)      60   def wrap(wrapped, instance, args, kwargs):      61     del instance, wrapped > 62     return new_fn(*args, **kwargs)      63   return wrap(original_fn)   pylint: disable=novalueforparameter      64  /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/python/internal/backend/jax/numpy_array.py in (dims, value, name)     392 fill = utils.copy_docstring(     393     'tf.fill', > 394     lambda dims, value, name=None: np.full(dims, ops.convert_to_tensor(value)))     395      396 gather = utils.copy_docstring( /opt/conda/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py in full(shape, fill_value, dtype)    3696   if ndim(fill_value) == 0:    3697     shape = (shape,) if ndim(shape) == 0 else shape > 3698     return lax.full(shape, fill_value, dtype)    3699   else:    3700     return broadcast_to(asarray(fill_value, dtype=dtype), shape)     [... skipping hidden 1 frame] /opt/conda/lib/python3.8/sitepackages/jax/core.py in canonicalize_shape(shape, context)    1567   except TypeError:    1568     pass > 1569   raise _invalid_shape_error(shape, context)    1570     1571 def canonicalize_dim(d: DimSize, context: str="""") > DimSize: TypeError: Shapes must be 1D sequences of concrete values of integer type, got Tracedwith. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. ``` The relevant bit is  ```python /opt/conda/lib/python3.8/sitepackages/tensorflow_probability/substrates/jax/mcmc/sample_annealed_importance.py in _bootstrap_results(init_state)     260       dtype = dtype_util.as_numpy_dtype(convex_combined_log_prob.dtype)     261       shape = tf.shape(convex_combined_log_prob) > 262       proposal_log_prob = tf.fill(shape, dtype(np.nan),     263                                   name='bootstrap_proposal_log_prob') ``` Line 261 contains the `tf.shape` which is still ShapedArray.",2022-03-15T08:49:35Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/9882,"Looking through tensorflow probability, they use `tfp.internal.prefer_static.rank` and `tfp.internal.prefer_static.shape` in place of tf.shape to solve this issue. Much smaller replication ```python from tensorflow_probability.python.internal.backend import jax as tf import tensorflow_probability as tfp; tfp = tfp.substrates.jax from jax import jit, numpy as jnp ps = tfp.internal.prefer_static def tf_dynamic(x):     s = tf.shape(x)     return tf.zeros(s) def tf_static(x):     s = ps.shape(x)     return tf.zeros(s) x = jnp.arange(4) jit(tf_static)(x) jit(tf_dynamic)(x) ``` The static will work, the dynamic will not. Closing issue as this is a tensorflow_probability issue in syntax, not JAX."
2762,"以下是一个github上的jax下的一个issue, 标题是(FFT precision appears to ignore jax.default_matmul_precision setting)， 内容是 (For a university project I'm trying to implement a Mersenne prime search program using the TPU as a hardware accelerator. The algorithm we're using uses the FFT to square numbers mod the Mersenne prime candidate, and this calculation is quite prone to precision errors. We've discovered that `bfloat16` is too imprecise for the calculations we're trying to perform and thus will need to see if `bfloat16_3x` will give us the precision we need. In some JAX functions such as `jax.numpy.matmul()` I can manually specify a `precision` keyword argument to tell JAX to use a specific precision for the operation. This is apparently not an option with `jax.numpy.fft.fft()` so we have resulted to using the `jax.default_matmul_precision` config value to attempt to force the FFT to use the highest precision possible on the TPU. However, even with this flag set to `HIGHEST` we are still getting the exact same precision error whether `HIGHEST` is set or not. So my question is, how do we force the `jax.numpy.fft()` function to use `bfloat16_3x`/`tensorfloat32` for its internal calculations? Or is there a way to verify the internal precision of the calculations done on the TPU? If specifying the precision is not possible for the FFT, could this be clarified in the documentation so others are aware of the issue? Below is the code that will reproduce the issue, and a few sample precision errors. This code was run in Google Colab: ``` import jax import jax.numpy as jnp from jax import jit, lax, device_put from functools import partial import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu()  Tell JAX to use the highest possible precision by default jax.config.update(""jax_default_matmul_precision"", jax.lax.Precision.HIGHEST)  Example of squaring a number using the FFT x = jnp.array([6561, 0, 0, 0], dtype=jnp.float32) x = jnp.fft.fft(x) x = jnp.multiply(x, x)  On a system with zero precision error, x is an integer with no imaginary component x = jnp.real(jnp.fft.ifft(x))  We compute the difference between the actual result and the next nearest integer (this is our precision error) x_rounded = jnp.round(x) print(x  x_rounded) ```  TPU roundoff error, default precision: [ 0.0000000e+00  1.3183920e07  0.0000000e+00 1.8457621e08]  TPU roundoff error, highest precision: [ 0.0000000e+00  1.3183920e07  0.0000000e+00 1.8457621e08]   GPU roundoff error, default precision: [0. 0. 0. 0.] ( note the roundoff error is *exactly* the same for both the default and highest default precision setting on the TPU))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,FFT precision appears to ignore jax.default_matmul_precision setting,"For a university project I'm trying to implement a Mersenne prime search program using the TPU as a hardware accelerator. The algorithm we're using uses the FFT to square numbers mod the Mersenne prime candidate, and this calculation is quite prone to precision errors. We've discovered that `bfloat16` is too imprecise for the calculations we're trying to perform and thus will need to see if `bfloat16_3x` will give us the precision we need. In some JAX functions such as `jax.numpy.matmul()` I can manually specify a `precision` keyword argument to tell JAX to use a specific precision for the operation. This is apparently not an option with `jax.numpy.fft.fft()` so we have resulted to using the `jax.default_matmul_precision` config value to attempt to force the FFT to use the highest precision possible on the TPU. However, even with this flag set to `HIGHEST` we are still getting the exact same precision error whether `HIGHEST` is set or not. So my question is, how do we force the `jax.numpy.fft()` function to use `bfloat16_3x`/`tensorfloat32` for its internal calculations? Or is there a way to verify the internal precision of the calculations done on the TPU? If specifying the precision is not possible for the FFT, could this be clarified in the documentation so others are aware of the issue? Below is the code that will reproduce the issue, and a few sample precision errors. This code was run in Google Colab: ``` import jax import jax.numpy as jnp from jax import jit, lax, device_put from functools import partial import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu()  Tell JAX to use the highest possible precision by default jax.config.update(""jax_default_matmul_precision"", jax.lax.Precision.HIGHEST)  Example of squaring a number using the FFT x = jnp.array([6561, 0, 0, 0], dtype=jnp.float32) x = jnp.fft.fft(x) x = jnp.multiply(x, x)  On a system with zero precision error, x is an integer with no imaginary component x = jnp.real(jnp.fft.ifft(x))  We compute the difference between the actual result and the next nearest integer (this is our precision error) x_rounded = jnp.round(x) print(x  x_rounded) ```  TPU roundoff error, default precision: [ 0.0000000e+00  1.3183920e07  0.0000000e+00 1.8457621e08]  TPU roundoff error, highest precision: [ 0.0000000e+00  1.3183920e07  0.0000000e+00 1.8457621e08]   GPU roundoff error, default precision: [0. 0. 0. 0.] ( note the roundoff error is *exactly* the same for both the default and highest default precision setting on the TPU)",2022-03-15T01:37:06Z,question documentation,open,2,2,https://github.com/jax-ml/jax/issues/9881,"My understanding is that the JAX FFT implementation on TPU always uses HIGHEST precision matrix multiplication internally, which explains why you haven't found an option to customize it. It may not have the accuracy of GPU FFT implementations, but it also most likely uses different a numerical method to compute the FFT. If this isn't high enough precision, you may need to use CPU or GPU implementations for now, particularly if you need float64 precision.","I think we would probably be OK to add a clarifying note about precision support to jax.numpy.fft documentation, to clarify that matrixmultiplication precision flags do not affect the accuracy of these functions, even if they use matrixmultiplication internally. Though I hope that would also be your first guess, given we do not have any options for customizing precision."
1570,"以下是一个github上的jax下的一个issue, 标题是(Autodidax has some bugs involving reduce_sum)， 内容是 (This works in JAX: ```python import jax from jax import grad, vmap import jax.numpy as np def simple(a):   b = a   t = a + b   return t * b def f(a):   L0 = np.sum(simple(a))   return L0 def g(a):   dL0_da = vmap(grad(f), in_axes=0)(a)   L1 = np.sum(dL0_da * dL0_da)   return L1 key = jax.random.PRNGKey(0) print(grad(g)(jax.random.normal(key,(2,4)))) ``` However, the same code does not work in autodidax: ``` def simple(a):   b = a   t = a + b   return t * b def f(a):   L0 = reduce_sum(simple(a))   return L0 def g(a):   dL0_da = vmap(grad(f), in_axes=0)(a)   L1 = reduce_sum(dL0_da * dL0_da)   return L1 print(grad(g)(np.random.rand(2,4))) ``` gives ``` [](https://localhost:8080/) in batched_f(*args)      17     args_flat, in_tree = tree_flatten(args)      18     in_axes_flat, in_tree2 = tree_flatten(in_axes) > 19     if in_tree != in_tree2: raise TypeError      20     f_flat, out_tree = flatten_fun(f, in_tree)      21     outs_flat = vmap_flat(f_flat, in_axes_flat, *args_flat) TypeError: ``` Actually, I was thinking that it would fail because there doesn't appear to be any transpose rule for reduce_sum, but it doesn't seem that Autodidax got that far. What I was actually trying to do was see how Autodidax formulated the transpose rule for sum, because if you support vmap + arbitrary dims to reduce on it gets pretty complicated.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Autodidax has some bugs involving reduce_sum,"This works in JAX: ```python import jax from jax import grad, vmap import jax.numpy as np def simple(a):   b = a   t = a + b   return t * b def f(a):   L0 = np.sum(simple(a))   return L0 def g(a):   dL0_da = vmap(grad(f), in_axes=0)(a)   L1 = np.sum(dL0_da * dL0_da)   return L1 key = jax.random.PRNGKey(0) print(grad(g)(jax.random.normal(key,(2,4)))) ``` However, the same code does not work in autodidax: ``` def simple(a):   b = a   t = a + b   return t * b def f(a):   L0 = reduce_sum(simple(a))   return L0 def g(a):   dL0_da = vmap(grad(f), in_axes=0)(a)   L1 = reduce_sum(dL0_da * dL0_da)   return L1 print(grad(g)(np.random.rand(2,4))) ``` gives ``` [](https://localhost:8080/) in batched_f(*args)      17     args_flat, in_tree = tree_flatten(args)      18     in_axes_flat, in_tree2 = tree_flatten(in_axes) > 19     if in_tree != in_tree2: raise TypeError      20     f_flat, out_tree = flatten_fun(f, in_tree)      21     outs_flat = vmap_flat(f_flat, in_axes_flat, *args_flat) TypeError: ``` Actually, I was thinking that it would fail because there doesn't appear to be any transpose rule for reduce_sum, but it doesn't seem that Autodidax got that far. What I was actually trying to do was see how Autodidax formulated the transpose rule for sum, because if you support vmap + arbitrary dims to reduce on it gets pretty complicated.",2022-03-14T03:25:08Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/9865,Thanks for finding this! (And I'm glad you're looking at Autodidax!) I'll fix it. But in the meantime maybe take a look at JAX's transpose rule for `reduce_sum` if you haven't already. It's basically just a call to broadcast. What complexity do you have in mind?,Oh this is great! I think BroadcastInDim answers the question; in PyTorch we don't natively have this operator so we have to manually implement it with unsqueezes and expands.,"On the off chance it's useful, `broadcast_in_dim` can be implemented in a few lines of NumPy like this. We handled `np.sum` directly in the original Autograd this way with a helper.","The main problem with Autodidax here is just a bad error message. The assertion failure means that the `in_axes` specification doesn't match the provided argument tuple for the `vmap`decorated function. In this case, it's because Autodidax's `vmap` doesn't have `in_axes`/`out_axes` tree prefix support (though maybe I should add that...), and so `in_axes` needs to be a tuple corresponding to the argument tuple. That is, if we pass `in_axes=(0,)` then the issue goes away. Note that all the other calls to `vmap` in the file pass an `in_axes` which is a tuple rather than an integer. However, if we fix that it runs into a real bug!","After CC(improvements to autodidax reduce_sum) this works in Autodidax: ```python def simple(a):   b = a   t = a + b   return t * b def f(a):   L0 = reduce_sum(simple(a))   return L0 def g(a):   dL0_da = vmap(grad(f), in_axes=(0,))(a)   L1 = reduce_sum(dL0_da * dL0_da)   return L1 print(grad(g)(np.random.normal((2,4)))) ``` Autodidax doesn't have any tests though, so it probably has bugs.",Here's the transpose rule for `reduce_sum` in that commit.,Thanks a lot!
1570,"以下是一个github上的jax下的一个issue, 标题是(Will 64-bit data type support be added to the TPU driver API?)， 内容是 (Hi, When I try to create a jax array of type `float64` in Google Colab with the TPU runtime selected I get the following message: `RuntimeError: INVALID_ARGUMENT: 64bit data types are not yet supported on the TPU driver API. Convert inputs to float32/int32_t before using.` The verbiage here suggests this support may one day be added to the TPU driver API, so I'd like to know if this is something that's planned to be added in the future. If it is planned, is there an ETA on when this feature may be available? I opened an issue on the Tensorflow repo and was told this would be a more appropriate place to ask. For a university project I'm trying to implement a Mersenne prime number search program which runs on the TPU. These calculations are very sensitive to precision errors, and we've found that although we may be able to use the TPU to improve performance over existing GPUbased programs, the precision of `bfloat16` or even `float32` are too low for the calculations we're trying to run. running the code below in a Google Colab notebook with the TPU runtime selected should produce the error: ``` import jax import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() jax.config.update(""jax_enable_x64"", True) x = jax.numpy.array([1, 2, 3, 4], dtype=jax.numpy.float64)  Should throw the exception mentioned above ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Will 64-bit data type support be added to the TPU driver API?,"Hi, When I try to create a jax array of type `float64` in Google Colab with the TPU runtime selected I get the following message: `RuntimeError: INVALID_ARGUMENT: 64bit data types are not yet supported on the TPU driver API. Convert inputs to float32/int32_t before using.` The verbiage here suggests this support may one day be added to the TPU driver API, so I'd like to know if this is something that's planned to be added in the future. If it is planned, is there an ETA on when this feature may be available? I opened an issue on the Tensorflow repo and was told this would be a more appropriate place to ask. For a university project I'm trying to implement a Mersenne prime number search program which runs on the TPU. These calculations are very sensitive to precision errors, and we've found that although we may be able to use the TPU to improve performance over existing GPUbased programs, the precision of `bfloat16` or even `float32` are too low for the calculations we're trying to run. running the code below in a Google Colab notebook with the TPU runtime selected should produce the error: ``` import jax import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() jax.config.update(""jax_enable_x64"", True) x = jax.numpy.array([1, 2, 3, 4], dtype=jax.numpy.float64)  Should throw the exception mentioned above ```",2022-03-13T04:16:47Z,,closed,2,3,https://github.com/jax-ml/jax/issues/9862,"There are no immediate plans to add this support. The real fix will be to migrate Colab to the new Cloud TPU VM architecture (https://cloud.google.com/blog/products/compute/introducingcloudtpuvms), but we don't have a timeline for that yet. I suggest applying to TRC (https://sites.research.google/trc/about/), which can provide free Cloud TPU VM credits. You can also try out a Cloud TPU VM yourself of course, but then it's not free. Sorry for the inconvenience!","I'm gonna close this issue, but feel free to continue the discussion.","**EDIT**: I posted this as a new issue: https://github.com/google/jax/issues/10158 Hello . Thank you for the response. We ended up signing up and getting approved for the TRC program.  When I ran the same code above on a V3 TPU VM, it runs without error. However, when we add an `fft`, we get a huge nasty error trace: ElementTypeC128NotSupported.txt. The altered code looks like this: ``` import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True) x = jax.numpy.array([1, 2, 3, 4], dtype=jax.numpy.float64)  Should throw the exception mentioned above y = jnp.fft.fft(x) ```"
1522,"以下是一个github上的jax下的一个issue, 标题是(Using GPUs with Jax)， 内容是 ( Discussed in https://github.com/google/jax/discussions/9858  Originally posted by **akhilnadigatla** March 11, 2022 Hello! I am trying to get Jax running on my GPUs, but face these error messages: ``` I0311 16:52:36.027944 140563802497728 xla_bridge.py:247] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker:  20220311 16:52:36.058206: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected I0311 16:52:36.058603 140563802497728 xla_bridge.py:247] Unable to initialize backend 'gpu': FAILED_PRECONDITION: No visible GPU devices. I0311 16:52:36.059262 140563802497728 xla_bridge.py:247] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available. W0311 16:52:36.059466 140563802497728 xla_bridge.py:252] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` I have looked for solutions to the problems, and made sure all drivers/CUDA version/cuDNN version are as expected. I am working on Ubuntu 18.04 and the outputs of some key commands are: ``` >> nvidiasmi ++  grep jaxlib jaxlib==0.3.0+cuda11.cudnn82 ``` I am not sure what could be the issue here. Any help would be greatly appreciated! Thank you.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Using GPUs with Jax," Discussed in https://github.com/google/jax/discussions/9858  Originally posted by **akhilnadigatla** March 11, 2022 Hello! I am trying to get Jax running on my GPUs, but face these error messages: ``` I0311 16:52:36.027944 140563802497728 xla_bridge.py:247] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker:  20220311 16:52:36.058206: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected I0311 16:52:36.058603 140563802497728 xla_bridge.py:247] Unable to initialize backend 'gpu': FAILED_PRECONDITION: No visible GPU devices. I0311 16:52:36.059262 140563802497728 xla_bridge.py:247] Unable to initialize backend 'tpu': INVALID_ARGUMENT: TpuPlatform is not available. W0311 16:52:36.059466 140563802497728 xla_bridge.py:252] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` I have looked for solutions to the problems, and made sure all drivers/CUDA version/cuDNN version are as expected. I am working on Ubuntu 18.04 and the outputs of some key commands are: ``` >> nvidiasmi ++  grep jaxlib jaxlib==0.3.0+cuda11.cudnn82 ``` I am not sure what could be the issue here. Any help would be greatly appreciated! Thank you.",2022-03-11T22:12:41Z,P2 (eventual) needs info NVIDIA GPU,closed,0,9,https://github.com/jax-ml/jax/issues/9859,"Hi! I am also having this exact same problem, any progress on solutions to this?","Unfortunately, no. :(","It seems there is a problem with tensorflow regarding the way it looks to your gpu, which version of tensorflow do you have and do you get a problem when only using tensorflow and not jax? ","I have `tensorflow` version 2.8.0, and I do not have any problems running workloads on TensorFlow (which does seem to recognize my GPUs).  Running the following returns `Num GPUs Available: 2` as expected: ``` import tensorflow as tf print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU'))) ```",Was this issue resolved?  ,"Hi, I'm running into the same issue. Are there any solutions to resolve this?",I'm closing this issue because we don't have a way to reproduce it and it relates to an old jax version. Please reopen it (or file a new issue) if you can still reproduce the problem with an up to date `jax`.,I still have similar error when my machine (ubuntu22+cuda12.0+jax0.4.16) wakes up after sleep.  ``` I0000 00:00:1695473905.493231    7204 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.                                                                                                                                                                                    20230923 21:58:25.538852: W external/xla/xla/service/platform_util.cc:198] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN: unknown error  I0923 21:58:25.539228 139973430083584 xla_bridge.py:513] Unable to initialize backend 'cuda': INTERNAL: no supported devices found for platform CUDA                                                                                                                         ```, Hi Peter We are trying to run fit_all.sh https://github.com/martiningram/mcmc_runtime_comparison?tab=readmeovfile  [ on an ubuntu 20.04 GPU instance that has single GPU device; the execution throws the same error as in the original description ``` PyMC JAX CPU vectorized WARNING (pytensor.tensor.blas): Using NumPy CAPI based implementation for BLAS functions. Compiling... 20240107 20:42:07.355217: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected CUDA backend failed to initialize: FAILED_PRECONDITION: No visible GPU devices. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` if I just retrieve from jax about gpu it would list one gpu[0] do you need a new issue created in ordder to render a helping hand? Thank you
2017,"以下是一个github上的jax下的一个issue, 标题是(Cannot vmap variadic lax.reduce_window  )， 内容是 ( Discussed in https://github.com/google/jax/discussions/9818    Minimal code to reproduce ```python import jax import jax.numpy as jnp from jax import lax def index_maxpool(x):     def g(a, b):         an, ai = a         bn, bi = b         which = an >= bn         return (jnp.where(which, an, bn), jnp.where(which, ai, bi))     _, idxs = lax.reduce_window(         (x, jnp.arange(x.shape[0])),         (jnp.inf, 1),         g,         window_dimensions=(2,),         window_strides=(2,),         padding=((0, 0),),     )     return idxs index_maxpool(jnp.ones((10,))) jax.vmap(index_maxpool)(jnp.ones((10, 10))) ``` Error message ```  ValueError                                Traceback (most recent call last) /tmp/ipykernel_26237/2064479805.py in       22       23 index_maxpool(jnp.ones((10,))) > 24 jax.vmap(index_maxpool)(jnp.ones((10, 10)))     [... skipping hidden 3 frame] /tmp/ipykernel_26237/2064479805.py in index_maxpool(x)      11         return (jnp.where(which, an, bn), jnp.where(which, ai, bi))      12  > 13     _, idxs = lax.reduce_window(      14         (x, jnp.arange(x.shape[0])),      15         (jnp.inf, 1),     [... skipping hidden 4 frame] ~/base/lib/python3.9/sitepackages/jax/_src/lax/windowed_reductions.py in _generic_reduce_window_batch_rule(batched_args, batch_dims, jaxpr, consts, window_dimensions, window_strides, padding, base_dilation, window_dilation)     279   operand_bdims, init_value_bdims = util.split_list(batch_dims, [num_operands])     280  > 281   operand, init = batched_args     282   bdim, init_bdim = batch_dims     283   if any(init_bdim is not None for init_bdim in init_value_bdims): ValueError: too many values to unpack (expected 2) ``` Note: there is as well an issue when running that code on GPU (see  comment in the Discussion thread))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot vmap variadic lax.reduce_window  ," Discussed in https://github.com/google/jax/discussions/9818    Minimal code to reproduce ```python import jax import jax.numpy as jnp from jax import lax def index_maxpool(x):     def g(a, b):         an, ai = a         bn, bi = b         which = an >= bn         return (jnp.where(which, an, bn), jnp.where(which, ai, bi))     _, idxs = lax.reduce_window(         (x, jnp.arange(x.shape[0])),         (jnp.inf, 1),         g,         window_dimensions=(2,),         window_strides=(2,),         padding=((0, 0),),     )     return idxs index_maxpool(jnp.ones((10,))) jax.vmap(index_maxpool)(jnp.ones((10, 10))) ``` Error message ```  ValueError                                Traceback (most recent call last) /tmp/ipykernel_26237/2064479805.py in       22       23 index_maxpool(jnp.ones((10,))) > 24 jax.vmap(index_maxpool)(jnp.ones((10, 10)))     [... skipping hidden 3 frame] /tmp/ipykernel_26237/2064479805.py in index_maxpool(x)      11         return (jnp.where(which, an, bn), jnp.where(which, ai, bi))      12  > 13     _, idxs = lax.reduce_window(      14         (x, jnp.arange(x.shape[0])),      15         (jnp.inf, 1),     [... skipping hidden 4 frame] ~/base/lib/python3.9/sitepackages/jax/_src/lax/windowed_reductions.py in _generic_reduce_window_batch_rule(batched_args, batch_dims, jaxpr, consts, window_dimensions, window_strides, padding, base_dilation, window_dilation)     279   operand_bdims, init_value_bdims = util.split_list(batch_dims, [num_operands])     280  > 281   operand, init = batched_args     282   bdim, init_bdim = batch_dims     283   if any(init_bdim is not None for init_bdim in init_value_bdims): ValueError: too many values to unpack (expected 2) ``` Note: there is as well an issue when running that code on GPU (see  comment in the Discussion thread)",2022-03-10T18:52:36Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/9837,"This might be an easy one! I think Peter basically already solved this in his original commit, except these two lines were accidentally left in. That plus not adding tests for this case :P"
1321,"以下是一个github上的jax下的一个issue, 标题是(lstsq/svd always uses the full quadratic memory: should sometimes use `cuSolverDnSgesvdp` or similar)， 内容是 (Using lstsq on moderately large inputs runs out of memory allocating a buffer of size `m^2` For instance: ```python import numpy as np import jax import jax.numpy as jnp x = np.arange(100_000_000, dtype=np.float32)[:, None] y = x * 3 print(x.shape) print(np.linalg.lstsq(x, y, rcond=None))   totally fine print(jax.jit(lambda x, y: jnp.linalg.lstsq(x, y, rcond=None), backend=""cpu"")(x, y))   wrong? but doesn't OOM print(jnp.linalg.lstsq(x, y, rcond=None)[0])   OOM ``` This comes down to the svd implementation used for the gpu backend (always `cuSolverDnSgesvd`): ```python print(jnp.linalg.svd(x, full_matrices=False))   OOM ``` When `full_matrices=False` and `x` is far from square svd can use much less memory via `cuSolverDnSgesvdp` (or maybe some other cuSolver function). It might also be nice to support low rank svd via `cuSolverDnSgesvdr` like `torch.svd_lowrank`. Additionally, it looks like the cpu implementation is broken? It should get a coefficient of 3 in the example above, but it actually returns 0 with a large residual.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,lstsq/svd always uses the full quadratic memory: should sometimes use `cuSolverDnSgesvdp` or similar,"Using lstsq on moderately large inputs runs out of memory allocating a buffer of size `m^2` For instance: ```python import numpy as np import jax import jax.numpy as jnp x = np.arange(100_000_000, dtype=np.float32)[:, None] y = x * 3 print(x.shape) print(np.linalg.lstsq(x, y, rcond=None))   totally fine print(jax.jit(lambda x, y: jnp.linalg.lstsq(x, y, rcond=None), backend=""cpu"")(x, y))   wrong? but doesn't OOM print(jnp.linalg.lstsq(x, y, rcond=None)[0])   OOM ``` This comes down to the svd implementation used for the gpu backend (always `cuSolverDnSgesvd`): ```python print(jnp.linalg.svd(x, full_matrices=False))   OOM ``` When `full_matrices=False` and `x` is far from square svd can use much less memory via `cuSolverDnSgesvdp` (or maybe some other cuSolver function). It might also be nice to support low rank svd via `cuSolverDnSgesvdr` like `torch.svd_lowrank`. Additionally, it looks like the cpu implementation is broken? It should get a coefficient of 3 in the example above, but it actually returns 0 with a large residual.",2022-03-08T00:12:13Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/9793,"The `full_matrices` issue on GPU is already fixed (https://github.com/google/jax/pull/9760), although the fix isn't yet released as a wheel, so to get it you'd have to build from source. I'll look at the CPU issue.","I think the CPU issue stems from low precision for matrix multiplications with very large contracting dimensions on CPU. The workaround for now would be to enable x64 mode and use 64bit precision. ``` In [45]: z = jnp.ones((100000000, 1), jnp.float32) In [46]: z.T  @ z Out[46]: DeviceArray([[16777216.]], dtype=float32) In [47]: z = jnp.ones((100000000, 1), jnp.float64) In [48]: z.T  @ z Out[48]: DeviceArray([[1.e+08]], dtype=float64) ```","> The `full_matrices` issue on GPU is already fixed ( CC([JAX:GPU] Implement the full_matrices=False case of SVD without generating the full matrices and then slicing.)), although the fix isn't yet released as a wheel, so to get it you'd have to build from source. >  > I'll look at the CPU issue. Nice. I wonder how I missed this issue when searching."
2300,"以下是一个github上的jax下的一个issue, 标题是(Unable to build jax (tried 0.3.1, 0.2.25) on macos for Rosetta)， 内容是 (I am trying to get a build of jax for rosetta (m1 machine, build target is x86_64 tho).  My build command is:  `python build/build.py target_cpu darwin_x86_64 target_cpu_features default` The issue I run into is that tensorflow fails to link:  ``` [8,729 / 8,798] Compiling tensorflow/core/util/batch_util.cc; 20s local ... (10 actions, 9 running) ERROR: /private/var/tmp/_bazel_bschindler/9c7d3098c08d90fe723b02b20453f221/external/org_tensorflow/tensorflow/compiler/xla/python/BUILD:595:17: Linking external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so failed: (Aborted): cc_wrapper.sh failed: error executing command   (cd /private/var/tmp/_bazel_bschindler/9c7d3098c08d90fe723b02b20453f221/execroot/__main__ && \   exec env  \     APPLE_SDK_PLATFORM=MacOSX \     APPLE_SDK_VERSION_OVERRIDE=12.0 \     PATH=/Users/bschindler/miniconda/envs/libjax/bin:/Users/bschindler/.poetry/bin:/Users/bschindler/miniconda/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/appleinternal/bin:/opt/apple/bin:/usr/appleinternal/bin:/private/var/tanuki:/opt/xbs/bin:/Library/Apple/usr/bin \     XCODE_VERSION_OVERRIDE=13.2.1.13C100 \     ZERO_AR_DATE=1 \   external/local_config_cc/cc_wrapper.sh out/darwin_x86_64opt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so2.params)  Configuration: de32e4168323526e174466a8d8ec691ed3189a28eac420185008d52922cbe388  Execution platform: //:platform ld: unknown option: versionscript clang: error: linker command failed with exit code 1 (use v to see invocation) Error in child process '/usr/bin/xcrun'. 1 external/local_config_cc/cc_wrapper.sh: line 69: 12817 Abort trap: 6           ""$(/usr/bin/dirname ""$0"")""/wrapped_clang ""$@"" Target //build:build_wheel failed to build INFO: Elapsed time: 1804.154s, Critical Path: 136.21s INFO: 8798 processes: 1776 internal, 7022 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully ``` Any idea what is going wrong here? )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Unable to build jax (tried 0.3.1, 0.2.25) on macos for Rosetta","I am trying to get a build of jax for rosetta (m1 machine, build target is x86_64 tho).  My build command is:  `python build/build.py target_cpu darwin_x86_64 target_cpu_features default` The issue I run into is that tensorflow fails to link:  ``` [8,729 / 8,798] Compiling tensorflow/core/util/batch_util.cc; 20s local ... (10 actions, 9 running) ERROR: /private/var/tmp/_bazel_bschindler/9c7d3098c08d90fe723b02b20453f221/external/org_tensorflow/tensorflow/compiler/xla/python/BUILD:595:17: Linking external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so failed: (Aborted): cc_wrapper.sh failed: error executing command   (cd /private/var/tmp/_bazel_bschindler/9c7d3098c08d90fe723b02b20453f221/execroot/__main__ && \   exec env  \     APPLE_SDK_PLATFORM=MacOSX \     APPLE_SDK_VERSION_OVERRIDE=12.0 \     PATH=/Users/bschindler/miniconda/envs/libjax/bin:/Users/bschindler/.poetry/bin:/Users/bschindler/miniconda/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/appleinternal/bin:/opt/apple/bin:/usr/appleinternal/bin:/private/var/tanuki:/opt/xbs/bin:/Library/Apple/usr/bin \     XCODE_VERSION_OVERRIDE=13.2.1.13C100 \     ZERO_AR_DATE=1 \   external/local_config_cc/cc_wrapper.sh out/darwin_x86_64opt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so2.params)  Configuration: de32e4168323526e174466a8d8ec691ed3189a28eac420185008d52922cbe388  Execution platform: //:platform ld: unknown option: versionscript clang: error: linker command failed with exit code 1 (use v to see invocation) Error in child process '/usr/bin/xcrun'. 1 external/local_config_cc/cc_wrapper.sh: line 69: 12817 Abort trap: 6           ""$(/usr/bin/dirname ""$0"")""/wrapped_clang ""$@"" Target //build:build_wheel failed to build INFO: Elapsed time: 1804.154s, Critical Path: 136.21s INFO: 8798 processes: 1776 internal, 7022 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully ``` Any idea what is going wrong here? ",2022-03-04T16:01:53Z,bug contributions welcome,closed,0,2,https://github.com/jax-ml/jax/issues/9771,`version_script` is an option that makes sense on Linux but not Mac OS. I suspect something in the Bazel configuration has become confused by crosscompiling for x8664 from an ARM Mac. We only ever try crosscompiling the other direction ourselves. I don't have an ARM mac to debug this.,https://github.com/google/jax/pull/11922 pulls in https://github.com/tensorflow/tensorflow/commit/575daff8f02c7ac1b20faa02e02c287c02e443bc which should fix this.
809,"以下是一个github上的jax下的一个issue, 标题是(jax releases html page needs HTML doctype)， 内容是 (`pip` issued the following warning to me when installing from the GCS bucket: ``` warning: missingindexdoctype × The package index page being used does not have a proper HTML doctype declaration. ╰─> Problematic URL: https://storage.googleapis.com/jaxreleases/jax_releases.html note: This is an issue with the page at the URL mentioned above. hint: You might need to reach out to the owner of that package index, to get this fixed. See https://github.com/pypa/pip/issues/10825 for context. ``` It seems like there's little downside to adding a doctype, so we should probably just do that.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jax releases html page needs HTML doctype,"`pip` issued the following warning to me when installing from the GCS bucket: ``` warning: missingindexdoctype × The package index page being used does not have a proper HTML doctype declaration. ╰─> Problematic URL: https://storage.googleapis.com/jaxreleases/jax_releases.html note: This is an issue with the page at the URL mentioned above. hint: You might need to reach out to the owner of that package index, to get this fixed. See https://github.com/pypa/pip/issues/10825 for context. ``` It seems like there's little downside to adding a doctype, so we should probably just do that.",2022-03-04T15:47:47Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/9770,Is this similar to the issue reported in CC(Please provide PEP 503 compliant indices for CUDA versions of packages)?,"I think it's related to the issue in the last comment in that issue, but maybe not the same as the entire issue? This appears to exhibit on the latest `pip`, btw, so try `pip upgrade pip`.",This should now be fixed!,I am going to close this. Feel free to open if its not fixed.,"(It won't be fixed until we regenerate the index, but that can wait for the next release.)",I regenerated the index. We don't need to wait for the next release.
4907,"以下是一个github上的jax下的一个issue, 标题是(Multiple GPU devices broken?)， 内容是 (The following code was working last time I have checked, but not now using the pip wheels This is on an Ubuntu workstation with 4 A100 GPUs, using python 3.9, cuda 11.2 (I have tried many different 11.X CUDA versions, same error), driver version  `495.29.05` ``` $ pip install jax[cuda11_cudnn82] f https://storage.googleapis.com/jaxreleases/jax_releases.html .... $ cat test.py  import jax import cupy as cp import jax.numpy as jnp import numpy as np for d in range(cp.cuda.runtime.getDeviceCount()):   cp.cuda.runtime.setDevice(d)   a = np.random.rand(4)   x = cp.array(a)   print(a,x,x.device) for d in jax.devices():   a = np.random.rand(4)   x = jax.device_put(jnp.array(a),d)   print(a,x,d) $ python test.py  [0.36026366 0.49166635 0.02453196 0.06113282] [0.36026366 0.49166635 0.02453196 0.06113282]  [0.58960801 0.87522834 0.06568807 0.91739894] [0.58960801 0.87522834 0.06568807 0.91739894]  [0.62792459 0.31616489 0.75599587 0.13468365] [0.62792459 0.31616489 0.75599587 0.13468365]  [0.63147776 0.40358606 0.82040093 0.95835464] [0.63147776 0.40358606 0.82040093 0.95835464]  [0.64609584 0.90184385 0.18761828 0.34506156] [0.6460959  0.90184385 0.18761827 0.34506157] gpu:0 [0.89907119 0.94931636 0.24913002 0.63580519] [0. 0. 0. 0.] gpu:1 [0.08435405 0.37044366 0.45964628 0.7886945 ] [0.08435405 0.37044367 0.45964628 0.7886945 ] gpu:2 [0.83871242 0.30987635 0.31794099 0.64514956] [0.8387124  0.30987635 0.31794098 0.6451495 ] gpu:3 ``` Notice that `gpu:1` device array contains only zeros. Building from source using tag `jaxv0.2.9` works fine. Am I doing something wrong or is it a bug? ``` $ git log 1 commit 10947403202af72299bf19feba40d0d6f7ecb350 (HEAD, tag: jaxv0.2.9) Merge: a9e5fabe d2ae949b Author: jax authors  Date:   Tue Jan 26 20:47:13 2021 0800     Merge pull request CC(update version and changelog for pypi) from google:updatepypi     PiperOriginRevId: 354012572 $ pip install dist/jaxlib0.1.60cp39nonemanylinux2010_x86_64.whl  Processing ./dist/jaxlib0.1.60cp39nonemanylinux2010_x86_64.whl Requirement already satisfied: numpy>=1.12 in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jaxlib==0.1.60) (1.22.2) Requirement already satisfied: flatbuffers in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jaxlib==0.1.60) (2.0) Requirement already satisfied: scipy in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jaxlib==0.1.60) (1.8.0) Requirement already satisfied: abslpy in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jaxlib==0.1.60) (1.0.0) Requirement already satisfied: six in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from abslpy>jaxlib==0.1.60) (1.16.0) Installing collected packages: jaxlib   Attempting uninstall: jaxlib     Found existing installation: jaxlib 0.3.0+cuda11.cudnn82     Uninstalling jaxlib0.3.0+cuda11.cudnn82:       Successfully uninstalled jaxlib0.3.0+cuda11.cudnn82 Successfully installed jaxlib0.1.60 $ pip install e . Obtaining file:///home/zampins/sandbox/jax   Preparing metadata (setup.py) ... done Requirement already satisfied: numpy>=1.12 in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jax==0.2.9) (1.22.2) Requirement already satisfied: abslpy in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jax==0.2.9) (1.0.0) Requirement already satisfied: opt_einsum in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jax==0.2.9) (3.3.0) Requirement already satisfied: six in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from abslpy>jax==0.2.9) (1.16.0) Installing collected packages: jax   Attempting uninstall: jax     Found existing installation: jax 0.3.1     Uninstalling jax0.3.1:       Successfully uninstalled jax0.3.1   Running setup.py develop for jax Successfully installed jax0.2.9 $ python test.py  [0.72413586 0.03981569 0.82808771 0.08488717] [0.72413586 0.03981569 0.82808771 0.08488717]  [0.20310118 0.20229541 0.17232789 0.33406987] [0.20310118 0.20229541 0.17232789 0.33406987]  [0.38727162 0.83158815 0.74289441 0.16270889] [0.38727162 0.83158815 0.74289441 0.16270889]  [0.61987612 0.10876816 0.33720292 0.88087605] [0.61987612 0.10876816 0.33720292 0.88087605]  [0.22083954 0.78400095 0.12021694 0.49609661] [0.22083955 0.78400093 0.12021694 0.4960966 ] gpu:0 [0.32103459 0.01537868 0.96763227 0.10868705] [0.32103458 0.01537868 0.9676323  0.10868705] gpu:1 [0.3957342  0.86166002 0.87006812 0.80167636] [0.3957342  0.86166    0.87006813 0.8016764 ] gpu:2 [0.72442095 0.83735143 0.3064454  0.69193466] [0.72442096 0.83735144 0.3064454  0.69193465] gpu:3 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Multiple GPU devices broken?,"The following code was working last time I have checked, but not now using the pip wheels This is on an Ubuntu workstation with 4 A100 GPUs, using python 3.9, cuda 11.2 (I have tried many different 11.X CUDA versions, same error), driver version  `495.29.05` ``` $ pip install jax[cuda11_cudnn82] f https://storage.googleapis.com/jaxreleases/jax_releases.html .... $ cat test.py  import jax import cupy as cp import jax.numpy as jnp import numpy as np for d in range(cp.cuda.runtime.getDeviceCount()):   cp.cuda.runtime.setDevice(d)   a = np.random.rand(4)   x = cp.array(a)   print(a,x,x.device) for d in jax.devices():   a = np.random.rand(4)   x = jax.device_put(jnp.array(a),d)   print(a,x,d) $ python test.py  [0.36026366 0.49166635 0.02453196 0.06113282] [0.36026366 0.49166635 0.02453196 0.06113282]  [0.58960801 0.87522834 0.06568807 0.91739894] [0.58960801 0.87522834 0.06568807 0.91739894]  [0.62792459 0.31616489 0.75599587 0.13468365] [0.62792459 0.31616489 0.75599587 0.13468365]  [0.63147776 0.40358606 0.82040093 0.95835464] [0.63147776 0.40358606 0.82040093 0.95835464]  [0.64609584 0.90184385 0.18761828 0.34506156] [0.6460959  0.90184385 0.18761827 0.34506157] gpu:0 [0.89907119 0.94931636 0.24913002 0.63580519] [0. 0. 0. 0.] gpu:1 [0.08435405 0.37044366 0.45964628 0.7886945 ] [0.08435405 0.37044367 0.45964628 0.7886945 ] gpu:2 [0.83871242 0.30987635 0.31794099 0.64514956] [0.8387124  0.30987635 0.31794098 0.6451495 ] gpu:3 ``` Notice that `gpu:1` device array contains only zeros. Building from source using tag `jaxv0.2.9` works fine. Am I doing something wrong or is it a bug? ``` $ git log 1 commit 10947403202af72299bf19feba40d0d6f7ecb350 (HEAD, tag: jaxv0.2.9) Merge: a9e5fabe d2ae949b Author: jax authors  Date:   Tue Jan 26 20:47:13 2021 0800     Merge pull request CC(update version and changelog for pypi) from google:updatepypi     PiperOriginRevId: 354012572 $ pip install dist/jaxlib0.1.60cp39nonemanylinux2010_x86_64.whl  Processing ./dist/jaxlib0.1.60cp39nonemanylinux2010_x86_64.whl Requirement already satisfied: numpy>=1.12 in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jaxlib==0.1.60) (1.22.2) Requirement already satisfied: flatbuffers in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jaxlib==0.1.60) (2.0) Requirement already satisfied: scipy in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jaxlib==0.1.60) (1.8.0) Requirement already satisfied: abslpy in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jaxlib==0.1.60) (1.0.0) Requirement already satisfied: six in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from abslpy>jaxlib==0.1.60) (1.16.0) Installing collected packages: jaxlib   Attempting uninstall: jaxlib     Found existing installation: jaxlib 0.3.0+cuda11.cudnn82     Uninstalling jaxlib0.3.0+cuda11.cudnn82:       Successfully uninstalled jaxlib0.3.0+cuda11.cudnn82 Successfully installed jaxlib0.1.60 $ pip install e . Obtaining file:///home/zampins/sandbox/jax   Preparing metadata (setup.py) ... done Requirement already satisfied: numpy>=1.12 in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jax==0.2.9) (1.22.2) Requirement already satisfied: abslpy in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jax==0.2.9) (1.0.0) Requirement already satisfied: opt_einsum in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from jax==0.2.9) (3.3.0) Requirement already satisfied: six in /home/zampins/Devel/miniforge/envs/testjax/lib/python3.9/sitepackages (from abslpy>jax==0.2.9) (1.16.0) Installing collected packages: jax   Attempting uninstall: jax     Found existing installation: jax 0.3.1     Uninstalling jax0.3.1:       Successfully uninstalled jax0.3.1   Running setup.py develop for jax Successfully installed jax0.2.9 $ python test.py  [0.72413586 0.03981569 0.82808771 0.08488717] [0.72413586 0.03981569 0.82808771 0.08488717]  [0.20310118 0.20229541 0.17232789 0.33406987] [0.20310118 0.20229541 0.17232789 0.33406987]  [0.38727162 0.83158815 0.74289441 0.16270889] [0.38727162 0.83158815 0.74289441 0.16270889]  [0.61987612 0.10876816 0.33720292 0.88087605] [0.61987612 0.10876816 0.33720292 0.88087605]  [0.22083954 0.78400095 0.12021694 0.49609661] [0.22083955 0.78400093 0.12021694 0.4960966 ] gpu:0 [0.32103459 0.01537868 0.96763227 0.10868705] [0.32103458 0.01537868 0.9676323  0.10868705] gpu:1 [0.3957342  0.86166002 0.87006812 0.80167636] [0.3957342  0.86166    0.87006813 0.8016764 ] gpu:2 [0.72442095 0.83735143 0.3064454  0.69193466] [0.72442096 0.83735144 0.3064454  0.69193465] gpu:3 ```",2022-03-04T15:37:24Z,bug P1 (soon) needs info NVIDIA GPU,closed,0,5,https://github.com/jax-ml/jax/issues/9769,"I'm unable to reproduce this. I created a GCP VM with 4xA100 GPUs, installed CUDA toolkit 11.2 and driver 495.44. I installed Python 3.9 and packages `cupycuda112` v10.2.0, `jax` v0.3.1, and  `jaxlib` v0.3.0. I get: ``` [0.60967659 0.99469146 0.42833437 0.92369786] [0.60967659 0.99469146 0.42833437 0.92369786]  [0.90148377 0.2266339  0.09378169 0.68339479] [0.90148377 0.2266339  0.09378169 0.68339479]  [0.08509242 0.02069798 0.56742285 0.62548422] [0.08509242 0.02069798 0.56742285 0.62548422]  [0.00875828 0.72183364 0.4777141  0.8376914 ] [0.00875828 0.72183364 0.4777141  0.8376914 ]  [0.82617048 0.02544475 0.54643491 0.67557591] [0.8261705  0.02544475 0.54643494 0.6755759 ] gpu:0 [0.41064849 0.76792539 0.2629806  0.62022472] [0.4106485 0.7679254 0.2629806 0.6202247] gpu:1 [0.59699067 0.56032479 0.23362261 0.47442987] [0.59699064 0.5603248  0.23362261 0.47442988] gpu:2 [0.77527778 0.36935768 0.09267553 0.23960158] [0.7752778  0.36935768 0.09267553 0.23960158] gpu:3 ``` Without a way to reproduce the problem, I'm not going to be able to debug it. Are you by any chance able to give complete instructions to reproduce it in a cloud VM of some sort? That way, I could follow them to reproduce. Note v0.2.9 of `jax` is pretty old, there are dozens of releases between 0.2.9 and 0.3.0. (The third digit is not a decimal value).",Can you share the output of `nvidiasmi topo m` ?,"``` $ nvidiasmi topo m 	GPU0	GPU1	GPU2	GPU3	mlx5_0	mlx5_1	mlx5_2	mlx5_3	CPU Affinity	NUMA Affinity GPU0	 X 	PXB	SYS	SYS	SYS	SYS	SYS	SYS	027	0 GPU1	PXB	 X 	SYS	SYS	SYS	SYS	SYS	SYS	027	0 GPU2	SYS	SYS	 X 	PXB	PXB	PXB	PXB	PXB	2855	1 GPU3	SYS	SYS	PXB	 X 	PXB	PXB	PXB	PXB	2855	1 mlx5_0	SYS	SYS	PXB	PXB	 X 	PIX	PIX	PIX		 mlx5_1	SYS	SYS	PXB	PXB	PIX	 X 	PIX	PIX		 mlx5_2	SYS	SYS	PXB	PXB	PIX	PIX	 X 	PIX		 mlx5_3	SYS	SYS	PXB	PXB	PIX	PIX	PIX	 X 		 Legend:   X    = Self   SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)   NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node   PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)   PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)   PIX  = Connection traversing at most a single PCIe bridge   NV  = Connection traversing a bonded set of  NVLinks ```",Was this issue resolved?  ,"Closing since no activity, feel free to open again with more info!  Edit: For such issues, it's a good idea to check the correctness of GPUs communication with nccltests"
10066,"以下是一个github上的jax下的一个issue, 标题是(TPU hang (when loading data from an external process))， 内容是 (Possibly related to  but this occurs even without using pytorch dataloaders or fork(). Minimal reproduction in . This is a significantly reduced script from my actual training code which would constantly hang. The code hangs when trying to move the int32 scalar `fail` tensor from TPU to cpu, `fail = fail.item()` at line 89 in `bug_repro.py`. The stack trace from gdb with debugging symbols is this: ``` CC(未找到相关数据)  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38 CC(Python 3 compatibility issues)  0x00007ffb0e77c08e in absl::lts_20210324::synchronization_internal::Waiter::Wait(absl::lts_20210324::synchronization_internal::KernelTimeout) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Explicit tuples are not valid function parameters in Python 3)  0x00007ffb0e77bfa2 in AbslInternalPerThreadSemWait_lts_20210324 () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from ..core import JaxTuple)  0x00007ffb0e77a59d in absl::lts_20210324::Mutex::Block(absl::lts_20210324::base_internal::PerThreadSynch*) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from six.moves import xrange)  0x00007ffb0e77b759 in absl::lts_20210324::Mutex::LockSlowWithDeadline(absl::lts_20210324::MuHowS const*, absl::lts_20210324::Condition const*, absl::lts_20210324::synchronization_internal::KernelTimeout, int) ()    from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Building on OSX with CUDA)  0x00007ffb090ed6f8 in absl::lts_20210324::Mutex::LockSlow(absl::lts_20210324::MuHowS const*, absl::lts_20210324::Condition const*, int) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Made a shim to handle configuration without having absl parse flags)  0x00007ffb0e77bc33 in absl::lts_20210324::Notification::WaitForNotification() const () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Quickish check)  0x00007ffb09465161 in xla::PyBuffer::AsNumPyArray(pybind11::handle) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Quickish check)  0x00007ffb09465859 in pybind11::cpp_function::initialize, xla::PyBuffer::pyobject>(xla::PyBuffer::RegisterTypes(pybind11::module_&)::{lambda(xla::PyBuffer::pyobject) CC(Add copyright notice to quickstart notebook.)}&&, tensorflow::StatusOr (*)(xla::PyBuffer::pyobject))::{lambda(pybind11::detail::function_call&) CC(Undefined name: from ..core import JaxTuple)}::_FUN(pybind11::detail::function_call) ()    from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Adding quickstart notebook, and corresponding gitignore rules)  0x00007ffb091b6250 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) 0x00000000005f5e79 in cfunction_call_varargs (kwargs=, args=, func=) at ../Objects/call.c:773 CC(Split out `jax` and `jaxlib` packages) PyCFunction_Call (func=, args=, kwargs=) at ../Objects/call.c:773 CC(Update the quickstart notebook.) 0x00000000005f6a46 in _PyObject_MakeTpCall (callable=, args=, nargs=, keywords=) at ../Include/internal/pycore_pyerrors.h:13 CC(Fixing logo size so resize is not required) 0x0000000000503337 in _PyObject_Vectorcall (nargsf=1, kwnames=0x0, args=0x7fff139b6208, callable=) at ../Include/object.h:439 CC(Add copyright notice to quickstart notebook.) _PyObject_FastCall (nargs=1, args=0x7fff139b6208, func=) at ../Include/cpython/abstract.h:147 CC(rename in_bdims, out_bdims > in_axes, out_axes) property_descr_get (self=, obj=, type=) at ../Objects/descrobject.c:1496 CC(Add wheelbuilding scripts) 0x00000000005c1ed7 in _PyObject_GenericGetAttrWithDict (suppress=0, dict=0x0, name='_value', obj=) at ../Objects/object.c:1254 CC(Implement np.repeat for scalar repeats.) PyObject_GenericGetAttr (name='_value', obj=) at ../Objects/object.c:1335 CC(Populate readme) PyObject_GetAttr (v=, name='_value') at ../Objects/object.c:941 CC(Notebook showing how to write gufuncs with vmap) 0x000000000056b32e in _PyEval_EvalFrameDefault (f=, throwflag=) at ../Python/ceval.c:2966 CC(Fix link in gufuncs notebook) 0x00000000005f6226 in PyEval_EvalFrameEx (throwflag=0, f=Frame 0x7ffaf5cd0520, for file /home/em/jax/jax/interpreters/xla.py, line 1626, in  (self=)) at ../Python/ceval.c:741 CC(Typo) function_code_fastcall (globals=, nargs=, args=, co=) at ../Objects/call.c:284 CC(differention > differentiation) _PyFunction_Vectorcall (func=, stack=, nargsf=, kwnames=) at ../Objects/call.c:411 CC(Typo, Python parens) 0x00000000005a9123 in _PyObject_Vectorcall (kwnames=0x0, nargsf=, args=0x7fff139b6450, callable=) at ../Include/cpython/abstract.h:127 CC(attempt to centerjustify the jax logo in readme) _PyObject_FastCall (nargs=, args=0x7fff139b6450, func=) at ../Include/cpython/abstract.h:147 CC(Barebones neural network and data loading example notebook) _PyObject_FastCall_Prepend (nargs=, args=, obj=, callable=) at ../Objects/call.c:850 CC(fix symbolic zero handling in concat transpose) call_unbound (nargs=, args=, self=, func=, unbound=1) at ../Objects/typeobject.c:1453 CC(Cloud TPU Support) call_method (obj=, name=, args=, nargs=) at ../Objects/typeobject.c:1485 CC(examples/datasets.py doesn’t work in python3) 0x00000000005e58d5 in _PyLong_FromNbInt (integral=) at ../Objects/longobject.c:165 CC(Add support for `np.trace` ) 0x00000000005eb29b in PyNumber_Long (o=) at ../Objects/abstract.c:1370 CC(Error on NaN?) long_new_impl (obase=, x=, type=0x90b400 ) at ../Objects/longobject.c:5119 CC(Bug in examples?) long_new (type=0x90b400 , args=, kwargs=) at ../Objects/clinic/longobject.c.h:36 CC(Fix the bug in classifier example, batching_test and README) 0x00000000005f6933 in type_call (kwds=0x0, args=(,), type=0x90b400 ) at ../Objects/typeobject.c:974 CC(Broadcasting of size0 dimensions not implemented) _PyObject_MakeTpCall (callable=, args=, nargs=, keywords=) at ../Objects/call.c:159 CC(minor spelling tweaks) 0x0000000000570612 in _PyObject_Vectorcall (kwnames=0x0, nargsf=, args=0x7ffaf5ccfbd8, callable=) at ../Include/cpython/abstract.h:125 CC(CUDA90 and py3 ) _PyObject_Vectorcall (kwnames=0x0, nargsf=, args=0x7ffaf5ccfbd8, callable=) at ../Include/cpython/abstract.h:115 CC(add dot_general batching rule) call_function (kwnames=0x0, oparg=, pp_stack=, tstate=0x2144000) at ../Python/ceval.c:4963 CC(np.einsum support) _PyEval_EvalFrameDefault (f=, throwflag=) at ../Python/ceval.c:3500 CC(Require protobuf 3.6.0 or later) 0x00000000005f6226 in PyEval_EvalFrameEx (throwflag=0, f=Frame 0x7ffaf5ccfa60, for file /home/em/jax/jax/interpreters/xla.py, line 1579, in item (self=)) at ../Python/ceval.c:741 CC(Hard crash when no compatible cuda devices found) function_code_fastcall (globals=, nargs=, args=, co=) at ../Objects/call.c:284 CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") _PyFunction_Vectorcall (func=, stack=, nargsf=, kwnames=) at ../Objects/call.c:411 CC(Fix support for arrays with size0 dimensions.) 0x000000000056b5e0 in _PyObject_Vectorcall (kwnames=0x0, nargsf=, args=0x7ff7faa665c0, callable=) at ../Include/cpython/abstract.h:127 CC(Set distinct_host_configuration=false in the bazel options.) call_function (kwnames=0x0, oparg=, pp_stack=, tstate=0x2144000) at ../Python/ceval.c:4963 CC(Open Source Contributions) _PyEval_EvalFrameDefault (f=, throwflag=) at ../Python/ceval.c:3486 CC(np.linalg.inv support) 0x00000000005009e3 in PyEval_EvalFrameEx (throwflag=0,     f=Frame 0x7ff7faa66420, for file bug_repro.py, line 89, in __iter__ (self=) at remote 0x7ff9b3f6d880>, size=None) at remote 0x7ff9552785e0>, num_workers=4, prefetch=16) at remote 0x7ff9b3f7c2b0>, size=180, maxsize=180) at remote 0x7ff9b3f7c370>, it=, count=0, item=, fail=))     at ../Python/ceval.c:741 CC(Feature request: export TF ops) gen_send_ex (closing=0, exc=0, arg=0x0, gen=0x7ff954ac4970) at ../Objects/genobject.c:222 CC(Update XLA and reenable numpy tests that failed on Mac) gen_iternext (gen=0x7ff954ac4970) at ../Objects/genobject.c:543 CC(jacrev and jacfwd usage example) 0x000000000056b7c9 in _PyEval_EvalFrameDefault (f=, throwflag=) at ../Python/ceval.c:3202 CC(Unimplemented: binary integer op 'power') 0x00000000005009e3 in PyEval_EvalFrameEx (throwflag=0,     f=Frame 0x88bb4fd0, for file /home/em/.pyenv/lib/python3.8/sitepackages/tqdm/std.py, line 1195, in __iter__ (self=) at remote 0x7ff9b3f6d880>, size=None) at remote 0x7ff9552785e0>, num_workers=4, prefetch=16) at remote 0x7ff9b3f7c2b0>, size=180, maxsize=180) at remote 0x7ff9b3f7c370>, desc='', total=180, leave=True, fp=, write=, flush=) at remote 0x7ffaedd03460>, ncols=119, nrows=61, mininterval=, maxinterval=, miniters=0, dynamic_miniters=True, ascii=False, disable=False, unit='it', unit_scale=False, unit_divisor=1000, initial=0, lock_args=None, delay=0, gui=False, dynamic_ncols=False, smoothing=, _ema_dn=, last=0, calls=0) at ...(truncated)) at ../Python/ceval.c:741 ``` This appears to be a regression: this code did not hang in jax 0.2.25 but does in 0.2.27. I git bisected the jax repo (using `bug_bisect.sh` from that gist) and the first bad commit appears to be: ``` 96623c3048351d4f43dee8d999fb440ef1a2c557 is the first bad commit commit 96623c3048351d4f43dee8d999fb440ef1a2c557 Author: Matthew Johnson  Date:   Wed Jul 22 12:10:43 2020 0700     make iter(DeviceArray) return DeviceArrays w/o sync  jax/_src/lax/lax.py          11 +++++++++++  6 files changed, 58 insertions(+), 18 deletions() ``` This looks very likely to be behind the problem somehow. As for why the hang only occurs when my script is loading data from an external process, it doesn't seem to be deterministic so possibly it is a timing thing.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,TPU hang (when loading data from an external process),"Possibly related to  but this occurs even without using pytorch dataloaders or fork(). Minimal reproduction in . This is a significantly reduced script from my actual training code which would constantly hang. The code hangs when trying to move the int32 scalar `fail` tensor from TPU to cpu, `fail = fail.item()` at line 89 in `bug_repro.py`. The stack trace from gdb with debugging symbols is this: ``` CC(未找到相关数据)  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38 CC(Python 3 compatibility issues)  0x00007ffb0e77c08e in absl::lts_20210324::synchronization_internal::Waiter::Wait(absl::lts_20210324::synchronization_internal::KernelTimeout) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Explicit tuples are not valid function parameters in Python 3)  0x00007ffb0e77bfa2 in AbslInternalPerThreadSemWait_lts_20210324 () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from ..core import JaxTuple)  0x00007ffb0e77a59d in absl::lts_20210324::Mutex::Block(absl::lts_20210324::base_internal::PerThreadSynch*) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from six.moves import xrange)  0x00007ffb0e77b759 in absl::lts_20210324::Mutex::LockSlowWithDeadline(absl::lts_20210324::MuHowS const*, absl::lts_20210324::Condition const*, absl::lts_20210324::synchronization_internal::KernelTimeout, int) ()    from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Building on OSX with CUDA)  0x00007ffb090ed6f8 in absl::lts_20210324::Mutex::LockSlow(absl::lts_20210324::MuHowS const*, absl::lts_20210324::Condition const*, int) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Made a shim to handle configuration without having absl parse flags)  0x00007ffb0e77bc33 in absl::lts_20210324::Notification::WaitForNotification() const () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Quickish check)  0x00007ffb09465161 in xla::PyBuffer::AsNumPyArray(pybind11::handle) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Quickish check)  0x00007ffb09465859 in pybind11::cpp_function::initialize, xla::PyBuffer::pyobject>(xla::PyBuffer::RegisterTypes(pybind11::module_&)::{lambda(xla::PyBuffer::pyobject) CC(Add copyright notice to quickstart notebook.)}&&, tensorflow::StatusOr (*)(xla::PyBuffer::pyobject))::{lambda(pybind11::detail::function_call&) CC(Undefined name: from ..core import JaxTuple)}::_FUN(pybind11::detail::function_call) ()    from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC(Adding quickstart notebook, and corresponding gitignore rules)  0x00007ffb091b6250 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /home/em/.pyenv/lib/python3.8/sitepackages/jaxlib/xla_extension.so CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) 0x00000000005f5e79 in cfunction_call_varargs (kwargs=, args=, func=) at ../Objects/call.c:773 CC(Split out `jax` and `jaxlib` packages) PyCFunction_Call (func=, args=, kwargs=) at ../Objects/call.c:773 CC(Update the quickstart notebook.) 0x00000000005f6a46 in _PyObject_MakeTpCall (callable=, args=, nargs=, keywords=) at ../Include/internal/pycore_pyerrors.h:13 CC(Fixing logo size so resize is not required) 0x0000000000503337 in _PyObject_Vectorcall (nargsf=1, kwnames=0x0, args=0x7fff139b6208, callable=) at ../Include/object.h:439 CC(Add copyright notice to quickstart notebook.) _PyObject_FastCall (nargs=1, args=0x7fff139b6208, func=) at ../Include/cpython/abstract.h:147 CC(rename in_bdims, out_bdims > in_axes, out_axes) property_descr_get (self=, obj=, type=) at ../Objects/descrobject.c:1496 CC(Add wheelbuilding scripts) 0x00000000005c1ed7 in _PyObject_GenericGetAttrWithDict (suppress=0, dict=0x0, name='_value', obj=) at ../Objects/object.c:1254 CC(Implement np.repeat for scalar repeats.) PyObject_GenericGetAttr (name='_value', obj=) at ../Objects/object.c:1335 CC(Populate readme) PyObject_GetAttr (v=, name='_value') at ../Objects/object.c:941 CC(Notebook showing how to write gufuncs with vmap) 0x000000000056b32e in _PyEval_EvalFrameDefault (f=, throwflag=) at ../Python/ceval.c:2966 CC(Fix link in gufuncs notebook) 0x00000000005f6226 in PyEval_EvalFrameEx (throwflag=0, f=Frame 0x7ffaf5cd0520, for file /home/em/jax/jax/interpreters/xla.py, line 1626, in  (self=)) at ../Python/ceval.c:741 CC(Typo) function_code_fastcall (globals=, nargs=, args=, co=) at ../Objects/call.c:284 CC(differention > differentiation) _PyFunction_Vectorcall (func=, stack=, nargsf=, kwnames=) at ../Objects/call.c:411 CC(Typo, Python parens) 0x00000000005a9123 in _PyObject_Vectorcall (kwnames=0x0, nargsf=, args=0x7fff139b6450, callable=) at ../Include/cpython/abstract.h:127 CC(attempt to centerjustify the jax logo in readme) _PyObject_FastCall (nargs=, args=0x7fff139b6450, func=) at ../Include/cpython/abstract.h:147 CC(Barebones neural network and data loading example notebook) _PyObject_FastCall_Prepend (nargs=, args=, obj=, callable=) at ../Objects/call.c:850 CC(fix symbolic zero handling in concat transpose) call_unbound (nargs=, args=, self=, func=, unbound=1) at ../Objects/typeobject.c:1453 CC(Cloud TPU Support) call_method (obj=, name=, args=, nargs=) at ../Objects/typeobject.c:1485 CC(examples/datasets.py doesn’t work in python3) 0x00000000005e58d5 in _PyLong_FromNbInt (integral=) at ../Objects/longobject.c:165 CC(Add support for `np.trace` ) 0x00000000005eb29b in PyNumber_Long (o=) at ../Objects/abstract.c:1370 CC(Error on NaN?) long_new_impl (obase=, x=, type=0x90b400 ) at ../Objects/longobject.c:5119 CC(Bug in examples?) long_new (type=0x90b400 , args=, kwargs=) at ../Objects/clinic/longobject.c.h:36 CC(Fix the bug in classifier example, batching_test and README) 0x00000000005f6933 in type_call (kwds=0x0, args=(,), type=0x90b400 ) at ../Objects/typeobject.c:974 CC(Broadcasting of size0 dimensions not implemented) _PyObject_MakeTpCall (callable=, args=, nargs=, keywords=) at ../Objects/call.c:159 CC(minor spelling tweaks) 0x0000000000570612 in _PyObject_Vectorcall (kwnames=0x0, nargsf=, args=0x7ffaf5ccfbd8, callable=) at ../Include/cpython/abstract.h:125 CC(CUDA90 and py3 ) _PyObject_Vectorcall (kwnames=0x0, nargsf=, args=0x7ffaf5ccfbd8, callable=) at ../Include/cpython/abstract.h:115 CC(add dot_general batching rule) call_function (kwnames=0x0, oparg=, pp_stack=, tstate=0x2144000) at ../Python/ceval.c:4963 CC(np.einsum support) _PyEval_EvalFrameDefault (f=, throwflag=) at ../Python/ceval.c:3500 CC(Require protobuf 3.6.0 or later) 0x00000000005f6226 in PyEval_EvalFrameEx (throwflag=0, f=Frame 0x7ffaf5ccfa60, for file /home/em/jax/jax/interpreters/xla.py, line 1579, in item (self=)) at ../Python/ceval.c:741 CC(Hard crash when no compatible cuda devices found) function_code_fastcall (globals=, nargs=, args=, co=) at ../Objects/call.c:284 CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") _PyFunction_Vectorcall (func=, stack=, nargsf=, kwnames=) at ../Objects/call.c:411 CC(Fix support for arrays with size0 dimensions.) 0x000000000056b5e0 in _PyObject_Vectorcall (kwnames=0x0, nargsf=, args=0x7ff7faa665c0, callable=) at ../Include/cpython/abstract.h:127 CC(Set distinct_host_configuration=false in the bazel options.) call_function (kwnames=0x0, oparg=, pp_stack=, tstate=0x2144000) at ../Python/ceval.c:4963 CC(Open Source Contributions) _PyEval_EvalFrameDefault (f=, throwflag=) at ../Python/ceval.c:3486 CC(np.linalg.inv support) 0x00000000005009e3 in PyEval_EvalFrameEx (throwflag=0,     f=Frame 0x7ff7faa66420, for file bug_repro.py, line 89, in __iter__ (self=) at remote 0x7ff9b3f6d880>, size=None) at remote 0x7ff9552785e0>, num_workers=4, prefetch=16) at remote 0x7ff9b3f7c2b0>, size=180, maxsize=180) at remote 0x7ff9b3f7c370>, it=, count=0, item=, fail=))     at ../Python/ceval.c:741 CC(Feature request: export TF ops) gen_send_ex (closing=0, exc=0, arg=0x0, gen=0x7ff954ac4970) at ../Objects/genobject.c:222 CC(Update XLA and reenable numpy tests that failed on Mac) gen_iternext (gen=0x7ff954ac4970) at ../Objects/genobject.c:543 CC(jacrev and jacfwd usage example) 0x000000000056b7c9 in _PyEval_EvalFrameDefault (f=, throwflag=) at ../Python/ceval.c:3202 CC(Unimplemented: binary integer op 'power') 0x00000000005009e3 in PyEval_EvalFrameEx (throwflag=0,     f=Frame 0x88bb4fd0, for file /home/em/.pyenv/lib/python3.8/sitepackages/tqdm/std.py, line 1195, in __iter__ (self=) at remote 0x7ff9b3f6d880>, size=None) at remote 0x7ff9552785e0>, num_workers=4, prefetch=16) at remote 0x7ff9b3f7c2b0>, size=180, maxsize=180) at remote 0x7ff9b3f7c370>, desc='', total=180, leave=True, fp=, write=, flush=) at remote 0x7ffaedd03460>, ncols=119, nrows=61, mininterval=, maxinterval=, miniters=0, dynamic_miniters=True, ascii=False, disable=False, unit='it', unit_scale=False, unit_divisor=1000, initial=0, lock_args=None, delay=0, gui=False, dynamic_ncols=False, smoothing=, _ema_dn=, last=0, calls=0) at ...(truncated)) at ../Python/ceval.c:741 ``` This appears to be a regression: this code did not hang in jax 0.2.25 but does in 0.2.27. I git bisected the jax repo (using `bug_bisect.sh` from that gist) and the first bad commit appears to be: ``` 96623c3048351d4f43dee8d999fb440ef1a2c557 is the first bad commit commit 96623c3048351d4f43dee8d999fb440ef1a2c557 Author: Matthew Johnson  Date:   Wed Jul 22 12:10:43 2020 0700     make iter(DeviceArray) return DeviceArrays w/o sync  jax/_src/lax/lax.py          11 +++++++++++  6 files changed, 58 insertions(+), 18 deletions() ``` This looks very likely to be behind the problem somehow. As for why the hang only occurs when my script is loading data from an external process, it doesn't seem to be deterministic so possibly it is a timing thing.",2022-03-04T14:24:34Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/9767,"I've been experiencing the same issue and I managed to find a workaround temporarily. The workaround is to launch the child processes before we touch anything in Jax. I also need to be careful to use the spawn method to start new processes and prevent the child processes from importing Jax (I simply put all my Jax related imports inside a `if __name__ == '__main__':`). For PyTorch dataloader, I use persistent workers since I need to avoid creating new child processes every time I start iterating through the dataloader.  This issue is also related to CC(RuntimeError: INTERNAL: Core halted unexpectedly: No error message available as no compiler metadata was provided.). If I call a Jax function that doesn't put any tensors on TPU, launch the dataloader child processes and then start training, I would get that RuntimeError: INTERNAL error message instead of hanging.","Is there any progress on this problem? I encountered this issue, too. But I was running JAX on GPUs (NVIDIA A100) rather than TPUs. After training for about 2 hours, my program just hung and GPUUtil dropped to 0%. I got a similar stack trace from gdb: ``` CC(未找到相关数据)  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38 CC(Python 3 compatibility issues)  0x00007f0832cf672e in absl::lts_20211102::synchronization_internal::Waiter::Wait(absl::lts_20211102::synchronization_internal::KernelTimeout) () from /home/yousiki/miniconda3/envs/jax/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Explicit tuples are not valid function parameters in Python 3)  0x00007f0832cf6663 in AbslInternalPerThreadSemWait_lts_20211102 () from /home/yousiki/miniconda3/envs/jax/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from ..core import JaxTuple)  0x00007f0832cf4b5d in absl::lts_20211102::Mutex::Block(absl::lts_20211102::base_internal::PerThreadSynch*) () from /home/yousiki/miniconda3/envs/jax/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Undefined name: from six.moves import xrange)  0x00007f0832cf5a74 in absl::lts_20211102::Mutex::AwaitCommon(absl::lts_20211102::Condition const&, absl::lts_20211102::synchronization_internal::KernelTimeout) () from /home/yousiki/miniconda3/envs/jax/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Building on OSX with CUDA)  0x00007f0832cf5ad5 in absl::lts_20211102::Mutex::Await(absl::lts_20211102::Condition const&) () from /home/yousiki/miniconda3/envs/jax/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Made a shim to handle configuration without having absl parse flags)  0x00007f082e893d88 in xla::PjRtStreamExecutorExecutable::Execute(absl::lts_20211102::Span > const>, xla::ExecuteOptions const&, absl::lts_20211102::optional, std::allocator > > >&) () from /home/yousiki/miniconda3/envs/jax/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Quickish check)  0x00007f082dade964 in jax::PmapFunction::Call(pybind11::args, pybind11::kwargs) () from /home/yousiki/miniconda3/envs/jax/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Quickish check)  0x00007f082dadfe96 in JaxPmapFunction_tp_call () from /home/yousiki/miniconda3/envs/jax/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Adding quickstart notebook, and corresponding gitignore rules)  0x000055f7e976552e in _PyObject_MakeTpCall (tstate=0x55f7e9df43f0, callable=0x7f07d0153690, args=, nargs=, keywords=0x0) at /opt/conda/condabld/pythonsplit_1648715631366/work/Objects/call.c:215 CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) 0x000055f7e980444b in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=, args=0x55f7ef227ef8, callable=, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:112 CC(Split out `jax` and `jaxlib` packages) PyObject_Vectorcall () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:123 CC(Update the quickstart notebook.) call_function (kwnames=0x0, oparg=, pp_stack=, trace_info=0x7ffeb2405340, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5867 CC(Fixing logo size so resize is not required) _PyEval_EvalFrameDefault (tstate=, f=, throwflag=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:4213 CC(Add copyright notice to quickstart notebook.) 0x000055f7e97b2742 in _PyEval_EvalFrame () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/internal/pycore_ceval.h:46 CC(rename in_bdims, out_bdims > in_axes, out_axes) _PyEval_Vector (tstate=, con=0x7f07e39a8320, locals=, args=, argcount=, kwnames=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5065 CC(Add wheelbuilding scripts) 0x000055f7e9727425 in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=, args=0x7f07e3c6c1b0, callable=0x7f07e39a8310, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:114 CC(Implement np.repeat for scalar repeats.) PyObject_Vectorcall () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:123 CC(Populate readme) call_function (kwnames=0x0, oparg=, pp_stack=, trace_info=0x7ffeb2405570, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5867 CC(Notebook showing how to write gufuncs with vmap) _PyEval_EvalFrameDefault (tstate=, f=, throwflag=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:4213 CC(Fix link in gufuncs notebook) 0x000055f7e97b2742 in _PyEval_EvalFrame () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/internal/pycore_ceval.h:46 CC(Typo) _PyEval_Vector (tstate=, con=0x7f07e39a83b0, locals=, args=, argcount=, kwnames=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5065 CC(differention > differentiation) 0x000055f7e9727425 in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=, args=0x7f07e3ee21c8, callable=0x7f07e39a83a0, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:114 CC(Typo, Python parens) PyObject_Vectorcall () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:123 CC(attempt to centerjustify the jax logo in readme) call_function (kwnames=0x0, oparg=, pp_stack=, trace_info=0x7ffeb24057a0, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5867 CC(Barebones neural network and data loading example notebook) _PyEval_EvalFrameDefault (tstate=, f=, throwflag=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:4213 CC(fix symbolic zero handling in concat transpose) 0x000055f7e97b2742 in _PyEval_EvalFrame () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/internal/pycore_ceval.h:46 CC(Cloud TPU Support) _PyEval_Vector (tstate=, con=0x7f0827c37e30, locals=, args=, argcount=, kwnames=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5065 CC(examples/datasets.py doesn’t work in python3) 0x000055f7e9727425 in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=, args=0x55f7ee6d9fb8, callable=0x7f0827c37e20, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:114 CC(Add support for `np.trace` ) PyObject_Vectorcall () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:123 CC(Error on NaN?) call_function (kwnames=0x0, oparg=, pp_stack=, trace_info=0x7ffeb24059d0, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5867 CC(Bug in examples?) _PyEval_EvalFrameDefault (tstate=, f=, throwflag=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:4213 CC(Fix the bug in classifier example, batching_test and README) 0x000055f7e97b2742 in _PyEval_EvalFrame () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/internal/pycore_ceval.h:46 CC(Broadcasting of size0 dimensions not implemented) _PyEval_Vector (tstate=, con=0x7f0827c37f50, locals=, args=, argcount=, kwnames=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5065 CC(minor spelling tweaks) 0x000055f7e97282a5 in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=, args=0x7f0955761e60, callable=0x7f0827c37f40, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:114 CC(CUDA90 and py3 ) PyObject_Vectorcall () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:123 CC(add dot_general batching rule) call_function (kwnames=0x0, oparg=, pp_stack=, trace_info=0x7ffeb2405c00, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5867 CC(np.einsum support) _PyEval_EvalFrameDefault (tstate=, f=, throwflag=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:4181 CC(Require protobuf 3.6.0 or later) 0x000055f7e97b2742 in _PyEval_EvalFrame () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/internal/pycore_ceval.h:46 CC(Hard crash when no compatible cuda devices found) _PyEval_Vector (tstate=, con=0x7ffeb2405d80, locals=, args=, argcount=, kwnames=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5065 CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") 0x000055f7e97c1578 in PyEval_EvalCode (co=0x7f09554fbd60, globals=0x7f095565d600, locals=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:1134 CC(Fix support for arrays with size0 dimensions.) 0x000055f7e98b5b1a in builtin_exec_impl.isra.18 (locals=0x7f095565d600, globals=0x7f095565d600, source=0x7f09554fbd60) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/bltinmodule.c:1056 CC(Set distinct_host_configuration=false in the bazel options.) builtin_exec (module=, args=0x55f7e9e90258, nargs=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/clinic/bltinmodule.c.h:371 CC(Open Source Contributions) 0x000055f7e976fa61 in cfunction_vectorcall_FASTCALL (func=0x7f09557b0a90, args=0x55f7e9e90258, nargsf=, kwnames=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Objects/methodobject.c:430 CC(np.linalg.inv support) 0x000055f7e9727425 in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=, args=0x55f7e9e90258, callable=0x7f09557b0a90, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:114 CC(Feature request: export TF ops) PyObject_Vectorcall () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:123 CC(Update XLA and reenable numpy tests that failed on Mac) call_function (kwnames=0x0, oparg=, pp_stack=, trace_info=0x7ffeb2405f40, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5867 CC(jacrev and jacfwd usage example) _PyEval_EvalFrameDefault (tstate=, f=, throwflag=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:4213 CC(Unimplemented: binary integer op 'power') 0x000055f7e97b2742 in _PyEval_EvalFrame () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/internal/pycore_ceval.h:46 CC(Update neural_network_and_data_loading.ipynb) _PyEval_Vector (tstate=, con=0x7f0955505d90, locals=, args=, argcount=, kwnames=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5065 CC(Update README.md) 0x000055f7e9727425 in _PyObject_VectorcallTstate (kwnames=0x0, nargsf=, args=0x7f0955745be0, callable=0x7f0955505d80, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:114 CC(add docstrings for major public functions) PyObject_Vectorcall () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/cpython/abstract.h:123 CC(Scenarios to prefer over cupy) call_function (kwnames=0x0, oparg=, pp_stack=, trace_info=0x7ffeb2406170, tstate=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5867 CC(More informative error on trying to concatenate zerodimensional arrays) _PyEval_EvalFrameDefault (tstate=, f=, throwflag=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:4213 CC(Batching rules for pad and concatenate primitives not implemented) 0x000055f7e97b2742 in _PyEval_EvalFrame () at /opt/conda/condabld/pythonsplit_1648715631366/work/Include/internal/pycore_ceval.h:46 CC(np.rot90 support) _PyEval_Vector (tstate=, con=0x7f0955506330, locals=, args=, argcount=, kwnames=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Python/ceval.c:5065 CC(Improving jax.scipy.stats) 0x000055f7e976afaa in PyVectorcall_Call (kwargs=, tuple=0x7f0955714380, callable=0x7f0955506320) at /opt/conda/condabld/pythonsplit_1648715631366/work/Objects/call.c:255 CC(v0.2 tasks) _PyObject_Call (tstate=, callable=0x7f0955506320, args=0x7f0955714380, kwargs=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Objects/call.c:290 CC(Frequenty asked questions doc) 0x000055f7e98b0f7a in pymain_run_module (modname=, set_argv0=1) at /opt/conda/condabld/pythonsplit_1648715631366/work/Modules/main.c:297 CC(Autodiff cookbook) 0x000055f7e98b53c9 in pymain_run_python (exitcode=0x7ffeb24063a0) at /opt/conda/condabld/pythonsplit_1648715631366/work/Modules/main.c:585 CC(Vmap cookbook) Py_RunMain () at /opt/conda/condabld/pythonsplit_1648715631366/work/Modules/main.c:670 CC(Docstrings in api.py) 0x000055f7e98b57d9 in Py_BytesMain (argc=, argv=) at /opt/conda/condabld/pythonsplit_1648715631366/work/Modules/main.c:1083 CC(Upload existing JAX talks to docs/ or talks/ folder) 0x00007f09558730b3 in __libc_start_main (main=0x55f7e9748650 , argc=9, argv=0x7ffeb24065b8, init=, fini=, rtld_fini=, stack_end=0x7ffeb24065a8) at ../csu/libcstart.c:308 CC(Keras NumPy backend demo) 0x000055f7e98355d4 in _start () ``` I'm using `tf.data` to load data and `clu.metric_writers` to write metrics, which may create child process. "
400,"以下是一个github上的jax下的一个issue, 标题是([JAX:GPU] Implement the full_matrices=False case of SVD without generating the full matrices and then slicing.)， 内容是 ([JAX:GPU] Implement the full_matrices=False case of SVD without generating the full matrices and then slicing.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[JAX:GPU] Implement the full_matrices=False case of SVD without generating the full matrices and then slicing.,[JAX:GPU] Implement the full_matrices=False case of SVD without generating the full matrices and then slicing.,2022-03-03T18:56:59Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9760
3568,"以下是一个github上的jax下的一个issue, 标题是(SVD: Very Memory Inefficient)， 内容是 (The following operations would run easily in both Tensorflow and PyTorch (GPU), but would cause OOM in Jax. When it runs on PyTorch, GPU memory usage is only ~2GB. Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import jax.numpy as jnp import jax.random as jrandom A = jrandom.normal(jrandom.PRNGKey(0), [300000, 50]) print(A.shape) jnp.linalg.svd(A, full_matrices=False) ```  [x] If applicable, include full error messages/tracebacks. ```  RuntimeError                              Traceback (most recent call last) /tmp/ipykernel_57631/2574565684.py in        3 A = jrandom.normal(jrandom.PRNGKey(0), [300000, 50])       4 print(A.shape) > 5 jnp.linalg.svd(A, full_matrices=False)     [... skipping hidden 6 frame] /opt/conda/lib/python3.7/sitepackages/jax/_src/dispatch.py in _execute_compiled(name, compiled, output_buffer_counts, result_handlers, kept_var_idx, *args)     442   input_bufs = util.flatten(     443       device_put(x, device) for i, x in enumerate(args) if i in kept_var_idx) > 444   out_bufs = compiled.execute(input_bufs)     445   check_special(name, out_bufs)     446   if output_buffer_counts is None: RuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 360336162820 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:   57.22MiB               constant allocation:         0B         maybe_live_out allocation:   57.23MiB      preallocated temp allocation:  335.59GiB   preallocated temp fragmentation:       112B (0.00%)                  total allocation:  335.70GiB               total fragmentation:    10.0KiB (0.00%) Peak buffers: 	Buffer 1: 		Size: 335.28GiB 		XLA Label: customcall 		Shape: f32[300000,300000] 		========================== 	Buffer 2: 		Size: 263.36MiB 		XLA Label: customcall 		Shape: f32[69038144] 		========================== 	Buffer 3: 		Size: 57.22MiB 		XLA Label: customcall 		Shape: f32[300000,50] 		========================== 	Buffer 4: 		Size: 57.22MiB 		Entry Parameter Subshape: f32[300000,50] 		========================== 	Buffer 5: 		Size: 57.22MiB 		Operator: op_name=""jit(svd)/jit(main)/svd[full_matrices=False compute_uv=True]"" source_file=""/tmp/ipykernel_57631/2574565684.py"" source_line=5 		XLA Label: fusion 		Shape: f32[300000,50] 		========================== 	Buffer 6: 		Size: 9.8KiB 		XLA Label: customcall 		Shape: f32[50,50] 		========================== 	Buffer 7: 		Size: 9.8KiB 		Operator: op_name=""jit(svd)/jit(main)/svd[full_matrices=False compute_uv=True]"" source_file=""/tmp/ipykernel_57631/2574565684.py"" source_line=5 		XLA Label: fusion 		Shape: f32[50,50] 		========================== 	Buffer 8: 		Size: 200B 		Operator: op_name=""jit(svd)/jit(main)/svd[full_matrices=False compute_uv=True]"" source_file=""/tmp/ipykernel_57631/2574565684.py"" source_line=5 		XLA Label: fusion 		Shape: f32[50] 		========================== 	Buffer 9: 		Size: 48B 		XLA Label: customcall 		Shape: (f32[300000,50], f32[50], f32[300000,300000], f32[50,50], s32[], /*index=5*/f32[69038144]) 		========================== 	Buffer 10: 		Size: 24B 		XLA Label: tuple 		Shape: (f32[300000,50], f32[50], f32[50,50]) 		========================== 	Buffer 11: 		Size: 4B 		XLA Label: customcall 		Shape: s32[] 		========================== ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,SVD: Very Memory Inefficient,"The following operations would run easily in both Tensorflow and PyTorch (GPU), but would cause OOM in Jax. When it runs on PyTorch, GPU memory usage is only ~2GB. Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python import jax.numpy as jnp import jax.random as jrandom A = jrandom.normal(jrandom.PRNGKey(0), [300000, 50]) print(A.shape) jnp.linalg.svd(A, full_matrices=False) ```  [x] If applicable, include full error messages/tracebacks. ```  RuntimeError                              Traceback (most recent call last) /tmp/ipykernel_57631/2574565684.py in        3 A = jrandom.normal(jrandom.PRNGKey(0), [300000, 50])       4 print(A.shape) > 5 jnp.linalg.svd(A, full_matrices=False)     [... skipping hidden 6 frame] /opt/conda/lib/python3.7/sitepackages/jax/_src/dispatch.py in _execute_compiled(name, compiled, output_buffer_counts, result_handlers, kept_var_idx, *args)     442   input_bufs = util.flatten(     443       device_put(x, device) for i, x in enumerate(args) if i in kept_var_idx) > 444   out_bufs = compiled.execute(input_bufs)     445   check_special(name, out_bufs)     446   if output_buffer_counts is None: RuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 360336162820 bytes. BufferAssignment OOM Debugging. BufferAssignment stats:              parameter allocation:   57.22MiB               constant allocation:         0B         maybe_live_out allocation:   57.23MiB      preallocated temp allocation:  335.59GiB   preallocated temp fragmentation:       112B (0.00%)                  total allocation:  335.70GiB               total fragmentation:    10.0KiB (0.00%) Peak buffers: 	Buffer 1: 		Size: 335.28GiB 		XLA Label: customcall 		Shape: f32[300000,300000] 		========================== 	Buffer 2: 		Size: 263.36MiB 		XLA Label: customcall 		Shape: f32[69038144] 		========================== 	Buffer 3: 		Size: 57.22MiB 		XLA Label: customcall 		Shape: f32[300000,50] 		========================== 	Buffer 4: 		Size: 57.22MiB 		Entry Parameter Subshape: f32[300000,50] 		========================== 	Buffer 5: 		Size: 57.22MiB 		Operator: op_name=""jit(svd)/jit(main)/svd[full_matrices=False compute_uv=True]"" source_file=""/tmp/ipykernel_57631/2574565684.py"" source_line=5 		XLA Label: fusion 		Shape: f32[300000,50] 		========================== 	Buffer 6: 		Size: 9.8KiB 		XLA Label: customcall 		Shape: f32[50,50] 		========================== 	Buffer 7: 		Size: 9.8KiB 		Operator: op_name=""jit(svd)/jit(main)/svd[full_matrices=False compute_uv=True]"" source_file=""/tmp/ipykernel_57631/2574565684.py"" source_line=5 		XLA Label: fusion 		Shape: f32[50,50] 		========================== 	Buffer 8: 		Size: 200B 		Operator: op_name=""jit(svd)/jit(main)/svd[full_matrices=False compute_uv=True]"" source_file=""/tmp/ipykernel_57631/2574565684.py"" source_line=5 		XLA Label: fusion 		Shape: f32[50] 		========================== 	Buffer 9: 		Size: 48B 		XLA Label: customcall 		Shape: (f32[300000,50], f32[50], f32[300000,300000], f32[50,50], s32[], /*index=5*/f32[69038144]) 		========================== 	Buffer 10: 		Size: 24B 		XLA Label: tuple 		Shape: (f32[300000,50], f32[50], f32[50,50]) 		========================== 	Buffer 11: 		Size: 4B 		XLA Label: customcall 		Shape: s32[] 		========================== ```",2022-03-03T03:18:17Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/9755,Yeah we're actually implementing this as `full_matrices=True` under the hood. I did this because NVidia's documentation for gesvd is pretty confusing and it wasn't clear to me it even supports the reduced output (it does). The fix is easy enough but will need a new jaxlib release.,"Oh nice, thanks for letting me know!","No need to close this, there's certainly something for us to fix here...",https://github.com/google/jax/pull/9760 probably fixes this.,Great  looking for the new release and will try it out!," CC([JAX:GPU] Implement the full_matrices=False case of SVD without generating the full matrices and then slicing.) is now merged, which should have fixed this problem.",Thanks again!
338,"以下是一个github上的jax下的一个issue, 标题是(Fix typo: JAX_CUSPARSE_11030 -> JAX_CUSPARSE_11300)， 内容是 (Fix typo: JAX_CUSPARSE_11030 > JAX_CUSPARSE_11300 This is a silly typo, but it's been annoying me for months)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix typo: JAX_CUSPARSE_11030 -> JAX_CUSPARSE_11300,"Fix typo: JAX_CUSPARSE_11030 > JAX_CUSPARSE_11300 This is a silly typo, but it's been annoying me for months",2022-03-03T00:23:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9751
331,"以下是一个github上的jax下的一个issue, 标题是(jnp.unravel_index: simplify return statement)， 内容是 (Issue referenced in TODO has been fixed ( CC(DeviceArray.__iter__ returns DeviceArrays, without host sync)))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jnp.unravel_index: simplify return statement,"Issue referenced in TODO has been fixed ( CC(DeviceArray.__iter__ returns DeviceArrays, without host sync))",2022-03-03T00:04:44Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9750
12798,"以下是一个github上的jax下的一个issue, 标题是(Escaped tracer when evaluationg gradients through cpu-parallelised code)， 内容是 (Hey Jax team, I'm currently trying to deploy optical modelling code across multiple CPUs for faster model evaluation and optimisation.  Here is a short overview of what the code is doing: We have a low level function modelling our optical system `get_psfs(x, y, osys)` that is being deployed across multiple CPUs over a 1D array of the first two input parameters. I have tried multiple ways of getting this to deploy in parallel with pmap, vmap,. Note the 'osys' object is a nonJAX type defining the optical system we are using so it must be set to static. These are the ways I have tried: ``` psfs_parallel = jit(vmap(get_psfs, inaxes=(0, 0, None), static_argnums=2, backend=cpu) ``` ``` import os os.environ['XLA_FLAGS'] = 'xla_force_host_platform_device_count=100'  we are passing in (100,) shaped vectors psfs_parallel = pmap(get_psfs, inaxes=(0, 0, None), static_broadcasted_argnums=2) ``` Note: the XLA_FLAG is set at the start of the script as required. These are then wrapped by a loss function ``` def loss(X, data, osys):     xs, ys = X[:100], X[100:]     image = np.sum(psfs_parallel(xs, ys, osys), axis=0)     return np.sum(np.square(data  image)) loss_grad = value_and_grad(loss, static_argnums=(1, 2)) ``` `np.sum(psfs_parallel(xs, ys, osys), axis=0)` works as expected and can be evaluated many times. The issue arises from trying to calculate the gradients through the loss function.  `loss_grad()` can be evaluated a single time successfully and returns nonzero and nonnan gradients, but upon calling it a second time throws this error: (The full stack trace is at the bottom of the post!) UnexpectedTracerError: Encountered an unexpected tracer. Perhaps this tracer escaped through global state from a previously traced function. The functions being transformed should not save traced values to global state. Details: Different traces at same level: Traced, JaxprTrace(level=0/0). I have searched extensively and can not find how exactly to locate the source of the problem, or how to fix it. I suspect there maybe a problem in the way I am composing this, as the same error arises using either the jitted vmap or pmap. As far as I can tell this can arise from operating on an the attributes of an object rather than passing around native jax types, however my understanding is that this is required for all jitted or grad() functions and I have never encountered this problem before trying to parallelise over CPUs. Could this be an issue with the way jax deploys across CPUs? Am I composing these functions appropriately for CPU parallelisation? Unfortunately this code is running on older versions of jax and jaxlib because we have not had time to rescript our underlying modelling code to integrate with jax 0.2x +. We instead run on  jax==0.1.77 jaxlib==0.1.57  Please let me know if you have an idea about whats going on! Cheers  FilteredStackTrace                        Traceback (most recent call last)  in        2  grads = error_grad(X_true, sim_params, data, new_osys) > 3 grads = error_grad(X_true, sim_params, data, osys)  in error_min(X, args, data, osys)      53      psfs = model(X, args, osys) > 54     psfs = model_min(X, args, osys)      55     error = np.sum(np.square(psfsdata))  in model_min(X, args, osys)      43      print(x) > 44     psf_stack = calc_fresnel_psf_vmap(xs_in, ys_in, Fres, wl)      45      psf_stack = calc_fresnel_psf_pmap(xs_in, ys_in, Fres, wl) FilteredStackTrace: jax.core.UnexpectedTracerError: Encountered an unexpected tracer. Perhaps this tracer escaped through global state from a previously traced function. The functions being transformed should not save traced values to global state. Details: Different traces at same level: Traced, JaxprTrace(level=0/0). The stack trace above excludes JAXinternal frames. The following is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: UnexpectedTracerError                     Traceback (most recent call last)  in        1  new_osys = get_fresnel_osys(aperture, pupil_npix, npix, pix_size, fl, fl_shift, beam_ratio, wl)       2  grads = error_grad(X_true, sim_params, data, new_osys) > 3 grads = error_grad(X_true, sim_params, data, osys) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     134   def reraise_with_filtered_traceback(*args, **kwargs):     135     try: > 136       return fun(*args, **kwargs)     137     except Exception as e:     138       if not is_under_reraiser(e): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/api.py in value_and_grad_f(*args, **kwargs)     809     tree_map(partial(_check_input_dtype_grad, holomorphic), dyn_args)     810     if not has_aux: > 811       ans, vjp_py = _vjp(f_partial, *dyn_args)     812     else:     813       ans, vjp_py, aux = _vjp(f_partial, *dyn_args, has_aux=True) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/api.py in _vjp(fun, *primals, **kwargs)    1854   if not has_aux:    1855     flat_fun, out_tree = flatten_fun_nokwargs(fun, in_tree) > 1856     out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)    1857     out_tree = out_tree()    1858   else: /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/ad.py in vjp(traceable, primals, has_aux)     108 def vjp(traceable, primals, has_aux=False):     109   if not has_aux: > 110     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)     111   else:     112     out_primals, pvals, jaxpr, consts, aux = linearize(traceable, *primals, has_aux=True) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/ad.py in linearize(traceable, *primals, **kwargs)      95   _, in_tree = tree_flatten(((primals, primals), {}))      96   jvpfun_flat, out_tree = flatten_fun(jvpfun, in_tree) > 97   jaxpr, out_pvals, consts = pe.trace_to_jaxpr(jvpfun_flat, in_pvals)      98   out_primals_pvals, out_tangents_pvals = tree_unflatten(out_tree(), out_pvals)      99   assert all(out_primal_pval.is_known() for out_primal_pval in out_primals_pvals) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py in trace_to_jaxpr(fun, pvals, instantiate, stage_out, bottom, trace_type)     420   with core.new_main(trace_type, bottom=bottom) as main:     421     fun = trace_to_subjaxpr(fun, main, instantiate) > 422     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)     423     assert not env     424     del main /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/linear_util.py in call_wrapped(self, *args, **kwargs)     149      150     try: > 151       ans = self.f(*args, **dict(self.params, **kwargs))     152     except:     153        Some transformations yield from inside context managers, so we have to  in error_min(X, args, data, osys)      52 def error_min(X, args, data, osys):      53      psfs = model(X, args, osys) > 54     psfs = model_min(X, args, osys)      55     error = np.sum(np.square(psfsdata))      56     return error  in model_min(X, args, osys)      42       43      print(x) > 44     psf_stack = calc_fresnel_psf_vmap(xs_in, ys_in, Fres, wl)      45      psf_stack = calc_fresnel_psf_pmap(xs_in, ys_in, Fres, wl)      46     psf_arrays = fluxes_in * psf_stack.reshape([Nims, Nstars, npix, npix]) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     134   def reraise_with_filtered_traceback(*args, **kwargs):     135     try: > 136       return fun(*args, **kwargs)     137     except Exception as e:     138       if not is_under_reraiser(e): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/api.py in f_jitted(*args, **kwargs)     206       _check_arg(arg)     207     flat_fun, out_tree = flatten_fun(f, in_tree) > 208     out = xla.xla_call(     209         flat_fun,     210         *args_flat, /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in bind(self, fun, *args, **params)    1144     1145   def bind(self, fun, *args, **params): > 1146     return call_bind(self, fun, *args, **params)    1147     1148   def process(self, trace, fun, tracers, params): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in call_bind(primitive, fun, *args, **params)    1136   else:    1137     tracers = map(top_trace.full_raise, args) > 1138     outs = primitive.process(top_trace, fun, tracers, params)    1139   return apply_todos(env_trace_todo(), map(full_lower, outs))    1140  /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in process(self, trace, fun, tracers, params)    1147     1148   def process(self, trace, fun, tracers, params): > 1149     return trace.process_call(self, fun, tracers, params)    1150     1151   def post_process(self, trace, out_tracers, params): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/ad.py in process_call(self, call_primitive, f, tracers, params)     273     update_params = call_param_updaters.get(call_primitive)     274     new_params = update_params(params, nz_tangents) if update_params else params > 275     result = call_primitive.bind(f_jvp, *primals, *nonzero_tangents, **new_params)     276     primal_out, tangent_out = tree_unflatten(out_tree_def(), result)     277     return [JVPTracer(self, p, t) for p, t in zip(primal_out, tangent_out)] /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in bind(self, fun, *args, **params)    1144     1145   def bind(self, fun, *args, **params): > 1146     return call_bind(self, fun, *args, **params)    1147     1148   def process(self, trace, fun, tracers, params): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in call_bind(primitive, fun, *args, **params)    1136   else:    1137     tracers = map(top_trace.full_raise, args) > 1138     outs = primitive.process(top_trace, fun, tracers, params)    1139   return apply_todos(env_trace_todo(), map(full_lower, outs))    1140  /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in process(self, trace, fun, tracers, params)    1147     1148   def process(self, trace, fun, tracers, params): > 1149     return trace.process_call(self, fun, tracers, params)    1150     1151   def post_process(self, trace, out_tracers, params): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py in process_call(self, primitive, f, tracers, params)     179                   else PartialVal.unknown(mapped_aval(pval[0]))     180                   for pval, is_mapped in zip(in_pvals, params['mapped_invars'])] > 181     jaxpr, out_pvals, consts, env_tracers = self.partial_eval(     182         f, in_pvals, partial(primitive.bind, **params))     183     if primitive.map_primitive: /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py in partial_eval(self, f, pvals, app)     282     out_consts, consts = split_list(out_flat, [len(out_flat)len(jaxpr.constvars)])     283     out_pvs = map(PartialVal, zip(out_avals, out_consts)) > 284     env_tracers = map(self.full_raise, env)     285     return jaxpr, out_pvs, consts, env_tracers     286  /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/util.py in safe_map(f, *args)      33   for arg in args[1:]:      34     assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args))) > 35   return list(map(f, *args))      36       37 def unzip2(xys): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in full_raise(self, val)     390                                  .format(val, self))     391     else:   val._trace.level == self.level: > 392       raise escaped_tracer_error(""Different traces at same level: {}, {}""     393                                  .format(val, self))     394  UnexpectedTracerError: Encountered an unexpected tracer. Perhaps this tracer escaped through global state from a previously traced function. The functions being transformed should not save traced values to global state. Details: Different traces at same level: Traced, JaxprTrace(level=0/0).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Escaped tracer when evaluationg gradients through cpu-parallelised code,"Hey Jax team, I'm currently trying to deploy optical modelling code across multiple CPUs for faster model evaluation and optimisation.  Here is a short overview of what the code is doing: We have a low level function modelling our optical system `get_psfs(x, y, osys)` that is being deployed across multiple CPUs over a 1D array of the first two input parameters. I have tried multiple ways of getting this to deploy in parallel with pmap, vmap,. Note the 'osys' object is a nonJAX type defining the optical system we are using so it must be set to static. These are the ways I have tried: ``` psfs_parallel = jit(vmap(get_psfs, inaxes=(0, 0, None), static_argnums=2, backend=cpu) ``` ``` import os os.environ['XLA_FLAGS'] = 'xla_force_host_platform_device_count=100'  we are passing in (100,) shaped vectors psfs_parallel = pmap(get_psfs, inaxes=(0, 0, None), static_broadcasted_argnums=2) ``` Note: the XLA_FLAG is set at the start of the script as required. These are then wrapped by a loss function ``` def loss(X, data, osys):     xs, ys = X[:100], X[100:]     image = np.sum(psfs_parallel(xs, ys, osys), axis=0)     return np.sum(np.square(data  image)) loss_grad = value_and_grad(loss, static_argnums=(1, 2)) ``` `np.sum(psfs_parallel(xs, ys, osys), axis=0)` works as expected and can be evaluated many times. The issue arises from trying to calculate the gradients through the loss function.  `loss_grad()` can be evaluated a single time successfully and returns nonzero and nonnan gradients, but upon calling it a second time throws this error: (The full stack trace is at the bottom of the post!) UnexpectedTracerError: Encountered an unexpected tracer. Perhaps this tracer escaped through global state from a previously traced function. The functions being transformed should not save traced values to global state. Details: Different traces at same level: Traced, JaxprTrace(level=0/0). I have searched extensively and can not find how exactly to locate the source of the problem, or how to fix it. I suspect there maybe a problem in the way I am composing this, as the same error arises using either the jitted vmap or pmap. As far as I can tell this can arise from operating on an the attributes of an object rather than passing around native jax types, however my understanding is that this is required for all jitted or grad() functions and I have never encountered this problem before trying to parallelise over CPUs. Could this be an issue with the way jax deploys across CPUs? Am I composing these functions appropriately for CPU parallelisation? Unfortunately this code is running on older versions of jax and jaxlib because we have not had time to rescript our underlying modelling code to integrate with jax 0.2x +. We instead run on  jax==0.1.77 jaxlib==0.1.57  Please let me know if you have an idea about whats going on! Cheers  FilteredStackTrace                        Traceback (most recent call last)  in        2  grads = error_grad(X_true, sim_params, data, new_osys) > 3 grads = error_grad(X_true, sim_params, data, osys)  in error_min(X, args, data, osys)      53      psfs = model(X, args, osys) > 54     psfs = model_min(X, args, osys)      55     error = np.sum(np.square(psfsdata))  in model_min(X, args, osys)      43      print(x) > 44     psf_stack = calc_fresnel_psf_vmap(xs_in, ys_in, Fres, wl)      45      psf_stack = calc_fresnel_psf_pmap(xs_in, ys_in, Fres, wl) FilteredStackTrace: jax.core.UnexpectedTracerError: Encountered an unexpected tracer. Perhaps this tracer escaped through global state from a previously traced function. The functions being transformed should not save traced values to global state. Details: Different traces at same level: Traced, JaxprTrace(level=0/0). The stack trace above excludes JAXinternal frames. The following is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: UnexpectedTracerError                     Traceback (most recent call last)  in        1  new_osys = get_fresnel_osys(aperture, pupil_npix, npix, pix_size, fl, fl_shift, beam_ratio, wl)       2  grads = error_grad(X_true, sim_params, data, new_osys) > 3 grads = error_grad(X_true, sim_params, data, osys) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     134   def reraise_with_filtered_traceback(*args, **kwargs):     135     try: > 136       return fun(*args, **kwargs)     137     except Exception as e:     138       if not is_under_reraiser(e): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/api.py in value_and_grad_f(*args, **kwargs)     809     tree_map(partial(_check_input_dtype_grad, holomorphic), dyn_args)     810     if not has_aux: > 811       ans, vjp_py = _vjp(f_partial, *dyn_args)     812     else:     813       ans, vjp_py, aux = _vjp(f_partial, *dyn_args, has_aux=True) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/api.py in _vjp(fun, *primals, **kwargs)    1854   if not has_aux:    1855     flat_fun, out_tree = flatten_fun_nokwargs(fun, in_tree) > 1856     out_primal, out_vjp = ad.vjp(flat_fun, primals_flat)    1857     out_tree = out_tree()    1858   else: /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/ad.py in vjp(traceable, primals, has_aux)     108 def vjp(traceable, primals, has_aux=False):     109   if not has_aux: > 110     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)     111   else:     112     out_primals, pvals, jaxpr, consts, aux = linearize(traceable, *primals, has_aux=True) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/ad.py in linearize(traceable, *primals, **kwargs)      95   _, in_tree = tree_flatten(((primals, primals), {}))      96   jvpfun_flat, out_tree = flatten_fun(jvpfun, in_tree) > 97   jaxpr, out_pvals, consts = pe.trace_to_jaxpr(jvpfun_flat, in_pvals)      98   out_primals_pvals, out_tangents_pvals = tree_unflatten(out_tree(), out_pvals)      99   assert all(out_primal_pval.is_known() for out_primal_pval in out_primals_pvals) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py in trace_to_jaxpr(fun, pvals, instantiate, stage_out, bottom, trace_type)     420   with core.new_main(trace_type, bottom=bottom) as main:     421     fun = trace_to_subjaxpr(fun, main, instantiate) > 422     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)     423     assert not env     424     del main /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/linear_util.py in call_wrapped(self, *args, **kwargs)     149      150     try: > 151       ans = self.f(*args, **dict(self.params, **kwargs))     152     except:     153        Some transformations yield from inside context managers, so we have to  in error_min(X, args, data, osys)      52 def error_min(X, args, data, osys):      53      psfs = model(X, args, osys) > 54     psfs = model_min(X, args, osys)      55     error = np.sum(np.square(psfsdata))      56     return error  in model_min(X, args, osys)      42       43      print(x) > 44     psf_stack = calc_fresnel_psf_vmap(xs_in, ys_in, Fres, wl)      45      psf_stack = calc_fresnel_psf_pmap(xs_in, ys_in, Fres, wl)      46     psf_arrays = fluxes_in * psf_stack.reshape([Nims, Nstars, npix, npix]) /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     134   def reraise_with_filtered_traceback(*args, **kwargs):     135     try: > 136       return fun(*args, **kwargs)     137     except Exception as e:     138       if not is_under_reraiser(e): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/api.py in f_jitted(*args, **kwargs)     206       _check_arg(arg)     207     flat_fun, out_tree = flatten_fun(f, in_tree) > 208     out = xla.xla_call(     209         flat_fun,     210         *args_flat, /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in bind(self, fun, *args, **params)    1144     1145   def bind(self, fun, *args, **params): > 1146     return call_bind(self, fun, *args, **params)    1147     1148   def process(self, trace, fun, tracers, params): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in call_bind(primitive, fun, *args, **params)    1136   else:    1137     tracers = map(top_trace.full_raise, args) > 1138     outs = primitive.process(top_trace, fun, tracers, params)    1139   return apply_todos(env_trace_todo(), map(full_lower, outs))    1140  /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in process(self, trace, fun, tracers, params)    1147     1148   def process(self, trace, fun, tracers, params): > 1149     return trace.process_call(self, fun, tracers, params)    1150     1151   def post_process(self, trace, out_tracers, params): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/ad.py in process_call(self, call_primitive, f, tracers, params)     273     update_params = call_param_updaters.get(call_primitive)     274     new_params = update_params(params, nz_tangents) if update_params else params > 275     result = call_primitive.bind(f_jvp, *primals, *nonzero_tangents, **new_params)     276     primal_out, tangent_out = tree_unflatten(out_tree_def(), result)     277     return [JVPTracer(self, p, t) for p, t in zip(primal_out, tangent_out)] /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in bind(self, fun, *args, **params)    1144     1145   def bind(self, fun, *args, **params): > 1146     return call_bind(self, fun, *args, **params)    1147     1148   def process(self, trace, fun, tracers, params): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in call_bind(primitive, fun, *args, **params)    1136   else:    1137     tracers = map(top_trace.full_raise, args) > 1138     outs = primitive.process(top_trace, fun, tracers, params)    1139   return apply_todos(env_trace_todo(), map(full_lower, outs))    1140  /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in process(self, trace, fun, tracers, params)    1147     1148   def process(self, trace, fun, tracers, params): > 1149     return trace.process_call(self, fun, tracers, params)    1150     1151   def post_process(self, trace, out_tracers, params): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py in process_call(self, primitive, f, tracers, params)     179                   else PartialVal.unknown(mapped_aval(pval[0]))     180                   for pval, is_mapped in zip(in_pvals, params['mapped_invars'])] > 181     jaxpr, out_pvals, consts, env_tracers = self.partial_eval(     182         f, in_pvals, partial(primitive.bind, **params))     183     if primitive.map_primitive: /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py in partial_eval(self, f, pvals, app)     282     out_consts, consts = split_list(out_flat, [len(out_flat)len(jaxpr.constvars)])     283     out_pvs = map(PartialVal, zip(out_avals, out_consts)) > 284     env_tracers = map(self.full_raise, env)     285     return jaxpr, out_pvs, consts, env_tracers     286  /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/util.py in safe_map(f, *args)      33   for arg in args[1:]:      34     assert len(arg) == n, 'length mismatch: {}'.format(list(map(len, args))) > 35   return list(map(f, *args))      36       37 def unzip2(xys): /import/morgana2/snert/louis/anaconda3/envs/morph/lib/python3.8/sitepackages/jax/core.py in full_raise(self, val)     390                                  .format(val, self))     391     else:   val._trace.level == self.level: > 392       raise escaped_tracer_error(""Different traces at same level: {}, {}""     393                                  .format(val, self))     394  UnexpectedTracerError: Encountered an unexpected tracer. Perhaps this tracer escaped through global state from a previously traced function. The functions being transformed should not save traced values to global state. Details: Different traces at same level: Traced, JaxprTrace(level=0/0).",2022-03-01T07:07:59Z,needs info,closed,0,4,https://github.com/jax-ml/jax/issues/9728,"Okay so I have found a work around to this. I am able to successfully calculate gradients through this system by creating a copy of the `osys` object using `copy.deepcopy()` before every call of loss_grad().  However this seems to introduce a memory leak, could this be due to the way that jax stores traced arrays? ","I think you might be storing in `osys` some intermediate arrays in your `get_psfs` function, and you are not supposed to do this, because when operating under jax transformations such as jit/vmap/grad, those arrays are not really arrays, but tracers, and you should not 'capture' them in your 'static' data structures. By the way, a much better approach to 'static' data structures (that helps you not do errors) is to use `flax.struct.dataclass`. Those are frozen dataclasses, so you they oblige you not to store intermediate data inside of them. Moreover they help you distinguish 'mutable' data such as arrays from compiletime static data.",Perhaps `with jax.checking_leaks():` can help you narrow down where the tracer leak happens in `get_psfs`,"Since this seems to be a userfunction not being pure, and not a JAX bug, let me close this issue."
1410,"以下是一个github上的jax下的一个issue, 标题是(jax.scipy.sparse.linalg.cg memory leak)， 内容是 (I just stumbled across the following behaviour with `jax.scipy.sparse.linalg.cg`: ```python import jax import jax.numpy as jnp import scipy import numpy as np import os import psutil print(jax.__version__) i = 0 size = 2000 while True:     i += 1     key = jax.random.PRNGKey(i)     key1, key2 = jax.random.split(key)     A = jax.random.normal(key1, shape=(size, size))     S = A @ A.T     F = jax.random.normal(key2, shape=(size,))     update, _ = jax.scipy.sparse.linalg.cg(S, F, tol=1e5, maxiter=5000)      update, _ = scipy.sparse.linalg.cg(np.array(S), np.array(F), tol=1e5, maxiter=5000)     print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2) ``` which produces the following output: ``` WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 0.3.1 329.5390625 371.21484375 394.41015625 401.88671875 425.2890625 440.75 440.9765625 463.9140625 487.109375 499.234375 518.00390625 533.45703125 548.6640625 561.03125 579.87109375 595.0546875 610.7734375 625.984375 ... ``` This does not occur for the scipy version that is commented out. The jax version is 0.3.1. Also tried `jnp.linalg.solve` which does not have this issue.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.scipy.sparse.linalg.cg memory leak,"I just stumbled across the following behaviour with `jax.scipy.sparse.linalg.cg`: ```python import jax import jax.numpy as jnp import scipy import numpy as np import os import psutil print(jax.__version__) i = 0 size = 2000 while True:     i += 1     key = jax.random.PRNGKey(i)     key1, key2 = jax.random.split(key)     A = jax.random.normal(key1, shape=(size, size))     S = A @ A.T     F = jax.random.normal(key2, shape=(size,))     update, _ = jax.scipy.sparse.linalg.cg(S, F, tol=1e5, maxiter=5000)      update, _ = scipy.sparse.linalg.cg(np.array(S), np.array(F), tol=1e5, maxiter=5000)     print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2) ``` which produces the following output: ``` WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 0.3.1 329.5390625 371.21484375 394.41015625 401.88671875 425.2890625 440.75 440.9765625 463.9140625 487.109375 499.234375 518.00390625 533.45703125 548.6640625 561.03125 579.87109375 595.0546875 610.7734375 625.984375 ... ``` This does not occur for the scipy version that is commented out. The jax version is 0.3.1. Also tried `jnp.linalg.solve` which does not have this issue.",2022-02-28T14:05:36Z,bug,open,2,12,https://github.com/jax-ml/jax/issues/9714,"I just tried, and if you jit `cg` memory still goes up but not by much. (Still, this increase is non negligible as it is ~100kB per 5 iterations... I wonder what is jax caching) ```python In [7]: cg = jax.jit(jax.scipy.sparse.linalg.cg, static_argnames=(""maxiter"")) In [8]: import jax    ...: import jax.numpy as jnp    ...:    ...: import scipy    ...: import numpy as np    ...:    ...: import os    ...: import psutil    ...:    ...: print(jax.__version__)    ...: i = 0    ...: size = 2000    ...: while True:    ...:     i += 1    ...:     key = jax.random.PRNGKey(i)    ...:     key1, key2 = jax.random.split(key)    ...:    ...:     A = jax.random.normal(key1, shape=(size, size))    ...:     S = A @ A.T    ...:    ...:     F = jax.random.normal(key2, shape=(size,))    ...:    ...:     update, _ = cg(S, F, tol=1e5, maxiter=5000)    ...:     update.block_until_ready()    ...:     print(psutil.Process(os.getpid()).memory_info().rss / 1024**2)    ...: 0.2.28 417.38671875 440.08984375 440.19921875 440.19921875 440.53125 440.5859375 440.5859375 440.91015625 440.91015625 440.91015625 441.01171875 441.01171875 441.16796875 441.16796875 441.16796875 441.16796875 441.16796875 ```","I printed the size and shape of all live buffers in each loop ```python   bs = jax.lib.xla_bridge.get_backend().live_buffers()   print('', len(bs))   for b in bs:     print(b.shape, '; ', b.size) ``` ```➜  ~ python t.py 0.2.28 WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 235.3359375  8 (2000,) ;  2000 (2000,) ;  2000 (2000,) ;  2000 (2000, 2000) ;  4000000 (2000, 2000) ;  4000000 (2,) ;  2 (2,) ;  2 (2,) ;  2 272.4296875  16 (2000,) ;  2000 (2000,) ;  2000 (2000,) ;  2000 (2000, 2000) ;  4000000 (2000, 2000) ;  4000000 (2,) ;  2 (2,) ;  2 (2,) ;  2 (2000,) ;  2000 (2000,) ;  2000 (2000,) ;  2000 (2000, 2000) ;  4000000 (2000, 2000) ;  4000000 (2,) ;  2 (2,) ;  2 (2,) ;  2 302.734375  24 (2000,) ;  2000 (2000,) ;  2000 (2000,) ;  2000 (2000, 2000) ;  4000000 (2000, 2000) ;  4000000 (2,) ;  2 (2,) ;  2 (2,) ;  2 (2000,) ;  2000 (2000,) ;  2000 (2000,) ;  2000 (2000, 2000) ;  4000000 (2000, 2000) ;  4000000 (2,) ;  2 (2,) ;  2 (2,) ;  2 (2000,) ;  2000 (2000,) ;  2000 (2000,) ;  2000 (2000, 2000) ;  4000000 (2000, 2000) ;  4000000 (2,) ;  2 (2,) ;  2 (2,) ;  2 ``` Based on the size and shapes, we seem to be keeping around the keys, A, S, F and some more in each loop. I didn't seem anything obvious from cg implementation. I wonder if  has an idea?","Is there any progress? Friendly ping:) I find that even with `jit`, there is a significant memory leak…… using `bs = jax.lib.xla_bridge.get_backend().live_buffers()`, I find that there is a significant memory leak on GPU too!!!  reported a small memory leak with `jit`, because he/she ran `cg` on GPU/TPU but inspect CPU memory only.","Indeed. on CPU (I'm running this on Mac), if I allocate the arrays outside of the loop I get ``` import jax import jax.numpy as jnp import scipy import numpy as np import os import psutil print(jax.__version__) i = 0 size = 2000 key = jax.random.PRNGKey(i) key1, key2 = jax.random.split(key) A = jax.random.normal(key1, shape=(size, size)) S = A @ A.T F = jax.random.normal(key2, shape=(size,)) cg = jax.jit(jax.scipy.sparse.linalg.cg, static_argnames=(""maxiter"")) while True:     i += 1     update, _ = cg(S, F, tol=1e5, maxiter=5000)     update.block_until_ready()     print(psutil.Process(os.getpid()).memory_info().rss / 1024**2) 0.3.3 WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 204.59765625 204.59765625 204.59765625 204.59765625 204.59765625 204.59765625 204.59765625 204.59765625 204.59765625 204.59765625 ``` suggesting that the leak is in the allocation of the arrays. It seems that the line that allocates is only S=A.T and not the `random.normal`. It also seems that the allocations slow down exponentially suggesting that this is some sort of buffer that is growing somewhere, maybe just a list of previously allocated arrays.  I think it's mostly harmless.","May I humbly and politely suggest you to follow a bit more some sort of _Internet etiquette_ as saying _WTF!!_, creating issues like `!!!Severe Memory Leak` where you tag several people but do not provide an actual good description of what's the issue, ask people ""to give a look"" (what should I look at?) without providing an explanation may be annoying if not offensive for people? Etiquette in general is very ill defined, and people will perceive it differently, but if you ask me to spend time on your issues, I'd at least appreciate having a clear actionable item instead of just being told 'please investigate this thing for me'. In this particular case when you say `If I use 1000 ** 3 size array, it will run into OOM on 16G V100.` I have no idea what you are referring to. How long before it runs OOM. Also because assuming that Jax allocates 75% of 15G, a 1000^3 array takes almost 4Gb, so depending on the operations involved I would not be surprised of an OOM if you are not careful in freeing previous arrays.  With that said, I have very little time those days. From a brief test, it seems to me that the memory growth does not depend on array size. If you want to convince me/other people that this is an actual serious leak, just make a plot of the consumed memory as a function of the iteration number for different sizes of the allocated arrays. Then it will be clear.","Very sorry for my bad and rude presentation. I just use _WTF_ to express my shock. I create CC(Memory Leak when create DeviceArray in python loop(without JIT)?) with a minimal reproduce code, and ""Memory Leak when create DeviceArray in python loop(without JIT)"" is a breif  description for the problem. (Do you can't see full title?) If you see the code in CC(Memory Leak when create DeviceArray in python loop(without JIT)?), I think it is easy to know what I am referring to for `If I use 1000 ** 3 size array, it will run into OOM on 16G V100`, since I use `for _ in range(5)` instead of `while True` in CC(Memory Leak when create DeviceArray in python loop(without JIT)?) . I also provide the output of my reproduce code in CC(Memory Leak when create DeviceArray in python loop(without JIT)?) , which clearly shows the problem.","  I am sorry for my bad representation which makes people think it is offensive or annoying. Here I want to clarify my intention of ""Can you have a look at CC(Memory Leak when create DeviceArray in python loop(without JIT)?)"": I am not asking you to ""investigate this thing for me"" or ""spend time on my issues"". I just want to share my test result with you, and I don't want to copy my code and result from that issue.  I didn't provide a explanation here, because I wrongly assumed that you have time to open the linke and see my issue. This is my fault.","  Hi! I find that you make a mistake in your print code: `bs` and `b` hold references of arrays. Actually only 3 arrays are leaked per loop, not 8. ```python import jax print(jax.__version__) size = 2000 for i in range(2):     key = jax.random.PRNGKey(i)     key1, key2 = jax.random.split(key)     A = jax.random.normal(key1, shape=(size, size))     S = A @ A.T     F = jax.random.normal(key2, shape=(size,))     update, _ = jax.scipy.sparse.linalg.cg(S, F, tol=1e5, maxiter=5000)     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(b.shape, '; ', b.size)     del b     del bs ``` Will output ``` 0.3.4  8 (2000,) ;  2000 (2000,) ;  2000 (2000,) ;  2000 (2000, 2000) ;  4000000 (2000, 2000) ;  4000000 (2,) ;  2 (2,) ;  2 (2,) ;  2  11 (2000,) ;  2000 (2000,) ;  2000 (2000,) ;  2000 (2000, 2000) ;  4000000 (2000, 2000) ;  4000000 (2,) ;  2 (2,) ;  2 (2,) ;  2 (2000,) ;  2000 (2000,) ;  2000 (2000, 2000) ;  4000000 ```","I would like to share my tests results: 1. There isn't a memory leak with `jit`. ```python import os, psutil import jax USE_JIT = True cg = jax.jit(jax.scipy.sparse.linalg.cg, static_argnames=(""maxiter"")) if USE_JIT else jax.scipy.sparse.linalg.cg print(jax.__version__) size = 2000 def print_live_buffers():     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(b.shape, ';', b.size, ';', id(b)) for kk in range(10):     key = jax.random.PRNGKey(0)     key1, key2 = jax.random.split(key)     A = jax.random.normal(key1, shape=(size, size))     F = jax.random.normal(key2, shape=(size,))     S = A @ A.T     update, _ = cg(S, F, tol=1e5, maxiter=5000)     update.block_until_ready()     print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)     print_live_buffers()  comment out when using CPU ``` using CPU: ``` 0.3.4 20220323 17:05:59.622063: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 297.875 313.49609375 327.859375 328.859375 328.98828125 328.98828125 329.27734375 329.27734375 329.3515625 329.3515625 ``` using GPU ``` 0.3.4 1355.1484375  7 (2000,) ; 2000 ; 140091638530096 (2000, 2000) ; 4000000 ; 140091939945520 (2000,) ; 2000 ; 140091939941680 (2000, 2000) ; 4000000 ; 140091939942064 (2,) ; 2 ; 140091939944368 (2,) ; 2 ; 140091939942448 (2,) ; 2 ; 140091939943216 1355.1484375  7 (2000,) ; 2000 ; 140091939945520 (2000, 2000) ; 4000000 ; 140091939942448 (2000,) ; 2000 ; 140091939942064 (2000, 2000) ; 4000000 ; 140091939944368 (2,) ; 2 ; 140091939945904 (2,) ; 2 ; 140091939945136 (2,) ; 2 ; 140091638530480 1355.1484375  7 (2000,) ; 2000 ; 140091939942448 (2000, 2000) ; 4000000 ; 140091939945136 (2000,) ; 2000 ; 140091939944368 (2000, 2000) ; 4000000 ; 140091939945904 (2,) ; 2 ; 140091939943600 (2,) ; 2 ; 140091939943216 (2,) ; 2 ; 140091638530096 1355.1953125  7 (2000,) ; 2000 ; 140091939945136 (2000, 2000) ; 4000000 ; 140091939943216 (2000,) ; 2000 ; 140091939945904 (2000, 2000) ; 4000000 ; 140091939943600 (2,) ; 2 ; 140091939942064 (2,) ; 2 ; 140091939945520 (2,) ; 2 ; 140091638530480 1355.1953125  7 (2000,) ; 2000 ; 140091939943216 (2000, 2000) ; 4000000 ; 140091939945520 (2000,) ; 2000 ; 140091939943600 (2000, 2000) ; 4000000 ; 140091939942064 (2,) ; 2 ; 140091939944368 (2,) ; 2 ; 140091939942448 (2,) ; 2 ; 140091638530096 1355.1953125  7 (2000,) ; 2000 ; 140091939945520 (2000, 2000) ; 4000000 ; 140091939942448 (2000,) ; 2000 ; 140091939942064 (2000, 2000) ; 4000000 ; 140091939944368 (2,) ; 2 ; 140091939945904 (2,) ; 2 ; 140091939945136 (2,) ; 2 ; 140091638530480 1355.1953125  7 (2000,) ; 2000 ; 140091939942448 (2000, 2000) ; 4000000 ; 140091939945136 (2000,) ; 2000 ; 140091939944368 (2000, 2000) ; 4000000 ; 140091939945904 (2,) ; 2 ; 140091939943600 (2,) ; 2 ; 140091939943216 (2,) ; 2 ; 140091638530096 1355.1953125  7 (2000,) ; 2000 ; 140091939945136 (2000, 2000) ; 4000000 ; 140091939943216 (2000,) ; 2000 ; 140091939945904 (2000, 2000) ; 4000000 ; 140091939943600 (2,) ; 2 ; 140091939942064 (2,) ; 2 ; 140091939945520 (2,) ; 2 ; 140091638530480 1355.1953125  7 (2000,) ; 2000 ; 140091939943216 (2000, 2000) ; 4000000 ; 140091939945520 (2000,) ; 2000 ; 140091939943600 (2000, 2000) ; 4000000 ; 140091939942064 (2,) ; 2 ; 140091939944368 (2,) ; 2 ; 140091939942448 (2,) ; 2 ; 140091638530096 1355.1953125  7 (2000,) ; 2000 ; 140091939945520 (2000, 2000) ; 4000000 ; 140091939942448 (2000,) ; 2000 ; 140091939942064 (2000, 2000) ; 4000000 ; 140091939944368 (2,) ; 2 ; 140091939945904 (2,) ; 2 ; 140091939945136 (2,) ; 2 ; 140091638530480 ``` 2. There is a memory leak without `jit`, and the number of leaks depends on how many lines are inside the loop. ```python import jax USE_JIT = False cg = jax.jit(jax.scipy.sparse.linalg.cg, static_argnames=(""maxiter"")) if USE_JIT else jax.scipy.sparse.linalg.cg print(jax.__version__) size = 2000 def print_live_buffers():     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(b.shape, ';', b.size, ';', id(b)) for kk in range(3):     key = jax.random.PRNGKey(0)     key1, key2 = jax.random.split(key)     A = jax.random.normal(key1, shape=(size, size))     F = jax.random.normal(key2, shape=(size,))     S = A @ A.T     update, _ = cg(S, F, tol=1e5, maxiter=5000)     update.block_until_ready()     print_live_buffers() ``` ``` 0.3.4  8 (2000,) ; 2000 ; 139887831014192 (2000,) ; 2000 ; 139888298028336 (2000, 2000) ; 4000000 ; 139888298031024 (2000,) ; 2000 ; 139888298027184 (2000, 2000) ; 4000000 ; 139888298027568 (2,) ; 2 ; 139888298029872 (2,) ; 2 ; 139888298027952 (2,) ; 2 ; 139888298028720  11 (2000,) ; 2000 ; 139887831011888 (2000,) ; 2000 ; 139888298032560 (2000, 2000) ; 4000000 ; 139888298028720 (2000,) ; 2000 ; 139888298027568 (2000, 2000) ; 4000000 ; 139888298029872 (2,) ; 2 ; 139888298032176 (2,) ; 2 ; 139888298030640 (2,) ; 2 ; 139888298031408 (2000,) ; 2000 ; 139888298028336 (2000, 2000) ; 4000000 ; 139888298031024 (2000,) ; 2000 ; 139888298027184  14 (2000,) ; 2000 ; 139887831013040 (2000,) ; 2000 ; 139887831016880 (2000, 2000) ; 4000000 ; 139888298031408 (2000,) ; 2000 ; 139888298029872 (2000, 2000) ; 4000000 ; 139888298032176 (2,) ; 2 ; 139888298027952 (2,) ; 2 ; 139888298030256 (2,) ; 2 ; 139887831017264 (2000,) ; 2000 ; 139888298032560 (2000, 2000) ; 4000000 ; 139888298028720 (2000,) ; 2000 ; 139888298027568 (2000,) ; 2000 ; 139888298028336 (2000, 2000) ; 4000000 ; 139888298031024 (2000,) ; 2000 ; 139888298027184 ``` ```python import jax USE_JIT = False cg = jax.jit(jax.scipy.sparse.linalg.cg, static_argnames=(""maxiter"")) if USE_JIT else jax.scipy.sparse.linalg.cg print(jax.__version__) size = 2000 def print_live_buffers():     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(b.shape, ';', b.size, ';', id(b)) key = jax.random.PRNGKey(0) key1, key2 = jax.random.split(key) for kk in range(3):     A = jax.random.normal(key1, shape=(size, size))     F = jax.random.normal(key2, shape=(size,))     S = A @ A.T     update, _ = cg(S, F, tol=1e5, maxiter=5000)     update.block_until_ready()     print_live_buffers() ``` ``` 0.3.4  8 (2000,) ; 2000 ; 140467351542576 (2000,) ; 2000 ; 140467362377008 (2000, 2000) ; 4000000 ; 140467362379696 (2000,) ; 2000 ; 140467362375856 (2000, 2000) ; 4000000 ; 140467362376240 (2,) ; 2 ; 140467362378544 (2,) ; 2 ; 140467362376624 (2,) ; 2 ; 140467362377392  11 (2000,) ; 2000 ; 140467351540272 (2000,) ; 2000 ; 140467362379312 (2000, 2000) ; 4000000 ; 140467362380464 (2000,) ; 2000 ; 140467362376240 (2000, 2000) ; 4000000 ; 140467362381232 (2000,) ; 2000 ; 140467362377008 (2000, 2000) ; 4000000 ; 140467362379696 (2000,) ; 2000 ; 140467362375856 (2,) ; 2 ; 140467362378544 (2,) ; 2 ; 140467362376624 (2,) ; 2 ; 140467362377392  14 (2000,) ; 2000 ; 140467351541424 (2000,) ; 2000 ; 140467351545264 (2000, 2000) ; 4000000 ; 140467362380080 (2000,) ; 2000 ; 140467362381232 (2000, 2000) ; 4000000 ; 140467362380848 (2000,) ; 2000 ; 140467362379312 (2000, 2000) ; 4000000 ; 140467362380464 (2000,) ; 2000 ; 140467362376240 (2000,) ; 2000 ; 140467362377008 (2000, 2000) ; 4000000 ; 140467362379696 (2000,) ; 2000 ; 140467362375856 (2,) ; 2 ; 140467362378544 (2,) ; 2 ; 140467362376624 (2,) ; 2 ; 140467362377392 ``` ```python import jax USE_JIT = False cg = jax.jit(jax.scipy.sparse.linalg.cg, static_argnames=(""maxiter"")) if USE_JIT else jax.scipy.sparse.linalg.cg print(jax.__version__) size = 2000 def print_live_buffers():     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(b.shape, ';', b.size, ';', id(b)) key = jax.random.PRNGKey(0) key1, key2 = jax.random.split(key) A = jax.random.normal(key1, shape=(size, size)) for kk in range(3):     F = jax.random.normal(key2, shape=(size,))     S = A @ A.T     update, _ = cg(S, F, tol=1e5, maxiter=5000)     update.block_until_ready()     print_live_buffers() ``` ``` 0.3.4  8 (2000,) ; 2000 ; 140333629512496 (2000,) ; 2000 ; 140333676883248 (2000, 2000) ; 4000000 ; 140333676885936 (2000,) ; 2000 ; 140333676882096 (2000, 2000) ; 4000000 ; 140333676882480 (2,) ; 2 ; 140333676884784 (2,) ; 2 ; 140333676882864 (2,) ; 2 ; 140333676883632  11 (2000,) ; 2000 ; 140333629510192 (2000,) ; 2000 ; 140333676885552 (2000, 2000) ; 4000000 ; 140333676886704 (2000,) ; 2000 ; 140333676887472 (2000,) ; 2000 ; 140333676883248 (2000, 2000) ; 4000000 ; 140333676885936 (2000,) ; 2000 ; 140333676882096 (2000, 2000) ; 4000000 ; 140333676882480 (2,) ; 2 ; 140333676884784 (2,) ; 2 ; 140333676882864 (2,) ; 2 ; 140333676883632  14 (2000,) ; 2000 ; 140333629511344 (2000,) ; 2000 ; 140333629515184 (2000, 2000) ; 4000000 ; 140333676886320 (2000,) ; 2000 ; 140333676887088 (2000,) ; 2000 ; 140333676885552 (2000, 2000) ; 4000000 ; 140333676886704 (2000,) ; 2000 ; 140333676887472 (2000,) ; 2000 ; 140333676883248 (2000, 2000) ; 4000000 ; 140333676885936 (2000,) ; 2000 ; 140333676882096 (2000, 2000) ; 4000000 ; 140333676882480 (2,) ; 2 ; 140333676884784 (2,) ; 2 ; 140333676882864 (2,) ; 2 ; 140333676883632 ``` ```python import jax USE_JIT = False cg = jax.jit(jax.scipy.sparse.linalg.cg, static_argnames=(""maxiter"")) if USE_JIT else jax.scipy.sparse.linalg.cg print(jax.__version__) size = 2000 def print_live_buffers():     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(b.shape, ';', b.size, ';', id(b)) key = jax.random.PRNGKey(0) key1, key2 = jax.random.split(key) A = jax.random.normal(key1, shape=(size, size)) F = jax.random.normal(key2, shape=(size,)) for kk in range(3):     S = A @ A.T     update, _ = cg(S, F, tol=1e5, maxiter=5000)     update.block_until_ready()     print_live_buffers() ``` ``` 0.3.4  8 (2000,) ; 2000 ; 140423419779888 (2000,) ; 2000 ; 140423500737840 (2000, 2000) ; 4000000 ; 140423500740528 (2000,) ; 2000 ; 140423500736688 (2000, 2000) ; 4000000 ; 140423500737072 (2,) ; 2 ; 140423500739376 (2,) ; 2 ; 140423500737456 (2,) ; 2 ; 140423500738224  10 (2000,) ; 2000 ; 140423419777200 (2000,) ; 2000 ; 140423500739760 (2000, 2000) ; 4000000 ; 140423500741680 (2000,) ; 2000 ; 140423500737840 (2000, 2000) ; 4000000 ; 140423500740528 (2000,) ; 2000 ; 140423500736688 (2000, 2000) ; 4000000 ; 140423500737072 (2,) ; 2 ; 140423500739376 (2,) ; 2 ; 140423500737456 (2,) ; 2 ; 140423500738224  12 (2000,) ; 2000 ; 140423419777968 (2000,) ; 2000 ; 140423419779888 (2000, 2000) ; 4000000 ; 140423500740144 (2000,) ; 2000 ; 140423500739760 (2000, 2000) ; 4000000 ; 140423500741680 (2000,) ; 2000 ; 140423500737840 (2000, 2000) ; 4000000 ; 140423500740528 (2000,) ; 2000 ; 140423500736688 (2000, 2000) ; 4000000 ; 140423500737072 (2,) ; 2 ; 140423500739376 (2,) ; 2 ; 140423500737456 (2,) ; 2 ; 140423500738224 ``` ```python import jax USE_JIT = False cg = jax.jit(jax.scipy.sparse.linalg.cg, static_argnames=(""maxiter"")) if USE_JIT else jax.scipy.sparse.linalg.cg print(jax.__version__) size = 2000 def print_live_buffers():     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(b.shape, ';', b.size, ';', id(b)) key = jax.random.PRNGKey(0) key1, key2 = jax.random.split(key) A = jax.random.normal(key1, shape=(size, size)) S = A @ A.T for kk in range(3):     F = jax.random.normal(key2, shape=(size,))     update, _ = cg(S, F, tol=1e5, maxiter=5000)     update.block_until_ready()     print_live_buffers() ``` ``` 0.3.4  8 (2000,) ; 2000 ; 139806023392048 (2000,) ; 2000 ; 139806347605936 (2000,) ; 2000 ; 139806347603248 (2000, 2000) ; 4000000 ; 139806347605552 (2000, 2000) ; 4000000 ; 139806347602480 (2,) ; 2 ; 139806347604784 (2,) ; 2 ; 139806347602864 (2,) ; 2 ; 139806347603632  10 (2000,) ; 2000 ; 139806023389360 (2000,) ; 2000 ; 139806347605168 (2000,) ; 2000 ; 139806347607472 (2000,) ; 2000 ; 139806347605936 (2000,) ; 2000 ; 139806347603248 (2000, 2000) ; 4000000 ; 139806347605552 (2000, 2000) ; 4000000 ; 139806347602480 (2,) ; 2 ; 139806347604784 (2,) ; 2 ; 139806347602864 (2,) ; 2 ; 139806347603632  12 (2000,) ; 2000 ; 139806023390128 (2000,) ; 2000 ; 139806023392048 (2000,) ; 2000 ; 139806347606704 (2000,) ; 2000 ; 139806347605168 (2000,) ; 2000 ; 139806347607472 (2000,) ; 2000 ; 139806347605936 (2000,) ; 2000 ; 139806347603248 (2000, 2000) ; 4000000 ; 139806347605552 (2000, 2000) ; 4000000 ; 139806347602480 (2,) ; 2 ; 139806347604784 (2,) ; 2 ; 139806347602864 (2,) ; 2 ; 139806347603632 ``` ```python import jax USE_JIT = False cg = jax.jit(jax.scipy.sparse.linalg.cg, static_argnames=(""maxiter"")) if USE_JIT else jax.scipy.sparse.linalg.cg print(jax.__version__) size = 2000 def print_live_buffers():     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(b.shape, ';', b.size, ';', id(b)) key = jax.random.PRNGKey(0) key1, key2 = jax.random.split(key) A = jax.random.normal(key1, shape=(size, size)) F = jax.random.normal(key2, shape=(size,)) S = A @ A.T for kk in range(3):     update, _ = cg(S, F, tol=1e5, maxiter=5000)     update.block_until_ready()     print_live_buffers() ``` ``` 0.3.4  8 (2000,) ; 2000 ; 140521680952112 (2000,) ; 2000 ; 140521691786544 (2000, 2000) ; 4000000 ; 140521691789232 (2000,) ; 2000 ; 140521691785392 (2000, 2000) ; 4000000 ; 140521691785776 (2,) ; 2 ; 140521691788080 (2,) ; 2 ; 140521691786160 (2,) ; 2 ; 140521691786928  9 (2000,) ; 2000 ; 140521680949040 (2000,) ; 2000 ; 140521691789616 (2000,) ; 2000 ; 140521691786544 (2000, 2000) ; 4000000 ; 140521691789232 (2000,) ; 2000 ; 140521691785392 (2000, 2000) ; 4000000 ; 140521691785776 (2,) ; 2 ; 140521691788080 (2,) ; 2 ; 140521691786160 (2,) ; 2 ; 140521691786928  10 (2000,) ; 2000 ; 140521680945200 (2000,) ; 2000 ; 140521691790000 (2000,) ; 2000 ; 140521691789616 (2000,) ; 2000 ; 140521691786544 (2000, 2000) ; 4000000 ; 140521691789232 (2000,) ; 2000 ; 140521691785392 (2000, 2000) ; 4000000 ; 140521691785776 (2,) ; 2 ; 140521691788080 (2,) ; 2 ; 140521691786160 (2,) ; 2 ; 140521691786928 ```","More tests without jit: In conclusion, the memory leak is exactly the leak of `A`, `b` and `x0`. ```python import jax from jax.scipy.sparse.linalg import cg print(jax.__version__) size = 2 def print_live_buffers():     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(id(b))     print('') A = jax.numpy.zeros((size, size)) b = jax.numpy.zeros(size) x0 = jax.numpy.zeros(size) for _ in range(2):     cg(A, b, x0=x0, tol=1e5, maxiter=5000)     print_live_buffers() ``` ``` 0.3.4  3 140696627441712 140696627441328 140696627440944   3 140696627441712 140696627441328 140696627440944  ```  ```python import jax from jax.scipy.sparse.linalg import cg print(jax.__version__) size = 2 def print_live_buffers():     bs = jax.lib.xla_bridge.get_backend().live_buffers()     print('', len(bs))     for b in bs:         print(id(b))     print('') for _ in range(2):     A = jax.numpy.zeros((size, size))     b = jax.numpy.zeros(size)     x0 = jax.numpy.zeros(size)     print(f'{id(x0)=}')     print(f'{id(b)=}')     print(f'{id(A)=}')     cg(A, b, x0=x0, tol=1e5, maxiter=5000)     print_live_buffers() ``` ``` 0.3.4 id(x0)=140385840197680 id(b)=140385840197296 id(A)=140385840196912  3 140385840197680 140385840197296 140385840196912  id(x0)=140385840195760 id(b)=140385840199216 id(A)=140385840199984  6 140385840195760 140385840199216 140385840199984 140385840197680 140385840197296 140385840196912  ```","If I completely disable `jit`, i.e. `with jax.disable_jit()` and replace `lax.custom_linear_solve` with direct solve, there is no memory leak. https://github.com/google/jax/blob/b43ca61a93aec9a638b5fd6b04518869d45201aa/jax/_src/scipy/sparse/linalg.pyL189L227 ```python def _isolve(_isolve_solve, A, b, x0=None, *, tol=1e5, atol=0.0,             maxiter=None, M=None, check_symmetric=False):   if x0 is None:     x0 = tree_map(jnp.zeros_like, b)   b, x0 = device_put((b, x0))   if maxiter is None:     size = sum(bi.size for bi in tree_leaves(b))     maxiter = 10 * size   copied from scipy   if M is None:     M = _identity   A = _normalize_matvec(A)   M = _normalize_matvec(M)   if tree_structure(x0) != tree_structure(b):     raise ValueError(         'x0 and b must have matching tree structure: '         f'{tree_structure(x0)} vs {tree_structure(b)}')   if _shapes(x0) != _shapes(b):     raise ValueError(         'arrays in x0 and b must have matching shapes: '         f'{_shapes(x0)} vs {_shapes(b)}')   isolve_solve = partial(       _isolve_solve, x0=x0, tol=tol, atol=atol, maxiter=maxiter, M=M)    realvalued positivedefinite linear operators are symmetric   def real_valued(x):     return not issubclass(x.dtype.type, np.complexfloating)   symmetric = all(map(real_valued, tree_leaves(b))) \     if check_symmetric else False    x = lax.custom_linear_solve(        A, b, solve=isolve_solve, transpose_solve=isolve_solve,        symmetric=symmetric)   x = isolve_solve(A, b)   info = None   return x, info ```","I think this issue can be closed, since it is a jit cache, not a memory leak. I believe memory occupation will stop increasing after at most 4096 iterations. In addition, the large memory occupation can be eliminate with jitted `cg` or completely disable JIT. But there may be an enhancement to have a more flexible cache."
5449,"以下是一个github上的jax下的一个issue, 标题是(`CustomRootTest.test_custom_root_with_aux` test failure when building against MKL)， 内容是 (Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: Build jax/jaxlib from source against MKL version 2021.1.1 and run the complete jax test suite. In nix, this can be accomplished relatively easily: 1. Checkout nixpkgs at https://github.com/nixOS/nixpkgs/commit/c38d3659e3fa2e63055197b8b73bba0cd55a40dc. 2. Add ``` final: prev: {   cudatoolkit = prev.cudatoolkit_11_5;   cudnn = prev.cudnn_8_3_cudatoolkit_11_5;   blas = prev.blas.override { blasProvider = final.mkl; };   lapack = prev.lapack.override { lapackProvider = final.mkl; }; } ``` to `~/.config/nixpkgs/overlays/cuda_and_mkl.nix`. 3. `nixbuild A python3Packages.jax`  [x] If applicable, include full error messages/tracebacks. ``` =================================== FAILURES =================================== ___________________ CustomRootTest.test_custom_root_with_aux ___________________ [gw8] linux  Python 3.9.10 /nix/store/afi0ysqw20yiiw2gr2d28dx40bc4ddf8python33.9.10/bin/python3.9 self =      def test_custom_root_with_aux(self):       def root_aux(a, b):         f = lambda x: high_precision_dot(a, x)  b         factors = jsp.linalg.cho_factor(a)         cho_solve = lambda f, b: (jsp.linalg.cho_solve(factors, b), orig_aux)         def pos_def_solve(g, b):            prune aux to allow use as tangent_solve           cho_solve_noaux = lambda f, b: cho_solve(f, b)[0]           return lax.custom_linear_solve(g, b, cho_solve_noaux, symmetric=True)         return lax.custom_root(f, b, cho_solve, pos_def_solve, has_aux=True)       orig_aux = {""converged"": np.array(1.), ""nfev"": np.array(12345.), ""grad"": np.array([1.0, 2.0, 3.0])}       rng = self.rng()       a = rng.randn(2, 2)       b = rng.randn(2)       actual, actual_aux = root_aux(high_precision_dot(a, a.T), b)       actual_jit, actual_jit_aux = jax.jit(root_aux)(high_precision_dot(a, a.T), b)       expected = jnp.linalg.solve(high_precision_dot(a, a.T), b)       self.assertAllClose(expected, actual)       self.assertAllClose(expected, actual_jit)       jtu.check_eq(actual_jit_aux, orig_aux)        grad check with aux       jtu.check_grads(lambda x, y: root_aux(high_precision_dot(x, x.T), y),                       (a, b), order=2, rtol={jnp.float32: 1e2})        test vmap and jvp combined by jacfwd       fwd = jax.jacfwd(lambda x, y: root_aux(high_precision_dot(x, x.T), y), argnums=(0, 1))       expected_fwd = jax.jacfwd(lambda x, y: jnp.linalg.solve(high_precision_dot(x, x.T), y), argnums=(0, 1))       fwd_val, fwd_aux = fwd(a, b)       expected_fwd_val = expected_fwd(a, b) >     self.assertAllClose(fwd_val, expected_fwd_val, rtol={np.float32: 1E6, np.float64: 1E12}) tests/custom_root_test.py:227:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:990: in assertAllClose     self.assertAllClose(x_elt, y_elt, check_dtypes=check_dtypes, atol=atol, jax/_src/test_util.py:999: in assertAllClose     self.assertArraysAllClose(x, y, check_dtypes=False, atol=atol, rtol=rtol, jax/_src/test_util.py:964: in assertArraysAllClose     _assert_numpy_allclose(x, y, atol=atol, rtol=rtol, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  a = array([[[1.3422977 ,  0.55962217],         [ 1.0310359 , 0.8444528 ]],        [[ 1.4493693 , 0.3698212 ],         [1.4489349 ,  1.1834809 ]]], dtype=float32) b = array([[[1.3422956 ,  0.5596222 ],         [ 1.0310338 , 0.8444524 ]],        [[ 1.4493668 , 0.36982143],         [1.4489323 ,  1.1834806 ]]], dtype=float32) atol = 1e06, rtol = 1e06, err_msg = ''     def _assert_numpy_allclose(a, b, atol=None, rtol=None, err_msg=''):       if a.dtype == b.dtype == _dtypes.float0:         np.testing.assert_array_equal(a, b, err_msg=err_msg)         return       a = a.astype(np.float32) if a.dtype == _dtypes.bfloat16 else a       b = b.astype(np.float32) if b.dtype == _dtypes.bfloat16 else b       kw = {}       if atol: kw[""atol""] = atol       if rtol: kw[""rtol""] = rtol       with np.errstate(invalid='ignore'):          TODO(phawkins): surprisingly, assert_allclose sometimes reports invalid          value errors. It should not do that. >       np.testing.assert_allclose(a, b, **kw, err_msg=err_msg) E       AssertionError:  E       Not equal to tolerance rtol=1e06, atol=1e06 E        E       Mismatched elements: 3 / 8 (37.5%) E       Max absolute difference: 2.6226044e06 E       Max relative difference: 2.0811804e06 E        x: array([[[1.342298,  0.559622], E               [ 1.031036, 0.844453]], E       ... E        y: array([[[1.342296,  0.559622], E               [ 1.031034, 0.844452]], E       ... jax/_src/test_util.py:185: AssertionError =========================== short test summary info ============================ FAILED tests/custom_root_test.py::CustomRootTest::test_custom_root_with_aux ========== 1 failed, 13955 passed, 1409 skipped in 216.44s (0:03:36) =========== error: builder for '/nix/store/7z1ydr7xc19108r3nlp5na4m45izasclpython3.9jax0.3.1.drv' failed with exit code 1; ``` What versions of BLAS/LAPACK does JAX support? Is using MKL with JAX an officially supported combination?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`CustomRootTest.test_custom_root_with_aux` test failure when building against MKL,"Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: Build jax/jaxlib from source against MKL version 2021.1.1 and run the complete jax test suite. In nix, this can be accomplished relatively easily: 1. Checkout nixpkgs at https://github.com/nixOS/nixpkgs/commit/c38d3659e3fa2e63055197b8b73bba0cd55a40dc. 2. Add ``` final: prev: {   cudatoolkit = prev.cudatoolkit_11_5;   cudnn = prev.cudnn_8_3_cudatoolkit_11_5;   blas = prev.blas.override { blasProvider = final.mkl; };   lapack = prev.lapack.override { lapackProvider = final.mkl; }; } ``` to `~/.config/nixpkgs/overlays/cuda_and_mkl.nix`. 3. `nixbuild A python3Packages.jax`  [x] If applicable, include full error messages/tracebacks. ``` =================================== FAILURES =================================== ___________________ CustomRootTest.test_custom_root_with_aux ___________________ [gw8] linux  Python 3.9.10 /nix/store/afi0ysqw20yiiw2gr2d28dx40bc4ddf8python33.9.10/bin/python3.9 self =      def test_custom_root_with_aux(self):       def root_aux(a, b):         f = lambda x: high_precision_dot(a, x)  b         factors = jsp.linalg.cho_factor(a)         cho_solve = lambda f, b: (jsp.linalg.cho_solve(factors, b), orig_aux)         def pos_def_solve(g, b):            prune aux to allow use as tangent_solve           cho_solve_noaux = lambda f, b: cho_solve(f, b)[0]           return lax.custom_linear_solve(g, b, cho_solve_noaux, symmetric=True)         return lax.custom_root(f, b, cho_solve, pos_def_solve, has_aux=True)       orig_aux = {""converged"": np.array(1.), ""nfev"": np.array(12345.), ""grad"": np.array([1.0, 2.0, 3.0])}       rng = self.rng()       a = rng.randn(2, 2)       b = rng.randn(2)       actual, actual_aux = root_aux(high_precision_dot(a, a.T), b)       actual_jit, actual_jit_aux = jax.jit(root_aux)(high_precision_dot(a, a.T), b)       expected = jnp.linalg.solve(high_precision_dot(a, a.T), b)       self.assertAllClose(expected, actual)       self.assertAllClose(expected, actual_jit)       jtu.check_eq(actual_jit_aux, orig_aux)        grad check with aux       jtu.check_grads(lambda x, y: root_aux(high_precision_dot(x, x.T), y),                       (a, b), order=2, rtol={jnp.float32: 1e2})        test vmap and jvp combined by jacfwd       fwd = jax.jacfwd(lambda x, y: root_aux(high_precision_dot(x, x.T), y), argnums=(0, 1))       expected_fwd = jax.jacfwd(lambda x, y: jnp.linalg.solve(high_precision_dot(x, x.T), y), argnums=(0, 1))       fwd_val, fwd_aux = fwd(a, b)       expected_fwd_val = expected_fwd(a, b) >     self.assertAllClose(fwd_val, expected_fwd_val, rtol={np.float32: 1E6, np.float64: 1E12}) tests/custom_root_test.py:227:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:990: in assertAllClose     self.assertAllClose(x_elt, y_elt, check_dtypes=check_dtypes, atol=atol, jax/_src/test_util.py:999: in assertAllClose     self.assertArraysAllClose(x, y, check_dtypes=False, atol=atol, rtol=rtol, jax/_src/test_util.py:964: in assertArraysAllClose     _assert_numpy_allclose(x, y, atol=atol, rtol=rtol, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  a = array([[[1.3422977 ,  0.55962217],         [ 1.0310359 , 0.8444528 ]],        [[ 1.4493693 , 0.3698212 ],         [1.4489349 ,  1.1834809 ]]], dtype=float32) b = array([[[1.3422956 ,  0.5596222 ],         [ 1.0310338 , 0.8444524 ]],        [[ 1.4493668 , 0.36982143],         [1.4489323 ,  1.1834806 ]]], dtype=float32) atol = 1e06, rtol = 1e06, err_msg = ''     def _assert_numpy_allclose(a, b, atol=None, rtol=None, err_msg=''):       if a.dtype == b.dtype == _dtypes.float0:         np.testing.assert_array_equal(a, b, err_msg=err_msg)         return       a = a.astype(np.float32) if a.dtype == _dtypes.bfloat16 else a       b = b.astype(np.float32) if b.dtype == _dtypes.bfloat16 else b       kw = {}       if atol: kw[""atol""] = atol       if rtol: kw[""rtol""] = rtol       with np.errstate(invalid='ignore'):          TODO(phawkins): surprisingly, assert_allclose sometimes reports invalid          value errors. It should not do that. >       np.testing.assert_allclose(a, b, **kw, err_msg=err_msg) E       AssertionError:  E       Not equal to tolerance rtol=1e06, atol=1e06 E        E       Mismatched elements: 3 / 8 (37.5%) E       Max absolute difference: 2.6226044e06 E       Max relative difference: 2.0811804e06 E        x: array([[[1.342298,  0.559622], E               [ 1.031036, 0.844453]], E       ... E        y: array([[[1.342296,  0.559622], E               [ 1.031034, 0.844452]], E       ... jax/_src/test_util.py:185: AssertionError =========================== short test summary info ============================ FAILED tests/custom_root_test.py::CustomRootTest::test_custom_root_with_aux ========== 1 failed, 13955 passed, 1409 skipped in 216.44s (0:03:36) =========== error: builder for '/nix/store/7z1ydr7xc19108r3nlp5na4m45izasclpython3.9jax0.3.1.drv' failed with exit code 1; ``` What versions of BLAS/LAPACK does JAX support? Is using MKL with JAX an officially supported combination?",2022-02-25T21:20:50Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/9705,``` E       Max absolute difference: 2.6226044e06 E       Max relative difference: 2.0811804e06 ``` This is close enough for me to believe that nothing is actually wrong and that this is just a matter of floating point weirdness between library versions. But it does imply that tolerances in the test suite need to be adjusted.,"Yes, I think we might need to relax some test tolerances for MKL. We don't test that way ourselves, so I'm not surprised there are a few issues like this. Want to send a PR?","Roger that, created https://github.com/google/jax/pull/9706","Hmm, there may be some other tests that require relaxed tolerances. See https://github.com/NixOS/nixpkgs/issues/161960 and this failing build log for more detail. Oddly enough I'm finding that it builds fine for me locally (after including CC(Relax tolerances slightly for MKL)), which I'm trying to get to the bottom of here.","Ok, this turned out to be a difference between AMD/Intel CPUs. Moral of the story: Intel CPUs cannot be trusted with floating point arithmetic, even when using their own MKL library.  Created https://github.com/NixOS/nixpkgs/pull/162055 to fix this on the Nixpkgsside of things."
5697,"以下是一个github上的jax下的一个issue, 标题是(Using `jax.jit` inside a function decorated by `jax.checkpoint` causes recompilation every time)， 内容是 (Using a jitted function inside a function decorated by `jax.checkpoint` causes a lot of extra compilations even if the arguments still have the same shape. Calculating the gradient for such a function causes a memory leak in the long rung since all the compiled jitted functions seem to be stored in the memory. This can be observed by the high memory footprint of `backend_compile` which cannot be seen if the checkpointing is disabled. A selfconsistent example would be: ```python import jax import jax.numpy as jnp .jit def f(a):     return jnp.sum(a) .checkpoint def g(a):     return f(a) arr = jnp.array([[1,2,3,4,5],[6,7,8,9,10]], dtype=float) g_v_and_grad = jax.value_and_grad(g) for i in range(3):     working_arr = arr + i     print(g_v_and_grad(working_arr)) ``` Running the script with `env JAX_LOG_COMPILES=1` enabled one can observe: ``` WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002334117889404297 sec WARNING:absl:Finished tracing + transforming fn for jit in 0.0003993511199951172 sec WARNING:absl:Compiling fn (139703279463296 for args (ShapedArray(float32[2,5]), ShapedArray(int32[], weak_type=True)). WARNING:absl:Finished XLA compilation of fn in 0.04700160026550293 sec WARNING:absl:Finished tracing + transforming f for jit in 0.0010411739349365234 sec WARNING:absl:Finished tracing + transforming  for jit in 0.00015473365783691406 sec WARNING:absl:Compiling  (139703209762752 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.04526233673095703 sec WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00016546249389648438 sec WARNING:absl:Finished tracing + transforming  for jit in 0.00014591217041015625 sec WARNING:absl:Compiling  (139703209798976 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.00750732421875 sec WARNING:absl:Finished tracing + transforming backward_pass for jit in 0.0011491775512695312 sec WARNING:absl:Compiling backward_pass (139703209802560 for args (ShapedArray(float32[]),). WARNING:absl:Finished XLA compilation of transpose(jvp(f)) in 0.041948556900024414 sec (DeviceArray(55., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) WARNING:absl:Finished tracing + transforming  for jit in 0.00014543533325195312 sec WARNING:absl:Compiling  (139703209800384 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.007508516311645508 sec WARNING:absl:Finished tracing + transforming  for jit in 0.0001461505889892578 sec WARNING:absl:Compiling  (139703209863232 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.007668972015380859 sec WARNING:absl:Finished tracing + transforming backward_pass for jit in 0.0005974769592285156 sec WARNING:absl:Compiling backward_pass (139703209362624 for args (ShapedArray(float32[]),). WARNING:absl:Finished XLA compilation of transpose(jvp(f)) in 0.005425214767456055 sec (DeviceArray(65., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) WARNING:absl:Finished tracing + transforming  for jit in 0.00014638900756835938 sec WARNING:absl:Compiling  (139703209350720 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.007513523101806641 sec WARNING:absl:Finished tracing + transforming  for jit in 0.00015473365783691406 sec WARNING:absl:Compiling  (139703209372160 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.007587909698486328 sec WARNING:absl:Finished tracing + transforming backward_pass for jit in 0.0005350112915039062 sec WARNING:absl:Compiling backward_pass (139703209370048 for args (ShapedArray(float32[]),). WARNING:absl:Finished XLA compilation of transpose(jvp(f)) in 0.0054433345794677734 sec (DeviceArray(75., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) ``` Comment out the checkpoint decorator leads to the wanted behavior: ``` WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002498626708984375 sec WARNING:absl:Finished tracing + transforming fn for jit in 0.00040721893310546875 sec WARNING:absl:Compiling fn (140693235752000 for args (ShapedArray(float32[2,5]), ShapedArray(int32[], weak_type=True)). WARNING:absl:Finished XLA compilation of fn in 0.04748940467834473 sec WARNING:absl:Finished tracing + transforming f for jit in 0.0010097026824951172 sec WARNING:absl:Compiling f (140692730754112 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.04457998275756836 sec WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0001583099365234375 sec WARNING:absl:Finished tracing + transforming backward_pass for jit in 0.0004944801330566406 sec WARNING:absl:Compiling backward_pass (140692730730304 for args (ShapedArray(float32[]),). WARNING:absl:Finished XLA compilation of transpose(jvp(f)) in 0.041858673095703125 sec (DeviceArray(55., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) (DeviceArray(65., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) (DeviceArray(75., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Using `jax.jit` inside a function decorated by `jax.checkpoint` causes recompilation every time,"Using a jitted function inside a function decorated by `jax.checkpoint` causes a lot of extra compilations even if the arguments still have the same shape. Calculating the gradient for such a function causes a memory leak in the long rung since all the compiled jitted functions seem to be stored in the memory. This can be observed by the high memory footprint of `backend_compile` which cannot be seen if the checkpointing is disabled. A selfconsistent example would be: ```python import jax import jax.numpy as jnp .jit def f(a):     return jnp.sum(a) .checkpoint def g(a):     return f(a) arr = jnp.array([[1,2,3,4,5],[6,7,8,9,10]], dtype=float) g_v_and_grad = jax.value_and_grad(g) for i in range(3):     working_arr = arr + i     print(g_v_and_grad(working_arr)) ``` Running the script with `env JAX_LOG_COMPILES=1` enabled one can observe: ``` WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002334117889404297 sec WARNING:absl:Finished tracing + transforming fn for jit in 0.0003993511199951172 sec WARNING:absl:Compiling fn (139703279463296 for args (ShapedArray(float32[2,5]), ShapedArray(int32[], weak_type=True)). WARNING:absl:Finished XLA compilation of fn in 0.04700160026550293 sec WARNING:absl:Finished tracing + transforming f for jit in 0.0010411739349365234 sec WARNING:absl:Finished tracing + transforming  for jit in 0.00015473365783691406 sec WARNING:absl:Compiling  (139703209762752 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.04526233673095703 sec WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00016546249389648438 sec WARNING:absl:Finished tracing + transforming  for jit in 0.00014591217041015625 sec WARNING:absl:Compiling  (139703209798976 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.00750732421875 sec WARNING:absl:Finished tracing + transforming backward_pass for jit in 0.0011491775512695312 sec WARNING:absl:Compiling backward_pass (139703209802560 for args (ShapedArray(float32[]),). WARNING:absl:Finished XLA compilation of transpose(jvp(f)) in 0.041948556900024414 sec (DeviceArray(55., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) WARNING:absl:Finished tracing + transforming  for jit in 0.00014543533325195312 sec WARNING:absl:Compiling  (139703209800384 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.007508516311645508 sec WARNING:absl:Finished tracing + transforming  for jit in 0.0001461505889892578 sec WARNING:absl:Compiling  (139703209863232 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.007668972015380859 sec WARNING:absl:Finished tracing + transforming backward_pass for jit in 0.0005974769592285156 sec WARNING:absl:Compiling backward_pass (139703209362624 for args (ShapedArray(float32[]),). WARNING:absl:Finished XLA compilation of transpose(jvp(f)) in 0.005425214767456055 sec (DeviceArray(65., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) WARNING:absl:Finished tracing + transforming  for jit in 0.00014638900756835938 sec WARNING:absl:Compiling  (139703209350720 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.007513523101806641 sec WARNING:absl:Finished tracing + transforming  for jit in 0.00015473365783691406 sec WARNING:absl:Compiling  (139703209372160 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.007587909698486328 sec WARNING:absl:Finished tracing + transforming backward_pass for jit in 0.0005350112915039062 sec WARNING:absl:Compiling backward_pass (139703209370048 for args (ShapedArray(float32[]),). WARNING:absl:Finished XLA compilation of transpose(jvp(f)) in 0.0054433345794677734 sec (DeviceArray(75., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) ``` Comment out the checkpoint decorator leads to the wanted behavior: ``` WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002498626708984375 sec WARNING:absl:Finished tracing + transforming fn for jit in 0.00040721893310546875 sec WARNING:absl:Compiling fn (140693235752000 for args (ShapedArray(float32[2,5]), ShapedArray(int32[], weak_type=True)). WARNING:absl:Finished XLA compilation of fn in 0.04748940467834473 sec WARNING:absl:Finished tracing + transforming f for jit in 0.0010097026824951172 sec WARNING:absl:Compiling f (140692730754112 for args (ShapedArray(float32[2,5]),). WARNING:absl:Finished XLA compilation of jvp(f) in 0.04457998275756836 sec WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0001583099365234375 sec WARNING:absl:Finished tracing + transforming backward_pass for jit in 0.0004944801330566406 sec WARNING:absl:Compiling backward_pass (140692730730304 for args (ShapedArray(float32[]),). WARNING:absl:Finished XLA compilation of transpose(jvp(f)) in 0.041858673095703125 sec (DeviceArray(55., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) (DeviceArray(65., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) (DeviceArray(75., dtype=float32), DeviceArray([[1., 1., 1., 1., 1.],              [1., 1., 1., 1., 1.]], dtype=float32)) ```",2022-02-22T17:51:44Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/9661,"Hey, since this is a big blocker for our project at the moment, I would be very thankful if some of the jaxinternals experts could help with this issue? I tried to dig into the code why this happening but found nothing yet :(",Thanks for pinging this and highlighting that it's a blocker. Sorry for not getting to it sooner!,"This was a bit tricky! There is a problem with gradofrematofjit causing jit cache misses. These two changes are sufficient to fix it, and I believe both are necessary as well (though see below for alternatives): 1. CC(improve remat transpose caching (cf. 9661)) ~ CC(cache tracing of (sub)calls when forming a jaxpr) (specifically the `trace_to_subjaxpr_dynamic_memoized` function, been meaning to land this for a while...)~ 2. CC(improve caching of jax.remat) The latter is necessary because when roundtripping through a jaxpr (i.e. basically doing evaljaxprofmakejaxpr, as we often do, including when we linearize) we need to make sure we get cache hits. But we were constructing new opaque callables (with hash/equality defined by object id) every time, meaning we never did get cache hits. In particular, consider this repro involving `jax.linearize`: ```python  linearize_repro.py import sys import jax identity = jax.checkpoint(jax.jit(lambda x: 2 * x)) _, f_lin = jax.linearize(identity, 1.) for _ in range(int(sys.argv[1])):   f_lin(1.) ``` Running `env JAX_LOG_COMPILES=1 python linearize_repro.py $N` would show a number of compilations that scale linearly with `N`. (I piped stderr into `grep 'XLA compilation' | wc l` like a pro.) After CC(improve caching of jax.remat) the number of compiles becomes constant with `N`. However, while that fixed the issue for `linearize`, it wasn't quite enough for `grad` (which is basically `linearize` plus a transposition step). For that, we had to improve the remat transpose rule to support caching. That's what CC(improve remat transpose caching (cf. 9661)) does. (TODO explain that the new version of `checkpoint` in `ad_checkpoint.checkpoint` also needed separate work) TODO talk about the new tooling I'm going to add to explain automatically why recompiles happen", is the blocker resolved? (At git HEAD I mean; we haven't done a pypi release with these changes yet.)," Yes, I got no OOM error for my test run with the git HEAD today :D Thank you very much for the help!"," I just encountered an exception in a run of my code using the git HEAD. Do not know if this is related with the change: ``` Exception ignored in: functools.partial(.remove_key at 0x7ff8dfcee670>, (True, 'allow', None), (), ()) Traceback (most recent call last):   File ""/local_scratch/janluca/pypoetry/pepsad2ZmN71Jpy3.9/lib/python3.9/sitepackages/jax/_src/util.py"", line 230, in remove_key     del cache[(weak_arg, tctx, args, kwargs)] KeyError: (, (True, 'allow', None), (), ()) ```","I don't think that's related. If it's still happening, can you open a new issue with a repro?  ","Nevermind, df1c478ec52fb75ce88c06ab0133d9f5263c6767 already fixed the problem for me, I was just on an outdated checkout during my test"
2137,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False)， 内容是 (Conversions for XlaScatter are currently unsupported when using `enabled_xla=False`. I'm wondering if support could be added? Here's the full error that I'm seeing: ``` NotImplementedError                       Traceback (most recent call last) /usr/local/lib/python3.7/distpackages/jax/experimental/jax2tf/impl_no_xla.py in op(*arg, **kwargs)      52       53   def op(*arg, **kwargs): > 54     raise _xla_disabled_error(name)      55       56   return op NotImplementedError: Call to scatter add cannot be converted with enable_xla=False. ``` Here is some code that reproduces this error: ``` !pip install upgrade flax !pip install git+https://github.com/josephrocca/transformers.git2 ``` ```python import jax from jax.experimental import jax2tf from jax import numpy as jnp import numpy as np import tensorflow as tf from transformers import FlaxCLIPModel clip = FlaxCLIPModel.from_pretrained(""openai/clipvitbasepatch32"") def score(pixel_values, input_ids, attention_mask):     pixel_values = jax.image.resize(pixel_values, (3, 224, 224), ""nearest"")     inputs = {""pixel_values"":jnp.array([pixel_values]), ""input_ids"":input_ids, ""attention_mask"":attention_mask}     outputs = clip(**inputs)     return outputs.logits_per_image[0][0][0] score_tf = jax2tf.convert(jax.grad(score), enable_xla=False) my_model = tf.Module() my_model.f = tf.function(score_tf, autograph=False, jit_compile=True, input_signature=[   tf.TensorSpec([3, 40, 40], tf.float32),   tf.TensorSpec([1, 30], tf.int32),   tf.TensorSpec([1, 30], tf.int32), ]) model_name = 'pixel_text_score_grad' tf.saved_model.save(my_model, model_name, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True)) ``` Here's a public colab with that code: https://colab.research.google.com/drive/1HjpRXsa8Ue9KWiKbVVWUX6DlXoIYx2r8?usp=sharing You can click ""Runtime > Run all"" to see the error. Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,[jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False,"Conversions for XlaScatter are currently unsupported when using `enabled_xla=False`. I'm wondering if support could be added? Here's the full error that I'm seeing: ``` NotImplementedError                       Traceback (most recent call last) /usr/local/lib/python3.7/distpackages/jax/experimental/jax2tf/impl_no_xla.py in op(*arg, **kwargs)      52       53   def op(*arg, **kwargs): > 54     raise _xla_disabled_error(name)      55       56   return op NotImplementedError: Call to scatter add cannot be converted with enable_xla=False. ``` Here is some code that reproduces this error: ``` !pip install upgrade flax !pip install git+https://github.com/josephrocca/transformers.git2 ``` ```python import jax from jax.experimental import jax2tf from jax import numpy as jnp import numpy as np import tensorflow as tf from transformers import FlaxCLIPModel clip = FlaxCLIPModel.from_pretrained(""openai/clipvitbasepatch32"") def score(pixel_values, input_ids, attention_mask):     pixel_values = jax.image.resize(pixel_values, (3, 224, 224), ""nearest"")     inputs = {""pixel_values"":jnp.array([pixel_values]), ""input_ids"":input_ids, ""attention_mask"":attention_mask}     outputs = clip(**inputs)     return outputs.logits_per_image[0][0][0] score_tf = jax2tf.convert(jax.grad(score), enable_xla=False) my_model = tf.Module() my_model.f = tf.function(score_tf, autograph=False, jit_compile=True, input_signature=[   tf.TensorSpec([3, 40, 40], tf.float32),   tf.TensorSpec([1, 30], tf.int32),   tf.TensorSpec([1, 30], tf.int32), ]) model_name = 'pixel_text_score_grad' tf.saved_model.save(my_model, model_name, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True)) ``` Here's a public colab with that code: https://colab.research.google.com/drive/1HjpRXsa8Ue9KWiKbVVWUX6DlXoIYx2r8?usp=sharing You can click ""Runtime > Run all"" to see the error. Thanks!",2022-02-22T11:43:53Z,enhancement,closed,0,35,https://github.com/jax-ml/jax/issues/9659,"Note that, as documented in this issue, the conversion works fine if I'm just converting the `score` function. But in the above example I'm converting `jax.grad(score)`. This is the only difference. (Aside: I'm not sure if it's common to want to convert the gradient of a function to TF, but my motivation here is that, as an initial proof of concept of getting nontrivial JAX stuff working in browsers, I'm trying to get VQGAN+CLIP tuning (or something similar) working on the web. This means I need the gradient of the imagetext similarity score.)"," have you considered just converting the original function, and then doing the gradient computation in the TFLite code? jax2tf supports differentiation of the converted function from Tensorflow if the flag `with_gradient=True` (which is is by default, see here). When you run differentiation through TF, the gradient will be computed by JAX. See saved model and differentiation for more details."," Hey Marc, thanks for your reply. My goal here is to create a simple pipeline to get JAX functions working in the browser, which means that I'm currently limited to 3 runtimes: tfjstflite (the browser version of tflite), tfjs, and ONNX Runtime Web. There are conversion tools that can get me from JAX to any of these 3 formats (well, assuming op coverage in the converters and runtimes is sufficient). I might be wrong here, but I don't think it's possible to get any of these runtimes to give me the gradients for a pretrained model? tfjstflite has a subset of the functionality of tflite, but I don't think it's even possible to get the regular tflite runtime to give you gradients? That is, if I'm reading this thread correctly. But it occurred to me just now that maybe you meant that I can (1) convert the function from JAX to TF, then (2) compute the gradient of the function with TF, and then (3) export that new function to tfjstflite/tflite/onnx? If so, do you know how (2) would work (considering the code example in my original post) in terms of which TF APIs to use? I'm not familiar with TF because it's just an intermediate conversion step for me, but perhaps I'll have to learn it.", I'm also getting a scatterrelated error when trying to use `jax2tf` on VQGAN: ``` NotImplementedError: CLIP scatter mode not implemented in jax2tf ``` and this error when trying to use `jax2tf` on DALLEmini (a BARTlike model): ``` NotImplementedError: Call to scatter cannot be converted with enable_xla=False. ``` I'm not using `jax.grad` on these models  just trying to directly convert the models themselves. The full errors are in the last two cells of this notebook: https://colab.research.google.com/drive/1wf4JIDFL2kEawDRgrcTz2KDYXYvF_J88,"Thanks for the details ! I'm still working on providing better support for the gather op ( CC([jax2tf] NotImplementedError: Call to gather cannot be converted with enable_xla=False)), once I've finished that I will work on `scatter`.","This may be useful. I recently had to for a set of simple scatters, however I found another route so closed the request. Please check first, but it did function correctly for a subset of cases. https://github.com/google/jax/pull/9289","Hey , , just thinking about whether I should try another route to unblock myself here1, and so I was wondering if this is likely/unlikely to be fixed any time soon (e.g. within a few months, say)? I completely understand if not! Just don't want to start work on another route if this is likely to be fixed soon. Please forgive the ping! 🙏 1 For reference, I'm specifically trying to get Google's LiT gradients working in the browser at this point, since it seems better than CLIP for my use case. `jax2tf` works great for almost all models I throw at it, but converting `jax.grad(fn)` seems to always fail due to lack of scatter add support.",Do you know the line in the code where the scatter add occurs?," Implementing the `XlaScatter` support with `enable_xla=False` means implementing the functionality of scatter using core TF ops. An example for gather is here. Both scatter and gather are complicated, so the strategy is to implement the most common use cases first.  Would you be interested in contributing some support for scatter?"," I believe the scatter add is generated by `jax.grad`. The jax2tf conversion works fine on the LiT model, but not on its derivative. If you're curious, here's a notebook that minimally replicates the error for the LiT model. You can just click `Runtime > Run all` to generate the error (or just scroll down and read the error that I've saved saved from a previous run). One workaround that I thought might work for people trying to get jax2tf working for gradients of models is to generate the gradients in TensorFlow using GradientTape. So instead of this: ```py def score_images(variables, images):   zimg, _, _ = lit_model.apply(variables, images=images)   return zimg.sum()  This is just a dummy score for now. lit_variables_tf = tf.nest.map_structure(tf.Variable, lit_variables) score_images_grad_tf = lambda images: jax2tf.convert(jax.grad(score_images), enable_xla=False, polymorphic_shapes=[None, ""(b, ...)""])(lit_variables_tf, images) my_model = tf.Module() my_model._variables = tf.nest.flatten(lit_variables_tf) my_model.f = tf.function(score_images_grad_tf, autograph=False, jit_compile=True, input_signature=[   tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32, name=""images""), ]) model_name = 'score_images_grad' tf.saved_model.save(my_model, model_name, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True))  Results in NotImplementedError: Call to scatter add cannot be converted with enable_xla=False. ``` you'd use something like this (i.e. `jax2tf.convert(score_images, ...)` instead of `jax2tf.convert(jax.grad(score_images), ...)` and then create `score_images_tf_grad` using `GradientTape`): ```py def score_images(variables, images):   zimg, _, _ = lit_model.apply(variables, images=images)   return zimg.sum()  This is just a dummy score for now. lit_variables_tf = tf.nest.map_structure(tf.Variable, lit_variables) score_images_tf = lambda images: jax2tf.convert(score_images, enable_xla=False, polymorphic_shapes=[None, ""(b, ...)""])(lit_variables_tf, images) def score_images_tf_grad(images):   with tf.GradientTape() as tape:     score = score_images_tf(images)   grads = tape.gradient(score, images)   return grads my_model = tf.Module() my_model._variables = tf.nest.flatten(lit_variables_tf) my_model.f = tf.function(score_images_tf_grad, autograph=False, jit_compile=True, input_signature=[   tf.TensorSpec(shape=[None, 224, 224, 3], dtype=tf.float32, name=""images""), ]) model_name = 'score_images_tf_grad' tf.saved_model.save(my_model, model_name, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True)) ``` The problem with that is that is that the resulting graph seems to be missing the input node (when I look at it with Netron), and when I try to convert it to TFLite it gives this error: ""ValueError: Only support at least one signature key."" It's very possible that I'm making a simple mistake there, because I have basically zero experience with TensorFlow. Here's a minimal notebook with the above code. > : Would you be interested in contributing some support for scatter? I don't think I have the technical chops to be able to do that on my own, but I would definitely be able to give it a shot if I had some dotpoint guidance on what needs to be done to achieve it  a few dot points from a pro would likely save me several days (I am just a humble web dev, tending to my HTML and CSS by day). It'd be useful to know how many days this would take a pro so I can calibrate and think about if it's worth it given the time cost. Can I assume from your comment that it's not a high priority for your team right now  i.e. support for scatter add is unlikely to be added by the team any time soon unless someone else adds it?","Perhaps I am missing something, but if you try to use `tape.gradient` on a function returned by `jax2tf.convert` you will get the same behavior as if you `jax2tf.convert(jax.grad)`, so I don't quite see how you are avoiding the problem you are running into."," It's more likely that I'm missing something here, but to summarise: The problem is that `jax2tf.convert(f)` works fine, but `jax2tf.convert(jax.grad(f))` doesn't due to lack of scatter add conversion. So my idea was to do `jax2tf.convert(f)`, and then get the gradient using TensorFlow's `GradientTape`, and then convert to SavedModel from there. Would the latter approach still require a scatteradd implementation in `jax2tf`? It didn't throw any error in this notebook, but as mentioned above, it's for some reason missing the input node to the network when I export it to a SavedModel. But I just now saw  has opened a pull request that might fix the lack of scatter add support in jax2tf 👀😮","I have implemented a subset of xlafree scatter which covers your case. Adding  ```python !git clone https://github.com/oliverdutton/jax.git && cd jax && git checkout no_xla_scatter_single_dim_simple && pip install e . ``` to your colab does now successfully save the model and checking with ```python key = jax.random.PRNGKey(0) images = jax.random.uniform(key, (7, 224,224,3), ) jax_output = jax.grad(score_images)(lit_variables, images) images_tf = tf.constant(images) tf_output = score_images_grad_tf(images_tf) jax.tree_map(lambda x,y: jnp.abs(x.numpy()y).max(), tf_output, jax_output) diffs = jax.tree_map(lambda x,y: jnp.abs(x.numpy()y).max(), tf_output, jax_output) jnp.array(jax.tree_flatten(diffs)[0]).max() ``` does match with a max abs diff 0.0001213 which seems within tolerance This doesn't solve the serving problem which is separate from this, I'd take a look at https://blog.tensorflow.org/2021/03/atourofsavedmodelsignatures.html  PS: Whether this works for any other cases is up for debate. I've tried to add asserts to guard against incorrect usage, but temporarily turned them off due to an issue, hopefully I can add them back in. But do not take it for granted this will work for any other models","  https://colab.research.google.com/drive/1ePrAXbnL4QhtHVCJKHC9VHZsXzngIZXR?usp=sharing has a serving signature so converts successfully to tflite.  Notes:   The input shape is concrete, I set the batch dimension to 4  The `lit_variables` have been kept as a function input even in the tf version  To make the `lit_variables` a valid tf saved_model input they've been flattened, with names based on the params dict names Please check this works for you, and that the tflite model is valid."," This is brilliant! Thank you! I'm checking the validity of the tflite file now. It seems a bit unusual that the `.tflite` file is 125mb, given that (unless I'm misunderstanding), the parameters aren't saved in it. IIUC, I'll need to save the parameters as a separate file and then load them and feed them in as the first argument during inference. 125mb seems too small to contain the actual parameters, but too big to just be due to having a huge number of nodes. That said, it seems to have too many nodes for Netron to render (so I'm guessing >100k, since I've rendered ~30k nodes in it before). Might there be something unusual happening which is causing the node count to blow up? Or is this just an inherent property of the networks generated by `jax.grad`? Also, do you know where issues should be filed to make this process easier for future users? In terms of not having to do the dot points in your previous comment, I mean. I.e. should changes ideally be made to jax2tf, or TensorFlow (or both) to make it all work ""out of the box"" if someone just follows the instructions in the jax2tf readme?","File size  Parameters shouldn't be saved in it, beyond those for config of the network, unless I've screwed up  I could believe it being huge, have a look at the difference in lengths of jaxpr between score and grad(score) with jax.make_jaxpr to get a sense for this. To confirm, the grads here are against the 400 separate parameters of the model, not against the image. Is this what was intended? Where to document:  Possibly in jax2tf where tf.saved_model is described the 'trick' of flattening can be mentioned to make nested jax inputs compatible with tf. However some things like the naming of those parameters only exist because the model is the final bit of a haiku.Model, in general there won't be simple naming and params may not be a simple dict, etc. etc. Other bits like serving signatures are completely separate from jax. **Update:**  Compare the sizes of the `score_images` (val) and `score_images_grad` (grad):  It's the tf saved model to tflite that just comes up horrifically large (1000x larger for grad, rather than 3x), not JAX or pure TF."," Thanks again for your help and investigations! I've filed an issue here about the unusual tflite size and they've replicated and are looking into it. I've also filed an issue here about the tflite file not loading with `tfjstflite` (the web runtime for tflite). Regarding documenting the flattening trick, is this something that could instead be handled internally by jax2tf, so the user doesn't need to do this trick? Or is that not really possible for some reason?","The users needs to be aware that their input signature has changed to something flat, and ideally they need to name it in a way that makes sense. They just define the model and a way to pass args to it, which can be tricky when you’re using it in a number of platforms/languages.  There could be a utility function left around though, which people are advised to use which wraps f and inputs."," I see, so it's just a fundamental problem of TensorFlow not allowing nested inputs. Is it also the case that it's not possible to solve this ""behind the scenes"" in jax2tf if we're talking about params stored in the `_variables` property of the `tf.Module`, rather than params passed as inputs to the model? It does seem kinda strange from my newbie perspective that SavedModel can't handle nested variables and inputs  I'm guessing it's more of a historical tensorflow thing rather than a technical limitation? Also, I'm wondering about the case where the params are stored as constants in the network rather than passed in as inputs or stored as SavedModel variables? There's not enough scatter support to get Flax CLIP working (notebook), so I used GradientTape to get the gradient with TF instead, and I'm storing the params as constants in the network (the goal is to tune the image, like VQGAN+CLIP image generation, so the network params are constants), but that runs into the same problem of missing input nodes (notebook). That seems strange because there are no variables, and the inputs aren't nested. > There could be a utility function left around though, which people are advised to use which wraps f and inputs. Going by your notebook's code, would the most simple example code to add to the jax2tf readme be something like this? ```python def your_fn(nested_data):    function that you want to convert to TensorFlow with jax2tf nested_data =  add an input data sample here flat_data = jax.tree_map(tf.constant, jax.tree_flatten(nested_data)[0]) flat_names = list(flatten(nested_data)) _, nested_data_structure = jax.tree_flatten(nested_data) def your_fn_flat(flat_data):   nested_data = jax.tree_unflatten(nested_data_structure, nested_data)   return your_fn(nested_data) your_fn_flat_tf = jax2tf.convert(your_fn_flat, enable_xla=False) my_model = tf.Module() my_model.f = tf.function(your_fn_flat_tf, autograph=False, jit_compile=True, input_signature=[   jax.tree_map(lambda x, name: tf.TensorSpec(x.shape, x.dtype, name=name), flat_data, flat_names), ]) model_name = 'your_fn_flat' tf.saved_model.save(my_model, model_name,  options=tf.saved_model.SaveOptions(experimental_custom_gradients=False), signatures=my_model.f.get_concrete_function()) ``` I think one problem with that is in cases like this: `list(flatten({'foo':{'bar':1}, 'foo_bar':2}))`, which returns `['foo_bar']`. It's also just kind of complicated, and I'm concerned that it would be hard for many people to use even if that were put in the jax2tf readme, especially if they have multiple inputs, and perhaps also need to use `_variables`. But maybe most users of jax2tf won't be as much of a noob as me? I'm not sure. I'm also wondering if the `signatures=my_model.f.get_concrete_function()` bit is necessary in `tf.saved_model.save`, since we've already defined `input_signature` during the `tf.function` creation? Strangely, if I add it to the CLIP notebook that I linked above, then I get this error during ``tf.saved_model.save``: ``` ValueError: Got a nonTensor value  for key 'output_0' in the output of the function __inference_score_tf_grad_95495 used to generate the SavedModel signature 'serving_default'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor. ```",">  I see, so it's just a fundamental problem of TensorFlow not allowing nested inputs. Is it also the case that it's not possible to solve this ""behind the scenes"" in jax2tf if we're talking about params stored in the `_variables` property of the `tf.Module`, rather than params passed as inputs to the model? It does seem kinda strange from my newbie perspective that SavedModel can't handle nested variables and inputs  I'm guessing it's more of a historical tensorflow thing rather than a technical limitation? It's not a fundamental problem of tensorflow, it's due to the design decision in tensorflow serving signatures. If you run jax2tf then call the fn in tensorflow with the variables still not nested it will function fine. It's saving with a serving signature that hits you.  See https://www.tensorflow.org/tfx/serving/signature_defs. > Also, I'm wondering about the case where the params are stored as constants in the network rather than passed in as inputs or stored as SavedModel variables? There's not enough scatter support to get Flax CLIP working (notebook), so I used GradientTape to get the gradient with TF instead, and I'm storing the params as constants in the network (the goal is to tune the image, like VQGAN+CLIP image generation, so the network params are constants), but that runs into the same problem of missing input nodes (notebook). That seems strange because there are no variables, and the inputs aren't nested. I am currently working on a fix that stops that `scatteradd` from ever happening, this allows for using the 'conventional' tflite pathway with folding the params. CC([jax2tf] Expand gathers and scatters to slices and dynamic slices where possible)  ```python import functools serving_func = functools.partial(jax.grad(score_images), lit_variables) x_input = jnp.zeros((4,224,224,3)) converter = tf.lite.TFLiteConverter.experimental_from_jax(     [serving_func], [[('input1', x_input)]]) tflite_model = converter.convert() with open('score_model_grad.tflite', 'wb') as f:   f.write(tflite_model) ``` > I think one problem with that is in cases like this: `list(flatten({'foo':{'bar':1}, 'foo_bar':2}))`, which returns `['foo_bar']`. It's also just kind of complicated, and I'm concerned that it would be hard for many people to use even if that were put in the jax2tf readme, especially if they have multiple inputs, and perhaps also need to use `_variables`. But maybe most users of jax2tf won't be as much of a noob as me? I'm not sure. Yep, it's problematic and hacky fix. > I'm also wondering if the `signatures=my_model.f.get_concrete_function()` bit is necessary in `tf.saved_model.save`, since we've already defined `input_signature` during the `tf.function` creation? Strangely, if I add it to the CLIP notebook that I linked above, then I get this error during `tf.saved_model.save`: > ``` > ValueError: Got a nonTensor value  for key 'output_0' in the output of the function __inference_score_tf_grad_95495 used to generate the SavedModel signature 'serving_default'. Outputs for functions used as signatures must be a single Tensor, a sequence of Tensors, or a dictionary from string to Tensor. > ``` I have no idea what's happening there, looks like you're passing in an op rather than tensor which seems very wrong.","Looked at CLIP, it's a great case to have, I hadn't considered when the update was scalar I'll add to the tests. There's trivial update, [0.] > [1.] by indexing at [0] which I've fixed.  ```python NotImplementedError: Call to scatter add cannot be converted with enable_xla=False. > /raid/app/oliver/repos/jax2/jax/jax/experimental/jax2tf/impl_no_xla.py(56)op()      54       55   def op(*arg, **kwargs): > 56     raise _xla_disabled_error(name)      57       58   return op ipdb> u > /raid/app/oliver/repos/jax2/jax/jax/experimental/jax2tf/jax2tf.py(826)invoke_impl()     824     def invoke_impl() > TfVal:     825       if impl_needs_avals: > 826         return impl(     827             *args_tf,     828             _in_avals=args_avals,   type: ignore ipdb> args_tf [, , ] ``` With that solved, theres a further scatteradd which is complex, scatter_indices aren't 1D and so are not unique. That would require very awkward rules to cover, I leave you to extend coverage to that yourself. The exact syntax you need to support in JAX is below. If you convert the inputs to tf you can play with tf functions like `tf.math.unsorted_segment_sum` to support it . ```python key = jax.random.PRNGKey(42) operand, updates = [jax.random.uniform(key, shape) for shape in [(3, 224, 40), (3, 224, 224)]] scatter_indices = (jax.random.uniform(key, (224,1))*40).astype(int) dimension_numbers = jax.lax.ScatterDimensionNumbers((0,1), (2,), (2,)) jax.lax.scatter_add(operand, scatter_indices, updates, dimension_numbers) ```"," Really awesome work on add scatter support, thanks a lot! I just reviewed your PR.  Please note (maybe you are aware of this already) that the problems you encounter with the SavedModel signature only occurs when you wrap your function in `jax.grad`. I am not sure why this happens, and it doesn't happen when using other JAX transformations (e.g., `jit` or `vmap`). I think we should try to understand why this happens in a simple example and see whether we can fix it, because perhaps there is a bug in jax2tf where we are not converting the dict from JAX to TF correctly. Do you have any thoughts? If not, I will file an issue for this. Actually if you want to convert to TFLIte, I don't see why you would convert to SavedModel in the first place. Instead, a much easier road is to convert JAX > TF > TFLite directly using the TFLite converter. I have modified you ViT example and converted it to TFLite using the TFLite converter, and `jax.grad` seems to work without problems: https://colab.research.google.com/drive/1cA2bmWYwMSJNGMrdYtMzaz8l7YrRtpQ?usp=sharing Also, it is not entirely clear to me what you are trying to do. It seems you want to run either ViT or CLIP on the web, but still you are using TFLite. I think this is because you want to use the tfjstflite converter, right? Why? I believe that tool is still experimental, and it seems to me you have better changes using TFjs directly instead.  Finally, some of my colleagues recently published a blog post containing a live LiT demo that was built using jax2tf with `enable_xla=False`, by first converting JAX > TF SavedModel, and then TF SavedModel > TFjs. It may be of interest to you since it seems you are trying to do something quite similar (although I am not entirely sure). I have helped them somewhat and we are planning to write a blog post about this soon. I know this conversion path is still quite rough, but I am really glad you are trying this since it is efforts like this that help us figuring out what is unclear!"," Thanks again for your guidance and fixes! Looking forward to those PRs landing.  Thanks for your comments Marc! Replies below. > I think we should try to understand why this happens in a simple example and see whether we can fix it, because perhaps there is a bug in jax2tf where we are not converting the dict from JAX to TF correctly. Do you have any thoughts? If not, I will file an issue for this. I think Oliver understands this better than me, but from his comments my understanding is that there were three issues: (1) Lack of some necessary scatter support ( CC([jax2tf] add rudimentary scatter support for enable_xla=False)). (2) Some scatters being introduced where they weren't actually necessary ( CC([jax2tf] Expand gathers and scatters to slices and dynamic slices where possible)). (3) SavedModel signatures not being able to be nested/PyTrees. But I think you're right  if (3) is a problem, then why does it only occur for the `grad` of the function? I also noticed that it occurs in this notebook despite there being no nesting in the inputs or outputs (and no `_variables` either  params are stored as constants in the network). > Actually if you want to convert to TFLIte, I don't see why you would convert to SavedModel in the first place. Instead, a much easier road is to convert JAX > TF > TFLite directly using the TFLite converter. 🤦 I didn't realise that the TFLite converter had a `from_concrete_functions` method, thank you! I guess that skips the SavedModel signature issue. One problem though: The resulting `vit.tflite` file from the notebook you linked is only 588kb, and according to Netron, only has only two notes  an input node and an output node. So something seems to be silently going wrong during the conversion? I did notice that there are a few (notsuperinformative) warnings. > I think this is because you want to use the tfjstflite converter, right? Why? I believe that tool is still experimental, and it seems to me you have better changes using TFjs directly instead. There are currently 3 viable runtimes for getting ML models working on the web  ONNX Runtime Web, tfjstflite, and tfjs. Unfortunately they all suffer from patchy op support (I think it's just hard for them to keep up  hopefully WebNN and/or ModelLoader will fix this). I did try tfjs and filed a Github issue for the required op support (1). I also tried ONNX (which usually has the best op coverage  at least in its Wasm backend), but I think that failed due to the input/output signature stuff. tfjstflite seems to have the required ops and with Oliver's signature flattening trick I did manage to get it to produce a model (although it's 30x larger than it should be  Oliver posted a theory in that thread), but I'm still waiting to hear back from the tfjstflite team about an opaque initialization error. But at the moment it may be that without Oliver's ""signature flattening"" trick, TFLite is the only option (assuming we get `from_concrete_functions` working properly RE the 588kb model output), since, IIUC, both ONNX and tfjs require using SavedModel as an intermediary, and as discussed, JAX > TF > SavedModel currently produces a network without an input node due to the seeminglynestingrelated SavedModel signature problem. > Finally, some of my colleagues recently published a blog post containing a live LiT demo I did see that! I figured that thanks to that work I wouldn't have to do much work on the text side of things (though I'm not sure if the browserported tokenizer is opensourced on Github yet?). AFAIK they didn't port the image model (and instead precomputed the image embeddings for a set number of images), but luckily the LiT model seems to convert easily  it's just the introduction of `jax.grad` that has given rise to the scatter and SavedModel signature issues. In case you're curious, the initial goal here is to add an option to use LiT (instead of CLIP) in this browser demo, and also to see how CLIP and LiT compare in gradientbased tuning/generation of images in the browser (in particular, collaborative tuning over WebRTC). > I know this conversion path is still quite rough, but I am really glad you are trying this since it is efforts like this that help us figuring out what is unclear! Thank you for your work on making jax2tf! I am very excited about getting JAX models working in the browser, and I'm dreaming of the day that I can just do *everything* in JAX and then port it to the browser with a conversion process that ""just works"". In my wildest dreams I run JAX models directly in the browser. There are some fearless people (like ) over in the PyScript/Pyodide ecosystem who are working on getting PyTorch and TensorFlow working in the browser, so I'm counting that as my license to dream about JAX running in the browser 😅",">Thank you for your work on making jax2tf! I am very excited about getting JAX models working in the browser, and I'm dreaming of the day that I can just do everything in JAX and then port it to the browser with a conversion process that ""just works"". In my wildest dreams I run JAX models https://github.com/google/jax/issues/1472. We thank you for your interest and patience."," Summary: [[jax2tf] NotImplementedError: Call to scatter add cannot be converted with enable_xla=False](https://github.com/google/jax/issues/9659) began as supporting XLA free scatters. XLA free scatters now supported, so this ticket is theoretically closed. However the root issue was the ability to run tflite models with the grad of LiT and CLIP. The way the scatters ended up in these models prompted me to solve an issue I've had before in CC([jax2tf] Expand gathers and scatters to slices and dynamic slices where possible) so the scatter simply doesn't appear anyhow. So two things have happened: one pull request which makes scatters xlafree, and another which avoids most scatters appearing to begin with. So two tasks: 1  grad of LiT in tflite 2  grad of CLIP in tflite  LiT grad For LiT this notebook does build the tflite model (which does end up huge, 893MB, of which ~130MB is the model with rest params) and runs, matching jax output with a reasonable tolerance. Model file being 130Mb (without params) makes sense to me as some of the grads on the params are zeros. They’re materialised in the tflite file to read straight away as they have no dependency on input. So, in my view tflite is working exactly as intended and that file size is optimal given tflites design choice to materialise arrays.  **Do you actually want the grads on the params, or do you want them on the image input?**  CLIP grad CLIP is ongoing, zero grads are observed in the tflite model while they are non zero in the jax version.","CLIP grads working correctly here notebook, though I've used a matching image size straight away. Please experiment with it and see if the resize steps are whats causing the issues.  Are you happy with not having the grads on unresized image?","  I feel that we are having multiple conversation on this issue and it is a bit difficult for me to keep track. I think we can separate the following four issues: 1. Add support for scatter when `enable_xla=False`. Main discussion in this issue.     * ==> Suggestion: close this issue once 's PR is in. 2. Converting LiT to TFLite when `enable_xla=False`.      * ==> Suggestion: Seems to be working fine, so nothing to do. (To be confirmed by ). 3. Converting CLIP to TFLite when `enable_xla=False`.     * ==> Suggestion: Create a new issue for this. 4. Converting to SavedModel needs annoying parameter flattening.     * ==> Suggestion: Create a new issue for this. Does this match how you see it? ","Agreed for 1. For 2. and 3. using `tflite.experimental_from_jax` the route LiT/CLIP>tflite avoided jax2tf entirely (so no `enable_xla=False`) and required another PR ( CC([jax2tf] Expand gathers and scatters to slices and dynamic slices where possible)) which expanded the gather/scatter at jaxpr level (possibly a poor general decision, but a useful one here). Though the CC([jax2tf] add rudimentary scatter support for enable_xla=False) also fixes them if routed via jax2tf, which I wouldn't recommend. So two PR's, each fix them in different ways. 4. I'm hazy about.","Thanks . Why are you hazy about 4? I first like to understand better why we would need the flattening logic (hopefully we don't need this at all!). I filed an issue CC([jax2tf] Error when converting function wrapped in jax.grad to SavedModel format). Also, why wouldn't you recommend the path that uses jax2tf?","> Also, why wouldn't you recommend the path that uses jax2tf? For converting to tflite Route 1. jax2tf involves mapping jaxpr of the program to tensorflow ops, saving a saved model then loading and passing it with tflite converter. When `enable_xla=False` is used (which for tflite it's not required) this involves mapping to high level tensorflow ops. Route 2. `tflite.experimental_from_jax` uses the HLO itself, and doesn't save any intermediate files. While both are perfectly valid, the second seems like less possible pain points. Converting to ONNX however, means jax2tf `enable_xla=False` still has a specific use. (That's the one I know of, there are likely many other cases I'm not aware of)"
511,"以下是一个github上的jax下的一个issue, 标题是([QoL] Add copy button in docs code snippets)， 内容是 (Since I'm a bit lazy, I'd like to have a ""copy to clipboard"" button in jax docs to copy over code snippets instead of dragselectcopying it.  Like this: !image Dupicate Checks: Nothing relevant comes up when searching for ""copy button"", ""docs copy button"" or even ""button"" for that matter.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[QoL] Add copy button in docs code snippets,"Since I'm a bit lazy, I'd like to have a ""copy to clipboard"" button in jax docs to copy over code snippets instead of dragselectcopying it.  Like this: !image Dupicate Checks: Nothing relevant comes up when searching for ""copy button"", ""docs copy button"" or even ""button"" for that matter.",2022-02-22T09:44:39Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/9657,"After a bit of digging, I added a ""sphinxcopybutton"" to the requirements and `config.py`, built anew (without executing notebooks). And it seems to work. ",Just made a PR CC(Add copy button to code snippets in documentation) on this issue. I'm more than happy to correct stuff if something's wrong. Thank you ❤️ 
7247,"以下是一个github上的jax下的一个issue, 标题是(Indentation Errors During BUILD)， 内容是 (Please see the following error messages regarding the error during the build. The error disappears and the build is successful after manually fixing the BUILD file. Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```bash python build/build.py enable_cuda ```  [x] If applicable, include full error messages/tracebacks. ``` (build_jax) /export/share/software/jax> python build/build.py enable_cuda      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: /workspace/anaconda3/envs/build_jax/bin/bazel Bazel version: 5.0.0 Python binary path: /workspace/anaconda3/envs/build_jax/bin/python Python version: 3.8 NumPy version: 1.20.3 MKLDNN enabled: yes Target CPU: ppc64le Target CPU features: release CUDA enabled: yes CUDA compute capabilities: 3.5,5.2,6.0,7.0,8.0 NCCL enabled: yes TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... /workspace/anaconda3/envs/build_jax/bin/bazel run verbose_failures=true config=mkl_open_source_only config=cuda :build_wheel  output_path=/export/share/software/jax/dist cpu=ppc64le Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /export/share/software/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /export/share/software/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /export/share/software/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/workspace/anaconda3/envs/build_jax/bin/python action_env=PYENV_ROOT python_path=/workspace/anaconda3/envs/build_jax/bin/python distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /export/share/software/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /export/share/software/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /export/share/software/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true INFO: Found applicable config definition build:cuda in file /export/share/software/jax/.jax_configure.bazelrc: action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,7.0,8.0 INFO: Found applicable config definition build:linux in file /export/share/software/jax/.bazelrc: config=posix copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /export/share/software/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/102f5befef27a33dec2cab2600f9b2b4bf3e5036.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found DEBUG: /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: AutoConfiguration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default Loading: Loading: 1 packages loaded Analyzing: target //build:build_wheel (2 packages loaded, 0 targets configured) DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 0400"" DEBUG: Repository io_bazel_rules_docker instantiated at:   /export/share/software/jax/WORKSPACE:37:14: in    /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/org_tensorflow/tensorflow/workspace0.bzl:108:34: in workspace   /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories Repository rule git_repository defined at:   /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in  ERROR: /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/zip/BUILD:11:1: indentation error ERROR: /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/zip/BUILD:14:2: Trailing comma is allowed only in parenthesized tuples. ERROR: /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/zip/BUILD:14:3: syntax error at 'outdent': expected expression ERROR: /export/share/software/jax/build/BUILD.bazel:25:10: every rule of type py_binary implicitly depends upon the target '//tools/zip:zipper', but this target could not be found because of: no such target '//tools/zip:zipper': target 'zipper' not declared in package 'tools/zip' defined by /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/zip/BUILD ERROR: Analysis of target '//build:build_wheel' failed; build aborted: Analysis failed INFO: Elapsed time: 212.845s INFO: 0 processes. FAILED: Build did NOT complete successfully (52 packages loaded, 212 targets configured) ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully (52 packages loaded, 212 targets configured) b'' Traceback (most recent call last):   File ""build/build.py"", line 524, in      main()   File ""build/build.py"", line 519, in main     shell(command)   File ""build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/workspace/anaconda3/envs/build_jax/lib/python3.8/subprocess.py"", line 415, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/workspace/anaconda3/envs/build_jax/lib/python3.8/subprocess.py"", line 516, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['/workspace/anaconda3/envs/build_jax/bin/bazel', 'run', 'verbose_failures=true', 'config=mkl_open_source_only', 'config=cuda', ':build_wheel', '', 'output_path=/export/share/software/jax/dist', 'cpu=ppc64le']' returned nonzero exit status 1. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Indentation Errors During BUILD,"Please see the following error messages regarding the error during the build. The error disappears and the build is successful after manually fixing the BUILD file. Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```bash python build/build.py enable_cuda ```  [x] If applicable, include full error messages/tracebacks. ``` (build_jax) /export/share/software/jax> python build/build.py enable_cuda      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: /workspace/anaconda3/envs/build_jax/bin/bazel Bazel version: 5.0.0 Python binary path: /workspace/anaconda3/envs/build_jax/bin/python Python version: 3.8 NumPy version: 1.20.3 MKLDNN enabled: yes Target CPU: ppc64le Target CPU features: release CUDA enabled: yes CUDA compute capabilities: 3.5,5.2,6.0,7.0,8.0 NCCL enabled: yes TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... /workspace/anaconda3/envs/build_jax/bin/bazel run verbose_failures=true config=mkl_open_source_only config=cuda :build_wheel  output_path=/export/share/software/jax/dist cpu=ppc64le Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /export/share/software/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /export/share/software/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. INFO: Reading rc options for 'run' from /export/share/software/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/workspace/anaconda3/envs/build_jax/bin/python action_env=PYENV_ROOT python_path=/workspace/anaconda3/envs/build_jax/bin/python distinct_host_configuration=false INFO: Found applicable config definition build:short_logs in file /export/share/software/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /export/share/software/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /export/share/software/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0 crosstool_top=//crosstool:toolchain //:enable_cuda define=xla_python_enable_gpu=true INFO: Found applicable config definition build:cuda in file /export/share/software/jax/.jax_configure.bazelrc: action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,7.0,8.0 INFO: Found applicable config definition build:linux in file /export/share/software/jax/.bazelrc: config=posix copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /export/share/software/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++14 host_cxxopt=std=c++14 WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/102f5befef27a33dec2cab2600f9b2b4bf3e5036.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found DEBUG: /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10: AutoConfiguration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default Loading: Loading: 1 packages loaded Analyzing: target //build:build_wheel (2 packages loaded, 0 targets configured) DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1596824487 0400"" DEBUG: Repository io_bazel_rules_docker instantiated at:   /export/share/software/jax/WORKSPACE:37:14: in    /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/org_tensorflow/tensorflow/workspace0.bzl:108:34: in workspace   /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories Repository rule git_repository defined at:   /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in  ERROR: /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/zip/BUILD:11:1: indentation error ERROR: /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/zip/BUILD:14:2: Trailing comma is allowed only in parenthesized tuples. ERROR: /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/zip/BUILD:14:3: syntax error at 'outdent': expected expression ERROR: /export/share/software/jax/build/BUILD.bazel:25:10: every rule of type py_binary implicitly depends upon the target '//tools/zip:zipper', but this target could not be found because of: no such target '//tools/zip:zipper': target 'zipper' not declared in package 'tools/zip' defined by /workspace/anaconda3/envs/build_jax/share/bazel/66cc389786f6f90c4be41c4758c2d3a1/external/bazel_tools/tools/zip/BUILD ERROR: Analysis of target '//build:build_wheel' failed; build aborted: Analysis failed INFO: Elapsed time: 212.845s INFO: 0 processes. FAILED: Build did NOT complete successfully (52 packages loaded, 212 targets configured) ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully (52 packages loaded, 212 targets configured) b'' Traceback (most recent call last):   File ""build/build.py"", line 524, in      main()   File ""build/build.py"", line 519, in main     shell(command)   File ""build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/workspace/anaconda3/envs/build_jax/lib/python3.8/subprocess.py"", line 415, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/workspace/anaconda3/envs/build_jax/lib/python3.8/subprocess.py"", line 516, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['/workspace/anaconda3/envs/build_jax/bin/bazel', 'run', 'verbose_failures=true', 'config=mkl_open_source_only', 'config=cuda', ':build_wheel', '', 'output_path=/export/share/software/jax/dist', 'cpu=ppc64le']' returned nonzero exit status 1. ```",2022-02-21T15:44:42Z,bug needs info,closed,0,5,https://github.com/jax-ml/jax/issues/9653,"Another (loosely) related question: At the end of the build process, I see the following line ``` package init file 'jaxlib/xla_extension/__init__.py' not found (or not a regular file) ``` Is this considered a warning that can be safely ignored? Thanks in advance for the help!","I can't reproduce the original issue. What version of `jaxlib` are you building? What's the git commit or version you used? The `jaxlib/xla_extension/__init__.py` warning is benign, you can ignore that.","Thanks! The messages shown above is for `jaxlibv0.1.76`. However, I encountered similar issues for `jaxlibv0.3.0` and the `main` branch too. I'm building this on a `ppc64le` machine and uses `Bazel=5.0.0` installed through conda ``` conda install c condaforge y bazel=5.0.0 ``` Not sure if the latter could the cause though.",I'm wondering if it's ppc64le specific for some reason. I can't verify that easily because I don't have access to a ppc64le machine...,"That makes sense, thanks for letting me know!"
1662,"以下是一个github上的jax下的一个issue, 标题是(RuntimeError: UNKNOWN: PTX JIT compiler library not found)， 内容是 (I'm getting a weird error when trying to `jit` things: ``` Traceback (most recent call last):   File ""/home/ubuntu/dev/research/lottery/cifar10_convnet_run.py"", line 228, in      test_make_batcher()   File ""/home/ubuntu/dev/research/lottery/cifar10_convnet_run.py"", line 87, in test_make_batcher     for fn in [make_batcher(5, 2), jit(make_batcher(5, 2))]:   File ""/home/ubuntu/dev/research/lottery/cifar10_convnet_run.py"", line 79, in make_batcher     splits = list(jnp.arange(1, num_examples // batch_size + 1) * batch_size)   File ""/nix/store/ix119bzn54cgvsm98cidfdb4c8w0v1qcpython3.9jax0.3.0/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 6747, in deferring_binary_op     return binary_op(self, other) RuntimeError: UNKNOWN: PTX JIT compiler library not found in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_asm_compiler.cc(56): 'cuLinkCreate(0, nullptr, nullptr, &link_state)' ``` What does this mean? What .so am I missing? I can confirm that I have `libnvidiaptxjitcompiler.so`, `libnvidiaptxjitcompiler.so.1`, and `libnvidiaptxjitcompiler.so.495.44` all on my system. Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python from jax import jit jit(lambda x: 2*x) ```  [x] If applicable, include full error messages/tracebacks. https://gist.github.com/samuela/7245561c382842d70a729caecbd11912)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,RuntimeError: UNKNOWN: PTX JIT compiler library not found,"I'm getting a weird error when trying to `jit` things: ``` Traceback (most recent call last):   File ""/home/ubuntu/dev/research/lottery/cifar10_convnet_run.py"", line 228, in      test_make_batcher()   File ""/home/ubuntu/dev/research/lottery/cifar10_convnet_run.py"", line 87, in test_make_batcher     for fn in [make_batcher(5, 2), jit(make_batcher(5, 2))]:   File ""/home/ubuntu/dev/research/lottery/cifar10_convnet_run.py"", line 79, in make_batcher     splits = list(jnp.arange(1, num_examples // batch_size + 1) * batch_size)   File ""/nix/store/ix119bzn54cgvsm98cidfdb4c8w0v1qcpython3.9jax0.3.0/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 6747, in deferring_binary_op     return binary_op(self, other) RuntimeError: UNKNOWN: PTX JIT compiler library not found in external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_asm_compiler.cc(56): 'cuLinkCreate(0, nullptr, nullptr, &link_state)' ``` What does this mean? What .so am I missing? I can confirm that I have `libnvidiaptxjitcompiler.so`, `libnvidiaptxjitcompiler.so.1`, and `libnvidiaptxjitcompiler.so.495.44` all on my system. Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: ```python from jax import jit jit(lambda x: 2*x) ```  [x] If applicable, include full error messages/tracebacks. https://gist.github.com/samuela/7245561c382842d70a729caecbd11912",2022-02-19T22:21:00Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/9644,"What's your driver version, jaxlib version, cuda version? What command did you use to install jaxlib?","I ended up resolving this in a different way (https://discourse.nixos.org/t/usingcudaenabledpackagesonnonnixossystems/17788). I don't have bandwidth to reproduce and investigate but I found the error message unclear. For people coming across this in the future, I would suggest trying with `LD_DEBUG=libs`."
530,"以下是一个github上的jax下的一个issue, 标题是(Revert back to adding aval on Device buffers inside local_shards and convert the cached property to just the normal property. )， 内容是 (Revert back to adding aval on Device buffers inside local_shards and convert the cached property to just the normal property.  This slows down the pjit path because now you are paying the cost to create avals during runtime.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Revert back to adding aval on Device buffers inside local_shards and convert the cached property to just the normal property. ,Revert back to adding aval on Device buffers inside local_shards and convert the cached property to just the normal property.  This slows down the pjit path because now you are paying the cost to create avals during runtime.,2022-02-18T21:29:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9638
1381,"以下是一个github上的jax下的一个issue, 标题是(use singleton dims in broadcasting binop batchers)， 内容是 ((I think we used to have this optimization!) We can make vmap more efficient in eager mode by relying on singletonaxis broadcasting built into our broadcasting binop primitives. Before this change we might've generated this for a doublevmapped pairwisel1distances: ``` { lambda ; a:f32[20,3]. let     b:f32[20,20,3] = broadcast_in_dim[       broadcast_dimensions=(0, 2)       shape=(20, 20, 3)     ] a     c:f32[20,20,3] = broadcast_in_dim[       broadcast_dimensions=(1, 2)       shape=(20, 20, 3)     ] a     d:f32[20,20,3] = sub c b     e:f32[20,20,3] = abs d     f:f32[20,20] = reduce_sum[axes=(2,)] e   in (f,) } ``` but after this change we can do this: ``` { lambda ; a:f32[20,3]. let     b:f32[20,1,3] = broadcast_in_dim[       broadcast_dimensions=(0, 2)       shape=(20, 1, 3)     ] a     c:f32[1,20,3] = broadcast_in_dim[       broadcast_dimensions=(1, 2)       shape=(1, 20, 3)     ] a     d:f32[20,20,3] = sub c b     e:f32[20,20,3] = abs d     f:f32[20,20] = reduce_sum[axes=(2,)] e   in (f,) } ``` Notice the singleton sizes on intermediates! (I didn't actually wait for tests to finish before pushing this, so let's see what happens...))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,use singleton dims in broadcasting binop batchers,"(I think we used to have this optimization!) We can make vmap more efficient in eager mode by relying on singletonaxis broadcasting built into our broadcasting binop primitives. Before this change we might've generated this for a doublevmapped pairwisel1distances: ``` { lambda ; a:f32[20,3]. let     b:f32[20,20,3] = broadcast_in_dim[       broadcast_dimensions=(0, 2)       shape=(20, 20, 3)     ] a     c:f32[20,20,3] = broadcast_in_dim[       broadcast_dimensions=(1, 2)       shape=(20, 20, 3)     ] a     d:f32[20,20,3] = sub c b     e:f32[20,20,3] = abs d     f:f32[20,20] = reduce_sum[axes=(2,)] e   in (f,) } ``` but after this change we can do this: ``` { lambda ; a:f32[20,3]. let     b:f32[20,1,3] = broadcast_in_dim[       broadcast_dimensions=(0, 2)       shape=(20, 1, 3)     ] a     c:f32[1,20,3] = broadcast_in_dim[       broadcast_dimensions=(1, 2)       shape=(1, 20, 3)     ] a     d:f32[20,20,3] = sub c b     e:f32[20,20,3] = abs d     f:f32[20,20] = reduce_sum[axes=(2,)] e   in (f,) } ``` Notice the singleton sizes on intermediates! (I didn't actually wait for tests to finish before pushing this, so let's see what happens...)",2022-02-17T07:14:55Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9613
1881,"以下是一个github上的jax下的一个issue, 标题是(Index update operators: add scatter_apply())， 内容是 (In the course of exploring a JAX API for numpy's ufunc methods ( CC(jax.numpy: add ufuncstyle APIs for several jnp functions)), it became clear that there is one class of operation for which JAX doesn't currently have any highlevel API: namely the equivalent of `np.ufunc.at(idx)` for a unary ufunc. For example: ```python >>> import numpy as np >>> x = np.array([2, 2, 2, 2]) >>> np.square.at(x, [1, 2, 2, 3, 3, 3]) >>> x array([  2,   4,  16, 256]) ``` The semantics are that the unary operation is repeatedly applied at the specified indices, something that is not currently possible with JAX (e.g. `x.at[idx].set(func(x[idx]))` only applies the function once in the case of repeated indices). It turns out we can efficiently provide this functionality in JAX with a relatively thin wrapper around `scatter()`, accessed via `np.ndarray.at`, as is done in this PR: ```python >>> import jax.numpy as jnp >>> x = jnp.array([2, 2, 2, 2]) >>> idx = jnp.array([1, 2, 2, 3, 3, 3]) >>> x.at[idx].apply(jnp.square) DeviceArray([  2,   4,  16, 256], dtype=int32) ``` This seems like it may be a useful API to make available, though admittedly I don't have any specific application in mind. Why not allow an extra argument and extend this to binary ufuncs? Allowing this may cause confusion, because the semantics of `scatter()` are that updates can be applied in any order, and thus order of operations should not matter. This is not true for general binary functions (which is why we only implement specific binary ops in `scatter_add`, `scatter_mul`, etc.), but it is true of unary functions, so we can provide this more general implementation for the unary case.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Index update operators: add scatter_apply(),"In the course of exploring a JAX API for numpy's ufunc methods ( CC(jax.numpy: add ufuncstyle APIs for several jnp functions)), it became clear that there is one class of operation for which JAX doesn't currently have any highlevel API: namely the equivalent of `np.ufunc.at(idx)` for a unary ufunc. For example: ```python >>> import numpy as np >>> x = np.array([2, 2, 2, 2]) >>> np.square.at(x, [1, 2, 2, 3, 3, 3]) >>> x array([  2,   4,  16, 256]) ``` The semantics are that the unary operation is repeatedly applied at the specified indices, something that is not currently possible with JAX (e.g. `x.at[idx].set(func(x[idx]))` only applies the function once in the case of repeated indices). It turns out we can efficiently provide this functionality in JAX with a relatively thin wrapper around `scatter()`, accessed via `np.ndarray.at`, as is done in this PR: ```python >>> import jax.numpy as jnp >>> x = jnp.array([2, 2, 2, 2]) >>> idx = jnp.array([1, 2, 2, 3, 3, 3]) >>> x.at[idx].apply(jnp.square) DeviceArray([  2,   4,  16, 256], dtype=int32) ``` This seems like it may be a useful API to make available, though admittedly I don't have any specific application in mind. Why not allow an extra argument and extend this to binary ufuncs? Allowing this may cause confusion, because the semantics of `scatter()` are that updates can be applied in any order, and thus order of operations should not matter. This is not true for general binary functions (which is why we only implement specific binary ops in `scatter_add`, `scatter_mul`, etc.), but it is true of unary functions, so we can provide this more general implementation for the unary case.",2022-02-17T00:40:38Z,pull ready,closed,1,1,https://github.com/jax-ml/jax/issues/9607,"So now a boolean mask is casted to `int` right? ``` >>> x.at[jnp.array([True, False, False, True])].apply(jnp.square) DeviceArray([4, 2, 2, 4], dtype=int32) ``` I guess when `idx.dtype == bool`, `a.at[idx].apply(f)` can be lowered into `jnp.where`? This doesn't match the `np.ufunc.at` interface, but does match the rest of the `jax.numpy.at` interface."
1049,"以下是一个github上的jax下的一个issue, 标题是(JAX arrays should expose itemsize property (bug?))， 内容是 (Similar to issue CC(JAX arrays should implement nbytes), in which JAX arrays were made to mimic NumPy's `numpy.ndarray.nbytes` property, they should also mimic the `numpy.ndarray.itemsize` property.  Note that JAX **does** already expose `itemsize` in its dtypes.   This unexpected lack of parity with NumPy might imply this is more like a bug than an enhancement. ```python import numpy as onp import jax.numpy as jnp a = onp.ones(0, dtype='ushort') ja = jnp.ones_like(a) print(f'{a.itemsize = }') print(f'{a.dtype.itemsize = }') if hasattr(ja, 'itemsize'):     print(f'{ja.itemsize = }') else:     print(""jax array does not define itemsize."") print(f'{ja.dtype.itemsize = }') ``` currently outputs ``` a.itemsize = 2 a.dtype.itemsize = 2 jax array does not define itemsize. ja.dtype.itemsize = 2 ``` Thanks in advance!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JAX arrays should expose itemsize property (bug?),"Similar to issue CC(JAX arrays should implement nbytes), in which JAX arrays were made to mimic NumPy's `numpy.ndarray.nbytes` property, they should also mimic the `numpy.ndarray.itemsize` property.  Note that JAX **does** already expose `itemsize` in its dtypes.   This unexpected lack of parity with NumPy might imply this is more like a bug than an enhancement. ```python import numpy as onp import jax.numpy as jnp a = onp.ones(0, dtype='ushort') ja = jnp.ones_like(a) print(f'{a.itemsize = }') print(f'{a.dtype.itemsize = }') if hasattr(ja, 'itemsize'):     print(f'{ja.itemsize = }') else:     print(""jax array does not define itemsize."") print(f'{ja.dtype.itemsize = }') ``` currently outputs ``` a.itemsize = 2 a.dtype.itemsize = 2 jax array does not define itemsize. ja.dtype.itemsize = 2 ``` Thanks in advance!",2022-02-16T07:34:45Z,enhancement,closed,0,0,https://github.com/jax-ml/jax/issues/9593
2014,"以下是一个github上的jax下的一个issue, 标题是(jit-compilable ""apply mask""-operation)， 内容是 (Working with masks is quite frustrating in jitcompiled functions and i would be amazing to have a function such that ```python .jit def f(x, mask):     assert x.shape == mask.shape      where mask is a booleanarray      masked_x = x[mask]     masked_x = jnp.apply_mask(x, mask) ``` this would be possible. This is the underlying problem why i require this. ```python from jax import random  import jax import jax.numpy as jnp def biject_alpha(idx, val):     alpha, i, key = val      hyperparameters for randomisation     cdf_bins=20     key, consume = random.split(key)     samples = random.uniform(consume, (cdf_bins1,), maxval=1.0)     samples = jnp.hstack((jnp.array([0.0]), samples))     montonous = jnp.cumsum(samples)     cdf = montonous/montonous[1]      idx of first occurenece     mask = (i1) == idx     start_idx = jnp.select(mask, jnp.arange(len(mask)))      biject slice      TODO: Issue is in next line     alpha_i = alpha[mask]     bijected_alpha = cdf[(alpha_i//(1/cdf_bins)).astype(int)]     alpha = jax.lax.dynamic_update_slice(alpha, bijected_alpha, start_idx)     return (alpha, i, key) def cosInterpolateRandomized(x, xp, fp, key):     assert x.ndim == xp.ndim == fp.ndim == 1     assert xp.shape == fp.shape      assert x.dtype == xp.dtype == fp.dtype == jnp.float32     i = jnp.clip(jnp.searchsorted(xp, x, side='right'), 1, len(xp)  1)     dx = xp[i]  xp[i  1]     alpha = (x  xp[i  1])/dx      alpha, _, _ = jax.lax.fori_loop(0, len(xp)1, biject_alpha, (alpha, i, key))     def cos_interpolate(x1, x2, alpha):         """""" x2 > x1         """"""         return (x1+x2)/2 + (x1x2)/2 * jnp.cos(alpha*jnp.pi)     f = jnp.where((dx == 0), fp[i], jax.vmap(cos_interpolate)(fp[i1], fp[i], alpha))     f = jnp.where(x > xp[1], fp[1], f)     assert f.shape == x.shape     return f  ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"jit-compilable ""apply mask""-operation","Working with masks is quite frustrating in jitcompiled functions and i would be amazing to have a function such that ```python .jit def f(x, mask):     assert x.shape == mask.shape      where mask is a booleanarray      masked_x = x[mask]     masked_x = jnp.apply_mask(x, mask) ``` this would be possible. This is the underlying problem why i require this. ```python from jax import random  import jax import jax.numpy as jnp def biject_alpha(idx, val):     alpha, i, key = val      hyperparameters for randomisation     cdf_bins=20     key, consume = random.split(key)     samples = random.uniform(consume, (cdf_bins1,), maxval=1.0)     samples = jnp.hstack((jnp.array([0.0]), samples))     montonous = jnp.cumsum(samples)     cdf = montonous/montonous[1]      idx of first occurenece     mask = (i1) == idx     start_idx = jnp.select(mask, jnp.arange(len(mask)))      biject slice      TODO: Issue is in next line     alpha_i = alpha[mask]     bijected_alpha = cdf[(alpha_i//(1/cdf_bins)).astype(int)]     alpha = jax.lax.dynamic_update_slice(alpha, bijected_alpha, start_idx)     return (alpha, i, key) def cosInterpolateRandomized(x, xp, fp, key):     assert x.ndim == xp.ndim == fp.ndim == 1     assert xp.shape == fp.shape      assert x.dtype == xp.dtype == fp.dtype == jnp.float32     i = jnp.clip(jnp.searchsorted(xp, x, side='right'), 1, len(xp)  1)     dx = xp[i]  xp[i  1]     alpha = (x  xp[i  1])/dx      alpha, _, _ = jax.lax.fori_loop(0, len(xp)1, biject_alpha, (alpha, i, key))     def cos_interpolate(x1, x2, alpha):         """""" x2 > x1         """"""         return (x1+x2)/2 + (x1x2)/2 * jnp.cos(alpha*jnp.pi)     f = jnp.where((dx == 0), fp[i], jax.vmap(cos_interpolate)(fp[i1], fp[i], alpha))     f = jnp.where(x > xp[1], fp[1], f)     assert f.shape == x.shape     return f  ```",2022-02-15T17:55:09Z,enhancement,open,0,11,https://github.com/jax-ml/jax/issues/9579,Hi  can you give an example of what you want the `apply_mask` function to do?,"Hey, thanks for the super quick reply. So, i tried to clear it up in my post. In my example usecase, there is on line in the `biject_alpha`function that i do not know how to solve differently. Randomness makes everything quite nasty :/","Ah, I see, sorry. I was looking for a definition of `apply_mask`, and didn't look at the comments near its use. As a general answer, there is currently no JITcompatible way to execute `y = x[mask]` for a dynamic `mask`. This is because the size of `y` must be known statically (at trace/compile time), but this depends on the content of `mask`, and so is not knowable until runtime. But you may be able to solve this for the particular case you're interested in without explicitly constructing the dynamicallysized array... I'm having trouble understanding the code, though, because there's no indication of the types/sizes of the inputs to `cosInterpolateRandomized`.",I added some asserts that hopefully make this more clear. The function is conceptually identical to `jax.numpy.interp` and shares its code structure https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.interp.html,"It looks like you're only pulling a single value out of the array, so perhaps you could use something like this instead? ```python      idx of first occurenece     start_idx = jnp.where(i  1 == idx, size=1)[0]     alpha_i = alpha[start_idx]     bijected_alpha = cdf[(alpha_i//(1/cdf_bins)).astype(int)]     alpha = alpha.at[start_idx].set(bijected_alpha)     return (alpha, i, key) ```","Actually, this `alpha_i` is deceiving. Let me try to clarify: ```python x = jnp.arange(5) xp = jnp.array([0, 1.5, 3, 4]) i = jnp.clip(jnp.searchsorted(xp, x, side=""right""), 1, len(xp)1) i [1,1,2,3] ``` Then, ```python dx = xp[i]  xp[i1] alpha = (x  xp[i1])/dx alpha [0., 0.66667, 0.33334, 0., 1.] ``` Then, ``` biject_alpha(0, (alpha, i, random.PRNGKey(0))) [0., 0.83654, 0.33334, 0., 1.] ``` In `biject_alpha` the maskarray would have been ```python (i1) == 0 [True, True, False, False] alpha[mask] [0., 0.66667] ``` I hope this helps :) Edit: I also wanted to add that the foriloop is not really good performancewise. Ideally, i would just like to vmap a function over the different interpolation intervals and then flatten ```python  alpha.shape = (len(xp), ""varying"") bijected_alpha = vmap(biject_alpha)(alpha, key) bijected_alpha = bijected_alpha.flatten() ```","This seems to do the trick, it is probably a nicer solution anyway. Though i do not fully understand why this index operation in `biject_alpha` now is fine. ```python from jax import random  import jax import jax.numpy as jnp def biject_alpha(alpha, cdf):     cdf_bins = len(cdf)     return cdf[(alpha//(1/cdf_bins)).astype(int)]     return jax.lax.dynamic_index_in_dim(cdf, (alpha//(1/cdf_bins)).astype(int)) def generate_cdf(key):     cdf_bins=20     samples = random.uniform(key, (cdf_bins1,), maxval=1.0)     samples = jnp.hstack((jnp.array([0.0]), samples))     montonous = jnp.cumsum(samples)     cdf = montonous/montonous[1]     return cdf .jit def cosInterpolateRandomized(x, xp, fp, key):     i = jnp.clip(jnp.searchsorted(xp, x, side='right'), 1, len(xp)  1)     dx = xp[i]  xp[i  1]     alpha = (x  xp[i  1])/dx      key, *consume = random.split(key, len(xp)+1)     consume = jnp.array(consume).reshape((len(xp), 2))     consume = consume[i  1]     cdfs = jax.vmap(generate_cdf)(consume)     alpha = jax.vmap(biject_alpha)(alpha, cdfs)     def cos_interpolate(x1, x2, alpha):         """""" x2 > x1         """"""         return (x1+x2)/2 + (x1x2)/2 * jnp.cos(alpha*jnp.pi)     f = jnp.where((dx == 0), fp[i], jax.vmap(cos_interpolate)(fp[i1], fp[i], alpha))     f = jnp.where(x > xp[1], fp[1], f)     return f x = jnp.arange(5) xp = jnp.array([0, 1.5, 3, 4]) fp = jnp.array([0, 3, 6, 8.]) cosInterpolateRandomized(x, xp, fp, random.PRNGKey(0)) [0, 2.46, 3.62, 6, 8] ```","Masking is also pretty important for dealing with variablelength timeseries (transformers, RNNs, etc). Are `jax` transformers just never jitted?","They're jitted! But if people need to batch together variable length sequences, they write the masking and padding manually.","Could you please point me to an example of this manual masking and padding?   I'm trying to implement a high precision cross entropy that is jitable, and this is what I would ideally want to do: ``` B: batch size C: number of classes inputs: f \in B x C, y \in B x C mask = y.astype(bool) c = f[mask].reshape(1, 1) w = f[~mask].reshape(1, C1) loss = jax.nn.softplus(jax.scipy.special.logsumexp(w  c, axis=1)).mean() ``` But I can't come up with an easy solution to this, neither `np.delete` nor `where` can help me. I could maybe do the `logsumexp` myself step by a step and use a `where` to solve the issue, but i'm afraid it's against the whole point of achieving a ""highprecision"" cross entropy that could handle values below 1e30. Any thoughts would be appreciated! **Update:** I found a way around this, by argsorting the onehot vector, but this function depends on the implementation of `argsort` (it needs it to keep the original order in case the values being sorted are equal  there's only one 1 value and others are 0). If anyone has a better idea of doing this, I'll appreciate them sharing. Thanks again. ```python def high_precision_ce(f, y):    assumes one datapoint (two vectors), vmap if you need to apply it to matrices!     sorted_y = y.argsort()     correct = f[sorted_y[1:]].reshape(1, 1)   1 x 1     wrong = f[sorted_y[:1]].reshape(1, y.shape[0]  1)   1 x C1     return nn.softplus(jsp.special.logsumexp(wrong  correct, axis=1)) ```","> it needs it to keep the original order in case the values being sorted are equal `argsort` in JAX is always a stable sort in JAX, so you should be able to depend on this being true > If anyone has a better idea of doing this, I'll appreciate them sharing. This version follows your first algorithm, ensuring shapes are static so it will remain compatible with `jit` and other jax transformations ```python import jax import jax.numpy as jnp from functools import partial .jit def f(x, y):   assert x.shape == y.shape   B, C = x.shape   indices = jnp.argmax(y, axis=1)   c = x[jnp.arange(B), indices][:, None]   w = jax.vmap(partial(jnp.delete, assume_unique_indices=True))(x, indices)   return jax.nn.softplus(jax.scipy.special.logsumexp(w  c, axis=1)).mean() ```"
389,"以下是一个github上的jax下的一个issue, 标题是([JAX] Fix crash when applying jit() to a callable that is not weak-referenceable.)， 内容是 ([JAX] Fix crash when applying jit() to a callable that is not weakreferenceable. Fixes https://github.com/google/jax/issues/9541)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[JAX] Fix crash when applying jit() to a callable that is not weak-referenceable.,[JAX] Fix crash when applying jit() to a callable that is not weakreferenceable. Fixes https://github.com/google/jax/issues/9541,2022-02-15T16:30:16Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9577
4543,"以下是一个github上的jax下的一个issue, 标题是(Performance regression in jit compilation time from (jax=0.2.21, jaxlib=0.1.71+cuda102) to (jax=0.3.0, jaxlib=0.3.0))， 内容是 (Thanks for the excellent library! On upgrading from `jax=0.2.21`, `jaxlib=0.1.71+cuda102` to `jax=0.3.0`, `jaxlib=0.3.0`, I am experiencing extreme performance regression with jit compilation times for my typical workload.  Here is an asminimalasIcanmakeit code that illustrates the problem: ```python import jax import jaxlib from jax import jit, vmap, grad from functools import partial import jax.numpy as jnp import numpy as onp from time import time jax.config.update('jax_enable_x64', True)   allows use of float64  def fft_correlation(s, t):     return jnp.fft.irfft2(jnp.fft.rfft2(s) * jnp.conj(jnp.fft.rfft2(t)))  def cosine_correlation(s, t):     eps = 1e6     os = jnp.where(s == 0, 0.0, 1.0)     ot = jnp.where(t == 0, 0.0, 1.0)     overlap = fft_correlation(os, ot)     num = fft_correlation(s, t)     d1 = fft_correlation(s * s, ot)     d2 = fft_correlation(os, t * t)     coscor = num / (jnp.sqrt(d1 * d2) + eps)     return coscor, overlap (jit, static_argnums=(2, 3)) def normalized_cross_correlation(im1, im2, min_overlap_frac=0.1, border_discard=10):     tot_size = (onp.array(im1.shape)[2:] + im2.shape[2:])  (         border_discard,         border_discard,     )     (p1r, p1c), (p2r, p2c) = tot_size  im1.shape[2:], tot_size  im2.shape[2:]     mask1 = im1 != 0     mask2 = im2 != 0     im1 = jnp.pad(         im1  mask1.astype(im1.dtype) * (im1.sum() / mask1.sum()),         ([p1r // 2, (p1r + 1) // 2], [p1c // 2, (p1c + 1) // 2]),     )     im2 = jnp.pad(         im2  mask2.astype(im2.dtype) * (im2.sum() / mask2.sum()),         ([p2r // 2, (p2r + 1) // 2], [p2c // 2, (p2c + 1) // 2]),     )     num_nz_1 = mask1.sum()     num_nz_2 = mask2.sum()     min_pixels_overlap = jnp.where(         num_nz_1  min_pixels_overlap))[         border_discard + 1 : cc.shape[2]  border_discard,         border_discard + 1 : cc.shape[1]  border_discard,     ]     return  = jax.jit(     jax.vmap(lambda s, t: normalized_cross_correlation(s, t, 0.1, 10), in_axes=(0, 0)) ) print(f'{jax.devices()[0].device_kind=}') print(f'{jax.__version__, jaxlib.__version__=}') s = jnp.ones((256, 512, 512)) t = jnp.ones((256, 512, 512)) start = time() warmup = ncc(s, t).block_until_ready() print(f'cold start duration = {time()start:.2f} s') start = time() for i in range(10):     results = ncc(s, t).block_until_ready() print(f'mean jitted duration (10 runs)= {(time()start)/10:.2f} s') ``` The outputs with the previous and new versions of `jax` and `jaxlib` are as follows: ```   OLD jax.devices()[0].device_kind='Tesla V100DGXS32GB' jax.__version__, jaxlib.__version__=('0.2.21', '0.1.71') cold start duration = 1.61 s mean jitted duration (10 runs)= 0.56 s   NEW jax.devices()[0].device_kind='Tesla V100DGXS32GB' jax.__version__, jaxlib.__version__=('0.3.0', '0.3.0') cold start duration = 36.45 s mean jitted duration (10 runs)= 0.50 s ``` Note that the time for the jit compilation is about 20x slower now. Also, while this particular example has about 10% faster jitted speed in the latest jax+jaxlib, my experience since upgrading is that for most of my daily workflow (image processing on massive multimodal images in the Rijksmuseum),  the **postcompilation** execution time of similar jitted functions is about twice as long as it was with the older version too (I'll file another bug if appropriate).  I'm using image tiles that have (literal) edge cases for their sizes, so the change from a couple seconds to 36 seconds for jit compilation time has a huge overall impact.   If this new behavior is not considered a bug, are there configuration settings that I can use to get back the old behavior? Many thanks in advance!  Additional info, in case it's relevant: ```sh $ uname a Linux DGXSTATION 5.4.099generic CC(enable linalg tests)Ubuntu SMP Thu Feb 3 13:50:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux $ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052021 NVIDIA Corporation Built on Mon_Oct_11_21:27:02_PDT_2021 Cuda compilation tools, release 11.4, V11.4.152 Build cuda_11.4.r11.4/compiler.30521435_0 $ egrep 'MAJORPATCHLEVEL' /usr/include/cudnn_version.h  define CUDNN_MAJOR 8 define CUDNN_MINOR 2 define CUDNN_PATCHLEVEL 4 define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Performance regression in jit compilation time from (jax=0.2.21, jaxlib=0.1.71+cuda102) to (jax=0.3.0, jaxlib=0.3.0)","Thanks for the excellent library! On upgrading from `jax=0.2.21`, `jaxlib=0.1.71+cuda102` to `jax=0.3.0`, `jaxlib=0.3.0`, I am experiencing extreme performance regression with jit compilation times for my typical workload.  Here is an asminimalasIcanmakeit code that illustrates the problem: ```python import jax import jaxlib from jax import jit, vmap, grad from functools import partial import jax.numpy as jnp import numpy as onp from time import time jax.config.update('jax_enable_x64', True)   allows use of float64  def fft_correlation(s, t):     return jnp.fft.irfft2(jnp.fft.rfft2(s) * jnp.conj(jnp.fft.rfft2(t)))  def cosine_correlation(s, t):     eps = 1e6     os = jnp.where(s == 0, 0.0, 1.0)     ot = jnp.where(t == 0, 0.0, 1.0)     overlap = fft_correlation(os, ot)     num = fft_correlation(s, t)     d1 = fft_correlation(s * s, ot)     d2 = fft_correlation(os, t * t)     coscor = num / (jnp.sqrt(d1 * d2) + eps)     return coscor, overlap (jit, static_argnums=(2, 3)) def normalized_cross_correlation(im1, im2, min_overlap_frac=0.1, border_discard=10):     tot_size = (onp.array(im1.shape)[2:] + im2.shape[2:])  (         border_discard,         border_discard,     )     (p1r, p1c), (p2r, p2c) = tot_size  im1.shape[2:], tot_size  im2.shape[2:]     mask1 = im1 != 0     mask2 = im2 != 0     im1 = jnp.pad(         im1  mask1.astype(im1.dtype) * (im1.sum() / mask1.sum()),         ([p1r // 2, (p1r + 1) // 2], [p1c // 2, (p1c + 1) // 2]),     )     im2 = jnp.pad(         im2  mask2.astype(im2.dtype) * (im2.sum() / mask2.sum()),         ([p2r // 2, (p2r + 1) // 2], [p2c // 2, (p2c + 1) // 2]),     )     num_nz_1 = mask1.sum()     num_nz_2 = mask2.sum()     min_pixels_overlap = jnp.where(         num_nz_1  min_pixels_overlap))[         border_discard + 1 : cc.shape[2]  border_discard,         border_discard + 1 : cc.shape[1]  border_discard,     ]     return  = jax.jit(     jax.vmap(lambda s, t: normalized_cross_correlation(s, t, 0.1, 10), in_axes=(0, 0)) ) print(f'{jax.devices()[0].device_kind=}') print(f'{jax.__version__, jaxlib.__version__=}') s = jnp.ones((256, 512, 512)) t = jnp.ones((256, 512, 512)) start = time() warmup = ncc(s, t).block_until_ready() print(f'cold start duration = {time()start:.2f} s') start = time() for i in range(10):     results = ncc(s, t).block_until_ready() print(f'mean jitted duration (10 runs)= {(time()start)/10:.2f} s') ``` The outputs with the previous and new versions of `jax` and `jaxlib` are as follows: ```   OLD jax.devices()[0].device_kind='Tesla V100DGXS32GB' jax.__version__, jaxlib.__version__=('0.2.21', '0.1.71') cold start duration = 1.61 s mean jitted duration (10 runs)= 0.56 s   NEW jax.devices()[0].device_kind='Tesla V100DGXS32GB' jax.__version__, jaxlib.__version__=('0.3.0', '0.3.0') cold start duration = 36.45 s mean jitted duration (10 runs)= 0.50 s ``` Note that the time for the jit compilation is about 20x slower now. Also, while this particular example has about 10% faster jitted speed in the latest jax+jaxlib, my experience since upgrading is that for most of my daily workflow (image processing on massive multimodal images in the Rijksmuseum),  the **postcompilation** execution time of similar jitted functions is about twice as long as it was with the older version too (I'll file another bug if appropriate).  I'm using image tiles that have (literal) edge cases for their sizes, so the change from a couple seconds to 36 seconds for jit compilation time has a huge overall impact.   If this new behavior is not considered a bug, are there configuration settings that I can use to get back the old behavior? Many thanks in advance!  Additional info, in case it's relevant: ```sh $ uname a Linux DGXSTATION 5.4.099generic CC(enable linalg tests)Ubuntu SMP Thu Feb 3 13:50:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux $ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052021 NVIDIA Corporation Built on Mon_Oct_11_21:27:02_PDT_2021 Cuda compilation tools, release 11.4, V11.4.152 Build cuda_11.4.r11.4/compiler.30521435_0 $ egrep 'MAJORPATCHLEVEL' /usr/include/cudnn_version.h  define CUDNN_MAJOR 8 define CUDNN_MINOR 2 define CUDNN_PATCHLEVEL 4 define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL) ```",2022-02-15T15:03:35Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/9575,The compilation overhead is due to an inefficiency in a new lowering path. Try setting `JAX_ENABLE_MLIR=0` as a workaround until we can release a new jaxlib with a fix.,"Thanks for the very fast reply.  Your workaround did the trick: ``` jax.devices()[0].device_kind='Tesla V100DGXS32GB' jax.__version__, jaxlib.__version__=('0.3.0', '0.3.0') cold start duration = 1.47 s mean jitted duration (10 runs)= 0.50 s ``` Shall I leave this open until the aforementioned fix?","Yes, let's leave this open until the fix is ready to go.","https://github.com/tensorflow/tensorflow/commit/ffb2efdbea1cf401aac0d58a9dc764cd0cb0b072 fixes this, and it will be incorporated into JAX whenever we next make a jaxlib release."
2532,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] NotImplementedError: Call to gather cannot be converted with enable_xla=False)， 内容是 (I see that there is only partial support for the `XlaGather` op conversion when using `enabled_xla=False` (needed in my case because I want to convert the saved model to tflite/tfjs). I'm wondering if more support for this op is on the roadmap? Here's the full error that I'm seeing: ``` NotImplementedError Traceback (most recent call last) /usr/local/lib/python3.7/distpackages/jax/experimental/jax2tf/impl_no_xla.py in _gather_using_tf_gather(operand, start_indices, dimension_numbers, slice_sizes, _in_avals)     568     raise _xla_disabled_error(     569         ""gather"", > 570         f""unsupported dimension_numbers '{dimension_numbers}'; op_shape={op_shape}.""     571     )     572    We added a trailing dimension of size 1 NotImplementedError: Call to gather cannot be converted with enable_xla=False. unsupported dimension_numbers 'GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0, 1), start_index_map=(0, 1))'; op_shape=(1, 30, 512). ``` Below is some code that reproduces this error. I've only just started playing around with `jax2tf` so apologies if I'm doing something silly here. ``` !pip install upgrade flax transformers ``` ```python import jax from jax.experimental import jax2tf from jax import numpy as jnp import numpy as np import tensorflow as tf from transformers import FlaxCLIPModel clip = FlaxCLIPModel.from_pretrained(""openai/clipvitbasepatch32"") def score(pixel_values, input_ids, attention_mask):     pixel_values = jax.image.resize(pixel_values, (3, 224, 224), ""nearest"")     inputs = {""pixel_values"":jnp.array([pixel_values]), ""input_ids"":input_ids, ""attention_mask"":attention_mask}     outputs = clip(**inputs)     return outputs.logits_per_image[0][0] score_tf = jax2tf.convert(score, enable_xla=False) my_model = tf.Module() my_model.f = tf.function(score_tf, autograph=False, jit_compile=True, input_signature=[   tf.TensorSpec([3, 40, 40], tf.float32),   tf.TensorSpec([1, 30], tf.int32),   tf.TensorSpec([1, 30], tf.int32), ]) model_name = 'pixel_text_score' tf.saved_model.save(my_model, model_name, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True)) ``` Here's a public colab with that code: https://colab.research.google.com/drive/18jFruauFcKEJ_SjBZqn5z6AiPqQAfa1j?usp=sharing Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] NotImplementedError: Call to gather cannot be converted with enable_xla=False,"I see that there is only partial support for the `XlaGather` op conversion when using `enabled_xla=False` (needed in my case because I want to convert the saved model to tflite/tfjs). I'm wondering if more support for this op is on the roadmap? Here's the full error that I'm seeing: ``` NotImplementedError Traceback (most recent call last) /usr/local/lib/python3.7/distpackages/jax/experimental/jax2tf/impl_no_xla.py in _gather_using_tf_gather(operand, start_indices, dimension_numbers, slice_sizes, _in_avals)     568     raise _xla_disabled_error(     569         ""gather"", > 570         f""unsupported dimension_numbers '{dimension_numbers}'; op_shape={op_shape}.""     571     )     572    We added a trailing dimension of size 1 NotImplementedError: Call to gather cannot be converted with enable_xla=False. unsupported dimension_numbers 'GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0, 1), start_index_map=(0, 1))'; op_shape=(1, 30, 512). ``` Below is some code that reproduces this error. I've only just started playing around with `jax2tf` so apologies if I'm doing something silly here. ``` !pip install upgrade flax transformers ``` ```python import jax from jax.experimental import jax2tf from jax import numpy as jnp import numpy as np import tensorflow as tf from transformers import FlaxCLIPModel clip = FlaxCLIPModel.from_pretrained(""openai/clipvitbasepatch32"") def score(pixel_values, input_ids, attention_mask):     pixel_values = jax.image.resize(pixel_values, (3, 224, 224), ""nearest"")     inputs = {""pixel_values"":jnp.array([pixel_values]), ""input_ids"":input_ids, ""attention_mask"":attention_mask}     outputs = clip(**inputs)     return outputs.logits_per_image[0][0] score_tf = jax2tf.convert(score, enable_xla=False) my_model = tf.Module() my_model.f = tf.function(score_tf, autograph=False, jit_compile=True, input_signature=[   tf.TensorSpec([3, 40, 40], tf.float32),   tf.TensorSpec([1, 30], tf.int32),   tf.TensorSpec([1, 30], tf.int32), ]) model_name = 'pixel_text_score' tf.saved_model.save(my_model, model_name, options=tf.saved_model.SaveOptions(experimental_custom_gradients=True)) ``` Here's a public colab with that code: https://colab.research.google.com/drive/18jFruauFcKEJ_SjBZqn5z6AiPqQAfa1j?usp=sharing Thanks!",2022-02-15T14:02:37Z,enhancement,closed,0,8,https://github.com/jax-ml/jax/issues/9572, has implemented much of the Gather support for enable_xla=False. Perhaps he can see if this case is easy to implement.,"Thanks for filing this issue, we indeed have experimental support for gather without XLA support, but we haven't yet tested this extensively, so it is great you are trying this out. It appears we don't properly implement multidimensional indexing right now. I actually found two bugs, the first is the one you also encountered. **Bug 1: gather bug** ```py def gather_fn1():   op = jnp.arange(4).reshape((2, 2, 1))   return op[[0], [0]] jax2tf.convert(gather_fn1, enable_xla=False)()   Should return: [[0]] ``` Outputs: ``` NotImplementedError: Call to gather cannot be converted with enable_xla=False. Unsupported arguments for gather: operand shape=(2, 2, 1), start_indices=[[0 0]], dimension_numbes=GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0, 1), start_index_map=(0, 1)), slice_sizes=(1, 1, 1), errors: : ValueError('start_indices shape should be 1') : ValueError('unsupported dimension numbers') : ValueError(""Batch dimensions in operand and start_indices don't agree"") ``` As you can see, there are three paths we implemented for gather, and actually the `gather_for_multidim_indexing` should trigger, but it doesn't. We should simply implement more support here. **Bug 2: clipping bug** ```py def gather_fn2():   op = jnp.arange(2).reshape((1, 2, 1))   return op[[0], [0]] jax2tf.convert(gather_fn2, enable_xla=False)()   Should return: [[0]] ``` Outputs: ```   File ""/Users/marcvanzee/github/jax/jax/experimental/jax2tf/impl_no_xla.py"", line 554, in wrapper     return gather_fn(args)   File ""/Users/marcvanzee/github/jax/jax/experimental/jax2tf/impl_no_xla.py"", line 691, in _gather_with_batch_dims     start_indices = _clip(op_shape, args.start_indices, args.slice_sizes)   File ""/Users/marcvanzee/github/jax/jax/experimental/jax2tf/impl_no_xla.py"", line 517, in _clip     return tf.clip_by_value(tf.cast(start_indices, dtype=tf.int32), 0, max_start)   File ""/Users/marcvanzee/.pyenv/versions/3.7.10/lib/python3.7/sitepackages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler     raise e.with_traceback(filtered_tb) from None   File ""/Users/marcvanzee/.pyenv/versions/3.7.10/lib/python3.7/sitepackages/tensorflow/python/framework/ops.py"", line 7107, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [1,2] vs. [3] [Op:Minimum] ``` This seems like a bug in clipping. This case will trigger if you run your code with batch size 1 (which could be quite likely on mobile. I will look into both issues.",Thanks Marc!! 🙏,"Hi , I've looked into your problem today. The problematic line in CLIP is the following: ```py pooled_output = last_hidden_state[jnp.arange(last_hidden_state.shape[0]), input_ids.argmax(axis=1)] ``` It seems that we can simplifying this to: ```py pooled_output = last_hidden_state[:, input_ids.argmax()] ``` Which should convert fine, so if you are able to edit the source code of the CLIP model, you can change that line, which should unblock you immediately. The reason that this does work is because the first code snippet calls `XlaGather` in a way that is not supported because we are selecting multiple indices in the first dimension. However, since this case actually selects _all_ indices, we can simply use a slice to select all elements in the dimension, which results in a simpler call to `XlaGather` that is supported. In the meantime I will work on a fix so that this case will just work out of the box. Let me know if that works for you!","That fixed it, thanks ! `jax2tf` is awesome  I had no idea that this has been a thing since 2020. Seems like a very viable path to getting a lot of jax stuff working in the browser (which is my main use case area). I *really* appreciate the work y'all are doing to get this working & mature!","Thanks for the feedback! Please do let us know when you manage to get any models running in the browser using jax2tf, we will gladly link to your examples from our README. Currently this project is still in experimental phase, so we can learn / benefit a lot from early users like you. So thank you back for experimenting with this! 😄 ", Is there a way to describe what uses of XlaGather are supported? We could then add this to an FAQ and include a link in the error message. ,>  Is there a way to describe what uses of XlaGather are supported? We could then add this to an FAQ and include a link in the error message. This is now documented here: https://github.com/google/jax/blob/main/jax/experimental/jax2tf/g3doc/no_xla_limitations.md I am closing this issue since it appears it is fixed.
1574,"以下是一个github上的jax下的一个issue, 标题是(Implement the JAX transfer guard API)， 内容是 (Implement the JAX transfer guard API Adds `jax_transfer_guard` flag and `jax.transfer_guard()` context manager that allows logging or disallowing unintended transfers. The API distinguishes between two types of transfers: * explicit transfers: `jax.device_put*()` and `jax.device_get()` calls. * implicit transfers: Other transfers (e.g., printing a `DeviceArray`). The transfer guard can take an action based on its guard level: * ""allow"": Silently allow all transfers (default; same as the previous behavior). * ""log"": Log and allow implicit transfers. Silently allow explicit transfers. * ""disallow"": Disallow implicit transfers. Silently allow explicit transfers. * ""log_explicit"": Log and allow all transfers. * ""disallow_explicit"": Disallow all transfers. The API also allows finecontrol the transfer guard level of individual transfer directions. Their flag and context manager names are suffixed with the transfer direction: * ""host_to_device"": Converting a Python value into a `DeviceBuffer`. * ""device_to_device"": Copying a `DeviceBuffer` to a different device. * ""device_to_host"": Fetching the value of a `DeviceBuffer`. Example: ``` x = jnp.array(1) y = jnp.array(2) z = jnp.array(3) print(x)   No error with jax.transfer_guard(""disallow""):   print(x)   No error; x is already fetched   print(jax.device_get(y))   No error   print(z)   Error! ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Implement the JAX transfer guard API,"Implement the JAX transfer guard API Adds `jax_transfer_guard` flag and `jax.transfer_guard()` context manager that allows logging or disallowing unintended transfers. The API distinguishes between two types of transfers: * explicit transfers: `jax.device_put*()` and `jax.device_get()` calls. * implicit transfers: Other transfers (e.g., printing a `DeviceArray`). The transfer guard can take an action based on its guard level: * ""allow"": Silently allow all transfers (default; same as the previous behavior). * ""log"": Log and allow implicit transfers. Silently allow explicit transfers. * ""disallow"": Disallow implicit transfers. Silently allow explicit transfers. * ""log_explicit"": Log and allow all transfers. * ""disallow_explicit"": Disallow all transfers. The API also allows finecontrol the transfer guard level of individual transfer directions. Their flag and context manager names are suffixed with the transfer direction: * ""host_to_device"": Converting a Python value into a `DeviceBuffer`. * ""device_to_device"": Copying a `DeviceBuffer` to a different device. * ""device_to_host"": Fetching the value of a `DeviceBuffer`. Example: ``` x = jnp.array(1) y = jnp.array(2) z = jnp.array(3) print(x)   No error with jax.transfer_guard(""disallow""):   print(x)   No error; x is already fetched   print(jax.device_get(y))   No error   print(z)   Error! ```",2022-02-09T23:28:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9513
1051,"以下是一个github上的jax下的一个issue, 标题是(Cannot export complex arrays as dlpack)， 内容是 (To my knowledge, dlpack now supports complex numbers (see https://github.com/dmlc/dlpack/blob/main/include/dlpack/dlpack.hL125), but Jax throws an error when trying to turn a complex array to a dlpack. This bug, combined with the other one I just filed  https://github.com/google/jax/issues/9496  actually creates a blocking issue of being unable to share complex number arrays from Jax with other array packages (except for NumPy, where it works due to Jax supporting `__array_interface__`). I'd be keen to know if there is any possible workaround for this. How can I share a complex array with CuPy, for example? ```python import numpy as np import jax import jax.dlpack import jax.numpy as jnp a = jnp.zeros(10, dtype = jnp.complex64) jax.dlpack.to_dlpack(a)  RuntimeError: UNIMPLEMENTED: XLA type C64 has no DLPack equivalent ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot export complex arrays as dlpack,"To my knowledge, dlpack now supports complex numbers (see https://github.com/dmlc/dlpack/blob/main/include/dlpack/dlpack.hL125), but Jax throws an error when trying to turn a complex array to a dlpack. This bug, combined with the other one I just filed  https://github.com/google/jax/issues/9496  actually creates a blocking issue of being unable to share complex number arrays from Jax with other array packages (except for NumPy, where it works due to Jax supporting `__array_interface__`). I'd be keen to know if there is any possible workaround for this. How can I share a complex array with CuPy, for example? ```python import numpy as np import jax import jax.dlpack import jax.numpy as jnp a = jnp.zeros(10, dtype = jnp.complex64) jax.dlpack.to_dlpack(a)  RuntimeError: UNIMPLEMENTED: XLA type C64 has no DLPack equivalent ```",2022-02-09T01:35:51Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/9497,"Thanks for the report – if complex types are available in the dlpack version that jaxlib is built against, we'll have to update https://github.com/tensorflow/tensorflow/blob/5968de3893ebc49a4f2a0f3b0b498a8b20c6f145/tensorflow/compiler/xla/python/dlpack.ccL98L103 and https://github.com/google/jax/blob/d262bae88b53365df222242bf1f4a5b7498c8210/jax/_src/dlpack.pyL21L23 in order to unblock their use. Do you know which dlpack version added this support?",0.4 IIRC,"Yeah, looks like 0.4, https://github.com/dmlc/dlpack/commit/56e6b6c96597074d718b770e65ab8eea99ec2133 Hopefully this is an easy change ...","Yes, this turns out to be a fairly easy fix. It should make it into the next jaxlib."," That's great news, glad it was so easy. Many thanks!","We just released `jax` and `jaxlib` 0.3.0, which should have this fix (lightly tested...). Please try it out!"," Many thanks, confirming that everything seems good to me  can export and import complex arrays on the GPU now!"
1243,"以下是一个github上的jax下的一个issue, 标题是(`jnp.svd` intermittently ignores `compute_uv=False`)， 内容是 (We've been having intermittent failures in our CI over at https://github.com/lanl/scico, and I believe I have tracked the cause down to some odd behavior in `jnp.svd`. The assert in the following code fails about one in one hundred calls. ```python import jax import jax.numpy as jnp def test():     key = jax.random.PRNGKey(3)     v = jax.random.normal(shape=(32, 1), key=key)     svdU, svdS, svdV = jnp.linalg.svd(v, full_matrices=False)     x = svdU @ jnp.diag(svdS) @ svdV     sigma = jnp.linalg.svd(x, compute_uv=False)     assert len(sigma) == 1 test() ``` The bash loop I'm using to test is ```bash while python main.py do done ``` I have not been able to get a failure if I remove the first `svd`, but I may have just been lucky/unlucky. I've seen the failures both on my MacOS laptop and on the github CI Linux machines. This is on jax 0.2.28 with jaxlib 0.1.76, CPU version. Any ideas on why this might happen, work arounds, or even ideas to help provoke the error more frequently would be appreciated!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,`jnp.svd` intermittently ignores `compute_uv=False`,"We've been having intermittent failures in our CI over at https://github.com/lanl/scico, and I believe I have tracked the cause down to some odd behavior in `jnp.svd`. The assert in the following code fails about one in one hundred calls. ```python import jax import jax.numpy as jnp def test():     key = jax.random.PRNGKey(3)     v = jax.random.normal(shape=(32, 1), key=key)     svdU, svdS, svdV = jnp.linalg.svd(v, full_matrices=False)     x = svdU @ jnp.diag(svdS) @ svdV     sigma = jnp.linalg.svd(x, compute_uv=False)     assert len(sigma) == 1 test() ``` The bash loop I'm using to test is ```bash while python main.py do done ``` I have not been able to get a failure if I remove the first `svd`, but I may have just been lucky/unlucky. I've seen the failures both on my MacOS laptop and on the github CI Linux machines. This is on jax 0.2.28 with jaxlib 0.1.76, CPU version. Any ideas on why this might happen, work arounds, or even ideas to help provoke the error more frequently would be appreciated!",2022-02-07T23:20:08Z,bug,closed,1,5,https://github.com/jax-ml/jax/issues/9483,"Thanks for the report – that is strange! I was able to reproduce it in a Colab CPU runtime after about 500 iterations with the following 3 cells: ```python pip install jax==0.2.28 jaxlib==0.1.76 ``` ```python %%writefile main.py import jax import jax.numpy as jnp def test():     key = jax.random.PRNGKey(3)     v = jax.random.normal(shape=(32, 1), key=key)     svdU, svdS, svdV = jnp.linalg.svd(v, full_matrices=False)     x = svdU @ jnp.diag(svdS) @ svdV     sigma = jnp.linalg.svd(x, compute_uv=False)     assert len(sigma) == 1 test() ``` ```python import subprocess for i in range(1000):   if i % 10 == 0: print(i)   out = subprocess.run(['python3', 'main.py'], capture_output=True)   if out.returncode:     print(out.stderr.decode())     break ```","I was wondering if this might be an issue with our static argument cacheing, so I tried to replicate this *without* a call to `svd` with these Colab notebook cells: ```python %%writefile main2.py from functools import partial from jax import jit import jax.numpy as jnp (jit, static_argnames=['ravel', 'make_tuple']) def func(x, ravel=True, make_tuple=True):   if ravel:     x = x.ravel()   return (x, x) if make_tuple else x def test():   x = jnp.ones((3, 3))   y, _ = func(x, ravel=False)   out = func(y, make_tuple=False)   assert len(out) == 9 test()   ``` ```python import subprocess for i in range(1000):   if i % 10 == 0: print(i)   out = subprocess.run(['python3', 'main2.py'], capture_output=True)   if out.returncode:     print(out.stderr.decode())     break ``` I ran this a couple times and didn't seem to reproduce the error. This makes me think the bug is related to the SVD implementation, rather than something related to JIT dispatch.","We repro'ed internally on CPU backend, will follow up with findings.  ",Thanks for the report! The repro was very helpful. The issue should be fixed by https://github.com/tensorflow/tensorflow/commit/c5c390ae36fab842a02bb171f3268c3faef84151 once that makes it into a jaxlib build.,"We released `jax` and `jaxlib` 0.3.0, which includes the fix for this issue."
323,"以下是一个github上的jax下的一个issue, 标题是(Update scipy intersphinx inventory for SciPy 1.8.0.)， 内容是 (According to https://github.com/scipy/scipy/issues/14267 the SciPy docs seems to have moved.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update scipy intersphinx inventory for SciPy 1.8.0.,According to https://github.com/scipy/scipy/issues/14267 the SciPy docs seems to have moved.,2022-02-07T21:22:29Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/9477
6116,"以下是一个github上的jax下的一个issue, 标题是(Error upon compilation while using jax.jit)， 内容是 (Pardon me I'm still a noob with the inner workings of Jax and trying to find my way around it. I have this code which works well without the jit. But when I try to jit it, it throws an error. I initially used an if else statement within the code which also did not work and had to rewrite the code this way without an if else statement. How do I get around this?. MWE is below. (I already posted on stack overflow but thought I could get help faster here) ``` import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True) num_rows = 5 num_cols = 20 smf = jnp.array([jnp.inf, 0.1, 0.1, 0.1, 0.1]) par_init = jnp.array([1.0,2.0,3.0,4.0,5.0]) lb = jnp.array([0.1, 0.1, 0.1, 0.1, 0.1]) ub = jnp.array([10.0, 10.0, 10.0, 10.0, 10.0]) par = jnp.broadcast_to(par_init[:,None],(num_rows,num_cols)) kvals = jnp.where(jnp.isinf(smf), 1, num_cols) kvals = jnp.insert(kvals, 0, 0) kvals = jnp.cumsum(kvals) par0_col = jnp.zeros(num_rows*num_cols  (num_cols1) * jnp.sum(jnp.isinf(smf))) lb_col = jnp.zeros(num_rows*num_cols  (num_cols1) * jnp.sum(jnp.isinf(smf))) ub_col = jnp.zeros(num_rows*num_cols (num_cols1) * jnp.sum(jnp.isinf(smf))) for i in range(num_rows):     par0_col = par0_col.at[kvals[i]:kvals[i+1]].set(par[i, :kvals[i+1]kvals[i]])     lb_col = lb_col.at[kvals[i]:kvals[i+1]].set(lb[i])     ub_col = ub_col.at[kvals[i]:kvals[i+1]].set(ub[i]) par_log = jnp.log10((par0_col  lb_col) / (1  par0_col / ub_col)) .jit   def compute(p):     arr_1 = jnp.zeros(shape = (num_rows, num_cols))     arr_2 = jnp.zeros(shape = (num_rows, num_cols))     for i in range(num_rows):         arr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))         arr_2 = arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])     return arr_1 arr = compute(par_log) print(arr)  WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)  Traceback (most recent call last):    File ""test_7.py"", line 47, in       arr = compute(par_log)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback      return fun(*args, **kwargs)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/api.py"", line 424, in cache_miss      out_flat = xla.xla_call(    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/core.py"", line 1661, in bind      return call_bind(self, fun, *args, **params)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/core.py"", line 1652, in call_bind      outs = primitive.process(top_trace, fun, tracers, params)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/core.py"", line 1664, in process      return trace.process_call(self, fun, tracers, params)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/core.py"", line 633, in process_call      return primitive.impl(f, *tracers, **params)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 128, in _xla_call_impl      compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/linear_util.py"", line 263, in memoized_fun      ans = call(fun, *args)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 155, in _xla_callable_uncached      return lower_xla_callable(fun, device, backend, name, donated_invars,    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 206, in wrapper      return func(*args, **kwargs)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 169, in lower_xla_callable      jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 206, in wrapper      return func(*args, **kwargs)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 1566, in trace_to_jaxpr_final      jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(fun, main, in_avals)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 1543, in trace_to_subjaxpr_dynamic      ans = fun.call_wrapped(*in_tracers)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/linear_util.py"", line 166, in call_wrapped      ans = self.f(*args, **dict(self.params, **kwargs))    File ""test_7.py"", line 42, in compute      arr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py"", line 5704, in _rewriting_take      return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py"", line 5713, in _gather      indexer = _index_to_gather(shape(arr), idx)   shared with _scatter_update    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py"", line 5956, in _index_to_gather      raise IndexError(msg)  jax._src.traceback_util.UnfilteredStackTrace: IndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Tracedwith, Tracedwith, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).  The stack trace below excludes JAXinternal frames.  The preceding is the original exception that occurred, unmodified.    The above exception was the direct cause of the following exception: ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Error upon compilation while using jax.jit,"Pardon me I'm still a noob with the inner workings of Jax and trying to find my way around it. I have this code which works well without the jit. But when I try to jit it, it throws an error. I initially used an if else statement within the code which also did not work and had to rewrite the code this way without an if else statement. How do I get around this?. MWE is below. (I already posted on stack overflow but thought I could get help faster here) ``` import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True) num_rows = 5 num_cols = 20 smf = jnp.array([jnp.inf, 0.1, 0.1, 0.1, 0.1]) par_init = jnp.array([1.0,2.0,3.0,4.0,5.0]) lb = jnp.array([0.1, 0.1, 0.1, 0.1, 0.1]) ub = jnp.array([10.0, 10.0, 10.0, 10.0, 10.0]) par = jnp.broadcast_to(par_init[:,None],(num_rows,num_cols)) kvals = jnp.where(jnp.isinf(smf), 1, num_cols) kvals = jnp.insert(kvals, 0, 0) kvals = jnp.cumsum(kvals) par0_col = jnp.zeros(num_rows*num_cols  (num_cols1) * jnp.sum(jnp.isinf(smf))) lb_col = jnp.zeros(num_rows*num_cols  (num_cols1) * jnp.sum(jnp.isinf(smf))) ub_col = jnp.zeros(num_rows*num_cols (num_cols1) * jnp.sum(jnp.isinf(smf))) for i in range(num_rows):     par0_col = par0_col.at[kvals[i]:kvals[i+1]].set(par[i, :kvals[i+1]kvals[i]])     lb_col = lb_col.at[kvals[i]:kvals[i+1]].set(lb[i])     ub_col = ub_col.at[kvals[i]:kvals[i+1]].set(ub[i]) par_log = jnp.log10((par0_col  lb_col) / (1  par0_col / ub_col)) .jit   def compute(p):     arr_1 = jnp.zeros(shape = (num_rows, num_cols))     arr_2 = jnp.zeros(shape = (num_rows, num_cols))     for i in range(num_rows):         arr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))         arr_2 = arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])     return arr_1 arr = compute(par_log) print(arr)  WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)  Traceback (most recent call last):    File ""test_7.py"", line 47, in       arr = compute(par_log)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback      return fun(*args, **kwargs)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/api.py"", line 424, in cache_miss      out_flat = xla.xla_call(    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/core.py"", line 1661, in bind      return call_bind(self, fun, *args, **params)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/core.py"", line 1652, in call_bind      outs = primitive.process(top_trace, fun, tracers, params)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/core.py"", line 1664, in process      return trace.process_call(self, fun, tracers, params)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/core.py"", line 633, in process_call      return primitive.impl(f, *tracers, **params)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 128, in _xla_call_impl      compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/linear_util.py"", line 263, in memoized_fun      ans = call(fun, *args)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 155, in _xla_callable_uncached      return lower_xla_callable(fun, device, backend, name, donated_invars,    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 206, in wrapper      return func(*args, **kwargs)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 169, in lower_xla_callable      jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 206, in wrapper      return func(*args, **kwargs)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 1566, in trace_to_jaxpr_final      jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(fun, main, in_avals)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/interpreters/partial_eval.py"", line 1543, in trace_to_subjaxpr_dynamic      ans = fun.call_wrapped(*in_tracers)    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/linear_util.py"", line 166, in call_wrapped      ans = self.f(*args, **dict(self.params, **kwargs))    File ""test_7.py"", line 42, in compute      arr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py"", line 5704, in _rewriting_take      return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py"", line 5713, in _gather      indexer = _index_to_gather(shape(arr), idx)   shared with _scatter_update    File ""/home/richinex/anaconda3/envs/numerical/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py"", line 5956, in _index_to_gather      raise IndexError(msg)  jax._src.traceback_util.UnfilteredStackTrace: IndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Tracedwith, Tracedwith, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).  The stack trace below excludes JAXinternal frames.  The preceding is the original exception that occurred, unmodified.    The above exception was the direct cause of the following exception: ```",2022-02-07T10:23:09Z,,closed,0,8,https://github.com/jax-ml/jax/issues/9463,"(I didn't have time to look at this fully before you closed it, but in general, try to avoid building arrays in JAX using Python loops if you can. Here, for example, it looks like you could build `arr_1` using an arrayattime operation rather than a loop. If you do want to make the loopindexing code work, you may need to use `jax.lax.dynamic_slice_in_dim`.","hi Hawkinsp, Jakevdp gave me a solution. He said to make the line kvals = jnp.cumsum(kvals) as kvals =list( jnp.cumsum(kvals)). This seemed to work. I'm still trying to find my way around jax.lax.synamic_slice. I would be glad to see if you have some other solution maybe when you have the time to look at it.","My point was don't write: ``` .jit   def compute(p):     arr_1 = jnp.zeros(shape = (num_rows, num_cols))     arr_2 = jnp.zeros(shape = (num_rows, num_cols))     for i in range(num_rows):         arr_1 = arr_1.at[i, :].set((par_log[kvals[i]:kvals[i+1]]))         arr_2 = arr_2.at[i, :].set(10**par_log[kvals[i]:kvals[i+1]])     return arr_1 ``` Instead here are two other ways to write that function that don't require explicit Python loops: ``` .vmap def f(i):     return jnp.broadcast_to(par_log[kvals[i]], (num_cols,)) print(f(jnp.arange(num_rows))) ``` ``` print(jnp.broadcast_to(par_log[kvals[jnp.arange(num_rows)], None], (num_rows, num_cols))) ``` Hope that helps!",Ahh Nice. it does help.,"I should add: that's good advice for getting better performance in classic NumPy, not just JAX!","Hi @ hawkinsp, I am sorry to reopen to issue. I still cant get this to work. Been trying fow a while now. I tried dynamic_slice_in_dim() like you suggested for the loop indexing but if try to apply vmap it throws an error. kindly assist me here. ```  Works for single indexing def f(i):     return jnp.broadcast_to(jax.lax.dynamic_slice_in_dim(par_log, kvals[i], len(list(jnp.arange(kvals[i],kvals[i+1])))), (num_cols,)) print(f(1))  [0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361   0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361   0.37566361 0.37566361 0.37566361 0.37566361 0.37566361 0.37566361   0.37566361 0.37566361]  Throws an error on using vmap .vmap def f(i):     return jnp.broadcast_to(jax.lax.dynamic_slice_in_dim(par_log, kvals[i], len(list(jnp.arange(kvals[i],kvals[i+1])))), (num_cols,)) print(f(jnp.arange(num_rows)))  Output exceeds the size limit. Open the full output data in a text editor    ConcretizationTypeError                   Traceback (most recent call last)  /home/richard/Dropbox/dynamic_impedance_application/deis_app/test_13.ipynb Cell 13 in ()        1 .vmap        2 def f(i):        3     return jnp.broadcast_to(jax.lax.dynamic_slice_in_dim(par_log, kvals[i], len(list(jnp.arange(kvals[i],kvals[i+1])))), (num_cols,))  > 5 print(f(jnp.arange(num_rows)))      [... skipping hidden 3 frame]  /home/richard/Dropbox/dynamic_impedance_application/deis_app/test_13.ipynb Cell 13 in f(i)        1 .vmap        2 def f(i):  > 3     return jnp.broadcast_to(jax.lax.dynamic_slice_in_dim(par_log, kvals[i], len(list(jnp.arange(kvals[i],kvals[i+1])))), (num_cols,))  File ~/mambaforge/envs/jax_env/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py:2131, in arange(start, stop, step, dtype)     2129   return lax.iota(dtype, start)     2130 else:  > 2131   start = require(start, msg(""start""))     2132   stop = None if stop is None else require(stop, msg(""stop""))     2133   step = None if step is None else require(step, msg(""step""))  File ~/mambaforge/envs/jax_env/lib/python3.9/sitepackages/jax/core.py:1185, in concrete_or_error(force, val, context)     1183     return force(val.aval.val)     1184   else:  ...    batch_dim = 0  It arose in jax.numpy.arange argument `start`.  This Tracer was created on line /tmp/ipykernel_1683475/2825972014.py:3 (f)  See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.ConcretizationTypeError ```","Hi  the issue here is that within your `vmap` transform of `f`, all mapped arguments will be traced. So `i` is traced, which means `kvals[i]` and `kvals[i + 1]` are traced, which means `jnp.arange(kvals[i], kvals[i + 1])` has a dynamic shape. From your example, it seems like what you are trying to do is create a jagged array: i.e. for each value of `i`, a different sized array will be returned, so the logical output of `vmap(f)(indices)` is an array where different rows have different sizes. JAX doesn't support ragged arrays: if you want to use JAX transforms, you'll have to figure out a different way of approaching your computation.","Hi Jakevdp, yea thanks.  I later figured it just couldn't work. I used a different approach which works. Thanks "
1808,"以下是一个github上的jax下的一个issue, 标题是(Jax numpy array_split is slow compared to numpy)， 内容是 (I was writing Jax code, substituting all of the places where I would normally use NumPy with jax.numpy In using jax.numpy.array_split with a large input, I found that the program was incredibly slow (for jax+numpy or jax then the program didn't finish in 5 minutes for me)  While running there was no issue however if I ctrl+c to terminate then the follow error was thrown `F external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:446] ptxas returned an error during compilation of ptx to sass: 'INTERNAL: ptxas exited with nonzero error code 2, output: '  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided. Aborted (core dumped)` It seems that Jax is trying to jit/compile the array input which is slow due to the size, I just wanted to check if this was expected or not If this is expected, could some documentation be added to note this problem on the relative functions  Example code ``` from tqdm import tqdm import numpy as onp import jax.numpy as jnp print('numpy') a = [batch for batch in tqdm(onp.array_split(onp.arange(10_000), 1_000))] b = [batch for batch in tqdm(onp.array_split(onp.arange(100_000), 10_000))] print('jax + numpy') c = [batch for batch in tqdm(jnp.array_split(onp.arange(100_000), 10_000))] d = [batch for batch in tqdm(jnp.array_split(onp.arange(100_000), 10_000))] print('jax') e = [batch for batch in tqdm(jnp.array_split(jnp.arange(100_000), 10_000))] f = [batch for batch in tqdm(jnp.array_split(jnp.arange(100_000), 10_000))] ``` Thanks for any help)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax numpy array_split is slow compared to numpy,"I was writing Jax code, substituting all of the places where I would normally use NumPy with jax.numpy In using jax.numpy.array_split with a large input, I found that the program was incredibly slow (for jax+numpy or jax then the program didn't finish in 5 minutes for me)  While running there was no issue however if I ctrl+c to terminate then the follow error was thrown `F external/org_tensorflow/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:446] ptxas returned an error during compilation of ptx to sass: 'INTERNAL: ptxas exited with nonzero error code 2, output: '  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided. Aborted (core dumped)` It seems that Jax is trying to jit/compile the array input which is slow due to the size, I just wanted to check if this was expected or not If this is expected, could some documentation be added to note this problem on the relative functions  Example code ``` from tqdm import tqdm import numpy as onp import jax.numpy as jnp print('numpy') a = [batch for batch in tqdm(onp.array_split(onp.arange(10_000), 1_000))] b = [batch for batch in tqdm(onp.array_split(onp.arange(100_000), 10_000))] print('jax + numpy') c = [batch for batch in tqdm(jnp.array_split(onp.arange(100_000), 10_000))] d = [batch for batch in tqdm(jnp.array_split(onp.arange(100_000), 10_000))] print('jax') e = [batch for batch in tqdm(jnp.array_split(jnp.arange(100_000), 10_000))] f = [batch for batch in tqdm(jnp.array_split(jnp.arange(100_000), 10_000))] ``` Thanks for any help",2022-02-04T12:01:45Z,bug,open,0,7,https://github.com/jax-ml/jax/issues/9445,"Thanks for the report – I think the issue is that `jnp.array_split` fails to push the numpy input to device before processing it, meaning that the splitting is effectively done on client side and then each output is pushed to device individually, incurring that overhead thousands of times instead of one time. I'll work on a fix."," It's also possible that that code would be faster if it used `dynamic_slice` rather than `slice`, especially when not under a `jit`. Otherwise we need to compile one ""slice"" operator for each index. In a `jit` it will make no difference.","Good point  my first PR gives an order of magnitude improvement by calling device_put, so I'll get that in first and then take a look at `dynamic_slice`.",Reopening to explore using `dynamic_slice` instead,"I tried switching to `lax.dynamic_slice`; for 1000 splits  nonjitted execution is about 5x slower than `lax.slice`  jitted execution is comparable to `lax.slice`  compile time using `lax.dynamic_slice` is 1000x faster (2ms vs. 2s) Here's the benchmark script: ```python import jax import numpy as np import jax.numpy as jnp x_np = np.arange(100_000) x_jnp = jnp.arange(100_000) N = 1_000 jit_array_split = jax.jit(jnp.array_split, static_argnums=1) %time jax.block_until_ready(jit_array_split(x_jnp, N)) %timeit np.array_split(x_np, N) %timeit jax.block_until_ready(jnp.array_split(x_np, N)) %timeit jax.block_until_ready(jnp.array_split(x_jnp, N)) %timeit jax.block_until_ready(jit_array_split(x_jnp, N)) ``` The output on the main branch (with CC(jnp.split: push inputs to device before splitting) included): ``` CPU times: user 1.9 s, sys: 32 ms, total: 1.93 s Wall time: 1.94 s 1.47 ms ± 88 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 43.8 ms ± 1.44 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 46.3 ms ± 2.22 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 1.63 ms ± 54.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) ``` The output with `dynamic_slice`: ``` CPU times: user 1.93 ms, sys: 520 µs, total: 2.45 ms Wall time: 2.11 ms 1.27 ms ± 19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 199 ms ± 2.67 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 196 ms ± 1.85 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 1.38 ms ± 27.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) ``` I think this indicates we should probably switch to `dynamic_slice`, and JITcompile by default. That will take some thinking, however, because the argument that must be marked static may be an array.","Hi! Are there any updates/workarounds? The code sample posted by OP hangs indefinitely for me on v0.3.15: ```python print('jax + numpy') c = [batch for batch in tqdm(jnp.array_split(onp.arange(100_000), 10_000))] d = [batch for batch in tqdm(jnp.array_split(onp.arange(100_000), 10_000))] ```",This is also true for `split`: CC(`jnp.split()` has long compile times for large numbers of splits)
592,"以下是一个github上的jax下的一个issue, 标题是(tests: add unsigned ints to all_dtypes)， 内容是 (In the course of testing the uint64 promotion change ( CC(type promotion: disable uint64>float)), I realized that we have poor test coverage of this because in `lax_numpy_test.py`, `all_dtypes` does not include unsigned! This PR rectifies this, as well as making a modification to `jax.test_util.default_rng` to avoid generating large unsigned integers in the standard case.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,tests: add unsigned ints to all_dtypes,"In the course of testing the uint64 promotion change ( CC(type promotion: disable uint64>float)), I realized that we have poor test coverage of this because in `lax_numpy_test.py`, `all_dtypes` does not include unsigned! This PR rectifies this, as well as making a modification to `jax.test_util.default_rng` to avoid generating large unsigned integers in the standard case.",2022-02-03T22:43:13Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9441
1235,"以下是一个github上的jax下的一个issue, 标题是(jax.tree_util.Partial is not hash-stable in jax>=0.2.22)， 内容是 (Following CC(Fixes for Python 3.10.) , `jax.tree_util.Partial.func` is no more hashstable: jax==0.2.21 ```python >>> import jax >>> from jax.tree_util import Partial >>> from functools import partial >>> fun = partial(print, 1) >>> hash(fun) 278539187 >>> hash(Partial(fun, 2).func) 278539187 >>> hash(Partial(fun, 2).func) 278539187 ``` latest release ```python >>> from jax.tree_util import Partial >>> from functools import partial >>> fun = partial(print, 1) >>> hash(fun) 8749850128694 >>> hash(Partial(fun, 2).func) 8750211514691 >>> hash(Partial(fun, 2).func) 8750211514664  instead, if the function wrapped is just a standard function, everything works fine >>> hash(Partial(print, 1).func) 56249262 >>> hash(Partial(print, 1).func) 56249262 ``` This is annoying because in some code in NetKet, we were wrapping userpassed functions into a `tree_util.Partial` in order to simplify some code, but now this triggers recompilation if the user's supplied function is wrapped in a partial or not. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.tree_util.Partial is not hash-stable in jax>=0.2.22,"Following CC(Fixes for Python 3.10.) , `jax.tree_util.Partial.func` is no more hashstable: jax==0.2.21 ```python >>> import jax >>> from jax.tree_util import Partial >>> from functools import partial >>> fun = partial(print, 1) >>> hash(fun) 278539187 >>> hash(Partial(fun, 2).func) 278539187 >>> hash(Partial(fun, 2).func) 278539187 ``` latest release ```python >>> from jax.tree_util import Partial >>> from functools import partial >>> fun = partial(print, 1) >>> hash(fun) 8749850128694 >>> hash(Partial(fun, 2).func) 8750211514691 >>> hash(Partial(fun, 2).func) 8750211514664  instead, if the function wrapped is just a standard function, everything works fine >>> hash(Partial(print, 1).func) 56249262 >>> hash(Partial(print, 1).func) 56249262 ``` This is annoying because in some code in NetKet, we were wrapping userpassed functions into a `tree_util.Partial` in order to simplify some code, but now this triggers recompilation if the user's supplied function is wrapped in a partial or not. ",2022-02-03T13:54:25Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/9429," CC(Ensure that tree_util.Partial's .func attribute is stable.) fixes the problem identified in this bug. Note that `.func` will still not return the original object: instead it returns a wrapper object that has the same hash and compares as equal to the original function. If we wanted to avoid that, we'd need to reimplement `Partial` without subclassing `functools.partial`.","That's great, thanks and solves our issue! I mainly care about the hash stability so that I can use that without triggering recompilation. The fact that the object is not identical does not seem a problem to me.",We released `jax` 0.3.0 which includes this fix.
1987,"以下是一个github上的jax下的一个issue, 标题是(add core.cur_level() function to get trace level)， 内容是 (This function may be useful for library authors who want to prevent common user errors related to applying JAX transformations to effectful code. By checking the level one can notice if new transformations have been applied when calling effectful functions (e.g. to get a new PRNG key) in the middle of an effectful context (when transformations should be avoided). This is an experiment in collaboration with particular library authors. If it's useful we may end up documenting this as a ""tier 2: libraray author API"", as opposed to a ""tier 1: public user API"". Here's how it can be used: ```python import jax def f():   print(jax.core.cur_level())   return 0. f()   0 jax.vmap(f, axis_size=3)()   1 jax.vmap(jax.vmap(f, axis_size=3), axis_size=2)()   2 jax.jvp(jax.vmap(f, axis_size=1), (), ())   2 ``` The level reflects proper transformation level and does not reflect staging level for `jit`, `pmap`, or `xmap`: ```python jax.jit(f)()   0 ``` If one wants to catch whether such staging is happening, one can check whether `jax.core.thread_local_state.trace_state.trace_stack.dynamic` is a `jax.core.EvalTrace` instance. Or one can just write a primitive for which the abstract eval rule raises an error: ```python def no_staging():   no_staging_p.bind() no_staging_p = jax.core.Primitive('no_staging') .def_impl def no_staging_impl():   pass .def_abstract_eval def no_staging_abstract_eval():   raise Exception(""can't be staged!"")  f()   fine! jax.jit(f)()   Exception: can't be staged! ``` But `jit`/`pmap`/`xmap` staging aside, the `cur_level` function added in this PR should be helpful for catching transforamtions. : * [ ] tweak this to something more like `get_hashable_trace_state`, which would include the level, sublevel, and dynamic trace type.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,add core.cur_level() function to get trace level,"This function may be useful for library authors who want to prevent common user errors related to applying JAX transformations to effectful code. By checking the level one can notice if new transformations have been applied when calling effectful functions (e.g. to get a new PRNG key) in the middle of an effectful context (when transformations should be avoided). This is an experiment in collaboration with particular library authors. If it's useful we may end up documenting this as a ""tier 2: libraray author API"", as opposed to a ""tier 1: public user API"". Here's how it can be used: ```python import jax def f():   print(jax.core.cur_level())   return 0. f()   0 jax.vmap(f, axis_size=3)()   1 jax.vmap(jax.vmap(f, axis_size=3), axis_size=2)()   2 jax.jvp(jax.vmap(f, axis_size=1), (), ())   2 ``` The level reflects proper transformation level and does not reflect staging level for `jit`, `pmap`, or `xmap`: ```python jax.jit(f)()   0 ``` If one wants to catch whether such staging is happening, one can check whether `jax.core.thread_local_state.trace_state.trace_stack.dynamic` is a `jax.core.EvalTrace` instance. Or one can just write a primitive for which the abstract eval rule raises an error: ```python def no_staging():   no_staging_p.bind() no_staging_p = jax.core.Primitive('no_staging') .def_impl def no_staging_impl():   pass .def_abstract_eval def no_staging_abstract_eval():   raise Exception(""can't be staged!"")  f()   fine! jax.jit(f)()   Exception: can't be staged! ``` But `jit`/`pmap`/`xmap` staging aside, the `cur_level` function added in this PR should be helpful for catching transforamtions. : * [ ] tweak this to something more like `get_hashable_trace_state`, which would include the level, sublevel, and dynamic trace type.",2022-02-03T06:40:36Z,pull ready,closed,3,1,https://github.com/jax-ml/jax/issues/9423,I'm going to close this PR as stale.
4320,"以下是一个github上的jax下的一个issue, 标题是(Vmap over list of haiku functions)， 内容是 ( Discussed in https://github.com/google/jax/discussions/9413  Originally posted by **wcarvalho** February  2, 2022 Hello, first, thanks for the great tool. I have a question about doing vmap over a list of haiku functions. This could be useful for example as an easy way to do multihead attention (though, I'm interested in other parallel computation as well). Following this, I tried using `hk.switch` but got an error I don't understand. Here is minimal code: ``` import jax import jax.numpy as jnp import haiku as hk  create network + initialize parameters def linear(x):     functions = [hk.Linear(64) for i in range(8)]     index = jnp.arange(len(functions))     vmap_functions = jax.vmap(lambda i, x: hk.switch(i, functions, x))     x = vmap_functions(index, x)     return x x = jnp.zeros((8, 10, 128)) net = hk.without_apply_rng(hk.transform(linear)) params = net.init(jax.random.PRNGKey(42), x) y = net.apply(params, x) ``` and this is the error: ```  TypeError                                 Traceback (most recent call last) Input In [3], in       12 x = jnp.zeros((8, 10, 128))      13 net = hk.without_apply_rng(hk.transform(linear)) > 14 params = net.init(jax.random.PRNGKey(42), x)      16 y = net.apply(params, x)      18 print(y.shape, jax.tree_map(lambda x: x.shape, params)) File ~/miniconda3/envs/acmejax/lib/python3.9/sitepackages/haiku/_src/transform.py:113, in without_state..init_fn(*args, **kwargs)     112 def init_fn(*args, **kwargs): > 113   params, state = f.init(*args, **kwargs)     114   if state:     115     raise ValueError(""If your transformed function uses `hk.{get,set}_state` ""     116                      ""then use `hk.transform_with_state`."") File ~/miniconda3/envs/acmejax/lib/python3.9/sitepackages/haiku/_src/transform.py:364, in transform_with_state..init_fn(rng, *args, **kwargs)     362 with base.new_context(rng=rng) as ctx:     363   try: > 364     f(*args, **kwargs)     365   except jax.errors.UnexpectedTracerError as e:     366     raise jax.errors.UnexpectedTracerError(unexpected_tracer_hint) from e Input In [3], in linear(x)       5 index = jnp.arange(len(functions))       7 vmap_functions = jax.vmap(lambda i, x: hk.switch(i, functions, x)) > 8 x = vmap_functions(index, x)      10 return x     [... skipping hidden 3 frame] Input In [3], in linear..(i, x)       4 functions = [hk.Linear(64) for i in range(8)]       5 index = jnp.arange(len(functions)) > 7 vmap_functions = jax.vmap(lambda i, x: hk.switch(i, functions, x))       8 x = vmap_functions(index, x)      10 return x File ~/miniconda3/envs/acmejax/lib/python3.9/sitepackages/haiku/_src/stateful.py:447, in switch(index, branches, operand)     445 stateful_branch_mem = _memoize_by_id(stateful_branch)     446 state = internal_state() > 447 out, state = jax.lax.switch(     448     index, tuple(map(stateful_branch_mem, branches)), (state, operand))     449 update_internal_state(state)     450 return out     [... skipping hidden 2 frame] File ~/miniconda3/envs/acmejax/lib/python3.9/sitepackages/jax/_src/lax/control_flow.py:2192, in _check_tree_and_avals(what, tree1, avals1, tree2, avals2)    2185 """"""Raises TypeError if (tree1, avals1) does not match (tree2, avals2).    2186     2187 Corresponding `tree` and `avals` must match in the sense that the number of    2188 leaves in `tree` must be equal to the length of `avals`. `what` will be    2189 prepended to details of the mismatch in TypeError.    2190 """"""    2191 if tree1 != tree2: > 2192   raise TypeError(    2193       f""{what} must have same type structure, got {tree1} and {tree2}."")    2194 if not all(_map(core.typematch, avals1, avals2)):    2195   diff = tree_multimap(_show_diff, tree_unflatten(tree1, avals1),    2196                        tree_unflatten(tree2, avals2)) TypeError: branch 0 and 1 outputs must have same type structure, got PyTreeDef((*, CustomNode(namedtuple[], [CustomNode([(, ('linear',))], [{'b': *, 'w': *}]), CustomNode([(, ())], []), (*, (*,))]))) and PyTreeDef((*, CustomNode(namedtuple[], [CustomNode([(, ('linear_1',))], [{'b': *, 'w': *}]), CustomNode([(, ())], []), (*, (*,))]))). ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Vmap over list of haiku functions," Discussed in https://github.com/google/jax/discussions/9413  Originally posted by **wcarvalho** February  2, 2022 Hello, first, thanks for the great tool. I have a question about doing vmap over a list of haiku functions. This could be useful for example as an easy way to do multihead attention (though, I'm interested in other parallel computation as well). Following this, I tried using `hk.switch` but got an error I don't understand. Here is minimal code: ``` import jax import jax.numpy as jnp import haiku as hk  create network + initialize parameters def linear(x):     functions = [hk.Linear(64) for i in range(8)]     index = jnp.arange(len(functions))     vmap_functions = jax.vmap(lambda i, x: hk.switch(i, functions, x))     x = vmap_functions(index, x)     return x x = jnp.zeros((8, 10, 128)) net = hk.without_apply_rng(hk.transform(linear)) params = net.init(jax.random.PRNGKey(42), x) y = net.apply(params, x) ``` and this is the error: ```  TypeError                                 Traceback (most recent call last) Input In [3], in       12 x = jnp.zeros((8, 10, 128))      13 net = hk.without_apply_rng(hk.transform(linear)) > 14 params = net.init(jax.random.PRNGKey(42), x)      16 y = net.apply(params, x)      18 print(y.shape, jax.tree_map(lambda x: x.shape, params)) File ~/miniconda3/envs/acmejax/lib/python3.9/sitepackages/haiku/_src/transform.py:113, in without_state..init_fn(*args, **kwargs)     112 def init_fn(*args, **kwargs): > 113   params, state = f.init(*args, **kwargs)     114   if state:     115     raise ValueError(""If your transformed function uses `hk.{get,set}_state` ""     116                      ""then use `hk.transform_with_state`."") File ~/miniconda3/envs/acmejax/lib/python3.9/sitepackages/haiku/_src/transform.py:364, in transform_with_state..init_fn(rng, *args, **kwargs)     362 with base.new_context(rng=rng) as ctx:     363   try: > 364     f(*args, **kwargs)     365   except jax.errors.UnexpectedTracerError as e:     366     raise jax.errors.UnexpectedTracerError(unexpected_tracer_hint) from e Input In [3], in linear(x)       5 index = jnp.arange(len(functions))       7 vmap_functions = jax.vmap(lambda i, x: hk.switch(i, functions, x)) > 8 x = vmap_functions(index, x)      10 return x     [... skipping hidden 3 frame] Input In [3], in linear..(i, x)       4 functions = [hk.Linear(64) for i in range(8)]       5 index = jnp.arange(len(functions)) > 7 vmap_functions = jax.vmap(lambda i, x: hk.switch(i, functions, x))       8 x = vmap_functions(index, x)      10 return x File ~/miniconda3/envs/acmejax/lib/python3.9/sitepackages/haiku/_src/stateful.py:447, in switch(index, branches, operand)     445 stateful_branch_mem = _memoize_by_id(stateful_branch)     446 state = internal_state() > 447 out, state = jax.lax.switch(     448     index, tuple(map(stateful_branch_mem, branches)), (state, operand))     449 update_internal_state(state)     450 return out     [... skipping hidden 2 frame] File ~/miniconda3/envs/acmejax/lib/python3.9/sitepackages/jax/_src/lax/control_flow.py:2192, in _check_tree_and_avals(what, tree1, avals1, tree2, avals2)    2185 """"""Raises TypeError if (tree1, avals1) does not match (tree2, avals2).    2186     2187 Corresponding `tree` and `avals` must match in the sense that the number of    2188 leaves in `tree` must be equal to the length of `avals`. `what` will be    2189 prepended to details of the mismatch in TypeError.    2190 """"""    2191 if tree1 != tree2: > 2192   raise TypeError(    2193       f""{what} must have same type structure, got {tree1} and {tree2}."")    2194 if not all(_map(core.typematch, avals1, avals2)):    2195   diff = tree_multimap(_show_diff, tree_unflatten(tree1, avals1),    2196                        tree_unflatten(tree2, avals2)) TypeError: branch 0 and 1 outputs must have same type structure, got PyTreeDef((*, CustomNode(namedtuple[], [CustomNode([(, ('linear',))], [{'b': *, 'w': *}]), CustomNode([(, ())], []), (*, (*,))]))) and PyTreeDef((*, CustomNode(namedtuple[], [CustomNode([(, ('linear_1',))], [{'b': *, 'w': *}]), CustomNode([(, ())], []), (*, (*,))]))). ```",2022-02-02T15:46:08Z,,closed,0,1,https://github.com/jax-ml/jax/issues/9414,This was answered over in https://github.com/deepmind/dmhaiku/issues/300
2848,"以下是一个github上的jax下的一个issue, 标题是(Unexpected result when using jax.jit and returning array and its transpose.)， 内容是 (I'm getting a really weird result that I can't understand, hopefully this is reproducible: ```python from jax import numpy as jnp, jit def doit(A):     A = A.at[0, 0].add(1).at[0, 1].add(1).at[1, 0].add(1)     return A, A.T jit(doit)(jnp.zeros((4, 4), int))  (DeviceArray([[1, 1, 0, 0],                [1, 0, 0, 0],                [0, 0, 0, 0],                [0, 0, 0, 0]], dtype=int32),   DeviceArray([[1, 2, 0, 0],                [1, 0, 0, 0],                [0, 0, 0, 0],                [0, 0, 0, 0]], dtype=int32)) ``` This is only a problem when using the GPU, using the CPU gives the correct result: ```python jit(doit, backend='cpu')(jnp.zeros((4, 4), int))  (DeviceArray([[1, 1, 0, 0],                [1, 0, 0, 0],                [0, 0, 0, 0],                [0, 0, 0, 0]], dtype=int32),   DeviceArray([[1, 1, 0, 0],                [1, 0, 0, 0],                [0, 0, 0, 0],                [0, 0, 0, 0]], dtype=int32)) ``` Using a different dtype e.g. float has the same behaviour. This only happens when using a dimension that is a multiple of 4x4: ```python for n in range(2, 33):     A, AT = jit(doit)(jnp.zeros((n, n), float))     if (AT[:2, :2] == jnp.array([[1., 2.], [1., 0.]])).all():         print(n)  4  8  12  16  20  24  28  32 ``` And returning only the transpose is fine: ```python def doit2(A):     A = A.at[0, 0].add(1).at[0, 1].add(1).at[1, 0].add(1)     return A.T jit(doit2)(jnp.zeros((4, 4), float))  DeviceArray([[1., 1., 0., 0.],               [1., 0., 0., 0.],               [0., 0., 0., 0.],               [0., 0., 0., 0.]], dtype=float32) ``` Returning both but 'ravel'ed is also ok: ```python def doit3(A):     A = A.at[0, 0].add(1).at[0, 1].add(1).at[1, 0].add(1)     return A.ravel(), A.T.ravel() jit(doit3)(jnp.zeros((4, 4), float))  (DeviceArray([1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,                0.], dtype=float32),   DeviceArray([1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,                0.], dtype=float32)) ``` Returning their sums is also ok: ```python def doit4(A):     A = A.at[0, 0].add(1).at[0, 1].add(1).at[1, 0].add(1)     return A + A.T jit(doit4)(jnp.zeros((4, 4), float))  DeviceArray([[2., 2., 0., 0.],               [2., 0., 0., 0.],               [0., 0., 0., 0.],               [0., 0., 0., 0.]], dtype=float32) ``` **Operating System:** 5.16.4arch11 **GPU:** GTX 1050 Ti 4GB **Python:** 3.10.2 **JAX:** 0.2.26 **CUDA:** 11.5 **cuDNN:** 8.3.0 **Nvdia Driver:** 495.46 I installed JAX using: ``` pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Unexpected result when using jax.jit and returning array and its transpose.,"I'm getting a really weird result that I can't understand, hopefully this is reproducible: ```python from jax import numpy as jnp, jit def doit(A):     A = A.at[0, 0].add(1).at[0, 1].add(1).at[1, 0].add(1)     return A, A.T jit(doit)(jnp.zeros((4, 4), int))  (DeviceArray([[1, 1, 0, 0],                [1, 0, 0, 0],                [0, 0, 0, 0],                [0, 0, 0, 0]], dtype=int32),   DeviceArray([[1, 2, 0, 0],                [1, 0, 0, 0],                [0, 0, 0, 0],                [0, 0, 0, 0]], dtype=int32)) ``` This is only a problem when using the GPU, using the CPU gives the correct result: ```python jit(doit, backend='cpu')(jnp.zeros((4, 4), int))  (DeviceArray([[1, 1, 0, 0],                [1, 0, 0, 0],                [0, 0, 0, 0],                [0, 0, 0, 0]], dtype=int32),   DeviceArray([[1, 1, 0, 0],                [1, 0, 0, 0],                [0, 0, 0, 0],                [0, 0, 0, 0]], dtype=int32)) ``` Using a different dtype e.g. float has the same behaviour. This only happens when using a dimension that is a multiple of 4x4: ```python for n in range(2, 33):     A, AT = jit(doit)(jnp.zeros((n, n), float))     if (AT[:2, :2] == jnp.array([[1., 2.], [1., 0.]])).all():         print(n)  4  8  12  16  20  24  28  32 ``` And returning only the transpose is fine: ```python def doit2(A):     A = A.at[0, 0].add(1).at[0, 1].add(1).at[1, 0].add(1)     return A.T jit(doit2)(jnp.zeros((4, 4), float))  DeviceArray([[1., 1., 0., 0.],               [1., 0., 0., 0.],               [0., 0., 0., 0.],               [0., 0., 0., 0.]], dtype=float32) ``` Returning both but 'ravel'ed is also ok: ```python def doit3(A):     A = A.at[0, 0].add(1).at[0, 1].add(1).at[1, 0].add(1)     return A.ravel(), A.T.ravel() jit(doit3)(jnp.zeros((4, 4), float))  (DeviceArray([1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,                0.], dtype=float32),   DeviceArray([1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,                0.], dtype=float32)) ``` Returning their sums is also ok: ```python def doit4(A):     A = A.at[0, 0].add(1).at[0, 1].add(1).at[1, 0].add(1)     return A + A.T jit(doit4)(jnp.zeros((4, 4), float))  DeviceArray([[2., 2., 0., 0.],               [2., 0., 0., 0.],               [0., 0., 0., 0.],               [0., 0., 0., 0.]], dtype=float32) ``` **Operating System:** 5.16.4arch11 **GPU:** GTX 1050 Ti 4GB **Python:** 3.10.2 **JAX:** 0.2.26 **CUDA:** 11.5 **cuDNN:** 8.3.0 **Nvdia Driver:** 495.46 I installed JAX using: ``` pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html ```",2022-02-01T05:46:05Z,bug XLA,closed,0,3,https://github.com/jax-ml/jax/issues/9399,"Thanks, this looks like an XLA:GPU bug. I reported it to the XLA folks (internal bug b/217359989; their usual bug tracker is internal).","No resolution on this yet, but the XLA folks are looking at it. It's a bit subtle.",I realized I never followed up on this bug. It should be fixed in the current jax/jaxlib release.  (This became a long and interesting exercise in tracking down some incorrect assumptions in the XLA buffer aliasing logic.)
306,"以下是一个github上的jax下的一个issue, 标题是(Remove unused _asarray utility)， 内容是 (I think we gradually replaced this with `_check_arraylike`, but never got around to deleting it.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Remove unused _asarray utility,"I think we gradually replaced this with `_check_arraylike`, but never got around to deleting it.",2022-01-31T20:48:13Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9393
1574,"以下是一个github上的jax下的一个issue, 标题是(Implement the JAX transfer guard API)， 内容是 (Implement the JAX transfer guard API Adds `jax_transfer_guard` flag and `jax.transfer_guard()` context manager that allows logging or disallowing unintended transfers. The API distinguishes between two types of transfers: * explicit transfers: `jax.device_put*()` and `jax.device_get()` calls. * implicit transfers: Other transfers (e.g., printing a `DeviceArray`). The transfer guard can take an action based on its guard level: * ""allow"": Silently allow all transfers (default; same as the previous behavior). * ""log"": Log and allow implicit transfers. Silently allow explicit transfers. * ""disallow"": Disallow implicit transfers. Silently allow explicit transfers. * ""log_explicit"": Log and allow all transfers. * ""disallow_explicit"": Disallow all transfers. The API also allows finecontrol the transfer guard level of individual transfer directions. Their flag and context manager names are suffixed with the transfer direction: * ""host_to_device"": Converting a Python value into a `DeviceBuffer`. * ""device_to_device"": Copying a `DeviceBuffer` to a different device. * ""device_to_host"": Fetching the value of a `DeviceBuffer`. Example: ``` x = jnp.array(1) y = jnp.array(2) z = jnp.array(3) print(x)   No error with jax.transfer_guard(""disallow""):   print(x)   No error; x is already fetched   print(jax.device_get(y))   No error   print(z)   Error! ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Implement the JAX transfer guard API,"Implement the JAX transfer guard API Adds `jax_transfer_guard` flag and `jax.transfer_guard()` context manager that allows logging or disallowing unintended transfers. The API distinguishes between two types of transfers: * explicit transfers: `jax.device_put*()` and `jax.device_get()` calls. * implicit transfers: Other transfers (e.g., printing a `DeviceArray`). The transfer guard can take an action based on its guard level: * ""allow"": Silently allow all transfers (default; same as the previous behavior). * ""log"": Log and allow implicit transfers. Silently allow explicit transfers. * ""disallow"": Disallow implicit transfers. Silently allow explicit transfers. * ""log_explicit"": Log and allow all transfers. * ""disallow_explicit"": Disallow all transfers. The API also allows finecontrol the transfer guard level of individual transfer directions. Their flag and context manager names are suffixed with the transfer direction: * ""host_to_device"": Converting a Python value into a `DeviceBuffer`. * ""device_to_device"": Copying a `DeviceBuffer` to a different device. * ""device_to_host"": Fetching the value of a `DeviceBuffer`. Example: ``` x = jnp.array(1) y = jnp.array(2) z = jnp.array(3) print(x)   No error with jax.transfer_guard(""disallow""):   print(x)   No error; x is already fetched   print(jax.device_get(y))   No error   print(z)   Error! ```",2022-01-31T19:58:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9392
1844,"以下是一个github上的jax下的一个issue, 标题是(jax2tf not working with conv got an unexpected keyword argument 'batch_group_count')， 内容是 (Please: Following jax2tf tutorial ```python (predict_fn, predict_params)  = mnist_flax.train() input_signatures = [     tf.TensorSpec((None,) + mnist_lib.input_shape, tf.float32), ] saved_model_lib.convert_and_save_model(         predict_fn,         predict_params,         ""model"",         input_signatures=input_signatures,         polymorphic_shapes=""(batch, ...)"",         compile_model=False) ``` ```    1499 def gen_conv(lhs, rhs, preferred_element_type: Optional[DType]): > 1500   out = tfxla.conv(    1501       lhs,    1502       rhs,    1503       window_strides,    1504       padding,    1505       lhs_dilation,    1506       rhs_dilation,    1507       dnums_proto,    1508       feature_group_count=feature_group_count,    1509       batch_group_count=batch_group_count,    1510       precision_config=precision_config_proto,    1511       preferred_element_type=preferred_element_type,    1512       use_v2=True)    1513    TODO: implement shape inference for XlaConv    1514   out.set_shape(out_tf_shape) TypeError: conv() got an unexpected keyword argument 'batch_group_count' ``` jax, jaxlib and tf versions Tested on both ubuntu 20.04 with cuda 11.5 and cudnn 8.2 and macos m1 with and without metal TF installed !image As i can see batch_group_count was added to xla.conv at 2.8 branch so it wouldnt work with stable tf (2.7) version https://github.com/tensorflow/tensorflow/commit/bfbb0595eee80f1032b44ced8de1eecf83d4c3c3 . I have a small headache trying to debug it 😄  So It is a bug? or should there only be a docs mention about that as jax isnt stable yet?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax2tf not working with conv got an unexpected keyword argument 'batch_group_count',"Please: Following jax2tf tutorial ```python (predict_fn, predict_params)  = mnist_flax.train() input_signatures = [     tf.TensorSpec((None,) + mnist_lib.input_shape, tf.float32), ] saved_model_lib.convert_and_save_model(         predict_fn,         predict_params,         ""model"",         input_signatures=input_signatures,         polymorphic_shapes=""(batch, ...)"",         compile_model=False) ``` ```    1499 def gen_conv(lhs, rhs, preferred_element_type: Optional[DType]): > 1500   out = tfxla.conv(    1501       lhs,    1502       rhs,    1503       window_strides,    1504       padding,    1505       lhs_dilation,    1506       rhs_dilation,    1507       dnums_proto,    1508       feature_group_count=feature_group_count,    1509       batch_group_count=batch_group_count,    1510       precision_config=precision_config_proto,    1511       preferred_element_type=preferred_element_type,    1512       use_v2=True)    1513    TODO: implement shape inference for XlaConv    1514   out.set_shape(out_tf_shape) TypeError: conv() got an unexpected keyword argument 'batch_group_count' ``` jax, jaxlib and tf versions Tested on both ubuntu 20.04 with cuda 11.5 and cudnn 8.2 and macos m1 with and without metal TF installed !image As i can see batch_group_count was added to xla.conv at 2.8 branch so it wouldnt work with stable tf (2.7) version https://github.com/tensorflow/tensorflow/commit/bfbb0595eee80f1032b44ced8de1eecf83d4c3c3 . I have a small headache trying to debug it 😄  So It is a bug? or should there only be a docs mention about that as jax isnt stable yet?",2022-01-31T15:33:08Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/9384,"Indeed, jax2tf works best with more recent versions of Tensorflow. I see that 2.8.0 is now the most recent version of TF.  I will add a check for a better error message when using convolutions with batch_group_count with TF < 2.8.0."
428,"以下是一个github上的jax下的一个issue, 标题是(Instead of getting users to run a tree_map over gdas, etc and run asyncio.run, absorb those APIs into the gda serde library.)， 内容是 (Instead of getting users to run a tree_map over gdas, etc and run asyncio.run, absorb those APIs into the gda serde library.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Instead of getting users to run a tree_map over gdas, etc and run asyncio.run, absorb those APIs into the gda serde library.","Instead of getting users to run a tree_map over gdas, etc and run asyncio.run, absorb those APIs into the gda serde library.",2022-01-28T16:20:11Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9367
4009,"以下是一个github上的jax下的一个issue, 标题是(Significant performance loss caused by matrix indexing in jitted function with nested jax operations)， 内容是 (Hi all, I've recently come across a very strange behaviour within a jitted jax function (so far only tested on CPU). The key issue is that the code is becoming insanely slow when a particular array access operation is performed inside a function body but runs a lot faster in other instances which should have comparable computational time. Unfortunately the issue seems to appear in a very specific mix of vmaps, while loops and conds, so that the following minimal example is a couple of lines long. What this example does is that it essentially evaluates contractions over masked matrices in a rather unconventional way by using while loops to do the contraction over the two matrix indices (which are used to avoid recompilation if the lengths of the indices allowed by the masks change, which will be the case in our application). The defined function takes the matrix which should be contracted, batches of valid indices for the first dimension (padded with ""1""s), a single array of valid indices for the second matrix dimension (again padded with ""1""s) and returns the batch of contracted matrix values. Interestingly, when the contracted value is chosen to be \sum matrix[i,i] * j rather than the value we actually want, \sum matrix[i,j], the code runs a lot faster (approx. factor 10 for the chosen dim=500 example). This can definitely not be right or am I missing something? Unfortunately I have not been able to track this issue further down the stack, the only other interesting observation I made is that the timing discrepancies seem to go away when the vmap across the batch dimension is left out (which to me looks like an indication that there is some issue with the vmapping). The minimal example is defined as follows: ```python import jax import jax.numpy as jnp def masked_contraction(my_array, stacked_mask_indices_i, mask_indices_j, comparison=False):     def contract_array(mask_indices_i):         """"""The loop_state arguments are just tuples where the first item holds the         count and the second the accumulated value.         """"""         def first_index_loop(outer_loop_state):             i = mask_indices_i[outer_loop_state[0]]             def compute_inner(_):                 def inner_loop(inner_loop_state):                     j = mask_indices_j[inner_loop_state[0]]                     if comparison:                          This case is much faster than the one below                         return (inner_loop_state[0] + 1, inner_loop_state[1] + my_array[i, i] * j)                     else:                          This is the case we are actually interested in (which is very slow)                         return (inner_loop_state[0] + 1, inner_loop_state[1] + my_array[i, j])                 return jax.lax.while_loop(lambda x: mask_indices_j[x[0]] != 1, inner_loop, (0, 0.))[1]             """"""             The following cond is pretty much useless in this case but it seems to amplify the issues,             without it the runtime differences are less substantial (still significant though).             """"""             value = jax.lax.cond(i %timeit masked_contraction_jitted(testarray, mask_indices_i_stacked, mask_indices_j, True) > 491 µs ± 10.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) > %timeit masked_contraction_jitted(testarray, mask_indices_i_stacked, mask_indices_j, False) > 62.9 ms ± 27.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) It would be great if somebody could look into this and help me understand what is going on. Please let me know if you require further information or if I should post this somewhere else (at the moment this really seems like a bug to me so I'm posting it here). Thank you very much and best wishes, Yannic)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Significant performance loss caused by matrix indexing in jitted function with nested jax operations,"Hi all, I've recently come across a very strange behaviour within a jitted jax function (so far only tested on CPU). The key issue is that the code is becoming insanely slow when a particular array access operation is performed inside a function body but runs a lot faster in other instances which should have comparable computational time. Unfortunately the issue seems to appear in a very specific mix of vmaps, while loops and conds, so that the following minimal example is a couple of lines long. What this example does is that it essentially evaluates contractions over masked matrices in a rather unconventional way by using while loops to do the contraction over the two matrix indices (which are used to avoid recompilation if the lengths of the indices allowed by the masks change, which will be the case in our application). The defined function takes the matrix which should be contracted, batches of valid indices for the first dimension (padded with ""1""s), a single array of valid indices for the second matrix dimension (again padded with ""1""s) and returns the batch of contracted matrix values. Interestingly, when the contracted value is chosen to be \sum matrix[i,i] * j rather than the value we actually want, \sum matrix[i,j], the code runs a lot faster (approx. factor 10 for the chosen dim=500 example). This can definitely not be right or am I missing something? Unfortunately I have not been able to track this issue further down the stack, the only other interesting observation I made is that the timing discrepancies seem to go away when the vmap across the batch dimension is left out (which to me looks like an indication that there is some issue with the vmapping). The minimal example is defined as follows: ```python import jax import jax.numpy as jnp def masked_contraction(my_array, stacked_mask_indices_i, mask_indices_j, comparison=False):     def contract_array(mask_indices_i):         """"""The loop_state arguments are just tuples where the first item holds the         count and the second the accumulated value.         """"""         def first_index_loop(outer_loop_state):             i = mask_indices_i[outer_loop_state[0]]             def compute_inner(_):                 def inner_loop(inner_loop_state):                     j = mask_indices_j[inner_loop_state[0]]                     if comparison:                          This case is much faster than the one below                         return (inner_loop_state[0] + 1, inner_loop_state[1] + my_array[i, i] * j)                     else:                          This is the case we are actually interested in (which is very slow)                         return (inner_loop_state[0] + 1, inner_loop_state[1] + my_array[i, j])                 return jax.lax.while_loop(lambda x: mask_indices_j[x[0]] != 1, inner_loop, (0, 0.))[1]             """"""             The following cond is pretty much useless in this case but it seems to amplify the issues,             without it the runtime differences are less substantial (still significant though).             """"""             value = jax.lax.cond(i %timeit masked_contraction_jitted(testarray, mask_indices_i_stacked, mask_indices_j, True) > 491 µs ± 10.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) > %timeit masked_contraction_jitted(testarray, mask_indices_i_stacked, mask_indices_j, False) > 62.9 ms ± 27.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) It would be great if somebody could look into this and help me understand what is going on. Please let me know if you require further information or if I should post this somewhere else (at the moment this really seems like a bug to me so I'm posting it here). Thank you very much and best wishes, Yannic",2022-01-28T15:59:21Z,bug performance needs info,closed,0,5,https://github.com/jax-ml/jax/issues/9366,"At first glance it looks like this might be the vmapofcondisselect bug/limitation. Namely, that an unvmap'd `lax.cond` only evaluates the branch it needs to, but a vmap'd `lax.cond` turns into a `lax.select`, and evaluates both branches unconditionally (before merging the results together).","Hi kidger, Thanks for the reply, that's an interesting shout! Could you point me to the bug description for that issue, I'd need to read up a little bit more on that to fully understand this issue. However, I don't really see why this bug would give us two different runtimes for the two different cases tested (if we use array[i,i] *j instead of array[i.j] inside the inner loop the computation is much faster). If really both branches are evaluated when the vmap is applied, this should be the same in both test cases (and thus slowing down both calculations) or not?","Just looking at the code, I suspect the performance issue comes from the compiler's ability to parallelize the expensive parts of your computation. Gather (i.e. indexing) operations are relatively costly, and your inner loop has two of them: in the fast version, they can be done in parallel. In the slow version, the output of the first (`j`) becomes the input of the second, so they must be done sequentially. You could dig into this more using some of JAX's profiling tools: https://jax.readthedocs.io/en/latest/profiling.html","Thanks for taking a look at the code! It must be something like that, however the slowdown goes away when not applying the vmap (i.e. when I just call the function for the two different masks one after the other the timings are as expected even with the full indexing inside the innermost loop), so I really suspect there is some compiler optimization which is not correctly applied in the vmaped case. I'll try to get some profiling data later and post them here (unfortunately I don't really understand the generated XLA language enough to chase down this bug on the XLA level)...","Sorry for leaving this issue untouched for so long. I think I have a much better understanding of what is going on now and I see where the performance discrepancies come from. I'm therefore finally closing this issue (after I had forgotten about it a little bit to be honest). It should really be what has been stated before: Without the vmap, the cond (and likely also the while) can shortcut so that the executed loop is constrained to the true cases, which is no longer possible with the vmapping. I don't fully understand how while loops inside a vmap behave but it must be similar (in the sense that all while loops are to some degree extended to the same execution length for each individual run inside the vmap). The comparison case above with the alternative indexing was indeed not really sensible as this allows for much more compiler optimization + parallelization (explaining why  this was faster). Anyway, thanks again for the really helpful input, which definitely helped me understand jax a little bit better!"
1762,"以下是一个github上的jax下的一个issue, 标题是(Cannot import jax.experimental.pjit in Colab with version 0.2.27)， 内容是 (Hi,  I came across the following behavior today:  In Colab, today, the current default jax version is 0.2.25 and I can import jax.experimental.pjit ```from jax.experimental import pjit``` Though, if I update jax[tpu] to the 0.2.27 version with  ```!pip install jax[tpu]==0.2.27 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html``` I get the following error when importing jax.experimental.pjit:        AttributeError                            Traceback (most recent call last)        in ()       > 1 from jax.experimental import pjit       1 frames       /usr/local/lib/python3.7/distpackages/jax/experimental/pjit.py in ()            22             23 from jax.experimental import maps       > 24 from jax.experimental.global_device_array import GlobalDeviceArray as GDA            25 from jax import core            26 from jax import linear_util as lu       /usr/local/lib/python3.7/distpackages/jax/experimental/global_device_array.py in ()           431   return lambda bufs: GlobalDeviceArray(           432       global_aval.shape, global_mesh, out_axis_resources, bufs, fast_path_args)       > 433 pxla.global_result_handlers[core.ShapedArray] = _gda_array_result_handler           434 pxla.global_result_handlers[core.ConcreteArray] = _gda_array_result_handler       AttributeError: module 'jax.interpreters.pxla' has no attribute 'global_result_handlers' I'm not sure what it means, and I'll work with previous jax version but I thought it would be interesting to point this out. Thanks, Leo)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Cannot import jax.experimental.pjit in Colab with version 0.2.27,"Hi,  I came across the following behavior today:  In Colab, today, the current default jax version is 0.2.25 and I can import jax.experimental.pjit ```from jax.experimental import pjit``` Though, if I update jax[tpu] to the 0.2.27 version with  ```!pip install jax[tpu]==0.2.27 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html``` I get the following error when importing jax.experimental.pjit:        AttributeError                            Traceback (most recent call last)        in ()       > 1 from jax.experimental import pjit       1 frames       /usr/local/lib/python3.7/distpackages/jax/experimental/pjit.py in ()            22             23 from jax.experimental import maps       > 24 from jax.experimental.global_device_array import GlobalDeviceArray as GDA            25 from jax import core            26 from jax import linear_util as lu       /usr/local/lib/python3.7/distpackages/jax/experimental/global_device_array.py in ()           431   return lambda bufs: GlobalDeviceArray(           432       global_aval.shape, global_mesh, out_axis_resources, bufs, fast_path_args)       > 433 pxla.global_result_handlers[core.ShapedArray] = _gda_array_result_handler           434 pxla.global_result_handlers[core.ConcreteArray] = _gda_array_result_handler       AttributeError: module 'jax.interpreters.pxla' has no attribute 'global_result_handlers' I'm not sure what it means, and I'll work with previous jax version but I thought it would be interesting to point this out. Thanks, Leo",2022-01-26T16:56:26Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/9337,"`pip install jax[tpu]` is designed for Google Cloud TPU, and is not appropriate for Colab TPU runtimes which use an older TPU architecture. To update JAX on Google Colab TPU, you can use the CPU version of jaxlib: ```python !pip install jax[cpu]==0.2.27 ``` Then you can setup the TPU and import `pjit` like this: ```python import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() from jax.experimental import pjit ```",Hopefully the doc update in CC(README: clarify installation instructions for Colab TPU) will clarify this.,"Thank you, it solves the issue 👍"
728,"以下是一个github上的jax下的一个issue, 标题是(fix caching example in jit tutorial)， 内容是 (closes CC(unjitted_loop_body is not constantly recompiled in the JIT tutorial )  This pull request fixes the problem as discussed in the above issue. To summarise, the caching in JAX can handle the original example, however it is not able to cache things like partial(f), lambda x: f(x) when calling `jax.jit` inside the loop. Which I've updated the paragraph for and showed the performance degradation.  I ran the notebook on colab, and then ran `jupytext sync` on the notebook. Happy to make any modifications! )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,fix caching example in jit tutorial,"closes CC(unjitted_loop_body is not constantly recompiled in the JIT tutorial )  This pull request fixes the problem as discussed in the above issue. To summarise, the caching in JAX can handle the original example, however it is not able to cache things like partial(f), lambda x: f(x) when calling `jax.jit` inside the loop. Which I've updated the paragraph for and showed the performance degradation.  I ran the notebook on colab, and then ran `jupytext sync` on the notebook. Happy to make any modifications! ",2022-01-25T05:23:34Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/9312,thanks for the feedback! I've incorporated the changes. Let me know if there's anything I can edit/add. (made another change to fix the execution counts so that's consistent).
3086,"以下是一个github上的jax下的一个issue, 标题是(Incorrect gradient of function with segment_prod)， 内容是 (Hello, I am using automatic differentiation in JAX (0.2.27, jaxlib 0.1.75, python 3.9) to obtain gradient of a simple conditional logit model during maximum likelihood estimation of the model parameters. If agents face several trials of choices, the probability of observing a sequence of choices is a product of conditional logit formulas. If I implement this in the likelihood formula using jax.ops segment_prod, the log likelihood produced by the function is correct, but the computed gradient is incorrect. I can obtain the correct gradient if the product is implemented using segment_sum and log/exp transformation. For the purpose of demonstrating the example, I attach a dummy dataset where agents face several trials of choices from different alternatives.  Choice.csv The following code reproduces the issue: ```python import jax.numpy as jnp import jax.ops as jo from numpy import genfromtxt from jax import grad  Column ""id"" in the Choice.csv file denotes individual identifier, ""choice_id"" denotes trials,   ""choice"" is a binary variable indicating whether an alternative was chosen.  The remaining columns are explanatory variables. data = genfromtxt('Choice.csv', delimiter=';', skip_header=1) id = data[:,0] id = id.astype(jnp.int64) choice_id = data[:,1] choice_id = choice_id.astype(jnp.int64) choice = data[:,2] x1 = data[:,3] x2 = data[:,4] x3 = data[:,5] def clogit_ll(b): 	numer = jnp.exp(b[0]*x1+b[1]*x2+b[2]*x3) 	ch = numer[(choice==1)] 	denom = jo.segment_sum(numer,choice_id) 	denom = jnp.delete(denom, 0) 	l1 = ch/denom 	ids = id[(choice==1)] 	ll1 = jo.segment_prod(l1,ids) 	ll1 = jnp.delete(ll1, 0)	 	lli = jnp.log(ll1) 	return(jnp.sum(lli)) coef = jnp.array([ .3400659,  .9289953,  .6646674]) grad_clogit = jax.value_and_grad(clogit_ll) grad_clogit(coef) ``` Output: ``` (DeviceArray(1477.655, dtype=float32), DeviceArray([ 377694.53, 213394.  ,  367053.06], dtype=float32)) ``` Which is not correct if we check: ```python def first_finite_differences(f, x):   eps = 1e3   return jnp.array([(f(x + eps * v)  f(x  eps * v)) / (2 * eps)                    for v in jnp.eye(len(x))]) first_finite_differences(clogit_ll, coef) ``` Output: ``` DeviceArray([146.72852,  126.95312, 173.21776], dtype=float32) ``` Now, if we replace segment_prod in the function with an exponent of segment_sum logs instead: ``` python def clogit_ll(b): 	numer = jnp.exp(b[0]*x1+b[1]*x2+b[2]*x3) 	ch = numer[(choice==1)] 	denom = jo.segment_sum(numer,choice_id) 	denom = jnp.delete(denom, 0) 	l1 = ch/denom 	ids = id[(choice==1)] 	ll1 = jnp.exp(jo.segment_sum(jnp.log(l1),ids)) 	ll1 = jnp.delete(ll1, 0)	 	lli = jnp.log(ll1) 	return(jnp.sum(lli)) grad_clogit = jax.value_and_grad(clogit_ll) grad_clogit(coef) ``` The gradient is now correct: ``` (DeviceArray(1477.655, dtype=float32), DeviceArray([146.70026 ,  126.960014, 173.30954 ], dtype=float32)) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,Incorrect gradient of function with segment_prod,"Hello, I am using automatic differentiation in JAX (0.2.27, jaxlib 0.1.75, python 3.9) to obtain gradient of a simple conditional logit model during maximum likelihood estimation of the model parameters. If agents face several trials of choices, the probability of observing a sequence of choices is a product of conditional logit formulas. If I implement this in the likelihood formula using jax.ops segment_prod, the log likelihood produced by the function is correct, but the computed gradient is incorrect. I can obtain the correct gradient if the product is implemented using segment_sum and log/exp transformation. For the purpose of demonstrating the example, I attach a dummy dataset where agents face several trials of choices from different alternatives.  Choice.csv The following code reproduces the issue: ```python import jax.numpy as jnp import jax.ops as jo from numpy import genfromtxt from jax import grad  Column ""id"" in the Choice.csv file denotes individual identifier, ""choice_id"" denotes trials,   ""choice"" is a binary variable indicating whether an alternative was chosen.  The remaining columns are explanatory variables. data = genfromtxt('Choice.csv', delimiter=';', skip_header=1) id = data[:,0] id = id.astype(jnp.int64) choice_id = data[:,1] choice_id = choice_id.astype(jnp.int64) choice = data[:,2] x1 = data[:,3] x2 = data[:,4] x3 = data[:,5] def clogit_ll(b): 	numer = jnp.exp(b[0]*x1+b[1]*x2+b[2]*x3) 	ch = numer[(choice==1)] 	denom = jo.segment_sum(numer,choice_id) 	denom = jnp.delete(denom, 0) 	l1 = ch/denom 	ids = id[(choice==1)] 	ll1 = jo.segment_prod(l1,ids) 	ll1 = jnp.delete(ll1, 0)	 	lli = jnp.log(ll1) 	return(jnp.sum(lli)) coef = jnp.array([ .3400659,  .9289953,  .6646674]) grad_clogit = jax.value_and_grad(clogit_ll) grad_clogit(coef) ``` Output: ``` (DeviceArray(1477.655, dtype=float32), DeviceArray([ 377694.53, 213394.  ,  367053.06], dtype=float32)) ``` Which is not correct if we check: ```python def first_finite_differences(f, x):   eps = 1e3   return jnp.array([(f(x + eps * v)  f(x  eps * v)) / (2 * eps)                    for v in jnp.eye(len(x))]) first_finite_differences(clogit_ll, coef) ``` Output: ``` DeviceArray([146.72852,  126.95312, 173.21776], dtype=float32) ``` Now, if we replace segment_prod in the function with an exponent of segment_sum logs instead: ``` python def clogit_ll(b): 	numer = jnp.exp(b[0]*x1+b[1]*x2+b[2]*x3) 	ch = numer[(choice==1)] 	denom = jo.segment_sum(numer,choice_id) 	denom = jnp.delete(denom, 0) 	l1 = ch/denom 	ids = id[(choice==1)] 	ll1 = jnp.exp(jo.segment_sum(jnp.log(l1),ids)) 	ll1 = jnp.delete(ll1, 0)	 	lli = jnp.log(ll1) 	return(jnp.sum(lli)) grad_clogit = jax.value_and_grad(clogit_ll) grad_clogit(coef) ``` The gradient is now correct: ``` (DeviceArray(1477.655, dtype=float32), DeviceArray([146.70026 ,  126.960014, 173.30954 ], dtype=float32)) ```",2022-01-24T10:54:43Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/9296,"Yes, this looks like a legitimate bug in `scatter_mul`'s gradient. I'm looking into it.","`scatter_mul`'s gradient is only correct if there aren't colliding indices. For now, I'm going to make the nonunique case an error. That means your example in this issue will report an error, which is much better than a wrong output. Does that suffice for your purposes? As you note, if you know your values are positive, you can do the computation in log space.","By nonunique indices you mean something like: ```python segment_ids = jnp.array([0, 0, 1, 1, 2, 2]) data = jnp.array([1, 1, 1, 1, 2, 2]) ``` where the products for indices 0 and 1 are nonunique? For the purpose of conditional logit, the log transform should work, as the individual contribution to the likelihood can't be negative, since it is a probability. A group product doesn't appear that often in likelihood functions of other estimators, at least for the moment I can't recall anything. Though, I guess it would be nice to have the option of nonunique indices with ```scatter_mul``` gradients anyway :)","Well, you'd need `segment_ids` to be all different, and to pass `unique_indices=True` to `segment_prod`. I acknowledge that makes the gradient much less useful... * Using log space is probably the best approach. * If it is useful, it would be possible for us to add a gradient that mirror's TensorFlow's gradient for its version of `segment_prod`:  https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/python/ops/math_grad.py;l=534 However I don't like that approach very much: using division feels like it has a good chance of being numerically unstable if, for example, one of the elements are small. I also looked at PyTorch for inspiration but as far as I can tell PyTorch doesn't have an equivalent differentiable operator. * Yet another approach would be to individually scatter each element into an array of ones, and then multiply the arrays. This would waste a lot of memory but has the virtue of having a derivative. * If we had a bunch of free time, it would be possible to define an efficient forwardmode derivative using a variadic version of the `scatter` operator, but unfortunately XLA doesn't implement such an operator. You could then turn the forwardmode derivative into the reversemode derivative using a trick similar to the one TF uses. So I'm tempted to just say ""use log space"" if you can.","Closing because the ""wrong output"" bug is fixed. If someone needs the `segment_prod` gradient and isn't happy with the proposed workaround, feel free to reopen.","This problem came up for me, I am completely happy with the log>add>exp pathway and agree the straight mul route could be horrifically numerically unstable. Thank you for the solution."
1335,"以下是一个github上的jax下的一个issue, 标题是(Allow comparing NamedShape to None)， 内容是 (Fixes CC(NamedShape cannot be compared with None) The description of jax.jit says `Static arguments should be hashable, meaning both __hash__ and __eq__ are implemented, and immutable.` but currently, we don't allow comparing NamedShape and None. Despite that, it seems that we can still jit a function that takes either NamedShape or None static arguments. Unfortunately, CI occasionally performs such comparison (NamedShape vs None): ``` >     return _bernoulli(key, p, shape)   type: ignore E     ValueError: static arguments should be comparable using __eq__.The following error was raised during a call to '_bernoulli' when comparing two objects of types  and . The error was: E     TypeError: NamedShape doesn't support comparisons with  E      E     At: E       /opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/sitepackages/jax/core.py(1642): __eq__ E       /opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/sitepackages/jax/_src/random.py(705): bernoulli ``` in `jax.random.bernoulli`. We can bypass the error by restarting the CI but it is a bit annoying (the errors have been happening for a few months).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Allow comparing NamedShape to None,"Fixes CC(NamedShape cannot be compared with None) The description of jax.jit says `Static arguments should be hashable, meaning both __hash__ and __eq__ are implemented, and immutable.` but currently, we don't allow comparing NamedShape and None. Despite that, it seems that we can still jit a function that takes either NamedShape or None static arguments. Unfortunately, CI occasionally performs such comparison (NamedShape vs None): ``` >     return _bernoulli(key, p, shape)   type: ignore E     ValueError: static arguments should be comparable using __eq__.The following error was raised during a call to '_bernoulli' when comparing two objects of types  and . The error was: E     TypeError: NamedShape doesn't support comparisons with  E      E     At: E       /opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/sitepackages/jax/core.py(1642): __eq__ E       /opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/sitepackages/jax/_src/random.py(705): bernoulli ``` in `jax.random.bernoulli`. We can bypass the error by restarting the CI but it is a bit annoying (the errors have been happening for a few months).",2022-01-22T17:40:28Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/9290,"Sorry for the slow response! But shouldn't we just return `False` after the two isinstance checks? That is, if `other` is not a NamedShape nor a tuple, then `self` is certainly not equal to it?",Makes sense to me. That will match tuple behavior :) ``` class A():   pass (() == A()) is False ```
1568,"以下是一个github上的jax下的一个issue, 标题是(unjitted_loop_body is not constantly re-compiled in the JIT tutorial )， 内容是 (Hello there, I could be misunderstanding something here(new to Jax still) but on the JIT tutorial it seems like the ""bad"" example is not actually that bad.  More concretely, the given example is as follows: ```python def g_inner_jitted_poorly(x, n):   i = 0   while i < n:      Don't do this!     i = jax.jit(unjitted_loop_body)(i)   return x + i ``` Contrary to the description of this example on the tutorial, it seems like the  unjitted_loop_body function is actually not being compiled every iteration, but cached somewhere.  ```python def unjitted_loop_body(prev_i):   print(""i am being jitted"")   return prev_i + 1 def g_inner_jitted_poorly(x, n):   i = 0   while i < n:      Don't do this!     i = jax.jit(unjitted_loop_body)(i)   return x + i g_inner_jitted_poorly(10, 3) ``` outputs ``` i am being jitted ``` However, if I switch this to a lambda function, it is indeed being compiled every iteration. Which seems like a more accurate example(if I'm not mistaken). ```python def unjitted_loop_body(prev_i):   print(""i am being jitted"")   return prev_i + 1 def g_inner_jitted_poorly(x, n):   i = 0   while i < n:      Don't do this!     i = jax.jit(lambda x: unjitted_loop_body(x))(i)   return x + i g_inner_jitted_poorly(10, 3) ``` outputs  ``` i am being jitted i am being jitted i am being jitted ``` Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,unjitted_loop_body is not constantly re-compiled in the JIT tutorial ,"Hello there, I could be misunderstanding something here(new to Jax still) but on the JIT tutorial it seems like the ""bad"" example is not actually that bad.  More concretely, the given example is as follows: ```python def g_inner_jitted_poorly(x, n):   i = 0   while i < n:      Don't do this!     i = jax.jit(unjitted_loop_body)(i)   return x + i ``` Contrary to the description of this example on the tutorial, it seems like the  unjitted_loop_body function is actually not being compiled every iteration, but cached somewhere.  ```python def unjitted_loop_body(prev_i):   print(""i am being jitted"")   return prev_i + 1 def g_inner_jitted_poorly(x, n):   i = 0   while i < n:      Don't do this!     i = jax.jit(unjitted_loop_body)(i)   return x + i g_inner_jitted_poorly(10, 3) ``` outputs ``` i am being jitted ``` However, if I switch this to a lambda function, it is indeed being compiled every iteration. Which seems like a more accurate example(if I'm not mistaken). ```python def unjitted_loop_body(prev_i):   print(""i am being jitted"")   return prev_i + 1 def g_inner_jitted_poorly(x, n):   i = 0   while i < n:      Don't do this!     i = jax.jit(lambda x: unjitted_loop_body(x))(i)   return x + i g_inner_jitted_poorly(10, 3) ``` outputs  ``` i am being jitted i am being jitted i am being jitted ``` Thanks!",2022-01-22T05:45:00Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/9288,"Thanks  I think cacheing has improved since this was originally written, but you're right that anonymous functions still cause the issue that this section speaks to. Is this something you'd like to address in a PR? If not I'm happy to take the issue. Thanks!",thanks! I'm happy to do this and made a PR for this just now. CC(fix caching example in jit tutorial) ,"Thanks for finding this, and explaining it so clearly!"
865,"以下是一个github上的jax下的一个issue, 标题是(support generating bit widths other than 32 in `lax.rng_bit_generator`)， 内容是 (Three parts: * The underlying XLA operation (RngBitGenerator) doesn't support generating bit widths 8 and 16, so generate 32 bits and truncate in the translation rule. * Canonicalize the dtype given to `rng_bit_generator` to avoid requests for U64 when x64 mode is off. * Test the effect of this on PRNG implementations backed by `rng_bit_generator`. Namely, their `random_bits` method should now support all bit widths, and their keys can be used in samplers such as `random.uniform` and `random.randint` to generate 16bit floats, and {8,16}bit integers respectively. Fixes CC(RBGbased PRNG doesn't work with bf16))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,support generating bit widths other than 32 in `lax.rng_bit_generator`,"Three parts: * The underlying XLA operation (RngBitGenerator) doesn't support generating bit widths 8 and 16, so generate 32 bits and truncate in the translation rule. * Canonicalize the dtype given to `rng_bit_generator` to avoid requests for U64 when x64 mode is off. * Test the effect of this on PRNG implementations backed by `rng_bit_generator`. Namely, their `random_bits` method should now support all bit widths, and their keys can be used in samplers such as `random.uniform` and `random.randint` to generate 16bit floats, and {8,16}bit integers respectively. Fixes CC(RBGbased PRNG doesn't work with bf16)",2022-01-21T06:58:51Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9276
1391,"以下是一个github上的jax下的一个issue, 标题是(Jax profiler won't work with Cuda 11.5)， 内容是 (I'm trying to use the Jax profiler however TensorFlow throws the following error ``` 20220121 00:07:31.649491: W external/org_tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.4'; dlerror: libcupti.so.11.4: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda11.5/targets/x86_64linux/lib/ ``` When I use `locate libcupti.so`, the following paths are provided, this is why I'm using a different directory than the one provided on the jax readme ``` /usr/local/cuda11.5/targets/x86_64linux/lib/libcupti.so /usr/local/cuda11.5/targets/x86_64linux/lib/libcupti.so.11.5 /usr/local/cuda11.5/targets/x86_64linux/lib/libcupti.so.2021.3.1 ``` While I can't find any documentation anywhere, I'm guessing that TensorFlow doesn't support Cuda 11.5 yet as it is trying to find the 11.4 version.  I just want to confirm this is not a jax issue before I post an issue on the TensorFlow Github If my guess is correct, is it possible to rename the libcupti.so.11.5 to 11.4 as a total bodge job?? Or must I change over to Cuda 11.4 to get the profiler to work Thanks for any support)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax profiler won't work with Cuda 11.5,"I'm trying to use the Jax profiler however TensorFlow throws the following error ``` 20220121 00:07:31.649491: W external/org_tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.4'; dlerror: libcupti.so.11.4: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda11.5/targets/x86_64linux/lib/ ``` When I use `locate libcupti.so`, the following paths are provided, this is why I'm using a different directory than the one provided on the jax readme ``` /usr/local/cuda11.5/targets/x86_64linux/lib/libcupti.so /usr/local/cuda11.5/targets/x86_64linux/lib/libcupti.so.11.5 /usr/local/cuda11.5/targets/x86_64linux/lib/libcupti.so.2021.3.1 ``` While I can't find any documentation anywhere, I'm guessing that TensorFlow doesn't support Cuda 11.5 yet as it is trying to find the 11.4 version.  I just want to confirm this is not a jax issue before I post an issue on the TensorFlow Github If my guess is correct, is it possible to rename the libcupti.so.11.5 to 11.4 as a total bodge job?? Or must I change over to Cuda 11.4 to get the profiler to work Thanks for any support",2022-01-21T00:16:20Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/9272,"I _think_ the issue is that the cuda11 version of jaxlib is built against cuda 11.4, and then the profiling code bakes in the cuda version when dlopening libcupti (here). I believe cuda is backwards and forwards compatible after 11.1, so this should theoretically work if the code weren't explicitly looking for an 11.4 library. This also seems work fixing, as it presumably affects anyone trying to profile on cuda version != 11.4. I'm not sure what the fix is; possibly making GetDsoHandle able to try to load any version. Building jaxlib from source is a possible workaround for now.   any thoughts?","I built Jaxlib from source and the profiler no longer throws the error  So I can confirm, this is an issue is with the pip install that uses 11.4  Is there anything I can do to help fix the error? ",Just to confirm  the profiling wasn't working after getting the error right? Sometimes it prints warnings but still manages to take a profile.,"To reiterate  's point, looking at the code, note that's a warning and not an error. Looking at the code: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/stream_executor/platform/default/dso_loader.cc;l=113 it first tries to load the specific version it was built against. If that doesn't work, it falls back to loading any version it can get its hands on. So my belief is profiling should have worked: it just would have emitted a (somewhat noisy) warning. Can you confirm that is the case?","Sorry, I didn't see this You are correct, it does run the profiler even with the old version so there isn't actually an issue I think my issue was that I was using ""/tmp/"" as my path rather than ""tmp/"" which is why I couldn't find the results"
849,"以下是一个github上的jax下的一个issue, 标题是(jax2tf no_xla implementation for scatter_*, advice requested)， 内容是 (Jax2tf currently does not support scatter_* in non XLA terms (see docs) I am planning on implementing this as I need it, but first I wanted to check  is it possible, or is there a very solid reason for why it simply isn't expressible in the tensorflow ops? Is this a foolish plan? I want to do it in nonXLA terms so I can use tf2onnx so it can be used as an ONNX model I was thinking about using a combination of:  `tf.math.unsorted_segment_{prod,sum,...}` for getting the updates to the array   `tf.scatter_nd` for applying those updates Note (to self): `x.at[indexs].get()` does has a nonXLA implementation.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"jax2tf no_xla implementation for scatter_*, advice requested","Jax2tf currently does not support scatter_* in non XLA terms (see docs) I am planning on implementing this as I need it, but first I wanted to check  is it possible, or is there a very solid reason for why it simply isn't expressible in the tensorflow ops? Is this a foolish plan? I want to do it in nonXLA terms so I can use tf2onnx so it can be used as an ONNX model I was thinking about using a combination of:  `tf.math.unsorted_segment_{prod,sum,...}` for getting the updates to the array   `tf.scatter_nd` for applying those updates Note (to self): `x.at[indexs].get()` does has a nonXLA implementation.",2022-01-20T22:14:48Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/9269,"The add looks very simple to port across for a simple case. ```python import tensorflow as tf import jax.numpy as jnp indexs = jnp.arange(0,12,3) x = jnp.arange(12)+2.2 update = 300. def scatter_add_tf(x, indexs, update):     updates = tf.broadcast_to(update, indexs.shape)     indexs = tf.expand_dims(indexs, 1)     return tf.tensor_scatter_nd_add(x, indexs, updates) scatter_add_tf(x, indexs, update), x.at[indexs].add(update) (, DeviceArray([297.8,    3.2,    4.2, 294.8,    6.2,    7.2, 291.8,                  9.2,   10.2, 288.8,   12.2,   13.2], dtype=float32)) ``` Similarly ```python .at[].set()  tf.tensor_scatter_nd_update ``` This works for a trivial case, but the logic for dimensions and shapes of indexs and updates is wrong, I will refactor with help from tensorflow docs.","Hi, we are implementing the no_xla support as needed. You are right that scatter is not yet supported, and thank you for considering adding it. The obvious main requirement is semantic correctness (and this should be verified with tests; see below).  Another (soft) requirement is that the translation should use as few TF ops as possible, because otherwise it becomes hard to expect that the performance of the translated code to be good (one can always encode any semantics with a soup of lowlevel TF ops, but then we place an unreasonable burden on the downstream runtime to ensure good performance).   We would welcome additions, but please keep them simple, and welldocumented, in case we need to debug them later. Also, please ensure that there are tests. Take a look at `tests/primitive_harness.py`, specifically at `_make_scatter_harness`. This will define a set of combinations of argument shapes and parameters that will be tested by `tests/primitives_test.py:test_prim`. By default all primitives harnesses are tested only with XLA enabled. To create harnesses specifically with `enable_xla=False` you will have to add parameter `enable_xla` to the harness. See how this is done for `gather`. There we duplicate almost all harnesses with `enable_xla=True` and `False`. If you are only going to add support for a few cases of `scatter` maybe it makes sense to add primitive harnesses for those cases.  One complication is that currently the primitives_test.py is not run automatically in github; it is run only internally when we import the code in the Google repo. I recommend that you run the tests manually: ``` pytest r a verbosity 1 s jax/experimental/jax2tf/tests/primitives_test.py k scatter ```", PTAL,"Response to the useful advice given in the linked pull request (now closed) I have found other routes for the task than through no_xla, closing task. Thank you for your suggestions and comments. For if of importance to later people:  Scatter_{add,mul,min,max} implemented for simple 1d indexs into a 1d array (other cases not checked) Again, thank you."
1031,"以下是一个github上的jax下的一个issue, 标题是(re-enable tests of #8955)， 内容是 (f35014d had to revert part of CC(add staging logic for polymorphic shapes in jaxprs) because of a surprising downstream breakage (relying on internal APIs). That breakage was isolated to how _inline_literals handled invars. The approach was a temporary one anyway: it relied on the fact that we expect only to bind axis size variables at the top level and hence if we didn't rename the input binders in _inline_literals we wouldn't need to substitute new variables for any variables appearing in types. But a more general approach would be to perform the necessary substitution everywhere; after all, we might be inlining a literal into an axis size! This commit takes the more general approach. It may fix the downstream breakage automatically, just by virtue of being different; if not, I'll figure out how to fix downstream.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,re-enable tests of #8955,"f35014d had to revert part of CC(add staging logic for polymorphic shapes in jaxprs) because of a surprising downstream breakage (relying on internal APIs). That breakage was isolated to how _inline_literals handled invars. The approach was a temporary one anyway: it relied on the fact that we expect only to bind axis size variables at the top level and hence if we didn't rename the input binders in _inline_literals we wouldn't need to substitute new variables for any variables appearing in types. But a more general approach would be to perform the necessary substitution everywhere; after all, we might be inlining a literal into an axis size! This commit takes the more general approach. It may fix the downstream breakage automatically, just by virtue of being different; if not, I'll figure out how to fix downstream.",2022-01-20T19:11:31Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9267
295,"以下是一个github上的jax下的一个issue, 标题是(Make array_copy a primitive)， 内容是 (relands CC(tweak implementation of lax._array_copy (underlying jnp.array(x, copy=True))))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Make array_copy a primitive,"relands CC(tweak implementation of lax._array_copy (underlying jnp.array(x, copy=True)))",2022-01-20T17:59:02Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/9264,Merged in https://github.com/google/jax/commit/31b53084987053e64df31ee2004fde681942d2ec
3537,"以下是一个github上的jax下的一个issue, 标题是(RNGs: key types and custom implementations)， 内容是 (This issue tracks the introduction of typed random key arrays with pluggable PRNG implementations. *Update: see JEP 9263 for an overview. Below is an earlier description.* Work on this began with CC(support custom PRNG implementations), which mostly works out the ""pluggable implementations"" part, makes initial progress on the ""typed"" part, and sets things up throughout the codebase for backwardscompatibility and upgrading. The main configuration flag guarding the system upgrade is `config.jax_enable_custom_prng`. The motivation for pluggability is straightforward: we'd like to set up various implementations for random bit generation, and still have all of `jax.random` work on top of any one of them. While we're at it, we could make this extensible. The motivation for having keys reflected in types (somehow, at some level) is broadly to improve safety, and with it, to offer both users and the system some structural guarantees. Desiderata include: * Moving to a userfacing representation of key arrays that reflects that they are indeed key arrays. Currently key arrays are plain `uint32` arrays to users, indistinguishable from any other data. * Restricting operations on key arrays. The current plain array representation allows for keyinvalidating operations (e.g. manual updating, addition, ...). We'd like to disallow these, or at least render mistakes unlikely. * More/better opportunities to check key misuse, reuse, and so on. Unsurprisingly, key types also help for mapping keys to RNG implementations (for the pluggability part above). There are roughly two ways we've thought to approach endowing jax with key types, specifically, keyelementtype arrays: One is as a frontend component handled during staging, projected away to plain `u32` in staged IR, in analogy with how pytrees behave. To introduce a pytree type in Python that wraps an underlying keydata (say, `uint32`) array is not quite correct, since it would misbehave under `vmap`, `scan`, and even `jax.tree_map`. Instead, we might want to rely on something like the typeclass mechanisms still in development (e.g. `vmappable`, CC(add internal `vmappable` interface (part 1))) for this approach. The other tack is to introduce key types into our IR and other internal machinery (with a corresponding lowering), and to map to that from a Python arrayofkeyslike type during staging. This might confer some extra advantages downstream as well, such as more opportunities for checking throughout our front and middleend. This is roughly the approach we're taking (as of around summer '22). Ahead of the complete upgrade, some byproducts are already available. We've implemented a couple of PRNGs that serve as alternatives to the default `threefry2x32` implementation (in CC(add experimental RngBitGenerator (""RBG"") PRNG), CC(improvements to RBG PRNG)) and build on compiler bit generator primitives. The processwide default RNG implementation can be controlled via `jax.default_prng_impl` and `config.jax_default_prng_impl` (from CC(config setting to control the default PRNG implementation)) and can be accessed via `jax.random.default_prng_impl` (from CC(add `random.default_prng_impl` to retrieve the default PRNG implementation)). These can be used to swap between the predefined RNG alternatives for the entire process at a time.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,RNGs: key types and custom implementations,"This issue tracks the introduction of typed random key arrays with pluggable PRNG implementations. *Update: see JEP 9263 for an overview. Below is an earlier description.* Work on this began with CC(support custom PRNG implementations), which mostly works out the ""pluggable implementations"" part, makes initial progress on the ""typed"" part, and sets things up throughout the codebase for backwardscompatibility and upgrading. The main configuration flag guarding the system upgrade is `config.jax_enable_custom_prng`. The motivation for pluggability is straightforward: we'd like to set up various implementations for random bit generation, and still have all of `jax.random` work on top of any one of them. While we're at it, we could make this extensible. The motivation for having keys reflected in types (somehow, at some level) is broadly to improve safety, and with it, to offer both users and the system some structural guarantees. Desiderata include: * Moving to a userfacing representation of key arrays that reflects that they are indeed key arrays. Currently key arrays are plain `uint32` arrays to users, indistinguishable from any other data. * Restricting operations on key arrays. The current plain array representation allows for keyinvalidating operations (e.g. manual updating, addition, ...). We'd like to disallow these, or at least render mistakes unlikely. * More/better opportunities to check key misuse, reuse, and so on. Unsurprisingly, key types also help for mapping keys to RNG implementations (for the pluggability part above). There are roughly two ways we've thought to approach endowing jax with key types, specifically, keyelementtype arrays: One is as a frontend component handled during staging, projected away to plain `u32` in staged IR, in analogy with how pytrees behave. To introduce a pytree type in Python that wraps an underlying keydata (say, `uint32`) array is not quite correct, since it would misbehave under `vmap`, `scan`, and even `jax.tree_map`. Instead, we might want to rely on something like the typeclass mechanisms still in development (e.g. `vmappable`, CC(add internal `vmappable` interface (part 1))) for this approach. The other tack is to introduce key types into our IR and other internal machinery (with a corresponding lowering), and to map to that from a Python arrayofkeyslike type during staging. This might confer some extra advantages downstream as well, such as more opportunities for checking throughout our front and middleend. This is roughly the approach we're taking (as of around summer '22). Ahead of the complete upgrade, some byproducts are already available. We've implemented a couple of PRNGs that serve as alternatives to the default `threefry2x32` implementation (in CC(add experimental RngBitGenerator (""RBG"") PRNG), CC(improvements to RBG PRNG)) and build on compiler bit generator primitives. The processwide default RNG implementation can be controlled via `jax.default_prng_impl` and `config.jax_default_prng_impl` (from CC(config setting to control the default PRNG implementation)) and can be accessed via `jax.random.default_prng_impl` (from CC(add `random.default_prng_impl` to retrieve the default PRNG implementation)). These can be used to swap between the predefined RNG alternatives for the entire process at a time.",2022-01-20T17:42:14Z,enhancement P1 (soon),closed,3,13,https://github.com/jax-ml/jax/issues/9263,"We plan on starting this migration once `PRNGKeyArray` is no longer a PyTree, ie. when we can register it as a JAXtype through the upcoming typeclass mechanism (like `vmappable` https://github.com/google/jax/pull/8451)","Indeed, the move away from a pytree is what's blocking us. Aside from that, we also have some operations to consider implementing, as categorized nicely in ' description for CC(Support reshape/concatenate/broadcast on PRNGKeyArrays.). But these don't seem to present an upfront barrier to the same degree. We believe that we can implement what's needed as we upgrade.","Just curious, and didn't know where to ask.  Is there a reason that `k.split()` does something completely different than `split(k)` if `k` is a newstyle `KeyArray`?  Could this be a trap for users?   It might be convenient to have `split` as a member function?","Also, is there an asymmetry between `vmap` and `split`?  Currently, if you split a key array `k`: ```python h = split(split(k, 3), 5) ``` then `h.shape` is `(3, 5)`. But if you vmap a function that accepts a key ```python def f(k: KeyArray) > Any: ... g = vmap(vmap(f)) ``` Then `g(h)` will make the outer `vmap` correspond to the inner `split`?  (If I have that right?)  It seems like split should prepend rather than append the dimension for consistency?","> Just curious, and didn't know where to ask. Is there a reason that `k.split()` does something completely different than `split(k)` if `k` is a newstyle `KeyArray`? Could this be a trap for users? My plan was for numpy `split` to be inapplicable altogether, whether as a method or via `jnp.split(keys)`, at least for the time being. You'll notice that it's only barely available now under the upgrade flag: you can incidentally do `jnp.split(k)` or `k.split()` currently when staging under `jit` or similar, but the implementation will err inscrutably. This is a known temporary discrepancy, but the eager behavior is the intended one. Do you expect a need for numpylike splitting support? > It might be convenient to have `split` as a member function? I think there isn't a solid case for this. No current code expects it, and to your point it would be ambiguous.","> It seems like split should prepend rather than append the dimension for consistency? `split` prepends in correspondence with the very common unpacking usage, e.g.: ```python key1, key2, key3 = jax.random.split(key, 3) ```","> My plan was for numpy split to be inapplicable altogether,  Yes, great plan! I was just confused by the different result.  Now I see that it's a numpy function being called on the underlying array. > Do you expect a need for numpylike splitting support? I do not!  I like the idea of making the RNG more opaque so that it can't be used as a numpy array.  In particular, I love how the shape of a key array now returns the shape of the keys rather than the shape of the underlying array. > No current code expects it, and to your point it would be ambiguous I guess it's ambiguous if you are still imagining the object as a numpylike array.  But isn't your point above that you want to get away from that? It's not a big deal, but just to explain my reasoning in case you're curious: The usual design pattern decision for whether something should be a member function or a free function is whether the function can be implemented through the public interface.  `split` cannot be implemented through a key array's public interface, so at least according that criterion, it should be a member function of the object. I agree that no code expects it, but that's because historically key arrays were numpy arrays, and so `split` had to be a free function.  Going forward, it would be more convenient not to have to import split, and more logical to make it a member function. This argument doesn't apply to functions like `normal`, `gamma`, etc.  These are implemented through the public interface (`random_bits`), so they are rightly free functions. Anyway, no big deal :smile: ","I see. Thanks for spelling that out. Although key arrays are not entirely numpylike, they are still partly so. For example, they support transposition, reshaping, and a few others, and they offer these via methods as well as `jax.numpy` calls. So long as any amount of numpy is supported, I'm wary of possible ambiguity. I otherwise appreciate your case in favor of it. We can start with less for now and see whether we're drawn to add it.","> So long as any amount of numpy is supported, I'm wary of possible ambiguity. Yes, very good point. > I otherwise appreciate your case in favor of it. We can start with less for now and see whether we're drawn to add it. Sounds great.  And I love the new RNG interface if it wasn't obvious 😄 ",6abefa197776994926f5d5330fe994f94f0090dc makes dispatch fast again for functions over newstyle RNG keys!,"> I guess it's ambiguous if you are still imagining the object as a numpylike array. [...] Returning to this old thread with  – we've switched to thinking about this in a way that is hopefully more straightforward: key arrays are now like any other jax array from the user's point of view. They just have a different dtype. The dtype determines what operations are allowed on the array. Elementtypepolymorphic operations like transposition and slicing are fine. Addition is not, since the element type doesn't support it. Also, there is no longer a uservisible type distinction for the enclosing array. To check for whether an array is a key array, we recommend `issubdtype`. This is all covered by the JEP drafted in CC(JEP 9263: Typed keys & pluggable RNGs). I just thought to highlight it!", Thanks for linking me. Sounds like an excellent design!,"We ported our docs over in CC(upgrade random docs to typed RNG keys), haven't surfaced many bugs (fixing those that did), and already cut a few releases in the meantime. I think we can call this complete by now! We have some work to do on tidying up the dtype rules in the implementation, and making more reusable internal ""extended dtype"" array machinery, but that all seems well understood enough from here."
1172,"以下是一个github上的jax下的一个issue, 标题是(compilation cache: get/put with `Lowered` and `Compiled` instances)， 内容是 (The AOT API (cf. CC(aheadoftime lowering and compilation) and references therein) introduces public types representing lowered and compiled programs. The experimental compilation cache essentially maps (some types of) lowered programs to their compiled form. Introduce a means of cache query/insertion given public views of lowered and compiled programs. Err if the programs are not cachecompatible. Example usage would be something along the lines of: ```python from jax.experimental.compilation_cache import compilation_cache as (x, y): return x + y lowered = jax.jit(f).lower(1., 2.) compiled = cc.put(lowered) assert compiled(3., 4.) == 7. ``` ```python  ... later, in a new process ... compiled_then = cc.get(lowered) compiled_now = lowered.compile() assert compiled_then(3., 4.) == compiled_now(3., 4.) == 7. ``` Ideally the lowered program is sufficient for cache lookup. We suspect that it ought to be so in principle.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,compilation cache: get/put with `Lowered` and `Compiled` instances,"The AOT API (cf. CC(aheadoftime lowering and compilation) and references therein) introduces public types representing lowered and compiled programs. The experimental compilation cache essentially maps (some types of) lowered programs to their compiled form. Introduce a means of cache query/insertion given public views of lowered and compiled programs. Err if the programs are not cachecompatible. Example usage would be something along the lines of: ```python from jax.experimental.compilation_cache import compilation_cache as (x, y): return x + y lowered = jax.jit(f).lower(1., 2.) compiled = cc.put(lowered) assert compiled(3., 4.) == 7. ``` ```python  ... later, in a new process ... compiled_then = cc.get(lowered) compiled_now = lowered.compile() assert compiled_then(3., 4.) == compiled_now(3., 4.) == 7. ``` Ideally the lowered program is sufficient for cache lookup. We suspect that it ought to be so in principle.",2022-01-19T22:43:45Z,enhancement P2 (eventual),open,0,0,https://github.com/jax-ml/jax/issues/9250
1808,"以下是一个github上的jax下的一个issue, 标题是(Eliminate redundancies in jax.scipy.linalg.expm)， 内容是 ( Summary This PR introduces a minor change to the `expm` function:  Currently, given the matrix `A`, `expm` computes all Pade approximant numerators/denominators, then selects which one to use based on the norm of the matrix.  This PR modifies this to only compute the relevant Pade approximant numerator/denominator by using `switch` conditioned on the norm of the matrix `A`. Closes CC(Redundant computation in expm?)  Speed testing I unfortunately don't currently have access to a GPU, but have run some CPU speed tests comparing the code in this PR to the current release. Testing in 64 bit mode with complex matrices of dimensions `[5, 10, 50, 100, 500, 1000, 5000]`, the ratios by dimension of ""release run time"" / ""PR run time"" for `expm` (once compiled) are: ``` [1.0794702 , 3.63815789, 3.41229117, 3.38029568, 1.68847097, 1.54355993, 1.61012301] ``` These are not the most rigorous (I'm just generating random matrices and measuring time by sandwiching `time()` calls around a single exponentiation for each dimension), but seem pretty reasonable based on the number of matrix multiplications this is potentially saving (which varies based on matrix norm). I've attached the jupyter notebook I've run the tests in jax_expm_speed_test.ipynb.zip, which include a cell of the saved list of execution times for JAX version (release and this PR).   Going forward I'm trying to finally get GPU access in my org and will reply with GPU speed comparisons here when possible. In the meantime, please let me know if there's anything you'd like to see in terms of speed comparisons.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Eliminate redundancies in jax.scipy.linalg.expm," Summary This PR introduces a minor change to the `expm` function:  Currently, given the matrix `A`, `expm` computes all Pade approximant numerators/denominators, then selects which one to use based on the norm of the matrix.  This PR modifies this to only compute the relevant Pade approximant numerator/denominator by using `switch` conditioned on the norm of the matrix `A`. Closes CC(Redundant computation in expm?)  Speed testing I unfortunately don't currently have access to a GPU, but have run some CPU speed tests comparing the code in this PR to the current release. Testing in 64 bit mode with complex matrices of dimensions `[5, 10, 50, 100, 500, 1000, 5000]`, the ratios by dimension of ""release run time"" / ""PR run time"" for `expm` (once compiled) are: ``` [1.0794702 , 3.63815789, 3.41229117, 3.38029568, 1.68847097, 1.54355993, 1.61012301] ``` These are not the most rigorous (I'm just generating random matrices and measuring time by sandwiching `time()` calls around a single exponentiation for each dimension), but seem pretty reasonable based on the number of matrix multiplications this is potentially saving (which varies based on matrix norm). I've attached the jupyter notebook I've run the tests in jax_expm_speed_test.ipynb.zip, which include a cell of the saved list of execution times for JAX version (release and this PR).   Going forward I'm trying to finally get GPU access in my org and will reply with GPU speed comparisons here when possible. In the meantime, please let me know if there's anything you'd like to see in terms of speed comparisons.",2022-01-19T15:20:10Z,pull ready,closed,0,5,https://github.com/jax-ml/jax/issues/9239,"I've just signed the CLA, so please rescan!",You could use a public Colab runtime to run benchmarks on GPU if you wish,"> You could use a public Colab runtime to run benchmarks on GPU if you wish Thanks for this suggestion. I've just run some comparisons in colab on GPU using the latest release version and the version in this PR. Using the same code to generate the random arrays and measuring the speed after an initial run of `expm` that includes compilation, for dimensions as before `[5, 10, 50, 100, 500, 1000, 5000]`, we get:  Execution times for latest release version:  ``` [0.0013790130615234375, 0.0016705989837646484, 0.004743814468383789, 0.00931692123413086, 0.137678861618042,  0.7950971126556396, 100.97727608680725] ```  Execution times for PR version: ``` [0.0011055469512939453, 0.0013234615325927734, 0.0035729408264160156, 0.00675511360168457, 0.08099985122680664, 0.5249743461608887, 67.66769027709961] ```  Ratio of ""release version"" / ""PR version"" ``` [1.24735821, 1.26229508, 1.32770586, 1.37923976, 1.69974215, 1.5145447 , 1.49225244] ``` Again this is all done with 64bit. I expect that this change will have less of an impact on 32bit execution as there were fewer redundant calculations being done.","Keep in mind that `searchsorted` is particularly inefficient on GPU, so your switch to `digitize` may have an outsized impact there.","Just switched back to using `argwhere` and reran the tests, the times on GPU after initial compilation are: ``` [0.0011105537414550781,  0.0013575553894042969,  0.003557443618774414,  0.00667119026184082,  0.08334660530090332,  0.5242531299591064,  68.71966004371643] ``` which are essentially the same as before."
1382,"以下是一个github上的jax下的一个issue, 标题是((probably no) Performance issues with v0.2.26 and above?)， 内容是 (I have encountered some performance issues with JAX version 0.2.26 and 0.2.27. This has surfaced while running the TestU01 smallcrush RNG testsuite against the `jax.random` RNG as well as my own implementation in https://github.com/DPBayes/jaxchachaprng which run significantly slower than in previous versions. Running JAX version 0.2.26 with JAXlib version 0.1.74 took me 30:47.50 minutes (close to that with JAX version 0.2.27). With JAX version 0.2.25, same JAXlib version, took only 0:38.59 minutes. I'm not sure what causes this and it's hard to track down. It could of course be an issue in my wrapping code around the Python rngs as I haven't yet found a simpler working example that demonstrates this behaviour. I'm trying to narrow it down further, but wanted to ask whether anyone else has encountered something similar or if there were any changes in JAX version 0.2.26 that I'm not aware of and that could explain this. Please:  [x] Check for duplicate issues.  [ ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: As said above, I have not boiled this down to a simple complete example.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,(probably no) Performance issues with v0.2.26 and above?,"I have encountered some performance issues with JAX version 0.2.26 and 0.2.27. This has surfaced while running the TestU01 smallcrush RNG testsuite against the `jax.random` RNG as well as my own implementation in https://github.com/DPBayes/jaxchachaprng which run significantly slower than in previous versions. Running JAX version 0.2.26 with JAXlib version 0.1.74 took me 30:47.50 minutes (close to that with JAX version 0.2.27). With JAX version 0.2.25, same JAXlib version, took only 0:38.59 minutes. I'm not sure what causes this and it's hard to track down. It could of course be an issue in my wrapping code around the Python rngs as I haven't yet found a simpler working example that demonstrates this behaviour. I'm trying to narrow it down further, but wanted to ask whether anyone else has encountered something similar or if there were any changes in JAX version 0.2.26 that I'm not aware of and that could explain this. Please:  [x] Check for duplicate issues.  [ ] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this: As said above, I have not boiled this down to a simple complete example.",2022-01-19T12:21:01Z,bug needs info,closed,0,5,https://github.com/jax-ml/jax/issues/9238,"I think we're going to need some sort of reproduction or trace or profile to track this down. It has been over a month since the previous release, so many things have changed. Without something we can run it's hard to know where to even start debugging this!","That's quite a big regression! Should be easy to spot once we have a repro. With a repro, we could bisect against git commits.","Alright, we have now tracked this down somewhat. I was using Pythons `PySequence_Fast*` family of API functions to access the values in a `DeviceArray` from the C/C++ side. This is very fast if Python recognises the input as a list/tupleequivalent  this was the case for `DeviceArray` before 0.2.26 but seems to be no longer the case now, which causes the call to `PySequence_Fast` to construct a new list filled by iterating over the input sequence (i.e., the `DeviceArray`). This in turn seems to now create a list of `DeviceArray` objects containing single values. The same behaviour occurs by constructing a list out of the array directly in Python: ```python from jax import numpy as jnp x = jnp.zeros(2) print(list(x))  on 0.2.25 prints: [0.0, 0.0]  on 0.2.26 prints: [DeviceArray(0., dtype=float32), DeviceArray(0., dtype=float32)] ``` The new behaviour seems counterintuitive to me. However, I can work around it by converting the DeviceArray into a plain numpy array before passing it back to the C/C++ side. None of this seems to have any performance effect in any of our pure Python environments, so this probably does not affect most users. I still wonder, is the above is expected behaviour now or an unintended artifact of recent changes? P.S.: Files for a MWE with C++: performance_bug_9238.cpp.zip Unzip, then do the following in a terminal with a Python environment and jax installed ``` mkdir build && cd build && cmake .. && make & cd .. build/performance_bug_9238 ./performance_bug_9238.py  on 0.2.25 prints: took: 103018  on 0.2.26 prints: took: 5736011 ```","Yes, this was an intentional change: https://github.com/google/jax/pull/8043 See the discussion on that PR. WDYT?","Well, I guess that makes sense overall. I'll consider this now resolved and will close, thanks for the feedback."
1039,"以下是一个github上的jax下的一个issue, 标题是(Add more documentation for how argument donation works)， 内容是 (A user was asking what is the meaning of the warning ``` UserWarning: Some donated buffers were not usable: ShapedArray(float32[185,16,16,3]), ShapedArray(int32[185]) ``` It seems that the documentation does not say much about argument donation, except what is written in the docstring for `jit` and `pmap`: ```     donate_argnums: Specify which arguments are ""donated"" to the computation.       It is safe to donate arguments if you no longer need them once the       computation has finished. In some cases XLA can make use of donated       buffers to reduce the amount of memory needed to perform a computation,       for example recycling one of your input buffers to store a result. You       should not reuse buffers that you donate to a computation, JAX will raise       an error if you try to. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add more documentation for how argument donation works,"A user was asking what is the meaning of the warning ``` UserWarning: Some donated buffers were not usable: ShapedArray(float32[185,16,16,3]), ShapedArray(int32[185]) ``` It seems that the documentation does not say much about argument donation, except what is written in the docstring for `jit` and `pmap`: ```     donate_argnums: Specify which arguments are ""donated"" to the computation.       It is safe to donate arguments if you no longer need them once the       computation has finished. In some cases XLA can make use of donated       buffers to reduce the amount of memory needed to perform a computation,       for example recycling one of your input buffers to store a result. You       should not reuse buffers that you donate to a computation, JAX will raise       an error if you try to. ```",2022-01-19T09:50:20Z,enhancement documentation better_errors,closed,0,1,https://github.com/jax-ml/jax/issues/9237,"Furthermore, if the donated buffer is reused the error is: ``` ValueError: INVALID_ARGUMENT: Invalid buffer passed to Execute() as argument 0 to replica 0: INVALID_ARGUMENT: Donation requested for invalid buffer ```"
458,"以下是一个github上的jax下的一个issue, 标题是(checkify: fix and test post_process_call/map)， 内容是 (This PR also wrapped some lines to 80chars, though those weren't necessary for correctness. The only action is in `CheckifyTrace.post_process_call` and `post_process_map` and the corresponding tests. I think we just never tested them!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,checkify: fix and test post_process_call/map,"This PR also wrapped some lines to 80chars, though those weren't necessary for correctness. The only action is in `CheckifyTrace.post_process_call` and `post_process_map` and the corresponding tests. I think we just never tested them!",2022-01-19T06:23:51Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9236
3088,"以下是一个github上的jax下的一个issue, 标题是(Cache the expensive computations in GDA. For example `get_shard_indices_replica_ids` can be the same for multiple variables in a neural network (global_shape, mesh_axes and global_mesh) can be the same)， 内容是 (Cache the expensive computations in GDA. For example `get_shard_indices_replica_ids` can be the same for multiple variables in a neural network (global_shape, mesh_axes and global_mesh) can be the same Note that the first time will be a little slow. The below timings you are seeing shows the caching working because the benchmark is running for multiple iterations and then the time is averaged over the number of iterations. ``` name                                                     time/op gda_construction_callback_(4, 2)_['x', 'y']              4.50ms ±10% gda_construction_raw_(256, 8)_['x', 'y']                 5.82ms ± 2% indices_replica_id_calc__uncached_(256, 8)_['x', 'y']    2.95ms ± 6% indices_replica_id_calc_cached_(256, 8)_['x', 'y']       28.7µs ± 1% gda_construction_callback_(4, 2)_[None]                  31.9ms ±20% gda_construction_raw_(256, 8)_[None]                     5.85ms ± 5% indices_replica_id_calc__uncached_(256, 8)_[None]        1.75ms ± 1% indices_replica_id_calc_cached_(256, 8)_[None]           29.0µs ± 4% gda_construction_callback_(4, 2)_['x']                   8.40ms ± 4% gda_construction_raw_(256, 8)_['x']                      5.48ms ± 2% indices_replica_id_calc__uncached_(256, 8)_['x']         1.89ms ± 1% indices_replica_id_calc_cached_(256, 8)_['x']            29.0µs ± 4% gda_construction_callback_(4, 2)_['y']                   15.3ms ± 6% gda_construction_raw_(256, 8)_['y']                      5.66ms ± 5% indices_replica_id_calc__uncached_(256, 8)_['y']         1.82ms ± 2% indices_replica_id_calc_cached_(256, 8)_['y']            29.4µs ± 3% gda_construction_callback_(4, 2)_[('x', 'y')]            4.29ms ± 5% gda_construction_raw_(256, 8)_[('x', 'y')]               5.61ms ± 7% indices_replica_id_calc__uncached_(256, 8)_[('x', 'y')]  3.81ms ±10% indices_replica_id_calc_cached_(256, 8)_[('x', 'y')]     29.0µs ± 5% gda_construction_raw_(128, 8)_['x', 'y']                 2.42ms ± 1% indices_replica_id_calc__uncached_(128, 8)_['x', 'y']    1.14ms ±11% indices_replica_id_calc_cached_(128, 8)_['x', 'y']       19.9µs ± 1% gda_construction_raw_(4, 2)_['x', 'y']                   46.7µs ± 0% indices_replica_id_calc__uncached_(4, 2)_['x', 'y']       153µs ± 4% indices_replica_id_calc_cached_(4, 2)_['x', 'y']         11.1µs ± 8% gda_construction_raw_(16, 4)_['x', 'y']                   164µs ± 2% indices_replica_id_calc__uncached_(16, 4)_['x', 'y']      212µs ± 3% indices_replica_id_calc_cached_(16, 4)_['x', 'y']        11.3µs ± 1% gda_construction_raw_(16, 4)_[('x', 'y')]                 163µs ± 2% indices_replica_id_calc__uncached_(16, 4)_[('x', 'y')]    210µs ± 2% indices_replica_id_calc_cached_(16, 4)_[('x', 'y')]      11.6µs ± 8% ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"Cache the expensive computations in GDA. For example `get_shard_indices_replica_ids` can be the same for multiple variables in a neural network (global_shape, mesh_axes and global_mesh) can be the same","Cache the expensive computations in GDA. For example `get_shard_indices_replica_ids` can be the same for multiple variables in a neural network (global_shape, mesh_axes and global_mesh) can be the same Note that the first time will be a little slow. The below timings you are seeing shows the caching working because the benchmark is running for multiple iterations and then the time is averaged over the number of iterations. ``` name                                                     time/op gda_construction_callback_(4, 2)_['x', 'y']              4.50ms ±10% gda_construction_raw_(256, 8)_['x', 'y']                 5.82ms ± 2% indices_replica_id_calc__uncached_(256, 8)_['x', 'y']    2.95ms ± 6% indices_replica_id_calc_cached_(256, 8)_['x', 'y']       28.7µs ± 1% gda_construction_callback_(4, 2)_[None]                  31.9ms ±20% gda_construction_raw_(256, 8)_[None]                     5.85ms ± 5% indices_replica_id_calc__uncached_(256, 8)_[None]        1.75ms ± 1% indices_replica_id_calc_cached_(256, 8)_[None]           29.0µs ± 4% gda_construction_callback_(4, 2)_['x']                   8.40ms ± 4% gda_construction_raw_(256, 8)_['x']                      5.48ms ± 2% indices_replica_id_calc__uncached_(256, 8)_['x']         1.89ms ± 1% indices_replica_id_calc_cached_(256, 8)_['x']            29.0µs ± 4% gda_construction_callback_(4, 2)_['y']                   15.3ms ± 6% gda_construction_raw_(256, 8)_['y']                      5.66ms ± 5% indices_replica_id_calc__uncached_(256, 8)_['y']         1.82ms ± 2% indices_replica_id_calc_cached_(256, 8)_['y']            29.4µs ± 3% gda_construction_callback_(4, 2)_[('x', 'y')]            4.29ms ± 5% gda_construction_raw_(256, 8)_[('x', 'y')]               5.61ms ± 7% indices_replica_id_calc__uncached_(256, 8)_[('x', 'y')]  3.81ms ±10% indices_replica_id_calc_cached_(256, 8)_[('x', 'y')]     29.0µs ± 5% gda_construction_raw_(128, 8)_['x', 'y']                 2.42ms ± 1% indices_replica_id_calc__uncached_(128, 8)_['x', 'y']    1.14ms ±11% indices_replica_id_calc_cached_(128, 8)_['x', 'y']       19.9µs ± 1% gda_construction_raw_(4, 2)_['x', 'y']                   46.7µs ± 0% indices_replica_id_calc__uncached_(4, 2)_['x', 'y']       153µs ± 4% indices_replica_id_calc_cached_(4, 2)_['x', 'y']         11.1µs ± 8% gda_construction_raw_(16, 4)_['x', 'y']                   164µs ± 2% indices_replica_id_calc__uncached_(16, 4)_['x', 'y']      212µs ± 3% indices_replica_id_calc_cached_(16, 4)_['x', 'y']        11.3µs ± 1% gda_construction_raw_(16, 4)_[('x', 'y')]                 163µs ± 2% indices_replica_id_calc__uncached_(16, 4)_[('x', 'y')]    210µs ± 2% indices_replica_id_calc_cached_(16, 4)_[('x', 'y')]      11.6µs ± 8% ```",2022-01-18T08:04:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9223
1289,"以下是一个github上的jax下的一个issue, 标题是(Import error after compiling JAX from source on a TPU instance)， 内容是 ( Discussed in https://github.com/google/jax/discussions/9219  Originally posted by **awav** January 17, 2022 Hello everyone, I'm trying to compile JAX with a custom XLA on the TPU VM, but I'm getting an error showed below. I would appreciate it if someone could help me to figure out what is the correct procedure for compiling JAX on TPUs. I'm building JAX with this command (python with anaconda and python3 without): ``` > cd ~/jax > python build/build.py enable_tpu > pip install forcereinstall dist/jaxlib*.whl ``` The import error after successfull compilation: ``` In [1]: import jax 20220117 20:08:33.794365: F external/org_tensorflow/tensorflow/core/tpu/tpu_library_init_fns.inc:34] TpuEmbeddingEngineState_Create not available in this library. ``` A bit more information on the TPU instance: ``` (py37) artemnblablablahost0:~/code/jax$ lsb_release a No LSB modules are available. Distributor ID: Ubuntu Description:    Ubuntu 20.04.2 LTS Release:        20.04 Codename:       focal ``` JAX SHA: `efa5edfd39632c5864e121de3290326b42b26326`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Import error after compiling JAX from source on a TPU instance," Discussed in https://github.com/google/jax/discussions/9219  Originally posted by **awav** January 17, 2022 Hello everyone, I'm trying to compile JAX with a custom XLA on the TPU VM, but I'm getting an error showed below. I would appreciate it if someone could help me to figure out what is the correct procedure for compiling JAX on TPUs. I'm building JAX with this command (python with anaconda and python3 without): ``` > cd ~/jax > python build/build.py enable_tpu > pip install forcereinstall dist/jaxlib*.whl ``` The import error after successfull compilation: ``` In [1]: import jax 20220117 20:08:33.794365: F external/org_tensorflow/tensorflow/core/tpu/tpu_library_init_fns.inc:34] TpuEmbeddingEngineState_Create not available in this library. ``` A bit more information on the TPU instance: ``` (py37) artemnblablablahost0:~/code/jax$ lsb_release a No LSB modules are available. Distributor ID: Ubuntu Description:    Ubuntu 20.04.2 LTS Release:        20.04 Codename:       focal ``` JAX SHA: `efa5edfd39632c5864e121de3290326b42b26326`",2022-01-17T20:32:48Z,build,closed,0,11,https://github.com/jax-ml/jax/issues/9220,Seems like the custom jaxlib is incompatible with libtpu.so.  Try this: cd jax pip install e .[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html  might know more about JAX CloudTPU build," Thanks! It didn't work, I still can't import jax (the same core dump), after compiling JAX with custom XLA. Installed JAX from the release page works fine.","Hi , try this with your custombuilt jaxlib: ``` pip install upgrade libtpunightly f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` Or that doesn't work, pick the same libtpunightly date that you built your jaxlib: ``` pip install upgrade libtpunightly==0.1.devYYYYMMDD f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` (Replace the YYYYMMDD with the build date) Explanation: the `TpuEmbeddingEngineState_Create not available in this library` message means the jaxlib you compiled requires a newer version of libtpu, the library that provides lowlevel access to Cloud TPUs (sorry the error message is unclear!). By default, we automatically install compatible versions of jaxlib and libtpunightly when you `pip install jax[tpu]`. However, when you build your own jaxlib, you'll have to manualy upgrade libtpunightly as shown above."," Thanks, I will try that. Quick question: I'm using the JAX repo with SHA shown below, therefore should I use 20211107 as the build date? ``` (py37) artemblahost: ~/code/jax$ git log 2 commit efa5edfd39632c5864e121de3290326b42b26326 (HEAD > main, origin/main, origin/HEAD)a Merge: c877f8bc ece53255 Author: jax authors  Date:   Thu Oct 7 06:23:59 2021 0700     Merge pull request CC(Add float0 support to equality and closeness check) from eelregit:checkeqfloat0     PiperOriginRevId: 401494757 ```","Sorry, I misspoke slightly above. The libtpu version you need depends on what version of tensorflow you use to build jaxlib (most of the jaxlib code lives in the tensorflow repo, e.g. all of XLA). By default, build.py picks up the tensorflow commit specified in jax's WORKSPACE (the long hash in the URL is the TF git commit, e.g. it's https://github.com/tensorflow/tensorflow/commit/d8b3d5b22cb61b1a19e7d1c3524f1be92ee45ca6 at the time of writing, which was committed Dec 29). If in doubt, using the latest libtpunightly often works though (we add new libtpu features more often than we change existing ones, and it's fine to use a libtpu with extra features compared to your jaxlib, as long as none of the features jaxlib uses have changed). OOC, why are you trying to build a custom jaxlib?",", Hurray, it worked. Thank you so much!  ``` artemblahost:~/code$ pip3  freeze | grep jax e git+https://github.com/google/jax.gitegg=jax jaxlib @ file:///home/artem/code/jax/dist/jaxlib0.1.72cp38nonemanylinux2010_x86_64.whl ``` > OOC, why are you trying to build a custom jaxlib? I'm working on the extension to XLA  some extra optimisation passes (it helps a lot with research tasks I'm dealing with). I wonder if the extension will work out of the box on a TPU VM, additionally to CPU and GPU.","Very cool! Then you've probably figured out how to update the WORKSPACE to point to your custom TF code. FYI you can also use the following to build from a local TF checkout instead of updating the WORKSPACE if it's easier: ``` python3 build/build.py enable_tpu bazel_options=""override_repository=org_tensorflow=$HOME/tensorflow"" ``` This is what I like to use at least :)","> Very cool! Then you've probably figured out how to update the WORKSPACE to point to your custom TF code. FYI you can also use the following to build from a local TF checkout instead of updating the WORKSPACE if it's easier: >  > ``` > python3 build/build.py enable_tpu bazel_options=""override_repository=org_tensorflow=$HOME/tensorflow"" > ``` >  > This is what I like to use at least :) Thanks for the tip! That will save me some time :)",", , I might have started celebrating too early","Btw, I think this issue should be closed, the build works and I can import JAX w/o the issue. I opened another one CC(Unrecognisable XLA flags in JAX built on a TPU VM with custom XLA), and it is connected to the XLA and TPU interaction.","Hi , thanks for the explanation! I still have a question about that. > The libtpu version you need depends on what version of tensorflow you use to build jaxlib How can I determine the libtpu version I need based on the tensorflow version?"
2074,"以下是一个github上的jax下的一个issue, 标题是(problems installing JAX on a GCP deep learning VM with GPU)， 内容是 (I have created a GCP VM with an A100 GPU and this default image: c0deeplearningcommoncu113v20211219debian10 This is cuda_11.3 , CUDNN 8.2 and Debian 10, python 3.7. I installed JAX thus: ``` pip install upgrade pip pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html   ``` Inside python 3.7 I type 'import jax' but t I get this error: ```  version `GLIBCXX_3.4.26' not found ``` According to this issue, , I can solve this by first creating a venv and then installing: ``` python m venv env source env/bin/activate pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html   ``` This partly works, in that I can now 'import jax' and run it. However, it fails when I use 'jax.scan': In particlar, the code snippet below gives this error: ``` 20220117 19:46:23.259785: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2086] Execution of replica 0 failed: INTERNAL: CustomCall failed: jaxlib/cuda_prng_kernels.cc:30: operation cudaGetLastError() failed: the provided PTX was compiled with an unsupported toolchain. ``` Here is the code: ``` import jax import jax.numpy as jnp  sample from a Markov chain init_dist = jnp.array([0.8, 0.2]) trans_mat = jnp.array([[0.9, 0.1], [0.5, 0.5]]) rng_key = jax.random.PRNGKey(0) from jax.scipy.special import logit seq_len = 15 initial_state = jax.random.categorical(rng_key, logits=logit(init_dist), shape=(1,)) def draw_state(prev_state, key):     logits = logit(trans_mat[:, prev_state])     state = jax.random.categorical(key, logits=logits.flatten(), shape=(1,))     return state, state rng_key, rng_state, rng_obs = jax.random.split(rng_key, 3) keys = jax.random.split(rng_state, seq_len  1) final_state, states = jax.lax.scan(draw_state, initial_state, keys) print(states) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,problems installing JAX on a GCP deep learning VM with GPU,"I have created a GCP VM with an A100 GPU and this default image: c0deeplearningcommoncu113v20211219debian10 This is cuda_11.3 , CUDNN 8.2 and Debian 10, python 3.7. I installed JAX thus: ``` pip install upgrade pip pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html   ``` Inside python 3.7 I type 'import jax' but t I get this error: ```  version `GLIBCXX_3.4.26' not found ``` According to this issue, , I can solve this by first creating a venv and then installing: ``` python m venv env source env/bin/activate pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html   ``` This partly works, in that I can now 'import jax' and run it. However, it fails when I use 'jax.scan': In particlar, the code snippet below gives this error: ``` 20220117 19:46:23.259785: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2086] Execution of replica 0 failed: INTERNAL: CustomCall failed: jaxlib/cuda_prng_kernels.cc:30: operation cudaGetLastError() failed: the provided PTX was compiled with an unsupported toolchain. ``` Here is the code: ``` import jax import jax.numpy as jnp  sample from a Markov chain init_dist = jnp.array([0.8, 0.2]) trans_mat = jnp.array([[0.9, 0.1], [0.5, 0.5]]) rng_key = jax.random.PRNGKey(0) from jax.scipy.special import logit seq_len = 15 initial_state = jax.random.categorical(rng_key, logits=logit(init_dist), shape=(1,)) def draw_state(prev_state, key):     logits = logit(trans_mat[:, prev_state])     state = jax.random.categorical(key, logits=logits.flatten(), shape=(1,))     return state, state rng_key, rng_state, rng_obs = jax.random.split(rng_key, 3) keys = jax.random.split(rng_state, seq_len  1) final_state, states = jax.lax.scan(draw_state, initial_state, keys) print(states) ```",2022-01-17T19:59:02Z,bug,closed,3,17,https://github.com/jax-ml/jax/issues/9218,This is likely specific to c0deeplearningcommoncu113v20211219debian10.  could you help take a look?,"The `GLIBCXX` version issue was for the last user a `scipy` issue. There's not much we can do other than to drop our scipy dependency or make it optional. That may be possible, we'd have to look into it. ""the provided PTX was compiled with an unsupported toolchain"" means that the driver version on the VM is too old for the JAX binary. We may need to build with an older CUDA release. Another option would be for JAX to warn if the CUDA release is too old and hint that the user needs to upgrade their driver.","On Tue, Jan 18, 2022 at 6:12 AM Peter Hawkins ***@***.***> wrote: > The GLIBCXX version issue was for the last user a scipy issue. There's > not much we can do other than to drop our scipy dependency or make it > optional. That may be possible, we'd have to look into it. > > ""the provided PTX was compiled with an unsupported toolchain"" means that > the driver version on the VM is too old for the JAX binary. We may need to > build with an older CUDA release. Another option would be for JAX to warn > if the CUDA release is too old and hint that the user needs to upgrade > their driver. > How can we do that? Given that this is what every GCP user (who requesta GPU VM) is going to experience, I think the instructions should be clear, otherwise it will just drive users towards other cloud providers. > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > Triage notifications on the go with GitHub Mobile for iOS >  > or Android > . > > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","Any updates?  I verified the problem persists with V100 (not A100) since it's the same machine image. It seems the same problem was reported in https://github.com/google/jax/issues/6628. I tried that solution (`export XLA_FLAGS=""xla_gpu_cuda_data_dir=/usr/local/cuda/""`) to no avail. Are you proposing users install CUDA 11.2 themselves just so they can run JAX (following https://cloud.google.com/compute/docs/gpus/installdriversgpu)? Since it seems the easy route of using the default 'deep learning VM image'  gives 11.3 which does not work. BTW the issue with scipy is really about jax, not scipy. If you create a new VM, install jax, start python and type `import jax` you get the scipy GLIBCXX error. By contrast, if you `import scipy` you do not get that error. So I think this error is a warning sign of the deeper problem about incompatible versions.","I've never used the ""deep learning"" images, which is why this has never come up for me. I tend to just install Debian images and install the Python/Nvidia packages myself. I looked into the GLIBCXX version issue a bit. It's actually a bit more complicated than I thought. It's related to conda. TL;DR: three workarounds are: a) `import scipy.signal` before importing JAX. We could add this workaround to jax itself, and I think we should probably just do that for now. b) make sure Conda's copy of `libstdc++` is first in your library path, e.g.,: ``` $ LD_LIBRARY_PATH=/opt/conda/lib ipython ``` c) use a conda venv, which I think has the effect of doing (b). The issue is that the VM image has two copies of libstdc++, one in `/lib` from the OS, and one in `/opt/conda/lib` from Conda. The one in `/lib` is older than the one provided by conda. JAX is happy using either, and it doesn't know anything about the copy from conda. `scipy.signal`, when built from Conda requires the newer copy from conda. (If you used a pipinstalled copy of `scipy` it would not have this problem: the manylinux2014 specification forbids such a new `libstdc++` requirement.) Whichever version of libstdc++ you import first in the process is the copy you get. If you import JAX first, then you get the copy from `/lib`, which is too old for the `scipy.signal` package from conda. If you import `scipy.signal` first, then you get Conda's copy, which is newer and everyone is happy. Note: this isn't really a JAX issue! The same would be true for any pipinstalled package that uses C++. For example, you can reproduce this with TensorFlow: ``` $ pip install tensorflow $ ipython Python 3.7.12  (default, Oct 26 2021, 06:08:53) Type 'copyright', 'credits' or 'license' for more information IPython 7.30.1  An enhanced Interactive Python. Type '?' for help. In [1]: import torch In [2]: import scipy.signal  ImportError                               Traceback (most recent call last)  in  > 1 import scipy.signal /opt/conda/lib/python3.7/sitepackages/scipy/signal/__init__.py in      292 from . import sigtools, windows     293 from .waveforms import * > 294 from ._max_len_seq import max_len_seq     295 from ._upfirdn import upfirdn     296 /opt/conda/lib/python3.7/sitepackages/scipy/signal/_max_len_seq.py in        6 import numpy as np       7 > 8 from ._max_len_seq_inner import _max_len_seq_inner       9      10 __all__ = ['max_len_seq'] ImportError: /lib/x86_64linuxgnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by /opt/conda/lib/python3.7/sitepackages/scipy/signal/_max_len_seq_inner.cpython37mx86_64linuxgnu.so) ``` I can add the workaround of importing `scipy.signal` to JAX before we import any of JAX's own libraries, effectively replicating that workaround. But I'd argue it's really a Conda bug, although I'm not quite sure how to fix it. Another possible workaround would be if JAX provided conda packages and you installed JAX via Conda. I'll look more at the CUDA versioning issue later.","I looked into the CUDA version problem. The issue is that the driver installed by the deep learning VM (NB. the *driver* version, not the CUDA version: the CUDA version is fine) is too old for the JAX wheels, in the case that you are using a GPU (like an A100) for which we don't explicitly compile SASS code. Good news: we added A100 to the list of GPUs we support for the next jaxlib release (0.1.76, which hopefully we will release soon). (I was unable to reproduce this problem on a V100.) I think we may also need to change at least one of our wheels to be built with a lower driver version requirement for folks with older drivers to use. I'm not sure why the deep learning VM has such an old driver version. I'll ask them if it can be upgraded. Building jaxlib from source on the VM would have solved this problem also.","I verified that upgrading the driver to `470.82.01` on the VM fixed the problem (well, it takes a few minutes to start JAX the first time because it has to build a bunch of PTX code for the A100 GPU, but the next jaxlib release will fix that).","Great news, thanks! When will the new jaxlib release come out?","I see that jaxlib 0.1.76 is released! On my A100 machine, I uninstalled jax and jaxlib and then ran  ``` pip install upgrade pip pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html   ``` but it still uses jaxlib0.1.75 (see below). I think you need to update https://storage.googleapis.com/jaxreleases/jax_releases.html? ```  pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_releases.html   Looking in links: https://storage.googleapis.com/jaxreleases/jax_releases.html Requirement already satisfied: jax[cuda] in /opt/conda/lib/python3.7/sitepackages (0.2.27) Requirement already satisfied: abslpy in /opt/conda/lib/python3.7/sitepackages (from jax[cuda]) (1.0.0) Requirement already satisfied: opteinsum in /opt/conda/lib/python3.7/sitepackages (from jax[cuda]) (3.3.0) Requirement already satisfied: scipy>=1.2.1 in /opt/conda/lib/python3.7/sitepackages (from jax[cuda]) (1.7.3) Requirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.7/sitepackages (from jax[cuda]) (1.19.5) Requirement already satisfied: typingextensions in /opt/conda/lib/python3.7/sitepackages (from jax[cuda]) (4.0.1) Collecting jaxlib==0.1.75+cuda11.cudnn82   Using cached https://storage.googleapis.com/jaxreleases/cuda11/jaxlib0.1.75%2Bcuda11.cudnn82cp37nonemanylinux2010_x86_64.whl (149.4 MB) Requirement already satisfied: flatbuffers=1.12 in /opt/conda/lib/python3.7/sitepackages (from jaxlib==0.1.75+cuda11.cudnn82>jax[cuda]) (2.0) Requirement already satisfied: six in ./.local/lib/python3.7/sitepackages (from abslpy>jax[cuda]) (1.16.0) Installing collected packages: jaxlib Successfully installed jaxlib0.1.75+cuda11.cudnn82 ```","It's... not quite yet released. We found one last bug in our presubmit testing. But if you manually download e.g., https://storage.googleapis.com/jaxreleases/cuda11/jaxlib0.1.76+cuda11.cudnn805cp39nonemanylinux2010_x86_64.whl  substituting the python version as appropriate it may work for you. We may need to revise it before publishing it though.","We pushed the new wheels, so this should now work. I verified your example works on my DL GPU image on A100 (modulo the ""you should use a virtual env or import scipy.signal"" problem, for which workarounds are discussed above). Hope that helps!","I made a venv, and  tried ``` pip install https://storage.googleapis.com/jaxreleases/cuda11/jaxlib0.1.76+cuda11.cudnn82cp37nonemanylinux2010_x86_64.whl ``` In python I have the latest version of jax and jaxlib ``` >>> import jaxlib >>> jaxlib.__version__ '0.1.76' >>> import jax ja>>> jax.__version__ '0.2.27' ``` Most things seem to work, except for RNG: In particular, this fails: ```  import jax; import jax.numpy as jnp; from jax.scipy.special import logit; init_dist = jnp.array([0.8, 0.2]); rng_key = jax.random.PRNGKey(0);initial_state = jax.random.categorical(rng_key, logits=logit(init_dist), shape=(1,)) ``` with this error ```  File ""/home/kpmurphy_google_com/env/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 444, in _execute_compiled     out_bufs = compiled.execute(input_bufs) jax._src.traceback_util.UnfilteredStackTrace: RuntimeError: INTERNAL: CustomCall failed: jaxlib/cuda_prng_kernels.cc:30: operation cudaGetLastError() failed: the provided PTX was compiled with an unsupported toolchain. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/kpmurphy_google_com/env/lib/python3.7/sitepackages/jax/_src/random.py"", line 1212, in categorical     gumbel(key, sample_shape + logits.shape, logits.dtype) +   File ""/home/kpmurphy_google_com/env/lib/python3.7/sitepackages/jax/_src/random.py"", line 1170, in gumbel     return _gumbel(key, shape, dtype)   File ""/home/kpmurphy_google_com/env/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 444, in _execute_compiled     out_bufs = compiled.execute(input_bufs) RuntimeError: INTERNAL: CustomCall failed: jaxlib/cuda_prng_kernels.cc:30: operation cudaGetLastError() failed: the provided PTX was compiled with an unsupported toolchain. ```","To update this issue: the installation instructions (https://github.com/google/jaxpipinstallationgpucuda) now clearly state you must have a new enough driver version. Unfortunately the GCP deep learning image does not satisfy these requirements, so if you want to use the deep learning image you probably need to use NVidia's CUDA forward compatibility packages or update the driver version in the image. We've raised this issue with the owners of the image.","Quick update. I created a new V100 VM with Deep Learning image but chose the pytorch image  !image This has these specs ``` nvidiasmi NVIDIASMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2   cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR A 2 define CUDNN_MAJOR 8 define CUDNN_MINOR 0 define CUDNN_PATCHLEVEL 5 ``` I could install jax using  ``` pip install jax[cuda11_cudnn805] f https://storage.googleapis.com/jaxreleases/jax_releases.html ``` This runs without errors (including `import scipy.signal`), but I get warnings when I use RNG: ``` external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. ```","I also created a VM using cuda 11.2 ``` c0deeplearningcommoncu113v20220227debian10  NVIDIASMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2  ``` which now won't even run the RNG code: ``` RuntimeError: INTERNAL: CustomCall failed: jaxlib/cuda_prng_kernels.cc:30: operation cudaGetLastError() failed: the provided PTX was compiled with an unsupported toolchain. ``` So the bottom line is: it seems very unclear how to run JAX on GCP unless you are willing to install CUDA from scratch, which seems hard. In particular,  according to go/stepbystepgcp, the steps are: Manually install NVIDIA driver 470.82.01, CUDA 11.4.4, and CuDNN 8.2.4 (Here are two blog posts about how one might go about doing this: 1, 2). Note that you will need to register for a NVIDIA’s developer account to download the CuDNN tarball. You can download it on your local machine, and upload that file using the web ssh interface. https://github.com/ashutoshIITK/install_cuda_cudnn_ubuntu_20","I think there's still problem. `nvidiasmi` !image `cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR A 2` ``` define CUDNN_MAJOR 8 define CUDNN_MINOR 0 define CUDNN_PATCHLEVEL 5  define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL) endif /* CUDNN_VERSION_H */ ``` `nvcc version` ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052020 NVIDIA Corporation Built on Wed_Jul_22_19:09:09_PDT_2020 Cuda compilation tools, release 11.0, V11.0.221 Build cuda_11.0_bu.TC445_37.28845127_0 ``` I tried with `pip install ""jax[cuda11_cudnn805]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`. It did work but it complains: ``` You may not need to update to CUDA 11.1; cherrypicking the ptxas binary is often sufficient. 20220715 06:05:36.687886: W external/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalidaddress errors. ``` Any workarounds? ","The CUDA driver version incompatibility problem should be fixed in the next jaxlib release. JAX will automatically fall back to not using parallel compilation if the NVIDIA driver is too old. Unfortunately we had to revert the workaround for `version GLIBCXX_3.4.26 not found` because the workaround was to import `scipy` ourselves, but that turns out to be too slow to do every time jax is imported. If you still see that problem, I recommend one of the workarounds above. Note that `jax` is also available via `condaforge` (https://github.com/google/jaxcondainstallation) and using the `conda` installation of JAX will not have this issue.  the issue is you have CUDA 11.0 installed. JAX doesn't support CUDA 11.0. Install a newer CUDA."
7104,"以下是一个github上的jax下的一个issue, 标题是(PyTorch Dataloading doesn't work with >0 workers)， 内容是 (Hi! I'm new to the JAX ecosystem, have used PyTorch and TensorFlow extensively for over 5 years. My issue is that I can't get PyTorch data loading to work with jax/flax with `num_workers>0`.  Following is a minimal example to reproduce my issues ```python import argparse from typing import Sequence from functools import partial import flax from typing import Any import optax from flax.training import train_state import numpy as np import jax import jax.numpy as jnp import flax.linen as nn import tqdm from torchvision.datasets import CIFAR10 from flax.training import common_utils import torch import torchvision.transforms as transforms import torch.multiprocessing as multiprocessing multiprocessing.set_start_method('spawn') NUM_CLASSES = 10 NUM_EPOCHS = 50 BATCH_SIZE = 512 parser = argparse.ArgumentParser() parser.add_argument(""num_workers"", default=0, type=int) def collate_fn(batch):     inputs_np = []     targets_np = []     for item in batch:         inp_np = item[0].permute(1, 2, 0).detach().numpy()         tgts_np = item[1]         inputs_np.append(inp_np)         targets_np.append(tgts_np)     inputs_np = np.asarray(inputs_np)     targets_np = np.asarray(targets_np)     return inputs_np, targets_np class CNN(nn.Module):     .compact     def __call__(self, inputs, train=False):         conv = partial(nn.Conv, kernel_size=(3, 3), strides=(2, 2),                         use_bias=False, kernel_init=jax.nn.initializers.kaiming_normal())         bn = partial(nn.BatchNorm, use_running_average=not train, momentum=0.9,                    epsilon=1e5)         x = conv(features=32)(inputs)         x = bn()(x)         x = nn.relu(x)         x = conv(features=64)(x)         x = bn()(x)         x = nn.relu(x)         x = conv(features=128)(x)         x = bn()(x)         x = nn.relu(x)         x = nn.max_pool(x, window_shape=(4, 4), strides=(1, 1))         x = x.reshape((x.shape[0], 1))         x = nn.Dense(NUM_CLASSES)(x)         return x def initialize(key, inp_shape, model):   input_shape = (1,) + inp_shape   .jit   def init(*args):     return model.init(*args)   variables = init({'params': key}, jnp.ones(input_shape))   return variables['params'], variables['batch_stats'] .jit def cross_entropy_loss(logits, labels):     one_hot_labels = common_utils.onehot(labels, num_classes=NUM_CLASSES)     xentropy = optax.softmax_cross_entropy(logits=logits, labels=one_hot_labels)     return jnp.mean(xentropy) .jit def calculate_accuracy(logits, labels):     accuracy = jnp.mean(jnp.argmax(logits, 1) == labels)     return accuracy .jit def train_step(state, images, labels):     step = state.step     .jit     def cost_fn(params):         logits, new_model_state = state.apply_fn(             {""params"": params, ""batch_stats"": state.batch_stats},             images,             mutable=['batch_stats'],             train=True         )         loss = cross_entropy_loss(logits, labels)         weight_penalty_params = jax.tree_leaves(params)         weight_l2 = sum([jnp.sum(x ** 2)                         for x in weight_penalty_params                         if x.ndim > 1])         weight_decay=0.0001         weight_penalty = weight_decay * 0.5 * weight_l2         loss = loss + weight_penalty         return loss, (new_model_state, logits)     grad_fn = jax.value_and_grad(cost_fn, has_aux=True)     aux, grads = grad_fn(state.params)     new_model_state, logits = aux[1]     acc = calculate_accuracy(logits, labels)     new_state = state.apply_gradients(grads=grads, batch_stats=new_model_state['batch_stats'])     return new_state, aux[0], acc .jit def eval_step(state, images, labels):     logits = state.apply_fn(         {""params"": state.params,          ""batch_stats"": state.batch_stats},          images, train=False, mutable=False)     return calculate_accuracy(logits, labels) class TrainState(train_state.TrainState):     batch_stats: Any if __name__ == ""__main__"":   args = parser.parse_args()   cnn = CNN()   key = jax.random.PRNGKey(0)   key, *subkeys = jax.random.split(key, 4)   params, batch_stats = initialize(subkeys[0], (32, 32, 3), cnn)   tx = optax.adam(     1e3   )   state = TrainState.create(       apply_fn=cnn.apply,       params=params,       tx=tx,       batch_stats=batch_stats   )   transform = transforms.Compose(   [transforms.ToTensor(),       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])   batch_size = BATCH_SIZE   trainset = CIFAR10(root='./data', train=True,                                           download=True, transform=transform)   trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, drop_last=True,                                           shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn)   num_tr_steps = len(trainloader)   testset = CIFAR10(root='./data', train=False,                                       download=True, transform=transform)   testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, drop_last=True,                                           shuffle=False, num_workers=args.num_workers, collate_fn=collate_fn)   num_test_steps = len(testloader)   for epoch in range(1, NUM_EPOCHS+1):     print(""Starting epoch {}"".format(epoch))     train_loss = []     train_acc = []     itercnt = 0     for batch in trainloader:       images, labels = batch       state, loss, acc = train_step(state, images, labels)       if itercnt == 0:         print(""Input shape:"", images.shape)         print(""labels shape:"", labels.shape)       if itercnt % 25 == 0:         print(""[{:03d}]  Train Acc: {:.04f}"".format(       np.mean(train_loss), np.mean(train_acc)))     print(""\t Val Acc: {:.04f}"".format(np.mean(val_accs))) ```  Problem encountered: I've tried running the script on both `TPU` and `GPU`: it works fine when `num_workers = 0`, but doesn't work with  `num_workers > 0`. An earlier issue from 2020 recommended setting `torch.multiprocessing.set_start_method('spawn')`, but that didn't fix the issue for me. Unlike the author of that issue, I'm not using jax primitives in the data loading pipeline at all (as can be seen in the `collate_fn()` function) With `num_workers>0`, I get the following errors:  On GPU  With `torch.multiprocessing.set_start_method('spawn')` throws `RuntimeError: context has already been set`  With `torch.multiprocessing.set_start_method('fork')` throws `Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error`  On TPUv28 VM  With `torch.multiprocessing.set_start_method('spawn')` throws `libtpu.so already in use by another process`, followed by `RuntimeError: context has already been set` later in the stack trace.  With `torch.multiprocessing.set_start_method('fork')` I get no error, the dataloader hangs indefinitely. Following are the packages being used: ``` torch==1.9.0+cu111 jax==0.2.26 jaxlib==0.1.75       +cuda11.cudnn82 for GPU ``` Any help is appreciated!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,PyTorch Dataloading doesn't work with >0 workers,"Hi! I'm new to the JAX ecosystem, have used PyTorch and TensorFlow extensively for over 5 years. My issue is that I can't get PyTorch data loading to work with jax/flax with `num_workers>0`.  Following is a minimal example to reproduce my issues ```python import argparse from typing import Sequence from functools import partial import flax from typing import Any import optax from flax.training import train_state import numpy as np import jax import jax.numpy as jnp import flax.linen as nn import tqdm from torchvision.datasets import CIFAR10 from flax.training import common_utils import torch import torchvision.transforms as transforms import torch.multiprocessing as multiprocessing multiprocessing.set_start_method('spawn') NUM_CLASSES = 10 NUM_EPOCHS = 50 BATCH_SIZE = 512 parser = argparse.ArgumentParser() parser.add_argument(""num_workers"", default=0, type=int) def collate_fn(batch):     inputs_np = []     targets_np = []     for item in batch:         inp_np = item[0].permute(1, 2, 0).detach().numpy()         tgts_np = item[1]         inputs_np.append(inp_np)         targets_np.append(tgts_np)     inputs_np = np.asarray(inputs_np)     targets_np = np.asarray(targets_np)     return inputs_np, targets_np class CNN(nn.Module):     .compact     def __call__(self, inputs, train=False):         conv = partial(nn.Conv, kernel_size=(3, 3), strides=(2, 2),                         use_bias=False, kernel_init=jax.nn.initializers.kaiming_normal())         bn = partial(nn.BatchNorm, use_running_average=not train, momentum=0.9,                    epsilon=1e5)         x = conv(features=32)(inputs)         x = bn()(x)         x = nn.relu(x)         x = conv(features=64)(x)         x = bn()(x)         x = nn.relu(x)         x = conv(features=128)(x)         x = bn()(x)         x = nn.relu(x)         x = nn.max_pool(x, window_shape=(4, 4), strides=(1, 1))         x = x.reshape((x.shape[0], 1))         x = nn.Dense(NUM_CLASSES)(x)         return x def initialize(key, inp_shape, model):   input_shape = (1,) + inp_shape   .jit   def init(*args):     return model.init(*args)   variables = init({'params': key}, jnp.ones(input_shape))   return variables['params'], variables['batch_stats'] .jit def cross_entropy_loss(logits, labels):     one_hot_labels = common_utils.onehot(labels, num_classes=NUM_CLASSES)     xentropy = optax.softmax_cross_entropy(logits=logits, labels=one_hot_labels)     return jnp.mean(xentropy) .jit def calculate_accuracy(logits, labels):     accuracy = jnp.mean(jnp.argmax(logits, 1) == labels)     return accuracy .jit def train_step(state, images, labels):     step = state.step     .jit     def cost_fn(params):         logits, new_model_state = state.apply_fn(             {""params"": params, ""batch_stats"": state.batch_stats},             images,             mutable=['batch_stats'],             train=True         )         loss = cross_entropy_loss(logits, labels)         weight_penalty_params = jax.tree_leaves(params)         weight_l2 = sum([jnp.sum(x ** 2)                         for x in weight_penalty_params                         if x.ndim > 1])         weight_decay=0.0001         weight_penalty = weight_decay * 0.5 * weight_l2         loss = loss + weight_penalty         return loss, (new_model_state, logits)     grad_fn = jax.value_and_grad(cost_fn, has_aux=True)     aux, grads = grad_fn(state.params)     new_model_state, logits = aux[1]     acc = calculate_accuracy(logits, labels)     new_state = state.apply_gradients(grads=grads, batch_stats=new_model_state['batch_stats'])     return new_state, aux[0], acc .jit def eval_step(state, images, labels):     logits = state.apply_fn(         {""params"": state.params,          ""batch_stats"": state.batch_stats},          images, train=False, mutable=False)     return calculate_accuracy(logits, labels) class TrainState(train_state.TrainState):     batch_stats: Any if __name__ == ""__main__"":   args = parser.parse_args()   cnn = CNN()   key = jax.random.PRNGKey(0)   key, *subkeys = jax.random.split(key, 4)   params, batch_stats = initialize(subkeys[0], (32, 32, 3), cnn)   tx = optax.adam(     1e3   )   state = TrainState.create(       apply_fn=cnn.apply,       params=params,       tx=tx,       batch_stats=batch_stats   )   transform = transforms.Compose(   [transforms.ToTensor(),       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])   batch_size = BATCH_SIZE   trainset = CIFAR10(root='./data', train=True,                                           download=True, transform=transform)   trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, drop_last=True,                                           shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn)   num_tr_steps = len(trainloader)   testset = CIFAR10(root='./data', train=False,                                       download=True, transform=transform)   testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, drop_last=True,                                           shuffle=False, num_workers=args.num_workers, collate_fn=collate_fn)   num_test_steps = len(testloader)   for epoch in range(1, NUM_EPOCHS+1):     print(""Starting epoch {}"".format(epoch))     train_loss = []     train_acc = []     itercnt = 0     for batch in trainloader:       images, labels = batch       state, loss, acc = train_step(state, images, labels)       if itercnt == 0:         print(""Input shape:"", images.shape)         print(""labels shape:"", labels.shape)       if itercnt % 25 == 0:         print(""[{:03d}]  Train Acc: {:.04f}"".format(       np.mean(train_loss), np.mean(train_acc)))     print(""\t Val Acc: {:.04f}"".format(np.mean(val_accs))) ```  Problem encountered: I've tried running the script on both `TPU` and `GPU`: it works fine when `num_workers = 0`, but doesn't work with  `num_workers > 0`. An earlier issue from 2020 recommended setting `torch.multiprocessing.set_start_method('spawn')`, but that didn't fix the issue for me. Unlike the author of that issue, I'm not using jax primitives in the data loading pipeline at all (as can be seen in the `collate_fn()` function) With `num_workers>0`, I get the following errors:  On GPU  With `torch.multiprocessing.set_start_method('spawn')` throws `RuntimeError: context has already been set`  With `torch.multiprocessing.set_start_method('fork')` throws `Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error`  On TPUv28 VM  With `torch.multiprocessing.set_start_method('spawn')` throws `libtpu.so already in use by another process`, followed by `RuntimeError: context has already been set` later in the stack trace.  With `torch.multiprocessing.set_start_method('fork')` I get no error, the dataloader hangs indefinitely. Following are the packages being used: ``` torch==1.9.0+cu111 jax==0.2.26 jaxlib==0.1.75       +cuda11.cudnn82 for GPU ``` Any help is appreciated!",2022-01-13T15:27:46Z,bug,open,1,13,https://github.com/jax-ml/jax/issues/9190,You might get more traction asking about this in the torch project.,"> You might get more traction asking about this in the torch project. I'll give that a try, but this only happens when the model itself is implemented in Jax.  In the same env, everythingtorch works absolutely fine.","Hi!  I think that it's going to be really hard to make this work.  We generally don't try to support pythonmultiprocessing:  all the internal C++ libs we use aren't written to be forksafe, and I'm not sure that TPU `libtpu.so` can be used with multiprocessing at all. Usually we recommend that people use TFDS / tf.data based dataloaders as they're _far_ more CPU efficient for feeding multiple GPUs or TPUs than torch dataloaders with multiprocessing.","> Hi! I think that it's going to be really hard to make this work. We generally don't try to support pythonmultiprocessing: all the internal C++ libs we use aren't written to be forksafe, and I'm not sure that TPU `libtpu.so` can be used with multiprocessing at all. >  > Usually we recommend that people use TFDS / tf.data based dataloaders as they're _far_ more CPU efficient for feeding multiple GPUs or TPUs than torch dataloaders with multiprocessing. Thanks for the detailed reply. I'll see how much time moving all data operations to tensorflow based ops will take. However, I believe this information should be added to the official tutorial on using pytorch dataloaders with Jax, as this is quite the limitation. Using multiple workers in dataloaders is a standard practice in the PyTorch realm. It will work for small datasets and a quick proof of concept for researchers/teams thinking about making the move to Jax, sure, but for fullbore training, using torch data loaders with Jax would not be feasible. Adding this as a disclaimer to the abovementioned tutorial will save valuable time in my opinion.","+1 I agree these gotchas are major and should be mentioned front and center, it took me a long time and many lost hours this week to figure this out for myself. It's a limitation of the jax ecosystem right now! ","This issue appears to be a regression compared to one year ago. I was using multiworker data loaders in sabertooth and they worked fine at the time, but no longer work with newly started TPU VMs. I want to emphasize that the data workers are _not using JAX nor accessing the TPUs in any way_, just doing pure numpy computation. `torch.multiprocessing.set_start_method('spawn')` sort of works as a work around. I've managed to avoid the error `RuntimeError: context has already been set` with the idiom `if __name__ == '__main__': torch.multiprocessing.set_start_method('spawn')`  I had to wrap it so that each spawned worker don't itself attempt to set the start method. However this workaround still has issues: each worker takes a really long time to spawn, and generates a bunch of `libtpu.so already in use by another process` messages. Setting `persistent_workers=True` helps cut down on these but it's still annoying. Given that this is a regression, is it really the case that it can't be fixed? None of the child processes are actually doing anything with the TPU.","Agreed, would be great to find a solution ASAP thank you  !",Are there any updates on this? It is frustrating to find out that PyTorch data loader cannot work with Jax on TPU despite it is used in Jax's official examples.  ,  do you perhaps have a small repro for the failure?, there is a code snippet for the failure in  CC(TPU hang (when loading data from an external process)).,Following up on this,I also met similar problems on A100 GPU.  But I have no idea how to fix it.,"Same problem on NV T4 GPU, it's a disaster when training a tiny model with huge datasets."
1996,"以下是一个github上的jax下的一个issue, 标题是(Pjit does not handle static_argnums correctly)， 内容是 (It seems that pjit does not handle static_argnums correctly ```python class MyModel:   def __init__(self, num_layers: int):     self.num_layers = num_layers   def compute(self, x):     for i in range(self.num_layers):        Imagine here a real implementation of neural networks.       x = x + 10     return x def run_my_model(x, model):   print(""Runing/Tracing run_my_model"")   return model.compute(x) input = jnp.arange(4) model = MyModel(num_layers=3) with mesh(jax.devices(), [""data""]):   pjit(run_my_model, in_axis_resources=None,         out_axis_resources=None, static_argnums=1)(input, model) ``` This results in error: ``` TypeError                                 Traceback (most recent call last)  in ()       4 with mesh(jax.devices(), [""data""]):       5   pjit(run_my_model, in_axis_resources=None,  > 6        out_axis_resources=None, static_argnums=1)(input, model) 1 frames /usr/local/lib/python3.7/distpackages/jax/experimental/pjit.py in wrapped(*args, **kwargs)     229   def wrapped(*args, **kwargs):     230     for arg in tree_leaves(args): > 231       _check_arg(arg)     232     args_flat, params, _, out_tree = infer_params(*args, **kwargs)     233     out = pjit_p.bind(*args_flat, **params) /usr/local/lib/python3.7/distpackages/jax/_src/api.py in _check_arg(arg)    2685 def _check_arg(arg):    2686   if not (isinstance(arg, core.Tracer) or _valid_jaxtype(arg)): > 2687     raise TypeError(f""Argument '{arg}' of type {type(arg)} is not a valid JAX type."")    2688     2689  TODO(necula): this duplicates code in core.valid_jaxtype TypeError: Argument '' of type  is not a valid JAX type. ``` Colab repro  as you can see, the same thing with jit just works. https://colab.research.google.com/drive/1yiAibGXqfZqhTA7lptG2XCJhnaA95Vh0?usp=sharing)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Pjit does not handle static_argnums correctly,"It seems that pjit does not handle static_argnums correctly ```python class MyModel:   def __init__(self, num_layers: int):     self.num_layers = num_layers   def compute(self, x):     for i in range(self.num_layers):        Imagine here a real implementation of neural networks.       x = x + 10     return x def run_my_model(x, model):   print(""Runing/Tracing run_my_model"")   return model.compute(x) input = jnp.arange(4) model = MyModel(num_layers=3) with mesh(jax.devices(), [""data""]):   pjit(run_my_model, in_axis_resources=None,         out_axis_resources=None, static_argnums=1)(input, model) ``` This results in error: ``` TypeError                                 Traceback (most recent call last)  in ()       4 with mesh(jax.devices(), [""data""]):       5   pjit(run_my_model, in_axis_resources=None,  > 6        out_axis_resources=None, static_argnums=1)(input, model) 1 frames /usr/local/lib/python3.7/distpackages/jax/experimental/pjit.py in wrapped(*args, **kwargs)     229   def wrapped(*args, **kwargs):     230     for arg in tree_leaves(args): > 231       _check_arg(arg)     232     args_flat, params, _, out_tree = infer_params(*args, **kwargs)     233     out = pjit_p.bind(*args_flat, **params) /usr/local/lib/python3.7/distpackages/jax/_src/api.py in _check_arg(arg)    2685 def _check_arg(arg):    2686   if not (isinstance(arg, core.Tracer) or _valid_jaxtype(arg)): > 2687     raise TypeError(f""Argument '{arg}' of type {type(arg)} is not a valid JAX type."")    2688     2689  TODO(necula): this duplicates code in core.valid_jaxtype TypeError: Argument '' of type  is not a valid JAX type. ``` Colab repro  as you can see, the same thing with jit just works. https://colab.research.google.com/drive/1yiAibGXqfZqhTA7lptG2XCJhnaA95Vh0?usp=sharing",2022-01-11T11:41:33Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/9168,Is this still an issue? Looks like `Model` should be a pytree.,Closing because of lack of response. Feel free to reopen if you can still repro it.
8216,"以下是一个github上的jax下的一个issue, 标题是(Internal error when downcasting to half precision in pmap)， 内容是 (Attempted mixedprecision training with [NFNets] on Colab TPUv3, ended up with a weird bug that JAX insists is some kind of an internal runtime error (its words, not mine). It's taken me days at this point to figure out what was wrong, but everything works in full precision. The runtime error seems to result from trying to do the `pmean` to synchronize parameter replicas in halfprecision as shown in [Haiku's ImageNet demo][demo]. Maybe someone here can help me figure out why it doesn't work with NFNets? I am very surprised, and completely stumped. I thought it might be something to do with stochastic depth changing the topology of the model's parameter trees during training, but running with `stochdepth_rate=0.0` on my `NFNet` instantiation doesn't seem to have done much of anything. I would love to know how I can change my code to fix this, but the stack trace isn't particularly informative. Help? Edit: I feel as though I should note that, although that is not the reproduction shown here because it's not as representative of my usecase, one of the first things I tried was doing _everything_ in halfprecision, that is initializing the model on float16 data, ending up with a pytree of entirely float16 parameters, and then attempting training as normal. It wouldn't have been that useful to me if it had worked, but based on the error message about shapes changing somewhere I sort of expected it to. I was even more befuddled when it _didn't,_ so I feel as though that may be useful to know for someone trying to figure out why. [NFNets]: https://github.com/deepmind/deepmindresearch/tree/master/nfnets [demo]: https://github.com/deepmind/dmhaiku/blob/main/examples/imagenet/train.pyL161 ```python import jax import haiku import numpy as np import jax.numpy as jnp from jax.tools.colab_tpu import setup_tpu from nfnets.nfnet import NFNet   svn co https://github.com/deepmind/deepmindresearch/trunk/nfnets setup_tpu() .transform def model(x, is_training=True):   return NFNet(128, stochdepth_rate=0.0)(x, is_training=is_training)[""logits""] k = jax.local_device_count() rng = jax.random.PRNGKey(42) params = jax.pmap(model.init)(jax.random.split(rng, k), np.zeros([k, 224, 224, 3])) jax.tree_map(   lambda arr: jnp.any(jnp.isnan(arr)),   could be any op, downcasting in pmean triggers the error    jax.pmap(lambda params: jax.lax.pmean(params, ""i""), axis_name=""i"")(     jax.tree_map(lambda x: x.astype(jnp.float16), params)   ) ) ``` Trace: ```  UnfilteredStackTrace                      Traceback (most recent call last)  in ()       3   jax.pmap(lambda params: jax.lax.pmean(params, ""i""), axis_name=""i"")( > 4       jax.tree_map(lambda x: x.astype(jnp.float16), state.params)       5   ) 17 frames /usr/local/lib/python3.7/distpackages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     161     try: > 162       return fun(*args, **kwargs)     163     except Exception as e: /usr/local/lib/python3.7/distpackages/jax/_src/api.py in cache_miss(*args, **kwargs)    2057  > 2058     out_tree, out_flat = f_pmapped_(*args, **kwargs)    2059     out_pytree_def = out_tree() /usr/local/lib/python3.7/distpackages/jax/_src/api.py in f_pmapped(*args, **kwargs)    1939         name=p.flat_fun.__name__, donated_invars=p.donated_invars, > 1940         global_arg_shapes=p.global_arg_shapes_flat)    1941     return p.out_tree, out /usr/local/lib/python3.7/distpackages/jax/core.py in bind(self, fun, *args, **params)    1726     assert len(params['in_axes']) == len(args) > 1727     return call_bind(self, fun, *args, **params)    1728  /usr/local/lib/python3.7/distpackages/jax/core.py in call_bind(primitive, fun, *args, **params)    1651   tracers = map(top_trace.full_raise, args) > 1652   outs = primitive.process(top_trace, fun, tracers, params)    1653   return map(full_lower, apply_todos(env_trace_todo(), outs)) /usr/local/lib/python3.7/distpackages/jax/core.py in process(self, trace, fun, tracers, params)    1729   def process(self, trace, fun, tracers, params): > 1730     return trace.process_map(self, fun, tracers, params)    1731  /usr/local/lib/python3.7/distpackages/jax/core.py in process_call(self, primitive, f, tracers, params)     632   def process_call(self, primitive, f, tracers, params): > 633     return primitive.impl(f, *tracers, **params)     634   process_map = process_call /usr/local/lib/python3.7/distpackages/jax/interpreters/pxla.py in xla_pmap_impl(fun, backend, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, *args)     768       in_axes, out_axes_thunk, donated_invars, global_arg_shapes, > 769       *abstract_args)     770  /usr/local/lib/python3.7/distpackages/jax/linear_util.py in memoized_fun(fun, *args)     262     else: > 263       ans = call(fun, *args)     264       cache[key] = (ans, fun.stores) /usr/local/lib/python3.7/distpackages/jax/interpreters/pxla.py in parallel_callable(fun, backend_name, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, *avals)     796       in_axes, out_axes_thunk, donated_invars, global_arg_shapes, avals) > 797   pmap_executable = pmap_computation.compile()     798   return WeakRefList([pmap_executable.unsafe_call, pmap_executable.fingerprint]) /usr/local/lib/python3.7/distpackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper /usr/local/lib/python3.7/distpackages/jax/interpreters/pxla.py in compile(self)    1047     if self._executable is None: > 1048       self._executable = PmapExecutable.from_hlo(self._hlo, *self.compile_args)    1049     return self._executable /usr/local/lib/python3.7/distpackages/jax/interpreters/pxla.py in from_hlo(xla_computation, pci, replicas, parts, shards)    1171     compiled = dispatch.compile_or_get_cached( > 1172         pci.backend, xla_computation, compile_options)    1173     handle_args = InputsHandler( /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in compile_or_get_cached(backend, computation, compile_options)     533             return compiled > 534     return backend_compile(backend, computation, compile_options)     535  /usr/local/lib/python3.7/distpackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options)     513    separately in Python profiling results > 514   return backend.compile(built_c, compile_options=options)     515  UnfilteredStackTrace: RuntimeError: INTERNAL: during context [postoptimization]: Bitcast cannot have different shape sizes of output (8192) and operand (6144) (u32[1536]{0:T(1024)}) (f16[3072]{0:T(1024)(128)(2,1)}) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: RuntimeError                              Traceback (most recent call last)  in ()       2   lambda arr: jnp.any(jnp.isnan(arr)),       3   jax.pmap(lambda params: jax.lax.pmean(params, ""i""), axis_name=""i"")( > 4       jax.tree_map(lambda x: x.astype(jnp.float16), state.params)       5   )       6 ) /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options)     512    we use a separate function call to ensure that XLA compilation appears     513    separately in Python profiling results > 514   return backend.compile(built_c, compile_options=options)     515      516  TODO(phawkins): update users. RuntimeError: INTERNAL: during context [postoptimization]: Bitcast cannot have different shape sizes of output (8192) and operand (6144) (u32[1536]{0:T(1024)}) (f16[3072]{0:T(1024)(128)(2,1)}) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Internal error when downcasting to half precision in pmap,"Attempted mixedprecision training with [NFNets] on Colab TPUv3, ended up with a weird bug that JAX insists is some kind of an internal runtime error (its words, not mine). It's taken me days at this point to figure out what was wrong, but everything works in full precision. The runtime error seems to result from trying to do the `pmean` to synchronize parameter replicas in halfprecision as shown in [Haiku's ImageNet demo][demo]. Maybe someone here can help me figure out why it doesn't work with NFNets? I am very surprised, and completely stumped. I thought it might be something to do with stochastic depth changing the topology of the model's parameter trees during training, but running with `stochdepth_rate=0.0` on my `NFNet` instantiation doesn't seem to have done much of anything. I would love to know how I can change my code to fix this, but the stack trace isn't particularly informative. Help? Edit: I feel as though I should note that, although that is not the reproduction shown here because it's not as representative of my usecase, one of the first things I tried was doing _everything_ in halfprecision, that is initializing the model on float16 data, ending up with a pytree of entirely float16 parameters, and then attempting training as normal. It wouldn't have been that useful to me if it had worked, but based on the error message about shapes changing somewhere I sort of expected it to. I was even more befuddled when it _didn't,_ so I feel as though that may be useful to know for someone trying to figure out why. [NFNets]: https://github.com/deepmind/deepmindresearch/tree/master/nfnets [demo]: https://github.com/deepmind/dmhaiku/blob/main/examples/imagenet/train.pyL161 ```python import jax import haiku import numpy as np import jax.numpy as jnp from jax.tools.colab_tpu import setup_tpu from nfnets.nfnet import NFNet   svn co https://github.com/deepmind/deepmindresearch/trunk/nfnets setup_tpu() .transform def model(x, is_training=True):   return NFNet(128, stochdepth_rate=0.0)(x, is_training=is_training)[""logits""] k = jax.local_device_count() rng = jax.random.PRNGKey(42) params = jax.pmap(model.init)(jax.random.split(rng, k), np.zeros([k, 224, 224, 3])) jax.tree_map(   lambda arr: jnp.any(jnp.isnan(arr)),   could be any op, downcasting in pmean triggers the error    jax.pmap(lambda params: jax.lax.pmean(params, ""i""), axis_name=""i"")(     jax.tree_map(lambda x: x.astype(jnp.float16), params)   ) ) ``` Trace: ```  UnfilteredStackTrace                      Traceback (most recent call last)  in ()       3   jax.pmap(lambda params: jax.lax.pmean(params, ""i""), axis_name=""i"")( > 4       jax.tree_map(lambda x: x.astype(jnp.float16), state.params)       5   ) 17 frames /usr/local/lib/python3.7/distpackages/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     161     try: > 162       return fun(*args, **kwargs)     163     except Exception as e: /usr/local/lib/python3.7/distpackages/jax/_src/api.py in cache_miss(*args, **kwargs)    2057  > 2058     out_tree, out_flat = f_pmapped_(*args, **kwargs)    2059     out_pytree_def = out_tree() /usr/local/lib/python3.7/distpackages/jax/_src/api.py in f_pmapped(*args, **kwargs)    1939         name=p.flat_fun.__name__, donated_invars=p.donated_invars, > 1940         global_arg_shapes=p.global_arg_shapes_flat)    1941     return p.out_tree, out /usr/local/lib/python3.7/distpackages/jax/core.py in bind(self, fun, *args, **params)    1726     assert len(params['in_axes']) == len(args) > 1727     return call_bind(self, fun, *args, **params)    1728  /usr/local/lib/python3.7/distpackages/jax/core.py in call_bind(primitive, fun, *args, **params)    1651   tracers = map(top_trace.full_raise, args) > 1652   outs = primitive.process(top_trace, fun, tracers, params)    1653   return map(full_lower, apply_todos(env_trace_todo(), outs)) /usr/local/lib/python3.7/distpackages/jax/core.py in process(self, trace, fun, tracers, params)    1729   def process(self, trace, fun, tracers, params): > 1730     return trace.process_map(self, fun, tracers, params)    1731  /usr/local/lib/python3.7/distpackages/jax/core.py in process_call(self, primitive, f, tracers, params)     632   def process_call(self, primitive, f, tracers, params): > 633     return primitive.impl(f, *tracers, **params)     634   process_map = process_call /usr/local/lib/python3.7/distpackages/jax/interpreters/pxla.py in xla_pmap_impl(fun, backend, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, *args)     768       in_axes, out_axes_thunk, donated_invars, global_arg_shapes, > 769       *abstract_args)     770  /usr/local/lib/python3.7/distpackages/jax/linear_util.py in memoized_fun(fun, *args)     262     else: > 263       ans = call(fun, *args)     264       cache[key] = (ans, fun.stores) /usr/local/lib/python3.7/distpackages/jax/interpreters/pxla.py in parallel_callable(fun, backend_name, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, *avals)     796       in_axes, out_axes_thunk, donated_invars, global_arg_shapes, avals) > 797   pmap_executable = pmap_computation.compile()     798   return WeakRefList([pmap_executable.unsafe_call, pmap_executable.fingerprint]) /usr/local/lib/python3.7/distpackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper /usr/local/lib/python3.7/distpackages/jax/interpreters/pxla.py in compile(self)    1047     if self._executable is None: > 1048       self._executable = PmapExecutable.from_hlo(self._hlo, *self.compile_args)    1049     return self._executable /usr/local/lib/python3.7/distpackages/jax/interpreters/pxla.py in from_hlo(xla_computation, pci, replicas, parts, shards)    1171     compiled = dispatch.compile_or_get_cached( > 1172         pci.backend, xla_computation, compile_options)    1173     handle_args = InputsHandler( /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in compile_or_get_cached(backend, computation, compile_options)     533             return compiled > 534     return backend_compile(backend, computation, compile_options)     535  /usr/local/lib/python3.7/distpackages/jax/_src/profiler.py in wrapper(*args, **kwargs)     205     with TraceAnnotation(name, **decorator_kwargs): > 206       return func(*args, **kwargs)     207     return wrapper /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options)     513    separately in Python profiling results > 514   return backend.compile(built_c, compile_options=options)     515  UnfilteredStackTrace: RuntimeError: INTERNAL: during context [postoptimization]: Bitcast cannot have different shape sizes of output (8192) and operand (6144) (u32[1536]{0:T(1024)}) (f16[3072]{0:T(1024)(128)(2,1)}) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: RuntimeError                              Traceback (most recent call last)  in ()       2   lambda arr: jnp.any(jnp.isnan(arr)),       3   jax.pmap(lambda params: jax.lax.pmean(params, ""i""), axis_name=""i"")( > 4       jax.tree_map(lambda x: x.astype(jnp.float16), state.params)       5   )       6 ) /usr/local/lib/python3.7/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options)     512    we use a separate function call to ensure that XLA compilation appears     513    separately in Python profiling results > 514   return backend.compile(built_c, compile_options=options)     515      516  TODO(phawkins): update users. RuntimeError: INTERNAL: during context [postoptimization]: Bitcast cannot have different shape sizes of output (8192) and operand (6144) (u32[1536]{0:T(1024)}) (f16[3072]{0:T(1024)(128)(2,1)}) ```",2022-01-10T08:46:27Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/9151,"IT WORKS!!!!! ...Here is the change that I made. ``` jax.tree_map(   lambda arr: jnp.any(jnp.isnan(arr)),   could be any op, downcasting in pmean triggers the error    jax.pmap(lambda params: jax.lax.pmean(params, ""i""), axis_name=""i"")(     jax.tree_map(lambda x: x.astype(jnp.bfloat16), params)   < Bfloat. BFLOAT.   ) ) ``` Thanks for being my rubber ducks, beautiful people."
314,"以下是一个github上的jax下的一个issue, 标题是(Re-applying #9136 after it was rolled back.)， 内容是 (Reapplying CC(simpler jaxpr eqn params to bind params conversion) after it was rolled back.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Re-applying #9136 after it was rolled back.,Reapplying CC(simpler jaxpr eqn params to bind params conversion) after it was rolled back.,2022-01-08T20:11:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9140
973,"以下是一个github上的jax下的一个issue, 标题是(simplify custom_jvp_call_p, remove custom_jvp_call_jaxpr_p)， 内容是 (Remove custom_jvp_call_jaxpr_p and its transformation rules. They were superfluous! Instead use the new mechanism for converting from jaxpr params to bind params (in CC(simpler jaxpr eqn params to bind params conversion)). ~This PR currently includes the commit from CC(simpler jaxpr eqn params to bind params conversion), but it should be considered as a ""diffbase"".~ The simplification in JaxprTrace.process_custom_jvp_call was actually made possible by omnistaging CC(omnistaging), though we didn't apply it until now. In a followup PR we'll delete custom_vjp_call_jaxpr_p too (or die trying). I'd like to land this one fully first to make sure this approach works (and hence ensure doing the same to custom_vjp_call_p makes sense).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"simplify custom_jvp_call_p, remove custom_jvp_call_jaxpr_p","Remove custom_jvp_call_jaxpr_p and its transformation rules. They were superfluous! Instead use the new mechanism for converting from jaxpr params to bind params (in CC(simpler jaxpr eqn params to bind params conversion)). ~This PR currently includes the commit from CC(simpler jaxpr eqn params to bind params conversion), but it should be considered as a ""diffbase"".~ The simplification in JaxprTrace.process_custom_jvp_call was actually made possible by omnistaging CC(omnistaging), though we didn't apply it until now. In a followup PR we'll delete custom_vjp_call_jaxpr_p too (or die trying). I'd like to land this one fully first to make sure this approach works (and hence ensure doing the same to custom_vjp_call_p makes sense).",2022-01-08T04:24:02Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9137
1918,"以下是一个github上的jax下的一个issue, 标题是(simpler jaxpr eqn params to bind params conversion)， 内容是 (Finalstyle higherorder primitives, like call_p, xla_call_p (underlying jit), xla_pmap_p (underlying pmap), and xmap_p (underlying xmap) have slightly different bind signatures (while   tracing) from their signatures when they appear in jaxprs. In particular, their tracetime binds are parameterized by a Python callable (or really a lu.WrappedFun) representing the       function to be applied, while in jaxpr eqns they are parameterized by a jaxpr representing the same. As a result, to roundtrip from jaxpr to Python traceable, in core.eval_jaxpr we have to convert from one parameter signature to the other. (Basically we had to take the jaxpr and turn   it into a Python callable, via lu.wrap_init(partial(core.eval_jaxpr, call_jaxpr, ...)).) However due to historical path dependence these conversion mechanisms were all slightly distinct and kind of a mess. There was a case analysis for call_jaxpr and map_jaxpr in core.       eval_jaxpr_eqn (a helper function created only because of this complexity), and there was a separate table only used for the xmap rule. In this PR we uniformized things! We basically only have a table (to simplify core.eval_jaxpr), but instead of having it as a table we just attached the rules to the different primitive  classes (CallPrimitive, MapPrimitive, and XmapPrimitive) to make things less errorprone (we have a few different CallPrimitive instantiations, like call_p, xla_call_p, named_call_p, and remat_call_p, and this way we don't have to remember to populate the table separately for each). This was actually a warmup simplification before we attempt to simplify custom derivatives (to unify custom_jvp_call_p and custom_jvp_call_jaxpr_p).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,simpler jaxpr eqn params to bind params conversion,"Finalstyle higherorder primitives, like call_p, xla_call_p (underlying jit), xla_pmap_p (underlying pmap), and xmap_p (underlying xmap) have slightly different bind signatures (while   tracing) from their signatures when they appear in jaxprs. In particular, their tracetime binds are parameterized by a Python callable (or really a lu.WrappedFun) representing the       function to be applied, while in jaxpr eqns they are parameterized by a jaxpr representing the same. As a result, to roundtrip from jaxpr to Python traceable, in core.eval_jaxpr we have to convert from one parameter signature to the other. (Basically we had to take the jaxpr and turn   it into a Python callable, via lu.wrap_init(partial(core.eval_jaxpr, call_jaxpr, ...)).) However due to historical path dependence these conversion mechanisms were all slightly distinct and kind of a mess. There was a case analysis for call_jaxpr and map_jaxpr in core.       eval_jaxpr_eqn (a helper function created only because of this complexity), and there was a separate table only used for the xmap rule. In this PR we uniformized things! We basically only have a table (to simplify core.eval_jaxpr), but instead of having it as a table we just attached the rules to the different primitive  classes (CallPrimitive, MapPrimitive, and XmapPrimitive) to make things less errorprone (we have a few different CallPrimitive instantiations, like call_p, xla_call_p, named_call_p, and remat_call_p, and this way we don't have to remember to populate the table separately for each). This was actually a warmup simplification before we attempt to simplify custom derivatives (to unify custom_jvp_call_p and custom_jvp_call_jaxpr_p).",2022-01-08T01:30:15Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9136
1041,"以下是一个github上的jax下的一个issue, 标题是(Enable JAX->MLIR lowering by default.)， 内容是 (Enable JAX>MLIR lowering by default. Before this change, JAX produces HLO using the XLA:Python builder APIs. After this change JAX produces MHLO using MLIR:Python APIs, and converts the MHLO to HLO for compilation with XLA. This is a lateral shift that should have little immediate impact, but unlocks a number of interesting opportunities in the future (e.g., mixing MLIR dialects within a JAX program). [XLA:Python] Pass MLIR input as a std::string to work around https://github.com/pybind/pybind11/issues/2765. A better fix would be to update pybind11 but that is hitting Windowsrelated hurdles; for now, just avoid relying on reference lifetime extension. Brax: update test seeds to avoid test failures. Additional constant folding (canonicalization) in the MHLO lowering path seems to cause small numerical differences.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Enable JAX->MLIR lowering by default.,"Enable JAX>MLIR lowering by default. Before this change, JAX produces HLO using the XLA:Python builder APIs. After this change JAX produces MHLO using MLIR:Python APIs, and converts the MHLO to HLO for compilation with XLA. This is a lateral shift that should have little immediate impact, but unlocks a number of interesting opportunities in the future (e.g., mixing MLIR dialects within a JAX program). [XLA:Python] Pass MLIR input as a std::string to work around https://github.com/pybind/pybind11/issues/2765. A better fix would be to update pybind11 but that is hitting Windowsrelated hurdles; for now, just avoid relying on reference lifetime extension. Brax: update test seeds to avoid test failures. Additional constant folding (canonicalization) in the MHLO lowering path seems to cause small numerical differences.",2022-01-07T20:39:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/9131
1537,"以下是一个github上的jax下的一个issue, 标题是(clean up WrappedFun.call_wrapped refs on exception)， 内容是 (Functions decorated by linear_util.transformation or transformation_with_aux are coroutines (with two yields). They can raise exceptions, either before or after they yield the first time. linear_util.WrappedFun.call_wrapped, which is responsible for driving these coroutines, holds references to them. These coroutines often manipulate global trace state (i.e. core.thread_local_state.trace_state attributes) through context managers (e.g. core.new_main or core.extend_axis_env). These context managers use try/finally to clean up their state changes. When an exception is raised in a linear_util.transformation coroutine, it is raised into call_wrapped. If call_wrapped doesn't then clean up all the references it has to coroutines, the cleanup finally clauses may not execute until too late. To ensure the finally clauses are called at the right time (before exiting call_wrapped, basically as soon as possible) we need to clean up the references to the coroutines in call_wrapped. We had cleaned up these coroutine references when the coroutines raised exceptions in their first part (i.e. before their first yield) in CC(Interrupt lu transformation generators whenever an exception occurs). But we didn't do a similar cleanup for their second part (i.e. after their first yield and before their second).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,clean up WrappedFun.call_wrapped refs on exception,"Functions decorated by linear_util.transformation or transformation_with_aux are coroutines (with two yields). They can raise exceptions, either before or after they yield the first time. linear_util.WrappedFun.call_wrapped, which is responsible for driving these coroutines, holds references to them. These coroutines often manipulate global trace state (i.e. core.thread_local_state.trace_state attributes) through context managers (e.g. core.new_main or core.extend_axis_env). These context managers use try/finally to clean up their state changes. When an exception is raised in a linear_util.transformation coroutine, it is raised into call_wrapped. If call_wrapped doesn't then clean up all the references it has to coroutines, the cleanup finally clauses may not execute until too late. To ensure the finally clauses are called at the right time (before exiting call_wrapped, basically as soon as possible) we need to clean up the references to the coroutines in call_wrapped. We had cleaned up these coroutine references when the coroutines raised exceptions in their first part (i.e. before their first yield) in CC(Interrupt lu transformation generators whenever an exception occurs). But we didn't do a similar cleanup for their second part (i.e. after their first yield and before their second).",2022-01-06T00:11:43Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/9109
16064,"以下是一个github上的jax下的一个issue, 标题是(Getting NaNs when trying to optimize two parameters when a large number of points is involved.)， 内容是 (Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this:  [x] If applicable, include full error messages/tracebacks. I am trying to optimize two parameters for linear regression. This is happening when a large number of points are involved. I made a loop, but it failed silently. When I tried to see the updated parameters, I saw two `nan`s. ```python import jax.numpy as jnp import jax import jaxlib x_ = jnp.arange(2e7) y_ = jnp.arange(2e7, 4e7) theta = jnp.array([1., 1]) .jit def model(theta: jaxlib.xla_extension.DeviceArray, x: jaxlib.xla_extension.DeviceArray) > jaxlib.xla_extension.DeviceArray:     w, b = theta     return w * x + b .jit def loss_fn(theta: jaxlib.xla_extension.DeviceArray,             x: jaxlib.xla_extension.DeviceArray,             y: jaxlib.xla_extension.DeviceArray) > jaxlib.xla_extension.DeviceArray:     '''     Calculates the MSE loss.     '''     prediction = model(theta, x)             y_hat     return jnp.mean((prediction  y) ** 2)   J .jit def update(theta: jaxlib.xla_extension.DeviceArray,           x: jaxlib.xla_extension.DeviceArray,           y: jaxlib.xla_extension.DeviceArray,           lr: float = 0.1) > jaxlib.xla_extension.DeviceArray:     '''     updates the parameters based on gradient descent     '''     return theta  lr * jax.grad(loss_fn)(theta, x, y) for _ in range(1000):     theta = update(theta, x_, y_) ``` This failed silently. Then I followed the docs and enables `nan`parsing by ```ipython !JAX_DEBUG_NANS=True ``` And then I did ```python from jax.config import config config.update(""jax_debug_nans"", True) ``` Which threw me a error. After the error was thrown, I inspected the `theta` variable, and saw that it had very small values, i.e. the values before they were going `nan`. It was this: ```python >>> theta DeviceArray([1.6214236e+37, 7.4782789e+35], dtype=float32) ``` What is going wrong? Am I missing something? Note that I also tried initially with small values for the arrays. All values in array were temperature values, and the data is good (no outliers or wrong entry that might throw the parameters off). Full error message: ```bash Invalid nan value encountered in the output of a C++jit function. Calling the deoptimized version. Invalid value encountered in the output of a jit function. Calling the deoptimized version. Invalid value encountered in the output of a jit function. Calling the deoptimized version. Invalid value encountered in the output of a jit function. Calling the deoptimized version. Invalid value encountered in the output of a jit function. Calling the deoptimized version.  FloatingPointError                        Traceback (most recent call last) /opt/conda/lib/python3.7/sitepackages/jax/_src/api.py in _nan_check_posthook(fun, args, kwargs, output)     120         da.device_buffer > 121         for da in tree_leaves(output)     122         if hasattr(da, ""device_buffer"") /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: FloatingPointError                        Traceback (most recent call last)     [... skipping hidden 1 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _execute_compiled(compiled, avals, handlers, kept_var_idx, *args)     912   out_bufs = compiled.execute(input_bufs) > 913   check_special(xla_call_p.name, out_bufs)     914   return [handler(*bs) for handler, bs in zip(handlers, _partition_outputs(avals, out_bufs))] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: FloatingPointError                        Traceback (most recent call last)     [... skipping hidden 1 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _execute_compiled(compiled, avals, handlers, kept_var_idx, *args)     912   out_bufs = compiled.execute(input_bufs) > 913   check_special(xla_call_p.name, out_bufs)     914   return [handler(*bs) for handler, bs in zip(handlers, _partition_outputs(avals, out_bufs))] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: FloatingPointError                        Traceback (most recent call last)     [... skipping hidden 1 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _execute_compiled(compiled, avals, handlers, kept_var_idx, *args)     912   out_bufs = compiled.execute(input_bufs) > 913   check_special(xla_call_p.name, out_bufs)     914   return [handler(*bs) for handler, bs in zip(handlers, _partition_outputs(avals, out_bufs))] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: FloatingPointError                        Traceback (most recent call last)     [... skipping hidden 1 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _execute_compiled(compiled, avals, handlers, kept_var_idx, *args)     912   out_bufs = compiled.execute(input_bufs) > 913   check_special(xla_call_p.name, out_bufs)     914   return [handler(*bs) for handler, bs in zip(handlers, _partition_outputs(avals, out_bufs))] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: JaxStackTraceBeforeTransformation         Traceback (most recent call last) /opt/conda/lib/python3.7/runpy.py in _run_module_as_main(***failed resolving arguments***)     192     return _run_code(code, main_globals, None, > 193                      ""__main__"", mod_spec)     194  /opt/conda/lib/python3.7/runpy.py in _run_code(***failed resolving arguments***)      84                        __spec__ = mod_spec) > 85     exec(code, run_globals)      86     return run_globals /opt/conda/lib/python3.7/sitepackages/ipykernel_launcher.py in       15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() /opt/conda/lib/python3.7/sitepackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     844         app.initialize(argv) > 845         app.start()     846  /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     666             try: > 667                 self.io_loop.start()     668             except KeyboardInterrupt: /opt/conda/lib/python3.7/sitepackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     198             asyncio.set_event_loop(self.asyncio_loop) > 199             self.asyncio_loop.run_forever()     200         finally: /opt/conda/lib/python3.7/asyncio/base_events.py in run_forever(***failed resolving arguments***)     540             while True: > 541                 self._run_once()     542                 if self._stopping: /opt/conda/lib/python3.7/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1785             else: > 1786                 handle._run()    1787         handle = None   Needed to break cycles when an exception occurs. /opt/conda/lib/python3.7/asyncio/events.py in _run(***failed resolving arguments***)      87         try: > 88             self._context.run(self._callback, *self._args)      89         except Exception as exc: /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelbase.py in dispatch_queue(***failed resolving arguments***)     456             try: > 457                 await self.process_one()     458             except Exception: /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelbase.py in process_one(***failed resolving arguments***)     445                 return None > 446         await dispatch(*args)     447  /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     352                 if inspect.isawaitable(result): > 353                     await result     354             except Exception: /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     647         if inspect.isawaitable(reply_content): > 648             reply_content = await reply_content     649  /opt/conda/lib/python3.7/sitepackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     344                  letting shell dispatch to loop runners > 345                 res = shell.run_cell(code, store_history=store_history, silent=silent)     346         finally: /opt/conda/lib/python3.7/sitepackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     531         self._last_traceback = None > 532         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     533  /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2898             result = self._run_cell( > 2899                 raw_cell, store_history, silent, shell_futures)    2900         finally: /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in _run_cell(***failed resolving arguments***)    2943         try: > 2944             return runner(coro)    2945         except BaseException as e: /opt/conda/lib/python3.7/sitepackages/IPython/core/async_helpers.py in _pseudo_sync_runner(***failed resolving arguments***)      67     try: > 68         coro.send(None)      69     except StopIteration as exc: /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in run_cell_async(***failed resolving arguments***)    3169                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name, > 3170                        interactivity=interactivity, compiler=compiler, result=result)    3171  /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    3360                         asy = compare(code) > 3361                     if (await self.run_code(code, result,  async_=asy)):    3362                         return True /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    3440                 else: > 3441                     exec(code_obj, self.user_global_ns, self.user_ns)    3442             finally: /tmp/ipykernel_43/2138327653.py in        1 for _ in range(1000): > 2     theta = update(theta, x_train, y_train)       3  /tmp/ipykernel_43/1444374627.py in update(***failed resolving arguments***)       8     ''' > 9     return theta  lr * jax.grad(loss_fn)(theta, x, y) /tmp/ipykernel_43/3487162561.py in loss_fn(***failed resolving arguments***)       7     ''' > 8     prediction = model(theta, x)             y_hat       9     return jnp.mean((prediction  y) ** 2)   J /tmp/ipykernel_43/3906416787.py in model(***failed resolving arguments***)       3     w, b = theta > 4     return w * x + b /opt/conda/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py in deferring_binary_op(***failed resolving arguments***)    5923       return NotImplemented > 5924     return binary_op(self, other)    5925   return deferring_binary_op /opt/conda/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py in fn(***failed resolving arguments***)     435     x1, x2 = _promote_args(numpy_fn.__name__, x1, x2) > 436     return lax_fn(x1, x2) if x1.dtype != bool_ else bool_lax_fn(x1, x2)     437   fn = jit(fn, inline=True) JaxStackTraceBeforeTransformation: FloatingPointError: invalid value (nan) encountered in reduce_sum The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: FloatingPointError                        Traceback (most recent call last) /tmp/ipykernel_43/2138327653.py in        1 for _ in range(1000): > 2     theta = update(theta, x_train, y_train)       3        4 plt.scatter(x_train, y_train)       5 plt.plot(x_train, model(theta, x_train), 'r') /opt/conda/lib/python3.7/sitepackages/jax/_src/api.py in _nan_check_posthook(fun, args, kwargs, output)     127     print(""Invalid nan value encountered in the output of a C++jit ""     128           ""function. Calling the deoptimized version."") > 129     fun._cache_miss(*args, **kwargs)[0]   probably won't return     130      131 def _update_debug_special_global(_):     [... skipping hidden 8 frame] /tmp/ipykernel_43/1444374627.py in update(theta, x, y, lr)       7     updates the parameters based on gradient descent       8     ''' > 9     return theta  lr * jax.grad(loss_fn)(theta, x, y)     [... skipping hidden 39 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     414   if dtypes.issubdtype(xla_shape.element_type(), np.inexact):     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())):     418       raise FloatingPointError(f""invalid value (inf) encountered in {name}"") FloatingPointError: invalid value (nan) encountered in reduce_sum ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Getting NaNs when trying to optimize two parameters when a large number of points is involved.,"Please:  [x] Check for duplicate issues.  [x] Provide a complete example of how to reproduce the bug, wrapped in triple backticks like this:  [x] If applicable, include full error messages/tracebacks. I am trying to optimize two parameters for linear regression. This is happening when a large number of points are involved. I made a loop, but it failed silently. When I tried to see the updated parameters, I saw two `nan`s. ```python import jax.numpy as jnp import jax import jaxlib x_ = jnp.arange(2e7) y_ = jnp.arange(2e7, 4e7) theta = jnp.array([1., 1]) .jit def model(theta: jaxlib.xla_extension.DeviceArray, x: jaxlib.xla_extension.DeviceArray) > jaxlib.xla_extension.DeviceArray:     w, b = theta     return w * x + b .jit def loss_fn(theta: jaxlib.xla_extension.DeviceArray,             x: jaxlib.xla_extension.DeviceArray,             y: jaxlib.xla_extension.DeviceArray) > jaxlib.xla_extension.DeviceArray:     '''     Calculates the MSE loss.     '''     prediction = model(theta, x)             y_hat     return jnp.mean((prediction  y) ** 2)   J .jit def update(theta: jaxlib.xla_extension.DeviceArray,           x: jaxlib.xla_extension.DeviceArray,           y: jaxlib.xla_extension.DeviceArray,           lr: float = 0.1) > jaxlib.xla_extension.DeviceArray:     '''     updates the parameters based on gradient descent     '''     return theta  lr * jax.grad(loss_fn)(theta, x, y) for _ in range(1000):     theta = update(theta, x_, y_) ``` This failed silently. Then I followed the docs and enables `nan`parsing by ```ipython !JAX_DEBUG_NANS=True ``` And then I did ```python from jax.config import config config.update(""jax_debug_nans"", True) ``` Which threw me a error. After the error was thrown, I inspected the `theta` variable, and saw that it had very small values, i.e. the values before they were going `nan`. It was this: ```python >>> theta DeviceArray([1.6214236e+37, 7.4782789e+35], dtype=float32) ``` What is going wrong? Am I missing something? Note that I also tried initially with small values for the arrays. All values in array were temperature values, and the data is good (no outliers or wrong entry that might throw the parameters off). Full error message: ```bash Invalid nan value encountered in the output of a C++jit function. Calling the deoptimized version. Invalid value encountered in the output of a jit function. Calling the deoptimized version. Invalid value encountered in the output of a jit function. Calling the deoptimized version. Invalid value encountered in the output of a jit function. Calling the deoptimized version. Invalid value encountered in the output of a jit function. Calling the deoptimized version.  FloatingPointError                        Traceback (most recent call last) /opt/conda/lib/python3.7/sitepackages/jax/_src/api.py in _nan_check_posthook(fun, args, kwargs, output)     120         da.device_buffer > 121         for da in tree_leaves(output)     122         if hasattr(da, ""device_buffer"") /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: FloatingPointError                        Traceback (most recent call last)     [... skipping hidden 1 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _execute_compiled(compiled, avals, handlers, kept_var_idx, *args)     912   out_bufs = compiled.execute(input_bufs) > 913   check_special(xla_call_p.name, out_bufs)     914   return [handler(*bs) for handler, bs in zip(handlers, _partition_outputs(avals, out_bufs))] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: FloatingPointError                        Traceback (most recent call last)     [... skipping hidden 1 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _execute_compiled(compiled, avals, handlers, kept_var_idx, *args)     912   out_bufs = compiled.execute(input_bufs) > 913   check_special(xla_call_p.name, out_bufs)     914   return [handler(*bs) for handler, bs in zip(handlers, _partition_outputs(avals, out_bufs))] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: FloatingPointError                        Traceback (most recent call last)     [... skipping hidden 1 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _execute_compiled(compiled, avals, handlers, kept_var_idx, *args)     912   out_bufs = compiled.execute(input_bufs) > 913   check_special(xla_call_p.name, out_bufs)     914   return [handler(*bs) for handler, bs in zip(handlers, _partition_outputs(avals, out_bufs))] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: FloatingPointError                        Traceback (most recent call last)     [... skipping hidden 1 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _execute_compiled(compiled, avals, handlers, kept_var_idx, *args)     912   out_bufs = compiled.execute(input_bufs) > 913   check_special(xla_call_p.name, out_bufs)     914   return [handler(*bs) for handler, bs in zip(handlers, _partition_outputs(avals, out_bufs))] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in check_special(name, bufs)     409     for buf in bufs: > 410       _check_special(name, buf.xla_shape(), buf)     411  /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())): FloatingPointError: invalid value (nan) encountered in xla_call During handling of the above exception, another exception occurred: JaxStackTraceBeforeTransformation         Traceback (most recent call last) /opt/conda/lib/python3.7/runpy.py in _run_module_as_main(***failed resolving arguments***)     192     return _run_code(code, main_globals, None, > 193                      ""__main__"", mod_spec)     194  /opt/conda/lib/python3.7/runpy.py in _run_code(***failed resolving arguments***)      84                        __spec__ = mod_spec) > 85     exec(code, run_globals)      86     return run_globals /opt/conda/lib/python3.7/sitepackages/ipykernel_launcher.py in       15     from ipykernel import kernelapp as app > 16     app.launch_new_instance() /opt/conda/lib/python3.7/sitepackages/traitlets/config/application.py in launch_instance(***failed resolving arguments***)     844         app.initialize(argv) > 845         app.start()     846  /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelapp.py in start(***failed resolving arguments***)     666             try: > 667                 self.io_loop.start()     668             except KeyboardInterrupt: /opt/conda/lib/python3.7/sitepackages/tornado/platform/asyncio.py in start(***failed resolving arguments***)     198             asyncio.set_event_loop(self.asyncio_loop) > 199             self.asyncio_loop.run_forever()     200         finally: /opt/conda/lib/python3.7/asyncio/base_events.py in run_forever(***failed resolving arguments***)     540             while True: > 541                 self._run_once()     542                 if self._stopping: /opt/conda/lib/python3.7/asyncio/base_events.py in _run_once(***failed resolving arguments***)    1785             else: > 1786                 handle._run()    1787         handle = None   Needed to break cycles when an exception occurs. /opt/conda/lib/python3.7/asyncio/events.py in _run(***failed resolving arguments***)      87         try: > 88             self._context.run(self._callback, *self._args)      89         except Exception as exc: /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelbase.py in dispatch_queue(***failed resolving arguments***)     456             try: > 457                 await self.process_one()     458             except Exception: /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelbase.py in process_one(***failed resolving arguments***)     445                 return None > 446         await dispatch(*args)     447  /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelbase.py in dispatch_shell(***failed resolving arguments***)     352                 if inspect.isawaitable(result): > 353                     await result     354             except Exception: /opt/conda/lib/python3.7/sitepackages/ipykernel/kernelbase.py in execute_request(***failed resolving arguments***)     647         if inspect.isawaitable(reply_content): > 648             reply_content = await reply_content     649  /opt/conda/lib/python3.7/sitepackages/ipykernel/ipkernel.py in do_execute(***failed resolving arguments***)     344                  letting shell dispatch to loop runners > 345                 res = shell.run_cell(code, store_history=store_history, silent=silent)     346         finally: /opt/conda/lib/python3.7/sitepackages/ipykernel/zmqshell.py in run_cell(***failed resolving arguments***)     531         self._last_traceback = None > 532         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)     533  /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in run_cell(***failed resolving arguments***)    2898             result = self._run_cell( > 2899                 raw_cell, store_history, silent, shell_futures)    2900         finally: /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in _run_cell(***failed resolving arguments***)    2943         try: > 2944             return runner(coro)    2945         except BaseException as e: /opt/conda/lib/python3.7/sitepackages/IPython/core/async_helpers.py in _pseudo_sync_runner(***failed resolving arguments***)      67     try: > 68         coro.send(None)      69     except StopIteration as exc: /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in run_cell_async(***failed resolving arguments***)    3169                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name, > 3170                        interactivity=interactivity, compiler=compiler, result=result)    3171  /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in run_ast_nodes(***failed resolving arguments***)    3360                         asy = compare(code) > 3361                     if (await self.run_code(code, result,  async_=asy)):    3362                         return True /opt/conda/lib/python3.7/sitepackages/IPython/core/interactiveshell.py in run_code(***failed resolving arguments***)    3440                 else: > 3441                     exec(code_obj, self.user_global_ns, self.user_ns)    3442             finally: /tmp/ipykernel_43/2138327653.py in        1 for _ in range(1000): > 2     theta = update(theta, x_train, y_train)       3  /tmp/ipykernel_43/1444374627.py in update(***failed resolving arguments***)       8     ''' > 9     return theta  lr * jax.grad(loss_fn)(theta, x, y) /tmp/ipykernel_43/3487162561.py in loss_fn(***failed resolving arguments***)       7     ''' > 8     prediction = model(theta, x)             y_hat       9     return jnp.mean((prediction  y) ** 2)   J /tmp/ipykernel_43/3906416787.py in model(***failed resolving arguments***)       3     w, b = theta > 4     return w * x + b /opt/conda/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py in deferring_binary_op(***failed resolving arguments***)    5923       return NotImplemented > 5924     return binary_op(self, other)    5925   return deferring_binary_op /opt/conda/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py in fn(***failed resolving arguments***)     435     x1, x2 = _promote_args(numpy_fn.__name__, x1, x2) > 436     return lax_fn(x1, x2) if x1.dtype != bool_ else bool_lax_fn(x1, x2)     437   fn = jit(fn, inline=True) JaxStackTraceBeforeTransformation: FloatingPointError: invalid value (nan) encountered in reduce_sum The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: FloatingPointError                        Traceback (most recent call last) /tmp/ipykernel_43/2138327653.py in        1 for _ in range(1000): > 2     theta = update(theta, x_train, y_train)       3        4 plt.scatter(x_train, y_train)       5 plt.plot(x_train, model(theta, x_train), 'r') /opt/conda/lib/python3.7/sitepackages/jax/_src/api.py in _nan_check_posthook(fun, args, kwargs, output)     127     print(""Invalid nan value encountered in the output of a C++jit ""     128           ""function. Calling the deoptimized version."") > 129     fun._cache_miss(*args, **kwargs)[0]   probably won't return     130      131 def _update_debug_special_global(_):     [... skipping hidden 8 frame] /tmp/ipykernel_43/1444374627.py in update(theta, x, y, lr)       7     updates the parameters based on gradient descent       8     ''' > 9     return theta  lr * jax.grad(loss_fn)(theta, x, y)     [... skipping hidden 39 frame] /opt/conda/lib/python3.7/sitepackages/jax/interpreters/xla.py in _check_special(name, xla_shape, buf)     414   if dtypes.issubdtype(xla_shape.element_type(), np.inexact):     415     if config.jax_debug_nans and np.any(np.isnan(buf.to_py())): > 416       raise FloatingPointError(f""invalid value (nan) encountered in {name}"")     417     if config.jax_debug_infs and np.any(np.isinf(buf.to_py())):     418       raise FloatingPointError(f""invalid value (inf) encountered in {name}"") FloatingPointError: invalid value (nan) encountered in reduce_sum ```",2022-01-04T19:12:37Z,bug,closed,1,2,https://github.com/jax-ml/jax/issues/9092,"It looks like the issue is that your learning rate is too large, which is causing your estimates of `theta` to oscillate with very large amplitude around the optimal value. You can see this by printing the successive estimates: ```python for _ in range(5):   print(theta)   theta = update(theta, x_, y_) ``` Output: ``` [1. 1.] [1.3333331e+13 2.0000010e+06] [3.5555549e+26 2.6666664e+19] [          inf 7.1111106e+32] [nan nan] ``` You'll need to better tune your learning rate to prevent this kind of parameter oscillation.",This works. I switched to `lr=0.001` and that worked. I did not get `nan`s.
7345,"以下是一个github上的jax下的一个issue, 标题是(Cannot do ""nan_to_num"" in customized JVP functions)， 内容是 (We were trying to remove NAN in a customized JVP function but hit some issues. Please see below for a (overly) simplified example. Not sure if it's a feature or bug. If the behavior is as expected, please help provide some guidance on how to remove or mask out NAN (as well as INF) values in a customized JVP function. Thanks! Jax version: 0.2.26 Jaxlib version: 0.1.75 ```python import jax import jax.numpy as jnp .custom_jvp def func(x):     return jnp.sum(x) .defjvp def func_jvp(primals, tangents):     tangent, = tangents     tangent = jnp.nan_to_num(tangent)     return func(*primals), func(tangent) val_and_grad = jax.value_and_grad(func) val_and_grad(jnp.ones(3)) ``` Stack trace: ``` Traceback (most recent call last):   File ""bin/reproduce_simple_case.py"", line 16, in      val_and_grad(jnp.ones(3))   File ""bin/reproduce_simple_case.py"", line 11, in func_jvp     tangent = jnp.nan_to_num(tangent)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2455, in nan_to_num     x = where(isneginf(x), array(neginf, dtype=x.dtype), x)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2170, in where     return _where(condition, x, y)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2149, in _where     return lax.select(condition, x, y) if not core.is_empty_shape(np.shape(x)) else x jax._src.source_info_util.JaxStackTraceBeforeTransformation: AssertionError The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""bin/reproduce_simple_case.py"", line 16, in      val_and_grad(jnp.ones(3))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/api.py"", line 1064, in value_and_grad_f     g = vjp_py(jax.lax._one(ans))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/tree_util.py"", line 326, in      func = lambda *args, **kw: original_func(*args, **kw)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/api.py"", line 2373, in _vjp_pullback_wrapper     ans = fun(*args)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/tree_util.py"", line 326, in      func = lambda *args, **kw: original_func(*args, **kw)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 123, in unbound_vjp     arg_cts = backward_pass(jaxpr, reduce_axes, consts, dummy_args, cts)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 223, in backward_pass     params, call_jaxpr, invals, cts_in, cts_in_avals, reduce_axes)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 558, in call_transpose     out_flat = primitive.bind(fun, *all_args, **new_params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1661, in bind     return call_bind(self, fun, *args, **params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1652, in call_bind     outs = primitive.process(top_trace, fun, tracers, params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1664, in process     return trace.process_call(self, fun, tracers, params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 633, in process_call     return primitive.impl(f, *tracers, **params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 129, in _xla_call_impl     *unsafe_map(arg_spec, args))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/linear_util.py"", line 263, in memoized_fun     ans = call(fun, *args)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 156, in _xla_callable_uncached     *arg_specs).compile().unsafe_call   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/profiler.py"", line 206, in wrapper     return func(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 170, in lower_xla_callable     fun, abstract_args, pe.debug_info_final(fun, ""jit""))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/profiler.py"", line 206, in wrapper     return func(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/partial_eval.py"", line 1566, in trace_to_jaxpr_final     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(fun, main, in_avals)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/partial_eval.py"", line 1543, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/linear_util.py"", line 166, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 223, in backward_pass     params, call_jaxpr, invals, cts_in, cts_in_avals, reduce_axes)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 558, in call_transpose     out_flat = primitive.bind(fun, *all_args, **new_params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1661, in bind     return call_bind(self, fun, *args, **params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1652, in call_bind     outs = primitive.process(top_trace, fun, tracers, params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1664, in process     return trace.process_call(self, fun, tracers, params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/partial_eval.py"", line 1352, in process_call     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(f, self.main, in_avals)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/partial_eval.py"", line 1543, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/linear_util.py"", line 166, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 229, in backward_pass     **eqn.params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/lax/lax.py"", line 3104, in _select_transpose_rule     assert not ad.is_undefined_primal(pred) jax._src.traceback_util.UnfilteredStackTrace: AssertionError The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""bin/reproduce_simple_case.py"", line 16, in      val_and_grad(jnp.ones(3))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/lax/lax.py"", line 3104, in _select_transpose_rule     assert not ad.is_undefined_primal(pred) AssertionError ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Cannot do ""nan_to_num"" in customized JVP functions","We were trying to remove NAN in a customized JVP function but hit some issues. Please see below for a (overly) simplified example. Not sure if it's a feature or bug. If the behavior is as expected, please help provide some guidance on how to remove or mask out NAN (as well as INF) values in a customized JVP function. Thanks! Jax version: 0.2.26 Jaxlib version: 0.1.75 ```python import jax import jax.numpy as jnp .custom_jvp def func(x):     return jnp.sum(x) .defjvp def func_jvp(primals, tangents):     tangent, = tangents     tangent = jnp.nan_to_num(tangent)     return func(*primals), func(tangent) val_and_grad = jax.value_and_grad(func) val_and_grad(jnp.ones(3)) ``` Stack trace: ``` Traceback (most recent call last):   File ""bin/reproduce_simple_case.py"", line 16, in      val_and_grad(jnp.ones(3))   File ""bin/reproduce_simple_case.py"", line 11, in func_jvp     tangent = jnp.nan_to_num(tangent)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2455, in nan_to_num     x = where(isneginf(x), array(neginf, dtype=x.dtype), x)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2170, in where     return _where(condition, x, y)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2149, in _where     return lax.select(condition, x, y) if not core.is_empty_shape(np.shape(x)) else x jax._src.source_info_util.JaxStackTraceBeforeTransformation: AssertionError The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""bin/reproduce_simple_case.py"", line 16, in      val_and_grad(jnp.ones(3))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/api.py"", line 1064, in value_and_grad_f     g = vjp_py(jax.lax._one(ans))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/tree_util.py"", line 326, in      func = lambda *args, **kw: original_func(*args, **kw)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/api.py"", line 2373, in _vjp_pullback_wrapper     ans = fun(*args)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/tree_util.py"", line 326, in      func = lambda *args, **kw: original_func(*args, **kw)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 123, in unbound_vjp     arg_cts = backward_pass(jaxpr, reduce_axes, consts, dummy_args, cts)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 223, in backward_pass     params, call_jaxpr, invals, cts_in, cts_in_avals, reduce_axes)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 558, in call_transpose     out_flat = primitive.bind(fun, *all_args, **new_params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1661, in bind     return call_bind(self, fun, *args, **params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1652, in call_bind     outs = primitive.process(top_trace, fun, tracers, params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1664, in process     return trace.process_call(self, fun, tracers, params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 633, in process_call     return primitive.impl(f, *tracers, **params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 129, in _xla_call_impl     *unsafe_map(arg_spec, args))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/linear_util.py"", line 263, in memoized_fun     ans = call(fun, *args)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 156, in _xla_callable_uncached     *arg_specs).compile().unsafe_call   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/profiler.py"", line 206, in wrapper     return func(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/dispatch.py"", line 170, in lower_xla_callable     fun, abstract_args, pe.debug_info_final(fun, ""jit""))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/profiler.py"", line 206, in wrapper     return func(*args, **kwargs)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/partial_eval.py"", line 1566, in trace_to_jaxpr_final     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(fun, main, in_avals)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/partial_eval.py"", line 1543, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/linear_util.py"", line 166, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 223, in backward_pass     params, call_jaxpr, invals, cts_in, cts_in_avals, reduce_axes)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 558, in call_transpose     out_flat = primitive.bind(fun, *all_args, **new_params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1661, in bind     return call_bind(self, fun, *args, **params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1652, in call_bind     outs = primitive.process(top_trace, fun, tracers, params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/core.py"", line 1664, in process     return trace.process_call(self, fun, tracers, params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/partial_eval.py"", line 1352, in process_call     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(f, self.main, in_avals)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/partial_eval.py"", line 1543, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/linear_util.py"", line 166, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/interpreters/ad.py"", line 229, in backward_pass     **eqn.params)   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/lax/lax.py"", line 3104, in _select_transpose_rule     assert not ad.is_undefined_primal(pred) jax._src.traceback_util.UnfilteredStackTrace: AssertionError The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""bin/reproduce_simple_case.py"", line 16, in      val_and_grad(jnp.ones(3))   File ""/home/tiger/.local/lib/python3.7/sitepackages/jax/_src/lax/lax.py"", line 3104, in _select_transpose_rule     assert not ad.is_undefined_primal(pred) AssertionError ```",2022-01-04T12:13:43Z,question better_errors,closed,0,4,https://github.com/jax-ml/jax/issues/9087,"Sorry, some further investigation shows that it's ok to use `nan_to_num` in customized JVP functions. It's that we cannot use the `tangents` (the second argument of the customized JVP function) in `nan_to_num`. Even more general, `tangents` cannot show up in the condition part in the `jnp.where` functions.","Just realized doing `nan_to_num` on `tangents` may break the linearity required for doing transpose automatically (hinted by the function raising the exception, namely `_select_transpose_rule`). Not sure if it's the source of the issue though.  Let's say we are writing the customized JVP function for the loss function of our model, which would only be used in backpropagation. Does that mean we could write a customized VJP function, instead of JVP, to be used in BP, and we don't need to worry about the linearity and can do `nan_to_num` in customized VJP function in that case?","Thanks for the questions! You pretty much nailed it. Indeed it seems JAX considers `nan_to_num` to be nonlinear (because of the `where` as you say), and so using it on tangents makes the result nontransposable. (This is a pretty confusing error message though...) And yes, if you write a custom VJP then you're telling JAX how to perform the transposition, so automatic transposition is no longer necessary and this issue won't come up. Does using a custom VJP make sense for your use case?"," Thanks for your response and sorry for getting back so late. Yes, I implemented a custom VJP and it worked. I will close the issue here"
1172,"以下是一个github上的jax下的一个issue, 标题是(Converting Concrete Tracer values to regular float values doesnt work)， 内容是 (Hello, I am trying to convert concrete tracer values to regular float values using `astype(float)` but it still seems to return a concrete tracer value. However when I do `astype(int)` it seems to correctly convert the concrete tracer into an int.  Here is an example: ``` import jax from jax import numpy as jnp def func(mu):   print((mu.astype(float)))   print(mu)   print(mu.astype(int)) return mu f = jax.grad(func) print(f(jnp.array(5.0))) ``` This returns the following: ``` Tracedwith   with primal = Traced        tangent = Traced Tracedwith   with primal = Traced        tangent = Traced 5 1.0 ``` Clearly `print(mu)` and `print(mu.astype(float))` seem to return the exact same thing while `print(mu.astype(int))` returns the correct int value. Is this expected? This is a simple example but in my original function I would like to convert one of the input parameters into a regular float value. Thanks in advance!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Converting Concrete Tracer values to regular float values doesnt work,"Hello, I am trying to convert concrete tracer values to regular float values using `astype(float)` but it still seems to return a concrete tracer value. However when I do `astype(int)` it seems to correctly convert the concrete tracer into an int.  Here is an example: ``` import jax from jax import numpy as jnp def func(mu):   print((mu.astype(float)))   print(mu)   print(mu.astype(int)) return mu f = jax.grad(func) print(f(jnp.array(5.0))) ``` This returns the following: ``` Tracedwith   with primal = Traced        tangent = Traced Tracedwith   with primal = Traced        tangent = Traced 5 1.0 ``` Clearly `print(mu)` and `print(mu.astype(float))` seem to return the exact same thing while `print(mu.astype(int))` returns the correct int value. Is this expected? This is a simple example but in my original function I would like to convert one of the input parameters into a regular float value. Thanks in advance!",2022-01-04T08:29:17Z,,closed,0,4,https://github.com/jax-ml/jax/issues/9085,"Thanks for the question – if anything, I think the behavior of `mu.astype(int)` returning an array rather than a tracer looks to me like a bug. Can you say more about why you're interested in extracting values from concrete tracers? Questions like this often indicate that someone is trying to work around JAX's computational model in ways that can lead to unintended results.","Thanks for the quick response! So I want to use JAX to do autodifferentiation of the negative loglikelihood function which takes as input an array with the first entry of the array being the parameter `mu` and the second entry being the parameter `alpha`. Take for instance the following simple example: ``` def NLL(mu1):     mu = mu1[0]                                                                                  the first parameter mu     alpha = mu1[1]                                                                                 the second parameter alpha     alpha_lumi_kFact = t.obj(""alpha_ATLAS_LUMI"")                      initializing a ROOT object. Essential for the NLL computation     alpha_lumi_kFact.setVal(alpha)                                                  passing alpha value to the ROOT object     result = mu+alpha_lumi_kFact.getVal()                                     simplified result for illustration purpose       return result ``` Without going into too much technical detail, the `setVal` method of `alpha_lumi_kFact` object in the above function takes as input only regular float values. It may not seem apparent based on the simplified example, but it is essential for my original function to set the value of `alpha_lumi_kFact` to be equal to `mu1[1]` and then do differentiation. As it is, the following gradient computation: ``` f = jax.grad(NLL) print(f(jnp.array([1.0,0.5]))) ``` gives an error: ```  UnfilteredStackTrace                      Traceback (most recent call last)  in        1 f = jax.grad(NLL) > 2 print(f(jnp.array([1.0,0.0])))     [... skipping hidden 1 frame] ~/.local/lib/python3.6/sitepackages/jax/_src/api.py in grad_f(*args, **kwargs)     828   def grad_f(*args, **kwargs): > 829     _, g = value_and_grad_f(*args, **kwargs)     830     return g     [... skipping hidden 1 frame] ~/.local/lib/python3.6/sitepackages/jax/_src/api.py in value_and_grad_f(*args, **kwargs)     900     if not has_aux: > 901       ans, vjp_py = _vjp(f_partial, *dyn_args, reduce_axes=reduce_axes)     902     else: ~/.local/lib/python3.6/sitepackages/jax/_src/api.py in _vjp(fun, has_aux, reduce_axes, *primals)    1996     out_primal, out_vjp = ad.vjp( > 1997         flat_fun, primals_flat, reduce_axes=reduce_axes)    1998     out_tree = out_tree() ~/.local/lib/python3.6/sitepackages/jax/interpreters/ad.py in vjp(traceable, primals, has_aux, reduce_axes)     114   if not has_aux: > 115     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)     116   else: ~/.local/lib/python3.6/sitepackages/jax/interpreters/ad.py in linearize(traceable, *primals, **kwargs)     101   jvpfun_flat, out_tree = flatten_fun(jvpfun, in_tree) > 102   jaxpr, out_pvals, consts = pe.trace_to_jaxpr(jvpfun_flat, in_pvals)     103   out_primals_pvals, out_tangents_pvals = tree_unflatten(out_tree(), out_pvals) ~/.local/lib/python3.6/sitepackages/jax/interpreters/partial_eval.py in trace_to_jaxpr(fun, pvals, instantiate)     504     fun = trace_to_subjaxpr(fun, main, instantiate) > 505     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)     506     assert not env ~/.local/lib/python3.6/sitepackages/jax/_src/errors.py in __init__(self, tracer, context)     148     super().__init__( > 149         ""Abstract tracer value encountered where concrete value is expected: ""     150         f""{tracer}\n{context}{tracer._origin_msg()}\n"") UnfilteredStackTrace: AttributeError: 'str' object has no attribute '_origin_msg' The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: AttributeError                            Traceback (most recent call last)  in        1 f = jax.grad(NLL) > 2 print(f(jnp.array([1.0,0.0]))) ~/.local/lib/python3.6/sitepackages/jax/_src/errors.py in __init__(self, tracer, context)     147   def __init__(self, tracer: ""core.Tracer"", context: str = """"):     148     super().__init__( > 149         ""Abstract tracer value encountered where concrete value is expected: ""     150         f""{tracer}\n{context}{tracer._origin_msg()}\n"")     151  AttributeError: 'str' object has no attribute '_origin_msg' ``` The error is a bit cryptic, but I assume it is trying to say ""Abstract tracer value encountered where concrete value is expected"". Note that this error goes away if I pass an explicit float value to the `alpha_lumi_kFact` object: ``` def NLL(mu1):     mu = mu1[0]                                                                     the first parameter mu     alpha = mu1[1]                                                                   the second parameter alpha     alpha_lumi_kFact = t.obj(""alpha_ATLAS_LUMI"")     alpha_lumi_kFact.setVal(0.0)                                     setting the parameter to regular float 0.0     result = mu+alpha_lumi_kFact.getVal()        return result ``` which then gives the correct result for the following code: ``` f = jax.grad(NLL) print(f(jnp.array([1.0,0.0]))) ``` Result: ``` [1. 1.] ``` In summary, the `setVal` method takes as input only regular float values and does not seem to work with tracer values. Any suggestions?","It sounds like you're trying to use a JAX transform on a nonJAX function: this will not work. The way JAX traces the effect of inputs on outputs for the purposes of autodiff is via these Concrete tracer values: if you extract the underlying floating point values from these tracers and compute results with them, `grad` has no way of computing the correct result. I'd suggest either finding (or creating) a JAX implementation of the algorithm you want to use, or otherwise finding a different method of computing gradients for your code.","Okay, this is clear to me now. Thank you for the insight!"
985,"以下是一个github上的jax下的一个issue, 标题是(dynamic_update_slice start index adjustment fails when vmapped)， 内容是 (The documentation for `dynamic_update_slice` explains ""If the update slice is too large to fit in the array, the start index will be adjusted to make it fit"" and this behavior does indeed seem to work normally but fails when I apply vmap. No update occurs if the update slice would not fit even if the update slice would still be partially in the array as in the example below. ```python zeros = jnp.zeros(5) ones = jnp.ones(3) for i in range(1, 4):     print(jax.lax.dynamic_update_slice(zeros, ones, (jnp.array(i, dtype=int),))) print(jax.vmap(jax.lax.dynamic_update_slice, (None, None, 0))(zeros, ones, (jnp.arange(1, 4),)))  [0. 1. 1. 1. 0.]  [0. 0. 1. 1. 1.]  [0. 0. 1. 1. 1.]  [[0. 1. 1. 1. 0.]   [0. 0. 1. 1. 1.]   [0. 0. 0. 0. 0.]] ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,dynamic_update_slice start index adjustment fails when vmapped,"The documentation for `dynamic_update_slice` explains ""If the update slice is too large to fit in the array, the start index will be adjusted to make it fit"" and this behavior does indeed seem to work normally but fails when I apply vmap. No update occurs if the update slice would not fit even if the update slice would still be partially in the array as in the example below. ```python zeros = jnp.zeros(5) ones = jnp.ones(3) for i in range(1, 4):     print(jax.lax.dynamic_update_slice(zeros, ones, (jnp.array(i, dtype=int),))) print(jax.vmap(jax.lax.dynamic_update_slice, (None, None, 0))(zeros, ones, (jnp.arange(1, 4),)))  [0. 1. 1. 1. 0.]  [0. 0. 1. 1. 1.]  [0. 0. 1. 1. 1.]  [[0. 1. 1. 1. 0.]   [0. 0. 1. 1. 1.]   [0. 0. 0. 0. 0.]] ```",2022-01-04T01:51:55Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/9083,"Thanks for the report; it looks like this is a bug in the `dynamic_update_slice` batching rule: https://github.com/google/jax/blob/04369a35888581b2c93abccffbe61fb3db8c460e/jax/_src/lax/slicing.pyL918L936 On first look, I think the issue may be that it should use `mode=CLIP` rather than `mode=PROMISE_IN_BOUNDS`, but I'd have to dig deeper to be sure."
1640,"以下是一个github上的jax下的一个issue, 标题是(`KeyboardInterrupt` not handled consistently)， 内容是 (I have a program that runs until I request termination. An easy way to do this is: ``` try:     while True:          Call some jit'd functions except KeyboardInterrupt:      This isn't actually an error  just ignore it! Or maybe dump some data...     with open('file', 'wb') as f:         pickle.dump(1,f) ``` This approach is easy, and _mostly_ works. Unfortunately, sometimes the error thrown as a result of the KeyboardInterrupt is something else. For example: ``` Traceback (most recent call last):   File ""/home/scott/cauchy/mani/./aho.py"", line 126, in      chain.step(N=30)   File ""/home/scott/cauchy/mani/./aho.py"", line 71, in step     if jax.random.uniform(kacc) , (). The error was: KeyboardInterrupt: At:   /home/scott/cauchy/env/lib/python3.10/sitepackages/jax/core.py(1574): __hash__   /home/scott/cauchy/env/lib/python3.10/sitepackages/jax/_src/random.py(233): uniform   /home/scott/cauchy/mani/./aho.py(71): step   /home/scott/cauchy/mani/./aho.py(126):  ``` This issue is a bit of a challenge to reproduce reliably, since it only happens maybe one time in 50. I'm hoping those knowledgeable about jax internals will look at this situation and say ""well of course that happens"", and not need a concrete example. This is a minor issue  it's pretty easy to work around, and I could see it being a royal pain to fix  but I wanted to document it, at least. Let me know if a better reproduction is needed.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`KeyboardInterrupt` not handled consistently,"I have a program that runs until I request termination. An easy way to do this is: ``` try:     while True:          Call some jit'd functions except KeyboardInterrupt:      This isn't actually an error  just ignore it! Or maybe dump some data...     with open('file', 'wb') as f:         pickle.dump(1,f) ``` This approach is easy, and _mostly_ works. Unfortunately, sometimes the error thrown as a result of the KeyboardInterrupt is something else. For example: ``` Traceback (most recent call last):   File ""/home/scott/cauchy/mani/./aho.py"", line 126, in      chain.step(N=30)   File ""/home/scott/cauchy/mani/./aho.py"", line 71, in step     if jax.random.uniform(kacc) , (). The error was: KeyboardInterrupt: At:   /home/scott/cauchy/env/lib/python3.10/sitepackages/jax/core.py(1574): __hash__   /home/scott/cauchy/env/lib/python3.10/sitepackages/jax/_src/random.py(233): uniform   /home/scott/cauchy/mani/./aho.py(71): step   /home/scott/cauchy/mani/./aho.py(126):  ``` This issue is a bit of a challenge to reproduce reliably, since it only happens maybe one time in 50. I'm hoping those knowledgeable about jax internals will look at this situation and say ""well of course that happens"", and not need a concrete example. This is a minor issue  it's pretty easy to work around, and I could see it being a royal pain to fix  but I wanted to document it, at least. Let me know if a better reproduction is needed.",2022-01-04T00:47:22Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/9082,"Hrm, weird. Is the nonkeyboardinterrupt error always this `ValueError: Nonhashable static arguments...` thing? That error is raised in some C++ code for dispatching jitted computations. I wonder if that implementation is buggy, like it's catching all exceptions and always raising this particular error (rather than only catching hashability exceptions). ~ can you take a look?~","This issue is fixed in the source, but because there was a change in jaxlib, to get the fix you'd either need to build your own jaxlib from source or else wait for a new binary wheel release on pypi (jaxlib 0.1.76 or greater should have the fix)."
3939,"以下是一个github上的jax下的一个issue, 标题是(Fusing loops into kernels)， 内容是 (I am wondering what the current state of JAX is with respect to loop fusion; or what the roadmap looks like. In the simplest example, if we have an expression a * x + b of JAX scalars, and weve vmapped over that, what we want to avoid is to have to compute intermediates of minimal compute intensity. And not because using a fused muladd instruction instead will make that big of a difference; but because ondevice (and also on modern CPUs) code like this is almost always 100% memory bandwidth bound.  Im hoping JAX already handles a trivial case like this; but I havnt dug very deep and google isnt able to dig up much documentation. Now more generally, the pattern you often get in physics simulations for instance (as opposed to typical ML workloads), is like this. ``` .vmap def timestep(state):   state = state + foo    rest of a humongous compute graph of mostly puny compute intensity operations goes here   return state ``` From experimentation, I can tell that JAX isn't exactly going allout on fusing entire compute graphs like these. What id hope would be possible, is to create something like an .kernel decorator. It would behave somewhat like a numba kernel, in the sense that it would support a subset of JAX functionality. Only the most basic stuff, like forloops, and array broadcasting. Dots and sums and einsums and the like would be unrolled; certainly no dynamic allocation (duh); and no calling into other libraries, like BLAS or whatever. Just support the stuff that maps trivially onto opencl or whatever intermediate language is used.  Moreover, there would be no attempt to get clever about trying to expose internal parallelism inside the kernel. Each element of a vmap would map into a single GPU/TPU thread, that would serially crunch the entire kernel. No attempts to write super clever device code with memory tricks or whatnot; just have each thread read each input in a nicely memorycoalesced manner; so if we compiled the timestep function above as a kernel, each input would be read once from global memory, and each output would be written once to global memory. Just let vmap do all the work of exposing the parallelism. If you dont have any such parallelism to expose in your problem; though luck for you, shouldnt be using JAX then and rolling your own device kernels I suppose. You could get fancier than that with functionality supported in such a kernel; but implementing this subset shouldn't actually be *that* hard. I might even be able to contribute to it somewhat, having done similar fused loop code generation things for CUDA kernels in the past. Not too hard; and I think it would address 95% of use cases. My use cases at least. Lets call it 80/20 to be conservative. Also implementing forward mode differentiation for such kernels should be easy given the code/tools that I imagine are already on hand. (I think backwards generally does not make much sense for kernels like these, but I might be missing something; in any case there wouldn't be a usecase for the intermediate gradients in the kernel. Or if there were; just make it two kernels). Does this sound at all coherent? Is it already on the roadmap? Or should general future compiler improvements make more specialized functionality like this obsolete? Im supposing thats true in the long run; but you know what they say about the long run... Torch has a number of initiatives along these lines going on. If something like this should get any developer time also depends on who you see as the stakeholders in this projects. I imagine that people writing chat bots or image classifiers will view this as a waste of developer time; but I bet people working on say alphafold or brax would absolutely love a feature like this.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fusing loops into kernels,"I am wondering what the current state of JAX is with respect to loop fusion; or what the roadmap looks like. In the simplest example, if we have an expression a * x + b of JAX scalars, and weve vmapped over that, what we want to avoid is to have to compute intermediates of minimal compute intensity. And not because using a fused muladd instruction instead will make that big of a difference; but because ondevice (and also on modern CPUs) code like this is almost always 100% memory bandwidth bound.  Im hoping JAX already handles a trivial case like this; but I havnt dug very deep and google isnt able to dig up much documentation. Now more generally, the pattern you often get in physics simulations for instance (as opposed to typical ML workloads), is like this. ``` .vmap def timestep(state):   state = state + foo    rest of a humongous compute graph of mostly puny compute intensity operations goes here   return state ``` From experimentation, I can tell that JAX isn't exactly going allout on fusing entire compute graphs like these. What id hope would be possible, is to create something like an .kernel decorator. It would behave somewhat like a numba kernel, in the sense that it would support a subset of JAX functionality. Only the most basic stuff, like forloops, and array broadcasting. Dots and sums and einsums and the like would be unrolled; certainly no dynamic allocation (duh); and no calling into other libraries, like BLAS or whatever. Just support the stuff that maps trivially onto opencl or whatever intermediate language is used.  Moreover, there would be no attempt to get clever about trying to expose internal parallelism inside the kernel. Each element of a vmap would map into a single GPU/TPU thread, that would serially crunch the entire kernel. No attempts to write super clever device code with memory tricks or whatnot; just have each thread read each input in a nicely memorycoalesced manner; so if we compiled the timestep function above as a kernel, each input would be read once from global memory, and each output would be written once to global memory. Just let vmap do all the work of exposing the parallelism. If you dont have any such parallelism to expose in your problem; though luck for you, shouldnt be using JAX then and rolling your own device kernels I suppose. You could get fancier than that with functionality supported in such a kernel; but implementing this subset shouldn't actually be *that* hard. I might even be able to contribute to it somewhat, having done similar fused loop code generation things for CUDA kernels in the past. Not too hard; and I think it would address 95% of use cases. My use cases at least. Lets call it 80/20 to be conservative. Also implementing forward mode differentiation for such kernels should be easy given the code/tools that I imagine are already on hand. (I think backwards generally does not make much sense for kernels like these, but I might be missing something; in any case there wouldn't be a usecase for the intermediate gradients in the kernel. Or if there were; just make it two kernels). Does this sound at all coherent? Is it already on the roadmap? Or should general future compiler improvements make more specialized functionality like this obsolete? Im supposing thats true in the long run; but you know what they say about the long run... Torch has a number of initiatives along these lines going on. If something like this should get any developer time also depends on who you see as the stakeholders in this projects. I imagine that people writing chat bots or image classifiers will view this as a waste of developer time; but I bet people working on say alphafold or brax would absolutely love a feature like this.",2022-01-03T10:13:27Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/9080,"My impression is that XLA actually _is_ pretty good at fusing together the ""humongous compute graph of mostly puny compute intensity operations,"" even when vmapped. E.g., see the ""equation_of_state"" benchmark from https://github.com/dionhaefner/pyhpcbenchmarks I suppose it probably depends a bit on exactly what those puny operations are. A concrete example would certainly be helpful here. As for your proposal here, it sounds totally reasonable, but I'm not sure that _XLA_ in particular is the right compiler technology for it. We've definitely speculated about using tools like Numba or Dex as ""kernel languages"" for generating efficient kernels to drop into larger JAX programs, which sounds like it fill a similar niche. From my perspective, figuring out exactly how to write this ""kernel compiler"" (or identifying an existing system) would probably be the first challenge to solve, before plugging it into JAX (which I would hope would be relatively straightforward using XLA's CustomCall).","Im motivated by toying with geometric algebra in JAX, and trying to decide if its best to write JAX code using 'dense' kernels, such as can be found here, or 'unrolled' approaches, such as can be found a few lines below. Note that with general products/operations in geometric algebra, one is dealing with product of nary tensors that are usually quite sparse. Here another example; alternatively one could write that as a contraction with a [3,3,3] tensor of mostly zeros.  From my (admittedly limited) benchmarking, the unrolled versions get absolutely BTFO by the dense products. Even on CPU, doing a [16, 16, 16, 16] full 4d multivector sandwich product, which is a tensor with only 3% nonzeros, the dense version is 3x faster than the unrolled version. That seems to indicate that the unrolled loops do not get fused by the compiler very much; if they did youd expect to see some benefit from the 30x reduction in FLOPS. For more reasonable sized products, you are easily looking at a 1020x advantage for the dense approach. And im imagining this gap will only widen on GPU/TPU. Given that a GPU can do 100s of flops per float it can fetch form global mem, it would make sense that the sparsity and multiplying with zero a bunch isnt our problem in the first place; the real risk is writing your code in such a way that makes it fetch your data more often from global memory than is necessary.  But perhaps I should try those GPU benchmarks first? Maybe the compiler toolchain will pursue loop fusion more aggressively for those devices? (I notice that a focus on FLOPs rather than memory bandwidth is a common vestigial instinct with people who grew up tuning C code in the 90s; but modern CPUs are more like GPUs than 90s CPU in this regard anyway).  Ill try to cobble together a minimal piece of code to see if others can reproduce these differences. Note that doing all the loop fusion for a single GA product will easily require the fusion of many dozens of terms; and then your physics logic would consist of many dozens such operations chained together. The optimal solution from a memory bandwidth pov is clearly to fuse all of these intermediates when there is a nice bottleneck in that compute graph, like state > state; but I dont know that XLA has been tuned very much for these scenarios, as its quite different from typical ML ones. Its quite possible that enabling such tuning in XLA would be a path of lesser resistance compared to a completely different compilation path... but I don't know. Indeed perhaps its best if we could view a hypothetical .kernel as more of a compiler hint to XLA. That being said restricting yourself to a subset of regular JAX does seem necessary to me to make the most of such loop fusion opportunities.","Interesting benchmark you linked btw, and indeed a good example of a pile of low compute intensity trash thrown together. Indeed JAX is doing quite well for itself... but that doesn't mean it couldn't be doing better I suppose. There is no direct comparison to a manually fully fused kernel; and im not sure I trust any of these frameworks to have it completely figured out. Frankly I would expect bigger differences to numpy on cpu, if they did. Seeing it compared with a cupy elementwisekernel would be great; cant think of a way a compiler might screw up the loop fusion with that; and that should also allow me to write it in a way that I dont think the compiler can screw up emitting code with coalesced memory access. I probably should do that for my own benchmarks as well; should be trivial to transform my GAproducts into cupy elementwisekernels. Im mostly interested in getting my kernels to run optimally in JAX but I think that should serve as a nice groundtruth of what should be possible.","Hmmm; doing more benchmarking and finding some embarrassing limitations in my benchmarking code... ill take back everything I said above. Indeed definite evidence in favor of aggressive loop fusion going on. Still curious how it will end up comparing to a GPU kernel that I trust to really make the most of available compute and bandwidth though. It makes me wonder what the alphafold people were thinking with the way they have got their quat multiplication implemented, since that is beat handily now in all possible permutations of configurations. Still only CPU testing though and I imagine they optimized for ondevice compute. A TPU has a single instruction for 4x4 matrix multiplies I think... so that might be why? All the more reason to have a nicely configurable library that will autogenerate these type of products and make their codegeneration strategies configurable; because whatever way these benchmark results may be swinging, I have yet to see a subtle difference. (also compilation speed is a 10x win for the dense approach; so its nice to be able to toggle between debug/release modes; and no i wasnt missing a warmup call in my benchmark; it was another equally stupid mistake) But good to know that JAX can infact be trusted to reason about these things fairly effectively. Still... going to work on that cupy kernel as a ground truth."
