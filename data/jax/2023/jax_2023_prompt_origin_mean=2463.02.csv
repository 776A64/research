1490,"以下是一个github上的jax下的一个issue, 标题是(Testing __cuda_array_interface__ attribute of a CPU array throws an exception)， 内容是 ( Description As in the title. Reproducer: ```python >>> import jax >>> jax.config.update('jax_platform_name', 'cpu') >>> a = jax.numpy.array([1, 2]) >>> hasattr(a, '__cuda_array_interface__') Traceback (most recent call last):   File """", line 1, in  jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: __cuda_array_interface__ is only defined for NVidia GPU buffers. ``` The expected behavior is ```python >>> hasattr(a, '__cuda_array_interface__') False ```  similar to PyTorch or NumPy: ```python >>> import numpy >>> a = numpy.array([1, 2]) >>> hasattr(a, '__cuda_array_interface__') False >>> import torch >>> a = torch.tensor([1, 2], device='cpu') >>> hasattr(a, '__cuda_array_interface__') False >>> a = torch.tensor([1, 2], device='cuda') >>> hasattr(a, '__cuda_array_interface__') True ```  What jax/jaxlib version are you using? 0.4.24.dev20231215+41531123f 0.4.24.dev20231215+41531123f  Which accelerator(s) are you using? CPU  Additional system info? 1.26.2 3.11.0  (main, Jan 14 2023, 12:27:40) [GCC 11.3.0] uname_result(system='Linux', node='ex', release='5.4.0153generic', version=' CC(Random key error in stax.Dropout layer)Ubuntu SMP Fri Jun 16 13:43:31 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Testing __cuda_array_interface__ attribute of a CPU array throws an exception," Description As in the title. Reproducer: ```python >>> import jax >>> jax.config.update('jax_platform_name', 'cpu') >>> a = jax.numpy.array([1, 2]) >>> hasattr(a, '__cuda_array_interface__') Traceback (most recent call last):   File """", line 1, in  jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: __cuda_array_interface__ is only defined for NVidia GPU buffers. ``` The expected behavior is ```python >>> hasattr(a, '__cuda_array_interface__') False ```  similar to PyTorch or NumPy: ```python >>> import numpy >>> a = numpy.array([1, 2]) >>> hasattr(a, '__cuda_array_interface__') False >>> import torch >>> a = torch.tensor([1, 2], device='cpu') >>> hasattr(a, '__cuda_array_interface__') False >>> a = torch.tensor([1, 2], device='cuda') >>> hasattr(a, '__cuda_array_interface__') True ```  What jax/jaxlib version are you using? 0.4.24.dev20231215+41531123f 0.4.24.dev20231215+41531123f  Which accelerator(s) are you using? CPU  Additional system info? 1.26.2 3.11.0  (main, Jan 14 2023, 12:27:40) [GCC 11.3.0] uname_result(system='Linux', node='ex', release='5.4.0153generic', version=' CC(Random key error in stax.Dropout layer)Ubuntu SMP Fri Jun 16 13:43:31 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_",2023-12-27T21:04:05Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/19134,"The same issue also breaks `jax.numpy.from_dlpack` on CPU arrays: ```python >>> import jax >>> jax.config.update('jax_platform_name', 'cpu') >>> a = jax.numpy.array([1, 2]) >>> jax.numpy.from_dlpack(a) Traceback (most recent call last):   File """", line 1, in    File ""/home/pearu/git/pearu/jax/jax/_src/numpy/lax_numpy.py"", line 2386, in from_dlpack     return from_dlpack(x.__dlpack__())            ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/pearu/git/pearu/jax/jax/_src/dlpack.py"", line 123, in from_dlpack     return jnp.asarray(xla_client._xla.dlpack_managed_tensor_to_buffer(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/pearu/git/pearu/jax/jax/_src/numpy/lax_numpy.py"", line 2186, in asarray     if hasattr(a, ""__cuda_array_interface__""):        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INVALID_ARGUMENT: __cuda_array_interface__ is only defined for NVidia GPU buffers. ```","Hi  thanks for the report! Can you confirm what JAX version you're using? I can't reproduce your `jax.numpy.from_dlpack` error on any recent JAX release; in particular I can't find any JAX version where line 2186 of `lax_numpy.py` contains the line `    if hasattr(a, ""__cuda_array_interface__""):` (though I didn't look at versions older than 0.4.0). As for the `hasattr` function failing... I think this is working as expected. All arrays have this attribute, it's just that some of them raise an error because they are not on the appropriate device. We could have chosen to implement this differently, so that only arrays on particular devices have this attribute, but that would break the assumptions of static type checkers like `mypy` and `pytype`, in which the existence of object attributes cannot depend on runtime values. Rather than `hasattr`, you might think about doing something like this: ```python try:   a.__cuda_array_interface__ except:   is_cuda = False else:   is_cuda = True ``` What do you think?","It's possible that we should throw a different exception from our pybind11 code though, so Python thinks the attribute is not present.","If we throw an `AttributeError`, then the `isinstance` check at the top of this thread would work as expected.","> Can you confirm what JAX version you're using? Sure: ```python >>> jax.__version__ '0.4.24.dev20231228+d9aba6f67' ``` Also, confirming that with recent main branch, https://github.com/google/jax/issues/19134issuecomment1873304362 is fixed.","> As for the `hasattr` function failing... I think this is working as expected. All arrays have this attribute, it's just that some of them raise an error because they are not on the appropriate device. We could have chosen to implement this differently, so that only arrays on particular devices have this attribute, but that would break the assumptions of static type checkers like `mypy` and `pytype`, in which the existence of object attributes cannot depend on runtime values. The current behavior contradicts `hasattr` semantics. It should return True or False depending on whether the object has the attribute specified by a name. At the level of `hasattr`, devicespecific behavior is irrelevant. The devicespecific behavior should be implemented in `getattr`. There are a few choices how to handle cudaspecific attributes of noncudaspecific arrays:  throw a runtime error  I think this is not correct because the devicespecific attributes are set at array construction time and that cannot be changed during the array life time.  throw an attribute error  this is exact behavior when `hasattr()` returns `False` I think throwing an attribute error from `getattr` in this situation is not going to be a problem for `mypy`. For instance, PyTorch is mypycompliant (however, it could depend on PyTorch mypy configuration, I haven't check that yet) and it implements the following behavior: ```python >>> import torch >>> a = torch.tensor([1, 2]).cpu() >>> hasattr(a, ""__cuda_array_interface__"") False >>> a.__cuda_array_interface__ Traceback (most recent call last):   File """", line 1, in    File ""/home/pearu/git/pytorch/pytorchcsc/torch/_tensor.py"", line 1103, in __cuda_array_interface__     raise AttributeError( AttributeError: Can't get __cuda_array_interface__ on nonCUDA tensor type: torch.LongTensor If CUDA data is required use tensor.cuda() to copy tensor to device memory. >>> a = torch.tensor([1, 2]).cuda() >>> hasattr(a, ""__cuda_array_interface__"") True >>> a.__cuda_array_interface__ {'typestr': '<i8', 'shape': (2,), 'strides': None, 'data': (140434749784064, False), 'version': 2} ```","> The current behavior contradicts `hasattr` semantics. It should return True or False depending on whether the object has the attribute specified by a name. That's not how `hasattr` is implemented, nor is it how it is documented But I think we could raise `AttributeError` here instead of `XlaRuntimeError` to make `hasattr` work as one would expect here. It would involve changing this line: https://github.com/openxla/xla/blob/768c94954fed0104a7dac2a875970227e1f71380/xla/python/py_array.ccL1112",I'll make it throw an attribute error.,https://github.com/openxla/xla/pull/8183 (primarily) fixes this.,"This still is problematic for sharded arrays; tested on the most recent nightly builds on CPU: ```python import os os.environ[""XLA_FLAGS""] = "" xla_force_host_platform_device_count=8"" import jax import jaxlib import jax.numpy as jnp from jax.sharding import Mesh, NamedSharding, PartitionSpec as P print(jax.__version__)  0.4.24.dev20240117 print(jaxlib.__version__)  0.4.24.dev20240117 x = jnp.arange(16, dtype='float32').reshape(8, 2) print(hasattr(x, ""__cuda_array_interface__""))  False mesh = Mesh(jax.devices(), axis_names=(""i"")) x_sharded = jax.device_put(x, NamedSharding(mesh, P('i', None))) print(hasattr(x_sharded, ""__cuda_array_interface__""))    XlaRuntimeError                           Traceback (most recent call last)   in ()       19 mesh = Mesh(jax.devices(), axis_names=(""i""))       20 x_sharded = jax.device_put(x, NamedSharding(mesh, P('i', None)))  > 21 print(hasattr(x_sharded, ""__cuda_array_interface__""))       22   XlaRuntimeError: INVALID_ARGUMENT: UnsafeBufferPointer() is supported only for unsharded arrays. ```"
4669,"以下是一个github上的jax下的一个issue, 标题是(Potential memory issue on Apple Metal)， 内容是 ( Description Evaluating a simple multilayerperceptron (MLP) implemented in `flax` on the same input data and parameters potentially yields nondeterministic outputs on the **apple metal** device when the function is **NOT jitted**. When the function is **jitted** the outputs of the MLP are deterministic and the problem disappears. I was able to verify that this problem is specific to apple metal, as on a linux system with an nvidia gpu, the problem does not occur (with the current jax version). Empirically the problem frequency seems to be worse when the batch dimension is not of shape `2**n` and `n > 10`. For example for batch dimensions of 2500 and 5000 the problems occurs frequently.  Another empirical observation is that  the values are not random but repeat themself. For example `y[0, 0]` is always one of `m` different numbers (empirically `m \approx 34`) but it is random which one of the m options ones get, which kind of hints into a memory problem.   It is debatable whether this problem is a `jax`, `flax` or `apple metal plugin` issue.  I am happy to file this issue at a different location if preferred.   ``` import jax import numpy as np from jax import numpy as jnp from flax import linen as nn class MLP(nn.Module):     out_dims: int     hidden_dims: int     .compact     def __call__(self, x):         h1 = nn.Dense(self.hidden_dims)(x)         h2 = nn.Dense(self.hidden_dims)(h1)         return nn.Dense(self.out_dims)(h2) if __name__ == ""__main__"":     key = jax.random.PRNGKey(42)     n_dim = 2     for n_samples in [512, 1024, 2048, 2500, 4096, 5000, 8192]:         x = jax.random.uniform(key, shape=(n_samples, n_dim))         network = MLP(hidden_dims=256, out_dims=2)         params = network.init(key, x)          forward_fn = jax.jit(network.apply)         forward_fn = network.apply         y0 = forward_fn(params, x)         n_error, max_error = [], []         for i in range(20):             yi = forward_fn(params, x)             error = jnp.sum(jnp.abs(yi  y0), axis=1)             n_error.append(jnp.sum(error > 1e6).item())             max_error.append(error.max().item())         print(f'\n Samples = {n_samples}'               f'\n Error = {n_error}'               f'\nmin/max error = {np.array(max_error).min():.2f} / {np.array(max_error).max():.2f}') ``` **Output WITHOUT JIT => NonDeterministic Output**: ``` Metal device set to: Apple M3 Max systemMemory: 64.00 GB maxCacheSize: 24.00 GB  Samples = 512  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 1024  Error = [0, 409, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 2.51  Samples = 2048  Error = [0, 0, 0, 814, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 2.76  Samples = 2500  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 4096  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 5000  Error = [4163, 3681, 4229, 3648, 4218, 3692, 4244, 3745, 4160, 3654, 4423, 3629, 4152, 3674, 4225, 3613, 4172, 3658, 4193, 3630] min/max error = 2.31 / 3.64  Samples = 8192  Error = [0, 256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 1.42 ``` **Output WITH JIT => Everything works as expected** ``` Metal device set to: Apple M3 Max systemMemory: 64.00 GB maxCacheSize: 24.00 GB  Samples = 512  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 1024  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 2048  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 2500  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 4096  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 5000  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 8192  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00 ```  What jax/jaxlib version are you using? 0.4.11, 0.4.11  Which accelerator(s) are you using? Apple Metal  Additional system info? Python 3.10, Numpy 1.26.0, Platform MacOS / Darwin  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Potential memory issue on Apple Metal," Description Evaluating a simple multilayerperceptron (MLP) implemented in `flax` on the same input data and parameters potentially yields nondeterministic outputs on the **apple metal** device when the function is **NOT jitted**. When the function is **jitted** the outputs of the MLP are deterministic and the problem disappears. I was able to verify that this problem is specific to apple metal, as on a linux system with an nvidia gpu, the problem does not occur (with the current jax version). Empirically the problem frequency seems to be worse when the batch dimension is not of shape `2**n` and `n > 10`. For example for batch dimensions of 2500 and 5000 the problems occurs frequently.  Another empirical observation is that  the values are not random but repeat themself. For example `y[0, 0]` is always one of `m` different numbers (empirically `m \approx 34`) but it is random which one of the m options ones get, which kind of hints into a memory problem.   It is debatable whether this problem is a `jax`, `flax` or `apple metal plugin` issue.  I am happy to file this issue at a different location if preferred.   ``` import jax import numpy as np from jax import numpy as jnp from flax import linen as nn class MLP(nn.Module):     out_dims: int     hidden_dims: int     .compact     def __call__(self, x):         h1 = nn.Dense(self.hidden_dims)(x)         h2 = nn.Dense(self.hidden_dims)(h1)         return nn.Dense(self.out_dims)(h2) if __name__ == ""__main__"":     key = jax.random.PRNGKey(42)     n_dim = 2     for n_samples in [512, 1024, 2048, 2500, 4096, 5000, 8192]:         x = jax.random.uniform(key, shape=(n_samples, n_dim))         network = MLP(hidden_dims=256, out_dims=2)         params = network.init(key, x)          forward_fn = jax.jit(network.apply)         forward_fn = network.apply         y0 = forward_fn(params, x)         n_error, max_error = [], []         for i in range(20):             yi = forward_fn(params, x)             error = jnp.sum(jnp.abs(yi  y0), axis=1)             n_error.append(jnp.sum(error > 1e6).item())             max_error.append(error.max().item())         print(f'\n Samples = {n_samples}'               f'\n Error = {n_error}'               f'\nmin/max error = {np.array(max_error).min():.2f} / {np.array(max_error).max():.2f}') ``` **Output WITHOUT JIT => NonDeterministic Output**: ``` Metal device set to: Apple M3 Max systemMemory: 64.00 GB maxCacheSize: 24.00 GB  Samples = 512  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 1024  Error = [0, 409, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 2.51  Samples = 2048  Error = [0, 0, 0, 814, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 2.76  Samples = 2500  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 4096  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 5000  Error = [4163, 3681, 4229, 3648, 4218, 3692, 4244, 3745, 4160, 3654, 4423, 3629, 4152, 3674, 4225, 3613, 4172, 3658, 4193, 3630] min/max error = 2.31 / 3.64  Samples = 8192  Error = [0, 256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 1.42 ``` **Output WITH JIT => Everything works as expected** ``` Metal device set to: Apple M3 Max systemMemory: 64.00 GB maxCacheSize: 24.00 GB  Samples = 512  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 1024  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 2048  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 2500  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 4096  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 5000  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00  Samples = 8192  Error = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] min/max error = 0.00 / 0.00 ```  What jax/jaxlib version are you using? 0.4.11, 0.4.11  Which accelerator(s) are you using? Apple Metal  Additional system info? Python 3.10, Numpy 1.26.0, Platform MacOS / Darwin  NVIDIA GPU info _No response_",2023-12-27T18:36:08Z,bug Apple GPU (Metal) plugin,closed,0,3,https://github.com/jax-ml/jax/issues/19132,Hi   I executed the mentioned code with jaxmetal 0.0.6 on a Macbook Pro with an M1 Pro chip to see if the reported issue persists. The code produces the same output regardless of using JustInTime (JIT) compilation.    Could you please verify with jaxmetal 0.0.6 and confirm if the issue issue still persists. Thank you.,"Thanks for following up , I'm going to close the issue","Can confirm after upgrading `jaxmetal`, `jax` as well as `flax` this error seems to be gone."
471,"以下是一个github上的jax下的一个issue, 标题是(Help with aarch64 dependencies)， 内容是 (For the past week, I've been trying to get this: https://huggingface.co/blog/sdxl_jax to work with my coral tpu usb accelerator. I've noticed that Jax depends on Jaxlib for XLA, and on pypi, there are some aarch64 wheels but pip install jaxlib returns an error.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Help with aarch64 dependencies,"For the past week, I've been trying to get this: https://huggingface.co/blog/sdxl_jax to work with my coral tpu usb accelerator. I've noticed that Jax depends on Jaxlib for XLA, and on pypi, there are some aarch64 wheels but pip install jaxlib returns an error.",2023-12-26T22:29:37Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/19128,"never mind, I just reinstalled raspbian 64 bit this time. It is still very disappointing that nobody tried to help"
2332,"以下是一个github上的jax下的一个issue, 标题是(cuBLAS and JAX version mismatch)， 内容是 ( Description I noticed that the update for JAX 0.4.23 introduced some version mismatch errors when I am running a training script:  `CUDA backend failed to initialize: Found cuBLAS version 120103, but JAX was built against version 120205, which is newer. The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` I shall note that this issue was mentioned ( CC(Unable to correct CUDA vs. JAX version mismatch)) not long ago by another fellow user, and I have attempted the following command to fix, but to no avail.  `pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` The error unfortunately still persisted, and I suspect that this could be something to do with the CUDA backend. Please correct me if I am wrong.   What I have done  Did a fresh install for CUDA and CUDA Toolkit to upgrade it to the latest version (12.3)  `cudatoolkit123 is already the newest version (12.3.11).` My installation steps were based on the settings provided here. In addition, I have also updated my PATH and environment variables after the installation.  Ran `nvcc version`; here is my latest output after updating CUDA to the latest version.  ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Tue_Aug_15_22:02:13_PDT_2023 Cuda compilation tools, release 12.2, V12.2.140 Build cuda_12.2.r12.2/compiler.33191640_0 ```  Attempted to pip install a stable jaxlib build: `pip install https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.23+cuda12.cudnn89cp310cp310manylinux2014_x86_64.whlsha256=8e42000672599e7ec0ea7f551acfcc95dcdd0e22b05a1d1f12f97b56a9fce4a8` , before installing JAX (again): `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` via the default manner.   What jax/jaxlib version are you using? jax 0.4.23, jaxlib 0.4.23  Which accelerator(s) are you using? RTX A6000  Additional system info? Ubuntu Linux 22.04  NVIDIA GPU info ``` Mon Dec 25 18:41:09 2023 ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,cuBLAS and JAX version mismatch," Description I noticed that the update for JAX 0.4.23 introduced some version mismatch errors when I am running a training script:  `CUDA backend failed to initialize: Found cuBLAS version 120103, but JAX was built against version 120205, which is newer. The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` I shall note that this issue was mentioned ( CC(Unable to correct CUDA vs. JAX version mismatch)) not long ago by another fellow user, and I have attempted the following command to fix, but to no avail.  `pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` The error unfortunately still persisted, and I suspect that this could be something to do with the CUDA backend. Please correct me if I am wrong.   What I have done  Did a fresh install for CUDA and CUDA Toolkit to upgrade it to the latest version (12.3)  `cudatoolkit123 is already the newest version (12.3.11).` My installation steps were based on the settings provided here. In addition, I have also updated my PATH and environment variables after the installation.  Ran `nvcc version`; here is my latest output after updating CUDA to the latest version.  ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052023 NVIDIA Corporation Built on Tue_Aug_15_22:02:13_PDT_2023 Cuda compilation tools, release 12.2, V12.2.140 Build cuda_12.2.r12.2/compiler.33191640_0 ```  Attempted to pip install a stable jaxlib build: `pip install https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.23+cuda12.cudnn89cp310cp310manylinux2014_x86_64.whlsha256=8e42000672599e7ec0ea7f551acfcc95dcdd0e22b05a1d1f12f97b56a9fce4a8` , before installing JAX (again): `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` via the default manner.   What jax/jaxlib version are you using? jax 0.4.23, jaxlib 0.4.23  Which accelerator(s) are you using? RTX A6000  Additional system info? Ubuntu Linux 22.04  NVIDIA GPU info ``` Mon Dec 25 18:41:09 2023 ++  ++ ```",2023-12-25T19:15:19Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19121,"I've managed to fix my issue, simply pip installing a previous version of JAX.  `pip install jax[cuda12_pip]==0.4.21 f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`","Just in case it helps anyone coming here from Google (as I did): I got this version mismatch error in Google Colab when using _both_ Jax and Torch in one Notebook. For some to me unexplainable reason, the following reliably fixes the issue for me: Instead of ```python import jax import torch  ... your code here ... ``` insert any call to jax _between_ the `import jax` and `import torch`, for example: ```python import jax print(f""The jax default backend is: {jax.default_backend()}"") import torch  ... your code here ... ``` I assume this has something to do with how Jax and Torch handle initialisation of the GPU backend, with Torch messing things up for Jax upon being imported. Perhaps. See also related issue CC(JAX and TORCH).","Please check CC(Latest JAX `0.4.24` does not detect GPUs) . Installing it from pip works, provided you don't have another CUDA loaded. That downloads the correct CUDA and cuBLAS to site_packages."
1699,"以下是一个github上的jax下的一个issue, 标题是(jax.lax.scan ""tuple index out of range error"" when using as described in docs)， 内容是 ( Description I'm trying to migrate a forloop to `scan` as per the docs. Here's the code I'm trying: ```python3     def _calculate_node_errors(         branching_nodes: tuple[Node, ...],     ) > jnp.NDArray[jnp.float_]:          THIS IS THE OLD CODE          values = jnp.empty(shape=(len(branching_nodes)))          for idx, node in enumerate(branching_nodes):              error = node.get_normalised_error()              values = values.at[idx].set(error)          THIS IS THE NEW CODE         _, values = jax.lax.scan(          When the type of xs (denoted a above) is an array type or None, and the type of ys (denoted b above) is an array type, the semantics of scan() are given roughly by this Python implementation: ```python3 def scan(f, init, xs, length=None):   if xs is None:     xs = [None] * length   carry = init   ys = []   for x in xs:     carry, y = f(carry, x)     ys.append(y)   return carry, np.stack(ys) ``` The error doesn't seem to make much sense  are my `xs` not an array type? (tuple)? In fact further down it says: >**xs** (X) – the value of type [a] over which to scan along the leading axis, where [a] can be an array or any pytree (nested Python tuple/list/dict) thereof with consistent leading axis sizes. What am I missing? Is this a bug or are the docs confusing me?  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.13  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"jax.lax.scan ""tuple index out of range error"" when using as described in docs"," Description I'm trying to migrate a forloop to `scan` as per the docs. Here's the code I'm trying: ```python3     def _calculate_node_errors(         branching_nodes: tuple[Node, ...],     ) > jnp.NDArray[jnp.float_]:          THIS IS THE OLD CODE          values = jnp.empty(shape=(len(branching_nodes)))          for idx, node in enumerate(branching_nodes):              error = node.get_normalised_error()              values = values.at[idx].set(error)          THIS IS THE NEW CODE         _, values = jax.lax.scan(          When the type of xs (denoted a above) is an array type or None, and the type of ys (denoted b above) is an array type, the semantics of scan() are given roughly by this Python implementation: ```python3 def scan(f, init, xs, length=None):   if xs is None:     xs = [None] * length   carry = init   ys = []   for x in xs:     carry, y = f(carry, x)     ys.append(y)   return carry, np.stack(ys) ``` The error doesn't seem to make much sense  are my `xs` not an array type? (tuple)? In fact further down it says: >**xs** (X) – the value of type [a] over which to scan along the leading axis, where [a] can be an array or any pytree (nested Python tuple/list/dict) thereof with consistent leading axis sizes. What am I missing? Is this a bug or are the docs confusing me?  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.13  ++ ```",2023-12-24T11:21:55Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19113,Thanks for the report! I'm having trouble figuring out how to call your function in order to reproduce this error. Could you put together a minimal reproducible example that I could paste into a code interpreter to see the same output you're seeing?,Closing as not reproducible – feel free to open another issue if you're still having this problem. Thanks!
1869,"以下是一个github上的jax下的一个issue, 标题是(Underscore in setup.py for extras_require is (probably) not supported by pip)， 内容是 ( Description When I run the suggested installation command: ```pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html``` I get the following warning (which causes other downstream problems): ```WARNING: jax 0.4.23 does not provide the extra 'cuda12pip'``` Note that `cuda12pip` has a hyphen in the warning, not an underscore. This problem occurs with `pip` versions since 23.3, but not earlier ones. In the release notes for `pip 23.3` this change seems to be considered a bug fix: > Normalize extras according to PEP 685 from package metadata in the resolver for comparison. This ensures extras are correctly compared and merged as long as the package providing the extra(s) is built with values normalized according to the standard. Note, however, that this does not solve cases where the package itself contains unnormalized extra values in the metadata. ( CC(Reland 11498 after internal fixes.)) The normalization mentioned is the following: ```re.sub(r""[_.]+"", """", name).lower()``` i.e., all consecutive hyphens, underscores, and periods are replaced with a single hyphen. Given these circumstances, does it make sense to change the `setup.py` in `jax` and `jaxlib` to not use underscores? Or at least mention in the `README` that an older version of `pip` needs to be used. This caused me a lot of problems with the installation, especially since it just gives a warning without failing.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info? _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Underscore in setup.py for extras_require is (probably) not supported by pip," Description When I run the suggested installation command: ```pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html``` I get the following warning (which causes other downstream problems): ```WARNING: jax 0.4.23 does not provide the extra 'cuda12pip'``` Note that `cuda12pip` has a hyphen in the warning, not an underscore. This problem occurs with `pip` versions since 23.3, but not earlier ones. In the release notes for `pip 23.3` this change seems to be considered a bug fix: > Normalize extras according to PEP 685 from package metadata in the resolver for comparison. This ensures extras are correctly compared and merged as long as the package providing the extra(s) is built with values normalized according to the standard. Note, however, that this does not solve cases where the package itself contains unnormalized extra values in the metadata. ( CC(Reland 11498 after internal fixes.)) The normalization mentioned is the following: ```re.sub(r""[_.]+"", """", name).lower()``` i.e., all consecutive hyphens, underscores, and periods are replaced with a single hyphen. Given these circumstances, does it make sense to change the `setup.py` in `jax` and `jaxlib` to not use underscores? Or at least mention in the `README` that an older version of `pip` needs to be used. This caused me a lot of problems with the installation, especially since it just gives a warning without failing.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info? _No response_  NVIDIA GPU info _No response_",2023-12-24T01:38:11Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19111,"Never mind, I just realized my ""downstream problems"" were caused by something else. Sorry for the spam."
1251,"以下是一个github上的jax下的一个issue, 标题是(Wrappers for`scipy.linalg`quadratic control solvers (lyapunov, ARE))， 内容是 (Please:  [x ] Check for duplicate requests.  [x ] Describe your goal, and if possible provide a code snippet with a motivating example. I'm interested in implementing `solve_discrete_lyapunov`, `solve_continuous_lyapunov`, and `solve_discrete_are` from `scipy.linalg` as JAX primitives. My particular usecase is Kalman filtering  these functions are handy for computing initial and steadystate covariance matrices, but they also have wide application in linearquadratic control applications. There are gradients computed in this paper, https://arxiv.org/pdf/2011.11430.pdf and I've also done implementations in PyTensor here and here. I'm relying heavily on compiling pytensor graphs to JAX for highperformance scans, and not having these functions is a bit of a painpoint for me at the moment. I didn't see these functions from a quick search of the codebase, but I just wanted to check that 1) a contribute would be welcome, and 2) they didn't exist elsewhere in the JAX ecosystem before starting a PR.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Wrappers for`scipy.linalg`quadratic control solvers (lyapunov, ARE)","Please:  [x ] Check for duplicate requests.  [x ] Describe your goal, and if possible provide a code snippet with a motivating example. I'm interested in implementing `solve_discrete_lyapunov`, `solve_continuous_lyapunov`, and `solve_discrete_are` from `scipy.linalg` as JAX primitives. My particular usecase is Kalman filtering  these functions are handy for computing initial and steadystate covariance matrices, but they also have wide application in linearquadratic control applications. There are gradients computed in this paper, https://arxiv.org/pdf/2011.11430.pdf and I've also done implementations in PyTensor here and here. I'm relying heavily on compiling pytensor graphs to JAX for highperformance scans, and not having these functions is a bit of a painpoint for me at the moment. I didn't see these functions from a quick search of the codebase, but I just wanted to check that 1) a contribute would be welcome, and 2) they didn't exist elsewhere in the JAX ecosystem before starting a PR.",2023-12-23T22:00:22Z,enhancement,open,1,2,https://github.com/jax-ml/jax/issues/19109,How do you think these solvers match up according to our rubric for JAX scipy wrappers? https://jax.readthedocs.io/en/latest/jep/18137numpyscipyscope.html,"I think they're on the margin. They fail axis 1 spectacularly, but arguably pass the other axes, with varying levels of difficultly in making those arguments. I think they pass on 2, 3, 5, and arguably on 6  github code search finds 5001000 snippets using each of `solve_discrete_lyapunov` and `solve_discrete_are`, so it's clearly not as popular as `linalg.solve`, but more widely used than  `bessel_jn`. I guess the weakest case is on 4, ideally I'd hope to just wrap up some calls to LAPACK for forward computation together with some gradients, but this is likely to be more complicated than I realize (hardware targeting issues? introduction of additional package requirements? I have no idea if either of these would be issues, but I can imagine that they could be). On the other hand, quadratic control generally and Kalman filtering specifically aren't exactly niche topics in scientific computing, so I'm sure these functions would see some use if they were available. Plus `scipy.linalg` is in scope. But I could see them belonging in something more like jaxopt, though. "
398,"以下是一个github上的jax下的一个issue, 标题是(partial rollback of #19096 due to internal breakage (relying on jax internals))， 内容是 (partial rollback of CC(del add_any_p and zeros_like_p, replace avaldispatched traceable) due to internal breakage (relying on jax internals))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,partial rollback of #19096 due to internal breakage (relying on jax internals),"partial rollback of CC(del add_any_p and zeros_like_p, replace avaldispatched traceable) due to internal breakage (relying on jax internals)",2023-12-22T22:46:02Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19105
13916,"以下是一个github上的jax下的一个issue, 标题是(Pallas NotImplementedError: unsupported layout change)， 内容是 ( Description I'm trying to write a simple rnnlike for loop in pallas but get a ```Pallas NotImplementedError: unsupported layout change``` for some reason. If anyone can help fixing this that would be very helpful! The code is:  ``` def pallas_scan(k_ref,v_ref,r_ref,w_ref,u_ref,s_ref,out_ref,s_out_ref,T):     u = u_ref[...]     s = s_ref[...]     def loop(t,s):         k_t = pl.load(k_ref,(t,slice(None)))         v_t = pl.load(v_ref,(t,slice(None)))         r_t = pl.load(r_ref,(t,slice(None)))         w_t = pl.load(w_ref,(t,slice(None)))         kv_t = k_t * v_t         pl.store(out_ref,(t,slice(None)),(r_t * (u * kv_t + s)).squeeze(0))         s = w_t * s + kv_t         return s     s = jax.lax.fori_loop(0,T,loop,s)     pl.store(s_out_ref,(slice(None),slice(None)),s) def rwkv_pallas(k,v,r,w,u,s):     T,D = k.shape     return pl.pallas_call(         ft.partial(jax.jit(pallas_scan,static_argnums=8), T=T),         out_shape=[jax.ShapeDtypeStruct(k.shape, k.dtype),                   jax.ShapeDtypeStruct(s.shape, s.dtype)],         grid=(D,),         in_specs=[           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (0, j), (1, 1)),           pl.BlockSpec(lambda j: (j, j), (1, 1)),         ],         out_specs=[           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (j, j), (1, 1)),         ],       )(k,v,r,w,u,s) ```` And the full error message:  ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) File /usr/local/lib/python3.10/runpy.py:196, in _run_module_as_main()     195     sys.argv[0] = mod_spec.origin > 196 return _run_code(code, main_globals, None,     197                  ""__main__"", mod_spec) File /usr/local/lib/python3.10/runpy.py:86, in _run_code()      79 run_globals.update(__name__ = mod_name,      80                    __file__ = fname,      81                    __cached__ = cached,    (...)      84                    __package__ = pkg_name,      85                    __spec__ = mod_spec) > 86 exec(code, run_globals)      87 return run_globals File /usr/local/lib/python3.10/sitepackages/ipykernel_launcher.py:17      15 from ipykernel import kernelapp as app > 17 app.launch_new_instance() File /usr/local/lib/python3.10/sitepackages/traitlets/config/application.py:1053, in launch_instance()    1052 app.initialize(argv) > 1053 app.start() File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelapp.py:737, in start()     736 try: > 737     self.io_loop.start()     738 except KeyboardInterrupt: File /usr/local/lib/python3.10/sitepackages/tornado/platform/asyncio.py:195, in start()     194 def start(self) > None: > 195     self.asyncio_loop.run_forever() File /usr/local/lib/python3.10/asyncio/base_events.py:603, in run_forever()     602 while True: > 603     self._run_once()     604     if self._stopping: File /usr/local/lib/python3.10/asyncio/base_events.py:1909, in _run_once()    1908     else: > 1909         handle._run()    1910 handle = None File /usr/local/lib/python3.10/asyncio/events.py:80, in _run()      79 try: > 80     self._context.run(self._callback, *self._args)      81 except (SystemExit, KeyboardInterrupt): File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelbase.py:524, in dispatch_queue()     523 try: > 524     await self.process_one()     525 except Exception: File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelbase.py:513, in process_one()     512         return None > 513 await dispatch(*args) File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelbase.py:418, in dispatch_shell()     417     if inspect.isawaitable(result): > 418         await result     419 except Exception: File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelbase.py:758, in execute_request()     757 if inspect.isawaitable(reply_content): > 758     reply_content = await reply_content     760  Flush output before sending the reply. File /usr/local/lib/python3.10/sitepackages/ipykernel/ipkernel.py:426, in do_execute()     425 if with_cell_id: > 426     res = shell.run_cell(     427         code,     428         store_history=store_history,     429         silent=silent,     430         cell_id=cell_id,     431     )     432 else: File /usr/local/lib/python3.10/sitepackages/ipykernel/zmqshell.py:549, in run_cell()     548 self._last_traceback = None > 549 return super().run_cell(*args, **kwargs) File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3046, in run_cell()    3045 try: > 3046     result = self._run_cell(    3047         raw_cell, store_history, silent, shell_futures, cell_id    3048     )    3049 finally: File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3101, in _run_cell()    3100 try: > 3101     result = runner(coro)    3102 except BaseException as e: File /usr/local/lib/python3.10/sitepackages/IPython/core/async_helpers.py:129, in _pseudo_sync_runner()     128 try: > 129     coro.send(None)     130 except StopIteration as exc: File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3306, in run_cell_async()    3303 interactivity = ""none"" if silent else self.ast_node_interactivity > 3306 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3307        interactivity=interactivity, compiler=compiler, result=result)    3309 self.last_execution_succeeded = not has_raised File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3488, in run_ast_nodes()    3487     asy = compare(code) > 3488 if await self.run_code(code, result, async_=asy):    3489     return True File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3548, in run_code()    3547     else: > 3548         exec(code_obj, self.user_global_ns, self.user_ns)    3549 finally:    3550      Reset our crash handler in place Cell In[113], line 1 > 1 rwkv_pallas(k,v,r,w,u,s) Cell In[110], line 3, in rwkv_pallas()       2 T,D = k.shape > 3 return pl.pallas_call(       4     ft.partial(jax.jit(pallas_scan,static_argnums=8), T=T),       5     out_shape=[jax.ShapeDtypeStruct(k.shape, k.dtype),       6               jax.ShapeDtypeStruct(s.shape, s.dtype)],       7     grid=(D,),       8     in_specs=[       9       pl.BlockSpec(lambda j: (0, j), (T, 1)),      10       pl.BlockSpec(lambda j: (0, j), (T, 1)),      11       pl.BlockSpec(lambda j: (0, j), (T, 1)),      12       pl.BlockSpec(lambda j: (0, j), (T, 1)),      13       pl.BlockSpec(lambda j: (0, j), (1, 1)),      14       pl.BlockSpec(lambda j: (j, j), (1, 1)),      15     ],      16     out_specs=[      17       pl.BlockSpec(lambda j: (0, j), (T, 1)),      18       pl.BlockSpec(lambda j: (j, j), (1, 1)),      19     ],      20   )(k,v,r,w,u,s) File /usr/local/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py:383, in wrapped()     382 which_linear = (False,) * len(flat_args) > 383 out_flat = pallas_call_p.bind(     384     *consts, *flat_args, jaxpr=jaxpr, name=name, which_linear=which_linear,     385     in_shapes=tuple(jax.ShapeDtypeStruct(a.shape, a.dtype)     386                     for a in flat_args),     387     out_shapes=tuple(flat_out_shapes), debug=debug,     388     interpret=interpret,     389     grid_mapping=grid_mapping,     390     input_output_aliases=tuple(input_output_aliases.items()),     391     **compiler_params)     392 out = tree_util.tree_unflatten(out_tree, out_flat) JaxStackTraceBeforeTransformation: NotImplementedError: unsupported layout change for vector: VectorLayout(bitwidth=32, offsets=(0, 0), tiling=(2, 128), implicit_dim=None) > VectorLayout(bitwidth=32, offsets=(*, 0), tiling=(8, 128), implicit_dim=None) The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: NotImplementedError                       Traceback (most recent call last) Cell In[113], line 1 > 1 rwkv_pallas(k,v,r,w,u,s) Cell In[110], line 3, in rwkv_pallas(k, v, r, w, u, s)       1 def rwkv_pallas(k,v,r,w,u,s):       2     T,D = k.shape > 3     return pl.pallas_call(       4         ft.partial(jax.jit(pallas_scan,static_argnums=8), T=T),       5         out_shape=[jax.ShapeDtypeStruct(k.shape, k.dtype),       6                   jax.ShapeDtypeStruct(s.shape, s.dtype)],       7         grid=(D,),       8         in_specs=[       9           pl.BlockSpec(lambda j: (0, j), (T, 1)),      10           pl.BlockSpec(lambda j: (0, j), (T, 1)),      11           pl.BlockSpec(lambda j: (0, j), (T, 1)),      12           pl.BlockSpec(lambda j: (0, j), (T, 1)),      13           pl.BlockSpec(lambda j: (0, j), (1, 1)),      14           pl.BlockSpec(lambda j: (j, j), (1, 1)),      15         ],      16         out_specs=[      17           pl.BlockSpec(lambda j: (0, j), (T, 1)),      18           pl.BlockSpec(lambda j: (j, j), (1, 1)),      19         ],      20       )(k,v,r,w,u,s)     [... skipping hidden 17 frame] File /usr/local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py:87, in pallas_call_tpu_lowering_rule(ctx, jaxpr, name, which_linear, grid_mapping, input_output_aliases, in_shapes, out_shapes, debug, interpret, mosaic_params, *in_nodes, **compiler_params)      78 def _lower_fun(*args):      79   return mosaic.as_tpu_kernel(      80       mosaic_module,      81       out_avals,    (...)      85       cost_estimate=mosaic_params.get('cost_estimate', None),      86   )(*extra_args, *args) > 87 return mlir.lower_fun(_lower_fun, multiple_results=True)(      88     ctx, *in_nodes)     [... skipping hidden 5 frame] File /usr/local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py:79, in pallas_call_tpu_lowering_rule.._lower_fun(*args)      78 def _lower_fun(*args): > 79   return mosaic.as_tpu_kernel(      80       mosaic_module,      81       out_avals,      82       backend=ctx.module_context.backend,      83       kernel_name=name,      84       kernel_regeneration_metadata=kernel_regeneration_metadata,      85       cost_estimate=mosaic_params.get('cost_estimate', None),      86   )(*extra_args, *args) File /usr/local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py:339, in as_tpu_kernel(module, out_type, cost_estimate, backend, device_type, kernel_name, kernel_regeneration_metadata)     335 hardware_generation = int(device_kind[len(""TPU v"")])     336 has_communication, has_custom_barrier = tpu.private_has_communication(     337     module.operation     338 ) > 339 lowered_module_asm, constants = _lower_tpu_kernel(     340     module, hardware_generation, device_type=device_type     341 )     342  TODO(amagni): Kernel name and regeneration metadata could alternatively be     343  added as a custom attribute to the MLIR call op rather than including them     344  in the backend_config.     345 return _lowered_as_tpu_kernel(     346     lowered_module_asm,     347     out_type,    (...)     354     cost_estimate=cost_estimate,     355 ) File /usr/local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py:278, in _lower_tpu_kernel(module, hardware_generation, device_type)     276   pipeline.run(module.operation)     277 else: > 278   apply_vector_layout.apply(module, hardware_generation)     279 module.operation.verify()     280 dump_mlir(module, ""after apply vector layout pass"") File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1588, in apply(module, hardware_generation)    1586 if not isinstance(f, func.FuncOp):    1587   raise ValueError(f""Unexpected op in module body: {f.OPERATION_NAME}"") > 1588 apply_layout_func(ctx, f) File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1495, in apply_layout_func(ctx, f)    1488 """"""Rewrites the function according to layout annotations of its operations.    1489     1490 Args:    1491   ctx: The context used for rewriting.    1492   f: An MLIR function to be rewritten.    1493 """"""    1494 (entry_block,) = f.body > 1495 apply_layout_block(ctx, entry_block) File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1501, in apply_layout_block(ctx, block)    1498 def apply_layout_block(ctx: RewriteContext, block: ir.Block):    1499    We'll be modifying the block, so make a list of operations beforehand.    1500   for op in list(block): > 1501     apply_layout_op(ctx, op) File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1550, in apply_layout_op(ctx, op)    1548       continue    1549     with ir.InsertionPoint(op), op.location: > 1550       new_v = relayout(    1551           v, src=lo, dst=li, hw_generation=ctx.hardware_generation    1552       ).result    1553       ctx.set_operand(op, idx, new_v)    1554 else: File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1400, in relayout(v, src, dst, hw_generation)    1398   return assemble(vty, dst, dst_tiles)    1399  TODO(apaszke): Implement general relayout > 1400 raise NotImplementedError(    1401     f""unsupported layout change for {vty}: {src} > {dst}"") NotImplementedError: unsupported layout change for vector: VectorLayout(bitwidth=32, offsets=(0, 0), tiling=(2, 128), implicit_dim=None) > VectorLayout(bitwidth=32, offsets=(*, 0), tiling=(8, 128), implicit_dim=None) ```  What jax/jaxlib version are you using? jax v0.4.21 jaxlib v0.4.21  Which accelerator(s) are you using? TPU  Additional system info? Python 3.10, Kaggle Notebook  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",text generation,Pallas NotImplementedError: unsupported layout change," Description I'm trying to write a simple rnnlike for loop in pallas but get a ```Pallas NotImplementedError: unsupported layout change``` for some reason. If anyone can help fixing this that would be very helpful! The code is:  ``` def pallas_scan(k_ref,v_ref,r_ref,w_ref,u_ref,s_ref,out_ref,s_out_ref,T):     u = u_ref[...]     s = s_ref[...]     def loop(t,s):         k_t = pl.load(k_ref,(t,slice(None)))         v_t = pl.load(v_ref,(t,slice(None)))         r_t = pl.load(r_ref,(t,slice(None)))         w_t = pl.load(w_ref,(t,slice(None)))         kv_t = k_t * v_t         pl.store(out_ref,(t,slice(None)),(r_t * (u * kv_t + s)).squeeze(0))         s = w_t * s + kv_t         return s     s = jax.lax.fori_loop(0,T,loop,s)     pl.store(s_out_ref,(slice(None),slice(None)),s) def rwkv_pallas(k,v,r,w,u,s):     T,D = k.shape     return pl.pallas_call(         ft.partial(jax.jit(pallas_scan,static_argnums=8), T=T),         out_shape=[jax.ShapeDtypeStruct(k.shape, k.dtype),                   jax.ShapeDtypeStruct(s.shape, s.dtype)],         grid=(D,),         in_specs=[           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (0, j), (1, 1)),           pl.BlockSpec(lambda j: (j, j), (1, 1)),         ],         out_specs=[           pl.BlockSpec(lambda j: (0, j), (T, 1)),           pl.BlockSpec(lambda j: (j, j), (1, 1)),         ],       )(k,v,r,w,u,s) ```` And the full error message:  ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) File /usr/local/lib/python3.10/runpy.py:196, in _run_module_as_main()     195     sys.argv[0] = mod_spec.origin > 196 return _run_code(code, main_globals, None,     197                  ""__main__"", mod_spec) File /usr/local/lib/python3.10/runpy.py:86, in _run_code()      79 run_globals.update(__name__ = mod_name,      80                    __file__ = fname,      81                    __cached__ = cached,    (...)      84                    __package__ = pkg_name,      85                    __spec__ = mod_spec) > 86 exec(code, run_globals)      87 return run_globals File /usr/local/lib/python3.10/sitepackages/ipykernel_launcher.py:17      15 from ipykernel import kernelapp as app > 17 app.launch_new_instance() File /usr/local/lib/python3.10/sitepackages/traitlets/config/application.py:1053, in launch_instance()    1052 app.initialize(argv) > 1053 app.start() File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelapp.py:737, in start()     736 try: > 737     self.io_loop.start()     738 except KeyboardInterrupt: File /usr/local/lib/python3.10/sitepackages/tornado/platform/asyncio.py:195, in start()     194 def start(self) > None: > 195     self.asyncio_loop.run_forever() File /usr/local/lib/python3.10/asyncio/base_events.py:603, in run_forever()     602 while True: > 603     self._run_once()     604     if self._stopping: File /usr/local/lib/python3.10/asyncio/base_events.py:1909, in _run_once()    1908     else: > 1909         handle._run()    1910 handle = None File /usr/local/lib/python3.10/asyncio/events.py:80, in _run()      79 try: > 80     self._context.run(self._callback, *self._args)      81 except (SystemExit, KeyboardInterrupt): File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelbase.py:524, in dispatch_queue()     523 try: > 524     await self.process_one()     525 except Exception: File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelbase.py:513, in process_one()     512         return None > 513 await dispatch(*args) File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelbase.py:418, in dispatch_shell()     417     if inspect.isawaitable(result): > 418         await result     419 except Exception: File /usr/local/lib/python3.10/sitepackages/ipykernel/kernelbase.py:758, in execute_request()     757 if inspect.isawaitable(reply_content): > 758     reply_content = await reply_content     760  Flush output before sending the reply. File /usr/local/lib/python3.10/sitepackages/ipykernel/ipkernel.py:426, in do_execute()     425 if with_cell_id: > 426     res = shell.run_cell(     427         code,     428         store_history=store_history,     429         silent=silent,     430         cell_id=cell_id,     431     )     432 else: File /usr/local/lib/python3.10/sitepackages/ipykernel/zmqshell.py:549, in run_cell()     548 self._last_traceback = None > 549 return super().run_cell(*args, **kwargs) File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3046, in run_cell()    3045 try: > 3046     result = self._run_cell(    3047         raw_cell, store_history, silent, shell_futures, cell_id    3048     )    3049 finally: File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3101, in _run_cell()    3100 try: > 3101     result = runner(coro)    3102 except BaseException as e: File /usr/local/lib/python3.10/sitepackages/IPython/core/async_helpers.py:129, in _pseudo_sync_runner()     128 try: > 129     coro.send(None)     130 except StopIteration as exc: File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3306, in run_cell_async()    3303 interactivity = ""none"" if silent else self.ast_node_interactivity > 3306 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3307        interactivity=interactivity, compiler=compiler, result=result)    3309 self.last_execution_succeeded = not has_raised File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3488, in run_ast_nodes()    3487     asy = compare(code) > 3488 if await self.run_code(code, result, async_=asy):    3489     return True File /usr/local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py:3548, in run_code()    3547     else: > 3548         exec(code_obj, self.user_global_ns, self.user_ns)    3549 finally:    3550      Reset our crash handler in place Cell In[113], line 1 > 1 rwkv_pallas(k,v,r,w,u,s) Cell In[110], line 3, in rwkv_pallas()       2 T,D = k.shape > 3 return pl.pallas_call(       4     ft.partial(jax.jit(pallas_scan,static_argnums=8), T=T),       5     out_shape=[jax.ShapeDtypeStruct(k.shape, k.dtype),       6               jax.ShapeDtypeStruct(s.shape, s.dtype)],       7     grid=(D,),       8     in_specs=[       9       pl.BlockSpec(lambda j: (0, j), (T, 1)),      10       pl.BlockSpec(lambda j: (0, j), (T, 1)),      11       pl.BlockSpec(lambda j: (0, j), (T, 1)),      12       pl.BlockSpec(lambda j: (0, j), (T, 1)),      13       pl.BlockSpec(lambda j: (0, j), (1, 1)),      14       pl.BlockSpec(lambda j: (j, j), (1, 1)),      15     ],      16     out_specs=[      17       pl.BlockSpec(lambda j: (0, j), (T, 1)),      18       pl.BlockSpec(lambda j: (j, j), (1, 1)),      19     ],      20   )(k,v,r,w,u,s) File /usr/local/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py:383, in wrapped()     382 which_linear = (False,) * len(flat_args) > 383 out_flat = pallas_call_p.bind(     384     *consts, *flat_args, jaxpr=jaxpr, name=name, which_linear=which_linear,     385     in_shapes=tuple(jax.ShapeDtypeStruct(a.shape, a.dtype)     386                     for a in flat_args),     387     out_shapes=tuple(flat_out_shapes), debug=debug,     388     interpret=interpret,     389     grid_mapping=grid_mapping,     390     input_output_aliases=tuple(input_output_aliases.items()),     391     **compiler_params)     392 out = tree_util.tree_unflatten(out_tree, out_flat) JaxStackTraceBeforeTransformation: NotImplementedError: unsupported layout change for vector: VectorLayout(bitwidth=32, offsets=(0, 0), tiling=(2, 128), implicit_dim=None) > VectorLayout(bitwidth=32, offsets=(*, 0), tiling=(8, 128), implicit_dim=None) The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: NotImplementedError                       Traceback (most recent call last) Cell In[113], line 1 > 1 rwkv_pallas(k,v,r,w,u,s) Cell In[110], line 3, in rwkv_pallas(k, v, r, w, u, s)       1 def rwkv_pallas(k,v,r,w,u,s):       2     T,D = k.shape > 3     return pl.pallas_call(       4         ft.partial(jax.jit(pallas_scan,static_argnums=8), T=T),       5         out_shape=[jax.ShapeDtypeStruct(k.shape, k.dtype),       6                   jax.ShapeDtypeStruct(s.shape, s.dtype)],       7         grid=(D,),       8         in_specs=[       9           pl.BlockSpec(lambda j: (0, j), (T, 1)),      10           pl.BlockSpec(lambda j: (0, j), (T, 1)),      11           pl.BlockSpec(lambda j: (0, j), (T, 1)),      12           pl.BlockSpec(lambda j: (0, j), (T, 1)),      13           pl.BlockSpec(lambda j: (0, j), (1, 1)),      14           pl.BlockSpec(lambda j: (j, j), (1, 1)),      15         ],      16         out_specs=[      17           pl.BlockSpec(lambda j: (0, j), (T, 1)),      18           pl.BlockSpec(lambda j: (j, j), (1, 1)),      19         ],      20       )(k,v,r,w,u,s)     [... skipping hidden 17 frame] File /usr/local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py:87, in pallas_call_tpu_lowering_rule(ctx, jaxpr, name, which_linear, grid_mapping, input_output_aliases, in_shapes, out_shapes, debug, interpret, mosaic_params, *in_nodes, **compiler_params)      78 def _lower_fun(*args):      79   return mosaic.as_tpu_kernel(      80       mosaic_module,      81       out_avals,    (...)      85       cost_estimate=mosaic_params.get('cost_estimate', None),      86   )(*extra_args, *args) > 87 return mlir.lower_fun(_lower_fun, multiple_results=True)(      88     ctx, *in_nodes)     [... skipping hidden 5 frame] File /usr/local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py:79, in pallas_call_tpu_lowering_rule.._lower_fun(*args)      78 def _lower_fun(*args): > 79   return mosaic.as_tpu_kernel(      80       mosaic_module,      81       out_avals,      82       backend=ctx.module_context.backend,      83       kernel_name=name,      84       kernel_regeneration_metadata=kernel_regeneration_metadata,      85       cost_estimate=mosaic_params.get('cost_estimate', None),      86   )(*extra_args, *args) File /usr/local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py:339, in as_tpu_kernel(module, out_type, cost_estimate, backend, device_type, kernel_name, kernel_regeneration_metadata)     335 hardware_generation = int(device_kind[len(""TPU v"")])     336 has_communication, has_custom_barrier = tpu.private_has_communication(     337     module.operation     338 ) > 339 lowered_module_asm, constants = _lower_tpu_kernel(     340     module, hardware_generation, device_type=device_type     341 )     342  TODO(amagni): Kernel name and regeneration metadata could alternatively be     343  added as a custom attribute to the MLIR call op rather than including them     344  in the backend_config.     345 return _lowered_as_tpu_kernel(     346     lowered_module_asm,     347     out_type,    (...)     354     cost_estimate=cost_estimate,     355 ) File /usr/local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py:278, in _lower_tpu_kernel(module, hardware_generation, device_type)     276   pipeline.run(module.operation)     277 else: > 278   apply_vector_layout.apply(module, hardware_generation)     279 module.operation.verify()     280 dump_mlir(module, ""after apply vector layout pass"") File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1588, in apply(module, hardware_generation)    1586 if not isinstance(f, func.FuncOp):    1587   raise ValueError(f""Unexpected op in module body: {f.OPERATION_NAME}"") > 1588 apply_layout_func(ctx, f) File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1495, in apply_layout_func(ctx, f)    1488 """"""Rewrites the function according to layout annotations of its operations.    1489     1490 Args:    1491   ctx: The context used for rewriting.    1492   f: An MLIR function to be rewritten.    1493 """"""    1494 (entry_block,) = f.body > 1495 apply_layout_block(ctx, entry_block) File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1501, in apply_layout_block(ctx, block)    1498 def apply_layout_block(ctx: RewriteContext, block: ir.Block):    1499    We'll be modifying the block, so make a list of operations beforehand.    1500   for op in list(block): > 1501     apply_layout_op(ctx, op) File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1550, in apply_layout_op(ctx, op)    1548       continue    1549     with ir.InsertionPoint(op), op.location: > 1550       new_v = relayout(    1551           v, src=lo, dst=li, hw_generation=ctx.hardware_generation    1552       ).result    1553       ctx.set_operand(op, idx, new_v)    1554 else: File /usr/local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py:1400, in relayout(v, src, dst, hw_generation)    1398   return assemble(vty, dst, dst_tiles)    1399  TODO(apaszke): Implement general relayout > 1400 raise NotImplementedError(    1401     f""unsupported layout change for {vty}: {src} > {dst}"") NotImplementedError: unsupported layout change for vector: VectorLayout(bitwidth=32, offsets=(0, 0), tiling=(2, 128), implicit_dim=None) > VectorLayout(bitwidth=32, offsets=(*, 0), tiling=(8, 128), implicit_dim=None) ```  What jax/jaxlib version are you using? jax v0.4.21 jaxlib v0.4.21  Which accelerator(s) are you using? TPU  Additional system info? Python 3.10, Kaggle Notebook  NVIDIA GPU info _No response_",2023-12-20T04:31:28Z,bug pallas,open,0,0,https://github.com/jax-ml/jax/issues/19058
968,"以下是一个github上的jax下的一个issue, 标题是(GPU memory allocation not working as expected)， 内容是 ( Description When using the GPU memory allocation environment variables described here, I get unexpected results. ``XLA_PYTHON_CLIENT_PREALLOCATE=false`` still preallocates memory and ``XLA_PYTHON_CLIENT_MEM_FRACTION=.99`` does not preallocate 99% of memory. Code to reproduce (test.py): ```python import jax import jax.numpy as jnp print(jax.devices('gpu')[0]) matrix_shape = (1000, 1000) while True:     for i in range(20_000):         key = jax.random.PRNGKey(i)         a_key, b_key = jax.random.split(key, 2)         a = jax.random.normal(a_key, matrix_shape)         b = jax.random.normal(b_key, matrix_shape)         c = a @ b ``` When run with ``XLA_PYTHON_CLIENT_PREALLOCATE=false python3.10 test.py``, nvidiasmi shows: ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,GPU memory allocation not working as expected," Description When using the GPU memory allocation environment variables described here, I get unexpected results. ``XLA_PYTHON_CLIENT_PREALLOCATE=false`` still preallocates memory and ``XLA_PYTHON_CLIENT_MEM_FRACTION=.99`` does not preallocate 99% of memory. Code to reproduce (test.py): ```python import jax import jax.numpy as jnp print(jax.devices('gpu')[0]) matrix_shape = (1000, 1000) while True:     for i in range(20_000):         key = jax.random.PRNGKey(i)         a_key, b_key = jax.random.split(key, 2)         a = jax.random.normal(a_key, matrix_shape)         b = jax.random.normal(b_key, matrix_shape)         c = a @ b ``` When run with ``XLA_PYTHON_CLIENT_PREALLOCATE=false python3.10 test.py``, nvidiasmi shows: ``` ++  ++ ```",2023-12-18T21:41:11Z,bug,closed,1,2,https://github.com/jax-ml/jax/issues/19035,It looks like this has regressed in jax/jaxlib 0.4.23. The workaround for the moment is probably to revert to v0.4.20.,"I've submitted a fix, but it will need a new release. CC([JAX:GPU] Add a test that verifies that the XLA_PYTHON_CLIENT_PREALLOCATE environment variable is parsed correctly.) adds a test so this doesn't regress again."
2999,"以下是一个github上的jax下的一个issue, 标题是(Error reading persistent compilation cache entry: Instruction name is not unique)， 内容是 ( Description I am encountering this error while trying to run a function which I have a compiled executable for in the persistent compile cache.  It only seems to happen on larger batch sizes/resolutions (the script I am using operates on ~40 different input shapes with a fairly wide range of total area, on some batch sizes only half of my inputs will be affected).  Functions work absolutely fine regardless of size once compiled, but cannot be loaded from cache.  This happens with jit compilation and AOT compilation.  I have also experienced the same issue on both CUDA 11 and CUDA 12 (currently using 12) versions of JAX and at least as far back as 0.4.18. I suspect that it may be related to specific CUDA kernels being selected during benchmarking since it only happens on larger input sizes? I can provide HLO dumps or other diagnostic info if needed. Here is the error that shows up (empty lines removed).  The name of the constant is not always the same. ``` E external/xla/xla/status_macros.cc:54] INTERNAL: RET_CHECK failure (external/xla/xla/hlo/ir/hlo_module.cc:521) !ContainsKey(instruction_names, instruction>name()) Instruction name is not unique: constant_29831 *** Begin stack trace ***         _PyObject_MakeTpCall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyObject_MakeTpCall         _PyObject_MakeTpCall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Vectorcall         PyObject_Vectorcall         _PyEval_EvalFrameDefault         PyEval_EvalCode         _PyRun_SimpleFileObject         _PyRun_AnyFileObject         Py_RunMain         Py_BytesMain         __libc_start_main *** End stack trace *** /mnt/sonic/conda/jax_sd_test/lib/python3.11/sitepackages/jax/_src/compiler.py:354: UserWarning: Error reading persistent compilation cache entry for 'jit_train_step': XlaRuntimeError: INTERNAL: RET_CHECK failure (external/xla/xla/hlo/ir/hlo_module.cc:521) !ContainsKey(instruction_names, instruction>name()) Instruction name is not unique: constant_29831   warnings.warn( ```  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.11.7  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Error reading persistent compilation cache entry: Instruction name is not unique," Description I am encountering this error while trying to run a function which I have a compiled executable for in the persistent compile cache.  It only seems to happen on larger batch sizes/resolutions (the script I am using operates on ~40 different input shapes with a fairly wide range of total area, on some batch sizes only half of my inputs will be affected).  Functions work absolutely fine regardless of size once compiled, but cannot be loaded from cache.  This happens with jit compilation and AOT compilation.  I have also experienced the same issue on both CUDA 11 and CUDA 12 (currently using 12) versions of JAX and at least as far back as 0.4.18. I suspect that it may be related to specific CUDA kernels being selected during benchmarking since it only happens on larger input sizes? I can provide HLO dumps or other diagnostic info if needed. Here is the error that shows up (empty lines removed).  The name of the constant is not always the same. ``` E external/xla/xla/status_macros.cc:54] INTERNAL: RET_CHECK failure (external/xla/xla/hlo/ir/hlo_module.cc:521) !ContainsKey(instruction_names, instruction>name()) Instruction name is not unique: constant_29831 *** Begin stack trace ***         _PyObject_MakeTpCall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyObject_MakeTpCall         _PyObject_MakeTpCall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Vectorcall         PyObject_Vectorcall         _PyEval_EvalFrameDefault         PyEval_EvalCode         _PyRun_SimpleFileObject         _PyRun_AnyFileObject         Py_RunMain         Py_BytesMain         __libc_start_main *** End stack trace *** /mnt/sonic/conda/jax_sd_test/lib/python3.11/sitepackages/jax/_src/compiler.py:354: UserWarning: Error reading persistent compilation cache entry for 'jit_train_step': XlaRuntimeError: INTERNAL: RET_CHECK failure (external/xla/xla/hlo/ir/hlo_module.cc:521) !ContainsKey(instruction_names, instruction>name()) Instruction name is not unique: constant_29831   warnings.warn( ```  What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.11.7  ++++ ```",2023-12-18T07:29:04Z,bug needs info NVIDIA GPU,open,0,3,https://github.com/jax-ml/jax/issues/19028,"An HLO dump would be helpful, please. Failing that, some other sort of reproducer would be necessary.","576x576x4_working_lowered.txt 576x576x16_bugged_lowered.txt Here are the lowered forms of the function, one on an input size where the function works as expected, and one on an input size where the function fails to save to the compile cache correctly. I have the compiled forms of the function saved, but they are over 500MB each.  They will be difficult to upload, so tell me if you do explicitly need them.  I did search for the misbehaving constant which in this instance is `constant_29335`, and did in fact find two instances of it in the compiled function: ```   %constant_29335 = bf16[] constant(0.5), metadata={op_name=""jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/down_blocks_0/attentions_0/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/down_blocks_0/attentions_1/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/up_blocks_3/attentions_0/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/up_blocks_3/attentions_1/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/up_blocks_3/attentions_2/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/up_blocks_3/attentions_2/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/up_blocks_3/attentions_1/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/up_blocks_3/attentions_0/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/down_blocks_0/attentions_1/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/down_blocks_0/attentions_0/transformer_blocks_0/ff/net_0/mul""}   %convert.27249 = f32[] convert(bf16[] %constant_29335)   %constant_29335 = bf16[] constant(0.5), metadata={op_name=""jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/down_blocks_0/attentions_0/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/down_blocks_0/attentions_1/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/up_blocks_3/attentions_0/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/up_blocks_3/attentions_1/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/jvp(FlaxUNet2DConditionModel)/up_blocks_3/attentions_2/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/up_blocks_3/attentions_2/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/up_blocks_3/attentions_1/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/up_blocks_3/attentions_0/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/down_blocks_0/attentions_1/transformer_blocks_0/ff/net_0/mul;jit(train_step)/jit(main)/transpose(jvp(FlaxUNet2DConditionModel))/down_blocks_0/attentions_0/transformer_blocks_0/ff/net_0/mul""}   %convert.34794 = f32[] convert(bf16[] %constant_29335) ```","I am having the same issue,   did you solve it?"
1281,"以下是一个github上的jax下的一个issue, 标题是(Random spaces are added when printing `Array`)， 内容是 ( Description It seems some random spaces are added when printing arrays using both the built in `print()` and  `jax.debug.print`. A minimal, reproducible code example is provided below: ```python import jax from jax import random for i in range(3):     key = random.PRNGKey(i)     x = random.normal(key, (2, 1))     print(x)     jax.debug.print(""{}"", x) ``` The output exhibits the issue, showing extra spaces: ```shell [[0.78476596]  [ 0.85644484]] [[0.78476596]  [ 0.85644484]] [[0.11617039]  [ 2.2125063 ]] [[0.11617039]  [ 2.2125063 ]] [[1.0433589 ]  [ 0.71743053]] [[1.0433589 ]  [ 0.71743053]] ```  What jax/jaxlib version are you using? jax: 0.4.23 jaxlib: 0.4.23  Which accelerator(s) are you using? CPU  Additional system info?  numpy: 1.26.2  python: 3.11.6 (main, Oct  2 2023, 13:45:54) [Clang 15.0.0 (clang1500.0.40.1)]  platform uname:     system: 'Darwin'    release: '23.2.0'    version: 'Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu10002.61.3~2/RELEASE_ARM64_T6000'    machine: 'arm64'  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Random spaces are added when printing `Array`," Description It seems some random spaces are added when printing arrays using both the built in `print()` and  `jax.debug.print`. A minimal, reproducible code example is provided below: ```python import jax from jax import random for i in range(3):     key = random.PRNGKey(i)     x = random.normal(key, (2, 1))     print(x)     jax.debug.print(""{}"", x) ``` The output exhibits the issue, showing extra spaces: ```shell [[0.78476596]  [ 0.85644484]] [[0.78476596]  [ 0.85644484]] [[0.11617039]  [ 2.2125063 ]] [[0.11617039]  [ 2.2125063 ]] [[1.0433589 ]  [ 0.71743053]] [[1.0433589 ]  [ 0.71743053]] ```  What jax/jaxlib version are you using? jax: 0.4.23 jaxlib: 0.4.23  Which accelerator(s) are you using? CPU  Additional system info?  numpy: 1.26.2  python: 3.11.6 (main, Oct  2 2023, 13:45:54) [Clang 15.0.0 (clang1500.0.40.1)]  platform uname:     system: 'Darwin'    release: '23.2.0'    version: 'Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu10002.61.3~2/RELEASE_ARM64_T6000'    machine: 'arm64'  NVIDIA GPU info _No response_",2023-12-16T15:22:25Z,bug needs info,closed,0,2,https://github.com/jax-ml/jax/issues/19018,"JAX delegates its array printing to NumPy, so this is how NumPy would format these arrays. Can you say more about why this is a bug? It looks like it's working as intended to me: the printing is aligning the brackets and the leading digits.","Certainly, as you mentioned, spaces were simply added to align the appearance. I simply misunderstood since I ""noticed"" this behavior when printing an array reshaped with `.reshape(1)`. I apologize for any confusion."
904,"以下是一个github上的jax下的一个issue, 标题是(Unable to load the Mac M2 version of Tensor)， 内容是 ( Description WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found  What jax/jaxlib version are you using? 0.4.10  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.13 (main, Aug 24 2023, 22:36:46) [Clang 14.0.3 (clang1403.0.22.14.1)] uname_result(system='Darwin', node='JamessMacBookAir.local', release='23.3.0', version='Darwin Kernel Version 23.3.0: Fri Dec  1 03:20:24 PST 2023; root:xnu10002.80.11~58/RELEASE_ARM64_T8112', machine='arm64')  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Unable to load the Mac M2 version of Tensor," Description WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found  What jax/jaxlib version are you using? 0.4.10  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.13 (main, Aug 24 2023, 22:36:46) [Clang 14.0.3 (clang1403.0.22.14.1)] uname_result(system='Darwin', node='JamessMacBookAir.local', release='23.3.0', version='Darwin Kernel Version 23.3.0: Fri Dec  1 03:20:24 PST 2023; root:xnu10002.80.11~58/RELEASE_ARM64_T8112', machine='arm64')  NVIDIA GPU info _No response_",2023-12-16T03:51:40Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/19015,Can you include some information about what code you were executing when you saw this error? Thanks!
6431,"以下是一个github上的jax下的一个issue, 标题是(CUDA error when importing torch before creating any jax.numpy.array)， 内容是 ( Description If pytorch is imported before creating a jax.numpy.array then jax can't use cuda. ``` if __name__ == '__main__':     import jax.numpy as jnp     import torch     a = jnp.array([1, 2, 3])     print(a.devices())  CPU     print(torch.backends.cudnn.version())  8500  ``` will fail with `CUDA backend failed to initialize: Found cuDNN version 8500, but JAX was built against version 8600, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` while  ``` if __name__ == '__main__':     import jax.numpy as jnp     a = jnp.array([1, 2, 3])     print(a.devices())  GPU     import torch     print(torch.backends.cudnn.version())  8906  ``` will work with only a mildly annoying warning `The NVIDIA driver's CUDA version is 11.7 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages.` I don't know how reproducible this is outside of my own setting as I'm working on a server of which I don't have privileges to update. We are running `NVIDIASMI 515.76       Driver Version: 515.76       CUDA Version: 11.7  ` (output of `nvidiasmi`). Jax was installed using `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`, which gives a warning `WARNING: jax 0.4.23 does not provide the extra 'cuda11pip'` but otherwise runs fine. Pytorch was installed using `pip3 install torch torchvision torchaudio indexurl https://download.pytorch.org/whl/cu117` `LD_LIBRARY_PATH` is empty. If it's of any help the output of `pip list` is below (all the nvidia* packages where installed by the pip install jax command)  Let me know if I can give any more info! ``` Package                  Version   abslpy                  2.0.0 appdirs                  1.4.4 asttokens                2.4.1 attrs                    23.1.0 certifi                  2022.12.7 charsetnormalizer       2.1.1 chex                     0.1.85 click                    8.1.7 cmake                    3.25.0 comm                     0.2.0 contextlib2              21.6.0 contourpy                1.2.0 cycler                   0.12.1 dataclasses              0.6 debugpy                  1.8.0 decorator                4.4.2 dmhaiku                 0.0.11 dockerpycreds           0.4.0 e3nnjax                 0.20.3 einops                   0.7.0 etils                    1.6.0 exceptiongroup           1.2.0 executing                2.0.1 filelock                 3.9.0 flax                     0.7.5 fonttools                4.46.0 fsspec                   2023.12.1 gitdb                    4.0.11 GitPython                3.1.40 h5py                     3.10.0 idna                     3.4 imageio                  2.33.1 imageioffmpeg           0.4.9 importlibresources      6.1.1 ipykernel                6.27.1 ipython                  8.18.1 jax                      0.4.23 jaxmd                   0.2.8 jaxlib                   0.4.23+cuda11.cudnn86 jaxopt                   0.8.2 jedi                     0.19.1 Jinja2                   3.1.2 jmp                      0.0.4 jraph                    0.0.6.dev0 jupyter_client           8.6.0 jupyter_core             5.5.0 kiwisolver               1.4.5 lit                      15.0.7 markdownitpy           3.0.0 MarkupSafe               2.1.3 matplotlib               3.8.2 matplotlibinline        0.1.6 mdurl                    0.1.2 mlcollections           0.1.1 mldtypes                0.3.1 moviepy                  1.0.3 mpmath                   1.3.0 msgpack                  1.0.7 nestasyncio             1.5.8 networkx                 3.0 numpy                    1.26.2 nvidiacublascu11       11.11.3.6 nvidiacudacupticu11   11.8.87 nvidiacudanvcccu11    11.8.89 nvidiacudanvrtccu11   11.8.89 nvidiacudaruntimecu11 11.8.89 nvidiacudnncu11        8.9.6.50 nvidiacufftcu11        10.9.0.58 nvidiacusolvercu11     11.4.1.48 nvidiacusparsecu11     11.7.5.86 nvidiancclcu11         2.19.3 opteinsum               3.3.0 optax                    0.1.7 orbaxcheckpoint         0.4.7 packaging                23.2 pandas                   2.1.4 parso                    0.8.3 pexpect                  4.8.0 pickleshare              0.7.5 Pillow                   9.3.0 pip                      23.3.1 platformdirs             4.1.0 plotly                   5.18.0 proglog                  0.1.10 prompttoolkit           3.0.41 protobuf                 4.25.1 psutil                   5.9.6 ptyprocess               0.7.0 pureeval                0.2.2 Pygments                 2.17.2 pyparsing                3.1.1 pythondateutil          2.8.2 pytz                     2023.3.post1 PyYAML                   6.0.1 pyzmq                    25.1.2 requests                 2.28.1 rich                     13.7.0 scipy                    1.11.4 seaborn                  0.13.0 sentrysdk               1.39.1 setproctitle             1.3.3 setuptools               68.2.2 six                      1.16.0 smmap                    5.0.1 stackdata               0.6.2 sympy                    1.12 tabulate                 0.9.0 tenacity                 8.2.3 tensorstore              0.1.51 toolz                    0.12.0 torch                    2.0.1+cu117 torchaudio               2.0.2+cu117 torchvision              0.15.2+cu117 tornado                  6.4 tqdm                     4.66.1 traitlets                5.14.0 triton                   2.0.0 typing_extensions        4.9.0 tzdata                   2023.3 urllib3                  1.26.13 wandb                    0.16.1 wcwidth                  0.2.12 wheel                    0.42.0 zipp                     3.17.0 ```  What jax/jaxlib version are you using? jax==0.4.23, jaxlib==0.4.23  Which accelerator(s) are you using? NVIDIA GPU  Additional system info? Server running linux: >>> import platform, sys, numpy; print(numpy.__version__); print(sys.version); print(platform.uname()) 1.26.2 3.10.13  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,CUDA error when importing torch before creating any jax.numpy.array," Description If pytorch is imported before creating a jax.numpy.array then jax can't use cuda. ``` if __name__ == '__main__':     import jax.numpy as jnp     import torch     a = jnp.array([1, 2, 3])     print(a.devices())  CPU     print(torch.backends.cudnn.version())  8500  ``` will fail with `CUDA backend failed to initialize: Found cuDNN version 8500, but JAX was built against version 8600, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` while  ``` if __name__ == '__main__':     import jax.numpy as jnp     a = jnp.array([1, 2, 3])     print(a.devices())  GPU     import torch     print(torch.backends.cudnn.version())  8906  ``` will work with only a mildly annoying warning `The NVIDIA driver's CUDA version is 11.7 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages.` I don't know how reproducible this is outside of my own setting as I'm working on a server of which I don't have privileges to update. We are running `NVIDIASMI 515.76       Driver Version: 515.76       CUDA Version: 11.7  ` (output of `nvidiasmi`). Jax was installed using `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`, which gives a warning `WARNING: jax 0.4.23 does not provide the extra 'cuda11pip'` but otherwise runs fine. Pytorch was installed using `pip3 install torch torchvision torchaudio indexurl https://download.pytorch.org/whl/cu117` `LD_LIBRARY_PATH` is empty. If it's of any help the output of `pip list` is below (all the nvidia* packages where installed by the pip install jax command)  Let me know if I can give any more info! ``` Package                  Version   abslpy                  2.0.0 appdirs                  1.4.4 asttokens                2.4.1 attrs                    23.1.0 certifi                  2022.12.7 charsetnormalizer       2.1.1 chex                     0.1.85 click                    8.1.7 cmake                    3.25.0 comm                     0.2.0 contextlib2              21.6.0 contourpy                1.2.0 cycler                   0.12.1 dataclasses              0.6 debugpy                  1.8.0 decorator                4.4.2 dmhaiku                 0.0.11 dockerpycreds           0.4.0 e3nnjax                 0.20.3 einops                   0.7.0 etils                    1.6.0 exceptiongroup           1.2.0 executing                2.0.1 filelock                 3.9.0 flax                     0.7.5 fonttools                4.46.0 fsspec                   2023.12.1 gitdb                    4.0.11 GitPython                3.1.40 h5py                     3.10.0 idna                     3.4 imageio                  2.33.1 imageioffmpeg           0.4.9 importlibresources      6.1.1 ipykernel                6.27.1 ipython                  8.18.1 jax                      0.4.23 jaxmd                   0.2.8 jaxlib                   0.4.23+cuda11.cudnn86 jaxopt                   0.8.2 jedi                     0.19.1 Jinja2                   3.1.2 jmp                      0.0.4 jraph                    0.0.6.dev0 jupyter_client           8.6.0 jupyter_core             5.5.0 kiwisolver               1.4.5 lit                      15.0.7 markdownitpy           3.0.0 MarkupSafe               2.1.3 matplotlib               3.8.2 matplotlibinline        0.1.6 mdurl                    0.1.2 mlcollections           0.1.1 mldtypes                0.3.1 moviepy                  1.0.3 mpmath                   1.3.0 msgpack                  1.0.7 nestasyncio             1.5.8 networkx                 3.0 numpy                    1.26.2 nvidiacublascu11       11.11.3.6 nvidiacudacupticu11   11.8.87 nvidiacudanvcccu11    11.8.89 nvidiacudanvrtccu11   11.8.89 nvidiacudaruntimecu11 11.8.89 nvidiacudnncu11        8.9.6.50 nvidiacufftcu11        10.9.0.58 nvidiacusolvercu11     11.4.1.48 nvidiacusparsecu11     11.7.5.86 nvidiancclcu11         2.19.3 opteinsum               3.3.0 optax                    0.1.7 orbaxcheckpoint         0.4.7 packaging                23.2 pandas                   2.1.4 parso                    0.8.3 pexpect                  4.8.0 pickleshare              0.7.5 Pillow                   9.3.0 pip                      23.3.1 platformdirs             4.1.0 plotly                   5.18.0 proglog                  0.1.10 prompttoolkit           3.0.41 protobuf                 4.25.1 psutil                   5.9.6 ptyprocess               0.7.0 pureeval                0.2.2 Pygments                 2.17.2 pyparsing                3.1.1 pythondateutil          2.8.2 pytz                     2023.3.post1 PyYAML                   6.0.1 pyzmq                    25.1.2 requests                 2.28.1 rich                     13.7.0 scipy                    1.11.4 seaborn                  0.13.0 sentrysdk               1.39.1 setproctitle             1.3.3 setuptools               68.2.2 six                      1.16.0 smmap                    5.0.1 stackdata               0.6.2 sympy                    1.12 tabulate                 0.9.0 tenacity                 8.2.3 tensorstore              0.1.51 toolz                    0.12.0 torch                    2.0.1+cu117 torchaudio               2.0.2+cu117 torchvision              0.15.2+cu117 tornado                  6.4 tqdm                     4.66.1 traitlets                5.14.0 triton                   2.0.0 typing_extensions        4.9.0 tzdata                   2023.3 urllib3                  1.26.13 wandb                    0.16.1 wcwidth                  0.2.12 wheel                    0.42.0 zipp                     3.17.0 ```  What jax/jaxlib version are you using? jax==0.4.23, jaxlib==0.4.23  Which accelerator(s) are you using? NVIDIA GPU  Additional system info? Server running linux: >>> import platform, sys, numpy; print(numpy.__version__); print(sys.version); print(platform.uname()) 1.26.2 3.10.13  ++",2023-12-15T21:27:19Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/19004,"There's nothing we can do about this. If you have two libraries that use cudnn, the first one to load it ""wins"". If you load torch first, you end up with the older cudnn version loaded by your torch version. JAX loads cudnn when the backend is initialized, which happens during the first op usually. Calling `jax.devices()` would work also. However, cudnn is usually backward compatible, so you can probably just work around by loading JAX first. Hope that helps!","Oh I see, thanks for the answer. Is there any way to ask jax which cuda/cudnn is it using and where is it located in the system? Something like `torch.backends.cudnn.version()`.","Hi ,  You can use alternative solution, look for CUDA and cuDNN versions in dependencies: `pip show jaxlib `  OR `pip show jaxlib | grep cudatoolkit` You can find below code snippet to point out the CUDA installation directory: ``` import os print(os.environ.get(""CUDA_HOME""))   Check CUDA_HOME print(os.environ.get(""LD_LIBRARY_PATH""))   Check LD_LIBRARY_PATH (Linux/macOS) print(os.environ.get(""PATH""))   Check PATH (Windows) ``` Hope this helps!","Hey , thanks for answering. I'm not sure those commands work (maybe because of how the cluster is set up or because of micromamba). The variable are empty but jax still works on the GPU so cuda must be in use. !image"
757,"以下是一个github上的jax下的一个issue, 标题是(Make jax versions 0.2.* available on Google Storage)， 内容是 (   I am looking at the Google Storage bucket linked above and I don't see anything hosted for Jax ```0.2.*```.  The LRA codebase raises an error indicating a breaking change when I used Jax ```0.3.*```, and the LRA repo provides a looselyspecified dependency of ```jax>=0.2.4```, so including Jax ```0.2.*``` on the aforementioned Google Storage bucket would be very helpful.  Can you make these versions available as well? Thank you! _Originally posted by  in https://github.com/google/jax/issues/18368issuecomment1857432763_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Make jax versions 0.2.* available on Google Storage,"   I am looking at the Google Storage bucket linked above and I don't see anything hosted for Jax ```0.2.*```.  The LRA codebase raises an error indicating a breaking change when I used Jax ```0.3.*```, and the LRA repo provides a looselyspecified dependency of ```jax>=0.2.4```, so including Jax ```0.2.*``` on the aforementioned Google Storage bucket would be very helpful.  Can you make these versions available as well? Thank you! _Originally posted by  in https://github.com/google/jax/issues/18368issuecomment1857432763_",2023-12-15T07:56:13Z,question,closed,0,3,https://github.com/jax-ml/jax/issues/18993,"Hi  all previous jaxlib versions are available at https://storage.googleapis.com/jaxreleases/jax_releases.html; you can find information on installing them at https://jax.readthedocs.io/en/latest/installation.htmlinstallingolderjaxlibwheels Note that `jaxlib` 0.2.X does not exist: prior to 0.3.0, the jax and jaxlib versioning scheme did not match, and the last jaxlib version released before 0.3.0 was was (I believe) 0.1.76. Finding matching versions from that era of jax/jaxlib is a bit difficult, but from the release dates it looks like jax v0.2.4 is likely compaible with jaxlib v0.1.56 or so. So it sounds like you want something like this: ``` pip install jax==0.2.4 jaxlib==0.1.56 f https://storage.googleapis.com/jaxreleases/jax_releases.html ``` Note however that this jaxlib wheel is only available on linux, and only for Python 3.6, 3.7, and 3.8 (which were the three newest Python versions 3+ years ago when this version of jax was released). Best of luck!","Closing, but feel free to comment if Jake's answer didn't solve the problem!","The issue is resolved, thanks!  I had been under the impression the versions were synchronized the whole time. This, combined with the numbering jump of Jaxlib versions in the GCS bucket and a Jaxspecific pip error message, led me to believe Jax 0.2.* wasn't on PyPI or GCS. In fact, while not on GCS, Jax 0.2.* is available on PyPI still.  Editing to add: Having gotten everything to work on CPU, I also got everything to work on TPU v3 just now. For this, I also had to install the correct libtpunightly version, which I located by using the timestamps in the Jax changelog, and appending ```f https://storage.googleapis.com/jaxreleases/libtpu_releases.html```"
23984,"以下是一个github上的jax下的一个issue, 标题是(ROCM build fails when the ROCM_PATH is a symlinked location)， 内容是 ( Description The jaxlib build with ROCm will fail if the ROCM_PATH is directed to a symlinked location. This issue is particularly critical for NixOS, as the ROCM_PATH is invariably a symlinked location joining each individual library together, thereby making ROCM build on NixOS unfeasible. While this may be less significant for other distributions, it is still a common issue, given that /opt/rocm is likely to be a symlink. To reproduce this behavior, start from a ROCM container `rocm/devubuntu22.04:5.7.1complete` and run the following commands to prepare the build environment. ``` apt update apt install g++ python3dev pythonispython3 git pip install numpy wheel build git clone branch jaxlibv0.4.22 https://github.com/google/jax.git cd jax ``` In this environment the realpath for ROCM library is `/opt/rocm5.7.1` and there is also a symlink `/opt/rocm > /opt/rocm5.7.1` Now build jaxlib with `python build/build.py enable_rocm rocm_path=/opt/rocm` will result in a failure, but`python build/build.py enable_rocm rocm_path=/opt/rocm5.7.1` won't. Build log ``` Bazel binary path: ./bazel6.1.2linuxx86_64 Bazel version: 6.1.2 Python binary path: /usr/bin/python3 Python version: 3.10 NumPy version: 1.26.2 MKLDNN enabled: yes Target CPU: x86_64 Target CPU features: release CUDA enabled: no ROCm enabled: yes ROCm toolkit path: /opt/rocm ROCm amdgpu targets: gfx900,gfx906,gfx908,gfx90a,gfx1030 Building XLA and installing it in the jaxlib source tree... ./bazel6.1.2linuxx86_64 run verbose_failures=true //jaxlib/tools:build_wheel  output_path=/jax/dist cpu=x86_64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /jax/.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false INFO: Reading rc options for 'run' from /jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/usr/bin/python3 action_env=PYENV_ROOT python_path=/usr/bin/python3 action_env ROCM_PATH=/opt/rocm config=avx_posix config=mkl_open_source_only config=rocm INFO: Found applicable config definition build:short_logs in file /jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /jax/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:rocm in file /jax/.bazelrc: crosstool_top=//crosstool:toolchain define=using_rocm=true define=using_rocm_hipcc=true //xla/python:enable_gpu=true define=xla_python_enable_gpu=true repo_env TF_NEED_ROCM=1 action_env TF_ROCM_AMDGPU_TARGETS=gfx900,gfx906,gfx908,gfx90a,gfx1030 INFO: Found applicable config definition build:rocm in file /jax/.jax_configure.bazelrc: action_env TF_ROCM_AMDGPU_TARGETS=gfx900,gfx906,gfx908,gfx90a,gfx1030 INFO: Found applicable config definition build:linux in file /jax/.bazelrc: config=posix copt=Wnounknownwarningoption copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 Loading:  Loading:  Loading: 0 packages loaded Analyzing: target //jaxlib/tools:build_wheel (0 packages loaded, 0 targets configured) INFO: Analyzed target //jaxlib/tools:build_wheel (0 packages loaded, 0 targets configured). INFO: Found 1 target... [0 / 1] [Prepa] BazelWorkspaceStatusAction stablestatus.txt [34 / 291] Compiling llvm/lib/Support/DebugCounter.cpp [for tool]; 0s local ... (16 actions, 15 running) [50 / 291] Compiling llvm/lib/Support/Timer.cpp [for tool]; 1s local ... (16 actions, 15 running) ... [5,531 / 7,541] Compiling lib/Dialect/NVGPU/IR/Dialect.cpp; 20s local ... (16 actions running) ERROR: /root/.cache/bazel/_bazel_root/bee4ad1fd43279be7a03b33426e824d5/external/xla/xla/service/gpu/BUILD:1196:23: Compiling xla/service/gpu/cub_sort_kernel.cu.: undeclared inclusion(s) in rule '//xla/service/gpu:cub_sort_kernel_f64': this rule is missing dependency declarations for the following files included by 'xla/service/gpu/cub_sort_kernel.cu.cc':   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_runtime_wrapper.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/cuda_wrappers/cmath'   '/opt/rocm/llvm/lib/clang/17.0.0/include/stddef.h'   '/opt/rocm5.7.1/include/hip/hip_version.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_libdevice_declares.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_math.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/cuda_wrappers/algorithm'   '/opt/rocm/llvm/lib/clang/17.0.0/include/cuda_wrappers/new'   '/opt/rocm/llvm/lib/clang/17.0.0/include/limits.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/stdint.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_stdlib.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_cuda_math_forward_declares.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_cmath.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_cuda_complex_builtins.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/cuda_wrappers/complex'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__stddef_max_align_t.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/stdarg.h'   '/opt/rocm5.7.1/include/hip/hip_runtime.h'   '/opt/rocm5.7.1/include/hip/hip_common.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_runtime.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_common.h'   '/opt/rocm5.7.1/include/hip/hip_runtime_api.h'   '/opt/rocm5.7.1/include/hip/amd_detail/host_defines.h'   '/opt/rocm5.7.1/include/hip/driver_types.h'   '/opt/rocm5.7.1/include/hip/texture_types.h'   '/opt/rocm5.7.1/include/hip/channel_descriptor.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_channel_descriptor.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_vector_types.h'   '/opt/rocm5.7.1/include/hip/surface_types.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_runtime_pt_api.h'   '/opt/rocm5.7.1/include/hip/amd_detail/hip_ldg.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_atomic.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_device_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/math_fwd.h'   '/opt/rocm5.7.1/include/hip/hip_vector_types.h'   '/opt/rocm5.7.1/include/hip/amd_detail/device_library_decls.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_warp_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_unsafe_atomics.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_surface_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/texture_fetch_functions.h'   '/opt/rocm5.7.1/include/hip/hip_texture_types.h'   '/opt/rocm5.7.1/include/hip/amd_detail/ockl_image.h'   '/opt/rocm5.7.1/include/hip/amd_detail/texture_indirect_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_math_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/hip_fp16_math_fwd.h'   '/opt/rocm5.7.1/include/hip/library_types.h'   '/opt/rocm5.7.1/include/hip/hip_fp16.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_fp16.h'   '/opt/rocm5.7.1/include/hip/hip_bfloat16.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_bfloat16.h'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/types/future_value.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/types/double_buffer.hpp'   '/opt/rocm5.7.1/include/rocprim/types/integer_sequence.hpp'   '/opt/rocm5.7.1/include/rocprim/types/key_value_pair.hpp'   '/opt/rocm5.7.1/include/rocprim/types/tuple.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/all_true.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_adjacent_difference.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_adjacent_difference_impl.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_discontinuity.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_exchange.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/atomic.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/bit.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/warp.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/warp_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/warp_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_histogram.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_histogram_atomic.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_histogram_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_dpp.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_rank.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_scan_warp_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_shared_mem.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_scan_reduce_then_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_radix_rank_basic.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_radix_rank_match.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_discontinuity.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load_func.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/types/future_value.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_reduce_warp_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_dpp.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_segment_bounds.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_shared_mem.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_reduce_raking_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store_func.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store.hpp'   '/opt/rocm5.7.1/include/hipcub/thread/thread_operators.hpp'   '/opt/rocm5.7.1/include/hipcub/backend/rocprim/thread/thread_operators.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_adjacent_difference.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_adjacent_difference.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_adjacent_difference.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_adjacent_difference_config.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/device/config_types.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_transform.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/match_result_type.hpp'   '/opt/rocm5.7.1/include/rocprim/types/tuple.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/zip_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/types/tuple.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_transform_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_transform.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/match_result_type.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/temp_storage.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/counting_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/transform_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/match_result_type.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_histogram.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_histogram.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/uint_fast_div.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_histogram_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_histogram.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_config_helper.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/device/config_types.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_sort_bitonic.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_sort_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_sort_merge.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/merge_path.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_sort_stable.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_merge_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_merge.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/merge_path.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_merge_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load_func.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_merge_sort_mergepath.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_merge_sort_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_merge_sort_block_merge.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_merge_sort_block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_config_helper.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_partition.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_select_config.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_scan_common.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/lookback_scan_state.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/binary_op_wrappers.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/temp_storage.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/ordered_block_id.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_partition.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_discontinuity.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_radix_sort_onesweep.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_exchange.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_rank.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store_func.hpp'   '/opt/rocm5.7.1/include/rocprim/device/specialization/device_radix_block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_radix_sort_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_radix_sort_block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/specialization/device_radix_merge_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_merge_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/arg_index_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/types/key_value_pair.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce_config.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce_by_key_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_reduce_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/constant_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_run_length_encode.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/discard_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_run_length_encode_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_select.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/binary_op_wrappers.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/types/future_value.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_scan_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_scan_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/types/tuple.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan_by_key_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_segmented_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/reverse_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_segmented_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_load.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_exchange.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/warp_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load_func.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_store.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store_func.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_segmented_radix_sort_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_segmented_radix_sort_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_segmented_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_segmented_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/texture_cache_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/constant_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/counting_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/transform_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_exchange.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_scan.hpp' clang++: warning: argument unused during compilation: 'fcudaflushdenormalstozero' [Wunusedcommandlineargument] Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 1016.443s, Critical Path: 144.18s INFO: 5549 processes: 1076 internal, 4473 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target Traceback (most recent call last):   File ""/jax/build/build.py"", line 602, in      main()   File ""/jax/build/build.py"", line 573, in main     shell(command)   File ""/jax/build/build.py"", line 44, in shell     output = subprocess.check_output(cmd)   File ""/usr/lib/python3.10/subprocess.py"", line 421, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/usr/lib/python3.10/subprocess.py"", line 526, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel6.1.2linuxx86_64', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=/jax/dist', 'cpu=x86_64']' returned nonzero exit status 1. ```  It seems that the symlink location is included in the dependency, however the realpath is used during the build, resulting in a ""undeclared inclusion(s) in rule"" error.  What jax/jaxlib version are you using? v0.4.22 (v0.4.21 also has this bug)  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] uname_result(system='Linux', node='968a744453c3', release='6.6.6', version='SMP PREEMPT_DYNAMIC', machine='x86_64')  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,ROCM build fails when the ROCM_PATH is a symlinked location," Description The jaxlib build with ROCm will fail if the ROCM_PATH is directed to a symlinked location. This issue is particularly critical for NixOS, as the ROCM_PATH is invariably a symlinked location joining each individual library together, thereby making ROCM build on NixOS unfeasible. While this may be less significant for other distributions, it is still a common issue, given that /opt/rocm is likely to be a symlink. To reproduce this behavior, start from a ROCM container `rocm/devubuntu22.04:5.7.1complete` and run the following commands to prepare the build environment. ``` apt update apt install g++ python3dev pythonispython3 git pip install numpy wheel build git clone branch jaxlibv0.4.22 https://github.com/google/jax.git cd jax ``` In this environment the realpath for ROCM library is `/opt/rocm5.7.1` and there is also a symlink `/opt/rocm > /opt/rocm5.7.1` Now build jaxlib with `python build/build.py enable_rocm rocm_path=/opt/rocm` will result in a failure, but`python build/build.py enable_rocm rocm_path=/opt/rocm5.7.1` won't. Build log ``` Bazel binary path: ./bazel6.1.2linuxx86_64 Bazel version: 6.1.2 Python binary path: /usr/bin/python3 Python version: 3.10 NumPy version: 1.26.2 MKLDNN enabled: yes Target CPU: x86_64 Target CPU features: release CUDA enabled: no ROCm enabled: yes ROCm toolkit path: /opt/rocm ROCm amdgpu targets: gfx900,gfx906,gfx908,gfx90a,gfx1030 Building XLA and installing it in the jaxlib source tree... ./bazel6.1.2linuxx86_64 run verbose_failures=true //jaxlib/tools:build_wheel  output_path=/jax/dist cpu=x86_64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /jax/.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false INFO: Reading rc options for 'run' from /jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/usr/bin/python3 action_env=PYENV_ROOT python_path=/usr/bin/python3 action_env ROCM_PATH=/opt/rocm config=avx_posix config=mkl_open_source_only config=rocm INFO: Found applicable config definition build:short_logs in file /jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /jax/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:rocm in file /jax/.bazelrc: crosstool_top=//crosstool:toolchain define=using_rocm=true define=using_rocm_hipcc=true //xla/python:enable_gpu=true define=xla_python_enable_gpu=true repo_env TF_NEED_ROCM=1 action_env TF_ROCM_AMDGPU_TARGETS=gfx900,gfx906,gfx908,gfx90a,gfx1030 INFO: Found applicable config definition build:rocm in file /jax/.jax_configure.bazelrc: action_env TF_ROCM_AMDGPU_TARGETS=gfx900,gfx906,gfx908,gfx90a,gfx1030 INFO: Found applicable config definition build:linux in file /jax/.bazelrc: config=posix copt=Wnounknownwarningoption copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 Loading:  Loading:  Loading: 0 packages loaded Analyzing: target //jaxlib/tools:build_wheel (0 packages loaded, 0 targets configured) INFO: Analyzed target //jaxlib/tools:build_wheel (0 packages loaded, 0 targets configured). INFO: Found 1 target... [0 / 1] [Prepa] BazelWorkspaceStatusAction stablestatus.txt [34 / 291] Compiling llvm/lib/Support/DebugCounter.cpp [for tool]; 0s local ... (16 actions, 15 running) [50 / 291] Compiling llvm/lib/Support/Timer.cpp [for tool]; 1s local ... (16 actions, 15 running) ... [5,531 / 7,541] Compiling lib/Dialect/NVGPU/IR/Dialect.cpp; 20s local ... (16 actions running) ERROR: /root/.cache/bazel/_bazel_root/bee4ad1fd43279be7a03b33426e824d5/external/xla/xla/service/gpu/BUILD:1196:23: Compiling xla/service/gpu/cub_sort_kernel.cu.: undeclared inclusion(s) in rule '//xla/service/gpu:cub_sort_kernel_f64': this rule is missing dependency declarations for the following files included by 'xla/service/gpu/cub_sort_kernel.cu.cc':   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_runtime_wrapper.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/cuda_wrappers/cmath'   '/opt/rocm/llvm/lib/clang/17.0.0/include/stddef.h'   '/opt/rocm5.7.1/include/hip/hip_version.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_libdevice_declares.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_math.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/cuda_wrappers/algorithm'   '/opt/rocm/llvm/lib/clang/17.0.0/include/cuda_wrappers/new'   '/opt/rocm/llvm/lib/clang/17.0.0/include/limits.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/stdint.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_stdlib.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_cuda_math_forward_declares.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_hip_cmath.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__clang_cuda_complex_builtins.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/cuda_wrappers/complex'   '/opt/rocm/llvm/lib/clang/17.0.0/include/__stddef_max_align_t.h'   '/opt/rocm/llvm/lib/clang/17.0.0/include/stdarg.h'   '/opt/rocm5.7.1/include/hip/hip_runtime.h'   '/opt/rocm5.7.1/include/hip/hip_common.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_runtime.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_common.h'   '/opt/rocm5.7.1/include/hip/hip_runtime_api.h'   '/opt/rocm5.7.1/include/hip/amd_detail/host_defines.h'   '/opt/rocm5.7.1/include/hip/driver_types.h'   '/opt/rocm5.7.1/include/hip/texture_types.h'   '/opt/rocm5.7.1/include/hip/channel_descriptor.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_channel_descriptor.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_vector_types.h'   '/opt/rocm5.7.1/include/hip/surface_types.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_runtime_pt_api.h'   '/opt/rocm5.7.1/include/hip/amd_detail/hip_ldg.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_atomic.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_device_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/math_fwd.h'   '/opt/rocm5.7.1/include/hip/hip_vector_types.h'   '/opt/rocm5.7.1/include/hip/amd_detail/device_library_decls.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_warp_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_unsafe_atomics.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_surface_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/texture_fetch_functions.h'   '/opt/rocm5.7.1/include/hip/hip_texture_types.h'   '/opt/rocm5.7.1/include/hip/amd_detail/ockl_image.h'   '/opt/rocm5.7.1/include/hip/amd_detail/texture_indirect_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_math_functions.h'   '/opt/rocm5.7.1/include/hip/amd_detail/hip_fp16_math_fwd.h'   '/opt/rocm5.7.1/include/hip/library_types.h'   '/opt/rocm5.7.1/include/hip/hip_fp16.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_fp16.h'   '/opt/rocm5.7.1/include/hip/hip_bfloat16.h'   '/opt/rocm5.7.1/include/hip/amd_detail/amd_hip_bfloat16.h'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/types/future_value.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/types/double_buffer.hpp'   '/opt/rocm5.7.1/include/rocprim/types/integer_sequence.hpp'   '/opt/rocm5.7.1/include/rocprim/types/key_value_pair.hpp'   '/opt/rocm5.7.1/include/rocprim/types/tuple.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/all_true.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_adjacent_difference.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_adjacent_difference_impl.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_discontinuity.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_exchange.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/atomic.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/bit.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/warp.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/warp_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/warp_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_histogram.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_histogram_atomic.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_histogram_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_dpp.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_rank.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_scan_warp_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_shared_mem.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_scan_reduce_then_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_radix_rank_basic.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_radix_rank_match.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_discontinuity.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load_func.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/types/future_value.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_reduce_warp_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_dpp.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_segment_bounds.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_shared_mem.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_reduce_raking_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store_func.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store.hpp'   '/opt/rocm5.7.1/include/hipcub/thread/thread_operators.hpp'   '/opt/rocm5.7.1/include/hipcub/backend/rocprim/thread/thread_operators.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_adjacent_difference.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_adjacent_difference.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_adjacent_difference.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_adjacent_difference_config.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/device/config_types.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_transform.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/match_result_type.hpp'   '/opt/rocm5.7.1/include/rocprim/types/tuple.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/zip_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/config.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/types/tuple.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_transform_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_transform.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/match_result_type.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/temp_storage.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/counting_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/transform_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/match_result_type.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_histogram.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_histogram.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/uint_fast_div.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_histogram_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_histogram.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_config_helper.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/device/config_types.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_sort_bitonic.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_sort_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/block/detail/block_sort_merge.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/merge_path.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_sort_stable.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_merge_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_merge.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/merge_path.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_merge_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load_func.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_merge_sort_mergepath.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_merge_sort_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_merge_sort_block_merge.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_merge_sort_block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_config_helper.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_partition.hpp'   '/opt/rocm5.7.1/include/rocprim/type_traits.hpp'   '/opt/rocm5.7.1/include/rocprim/types.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_select_config.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_scan_common.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/thread.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/lookback_scan_state.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_reduce_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/detail/warp_scan_crosslane.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/binary_op_wrappers.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/functional.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/various.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/temp_storage.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/ordered_block_id.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_partition.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_discontinuity.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_radix_sort_onesweep.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_exchange.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_rank.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store_func.hpp'   '/opt/rocm5.7.1/include/rocprim/device/specialization/device_radix_block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_radix_sort_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_radix_sort_block_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/specialization/device_radix_merge_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_merge_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/arg_index_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/types/key_value_pair.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce_config.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_reduce_by_key_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_reduce_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/constant_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_run_length_encode.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/discard_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_run_length_encode_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_select.hpp'   '/opt/rocm5.7.1/include/rocprim/detail/binary_op_wrappers.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/types/future_value.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_scan.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/config/device_scan_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_scan_by_key.hpp'   '/opt/rocm5.7.1/include/rocprim/types/tuple.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_scan_by_key_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_segmented_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/reverse_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_segmented_radix_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_load.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_exchange.hpp'   '/opt/rocm5.7.1/include/rocprim/intrinsics/warp_shuffle.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_load_func.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_sort.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_store.hpp'   '/opt/rocm5.7.1/include/rocprim/block/block_store_func.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_segmented_radix_sort_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_segmented_radix_sort_config.hpp'   '/opt/rocm5.7.1/include/rocprim/device/device_segmented_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/device/detail/device_segmented_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/texture_cache_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/constant_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/counting_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/iterator/transform_iterator.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_exchange.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_reduce.hpp'   '/opt/rocm5.7.1/include/rocprim/warp/warp_scan.hpp' clang++: warning: argument unused during compilation: 'fcudaflushdenormalstozero' [Wunusedcommandlineargument] Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 1016.443s, Critical Path: 144.18s INFO: 5549 processes: 1076 internal, 4473 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target Traceback (most recent call last):   File ""/jax/build/build.py"", line 602, in      main()   File ""/jax/build/build.py"", line 573, in main     shell(command)   File ""/jax/build/build.py"", line 44, in shell     output = subprocess.check_output(cmd)   File ""/usr/lib/python3.10/subprocess.py"", line 421, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/usr/lib/python3.10/subprocess.py"", line 526, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel6.1.2linuxx86_64', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=/jax/dist', 'cpu=x86_64']' returned nonzero exit status 1. ```  It seems that the symlink location is included in the dependency, however the realpath is used during the build, resulting in a ""undeclared inclusion(s) in rule"" error.  What jax/jaxlib version are you using? v0.4.22 (v0.4.21 also has this bug)  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] uname_result(system='Linux', node='968a744453c3', release='6.6.6', version='SMP PREEMPT_DYNAMIC', machine='x86_64')  NVIDIA GPU info _No response_",2023-12-14T07:26:08Z,bug AMD GPU,open,0,2,https://github.com/jax-ml/jax/issues/18976,@ rahulbatra85, In the JAX containers we build we use real path. As you noted the symlink causes build to fail. I am guessing this is bazel(build system) thing where it ends up following symlink and creates a cache. I will need to look further to see what can be done here.
406,"以下是一个github上的jax下的一个issue, 标题是(jaxlib 0.4.22 has very spammy GPU compiler logs)， 内容是 (We messed up this release and the GPU compiler is very spammy, with lots of LOG output from C++. Workaround: set `TF_CPP_MIN_LOG_LEVEL=2`, and we're trying to cut another release.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jaxlib 0.4.22 has very spammy GPU compiler logs,"We messed up this release and the GPU compiler is very spammy, with lots of LOG output from C++. Workaround: set `TF_CPP_MIN_LOG_LEVEL=2`, and we're trying to cut another release.",2023-12-14T02:34:07Z,bug P0 (urgent) NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/18970,The immediate issue was fixed by the release of JAX v0.4.23.  https://github.com/google/jax/issues/18973 covers adding testing.,If you are like me and still see the logs after updating jax with `pip install jax upgrade`. Remember to also upgrade `jaxlib` with `pip install jaxlib upgrade`,"Yes, that's right! You need to update both `jax` and `jaxlib`. Sorry for the disruption."
344,"以下是一个github上的jax下的一个issue, 标题是(Improve shape validation when jax_dynamic_shapes=True)， 内容是 (Fixes CC([jax_dynamic_shapes] specifying shapes with floating point abstract values does not raise a TypeError))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve shape validation when jax_dynamic_shapes=True,Fixes CC([jax_dynamic_shapes] specifying shapes with floating point abstract values does not raise a TypeError),2023-12-12T20:57:06Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/18946,Probably needs a test before we can merge.
1732,"以下是一个github上的jax下的一个issue, 标题是(large groups of derived test cases are jointly sensitive to their common random seed)， 内容是 (Many of our pseudorandom tests carry out statistical tests (namely KS and chisquared), which have a failure probability `p`. Assuming there are no bugs, we might expect a that a single test case fails over a `p` fraction of random seeds. However, we sometimes generate many test cases from one fragment of test code, e.g. via test method parameterization or by testing several backends/environments. When these all share a seed (because they share the seeding code), changing that seed amounts to redrawing many pseudorandom trials at once. It is then quite likely (beyond `p`) that at least one of these cases fails across such a change. As one example, six generated testcases derived from `LaxRandomTest.testBinomialSample` fail if we change the seed here to `1234567`: https://github.com/google/jax/blob/b077483bfaaf197b79717a86bee3e626474e93f2/tests/random_lax_test.pyL1174L1175 Maybe we can try to decouple the seeds across test cases and/or decrease pvalues for commonlyseeded groups of test cases. Relatedly, if we do something like change the RNG algorithm (as in https://github.com/google/jax/discussions/18480), then we might expect some subset of all of our statistical test cases to fail. Assuming no bugs, it would be nice to be able to rotate seeds until all cases pass again, and this would be easier if large groups of test cases were not coupled by common seeds. Related: CC(`random_lax_test::LaxRandomTest.testBall` is sensitive to its random seed).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,large groups of derived test cases are jointly sensitive to their common random seed,"Many of our pseudorandom tests carry out statistical tests (namely KS and chisquared), which have a failure probability `p`. Assuming there are no bugs, we might expect a that a single test case fails over a `p` fraction of random seeds. However, we sometimes generate many test cases from one fragment of test code, e.g. via test method parameterization or by testing several backends/environments. When these all share a seed (because they share the seeding code), changing that seed amounts to redrawing many pseudorandom trials at once. It is then quite likely (beyond `p`) that at least one of these cases fails across such a change. As one example, six generated testcases derived from `LaxRandomTest.testBinomialSample` fail if we change the seed here to `1234567`: https://github.com/google/jax/blob/b077483bfaaf197b79717a86bee3e626474e93f2/tests/random_lax_test.pyL1174L1175 Maybe we can try to decouple the seeds across test cases and/or decrease pvalues for commonlyseeded groups of test cases. Relatedly, if we do something like change the RNG algorithm (as in https://github.com/google/jax/discussions/18480), then we might expect some subset of all of our statistical test cases to fail. Assuming no bugs, it would be nice to be able to rotate seeds until all cases pass again, and this would be easier if large groups of test cases were not coupled by common seeds. Related: CC(`random_lax_test::LaxRandomTest.testBall` is sensitive to its random seed).",2023-12-12T16:47:40Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/18941,In my experience a lot of these distribution tests are sensitive to a change in seed.,"This may be the expected effect of producing many trials, across generated cases, environments (CPU, GPU, TPU), etc. One workaround is to decrease the failure threshold (i.e. pvalue) for tests like this.",Agreed  but this is something that we need to look at more comprehensively in the random distribution test suite. I don't think the Binomial test is an outlier here!,I see what you mean and I agree. I've renamed the issue and tried to update the description to describe things better now.
5035,"以下是一个github上的jax下的一个issue, 标题是([jax_dynamic_shapes] specifying shapes with floating point abstract values does not raise a TypeError)， 内容是 ( Description The following example gets to produce valid StableHLO but should fail during tracing. Note that the StableHLO is valid for alternative lowering strategies. XLA's lowering strategy will raise an error during compilation. ```python import jax jax.config.update(""jax_enable_x64"", True) jax.config.update(""jax_platform_name"", ""cpu"") jax.config.update(""jax_dynamic_shapes"", True) .jit def test_this(size):   return jax.numpy.ones([size])       print(test_this(1.0))   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 177, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 255, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 161, in _python_pjit_helper     args_flat, _, params, in_tree, out_tree, _, _, _ = infer_params_fn(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/api.py"", line 317, in infer_params     return pjit.common_infer_params(pjit_info_args, *args, **kwargs)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 491, in common_infer_params     jaxpr, consts, canonicalized_out_shardings_flat, out_layouts_flat = _pjit_jaxpr(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 989, in _pjit_jaxpr     jaxpr, final_consts, out_type = _create_pjit_jaxpr(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/linear_util.py"", line 349, in memoized_fun     ans = call(fun, *args)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 934, in _create_pjit_jaxpr     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 334, in wrapper     return func(*args, **kwargs)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2283, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2305, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/linear_util.py"", line 191, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/erick.ochoalopez/Code/cataliist/test.py"", line 9, in test_this     return jax.numpy.ones([size])   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2291, in ones     shape = canonicalize_shape(shape)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 82, in canonicalize_shape     return core.canonicalize_shape(shape, context)   type: ignore   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/core.py"", line 2077, in canonicalize_shape     raise _invalid_shape_error(shape, context) TypeError: Shapes must be 1D sequences of concrete values of integer type, got [Tracedwith]. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. The error occurred while tracing the function test_this at /home/erick.ochoalopez/Code/cataliist/test.py:7 for jit. This concrete value was not available in Python because it depends on the value of the argument size. ``` I've narrowed down the error to `_canonicalize_dimension` inlined below: ```python def _canonicalize_dimension(dim: DimSize) > DimSize:    Dimensions are most commonly integral (by far), so we check that first.   try:     return operator.index(dim)   except TypeError as e:     type_error = e   if isinstance(dim, Tracer) and config.dynamic_shapes.value:  This check is not sufficient. It should also check that the tracer is of integer type.     return dim   elif (config.dynamic_shapes.value and isinstance(dim, DArray) and         type(dim._aval.dtype) is bint and not dim._aval.shape):     return dim   elif is_dim(dim):     return dim   else:     raise type_error ``` Happy to submit a PR perhaps on the next year.  What jax/jaxlib version are you using? 0.4.21  Which accelerator(s) are you using? CPU  Additional system info? 1.26.1 3.10.11 (main, May 13 2023, 12:07:51) [GCC 9.4.0] uname_result(system='Linux', node='DL7420GS4N1J3', release='5.15.088generic', version=' CC(make it easy to print jaxprs)~20.04.1Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax_dynamic_shapes] specifying shapes with floating point abstract values does not raise a TypeError," Description The following example gets to produce valid StableHLO but should fail during tracing. Note that the StableHLO is valid for alternative lowering strategies. XLA's lowering strategy will raise an error during compilation. ```python import jax jax.config.update(""jax_enable_x64"", True) jax.config.update(""jax_platform_name"", ""cpu"") jax.config.update(""jax_dynamic_shapes"", True) .jit def test_this(size):   return jax.numpy.ones([size])       print(test_this(1.0))   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 177, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 255, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 161, in _python_pjit_helper     args_flat, _, params, in_tree, out_tree, _, _, _ = infer_params_fn(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/api.py"", line 317, in infer_params     return pjit.common_infer_params(pjit_info_args, *args, **kwargs)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 491, in common_infer_params     jaxpr, consts, canonicalized_out_shardings_flat, out_layouts_flat = _pjit_jaxpr(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 989, in _pjit_jaxpr     jaxpr, final_consts, out_type = _create_pjit_jaxpr(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/linear_util.py"", line 349, in memoized_fun     ans = call(fun, *args)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 934, in _create_pjit_jaxpr     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 334, in wrapper     return func(*args, **kwargs)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2283, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2305, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/linear_util.py"", line 191, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/erick.ochoalopez/Code/cataliist/test.py"", line 9, in test_this     return jax.numpy.ones([size])   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2291, in ones     shape = canonicalize_shape(shape)   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 82, in canonicalize_shape     return core.canonicalize_shape(shape, context)   type: ignore   File ""/home/erick.ochoalopez/Code/env/lib/python3.10/sitepackages/jax/_src/core.py"", line 2077, in canonicalize_shape     raise _invalid_shape_error(shape, context) TypeError: Shapes must be 1D sequences of concrete values of integer type, got [Tracedwith]. If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. The error occurred while tracing the function test_this at /home/erick.ochoalopez/Code/cataliist/test.py:7 for jit. This concrete value was not available in Python because it depends on the value of the argument size. ``` I've narrowed down the error to `_canonicalize_dimension` inlined below: ```python def _canonicalize_dimension(dim: DimSize) > DimSize:    Dimensions are most commonly integral (by far), so we check that first.   try:     return operator.index(dim)   except TypeError as e:     type_error = e   if isinstance(dim, Tracer) and config.dynamic_shapes.value:  This check is not sufficient. It should also check that the tracer is of integer type.     return dim   elif (config.dynamic_shapes.value and isinstance(dim, DArray) and         type(dim._aval.dtype) is bint and not dim._aval.shape):     return dim   elif is_dim(dim):     return dim   else:     raise type_error ``` Happy to submit a PR perhaps on the next year.  What jax/jaxlib version are you using? 0.4.21  Which accelerator(s) are you using? CPU  Additional system info? 1.26.1 3.10.11 (main, May 13 2023, 12:07:51) [GCC 9.4.0] uname_result(system='Linux', node='DL7420GS4N1J3', release='5.15.088generic', version=' CC(make it easy to print jaxprs)~20.04.1Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_",2023-12-12T16:01:47Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/18937
4846,"以下是一个github上的jax下的一个issue, 标题是(Bump actions/setup-python from 4.7.1 to 5.0.0)， 内容是 (Bumps actions/setuppython from 4.7.1 to 5.0.0.  Release notes Sourced from actions/setuppython's releases.  v5.0.0 What's Changed In scope of this release, we update node version runtime from node16 to node20 (actions/setuppython CC(backward pass of scan is very slow to compile in CPU)). Besides, we update dependencies to the latest versions. Full Changelog: https://github.com/actions/setuppython/compare/v4.8.0...v5.0.0 v4.8.0 What's Changed In scope of this release we added support for GraalPy (actions/setuppython CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values)). You can use this snippet to set up GraalPy: steps:  uses: actions/checkout  uses: actions/setuppython    with:     pythonversion: 'graalpy22.3'   run: python my_script.py  Besides, the release contains such changes as:  Trim python version when reading from file by @​FerranPares in actions/setuppython CC(Forwardmode differentiation rule for 'select_and_scatter_add' not implemented) Use nondeprecated versions in examples by @​jeffwidman in actions/setuppython CC(Build error on MacOS) Change deprecation comment to past tense by @​jeffwidman in actions/setuppython CC(Support for GPU SVD) Bump @​babel/traverse from 7.9.0 to 7.23.2 by @​dependabot in actions/setuppython CC(Limit jax multithreading) advancedusage.md: Encourage the use actions/checkout by @​cclauss in actions/setuppython CC(improve documetnation of lax parallel operators) Examples now use checkout by @​simonw in actions/setuppython CC(np.trace is broken) Update actions/checkout to v4 by @​dmitryshibanov in actions/setuppython CC(make jax.random default dtypes 64bit)  New Contributors  @​FerranPares made their first contribution in actions/setuppython CC(Forwardmode differentiation rule for 'select_and_scatter_add' not implemented) @​timfel made their first contribution in actions/setuppython CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values) @​jeffwidman made their first contribution in actions/setuppython CC(Build error on MacOS)  Full Changelog: https://github.com/actions/setuppython/compare/v4...v4.8.0    Commits  0a5c615 Update action to node20 ( CC(backward pass of scan is very slow to compile in CPU)) 0ae5836 Add example of GraalPy to docs ( CC(Build for Ubuntu (ARMbased machine))) b64ffca update actions/checkout to v4 ( CC(make jax.random default dtypes 64bit)) 8d28961 Examples now use checkout ( CC(np.trace is broken)) 7bc6abb advancedusage.md: Encourage the use actions/checkout ( CC(improve documetnation of lax parallel operators)) e8111ce Bump @​babel/traverse from 7.9.0 to 7.23.2 ( CC(Limit jax multithreading)) a00ea43 add fix for graalpy ci ( CC(wrap np.trace axes (fixes 738))) 8635b1c Change deprecation comment to past tense ( CC(Support for GPU SVD)) f6cc428 Use nondeprecated versions in examples ( CC(Build error on MacOS)) 5f2af21 Add GraalPy support ( CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Bump actions/setup-python from 4.7.1 to 5.0.0,"Bumps actions/setuppython from 4.7.1 to 5.0.0.  Release notes Sourced from actions/setuppython's releases.  v5.0.0 What's Changed In scope of this release, we update node version runtime from node16 to node20 (actions/setuppython CC(backward pass of scan is very slow to compile in CPU)). Besides, we update dependencies to the latest versions. Full Changelog: https://github.com/actions/setuppython/compare/v4.8.0...v5.0.0 v4.8.0 What's Changed In scope of this release we added support for GraalPy (actions/setuppython CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values)). You can use this snippet to set up GraalPy: steps:  uses: actions/checkout  uses: actions/setuppython    with:     pythonversion: 'graalpy22.3'   run: python my_script.py  Besides, the release contains such changes as:  Trim python version when reading from file by @​FerranPares in actions/setuppython CC(Forwardmode differentiation rule for 'select_and_scatter_add' not implemented) Use nondeprecated versions in examples by @​jeffwidman in actions/setuppython CC(Build error on MacOS) Change deprecation comment to past tense by @​jeffwidman in actions/setuppython CC(Support for GPU SVD) Bump @​babel/traverse from 7.9.0 to 7.23.2 by @​dependabot in actions/setuppython CC(Limit jax multithreading) advancedusage.md: Encourage the use actions/checkout by @​cclauss in actions/setuppython CC(improve documetnation of lax parallel operators) Examples now use checkout by @​simonw in actions/setuppython CC(np.trace is broken) Update actions/checkout to v4 by @​dmitryshibanov in actions/setuppython CC(make jax.random default dtypes 64bit)  New Contributors  @​FerranPares made their first contribution in actions/setuppython CC(Forwardmode differentiation rule for 'select_and_scatter_add' not implemented) @​timfel made their first contribution in actions/setuppython CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values) @​jeffwidman made their first contribution in actions/setuppython CC(Build error on MacOS)  Full Changelog: https://github.com/actions/setuppython/compare/v4...v4.8.0    Commits  0a5c615 Update action to node20 ( CC(backward pass of scan is very slow to compile in CPU)) 0ae5836 Add example of GraalPy to docs ( CC(Build for Ubuntu (ARMbased machine))) b64ffca update actions/checkout to v4 ( CC(make jax.random default dtypes 64bit)) 8d28961 Examples now use checkout ( CC(np.trace is broken)) 7bc6abb advancedusage.md: Encourage the use actions/checkout ( CC(improve documetnation of lax parallel operators)) e8111ce Bump @​babel/traverse from 7.9.0 to 7.23.2 ( CC(Limit jax multithreading)) a00ea43 add fix for graalpy ci ( CC(wrap np.trace axes (fixes 738))) 8635b1c Change deprecation comment to past tense ( CC(Support for GPU SVD)) f6cc428 Use nondeprecated versions in examples ( CC(Build error on MacOS)) 5f2af21 Add GraalPy support ( CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values)) Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2023-12-11T17:15:29Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/18921,"Closing, I'll address this separately via ratchet","OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
2516,"以下是一个github上的jax下的一个issue, 标题是(0.4.21 release: ""JAX does not support string indexing"" error, but it used to work fine (for jax2tf only!))， 内容是 ( Description I pass a dictionary into my jitted function as an argument, including several string/jax array pairs. I can run my jitted function with no issues. Something like the following: ```python ... state = env.init(key)  state is a dictionary containing things like   {""numSteps"": jnp.array([0]), ""legal_actions"": jnp.array([[True, False],[...]]), ... } step_fn = jax.jit(env.step) action = jnp.array([1]) state = step_fn(state, action) ... ``` However, when I do this, it breaks: ```python tf_init = jax2tf.convert(jax.jit(env.init), jit_compile=True, autograph=False) tf_state = tf_init(tf.constant(key)) tf_step = jax2tf.convert(step_fn, jit_compile=True, autograph=False) state = tf_step(tf_state, tf.constant(action))   Relevant stack trace   My code    File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\myEnv\core.py"", line 205, in step      is_illegal = ~state['legal_actions'][action]  Jax code    File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 741, in op     return getattr(self.aval, f""_{name}"")(self, *args)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 354, in _getitem     return lax_numpy._rewriting_take(self, item)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\lax_numpy.py"", line 4497, in _rewriting_take     treedef, static_idx, dynamic_idx = _split_index_for_jit(idx, arr.shape)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\lax_numpy.py"", line 4571, in _split_index_for_jit     raise TypeError(f""JAX does not support string indexing; got {idx=}"") TypeError: JAX does not support string indexing; got idx=('legal_actions',)    This error check was added here to address this issue. That issue seems to be unrelated, so maybe a more specific condition for triggering this error is in order? I would like to continue indexing into dictionaries using strings in jax2tf converted functions! :) Going back to v0.4.20 (where the above code works) for now. Thanks!  What jax/jaxlib version are you using? 0.4.21 for both  Which accelerator(s) are you using? cpu  Additional system info? Both ubuntu and windows 10  NVIDIA GPU info n/a)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,"0.4.21 release: ""JAX does not support string indexing"" error, but it used to work fine (for jax2tf only!)"," Description I pass a dictionary into my jitted function as an argument, including several string/jax array pairs. I can run my jitted function with no issues. Something like the following: ```python ... state = env.init(key)  state is a dictionary containing things like   {""numSteps"": jnp.array([0]), ""legal_actions"": jnp.array([[True, False],[...]]), ... } step_fn = jax.jit(env.step) action = jnp.array([1]) state = step_fn(state, action) ... ``` However, when I do this, it breaks: ```python tf_init = jax2tf.convert(jax.jit(env.init), jit_compile=True, autograph=False) tf_state = tf_init(tf.constant(key)) tf_step = jax2tf.convert(step_fn, jit_compile=True, autograph=False) state = tf_step(tf_state, tf.constant(action))   Relevant stack trace   My code    File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\myEnv\core.py"", line 205, in step      is_illegal = ~state['legal_actions'][action]  Jax code    File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 741, in op     return getattr(self.aval, f""_{name}"")(self, *args)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 354, in _getitem     return lax_numpy._rewriting_take(self, item)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\lax_numpy.py"", line 4497, in _rewriting_take     treedef, static_idx, dynamic_idx = _split_index_for_jit(idx, arr.shape)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\lax_numpy.py"", line 4571, in _split_index_for_jit     raise TypeError(f""JAX does not support string indexing; got {idx=}"") TypeError: JAX does not support string indexing; got idx=('legal_actions',)    This error check was added here to address this issue. That issue seems to be unrelated, so maybe a more specific condition for triggering this error is in order? I would like to continue indexing into dictionaries using strings in jax2tf converted functions! :) Going back to v0.4.20 (where the above code works) for now. Thanks!  What jax/jaxlib version are you using? 0.4.21 for both  Which accelerator(s) are you using? cpu  Additional system info? Both ubuntu and windows 10  NVIDIA GPU info n/a",2023-12-10T00:21:38Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18903,"Turns out this was an unrelated usercode bug. My bad, closing."
1203,"以下是一个github上的jax下的一个issue, 标题是(Pallas NotImplementedError: Unimplemented primitive in Pallas TPU lowering: dynamic_slice.)， 内容是 ( Description I'm trying to RWKV in jax using Pallas but run into problems with the for loop. In my code, I'm using jax.lax.fori_loop to iterate over the sequence dimension. When indexing into my arrays I get the error:  **JaxStackTraceBeforeTransformation: NotImplementedError: Unimplemented primitive in Pallas TPU lowering: dynamic_slice. Please file an issue on https://github.com/google/jax/issues.** Code:  ```  r,k,v,w: T x D  u: D  out: T x D def loop(t,W_t):     out,s = W_t     kv_t = k[t] * v[t]  D x D     out = out.at[t].set((r[:,t] @ (u * kv_t + s)).squeeze(0))  D @ (D x D) > 1 x D     s = kv_t + w[t,None] * s  D x D     return out,s out_ref[...] = jax.lax.fori_loop(0,T,loop,(out,s))[0] ``` Problem Arises with ""k[t] * v[t]""  What jax/jaxlib version are you using? jax v0.4.21 jaxlib v0.4.21  Which accelerator(s) are you using? TPU  Additional system info? Python 3.10, Kaggle Notebook  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Pallas NotImplementedError: Unimplemented primitive in Pallas TPU lowering: dynamic_slice.," Description I'm trying to RWKV in jax using Pallas but run into problems with the for loop. In my code, I'm using jax.lax.fori_loop to iterate over the sequence dimension. When indexing into my arrays I get the error:  **JaxStackTraceBeforeTransformation: NotImplementedError: Unimplemented primitive in Pallas TPU lowering: dynamic_slice. Please file an issue on https://github.com/google/jax/issues.** Code:  ```  r,k,v,w: T x D  u: D  out: T x D def loop(t,W_t):     out,s = W_t     kv_t = k[t] * v[t]  D x D     out = out.at[t].set((r[:,t] @ (u * kv_t + s)).squeeze(0))  D @ (D x D) > 1 x D     s = kv_t + w[t,None] * s  D x D     return out,s out_ref[...] = jax.lax.fori_loop(0,T,loop,(out,s))[0] ``` Problem Arises with ""k[t] * v[t]""  What jax/jaxlib version are you using? jax v0.4.21 jaxlib v0.4.21  Which accelerator(s) are you using? TPU  Additional system info? Python 3.10, Kaggle Notebook  NVIDIA GPU info _No response_",2023-12-09T14:54:31Z,bug pallas,open,0,0,https://github.com/jax-ml/jax/issues/18897
736,"以下是一个github上的jax下的一个issue, 标题是(15% speed regression in LLM training code on v3 TPU from JAX 0.4.14 -> 0.4.16)， 内容是 ( Description Will attempt to minimize unless something is obvious. Attached are XLA outputs. The only difference is the JAX version. Verified the regression persists through 0.4.21 Tested with Levanter main (133dbba64ff08873ed07a5661211db867a471343) levanter_train_step.tar.gz Thanks! I really appreciate it!  What jax/jaxlib version are you using? 0.4.14>0.4.21  Which accelerator(s) are you using? TPU v3128  Additional system info? _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,15% speed regression in LLM training code on v3 TPU from JAX 0.4.14 -> 0.4.16, Description Will attempt to minimize unless something is obvious. Attached are XLA outputs. The only difference is the JAX version. Verified the regression persists through 0.4.21 Tested with Levanter main (133dbba64ff08873ed07a5661211db867a471343) levanter_train_step.tar.gz Thanks! I really appreciate it!  What jax/jaxlib version are you using? 0.4.14>0.4.21  Which accelerator(s) are you using? TPU v3128  Additional system info? _No response_  NVIDIA GPU info _No response_,2023-12-08T05:23:58Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/18875
5409,"以下是一个github上的jax下的一个issue, 标题是(unexpected UnshapedArray)， 内容是 ( Description ```python from dataclasses import dataclass import jax import jax.numpy as jnp .tree_util.register_pytree_node_class  class DataClass_sub:     def tree_flatten(self):         return ([self.array], None)          def tree_unflatten(cls, aux_data, children):         return cls(*children)     array: jnp.ndarray .tree_util.register_pytree_node_class () class DataClass:     def tree_flatten(self):         return ([self.array0, self.array1, self.dataclass_sub], None)          def tree_unflatten(cls, aux_data, children):         return cls(*children)     array0: jnp.ndarray     array1: jnp.ndarray     dataclass_sub: DataClass_sub .custom_vjp def subfunc( data, array ):     return 0. def subfunc_fwd(data, array):     return 0., (data, array) def subfunc_bwd(input, g):     grads = input[1]     return (0., g * grads ) subfunc.defvjp(subfunc_fwd, subfunc_bwd) .jit def func_custom_vjp(     array,     data0: DataClass ):     tmp = data0.array0.at[jnp.array([0])].set(array)     data = DataClass(tmp, 1.0/(1.0+tmp**2), data0.dataclass_sub)     return subfunc(data, array) size=10 dataclass_sub = DataClass_sub(jnp.zeros( (size,) )) dataclass = DataClass( array0 = jnp.zeros(size), array1 = jnp.zeros(size), dataclass_sub=dataclass_sub ) grad_func = jax.grad(func_custom_vjp, argnums=0) grads = grad_func(jnp.array([0.]), dataclass) print(grads) ``` Above code produces the below error message 1. When I change ""data = DataClass(tmp, 1.0/(1.0+tmp**2), data0.dataclass_sub)"" to ""data = DataClass(tmp, tmp, data0.dataclass_sub)"", I get below error message 2. (I have obscured part of the file paths). I am a beginner in using jax and pytree, so I would appreciate it if you could let me know if I am using it incorrectly. ===error message 1=== ``` Traceback (most recent call last):   File ""test.py"", line 60, in      grads = grad_func(jnp.array([0.]), dataclass)   File ""test.py"", line 51, in func_custom_vjp     tmp = data0.array0.at[jnp.array([0])].set(array)   File ""\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 495, in set     return scatter._scatter_update(self.array, self.index, values, lax.scatter,   File ""\lib\sitepackages\jax\_src\ops\scatter.py"", line 78, in _scatter_update     return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,   File ""\lib\sitepackages\jax\_src\ops\scatter.py"", line 126, in _scatter_impl     out = scatter_op( jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: UnshapedArray has no shape. Please open an issue at https://github.com/google/jax/issues because it's unexpected for UnshapedArray instances to ever be produced. The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test.py"", line 60, in      grads = grad_func(jnp.array([0.]), dataclass) TypeError: UnshapedArray has no shape. Please open an issue at https://github.com/google/jax/issues because it's unexpected for UnshapedArray instances to ever be produced. ``` ===error message 2=== ``` Traceback (most recent call last):   File ""test.py"", line 60, in      grads = grad_func(jnp.array([0.]), dataclass)   File ""test.py"", line 51, in func_custom_vjp     tmp = data0.array0.at[jnp.array([0])].set(array)   File ""\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 495, in set     return scatter._scatter_update(self.array, self.index, values, lax.scatter,   File ""\lib\sitepackages\jax\_src\ops\scatter.py"", line 78, in _scatter_update     return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,   File ""\lib\sitepackages\jax\_src\ops\scatter.py"", line 126, in _scatter_impl     out = scatter_op( jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: Invalid start_index_map; domain is [0, 0), got: 0>0. The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test.py"", line 60, in      grads = grad_func(jnp.array([0.]), dataclass) TypeError: Invalid start_index_map; domain is [0, 0), got: 0>0. ```  What jax/jaxlib version are you using? jax0.4.21 jaxlib0.4.21  Which accelerator(s) are you using? cpu  Additional system info? print(numpy.__version__):1.24.3,   print(sys.version):3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)],    print(platform.uname()):uname_result(system='Windows', node='DESKTOPTU7H8UO', release='10', version='10.0.22621', machine='AMD64')  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,unexpected UnshapedArray," Description ```python from dataclasses import dataclass import jax import jax.numpy as jnp .tree_util.register_pytree_node_class  class DataClass_sub:     def tree_flatten(self):         return ([self.array], None)          def tree_unflatten(cls, aux_data, children):         return cls(*children)     array: jnp.ndarray .tree_util.register_pytree_node_class () class DataClass:     def tree_flatten(self):         return ([self.array0, self.array1, self.dataclass_sub], None)          def tree_unflatten(cls, aux_data, children):         return cls(*children)     array0: jnp.ndarray     array1: jnp.ndarray     dataclass_sub: DataClass_sub .custom_vjp def subfunc( data, array ):     return 0. def subfunc_fwd(data, array):     return 0., (data, array) def subfunc_bwd(input, g):     grads = input[1]     return (0., g * grads ) subfunc.defvjp(subfunc_fwd, subfunc_bwd) .jit def func_custom_vjp(     array,     data0: DataClass ):     tmp = data0.array0.at[jnp.array([0])].set(array)     data = DataClass(tmp, 1.0/(1.0+tmp**2), data0.dataclass_sub)     return subfunc(data, array) size=10 dataclass_sub = DataClass_sub(jnp.zeros( (size,) )) dataclass = DataClass( array0 = jnp.zeros(size), array1 = jnp.zeros(size), dataclass_sub=dataclass_sub ) grad_func = jax.grad(func_custom_vjp, argnums=0) grads = grad_func(jnp.array([0.]), dataclass) print(grads) ``` Above code produces the below error message 1. When I change ""data = DataClass(tmp, 1.0/(1.0+tmp**2), data0.dataclass_sub)"" to ""data = DataClass(tmp, tmp, data0.dataclass_sub)"", I get below error message 2. (I have obscured part of the file paths). I am a beginner in using jax and pytree, so I would appreciate it if you could let me know if I am using it incorrectly. ===error message 1=== ``` Traceback (most recent call last):   File ""test.py"", line 60, in      grads = grad_func(jnp.array([0.]), dataclass)   File ""test.py"", line 51, in func_custom_vjp     tmp = data0.array0.at[jnp.array([0])].set(array)   File ""\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 495, in set     return scatter._scatter_update(self.array, self.index, values, lax.scatter,   File ""\lib\sitepackages\jax\_src\ops\scatter.py"", line 78, in _scatter_update     return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,   File ""\lib\sitepackages\jax\_src\ops\scatter.py"", line 126, in _scatter_impl     out = scatter_op( jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: UnshapedArray has no shape. Please open an issue at https://github.com/google/jax/issues because it's unexpected for UnshapedArray instances to ever be produced. The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test.py"", line 60, in      grads = grad_func(jnp.array([0.]), dataclass) TypeError: UnshapedArray has no shape. Please open an issue at https://github.com/google/jax/issues because it's unexpected for UnshapedArray instances to ever be produced. ``` ===error message 2=== ``` Traceback (most recent call last):   File ""test.py"", line 60, in      grads = grad_func(jnp.array([0.]), dataclass)   File ""test.py"", line 51, in func_custom_vjp     tmp = data0.array0.at[jnp.array([0])].set(array)   File ""\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 495, in set     return scatter._scatter_update(self.array, self.index, values, lax.scatter,   File ""\lib\sitepackages\jax\_src\ops\scatter.py"", line 78, in _scatter_update     return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,   File ""\lib\sitepackages\jax\_src\ops\scatter.py"", line 126, in _scatter_impl     out = scatter_op( jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: Invalid start_index_map; domain is [0, 0), got: 0>0. The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.   The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test.py"", line 60, in      grads = grad_func(jnp.array([0.]), dataclass) TypeError: Invalid start_index_map; domain is [0, 0), got: 0>0. ```  What jax/jaxlib version are you using? jax0.4.21 jaxlib0.4.21  Which accelerator(s) are you using? cpu  Additional system info? print(numpy.__version__):1.24.3,   print(sys.version):3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)],    print(platform.uname()):uname_result(system='Windows', node='DESKTOPTU7H8UO', release='10', version='10.0.22621', machine='AMD64')  NVIDIA GPU info _No response_",2023-12-07T06:00:26Z,better_errors,closed,0,2,https://github.com/jax-ml/jax/issues/18859,"The problem is ```python def subfunc_bwd(input, g):     grads = input[1]     return (0., g * grads ) ``` you're returning a scalar `0.` as the gradient for the `data` input. These are not matching pytree structures or matching array shapes. You want either: ```python return jax.tree_util.tree_map(jnp.zeros_like, input[0]), g * grads ``` to explicitly pass zeros for every position; or ```python return None, g *grads ``` as JAX understands a `None` to mean passing a zero gradient for the whole argument. I agree that this definitely isn't obvious though! The errors you get here don't communicate this at all. I've labelled this issue as a case where JAX needs a better error.",Thank you for pointing out the error in my code. I appreciate your guidance on the correct usage of the library!
539,"以下是一个github上的jax下的一个issue, 标题是([Pallas] Add missing shape checks in the Pallas FlashAttention kernel for TPUs)， 内容是 ([Pallas] Add missing shape checks in the Pallas FlashAttention kernel for TPUs Missing shape checks can cause hard to understand runtime errors caused by OOB checks inserted by XLA. We weren't verifying that the attention bias and the segment ids have the shapes we were expecting.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Pallas] Add missing shape checks in the Pallas FlashAttention kernel for TPUs,[Pallas] Add missing shape checks in the Pallas FlashAttention kernel for TPUs Missing shape checks can cause hard to understand runtime errors caused by OOB checks inserted by XLA. We weren't verifying that the attention bias and the segment ids have the shapes we were expecting.,2023-12-06T11:08:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18837
1168,"以下是一个github上的jax下的一个issue, 标题是(Fixed failing shardmap error message builder when using functions with variable number of arguments)， 内容是 (Previously, when using `shard_map` with a function that uses a variable number of arguments (i.e. `f(*args)`), if you made a mistake in your shard_map spec, instead of the really nicely formatted error message, you would actually get this error instead. ``` File ""/Users/chaser/jax/jax/experimental/failure.py"", line 23, in      jax.jit(f)(a, b)   ...   ...  Removed for brevity   ...   File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/sitepackages/jax/experimental/shard_map.py"", line 191, in _check_specs_vs_args     msg = _spec_rank_error(SpecErrorType.input, f, in_tree, in_specs, fail)   File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/sitepackages/jax/experimental/shard_map.py"", line 215, in _spec_rank_error     f""parameter '{list(ba.arguments.keys())[arg_key.idx]}',"") IndexError: list index out of range ``` This PR fixes the issue.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fixed failing shardmap error message builder when using functions with variable number of arguments,"Previously, when using `shard_map` with a function that uses a variable number of arguments (i.e. `f(*args)`), if you made a mistake in your shard_map spec, instead of the really nicely formatted error message, you would actually get this error instead. ``` File ""/Users/chaser/jax/jax/experimental/failure.py"", line 23, in      jax.jit(f)(a, b)   ...   ...  Removed for brevity   ...   File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/sitepackages/jax/experimental/shard_map.py"", line 191, in _check_specs_vs_args     msg = _spec_rank_error(SpecErrorType.input, f, in_tree, in_specs, fail)   File ""/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/sitepackages/jax/experimental/shard_map.py"", line 215, in _spec_rank_error     f""parameter '{list(ba.arguments.keys())[arg_key.idx]}',"") IndexError: list index out of range ``` This PR fixes the issue.",2023-12-05T15:02:24Z,,closed,0,2,https://github.com/jax-ml/jax/issues/18823,Any updates on this?,"I thought I had sent this comment before posting that PR, but it seems I didn't actually click 'Comment'! Oops: > Thanks for finding this issue, and suggesting a fix! >  > However I think we can still provide the parameter name: we can identify when the argument position must be bound to a varargs parameter, and thus still print the extra error info. >  > I attempted that in CC([shardmap] fix varargs error message bug), which I marked as coauthored by you. What do you think? Anyway, we basically merged this in CC([shardmap] fix varargs error message bug) so I'll close this PR."
722,"以下是一个github上的jax下的一个issue, 标题是(Allow compilation cache to be saved from process indices that are not process index 0)， 内容是 (At present, this check stops the compilation cache from being written on any process that is not process index 0. This makes sense if the compilation cache directory resides on shared storage. However, in our case, we do not wish to put this directory on shared storage and would instead prefer to save it on every process. WDYT of having an enum state `jax_persistent_cache_write` with values `['always', 'never', 'on_process_0']` to control this behavior?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Allow compilation cache to be saved from process indices that are not process index 0,"At present, this check stops the compilation cache from being written on any process that is not process index 0. This makes sense if the compilation cache directory resides on shared storage. However, in our case, we do not wish to put this directory on shared storage and would instead prefer to save it on every process. WDYT of having an enum state `jax_persistent_cache_write` with values `['always', 'never', 'on_process_0']` to control this behavior?",2023-12-05T07:47:17Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/18819,"This behaviour is supposed to be controlled by the `jax_share_binary_between_hosts` flag: https://github.com/google/jax/blob/42724ebc73e2bd06d5fc9c803c79e33377c7d93f/jax/_src/config.pyL1242L1249 However, the code is currently written in a way that this flag would be disregarded if `is_multi_process` is true.","More debugging on this. The lines https://github.com/google/jax/blob/7a3e2140fbda4674da855f611f515d271adba1a1/jax/_src/compiler.pyL387L389 may not be the root cause. The reason is that, on TPU v332, I added ```python print('distributed.global_state.process_id', distributed.global_state.process_id) ``` before these lines, but it only prints: ``` distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 distributed.global_state.process_id 0 ``` which means that `distributed.global_state.process_id != 0` is always `False`.",I've tested on TPU v332 and the compiled programs are indeed saved on all hosts. Seems that the issue is fixed now.,Screenshot: ![](https://github.com/userattachments/assets/0b22d2d63f5643f4b9c5307a64b16816)
907,"以下是一个github上的jax下的一个issue, 标题是(Jax GPU is slower than in CPU)， 内容是 ( Description Hi, I am trying to run Jax on an Nvidia GPU on a Ubuntu 20.04 computer. I succeeded to install it, but when I run my program, it is slower than with CPU. I tried to put the code from https://jax.readthedocs.io/en/latest/gpu_performance_tips.htmlxlaperformanceflags and https://jax.readthedocs.io/en/latest/gpu_performance_tips.htmlncclflags, at the beginning of my python file, but there is no change of the performances. Thanks in advance for helping me.  What jax/jaxlib version are you using? jax and jaxlib 0.4.19  Which accelerator(s) are you using? Nvidia GPU  Additional system info? Python 3.11, Ubuntu 20.04, Cuda 12.3, cudnn 8.9, driver Cuda 545.23.08  NVIDIA GPU info !image)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax GPU is slower than in CPU," Description Hi, I am trying to run Jax on an Nvidia GPU on a Ubuntu 20.04 computer. I succeeded to install it, but when I run my program, it is slower than with CPU. I tried to put the code from https://jax.readthedocs.io/en/latest/gpu_performance_tips.htmlxlaperformanceflags and https://jax.readthedocs.io/en/latest/gpu_performance_tips.htmlncclflags, at the beginning of my python file, but there is no change of the performances. Thanks in advance for helping me.  What jax/jaxlib version are you using? jax and jaxlib 0.4.19  Which accelerator(s) are you using? Nvidia GPU  Additional system info? Python 3.11, Ubuntu 20.04, Cuda 12.3, cudnn 8.9, driver Cuda 545.23.08  NVIDIA GPU info !image",2023-12-04T23:12:00Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/18816,"Thanks for the report! It will be hard to help without more information. For example, if your script is I/O bound, then the computational backend will not make much difference when it comes to endtoend execution. Can you put together a minimal reproducible example? Also please take a look at FAQ: Benchmarking JAX code for general tips on getting meaningful benchmarks of JAX code.","For a reproductible example, I got one optimization problem : you need to install the package noloadj (pip install noloadj) and run the 'optimize_mono' file  (in the zip folder attached to this message). With CPU optimization converges in 45 secunds, and with GPU in 55 secunds. I runned that code one year ago with an old jax version (0.1.67) and at  that time, optimization converges in 25 secunds with GPU, which was coherent. reproductible_example.zip","Thanks  if you can provide a selfcontained example without the need to download and extract a zip file, that would be helpful.","If you want a simpler example, you can test this : ```python import jax.numpy as np from noloadj.optimization.optimProblem import OptimProblem,Spec def StyblinskiTang(x):     fobj=0.5*(np.sum(x**4)16*np.sum(x**2)+5*np.sum(x))/len(x)     return locals().items() n=800 x0=[0. for i in range(n)] xbounds=[[5.,5.] for i in range(n)] spec=Spec(variables={'x':x0},bounds={'x':xbounds},objectives={'fobj':[0.,1.]}) optim=OptimProblem(StyblinskiTang,spec) from datetime import datetime start=datetime.today() optim.run() print(datetime.today()start) ``` Optimization converges in 13 secunds in CPU, and in 15 secunds in GPU. noloadj is a solvingoptimization problem library I contribued to develop. I just like to know if you have same performances than me, to understand if it is a problem of installation or a problem of parallelization of my code.","For the code example above, I also tested with a jacobian computation (jacfwd) instead of an optimization, and in that case, GPU is faster than CPU, which is coherent. ","Thanks  I'm not familiar with the `noloadj` package, and it look like it contains a pretty significant amount of code. I'm not sure offhand why it would execute faster on CPU than GPU; figuring out why would involve doing some profiling to look for the most costly operations on each device. There's some information on that here if you're interested in digging in: https://jax.readthedocs.io/en/latest/profiling.html Best of luck!",It looks like there's not enough information to answer this here – I'm going to close the issue because there's been no activity in quite a while. Please feel free to open another issue if you have further questions. Thanks!
2029,"以下是一个github上的jax下的一个issue, 标题是(Getting UserWarning: cloud_tpu_init failed)， 内容是 ( Description Trying to use TPU in CoLab and getting an error that asks to report a bug. ``` /usr/local/lib/python3.10/distpackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report ""  XlaRuntimeError                           Traceback (most recent call last) /usr/local/lib/python3.10/distpackages/jax/_src/lib/xla_bridge.py in backends()     332       try: > 333         backend = _init_backend(platform)     334         _backends[platform] = backend 7 frames XlaRuntimeError: NOT_FOUND: No ba16c7433 device found. During handling of the above exception, another exception occurred: RuntimeError                              Traceback (most recent call last) /usr/local/lib/python3.10/distpackages/jax/_src/lib/xla_bridge.py in backends()     348           if config.jax_platforms:     349             err_msg += "" (set JAX_PLATFORMS='' to automatically choose an available backend)"" > 350             raise RuntimeError(err_msg)     351           else:     352             _backends_errors[platform] = str(err) RuntimeError: Unable to initialize backend 'tpu': NOT_FOUND: No ba16c7433 device found. (set JAX_PLATFORMS='' to automatically choose an available backend) ``` CoLab: https://colab.research.google.com/drive/1tdAuqHBPTRV1lHXnfMv3CIDLDdCMEq?usp=sharing  What jax/jaxlib version are you using? 0.3.25 0.3.25  Which accelerator(s) are you using? TPU  Additional system info? 1.23.5 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] uname_result(system='Linux', node='e2a721115727', release='5.15.120+', version=' CC(Python 3 compatibility issues) SMP Wed Aug 30 11:19:59 UTC 2023', machine='x86_64')  NVIDIA GPU info N/A)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Getting UserWarning: cloud_tpu_init failed," Description Trying to use TPU in CoLab and getting an error that asks to report a bug. ``` /usr/local/lib/python3.10/distpackages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: KeyError('')  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report ""  XlaRuntimeError                           Traceback (most recent call last) /usr/local/lib/python3.10/distpackages/jax/_src/lib/xla_bridge.py in backends()     332       try: > 333         backend = _init_backend(platform)     334         _backends[platform] = backend 7 frames XlaRuntimeError: NOT_FOUND: No ba16c7433 device found. During handling of the above exception, another exception occurred: RuntimeError                              Traceback (most recent call last) /usr/local/lib/python3.10/distpackages/jax/_src/lib/xla_bridge.py in backends()     348           if config.jax_platforms:     349             err_msg += "" (set JAX_PLATFORMS='' to automatically choose an available backend)"" > 350             raise RuntimeError(err_msg)     351           else:     352             _backends_errors[platform] = str(err) RuntimeError: Unable to initialize backend 'tpu': NOT_FOUND: No ba16c7433 device found. (set JAX_PLATFORMS='' to automatically choose an available backend) ``` CoLab: https://colab.research.google.com/drive/1tdAuqHBPTRV1lHXnfMv3CIDLDdCMEq?usp=sharing  What jax/jaxlib version are you using? 0.3.25 0.3.25  Which accelerator(s) are you using? TPU  Additional system info? 1.23.5 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] uname_result(system='Linux', node='e2a721115727', release='5.15.120+', version=' CC(Python 3 compatibility issues) SMP Wed Aug 30 11:19:59 UTC 2023', machine='x86_64')  NVIDIA GPU info N/A",2023-12-02T05:07:05Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18780,"JAX versions 0.4 and greater do not support Colab TPUs at the moment because Colab TPUs use an older software architecture. That might change in the future, but at the moment if you want to use TPUs, your options are: a) use Colab TPU, but use an old JAX release (< 0.4). You're mostly on your own if you do this, since these JAX releases are now quite old and we aren't going to make fixes to old releases. I'd recommend against this. b) use Kaggle notebooks, which give a very similar experience to Colab, but offer TPUs supported by JAX, or c) use a Google Cloud TPU VM, on which you can run a selfhosted Colab kernel and connect to it from Colab. Closing because I don't think there's anything for us to do here. Hope that helps!",This is helpful! Thank you very much for the fast answer!
3042,"以下是一个github上的jax下的一个issue, 标题是(New random key design not numpy compatible)， 内容是 ( Description The new random key design introduces some sharpedges when interoperating with numpy.  This leads to problems in a number of dependent libraries built on jax. For example in chex https://github.com/googledeepmind/chex/issues/318: ```python import jax import chex chex.assert_trees_all_close(jax.random.PRNGKey(0), jax.random.PRNGKey(0))   Passes chex.assert_trees_all_close(jax.random.key(0), jax.random.key(0)) >> TypeError                                 Traceback (most recent call last) Cell In[62], line 1 > 1 chex.assert_trees_all_close(jax.random.key(0), jax.random.key(0)) ... File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/numpy/testing/_private/utils.py:1499, in assert_allclose..compare(x, y)    1498 def compare(x, y): > 1499     return np.core.numeric.isclose(x, y, rtol=rtol, atol=atol,    1500                                    equal_nan=equal_nan) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/numpy/core/numeric.py:2348, in isclose(a, b, rtol, atol, equal_nan)    2345     dt = multiarray.result_type(y, 1.)    2346     y = asanyarray(y, dtype=dt) > 2348 xfin = isfinite(x)    2349 yfin = isfinite(y)    2350 if all(xfin) and all(yfin): TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'' ``` I'm also running into issues with Orbax and serializing the random keys since the `dtype` cannot be converted to a valid numpy array. The problem is that Orbax tries to write `dtype` metadata by converting the jax `dtype` to a numpy `dtype`. But this of course leads to: ```python import jax import numpy as np key = jax.random.key(0) np.dtype(key.dtype).str >> TypeError                                 Traceback (most recent call last) Cell In[64], line 2       1 key = jax.random.key(0) > 2 np.dtype(key.dtype).str TypeError: Cannot interpret 'key' as a data type ``` I can write a custom TypeHandler or a forward and inverse transformation for the serialization, this is fine. The problem is that the keys produced by `jax.random.key` hide their data under private attributes `_base_array` and `_impl`. These should either be public, or jax should properly serialize the keys with the numpy API. It would be better if numpy conversion would just drop the PRNG implementation metadata in favor of compatibility: ```python import numpy as np import jax np.asanyarray(jax.random.PRNGKey(0)) >> array([0, 0], dtype=uint32) np.asanyarray(jax.random.key(0))   Should simply give: array([0, 0], dtype=uint32) >> array(Array((), dtype=key) overlaying: [0 0], dtype=object)   Current ```  What jax/jaxlib version are you using? 0.4.20 0.4.20  Which accelerator(s) are you using? _No response_  Additional system info? Windows WSL2.0  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,New random key design not numpy compatible," Description The new random key design introduces some sharpedges when interoperating with numpy.  This leads to problems in a number of dependent libraries built on jax. For example in chex https://github.com/googledeepmind/chex/issues/318: ```python import jax import chex chex.assert_trees_all_close(jax.random.PRNGKey(0), jax.random.PRNGKey(0))   Passes chex.assert_trees_all_close(jax.random.key(0), jax.random.key(0)) >> TypeError                                 Traceback (most recent call last) Cell In[62], line 1 > 1 chex.assert_trees_all_close(jax.random.key(0), jax.random.key(0)) ... File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/numpy/testing/_private/utils.py:1499, in assert_allclose..compare(x, y)    1498 def compare(x, y): > 1499     return np.core.numeric.isclose(x, y, rtol=rtol, atol=atol,    1500                                    equal_nan=equal_nan) File ~/anaconda3/envs/jax3.10/lib/python3.10/sitepackages/numpy/core/numeric.py:2348, in isclose(a, b, rtol, atol, equal_nan)    2345     dt = multiarray.result_type(y, 1.)    2346     y = asanyarray(y, dtype=dt) > 2348 xfin = isfinite(x)    2349 yfin = isfinite(y)    2350 if all(xfin) and all(yfin): TypeError: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'' ``` I'm also running into issues with Orbax and serializing the random keys since the `dtype` cannot be converted to a valid numpy array. The problem is that Orbax tries to write `dtype` metadata by converting the jax `dtype` to a numpy `dtype`. But this of course leads to: ```python import jax import numpy as np key = jax.random.key(0) np.dtype(key.dtype).str >> TypeError                                 Traceback (most recent call last) Cell In[64], line 2       1 key = jax.random.key(0) > 2 np.dtype(key.dtype).str TypeError: Cannot interpret 'key' as a data type ``` I can write a custom TypeHandler or a forward and inverse transformation for the serialization, this is fine. The problem is that the keys produced by `jax.random.key` hide their data under private attributes `_base_array` and `_impl`. These should either be public, or jax should properly serialize the keys with the numpy API. It would be better if numpy conversion would just drop the PRNG implementation metadata in favor of compatibility: ```python import numpy as np import jax np.asanyarray(jax.random.PRNGKey(0)) >> array([0, 0], dtype=uint32) np.asanyarray(jax.random.key(0))   Should simply give: array([0, 0], dtype=uint32) >> array(Array((), dtype=key) overlaying: [0 0], dtype=object)   Current ```  What jax/jaxlib version are you using? 0.4.20 0.4.20  Which accelerator(s) are you using? _No response_  Additional system info? Windows WSL2.0  NVIDIA GPU info _No response_",2023-12-01T14:00:34Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18767,"Thanks for the report. I think this is working as expected: the new key data type is not recognized by numpy, so you will not be able to use it with numpy functions. We understand that some downstream libraries are not yet compatible with the new key types; this is the reason we have not yet switched over to recommending the newstyle universally (e.g. all the JAX docs still recommend `jax.random.PRNGKey` at this point).","> Thanks for the report. I think this is working as expected: the new key data type is not recognized by numpy, so you will not be able to use it with numpy functions. >  > We understand that some downstream libraries are not yet compatible with the new key types; this is the reason we have not yet switched over to recommending the newstyle universally (e.g. all the JAX docs still recommend `jax.random.PRNGKey` at this point). I see, thanks for clarifying. I thought this was unintentional."
5987,"以下是一个github上的jax下的一个issue, 标题是(JAX should allow stack frames to be hidden in tracebacks)， 内容是 (**Context** When `jax.config.include_full_tracebacks_in_locations` is set to `True`, JAX propagates Python stack trace metadata to XLA. This can be useful for debugging. (If this option is set to `False`, only a single filename + line number is propagated). For models that have been constructed using frameworks layered on top of JAX, these stack traces can be quite noisy with many frames coming from implementation details of the frameworks. JAX already suppresses its own frames. An example is ```  /opt/paxml/paxml/main.py:549[]  /usr/local/lib/python3.10/distpackages/absl/app.py:308[run]  /usr/local/lib/python3.10/distpackages/absl/app.py:254[_run_main]  /opt/paxml/paxml/main.py:464[main]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/main.py:526[_main]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/main.py:439[run]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/main.py:303[run_experiment]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/train.py:278[train_and_evaluate]  /opt/paxml/paxml/executors.py:269[start]  /opt/paxml/paxml/executors.py:410[_train_and_evaluate_common]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/programs.py:346[run]  /opt/paxml/paxml/programs.py:637[train_step]  /opt/paxml/paxml/trainer_lib.py:1744[call]  /opt/paxml/paxml/partitioning.py:1002[_wrapped_step_fn]  /opt/paxml/paxml/trainer_lib.py:1112[train_step_single_learner]  /opt/paxml/paxml/trainer_lib.py:962[grad_fn]  /opt/paxml/paxml/trainer_lib.py:956[_loss]  /opt/paxml/paxml/trainer_lib.py:839[_loss_fn]  /opt/paxml/paxml/trainer_lib.py:798[_default_apply_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/base_layer.py:1886[apply]  /opt/flax/flax/linen/module.py:2008[apply]  /opt/flax/flax/core/scope.py:1073[wrapper]  /opt/flax/flax/linen/module.py:2695[scope_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/base_model.py:111[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/models.py:277[compute_predictions]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/transformer_models.py:814[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/transformers.py:2279[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:941[__call__]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:260[wrapper]  /opt/flax/flax/core/lift.py:341[wrapper]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:269[wrapper]  /opt/flax/flax/core/lift.py:991[inner]  /opt/flax/flax/core/axes_scan.py:161[scan_fn]  /opt/flax/flax/core/axes_scan.py:120[body_fn]  /opt/flax/flax/core/lift.py:972[scanned]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:260[wrapper]  /opt/flax/flax/core/lift.py:1433[inner]  /opt/flax/flax/core/lift.py:1430[rematted]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:744[_scan_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:487[body_fprop]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:269[wrapper]  /opt/flax/flax/core/lift.py:817[inner]  /opt/flax/flax/core/lift.py:814[mapped]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:391[body_fn]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:260[wrapper]  /opt/flax/flax/core/lift.py:341[wrapper]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:364[layer_fprop]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/transformers.py:1789[__call__]  /opt/praxis/praxis/layers/transformers.py:1769[_fprop]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/transformers.py:1352[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/attentions.py:1727[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/attentions.py:1540[_dot_atten] ``` which contains various ""wrapper"", ""wrapped"", etc. frames. **Suggestion** If JAX provided some relevant tools, frameworks could hint that some of their implementation details should not be shown by default in tools that present this information. JAX already does something similar internally to suppress its own implementation details. cc:   )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,JAX should allow stack frames to be hidden in tracebacks,"**Context** When `jax.config.include_full_tracebacks_in_locations` is set to `True`, JAX propagates Python stack trace metadata to XLA. This can be useful for debugging. (If this option is set to `False`, only a single filename + line number is propagated). For models that have been constructed using frameworks layered on top of JAX, these stack traces can be quite noisy with many frames coming from implementation details of the frameworks. JAX already suppresses its own frames. An example is ```  /opt/paxml/paxml/main.py:549[]  /usr/local/lib/python3.10/distpackages/absl/app.py:308[run]  /usr/local/lib/python3.10/distpackages/absl/app.py:254[_run_main]  /opt/paxml/paxml/main.py:464[main]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/main.py:526[_main]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/main.py:439[run]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/main.py:303[run_experiment]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/train.py:278[train_and_evaluate]  /opt/paxml/paxml/executors.py:269[start]  /opt/paxml/paxml/executors.py:410[_train_and_evaluate_common]  /opt/praxis/praxis/py_utils.py:1013[wrapper]  /opt/paxml/paxml/programs.py:346[run]  /opt/paxml/paxml/programs.py:637[train_step]  /opt/paxml/paxml/trainer_lib.py:1744[call]  /opt/paxml/paxml/partitioning.py:1002[_wrapped_step_fn]  /opt/paxml/paxml/trainer_lib.py:1112[train_step_single_learner]  /opt/paxml/paxml/trainer_lib.py:962[grad_fn]  /opt/paxml/paxml/trainer_lib.py:956[_loss]  /opt/paxml/paxml/trainer_lib.py:839[_loss_fn]  /opt/paxml/paxml/trainer_lib.py:798[_default_apply_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/base_layer.py:1886[apply]  /opt/flax/flax/linen/module.py:2008[apply]  /opt/flax/flax/core/scope.py:1073[wrapper]  /opt/flax/flax/linen/module.py:2695[scope_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/base_model.py:111[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/models.py:277[compute_predictions]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/transformer_models.py:814[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/transformers.py:2279[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:941[__call__]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:260[wrapper]  /opt/flax/flax/core/lift.py:341[wrapper]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:269[wrapper]  /opt/flax/flax/core/lift.py:991[inner]  /opt/flax/flax/core/axes_scan.py:161[scan_fn]  /opt/flax/flax/core/axes_scan.py:120[body_fn]  /opt/flax/flax/core/lift.py:972[scanned]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:260[wrapper]  /opt/flax/flax/core/lift.py:1433[inner]  /opt/flax/flax/core/lift.py:1430[rematted]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:744[_scan_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:487[body_fprop]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:269[wrapper]  /opt/flax/flax/core/lift.py:817[inner]  /opt/flax/flax/core/lift.py:814[mapped]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:391[body_fn]  /opt/flax/flax/linen/transforms.py:425[wrapped_fn]  /opt/flax/flax/core/lift.py:260[wrapper]  /opt/flax/flax/core/lift.py:341[wrapper]  /opt/flax/flax/linen/transforms.py:401[core_fn]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/pipeline.py:364[layer_fprop]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/transformers.py:1789[__call__]  /opt/praxis/praxis/layers/transformers.py:1769[_fprop]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/transformers.py:1352[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/attentions.py:1727[__call__]  /opt/flax/flax/linen/module.py:611[wrapped_module_method]  /opt/flax/flax/linen/module.py:1131[_call_wrapped_method]  /opt/praxis/praxis/layers/attentions.py:1540[_dot_atten] ``` which contains various ""wrapper"", ""wrapped"", etc. frames. **Suggestion** If JAX provided some relevant tools, frameworks could hint that some of their implementation details should not be shown by default in tools that present this information. JAX already does something similar internally to suppress its own implementation details. cc:   ",2023-12-01T12:50:06Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/18764
556,"以下是一个github上的jax下的一个issue, 标题是(Option to sanitise MLIR dumps)， 内容是 (When JAX is exporting MLIR (via whatever incantation you like, my preferred one is `JAX_DUMP_IR_TO=""tmp/jax_logs""`) a lot of sensitive information is included, in the form of lines that look like this ``` loc105 = loc(""state.params[“all”][“my”].company.everything.proprietary"") ``` It would be good to have an option to emit sanitised mlir dumps. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Option to sanitise MLIR dumps,"When JAX is exporting MLIR (via whatever incantation you like, my preferred one is `JAX_DUMP_IR_TO=""tmp/jax_logs""`) a lot of sensitive information is included, in the form of lines that look like this ``` loc105 = loc(""state.params[“all”][“my”].company.everything.proprietary"") ``` It would be good to have an option to emit sanitised mlir dumps. ",2023-11-29T23:08:39Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/18734,This should be easy enough to do. We have code in the compilation cache that strips MLIR location information for caching. We just need to do the same in the IR dumping path.
493,"以下是一个github上的jax下的一个issue, 标题是(Downgrade a bunch of logging to DEBUG)， 内容是 (The logs related to compilation cache ended up being quite chatty, which is quite unlike the other logs in JAX. This downgrades a bunch of them to debug, as they can always be enabled independently using JAX config. This should also fix the recent failures in logging_test.py.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,Downgrade a bunch of logging to DEBUG,"The logs related to compilation cache ended up being quite chatty, which is quite unlike the other logs in JAX. This downgrades a bunch of them to debug, as they can always be enabled independently using JAX config. This should also fix the recent failures in logging_test.py.",2023-11-29T12:12:23Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18724
9997,"以下是一个github上的jax下的一个issue, 标题是(Replace apply_primitive internals with `jax.jit`.)， 内容是 (Replace apply_primitive internals with `jax.jit`. This allows deletion of a lot of code and leads to ~40% eager performance speedup. Benchmarks: ``` name                                                      old time/op          new time/op          delta eager_unary_dispatch                                      31.3µs ± 1%          19.4µs ± 6%  37.91%    (p=0.016 n=4+5) eager_unary                                               32.1µs ± 0%          19.8µs ± 4%  38.26%    (p=0.016 n=4+5) eager_binary_dispatch                                     35.9µs ± 1%          20.5µs ± 4%  42.93%    (p=0.016 n=4+5) eager_binary                                              36.6µs ± 1%          21.1µs ± 4%  42.29%    (p=0.016 n=4+5) jit_trivial_dispatch                                      3.87µs ± 2%          4.12µs ±25%     ~       (p=1.000 n=5+5) jit_trivial                                               4.75µs ± 2%          4.82µs ±11%     ~       (p=0.690 n=5+5) jit_simple_dispatch                                       2.95µs ± 2%          2.97µs ± 7%     ~       (p=1.000 n=5+5) jit_simple                                                3.52µs ± 6%          3.51µs ± 5%     ~       (p=0.841 n=5+5) jit_simple_dispatch_array                                 2.95µs ± 2%          2.96µs ± 6%     ~       (p=1.000 n=5+5) jit_simple_array                                          3.46µs ± 2%          3.51µs ± 5%     ~       (p=0.690 n=5+5) jit_small_matmul                                          3.01µs ± 1%          3.00µs ± 4%     ~       (p=0.548 n=5+5) jit_big_matmul                                            34.0µs ±18%          35.5µs ±17%     ~       (p=0.310 n=5+5) jit_simple_many_args_dispatch/num_args:10                 6.93µs ± 6%          6.80µs ± 6%     ~     (p=0.481 n=10+10) jit_simple_many_args_dispatch/num_args:100                47.7µs ± 7%          45.4µs ± 2%     ~      (p=0.237 n=10+8) jit_simple_many_args_dispatch/num_args:1000                545µs ± 8%           516µs ± 2%     ~      (p=0.101 n=10+8) jit_simple_many_args_dispatch/num_args:2000               1.12ms ± 7%          1.07ms ± 2%     ~      (p=0.237 n=10+8) jit_simple_many_args/num_args:10                          7.42µs ± 5%          7.23µs ± 2%     ~      (p=0.173 n=10+8) jit_simple_many_args/num_args:100                         48.4µs ± 7%          45.6µs ± 2%     ~      (p=0.237 n=10+8) jit_simple_many_args/num_args:1000                         542µs ± 6%           524µs ± 8%     ~     (p=0.089 n=10+10) jit_simple_many_args/num_args:2000                        1.12ms ± 7%          1.08ms ± 1%     ~      (p=0.068 n=10+8) jit_simple_pruned_args_dispatch_10                        4.79µs ± 8%          4.98µs ±10%     ~       (p=0.421 n=5+5) jit_simple_pruned_args_10                                 5.32µs ± 6%          5.30µs ± 4%     ~       (p=1.000 n=5+5) jit_simple_pruned_args_dispatch_100                       24.7µs ± 6%          23.8µs ± 8%     ~       (p=0.548 n=5+5) jit_simple_pruned_args_100                                25.2µs ± 6%          24.4µs ± 8%     ~       (p=0.690 n=5+5) jit_simple_pruned_args_dispatch_1000                       238µs ± 7%           232µs ± 8%     ~       (p=0.841 n=5+5) jit_simple_pruned_args_1000                                240µs ± 7%           234µs ± 8%     ~       (p=1.000 n=5+5) jit_simple_pruned_args_dispatch_2000                       516µs ± 6%           497µs ± 1%     ~       (p=0.413 n=5+4) jit_simple_pruned_args_2000                                517µs ± 6%           505µs ± 7%     ~       (p=0.690 n=5+5) jit_dispatch_without_transfer                              719µs ± 9%           751µs ± 8%     ~       (p=0.222 n=5+5) jit_dispatch_with_transfer                                 799µs ±14%           793µs ± 9%     ~       (p=1.000 n=5+5) pmap_trivial_2_devices                                    49.9µs ±40%          48.2µs ±42%     ~       (p=0.841 n=5+5) pmap_trivial_dispatch_8_devices                           74.5µs ±24%          78.9µs ±29%     ~       (p=0.421 n=5+5) pmap_trivial_8_devices                                    79.3µs ± 6%          82.7µs ±20%     ~       (p=0.841 n=5+5) pmap_simple_2_devices                                     47.1µs ±17%          49.1µs ±20%     ~       (p=0.548 n=5+5) pmap_simple_dispatch_8_devices                            73.4µs ±16%          76.8µs ±21%     ~       (p=0.690 n=5+5) pmap_simple_8_devices                                     76.0µs ±10%          80.6µs ±29%     ~       (p=1.000 n=5+5) pmap_simple_dispatch_8_devices_100_args                   1.12ms ±22%          1.08ms ±42%     ~       (p=0.841 n=5+5) pmap_simple_8_devices_100_args                            12.5ms ± 8%          12.8ms ±10%     ~       (p=1.000 n=5+5) sda_index_1                                                413µs ± 1%           686µs ± 4%  +66.08%    (p=0.008 n=5+5) sda_index_2                                                850µs ± 1%          1378µs ± 4%  +62.02%    (p=0.008 n=5+5) sda_index_8                                               3.60ms ± 1%          5.69ms ± 4%  +58.00%    (p=0.008 n=5+5) bench_shaped_abstractify                                   300µs ± 1%           305µs ± 3%     ~       (p=0.056 n=5+5) bench_xla_abstractify_scalar_int                          6.45µs ± 1%          6.50µs ± 3%     ~       (p=0.548 n=5+5) bench_xla_abstractify_scalar_float                        3.73µs ± 1%          3.73µs ± 3%     ~       (p=0.690 n=5+5) bench_xla_abstractify_scalar_numpy_int32                  4.97µs ± 1%          4.83µs ± 3%     ~       (p=0.095 n=5+5) bench_xla_abstractify_scalar_numpy_uint32                 4.91µs ± 1%          4.75µs ± 0%   3.30%    (p=0.016 n=5+4) bench_xla_abstractify_numpy_random                        4.34µs ± 2%          4.31µs ± 3%     ~       (p=0.310 n=5+5) bench_xla_abstractify_numpy_arange_100_float32            3.94µs ± 1%          3.93µs ± 3%     ~       (p=0.548 n=5+5) bench_xla_abstractify_enum                                6.85µs ± 1%          7.06µs ± 7%   +3.07%    (p=0.032 n=5+5) bench_are_op_shardings_equal                              26.9µs ± 2%          27.0µs ± 3%     ~       (p=0.841 n=5+5) bench_pjit_check_aval_sharding                             691µs ± 2%           711µs ±13%     ~       (p=0.841 n=5+5) bench_addressable_shards_index                             656ns ± 4%           688ns ± 9%     ~       (p=0.095 n=5+5) bench_remat_eager_retracing_overheads                     12.7ms ± 4%          10.7ms ± 1%  15.48%    (p=0.016 n=5+4) bench_remat_eager_retracing_overheads_static_argnums      13.0ms ± 2%          11.3ms ± 6%  13.71%    (p=0.008 n=5+5) bench_slicing_compilation                                 12.1ms ± 1%          12.3ms ± 4%     ~       (p=0.690 n=5+5) bench_slicing_compilation2                                11.3ms ± 0%          11.5ms ± 6%     ~       (p=0.690 n=5+5) bench_repeated_static_indexing                            62.5ms ± 2%          40.8ms ± 8%  34.77%    (p=0.008 n=5+5) bench_repeated_static_slicing                             46.7ms ± 1%          31.4ms ± 2%  32.76%    (p=0.008 n=5+5) pjit_simple_1_device/num_args:1                           2.72µs ± 2%          2.68µs ± 5%     ~       (p=0.151 n=5+5) pjit_simple_1_device/num_args:10                          12.6µs ± 7%          12.3µs ± 3%     ~       (p=0.310 n=5+5) pjit_simple_1_device/num_args:100                          109µs ± 3%           108µs ± 4%     ~       (p=0.548 n=5+5) pjit_simple_4_device/num_args:1                           38.0µs ±26%          36.8µs ±19%     ~       (p=0.690 n=5+5) pjit_simple_4_device/num_args:10                          93.3µs ±19%          96.6µs ±23%     ~       (p=0.841 n=5+5) pjit_simple_4_device/num_args:100                          730µs ±16%           698µs ±48%     ~       (p=0.841 n=5+5) pjit_aot_1_device/num_args:1                              3.29µs ± 2%          3.12µs ± 4%   5.24%    (p=0.016 n=4+5) pjit_aot_1_device/num_args:10                             13.0µs ± 1%          12.7µs ± 2%     ~       (p=0.063 n=4+5) pjit_aot_1_device/num_args:100                             111µs ± 5%           110µs ±11%     ~       (p=0.421 n=5+5) pjit_aot_4_device/num_args:1                              38.4µs ±19%          38.9µs ±24%     ~       (p=1.000 n=5+5) pjit_aot_4_device/num_args:10                             91.3µs ±15%          96.9µs ±29%     ~       (p=0.548 n=5+5) pjit_aot_4_device/num_args:100                             676µs ±20%           689µs ±41%     ~       (p=0.841 n=5+5) host_local_array_to_global_array                           196µs ± 6%           194µs ± 4%     ~       (p=0.548 n=5+5) device_put                                                50.8µs ± 1%          50.7µs ± 4%     ~       (p=0.413 n=4+5) device_put_sharded                                         176µs ± 0%           177µs ± 4%     ~       (p=0.190 n=4+5) device_get_8_devices                                      3.96ms ± 4%          4.03ms ± 7%     ~       (p=0.413 n=4+5) np_asarray_8_devices                                      3.34ms ±18%          3.30ms ±10%     ~       (p=0.548 n=5+5) jax_array_arrays_8_devices                                5.01ms ±10%          5.09ms ±21%     ~       (p=0.421 n=5+5) batch_inplace_while_scatter                                440µs ± 1%           439µs ± 1%     ~       (p=0.421 n=5+5) batch_inplace_while_dynamic_update_slice                   454µs ± 0%           457µs ± 1%     ~       (p=0.905 n=4+5) serial_dot_products                                       4.51µs ± 3%          4.41µs ± 2%     ~       (p=0.151 n=5+5) bench_make_array_from_callback_fully_replicated_sharding  26.6µs ± 1%          27.0µs ± 2%     ~       (p=0.056 n=5+5) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Replace apply_primitive internals with `jax.jit`.,Replace apply_primitive internals with `jax.jit`. This allows deletion of a lot of code and leads to ~40% eager performance speedup. Benchmarks: ``` name                                                      old time/op          new time/op          delta eager_unary_dispatch                                      31.3µs ± 1%          19.4µs ± 6%  37.91%    (p=0.016 n=4+5) eager_unary                                               32.1µs ± 0%          19.8µs ± 4%  38.26%    (p=0.016 n=4+5) eager_binary_dispatch                                     35.9µs ± 1%          20.5µs ± 4%  42.93%    (p=0.016 n=4+5) eager_binary                                              36.6µs ± 1%          21.1µs ± 4%  42.29%    (p=0.016 n=4+5) jit_trivial_dispatch                                      3.87µs ± 2%          4.12µs ±25%     ~       (p=1.000 n=5+5) jit_trivial                                               4.75µs ± 2%          4.82µs ±11%     ~       (p=0.690 n=5+5) jit_simple_dispatch                                       2.95µs ± 2%          2.97µs ± 7%     ~       (p=1.000 n=5+5) jit_simple                                                3.52µs ± 6%          3.51µs ± 5%     ~       (p=0.841 n=5+5) jit_simple_dispatch_array                                 2.95µs ± 2%          2.96µs ± 6%     ~       (p=1.000 n=5+5) jit_simple_array                                          3.46µs ± 2%          3.51µs ± 5%     ~       (p=0.690 n=5+5) jit_small_matmul                                          3.01µs ± 1%          3.00µs ± 4%     ~       (p=0.548 n=5+5) jit_big_matmul                                            34.0µs ±18%          35.5µs ±17%     ~       (p=0.310 n=5+5) jit_simple_many_args_dispatch/num_args:10                 6.93µs ± 6%          6.80µs ± 6%     ~     (p=0.481 n=10+10) jit_simple_many_args_dispatch/num_args:100                47.7µs ± 7%          45.4µs ± 2%     ~      (p=0.237 n=10+8) jit_simple_many_args_dispatch/num_args:1000                545µs ± 8%           516µs ± 2%     ~      (p=0.101 n=10+8) jit_simple_many_args_dispatch/num_args:2000               1.12ms ± 7%          1.07ms ± 2%     ~      (p=0.237 n=10+8) jit_simple_many_args/num_args:10                          7.42µs ± 5%          7.23µs ± 2%     ~      (p=0.173 n=10+8) jit_simple_many_args/num_args:100                         48.4µs ± 7%          45.6µs ± 2%     ~      (p=0.237 n=10+8) jit_simple_many_args/num_args:1000                         542µs ± 6%           524µs ± 8%     ~     (p=0.089 n=10+10) jit_simple_many_args/num_args:2000                        1.12ms ± 7%          1.08ms ± 1%     ~      (p=0.068 n=10+8) jit_simple_pruned_args_dispatch_10                        4.79µs ± 8%          4.98µs ±10%     ~       (p=0.421 n=5+5) jit_simple_pruned_args_10                                 5.32µs ± 6%          5.30µs ± 4%     ~       (p=1.000 n=5+5) jit_simple_pruned_args_dispatch_100                       24.7µs ± 6%          23.8µs ± 8%     ~       (p=0.548 n=5+5) jit_simple_pruned_args_100                                25.2µs ± 6%          24.4µs ± 8%     ~       (p=0.690 n=5+5) jit_simple_pruned_args_dispatch_1000                       238µs ± 7%           232µs ± 8%     ~       (p=0.841 n=5+5) jit_simple_pruned_args_1000                                240µs ± 7%           234µs ± 8%     ~       (p=1.000 n=5+5) jit_simple_pruned_args_dispatch_2000                       516µs ± 6%           497µs ± 1%     ~       (p=0.413 n=5+4) jit_simple_pruned_args_2000                                517µs ± 6%           505µs ± 7%     ~       (p=0.690 n=5+5) jit_dispatch_without_transfer                              719µs ± 9%           751µs ± 8%     ~       (p=0.222 n=5+5) jit_dispatch_with_transfer                                 799µs ±14%           793µs ± 9%     ~       (p=1.000 n=5+5) pmap_trivial_2_devices                                    49.9µs ±40%          48.2µs ±42%     ~       (p=0.841 n=5+5) pmap_trivial_dispatch_8_devices                           74.5µs ±24%          78.9µs ±29%     ~       (p=0.421 n=5+5) pmap_trivial_8_devices                                    79.3µs ± 6%          82.7µs ±20%     ~       (p=0.841 n=5+5) pmap_simple_2_devices                                     47.1µs ±17%          49.1µs ±20%     ~       (p=0.548 n=5+5) pmap_simple_dispatch_8_devices                            73.4µs ±16%          76.8µs ±21%     ~       (p=0.690 n=5+5) pmap_simple_8_devices                                     76.0µs ±10%          80.6µs ±29%     ~       (p=1.000 n=5+5) pmap_simple_dispatch_8_devices_100_args                   1.12ms ±22%          1.08ms ±42%     ~       (p=0.841 n=5+5) pmap_simple_8_devices_100_args                            12.5ms ± 8%          12.8ms ±10%     ~       (p=1.000 n=5+5) sda_index_1                                                413µs ± 1%           686µs ± 4%  +66.08%    (p=0.008 n=5+5) sda_index_2                                                850µs ± 1%          1378µs ± 4%  +62.02%    (p=0.008 n=5+5) sda_index_8                                               3.60ms ± 1%          5.69ms ± 4%  +58.00%    (p=0.008 n=5+5) bench_shaped_abstractify                                   300µs ± 1%           305µs ± 3%     ~       (p=0.056 n=5+5) bench_xla_abstractify_scalar_int                          6.45µs ± 1%          6.50µs ± 3%     ~       (p=0.548 n=5+5) bench_xla_abstractify_scalar_float                        3.73µs ± 1%          3.73µs ± 3%     ~       (p=0.690 n=5+5) bench_xla_abstractify_scalar_numpy_int32                  4.97µs ± 1%          4.83µs ± 3%     ~       (p=0.095 n=5+5) bench_xla_abstractify_scalar_numpy_uint32                 4.91µs ± 1%          4.75µs ± 0%   3.30%    (p=0.016 n=5+4) bench_xla_abstractify_numpy_random                        4.34µs ± 2%          4.31µs ± 3%     ~       (p=0.310 n=5+5) bench_xla_abstractify_numpy_arange_100_float32            3.94µs ± 1%          3.93µs ± 3%     ~       (p=0.548 n=5+5) bench_xla_abstractify_enum                                6.85µs ± 1%          7.06µs ± 7%   +3.07%    (p=0.032 n=5+5) bench_are_op_shardings_equal                              26.9µs ± 2%          27.0µs ± 3%     ~       (p=0.841 n=5+5) bench_pjit_check_aval_sharding                             691µs ± 2%           711µs ±13%     ~       (p=0.841 n=5+5) bench_addressable_shards_index                             656ns ± 4%           688ns ± 9%     ~       (p=0.095 n=5+5) bench_remat_eager_retracing_overheads                     12.7ms ± 4%          10.7ms ± 1%  15.48%    (p=0.016 n=5+4) bench_remat_eager_retracing_overheads_static_argnums      13.0ms ± 2%          11.3ms ± 6%  13.71%    (p=0.008 n=5+5) bench_slicing_compilation                                 12.1ms ± 1%          12.3ms ± 4%     ~       (p=0.690 n=5+5) bench_slicing_compilation2                                11.3ms ± 0%          11.5ms ± 6%     ~       (p=0.690 n=5+5) bench_repeated_static_indexing                            62.5ms ± 2%          40.8ms ± 8%  34.77%    (p=0.008 n=5+5) bench_repeated_static_slicing                             46.7ms ± 1%          31.4ms ± 2%  32.76%    (p=0.008 n=5+5) pjit_simple_1_device/num_args:1                           2.72µs ± 2%          2.68µs ± 5%     ~       (p=0.151 n=5+5) pjit_simple_1_device/num_args:10                          12.6µs ± 7%          12.3µs ± 3%     ~       (p=0.310 n=5+5) pjit_simple_1_device/num_args:100                          109µs ± 3%           108µs ± 4%     ~       (p=0.548 n=5+5) pjit_simple_4_device/num_args:1                           38.0µs ±26%          36.8µs ±19%     ~       (p=0.690 n=5+5) pjit_simple_4_device/num_args:10                          93.3µs ±19%          96.6µs ±23%     ~       (p=0.841 n=5+5) pjit_simple_4_device/num_args:100                          730µs ±16%           698µs ±48%     ~       (p=0.841 n=5+5) pjit_aot_1_device/num_args:1                              3.29µs ± 2%          3.12µs ± 4%   5.24%    (p=0.016 n=4+5) pjit_aot_1_device/num_args:10                             13.0µs ± 1%          12.7µs ± 2%     ~       (p=0.063 n=4+5) pjit_aot_1_device/num_args:100                             111µs ± 5%           110µs ±11%     ~       (p=0.421 n=5+5) pjit_aot_4_device/num_args:1                              38.4µs ±19%          38.9µs ±24%     ~       (p=1.000 n=5+5) pjit_aot_4_device/num_args:10                             91.3µs ±15%          96.9µs ±29%     ~       (p=0.548 n=5+5) pjit_aot_4_device/num_args:100                             676µs ±20%           689µs ±41%     ~       (p=0.841 n=5+5) host_local_array_to_global_array                           196µs ± 6%           194µs ± 4%     ~       (p=0.548 n=5+5) device_put                                                50.8µs ± 1%          50.7µs ± 4%     ~       (p=0.413 n=4+5) device_put_sharded                                         176µs ± 0%           177µs ± 4%     ~       (p=0.190 n=4+5) device_get_8_devices                                      3.96ms ± 4%          4.03ms ± 7%     ~       (p=0.413 n=4+5) np_asarray_8_devices                                      3.34ms ±18%          3.30ms ±10%     ~       (p=0.548 n=5+5) jax_array_arrays_8_devices                                5.01ms ±10%          5.09ms ±21%     ~       (p=0.421 n=5+5) batch_inplace_while_scatter                                440µs ± 1%           439µs ± 1%     ~       (p=0.421 n=5+5) batch_inplace_while_dynamic_update_slice                   454µs ± 0%           457µs ± 1%     ~       (p=0.905 n=4+5) serial_dot_products                                       4.51µs ± 3%          4.41µs ± 2%     ~       (p=0.151 n=5+5) bench_make_array_from_callback_fully_replicated_sharding  26.6µs ± 1%          27.0µs ± 2%     ~       (p=0.056 n=5+5) ```,2023-11-29T04:46:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18722
3686,"以下是一个github上的jax下的一个issue, 标题是([shard-map] fix transpose replication checking bug with integer_pow)， 内容是 (At HEAD this `shard_map` code crashes with a replication checking error: ```python def loss(w, x):   (shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())   def f(x):     return jax.lax.psum(((w * x) ** 2).sum(), 'i')   return f(x) jax.grad(loss)(3.0, jnp.arange(8.)) ``` ``` Traceback (most recent call last):   File ""/usr/local/google/home/mattjj/packages/jax/shmap_check_rep_bug.py"", line 23, in      jax.grad(loss)(3.0, jnp.arange(8.))   File ""/usr/local/google/home/mattjj/packages/jax/shmap_check_rep_bug.py"", line 21, in loss     return f(x)   File ""/usr/local/google/home/mattjj/packages/jax/shmap_check_rep_bug.py"", line 20, in f     return jax.lax.psum(((w * x) ** 2).sum(), 'i')   File ""/usr/local/google/home/mattjj/packages/jax/jax/_src/numpy/array_methods.py"", line 744, in op     return getattr(self.aval, f""_{name}"")(self, *args)   File ""/usr/local/google/home/mattjj/packages/jax/jax/_src/numpy/array_methods.py"", line 272, in deferring_binary_op     return binary_op(*args)   File ""/usr/local/google/home/mattjj/packages/jax/jax/_src/numpy/ufuncs.py"", line 339, in power     return lax.integer_pow(x1, x2) Exception: Primitive mul requires argument replication types to match, but got ({'i'}, set()). Please open an     issue at https://github.com/google/jax/issues ``` The narrow issue is that the JVP of `integer_pow[y=2] x` produces a `mul x 2`, and so when `x` is devicevarying      across axis `'i'`, our checking rule is unhappy that `mul` gets two arguments with different replication types (where the replication type of `2` is just `{}`, i.e. it's not devicevarying over any axes). This is not an issue with differentiating `pow x 2`, since we would rewrite that to `pow x (pbroadcast[axes='i'] 2)`! The broader issue is that we're running checks _after_ transformations, but only performing the `pbroadcast`inserting rewrite _before_ transformations. That inconsistency means that rules, especially constants in rules, don't have `pbroadcast`s inserted. (We could try to be consistent about where we apply both: either do       `pbroadcast`insertion _after_ transformations, i.e. assuming the transformation rules themselves may require         `pbroadcast` operations in them, or else do the checking _before_ transformations, i.e. assuming that rules can't     introduce replication problems, i.e. assuming if a rule is applied to a welltyped term, it doesn't produce an illtyped term. But contra that latter assumption, `integer_pow` has shown us that it _is_ possible for a rule applied to a welltyped term to produce an illtyped term, at least in this one harmless way: constants/literals might not be typepromoted correctly. More generally, it's safest to keep the checks at the end and not make any assumptions about rules. Yet moving the rewrite to the same spot is tricky implementationwise.) This PR resolves the issue with constants just by handling them specially: instead of treating constants/literals just like ordinary values that happen to be replicated over all mesh axes, we give them a special symbol (`None`) which lets rules handle them polymorphically. For example, the `mul` rule doesn't have to error when multiplying a devicevarying value by a constant, because now it can tell that it's a constant. This approach maintains the strictness of the checks we had before (i.e. we still check after all transformations have been applied), without having to change the rewrite logic at all.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard-map] fix transpose replication checking bug with integer_pow,"At HEAD this `shard_map` code crashes with a replication checking error: ```python def loss(w, x):   (shard_map, mesh=mesh, in_specs=P('i'), out_specs=P())   def f(x):     return jax.lax.psum(((w * x) ** 2).sum(), 'i')   return f(x) jax.grad(loss)(3.0, jnp.arange(8.)) ``` ``` Traceback (most recent call last):   File ""/usr/local/google/home/mattjj/packages/jax/shmap_check_rep_bug.py"", line 23, in      jax.grad(loss)(3.0, jnp.arange(8.))   File ""/usr/local/google/home/mattjj/packages/jax/shmap_check_rep_bug.py"", line 21, in loss     return f(x)   File ""/usr/local/google/home/mattjj/packages/jax/shmap_check_rep_bug.py"", line 20, in f     return jax.lax.psum(((w * x) ** 2).sum(), 'i')   File ""/usr/local/google/home/mattjj/packages/jax/jax/_src/numpy/array_methods.py"", line 744, in op     return getattr(self.aval, f""_{name}"")(self, *args)   File ""/usr/local/google/home/mattjj/packages/jax/jax/_src/numpy/array_methods.py"", line 272, in deferring_binary_op     return binary_op(*args)   File ""/usr/local/google/home/mattjj/packages/jax/jax/_src/numpy/ufuncs.py"", line 339, in power     return lax.integer_pow(x1, x2) Exception: Primitive mul requires argument replication types to match, but got ({'i'}, set()). Please open an     issue at https://github.com/google/jax/issues ``` The narrow issue is that the JVP of `integer_pow[y=2] x` produces a `mul x 2`, and so when `x` is devicevarying      across axis `'i'`, our checking rule is unhappy that `mul` gets two arguments with different replication types (where the replication type of `2` is just `{}`, i.e. it's not devicevarying over any axes). This is not an issue with differentiating `pow x 2`, since we would rewrite that to `pow x (pbroadcast[axes='i'] 2)`! The broader issue is that we're running checks _after_ transformations, but only performing the `pbroadcast`inserting rewrite _before_ transformations. That inconsistency means that rules, especially constants in rules, don't have `pbroadcast`s inserted. (We could try to be consistent about where we apply both: either do       `pbroadcast`insertion _after_ transformations, i.e. assuming the transformation rules themselves may require         `pbroadcast` operations in them, or else do the checking _before_ transformations, i.e. assuming that rules can't     introduce replication problems, i.e. assuming if a rule is applied to a welltyped term, it doesn't produce an illtyped term. But contra that latter assumption, `integer_pow` has shown us that it _is_ possible for a rule applied to a welltyped term to produce an illtyped term, at least in this one harmless way: constants/literals might not be typepromoted correctly. More generally, it's safest to keep the checks at the end and not make any assumptions about rules. Yet moving the rewrite to the same spot is tricky implementationwise.) This PR resolves the issue with constants just by handling them specially: instead of treating constants/literals just like ordinary values that happen to be replicated over all mesh axes, we give them a special symbol (`None`) which lets rules handle them polymorphically. For example, the `mul` rule doesn't have to error when multiplying a devicevarying value by a constant, because now it can tell that it's a constant. This approach maintains the strictness of the checks we had before (i.e. we still check after all transformations have been applied), without having to change the rewrite logic at all.",2023-11-28T21:48:50Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/18711
787,"以下是一个github上的jax下的一个issue, 标题是(building jaxlib on Windows10: An error occurred during the fetch of repository 'local_config_cuda')， 内容是 ( Description Greetings I am trying to build jaxlib on Windows 10 within a python=3.11 env using the following command:             python .\build\build.py `             >>   enable_cuda `             >>   cuda_path='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3' `             >>   cudnn_path='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3' `             >>   cuda_version='12.3' `             >>   cudnn_version='8.9.' Below is the output:              _   _  __  __                  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,building jaxlib on Windows10: An error occurred during the fetch of repository 'local_config_cuda', Description Greetings I am trying to build jaxlib on Windows 10 within a python=3.11 env using the following command:             python .\build\build.py `             >>   enable_cuda `             >>   cuda_path='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3' `             >>   cudnn_path='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3' `             >>   cuda_version='12.3' `             >>   cudnn_version='8.9.' Below is the output:              _   _  __  __                  ++,2023-11-28T20:49:10Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18707,"JAX for Windows GPU is communitysupported. i.e., we will accept PRs to fix it, but we don't support it ourselves. If you want a supported path for JAX on Windows, either use the CPU build on Windows (which is on pypi, so you can just `pip install` it), or you can use the Linux CUDA build under WSL. Hope that helps!"
270,"以下是一个github上的jax下的一个issue, 标题是(Fix indexing bug when querying _input_layouts)， 内容是 (Fix indexing bug when querying _input_layouts)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix indexing bug when querying _input_layouts,Fix indexing bug when querying _input_layouts,2023-11-28T04:51:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18693
8781,"以下是一个github上的jax下的一个issue, 标题是(Spurious data during copying on multiple GPU Jax/ROCm setup )， 内容是 ( Description jax.device_put doesn't seem to work across multiple AMD GPUs. I would expect the following to copy the same data from the Mi25 to the Mi60, but instead, I see what appears to be spurious data(sometimes random numbers, sometimes zeros). The following is executed in the `rocm/jax:rocm5.7.0jax0.4.20py3.11.0` docker container, although I get similarly spurious results running on the host against jax://github.com/ROCmSoftwarePlatform/jax/releases/download/jaxlibv0.4.20/jaxlib0.4.20+rocm570cp311cp311manylinux2014_x86_64.whl as well. ```python3 import jax jnp = jax.numpy x = jnp.array([1,2]) print(f""{x=}"") print(f""{x.device()=}"") print(f""{x.dtype=}"") a = jax.device_put(x, jax.devices()[1]) print(f""{a=}"") print(f""{a.device()=}"") print(f""{a.dtype=}"") ``` prints out: x=Array([1, 2], dtype=int32) x.device()=rocm(id=0) x.dtype=dtype('int32') a=Array([0, 0], dtype=int32) a.device()=rocm(id=1) a.dtype=dtype('int32')  What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20  Which accelerator(s) are you using? Dual GPU, AMD Mi25 + AMD Mi60  Additional system info? Python 3.11, Linux x86 in docker; 1.26.2 3.11.0 (main, Nov 16 2023, 20:45:15) [GCC 9.4.0] uname_result(system='Linux', node='fb9e20c7dcf8', release='6.2.037generic', version=' CC(Require protobuf 3.6.0 or later)Ubuntu SMP PREEMPT_DYNAMIC Mon Oct 30 21:04:52 UTC 2023', machine='x86_64')  NVIDIA GPU info $ rocmsmi  ========================= ROCm System Management Interface ========================= =================================== Concise Info =================================== GPU  Temp (DieEdge)  AvgPwr  SCLK    MCLK    Fan     Perf  PwrCap  VRAM%  GPU%   0    26.0c           6.0W    852Mhz  167Mhz  99.22%  auto  220.0W    0%   0%     1    30.0c           20.0W   938Mhz  350Mhz  14.51%  auto  225.0W    0%   0%     ==================================================================================== =============================== End of ROCm SMI Log ================================ Truncated output from rocminfo: $ rocminfo   ISA Info:                     ISA 1                           Name:                    amdgcnamdamdhsagfx900:xnack          Machine Models:          HSA_MACHINE_MODEL_LARGE                   Profiles:                HSA_PROFILE_BASE                          Default Rounding Mode:   NEAR                                      Default Rounding Mode:   NEAR                                      Fast f16:                TRUE                                      Workgroup Max Size:      1024(0x400)                               Workgroup Max Size per Dimension:         x                        1024(0x400)                                 y                        1024(0x400)                                 z                        1024(0x400)                               Grid Max Size:           4294967295(0xffffffff)                    Grid Max Size per Dimension:         x                        4294967295(0xffffffff)                      y                        4294967295(0xffffffff)                      z                        4294967295(0xffffffff)                    FBarrier Max Size:       32                                  *******                   Agent 4                   *******                     Name:                    gfx906                                Uuid:                    GPU3f2a890172e620f4                  Marketing Name:                                                Vendor Name:             AMD                                   Feature:                 KERNEL_DISPATCH                       Profile:                 BASE_PROFILE                          Float Round Mode:        NEAR                                  Max Queue Number:        128(0x80)                             Queue Min Size:          64(0x40)                              Queue Max Size:          131072(0x20000)                       Queue Type:              MULTI                                 Node:                    3                                     Device Type:             GPU                                   Cache Info:                   L1:                      16(0x10) KB                             L2:                      8192(0x2000) KB                       Chip ID:                 26273(0x66a1)                         ASIC Revision:           1(0x1)                                Cacheline Size:          64(0x40)                              Max Clock Freq. (MHz):   1800                                  BDFID:                   17408                                 Internal Node ID:        3                                     Compute Unit:            64                                    SIMDs per CU:            4                                     Shader Engines:          4                                     Shader Arrs. per Eng.:   1                                     WatchPts on Addr. Ranges:4                                     Features:                KERNEL_DISPATCH    Fast F16 Operation:      TRUE                                  Wavefront Size:          64(0x40)                              Workgroup Max Size:      1024(0x400)                           Workgroup Max Size per Dimension:     x                        1024(0x400)                             y                        1024(0x400)                             z                        1024(0x400)                           Max Waves Per CU:        40(0x28)                              Max Workitem Per CU:    2560(0xa00)                           Grid Max Size:           4294967295(0xffffffff)                Grid Max Size per Dimension:     x                        4294967295(0xffffffff)                  y                        4294967295(0xffffffff)                  z                        4294967295(0xffffffff)                Max fbarriers/Workgrp:   32                                    Packet Processor uCode:: 469                                   SDMA engine uCode::      145                                   IOMMU Support::          None                                  Pool Info:                    Pool 1                          Segment:                 GLOBAL; FLAGS: COARSE GRAINED             Size:                    33538048(0x1ffc000) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 2                          Segment:                 GLOBAL; FLAGS:                            Size:                    33538048(0x1ffc000) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 3                          Segment:                 GROUP                                     Size:                    64(0x40) KB                               Allocatable:             FALSE                                     Alloc Granule:           0KB                                       Alloc Alignment:         0KB                                       Accessible by all:       FALSE                                 ISA Info:                     ISA 1                           Name:                    amdgcnamdamdhsagfx906:sramecc+:xnack       Machine Models:          HSA_MACHINE_MODEL_LARGE                   Profiles:                HSA_PROFILE_BASE                          Default Rounding Mode:   NEAR                                      Default Rounding Mode:   NEAR                                      Fast f16:                TRUE                                      Workgroup Max Size:      1024(0x400)                               Workgroup Max Size per Dimension:         x                        1024(0x400)                                 y                        1024(0x400)                                 z                        1024(0x400)                               Grid Max Size:           4294967295(0xffffffff)                    Grid Max Size per Dimension:         x                        4294967295(0xffffffff)                      y                        4294967295(0xffffffff)                      z                        4294967295(0xffffffff)                    FBarrier Max Size:       32                                  *** Done ***    )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Spurious data during copying on multiple GPU Jax/ROCm setup ," Description jax.device_put doesn't seem to work across multiple AMD GPUs. I would expect the following to copy the same data from the Mi25 to the Mi60, but instead, I see what appears to be spurious data(sometimes random numbers, sometimes zeros). The following is executed in the `rocm/jax:rocm5.7.0jax0.4.20py3.11.0` docker container, although I get similarly spurious results running on the host against jax://github.com/ROCmSoftwarePlatform/jax/releases/download/jaxlibv0.4.20/jaxlib0.4.20+rocm570cp311cp311manylinux2014_x86_64.whl as well. ```python3 import jax jnp = jax.numpy x = jnp.array([1,2]) print(f""{x=}"") print(f""{x.device()=}"") print(f""{x.dtype=}"") a = jax.device_put(x, jax.devices()[1]) print(f""{a=}"") print(f""{a.device()=}"") print(f""{a.dtype=}"") ``` prints out: x=Array([1, 2], dtype=int32) x.device()=rocm(id=0) x.dtype=dtype('int32') a=Array([0, 0], dtype=int32) a.device()=rocm(id=1) a.dtype=dtype('int32')  What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20  Which accelerator(s) are you using? Dual GPU, AMD Mi25 + AMD Mi60  Additional system info? Python 3.11, Linux x86 in docker; 1.26.2 3.11.0 (main, Nov 16 2023, 20:45:15) [GCC 9.4.0] uname_result(system='Linux', node='fb9e20c7dcf8', release='6.2.037generic', version=' CC(Require protobuf 3.6.0 or later)Ubuntu SMP PREEMPT_DYNAMIC Mon Oct 30 21:04:52 UTC 2023', machine='x86_64')  NVIDIA GPU info $ rocmsmi  ========================= ROCm System Management Interface ========================= =================================== Concise Info =================================== GPU  Temp (DieEdge)  AvgPwr  SCLK    MCLK    Fan     Perf  PwrCap  VRAM%  GPU%   0    26.0c           6.0W    852Mhz  167Mhz  99.22%  auto  220.0W    0%   0%     1    30.0c           20.0W   938Mhz  350Mhz  14.51%  auto  225.0W    0%   0%     ==================================================================================== =============================== End of ROCm SMI Log ================================ Truncated output from rocminfo: $ rocminfo   ISA Info:                     ISA 1                           Name:                    amdgcnamdamdhsagfx900:xnack          Machine Models:          HSA_MACHINE_MODEL_LARGE                   Profiles:                HSA_PROFILE_BASE                          Default Rounding Mode:   NEAR                                      Default Rounding Mode:   NEAR                                      Fast f16:                TRUE                                      Workgroup Max Size:      1024(0x400)                               Workgroup Max Size per Dimension:         x                        1024(0x400)                                 y                        1024(0x400)                                 z                        1024(0x400)                               Grid Max Size:           4294967295(0xffffffff)                    Grid Max Size per Dimension:         x                        4294967295(0xffffffff)                      y                        4294967295(0xffffffff)                      z                        4294967295(0xffffffff)                    FBarrier Max Size:       32                                  *******                   Agent 4                   *******                     Name:                    gfx906                                Uuid:                    GPU3f2a890172e620f4                  Marketing Name:                                                Vendor Name:             AMD                                   Feature:                 KERNEL_DISPATCH                       Profile:                 BASE_PROFILE                          Float Round Mode:        NEAR                                  Max Queue Number:        128(0x80)                             Queue Min Size:          64(0x40)                              Queue Max Size:          131072(0x20000)                       Queue Type:              MULTI                                 Node:                    3                                     Device Type:             GPU                                   Cache Info:                   L1:                      16(0x10) KB                             L2:                      8192(0x2000) KB                       Chip ID:                 26273(0x66a1)                         ASIC Revision:           1(0x1)                                Cacheline Size:          64(0x40)                              Max Clock Freq. (MHz):   1800                                  BDFID:                   17408                                 Internal Node ID:        3                                     Compute Unit:            64                                    SIMDs per CU:            4                                     Shader Engines:          4                                     Shader Arrs. per Eng.:   1                                     WatchPts on Addr. Ranges:4                                     Features:                KERNEL_DISPATCH    Fast F16 Operation:      TRUE                                  Wavefront Size:          64(0x40)                              Workgroup Max Size:      1024(0x400)                           Workgroup Max Size per Dimension:     x                        1024(0x400)                             y                        1024(0x400)                             z                        1024(0x400)                           Max Waves Per CU:        40(0x28)                              Max Workitem Per CU:    2560(0xa00)                           Grid Max Size:           4294967295(0xffffffff)                Grid Max Size per Dimension:     x                        4294967295(0xffffffff)                  y                        4294967295(0xffffffff)                  z                        4294967295(0xffffffff)                Max fbarriers/Workgrp:   32                                    Packet Processor uCode:: 469                                   SDMA engine uCode::      145                                   IOMMU Support::          None                                  Pool Info:                    Pool 1                          Segment:                 GLOBAL; FLAGS: COARSE GRAINED             Size:                    33538048(0x1ffc000) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 2                          Segment:                 GLOBAL; FLAGS:                            Size:                    33538048(0x1ffc000) KB                    Allocatable:             TRUE                                      Alloc Granule:           4KB                                       Alloc Alignment:         4KB                                       Accessible by all:       FALSE                                   Pool 3                          Segment:                 GROUP                                     Size:                    64(0x40) KB                               Allocatable:             FALSE                                     Alloc Granule:           0KB                                       Alloc Alignment:         0KB                                       Accessible by all:       FALSE                                 ISA Info:                     ISA 1                           Name:                    amdgcnamdamdhsagfx906:sramecc+:xnack       Machine Models:          HSA_MACHINE_MODEL_LARGE                   Profiles:                HSA_PROFILE_BASE                          Default Rounding Mode:   NEAR                                      Default Rounding Mode:   NEAR                                      Fast f16:                TRUE                                      Workgroup Max Size:      1024(0x400)                               Workgroup Max Size per Dimension:         x                        1024(0x400)                                 y                        1024(0x400)                                 z                        1024(0x400)                               Grid Max Size:           4294967295(0xffffffff)                    Grid Max Size per Dimension:         x                        4294967295(0xffffffff)                      y                        4294967295(0xffffffff)                      z                        4294967295(0xffffffff)                    FBarrier Max Size:       32                                  *** Done ***    ",2023-11-27T14:56:12Z,bug,open,1,3,https://github.com/jax-ml/jax/issues/18681,"I will note that although the Mi25 GPU is no longer officially supported by AMD, I'm able to run PyTorch models just fine on the Mi25 with the latest stable PyTorch. I want to switch to Jax or Tensorflow since taking PyTorch models into production with distributed training or jitted models is not straightforward, whilst it seems Jax and Tensorflow have out of the box support for this.","Heterogeneous GPUs is also technically not supported so perhaps the older GPU there is not playing nice. This does work as expected with 2 MI250s in that same container. $ python test.py 20231221 20:23:15.975131: E external/xla/xla/stream_executor/plugin_registry.cc:90] Invalid plugin kind specified: DNN x=Array([1, 2], dtype=int32) x.device()=rocm(id=0) x.dtype=dtype('int32') a=Array([1, 2], dtype=int32) a.device()=rocm(id=1) a.dtype=dtype('int32') $ rocminfo | grep gfx   Name:                    gfx90a       Name:                    amdgcnamdamdhsagfx90a:sramecc+:xnack   Name:                    gfx90a       Name:                    amdgcnamdamdhsagfx90a:sramecc+:xnack   Name:                    gfx90a       Name:                    amdgcnamdamdhsagfx90a:sramecc+:xnack   Name:                    gfx90a       Name:                    amdgcnamdamdhsagfx90a:sramecc+:xnack",Seems to work when using the same GPUs in a machine.
1010,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Simplify the indexing computations to be compatible with shape polymorphism)， 内容是 (Currently, we do not support shape polymorphism when we index with a slice, e.g., `x[a:b:c]`, and insted direct the user to use to `lax.dynamic_slice`. This is only because so far we have not tried to ensure that the index and bounds checking computations in gather are compatible with shape polymorphism. The problem was that there were a lot of conditionals, e.g., `if start >= stop` that cannot be handled in general in presence of symbolic shapes. Here we introduce a new helper function `_preprocess_slice` to contain all the computations for the start and the size of the slice. To test that this does not break the JAX index computations, I ran the tests with `JAX_NUM_GENERATED_CASES=1000`, especially the `lax_numpy_indexer_test.py`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Simplify the indexing computations to be compatible with shape polymorphism,"Currently, we do not support shape polymorphism when we index with a slice, e.g., `x[a:b:c]`, and insted direct the user to use to `lax.dynamic_slice`. This is only because so far we have not tried to ensure that the index and bounds checking computations in gather are compatible with shape polymorphism. The problem was that there were a lot of conditionals, e.g., `if start >= stop` that cannot be handled in general in presence of symbolic shapes. Here we introduce a new helper function `_preprocess_slice` to contain all the computations for the start and the size of the slice. To test that this does not break the JAX index computations, I ran the tests with `JAX_NUM_GENERATED_CASES=1000`, especially the `lax_numpy_indexer_test.py`.",2023-11-27T10:51:20Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18679
1209,"以下是一个github上的jax下的一个issue, 标题是(Clarify the Applicability and Efficiency of Loss Function in autodiff_cookbook)， 内容是 (In this PR, I am introducing additional comments to the loss function within the autodiff_cookbook to enhance the understanding of its scope and efficiency. The current implementation is optimized for binary targets, a detail that is now explicitly mentioned to guide beginners and prevent any potential confusion when it comes to more complex scenarios, such as label smoothing or multiclass targets. Furthermore, I've included an explanation on how this approach conserves computational resources by applying the log function selectively, which results in fewer floatingpoint operations (FLOPs). This commentary aims to provide clarity on why this method is efficient for the provided example, while also indicating that a different approach would be necessary for nonbinary cases. I trust that these annotations will enrich the educational value of the cookbook and I'm grateful for the opportunity to contribute to its precision and accessibility.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Clarify the Applicability and Efficiency of Loss Function in autodiff_cookbook,"In this PR, I am introducing additional comments to the loss function within the autodiff_cookbook to enhance the understanding of its scope and efficiency. The current implementation is optimized for binary targets, a detail that is now explicitly mentioned to guide beginners and prevent any potential confusion when it comes to more complex scenarios, such as label smoothing or multiclass targets. Furthermore, I've included an explanation on how this approach conserves computational resources by applying the log function selectively, which results in fewer floatingpoint operations (FLOPs). This commentary aims to provide clarity on why this method is efficient for the provided example, while also indicating that a different approach would be necessary for nonbinary cases. I trust that these annotations will enrich the educational value of the cookbook and I'm grateful for the opportunity to contribute to its precision and accessibility.",2023-11-27T03:56:14Z,,open,0,2,https://github.com/jax-ml/jax/issues/18677,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Please feel free to make any edits to my added comments to better suit the repository's style, as English is not my first language. Your expertise in clarifying these points is greatly appreciated."
545,"以下是一个github上的jax下的一个issue, 标题是(Trying to use Objax on Jax-metal)， 内容是 ( Description I am trying to use objax.nn.Conv2D and I keep getting an XlaRuntimeError :(  xlaruntimeerror.txt offendingline.txt  What jax/jaxlib version are you using? Jax v0.4.11, Jaxmetal 0.0.4, jaxlib v0.4.11  Which accelerator(s) are you using? GPU  Additional system info? Python 3.10.12, MacOS M1  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Trying to use Objax on Jax-metal," Description I am trying to use objax.nn.Conv2D and I keep getting an XlaRuntimeError :(  xlaruntimeerror.txt offendingline.txt  What jax/jaxlib version are you using? Jax v0.4.11, Jaxmetal 0.0.4, jaxlib v0.4.11  Which accelerator(s) are you using? GPU  Additional system info? Python 3.10.12, MacOS M1  NVIDIA GPU info _No response_",2023-11-26T01:48:52Z,bug Apple GPU (Metal) plugin,closed,0,1,https://github.com/jax-ml/jax/issues/18674,The issue is fixed in jaxmetal 0.0.7. Pls reopen the issue if otherwise. 
3490,"以下是一个github上的jax下的一个issue, 标题是(Resharding across MGPUs results in long series of cuMemAlloc_v2 calls)， 内容是 ( Description I'm implementing a time marching simulation across multiple GPUs. The calculation has field arrays sharded in one axis, and operators sharded in another (actual implementation involves ffts constrained using `experimental.custom_partitioning`, but I've omitted that here for simplicity). I use `lower `and `compile `for AOT compilation to make it easy for me to benchmark actual runtime. What I'm seeing is that on first execution a long time is spent executing a series of cuMemAlloc_v2 calls on each device in a series of streams of the form `Stream N(Memset)`. The time this takes appears to grow with the square of the GPUs. For the minimal example below I see the following:  2 GPU first call takes 2.2 s  4 GPU first call takes 7.8 s  8 GPU first call takes 29 s  16 GPU first call takes 118 s My questions: 1. Are these calls expected? 2. If so, is it expected that they should take so long? Below is a minimal example. In practice I'm using `donate_argnames` and specifying `in_shardings` and `out_shardings`, but have omitted here for brevity. Any help would be much appreciated! Minimal example: ```python import jax import jax.numpy as jnp from jax import jit from jax.lax import fori_loop from jax.experimental import mesh_utils from jax.sharding import Mesh, PartitionSpec as P, NamedSharding from time import perf_counter  inputs ngpu = 2 dims = (8192, 8192) nt = 100      define a single time step def run_step(i, carry):      unpack fields     flda, fldb, opa, opb = carry      calculations     fldb = opa * flda       flda = opb * fldb     return (flda, fldb, opa, opb)  define run function def run(nt, carry):     return fori_loop(0, nt, run_step, carry)  create mesh devices = mesh_utils.create_device_mesh((ngpu,), jax.devices()[0:ngpu]) mesh = Mesh(devices, axis_names=(""gpus"",)) shard_y = NamedSharding(mesh, P(None, ""gpus"")) shard_x = NamedSharding(mesh, P(""gpus"", None))  begin trace with jax.profiler.trace(""./tensorboard""):      create operators & fields     tbeg = perf_counter()     opa = jax.device_put(jnp.ones(dims), shard_x)     opb = jax.device_put(jnp.ones(dims), shard_x)            flda = jax.device_put(jnp.ones(dims), shard_y)     fldb = jax.device_put(jnp.ones(dims), shard_y)     carry = (flda, fldb, opa, opb)     trun = perf_counter()  tbeg     print(f""Array creation time:\t{1e3*trun:8.1f} ms"")      compile      tbeg = perf_counter()     run_jit = jit(run, static_argnames=(""nt""))     lowered = run_jit.lower(nt, carry)     compiled = lowered.compile()     trun = perf_counter()  tbeg     print(f""Compile time:\t\t{1e3*trun:8.1f} ms"")      single step warmup run     tbeg = perf_counter()     _ = run_jit(1, carry)     flda.block_until_ready()     trun = perf_counter()  tbeg     print(f""Single step run time:\t{1e3*trun:8.1f} ms"")      run the full calculation     tbeg = perf_counter()     flda, fldb, _, _ = run_jit(nt, carry)     flda.block_until_ready()     trun = perf_counter()  tbeg     print(f""Run time:\t\t{1e3*trun:8.1f} ms"") ``` Example trace: !image !image  What jax/jaxlib version are you using? jax==0.4.20, jaxlib==0.4.20+cuda11.cudnn86  Which accelerator(s) are you using? GPU (16x Nvidia A100 40GB, but can recreate on 2x)  Additional system info? 1.26.2 3.10.13  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Resharding across MGPUs results in long series of cuMemAlloc_v2 calls," Description I'm implementing a time marching simulation across multiple GPUs. The calculation has field arrays sharded in one axis, and operators sharded in another (actual implementation involves ffts constrained using `experimental.custom_partitioning`, but I've omitted that here for simplicity). I use `lower `and `compile `for AOT compilation to make it easy for me to benchmark actual runtime. What I'm seeing is that on first execution a long time is spent executing a series of cuMemAlloc_v2 calls on each device in a series of streams of the form `Stream N(Memset)`. The time this takes appears to grow with the square of the GPUs. For the minimal example below I see the following:  2 GPU first call takes 2.2 s  4 GPU first call takes 7.8 s  8 GPU first call takes 29 s  16 GPU first call takes 118 s My questions: 1. Are these calls expected? 2. If so, is it expected that they should take so long? Below is a minimal example. In practice I'm using `donate_argnames` and specifying `in_shardings` and `out_shardings`, but have omitted here for brevity. Any help would be much appreciated! Minimal example: ```python import jax import jax.numpy as jnp from jax import jit from jax.lax import fori_loop from jax.experimental import mesh_utils from jax.sharding import Mesh, PartitionSpec as P, NamedSharding from time import perf_counter  inputs ngpu = 2 dims = (8192, 8192) nt = 100      define a single time step def run_step(i, carry):      unpack fields     flda, fldb, opa, opb = carry      calculations     fldb = opa * flda       flda = opb * fldb     return (flda, fldb, opa, opb)  define run function def run(nt, carry):     return fori_loop(0, nt, run_step, carry)  create mesh devices = mesh_utils.create_device_mesh((ngpu,), jax.devices()[0:ngpu]) mesh = Mesh(devices, axis_names=(""gpus"",)) shard_y = NamedSharding(mesh, P(None, ""gpus"")) shard_x = NamedSharding(mesh, P(""gpus"", None))  begin trace with jax.profiler.trace(""./tensorboard""):      create operators & fields     tbeg = perf_counter()     opa = jax.device_put(jnp.ones(dims), shard_x)     opb = jax.device_put(jnp.ones(dims), shard_x)            flda = jax.device_put(jnp.ones(dims), shard_y)     fldb = jax.device_put(jnp.ones(dims), shard_y)     carry = (flda, fldb, opa, opb)     trun = perf_counter()  tbeg     print(f""Array creation time:\t{1e3*trun:8.1f} ms"")      compile      tbeg = perf_counter()     run_jit = jit(run, static_argnames=(""nt""))     lowered = run_jit.lower(nt, carry)     compiled = lowered.compile()     trun = perf_counter()  tbeg     print(f""Compile time:\t\t{1e3*trun:8.1f} ms"")      single step warmup run     tbeg = perf_counter()     _ = run_jit(1, carry)     flda.block_until_ready()     trun = perf_counter()  tbeg     print(f""Single step run time:\t{1e3*trun:8.1f} ms"")      run the full calculation     tbeg = perf_counter()     flda, fldb, _, _ = run_jit(nt, carry)     flda.block_until_ready()     trun = perf_counter()  tbeg     print(f""Run time:\t\t{1e3*trun:8.1f} ms"") ``` Example trace: !image !image  What jax/jaxlib version are you using? jax==0.4.20, jaxlib==0.4.20+cuda11.cudnn86  Which accelerator(s) are you using? GPU (16x Nvidia A100 40GB, but can recreate on 2x)  Additional system info? 1.26.2 3.10.13  ++",2023-11-24T10:51:38Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/18666,Any thoughts on this? 
1378,"以下是一个github上的jax下的一个issue, 标题是(Solution for https://github.com/google/jax/pull/18641)， 内容是 (In tests/BUILD file, set the environment variable to 0 for lobpcg_test. When using a clean installation, such as a base Python and relying solely on the testrequirements.txt from /build, as per the instructions in the documentation, the tests that require matplotlib tend to fail. Users are required to install matplotlib independently. To prevent these failures by default, it is advisable to disable matplotlib: name = ""lobpcg_test"",     srcs = [""lobpcg_test.py""],     env = {""LOBPCG_EMIT_DEBUG_PLOTS"": ""0""}, If someone wishes to visualize results using matplotlib, they should be informed in the documentation (developer.rmd). For example in section Running the tests, we may add extra information: Moreover, if you require visualization generated by Matplotlib during the tests, it is necessary to install it separately using the command: ``` pip install matplotlib ``` Additionally, set the LOBPCG_EMIT_DEBUG_PLOTS environment variable to 1 in the /tests/BUILD file or pass it via the command line. For example, for a Bazel test case: ``` bazel test //tests:cpu_tests //tests:backend_independent_tests test_env=LOBPCG_EMIT_DEBUG_PLOTS=1 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Solution for https://github.com/google/jax/pull/18641,"In tests/BUILD file, set the environment variable to 0 for lobpcg_test. When using a clean installation, such as a base Python and relying solely on the testrequirements.txt from /build, as per the instructions in the documentation, the tests that require matplotlib tend to fail. Users are required to install matplotlib independently. To prevent these failures by default, it is advisable to disable matplotlib: name = ""lobpcg_test"",     srcs = [""lobpcg_test.py""],     env = {""LOBPCG_EMIT_DEBUG_PLOTS"": ""0""}, If someone wishes to visualize results using matplotlib, they should be informed in the documentation (developer.rmd). For example in section Running the tests, we may add extra information: Moreover, if you require visualization generated by Matplotlib during the tests, it is necessary to install it separately using the command: ``` pip install matplotlib ``` Additionally, set the LOBPCG_EMIT_DEBUG_PLOTS environment variable to 1 in the /tests/BUILD file or pass it via the command line. For example, for a Bazel test case: ``` bazel test //tests:cpu_tests //tests:backend_independent_tests test_env=LOBPCG_EMIT_DEBUG_PLOTS=1 ```",2023-11-23T11:55:58Z,pull ready,closed,0,10,https://github.com/jax-ml/jax/issues/18660,One last thing  can you squash your changes into a single commit please? See https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests Thanks!,Ok I only must read https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests,Ok I only must read https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests,"> One last thing  can you squash your changes into a single commit please? See https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests >  > Thanks! Ok, I done squashed commit. I hope it is good","Thanks! There are still three commits on the branch, so I think something went awry in the squashing process",I have a problem to create one single commit based on the two other commits before made. I am not git master ;),"Try something like this, assuming `origin` points to `https://github.com/mmarcinmichael/jax` and `upstream` points to `https://github.com/google/jax` ``` git checkout main git pull upstream main git checkout detached   this is the local copy of your branch git rebase main   make sure the branch is uptodate with main git reset soft main   reset to the main branch, keeping changed files in staging git commit   make a new single commit with the staged files git push origin +detached   overwrite the remote branch with the local state ```","Looks like it didn't work... If you followed my instructions, then it probably means that your `main` branch includes additional commits that are not in `upstream/main`, which I would recommend avoiding in general.","Try doing the `rebase` and the `reset` commands against `upstream/main` directly, i.e substitute this in the workflow above: ```python git rebase upstream/main git reset soft upstream/main ```","Hi, I am sorry for the mess. Now, all should be ok."
7093,"以下是一个github上的jax下的一个issue, 标题是(Memory profiling )， 内容是 ( Description We are trying to profile the memory usage for the DESC package, but we are getting an unreasonable memory estimation. The report by the pprof tool estimates the memory usage as about 3.4MB, but we are sure it should be ~2.9GB.  The package can be installed from PyPI: `pip install descopt`, and here is a code sample:  ``` import numpy as np from desc.equilibrium import Equilibrium from desc.geometry import FourierRZToroidalSurface from desc.profiles import PowerSeriesProfile from desc.plotting import plot_1d, plot_section, plot_surfaces from desc.optimize import Optimizer from desc.objectives import (     get_fixed_boundary_constraints,     ObjectiveFunction,     FixBoundaryR,     FixBoundaryZ,     FixPressure,     FixIota,     FixPsi,     ForceBalance, ) import jax.profiler surface = FourierRZToroidalSurface(     R_lmn=[10, 1],     modes_R=[[0, 0], [1, 0]],   modes given as [m,n] for each coefficient                                                                                                                                                                                                                                                         Z_lmn=[0, 1],     modes_Z=[[0, 0], [1, 0]], ) pressure = PowerSeriesProfile(params=[0, 0], modes=[0, 2]) iota = PowerSeriesProfile(params=[1, 1.5], modes=[0, 2]) eq = Equilibrium(     surface=surface,     pressure=pressure,     iota=iota,     Psi=1.0,   flux (in Webers) within the last closed flux surface                                                                                                                                                                                                                                                               NFP=1,   number of field periods                                                                                                                                                                                                                                                                                              L=9,   radial spectral resolution                                                                                                                                                                                                                                                                                             M=9,   poloidal spectral resolution                                                                                                                                                                                                                                                                                           N=9,   toroidal spectral resolution (axisymmetric case, so we don't need any toroidal modes)                                                                                                                                                                                                                                   L_grid=7,   real space radial resolution, slightly oversampled                                                                                                                                                                                                                                                               M_grid=7,   real space poloidal resolution, slightly oversampled                                                                                                                                                                                                                                                             N_grid=7,   real space toroidal resolution (axisymmetric, so we don't need any grid points toroidally)                                                                                                                                                                                                                      sym=True,   explicitly enforce stellarator symmetry                                                                                                                                                                                                                                                                       ) optimizer = Optimizer(""lsqexact"") constraints = (     FixBoundaryR(eq=eq),   enforce fixed  LCFS for R                                                                                                                                                                                                                                                                              FixBoundaryZ(eq=eq),   enforce fixed  LCFS for R                                                                                                                                                                                                                                                                              FixPressure(eq=eq),   enforce that the pressure profile stay fixed                                                                                                                                                                                                                                                            FixIota(eq=eq),   enforce that the rotational transform profile stay fixed                                                                                                                                                                                                                                                    FixPsi(eq=eq),   enforce that the enclosed toroidal stay fixed                                                                                                                                                                                                                                                            )  choose the objectives to be ForceBalance(), which is a wrapper function for RadialForceBalance() and HelicalForceBalance()                                                                                                                                                                                                   objectives = ForceBalance(eq=eq)  the ObjectiveFunction object which we can pass to the eq.solve method                                                                                                                                                                                                                                                        obj = ObjectiveFunction(objectives=objectives) obj.build() obj.compile() jax.profiler.save_device_memory_profile(""memory.prof"") ```  What jax/jaxlib version are you using? jax V0.4.14  jaxlib V0.4.14  Which accelerator(s) are you using? CPU  Additional system info? _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Memory profiling ," Description We are trying to profile the memory usage for the DESC package, but we are getting an unreasonable memory estimation. The report by the pprof tool estimates the memory usage as about 3.4MB, but we are sure it should be ~2.9GB.  The package can be installed from PyPI: `pip install descopt`, and here is a code sample:  ``` import numpy as np from desc.equilibrium import Equilibrium from desc.geometry import FourierRZToroidalSurface from desc.profiles import PowerSeriesProfile from desc.plotting import plot_1d, plot_section, plot_surfaces from desc.optimize import Optimizer from desc.objectives import (     get_fixed_boundary_constraints,     ObjectiveFunction,     FixBoundaryR,     FixBoundaryZ,     FixPressure,     FixIota,     FixPsi,     ForceBalance, ) import jax.profiler surface = FourierRZToroidalSurface(     R_lmn=[10, 1],     modes_R=[[0, 0], [1, 0]],   modes given as [m,n] for each coefficient                                                                                                                                                                                                                                                         Z_lmn=[0, 1],     modes_Z=[[0, 0], [1, 0]], ) pressure = PowerSeriesProfile(params=[0, 0], modes=[0, 2]) iota = PowerSeriesProfile(params=[1, 1.5], modes=[0, 2]) eq = Equilibrium(     surface=surface,     pressure=pressure,     iota=iota,     Psi=1.0,   flux (in Webers) within the last closed flux surface                                                                                                                                                                                                                                                               NFP=1,   number of field periods                                                                                                                                                                                                                                                                                              L=9,   radial spectral resolution                                                                                                                                                                                                                                                                                             M=9,   poloidal spectral resolution                                                                                                                                                                                                                                                                                           N=9,   toroidal spectral resolution (axisymmetric case, so we don't need any toroidal modes)                                                                                                                                                                                                                                   L_grid=7,   real space radial resolution, slightly oversampled                                                                                                                                                                                                                                                               M_grid=7,   real space poloidal resolution, slightly oversampled                                                                                                                                                                                                                                                             N_grid=7,   real space toroidal resolution (axisymmetric, so we don't need any grid points toroidally)                                                                                                                                                                                                                      sym=True,   explicitly enforce stellarator symmetry                                                                                                                                                                                                                                                                       ) optimizer = Optimizer(""lsqexact"") constraints = (     FixBoundaryR(eq=eq),   enforce fixed  LCFS for R                                                                                                                                                                                                                                                                              FixBoundaryZ(eq=eq),   enforce fixed  LCFS for R                                                                                                                                                                                                                                                                              FixPressure(eq=eq),   enforce that the pressure profile stay fixed                                                                                                                                                                                                                                                            FixIota(eq=eq),   enforce that the rotational transform profile stay fixed                                                                                                                                                                                                                                                    FixPsi(eq=eq),   enforce that the enclosed toroidal stay fixed                                                                                                                                                                                                                                                            )  choose the objectives to be ForceBalance(), which is a wrapper function for RadialForceBalance() and HelicalForceBalance()                                                                                                                                                                                                   objectives = ForceBalance(eq=eq)  the ObjectiveFunction object which we can pass to the eq.solve method                                                                                                                                                                                                                                                        obj = ObjectiveFunction(objectives=objectives) obj.build() obj.compile() jax.profiler.save_device_memory_profile(""memory.prof"") ```  What jax/jaxlib version are you using? jax V0.4.14  jaxlib V0.4.14  Which accelerator(s) are you using? CPU  Additional system info? _No response_  NVIDIA GPU info _No response_",2023-11-22T19:08:31Z,bug,open,1,0,https://github.com/jax-ml/jax/issues/18650
999,"以下是一个github上的jax下的一个issue, 标题是(Error in Automatic Differenciation for Functions with Powers of Zero)， 内容是 ( Description I am trying to vectorize a procedure, where I need to do the elementwise power of a large array before differentiating it. If I have only one element, I can use this code which gives the expected output: ``` def func(x):     x = x ** 0     return x jac = jacfwd(func) jac(5.)  output = 0 ``` However, when I try to vectorize this procedure, I get a different result. ``` def func(x):     x = x ** jnp.array([[0]])     return x jac = jacfwd(func) jac(5.)  output = 1 ``` Is this expected? If yes, could you please explain the convention/reason? Thank you!  What jax/jaxlib version are you using? 0.4.16 0.4.14  Which accelerator(s) are you using? _No response_  Additional system info? python 3.11.5 on macOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Error in Automatic Differenciation for Functions with Powers of Zero," Description I am trying to vectorize a procedure, where I need to do the elementwise power of a large array before differentiating it. If I have only one element, I can use this code which gives the expected output: ``` def func(x):     x = x ** 0     return x jac = jacfwd(func) jac(5.)  output = 0 ``` However, when I try to vectorize this procedure, I get a different result. ``` def func(x):     x = x ** jnp.array([[0]])     return x jac = jacfwd(func) jac(5.)  output = 1 ``` Is this expected? If yes, could you please explain the convention/reason? Thank you!  What jax/jaxlib version are you using? 0.4.16 0.4.14  Which accelerator(s) are you using? _No response_  Additional system info? python 3.11.5 on macOS  NVIDIA GPU info _No response_",2023-11-22T15:54:46Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18640,"Hi  thanks for the question! Using more recent jaxlib, I get the expected result. I think this issue was fixed by CC(fix pow jvp rule with int exponent (broken since 16419)), but you'll need to update to a more recent JAX release to get the fix – I think JAX v0.4.19 or newer should do it.",Indeed both codes snippets give the same output with JAX v0.4.19. Thank you!
888,"以下是一个github上的jax下的一个issue, 标题是(Accessing array memory to perform an operation causes a SIGSEGV (Address boundary error))， 内容是 ( Description Hi, I encounter a SIGSEGV error, when trying to use jax numpy and I don't know why. ```python from jax import numpy as jnp key = jnp.array((0,0)) print(key[0])  > 0 print(key.at[0].get())  > 0  print(key.at[0].get() + 1)  > ""Job 1, 'environemnt/bin/…' terminated by signal SIGSEGV (Address boundary error)""  jax version = '0.4.21.dev20231120' jaxlib version = '0.4.21.dev20231120' ```  What jax/jaxlib version are you using? jax version = '0.4.21.dev20231120', jaxlib version = '0.4.21.dev20231120'  Which accelerator(s) are you using? GPU  Additional system info python3.11 Ubuntu  NVIDIA GPU info ++  ++++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Accessing array memory to perform an operation causes a SIGSEGV (Address boundary error)," Description Hi, I encounter a SIGSEGV error, when trying to use jax numpy and I don't know why. ```python from jax import numpy as jnp key = jnp.array((0,0)) print(key[0])  > 0 print(key.at[0].get())  > 0  print(key.at[0].get() + 1)  > ""Job 1, 'environemnt/bin/…' terminated by signal SIGSEGV (Address boundary error)""  jax version = '0.4.21.dev20231120' jaxlib version = '0.4.21.dev20231120' ```  What jax/jaxlib version are you using? jax version = '0.4.21.dev20231120', jaxlib version = '0.4.21.dev20231120'  Which accelerator(s) are you using? GPU  Additional system info python3.11 Ubuntu  NVIDIA GPU info ++  ++++",2023-11-21T18:38:56Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/18625,Can you confirm what version of Python 3.11 you have installed? What's the ubuntu package version? If it's the rc version provided with Ubuntu 22.04 it's broken.,It is the rc version `Python 3.11.0rc1`. I will try with another version and then report back.,> It is the rc version `Python 3.11.0rc1`. I will try with another version and then report back. Do you recommend a specific version? ,"If you need a newer Python on Ubuntu, I recommend either the deadsnakes PPA (https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa) or use pyenv to build a copy from source.",I installed python3.12 from the deadsnakes repository and now my test case doesn't crash. I will test my actual use case and then close the issue if I encounter no erros. Thank you for you help :),It fixed my error! Thanks a ton!
2860,"以下是一个github上的jax下的一个issue, 标题是(Gradient computations take more time for the first repetitions after compilation than de last)， 内容是 ( Description Hi, For benchmarking purposes, I need to measure the time spent to compute the gradient of some Flax model w.r.t. the model's parameters. The gradient is jitted, and a first run is performed for compilation. However, when running this code on a GPU, the five first computations are longer than the fifteen last, while computing the same thing. Is there an explanation for that? ```python import jax import optax import jax.numpy as jnp from flax.training import common_utils from transformers import FlaxResNetForImageClassification, ResNetConfig from time import perf_counter def cross_entropy_loss(logits, labels):     one_hot_labels = common_utils.onehot(labels, num_classes=2)     xentropy = optax.softmax_cross_entropy(logits=logits,                                            labels=one_hot_labels)     return jnp.mean(xentropy) config_resnet50 = ResNetConfig(     num_channels=3,     embedding_size=64,     hidden_sizes=[256, 512, 1024, 2048],     depths=[3, 4, 6, 3],     layer_type='bottleneck',     hidden_act='relu',     downsample_in_first_stage=False,     out_features=None,     out_indices=None, ) model = FlaxResNetForImageClassification(config_resnet50) def loss_fn(params, batch):     """"""loss function used for training.""""""     logits = model._module.apply(params, batch['images']).logits     loss = cross_entropy_loss(logits, batch['labels'])     return loss if __name__ == '__main__':     batch_size = 16     n_reps = 20     key = jax.random.PRNGKey(0)     key, subkey = jax.random.split(key)     batch = {         'images': jax.random.normal(key, (batch_size, 224, 224, 3)),         'labels': jax.random.randint(subkey, (batch_size,), 0, 2)     }     params = model.params     grad_fun = jax.jit(lambda x: jax.grad(loss_fn)(x, batch))     grad_fun(params)   First run for compilation     for _ in range(n_reps):         start = perf_counter()         jax.block_until_ready(grad_fun(params))         print(perf_counter()  start) ``` Output: ``` 0.027811646927148104 0.027487624902278185 0.0274412389844656 0.027404130902141333 0.027959060855209827 0.02018922194838524 0.018810193985700607 0.01876934664323926 0.018772422801703215 0.01853539375588298 0.018571130000054836 0.01854165457189083 0.01851730002090335 0.018671086058020592 0.018460053950548172 0.018476856406778097 0.01846574479714036 0.01866668788716197 0.01846056431531906 0.018483687192201614 ```  What jax/jaxlib version are you using? jax v0.4.7, jaxlib v0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.11, Linux  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Gradient computations take more time for the first repetitions after compilation than de last," Description Hi, For benchmarking purposes, I need to measure the time spent to compute the gradient of some Flax model w.r.t. the model's parameters. The gradient is jitted, and a first run is performed for compilation. However, when running this code on a GPU, the five first computations are longer than the fifteen last, while computing the same thing. Is there an explanation for that? ```python import jax import optax import jax.numpy as jnp from flax.training import common_utils from transformers import FlaxResNetForImageClassification, ResNetConfig from time import perf_counter def cross_entropy_loss(logits, labels):     one_hot_labels = common_utils.onehot(labels, num_classes=2)     xentropy = optax.softmax_cross_entropy(logits=logits,                                            labels=one_hot_labels)     return jnp.mean(xentropy) config_resnet50 = ResNetConfig(     num_channels=3,     embedding_size=64,     hidden_sizes=[256, 512, 1024, 2048],     depths=[3, 4, 6, 3],     layer_type='bottleneck',     hidden_act='relu',     downsample_in_first_stage=False,     out_features=None,     out_indices=None, ) model = FlaxResNetForImageClassification(config_resnet50) def loss_fn(params, batch):     """"""loss function used for training.""""""     logits = model._module.apply(params, batch['images']).logits     loss = cross_entropy_loss(logits, batch['labels'])     return loss if __name__ == '__main__':     batch_size = 16     n_reps = 20     key = jax.random.PRNGKey(0)     key, subkey = jax.random.split(key)     batch = {         'images': jax.random.normal(key, (batch_size, 224, 224, 3)),         'labels': jax.random.randint(subkey, (batch_size,), 0, 2)     }     params = model.params     grad_fun = jax.jit(lambda x: jax.grad(loss_fn)(x, batch))     grad_fun(params)   First run for compilation     for _ in range(n_reps):         start = perf_counter()         jax.block_until_ready(grad_fun(params))         print(perf_counter()  start) ``` Output: ``` 0.027811646927148104 0.027487624902278185 0.0274412389844656 0.027404130902141333 0.027959060855209827 0.02018922194838524 0.018810193985700607 0.01876934664323926 0.018772422801703215 0.01853539375588298 0.018571130000054836 0.01854165457189083 0.01851730002090335 0.018671086058020592 0.018460053950548172 0.018476856406778097 0.01846574479714036 0.01866668788716197 0.01846056431531906 0.018483687192201614 ```  What jax/jaxlib version are you using? jax v0.4.7, jaxlib v0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.11, Linux  NVIDIA GPU info ``` ++  ++ ```",2023-11-21T15:22:43Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/18622,"I suspect the issue is that you're not calling `block_until_ready` on the first invocation of `grad_fun(params)`, and so the subsequent calculations are being asynchronously dispatched while the device is still busy. When I change your compilation run to this: ```python jax.block_until_ready(grad_fun(params))   First run for compilation ``` I see more consistent timing in the first several runs of the benchmark.","Thank you for answering. I tried your solution and got the same results, unfortunately.",I'm unable to reproduce this on a Colab T4 TPU runtime with jax v0.4.20 and the `block_until_ready` that I suggested above. Can you try updating your jax and jaxlib version and see if that affects the result?
5210,"以下是一个github上的jax下的一个issue, 标题是(Bump actions/setup-python from 1 to 4)， 内容是 (Bumps actions/setuppython from 1 to 4.  Release notes Sourced from actions/setuppython's releases.  v4.0.0 What's Changed  Support for pythonversionfile input:  CC(rename ""minmax"" > ""optimizers"")  Example of usage:  uses: actions/setuppython   with:     pythonversionfile: '.pythonversion'  Read python version from a file  run: python my_script.py  There is no default python version for this setuppython major version, the action requires to specify either pythonversion input or pythonversionfile input. If the pythonversion input is not specified the action will try to read required version from file from pythonversionfile input.  Use pypyX.Y for PyPy pythonversion input:  CC(Error importing jax after certain tensorflow import)  Example of usage:  uses: actions/setuppython   with:     pythonversion: 'pypy3.9'  pypyX.Y kept for backward compatibility  run: python my_script.py    RUNNER_TOOL_CACHE environment variable is equal AGENT_TOOLSDIRECTORY:  CC(Change implementation of negative loglikelihood in README toy example)   Bugfix: create missing pypyX.Y symlinks:  CC(Issue taking gradients through np.where when one of branches is nan.)   PKG_CONFIG_PATH environment variable:  CC(Support tuples in translation rule for zeros_like_p.)   Added pythonpath output:  CC(Remove obsolete workarounds for bugs that seem fixed.) pythonpath output contains Python executable path.   Updated zeit/ncc to vercel/ncc package:  CC(override __new__ in dtype classes)   Bugfix: fixed output for prerelease version of poetry:  CC(Use a regular import to add jax.__version__ rather than exec() trickery.)   Made pythonLocation environment variable consistent for Python and PyPy:  CC(Document jax.disable_jit. Add an example to jax.grad.)   Bugfix for 3.xdev syntax:  CC(Default XLA/GPU memory allocator is synchronous/slow.)   Other improvements:  CC(actually fix nondeterminism in einsum)  CC(clarification to README)  CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.)  CC(Hessian calculation finds an UnshapedArray when jitted.)  CC(__invert__ doesn't take an argument.)   v3.1.4 What's Changed In the scope of this patch release, the warning for deprecating Python 2.x was added in actions/setuppython CC(Disable indexed update tests on GPU to work around LLVM failure.) by @​dmitryshibanov For more information, check out actions/setuppython CC(Cannot take gradient of np.prod)   ... (truncated)   Commits  65d7f2d Add range validation for toml files ( CC(avoid packing leaf outputs for jit/pmap funs)) f97b831 Bump wordwrap from 1.2.3 to 1.2.4 ( CC(Build on linux, got ""//crosstool:cccompilerwindows"" error)) 61a6322 Fix typos found by codespell ( CC(jacfwd through while_loop)) ea5b57f Bump semver from 7.3.8 to 7.5.2 ( CC(make static_argnums cache on value when possible)) 014d32a Bump toughcookie and @​azure/msrestjs ( CC(Use DeviceOrdinals method rather than accessing Computation._device_o…)) c16c4b8 Fix pipenv jobs ( CC(Implement eigh batching.)) 0d5da6a Read python version from pyproject.toml (fix  CC(fix typo in vmap (fixes 536))) ( CC(Support autodiff of Eigendecomposition with repeated eigenvalues)) 3f824b7 remove python 2.7 from the tests ( CC(Implement np.average)) bd6b4b6 Add warning for python 2.7 ( CC(vmap over a list of functions)) 0cbcb9a Merge pull request  CC(Bump minimum jaxlib version to 0.1.12.) from akvplatform/disallowimplicitdependencies Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,Bump actions/setup-python from 1 to 4,"Bumps actions/setuppython from 1 to 4.  Release notes Sourced from actions/setuppython's releases.  v4.0.0 What's Changed  Support for pythonversionfile input:  CC(rename ""minmax"" > ""optimizers"")  Example of usage:  uses: actions/setuppython   with:     pythonversionfile: '.pythonversion'  Read python version from a file  run: python my_script.py  There is no default python version for this setuppython major version, the action requires to specify either pythonversion input or pythonversionfile input. If the pythonversion input is not specified the action will try to read required version from file from pythonversionfile input.  Use pypyX.Y for PyPy pythonversion input:  CC(Error importing jax after certain tensorflow import)  Example of usage:  uses: actions/setuppython   with:     pythonversion: 'pypy3.9'  pypyX.Y kept for backward compatibility  run: python my_script.py    RUNNER_TOOL_CACHE environment variable is equal AGENT_TOOLSDIRECTORY:  CC(Change implementation of negative loglikelihood in README toy example)   Bugfix: create missing pypyX.Y symlinks:  CC(Issue taking gradients through np.where when one of branches is nan.)   PKG_CONFIG_PATH environment variable:  CC(Support tuples in translation rule for zeros_like_p.)   Added pythonpath output:  CC(Remove obsolete workarounds for bugs that seem fixed.) pythonpath output contains Python executable path.   Updated zeit/ncc to vercel/ncc package:  CC(override __new__ in dtype classes)   Bugfix: fixed output for prerelease version of poetry:  CC(Use a regular import to add jax.__version__ rather than exec() trickery.)   Made pythonLocation environment variable consistent for Python and PyPy:  CC(Document jax.disable_jit. Add an example to jax.grad.)   Bugfix for 3.xdev syntax:  CC(Default XLA/GPU memory allocator is synchronous/slow.)   Other improvements:  CC(actually fix nondeterminism in einsum)  CC(clarification to README)  CC(Fix dimension numbers in LHS transpose rule for conv_general_dilated.)  CC(Hessian calculation finds an UnshapedArray when jitted.)  CC(__invert__ doesn't take an argument.)   v3.1.4 What's Changed In the scope of this patch release, the warning for deprecating Python 2.x was added in actions/setuppython CC(Disable indexed update tests on GPU to work around LLVM failure.) by @​dmitryshibanov For more information, check out actions/setuppython CC(Cannot take gradient of np.prod)   ... (truncated)   Commits  65d7f2d Add range validation for toml files ( CC(avoid packing leaf outputs for jit/pmap funs)) f97b831 Bump wordwrap from 1.2.3 to 1.2.4 ( CC(Build on linux, got ""//crosstool:cccompilerwindows"" error)) 61a6322 Fix typos found by codespell ( CC(jacfwd through while_loop)) ea5b57f Bump semver from 7.3.8 to 7.5.2 ( CC(make static_argnums cache on value when possible)) 014d32a Bump toughcookie and @​azure/msrestjs ( CC(Use DeviceOrdinals method rather than accessing Computation._device_o…)) c16c4b8 Fix pipenv jobs ( CC(Implement eigh batching.)) 0d5da6a Read python version from pyproject.toml (fix  CC(fix typo in vmap (fixes 536))) ( CC(Support autodiff of Eigendecomposition with repeated eigenvalues)) 3f824b7 remove python 2.7 from the tests ( CC(Implement np.average)) bd6b4b6 Add warning for python 2.7 ( CC(vmap over a list of functions)) 0cbcb9a Merge pull request  CC(Bump minimum jaxlib version to 0.1.12.) from akvplatform/disallowimplicitdependencies Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2023-11-20T17:23:12Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/18607,We need to update the comments as well; I'll do this separately.,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
5462,"以下是一个github上的jax下的一个issue, 标题是(Bump actions/github-script from 6 to 7)， 内容是 (Bumps actions/githubscript from 6 to 7.  Release notes Sourced from actions/githubscript's releases.  v7.0.0 What's Changed  Add baseurl option by @​robandpdx in actions/githubscript CC(Add docstrings for lax.gather/scatter.) Expose asyncfunction argument type by @​viktorlott in actions/githubscript CC(Do JAX jit'd Python loops run faster than jit'd LAX loop constructs?), see for details https://github.com/actions/githubscriptusescriptswithjsdocsupport Update dependencies and use Node 20 by @​joshmgross in actions/githubscript CC(Implement np.float_power.)  New Contributors  @​navarroaxel made their first contribution in actions/githubscript CC(Fix average pooling to align the window element counts with the spatial dimensions.) @​robandpdx made their first contribution in actions/githubscript CC(Add docstrings for lax.gather/scatter.) @​viktorlott made their first contribution in actions/githubscript CC(Do JAX jit'd Python loops run faster than jit'd LAX loop constructs?)  Full Changelog: https://github.com/actions/githubscript/compare/v6.4.1...v7.0.0 v6.4.1 What's Changed  Add @​octokit/pluginrequestlog, to produce debug output for requests by @​mjpieters in actions/githubscript CC(added jvp rule for eigh, tests) fix input handling by @​mjpieters in actions/githubscript CC(jit inside while_loop causes an error) Remove unused dependencies by @​mjpieters in actions/githubscript CC(improve cholesky jvp, misc other improvements (closes 354)) Default debug to current runner debug state by @​mjpieters in actions/githubscript CC(Update latex_test.py)  New Contributors  @​mjpieters made their first contribution in actions/githubscript CC(added jvp rule for eigh, tests)  Full Changelog: https://github.com/actions/githubscript/compare/v6.4.0...v6.4.1 v6.4.0 What's Changed  Bump json5 from 2.1.3 to 2.2.3 by @​dependabot in actions/githubscript CC(Forward np.{bartlett,blackman,hamming,hanning,kaiser} to numpy.) Bump minimatch from 3.0.4 to 3.1.2 by @​dependabot in actions/githubscript CC(Fix some TODOs in linalg: use gather instead of matmul) Add nodefetch by @​danmichaelo in actions/githubscript CC(Implement scipy.linalg.{cho_factor,cho_solve}.)  New Contributors  @​jongwooo made their first contribution in actions/githubscript CC(skip some cases to satisfy internal tests) @​austinvazquez made their first contribution in actions/githubscript CC(Implement np.take (70).) @​danmichaelo made their first contribution in actions/githubscript CC(Implement scipy.linalg.{cho_factor,cho_solve}.)  Full Changelog: https://github.com/actions/githubscript/compare/v6.3.3...v6.4.0 v6.3.3 What's Changed  Update /glob to 0.3.0 by @​nineinchnick in actions/githubscript CC(added support for np.newaxis to jax.numpy)  New Contributors  @​nineinchnick made their first contribution in actions/githubscript CC(added support for np.newaxis to jax.numpy)  Full Changelog: https://github.com/actions/githubscript/compare/v6.3.2...v6.3.3 v6.3.2 What's Changed  Update @​actions/core to 1.10.0 by @​rentziass in actions/githubscript CC(未找到相关数据)    ... (truncated)   Commits  60a0d83 Merge pull request  CC(Expose logsumexp as scipy.special.logsumexp.) from actions/joshmgross/v7.0.1 b7fb200 Update version to 7.0.1 12e22ed Merge pull request  CC(fix nested pjit transpose bug) from actions/joshmgross/avoidsettingbaseurl d319f8f Avoid setting baseUrl to undefined when input is not provided e69ef54 Merge pull request  CC(Implement np.float_power.) from actions/joshmgross/node20 ee0914b Update licenses d6fc56f Use /node for Node 20 384d6cf Fix quotations in tests 8472492 Only validate GraphQL previews 84903f5 Remove nodefetch from type Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Bump actions/github-script from 6 to 7,"Bumps actions/githubscript from 6 to 7.  Release notes Sourced from actions/githubscript's releases.  v7.0.0 What's Changed  Add baseurl option by @​robandpdx in actions/githubscript CC(Add docstrings for lax.gather/scatter.) Expose asyncfunction argument type by @​viktorlott in actions/githubscript CC(Do JAX jit'd Python loops run faster than jit'd LAX loop constructs?), see for details https://github.com/actions/githubscriptusescriptswithjsdocsupport Update dependencies and use Node 20 by @​joshmgross in actions/githubscript CC(Implement np.float_power.)  New Contributors  @​navarroaxel made their first contribution in actions/githubscript CC(Fix average pooling to align the window element counts with the spatial dimensions.) @​robandpdx made their first contribution in actions/githubscript CC(Add docstrings for lax.gather/scatter.) @​viktorlott made their first contribution in actions/githubscript CC(Do JAX jit'd Python loops run faster than jit'd LAX loop constructs?)  Full Changelog: https://github.com/actions/githubscript/compare/v6.4.1...v7.0.0 v6.4.1 What's Changed  Add @​octokit/pluginrequestlog, to produce debug output for requests by @​mjpieters in actions/githubscript CC(added jvp rule for eigh, tests) fix input handling by @​mjpieters in actions/githubscript CC(jit inside while_loop causes an error) Remove unused dependencies by @​mjpieters in actions/githubscript CC(improve cholesky jvp, misc other improvements (closes 354)) Default debug to current runner debug state by @​mjpieters in actions/githubscript CC(Update latex_test.py)  New Contributors  @​mjpieters made their first contribution in actions/githubscript CC(added jvp rule for eigh, tests)  Full Changelog: https://github.com/actions/githubscript/compare/v6.4.0...v6.4.1 v6.4.0 What's Changed  Bump json5 from 2.1.3 to 2.2.3 by @​dependabot in actions/githubscript CC(Forward np.{bartlett,blackman,hamming,hanning,kaiser} to numpy.) Bump minimatch from 3.0.4 to 3.1.2 by @​dependabot in actions/githubscript CC(Fix some TODOs in linalg: use gather instead of matmul) Add nodefetch by @​danmichaelo in actions/githubscript CC(Implement scipy.linalg.{cho_factor,cho_solve}.)  New Contributors  @​jongwooo made their first contribution in actions/githubscript CC(skip some cases to satisfy internal tests) @​austinvazquez made their first contribution in actions/githubscript CC(Implement np.take (70).) @​danmichaelo made their first contribution in actions/githubscript CC(Implement scipy.linalg.{cho_factor,cho_solve}.)  Full Changelog: https://github.com/actions/githubscript/compare/v6.3.3...v6.4.0 v6.3.3 What's Changed  Update /glob to 0.3.0 by @​nineinchnick in actions/githubscript CC(added support for np.newaxis to jax.numpy)  New Contributors  @​nineinchnick made their first contribution in actions/githubscript CC(added support for np.newaxis to jax.numpy)  Full Changelog: https://github.com/actions/githubscript/compare/v6.3.2...v6.3.3 v6.3.2 What's Changed  Update @​actions/core to 1.10.0 by @​rentziass in actions/githubscript CC(未找到相关数据)    ... (truncated)   Commits  60a0d83 Merge pull request  CC(Expose logsumexp as scipy.special.logsumexp.) from actions/joshmgross/v7.0.1 b7fb200 Update version to 7.0.1 12e22ed Merge pull request  CC(fix nested pjit transpose bug) from actions/joshmgross/avoidsettingbaseurl d319f8f Avoid setting baseUrl to undefined when input is not provided e69ef54 Merge pull request  CC(Implement np.float_power.) from actions/joshmgross/node20 ee0914b Update licenses d6fc56f Use /node for Node 20 384d6cf Fix quotations in tests 8472492 Only validate GraphQL previews 84903f5 Remove nodefetch from type Additional commits viewable in compare view    ![Dependabot compatibility score](https://docs.github.com/en/github/managingsecurityvulnerabilities/aboutdependabotsecurityupdatesaboutcompatibilityscores) Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` show  ignore conditions` will show all of the ignore conditions of the specified dependency  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2023-11-20T17:23:05Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/18606,We need to update the comments as well; I'll do this separately.,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
955,"以下是一个github上的jax下的一个issue, 标题是(Arctan2 gradient explosion for small inputs)， 内容是 ( Description Arctan2 seems to have a large gradient when the inputs are small. If training a neural network, this blows up the weights, destroying the network. I believe the gradient of `arctan2` should be clamped to 1.0. This behavior does not seem to be present in arctan ```python import jax import jax.numpy as jnp print(jax.grad(jnp.arctan2)(1e10, 1e10))  Array(5.e+09, dtype=float32, weak_type=True) print(jax.grad(jnp.arctan)(1e10 / 1e10))  Array(0.5 dtype=float32, weak_type=True) ``` Related to https://github.com/google/jax/issues/15407  What jax/jaxlib version are you using? 0.4.19  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` GeForce RTX 2080 Ti cuda==12.2.140 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Arctan2 gradient explosion for small inputs," Description Arctan2 seems to have a large gradient when the inputs are small. If training a neural network, this blows up the weights, destroying the network. I believe the gradient of `arctan2` should be clamped to 1.0. This behavior does not seem to be present in arctan ```python import jax import jax.numpy as jnp print(jax.grad(jnp.arctan2)(1e10, 1e10))  Array(5.e+09, dtype=float32, weak_type=True) print(jax.grad(jnp.arctan)(1e10 / 1e10))  Array(0.5 dtype=float32, weak_type=True) ``` Related to https://github.com/google/jax/issues/15407  What jax/jaxlib version are you using? 0.4.19  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` GeForce RTX 2080 Ti cuda==12.2.140 ```",2023-11-20T16:18:34Z,bug needs info,closed,0,3,https://github.com/jax-ml/jax/issues/18605,"Hi  thanks for the report. It looks to me like the gradient is returning the expected result – analytically, the gradient is $$ \frac{\partial}{\partial x}{\rm atan2}(y, x) = \frac{y}{x^2 + y^2} $$ $$ \frac{\partial}{\partial y}{\rm atan2}(y, x) = \frac{x}{x^2 + y^2} $$ It's clear here that as `x` and `y` approach zero, both partial derivatives diverge. On the other hand, the derivative of `arctan` is: $$ \frac{\rm d}{{\rm d}x} {\rm arctan}(x) = \frac{1}{1 + x^2} $$ so as `x` approaches zero, the derivative approaches 1. Both seem consistent with the output you're seeing from JAX. What do you think?","Whoops, good point. Even though `arctan` is wellbehaved, I suppose the division in a neural network is really the issue (we explicitly divide when calling `arctan(y / x)`, where as `arctan2(y, x)` divides implicitly). I am not sure how good of an idea it is, but I can clamp the gradient from `arctan2` to be in [1, 1] to prevent these explosions (at the cost of slower convergence). Is this the correct way to do so? ```python import jax import jax.numpy as jnp .custom_vjp def clamped_atan2(y, x):     return jnp.arctan2(y, x) def clamped_atan2_f(y, x):     return clamped_atan2(y, x), (y, x) def clamped_atan2_b(res, g):     y, x = res      Compute the gradients with respect to y and x     dy = jax.grad(jnp.arctan2, argnums=0)(y, x)     dx = jax.grad(jnp.arctan2, argnums=1)(y, x)      Clip the gradients     dy_clipped = jnp.clip(dy, a_min=1.0, a_max=1.0)     dx_clipped = jnp.clip(dx, a_min=1.0, a_max=1.0)      Return the product of the clipped gradients and the upstream gradient     return g * dy_clipped, g * dx_clipped clamped_atan2.defvjp(clamped_atan2_f, clamped_atan2_b) standard_fn = lambda y, x: jnp.arctan2(y, x**2)  clamped_fn = lambda y, x: clamped_atan2(y, x**2)   Test it does the right thing when the grad is small y = jnp.array(2.0) x = jnp.array(1.0) standard = standard_fn(y, x) clamped = clamped_fn(y, x) standard_g = jax.grad(standard_fn)(y, x)  clamped_g = jax.grad(clamped_fn)(y, x) print(standard, clamped)  1.1071488 1.1071488 print(standard_g, clamped_g)  0.2 0.2  Test it does the right thing when the grad is big y = jnp.array(2e15) x = jnp.array(1e10) standard = standard_fn(y, x) clamped = clamped_fn(y, x) standard_g = jax.grad(standard_fn)(y, x) clamped_g = jax.grad(clamped_fn)(y, x) print(standard, clamped)  0.0 0.0 print(standard_g, clamped_g)  2500000300.0 1.0 ``` One final question: to get this to work over batches, I need to `vmap(clamped_atan2)`, as `jax.grad` only works for a single output. Is there a way to get it to broadcast like the original `arctan2` function?","You haven't said much about where you're using this function, but I would be hesitant to create custom autodiff rules that are deliberately producing incorrect derivatives for the function you're evaluating. I wonder if you might have better luck choosing an alternative function which has betterbehaved derivatives in the domains you're interested in?"
742,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Clean up the shape_poly_test.py)， 内容是 (When we recently moved much of shape_poly_test out of jax2tf we had to add a number of flags to avoid warnings (which are errors in GitHub CI). Here we clean the tests so that we can run them without the flags. The most common problem was that tests were relying on implicit rank promotion. We added a number of `jnp.expand_dims` to fix the rank and let the implicit broadcasting do the rest. The other problem solved here was for the `jax.random` functions, we add `jax.random.wrap_key_data` to turn arrays into keys.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Clean up the shape_poly_test.py,"When we recently moved much of shape_poly_test out of jax2tf we had to add a number of flags to avoid warnings (which are errors in GitHub CI). Here we clean the tests so that we can run them without the flags. The most common problem was that tests were relying on implicit rank promotion. We added a number of `jnp.expand_dims` to fix the rank and let the implicit broadcasting do the rest. The other problem solved here was for the `jax.random` functions, we add `jax.random.wrap_key_data` to turn arrays into keys.",2023-11-20T08:11:32Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18602
622,"以下是一个github上的jax下的一个issue, 标题是([shard_map] add eager axis_index implementation and tests)， 内容是 (This approach works because: 1. we can think of eager as only applying `jit` to individual primitives, which this implementation literally does; 2. when we run `jit(lambda: axis_index(axis_name))()`, the `axis_index` call gets staged into a jaxpr, and then we bind the `pjit_p` primitive, which works just like the noneager path (i.e. `shard_map`of`jit`of`collectives` already worked).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard_map] add eager axis_index implementation and tests,"This approach works because: 1. we can think of eager as only applying `jit` to individual primitives, which this implementation literally does; 2. when we run `jit(lambda: axis_index(axis_name))()`, the `axis_index` call gets staged into a jaxpr, and then we bind the `pjit_p` primitive, which works just like the noneager path (i.e. `shard_map`of`jit`of`collectives` already worked).",2023-11-19T19:12:37Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18599
6490,"以下是一个github上的jax下的一个issue, 标题是(Pallas implementation of attention doesn't work on CloudTPU)， 内容是 ( Description ```python import jax import jax.numpy as jnp from jax.experimental.pallas.ops import attention bs = 2 seqlen = 1000 n_heads = 32 dim = 128 rng = jax.random.PRNGKey(0) xq = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) xk = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) xv = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) print('reference') res = attention.mha_reference(xq, xk, xv, None) print(res) print(res.shape) print('real kernel') print(attention.mha(xq, xk, xv, None)) ``` Got: ``` hanqncfe84bb3w0:~/llama$ python test_jax.py reference    ...    [0.68359375 1.9609375  1.2734375  ...  0.02490234  0.703125      0.69921875]    [0.4453125  0.5859375   1.28125    ... 0.24414062  1.21875      0.47851562]    [ 0.0859375  1.703125   0.4921875  ... 0.38476562  0.8828125      0.09765625]]   [[ 0.93359375 0.6015625   1.4296875  ...  1.3515625   0.6796875     1.0859375 ]    [ 0.875       0.22460938  1.0625     ...  1.421875   0.09472656     0.18847656]    [ 1.34375     0.02441406 1.34375    ... 0.01501465 0.90234375     0.31054688]    ...    [ 0.4609375   0.609375   0.9921875  ... 1.015625    0.6796875     0.5546875 ]    [1.25       0.59765625  0.1328125  ...  0.0390625   0.43945312     0.97265625]    [ 1.140625   0.17382812 1.03125    ... 0.76171875  0.77734375     0.18261719]]   ...    [ 0.734375   1.3203125   0.92578125 ...  0.9453125   0.18554688      1.0078125 ]    [ 0.96875    1.5390625   0.47265625 ...  0.96875    1.1640625      0.29882812]    ...    [ 0.04296875  0.53515625  0.20410156 ...  1.28125    0.69140625     0.10058594]    [0.04150391 0.66796875 1.078125   ... 1.09375    0.4296875      0.828125  ]    [0.5234375   0.43164062 0.69140625 ...  1.1796875  2.296875      0.21972656]]   ...   [[ 0.06445312  0.40429688  0.03735352 ... 1.6796875   1.1796875     0.98828125]    [0.7109375  0.34375    0.23144531 ...  0.13085938 0.47070312     0.21875   ]    [0.66796875 0.02954102 1.046875   ...  0.2421875   1.203125     0.42382812]    ...    [ 0.14257812  0.58984375  0.40234375 ... 0.01672363 0.57421875      1.046875  ]    [0.62890625 1.1171875   0.84375    ... 0.35351562 0.22558594    ...    [ 0.859375    1.9765625   0.54296875 ...  1.109375    0.05639648      0.6796875 ]    [0.29101562 1.9921875  1.734375   ...  1.2265625   0.14453125     0.53125   ]    [ 1.6484375  0.40820312 0.828125   ... 0.265625   0.28320312     0.43164062]]   [[0.27539062 1.8671875  0.078125   ...  0.515625    0.90625     2.453125  ]    [ 0.17773438 0.11572266  0.5390625  ... 0.5546875  0.40625      0.9765625 ]    [0.05395508 0.00325012 0.08691406 ... 0.8046875  0.03979492      0.07666016]    ...    [0.3359375   0.87890625 1.453125   ...  1.1328125   0.46875      0.65625   ]    [ 0.30273438  0.546875    0.11083984 ...  0.98828125 0.10791016      1.4375    ]    [ 1.4375     0.46484375  1.71875    ...  1.265625    0.21386719     0.70703125]]]] (2, 1000, 32, 128) real kernel Traceback (most recent call last):   File ""/home/hanq/llama/test_jax.py"", line 20, in      print(attention.mha(xq, xk, xv, None))   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/attention.py"", line 216, in mha     return pl.pallas_call(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 410, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: Internal TPU kernel compiler error: Loading elements out of bounds The MLIR operation involved:   %209 = ""vector.load""(%14, %9, %0, %9, %9) : (memref>, index, index, index, index) > vector Please report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/hanq/llama/test_jax.py"", line 20, in      print(attention.mha(xq, xk, xv, None))  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/hanq/llama/test_jax.py"", line 20, in      print(attention.mha(xq, xk, xv, None))   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 88, in pallas_call_tpu_lowering_rule     return mlir.lower_fun(_lower_fun, multiple_results=True)(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 79, in _lower_fun     return mosaic.as_tpu_kernel(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 408, in as_tpu_kernel     lowered_module_asm, constants = _lower_tpu_kernel(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 335, in _lower_tpu_kernel     _run_pass_pipeline(pipeline, module, ""infer vector layout"")   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 264, in _run_pass_pipeline     raise RuntimeError(""\n"".join(msg)) from None RuntimeError: Internal TPU kernel compiler error: Loading elements out of bounds The MLIR operation involved:   %209 = ""vector.load""(%14, %9, %0, %9, %9) : (memref>, index, index, index, index) > vector Please report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke ```  What jax/jaxlib version are you using? jax==0.4.21.dev20231117 jaxlib==0.4.21.dev20231117  Which accelerator(s) are you using? TPU  Additional system info Python 3.10.2 Uname=Linux t1vncfe84bb3w0 5.19.01022gcp CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llama,Pallas implementation of attention doesn't work on CloudTPU," Description ```python import jax import jax.numpy as jnp from jax.experimental.pallas.ops import attention bs = 2 seqlen = 1000 n_heads = 32 dim = 128 rng = jax.random.PRNGKey(0) xq = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) xk = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) xv = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) print('reference') res = attention.mha_reference(xq, xk, xv, None) print(res) print(res.shape) print('real kernel') print(attention.mha(xq, xk, xv, None)) ``` Got: ``` hanqncfe84bb3w0:~/llama$ python test_jax.py reference    ...    [0.68359375 1.9609375  1.2734375  ...  0.02490234  0.703125      0.69921875]    [0.4453125  0.5859375   1.28125    ... 0.24414062  1.21875      0.47851562]    [ 0.0859375  1.703125   0.4921875  ... 0.38476562  0.8828125      0.09765625]]   [[ 0.93359375 0.6015625   1.4296875  ...  1.3515625   0.6796875     1.0859375 ]    [ 0.875       0.22460938  1.0625     ...  1.421875   0.09472656     0.18847656]    [ 1.34375     0.02441406 1.34375    ... 0.01501465 0.90234375     0.31054688]    ...    [ 0.4609375   0.609375   0.9921875  ... 1.015625    0.6796875     0.5546875 ]    [1.25       0.59765625  0.1328125  ...  0.0390625   0.43945312     0.97265625]    [ 1.140625   0.17382812 1.03125    ... 0.76171875  0.77734375     0.18261719]]   ...    [ 0.734375   1.3203125   0.92578125 ...  0.9453125   0.18554688      1.0078125 ]    [ 0.96875    1.5390625   0.47265625 ...  0.96875    1.1640625      0.29882812]    ...    [ 0.04296875  0.53515625  0.20410156 ...  1.28125    0.69140625     0.10058594]    [0.04150391 0.66796875 1.078125   ... 1.09375    0.4296875      0.828125  ]    [0.5234375   0.43164062 0.69140625 ...  1.1796875  2.296875      0.21972656]]   ...   [[ 0.06445312  0.40429688  0.03735352 ... 1.6796875   1.1796875     0.98828125]    [0.7109375  0.34375    0.23144531 ...  0.13085938 0.47070312     0.21875   ]    [0.66796875 0.02954102 1.046875   ...  0.2421875   1.203125     0.42382812]    ...    [ 0.14257812  0.58984375  0.40234375 ... 0.01672363 0.57421875      1.046875  ]    [0.62890625 1.1171875   0.84375    ... 0.35351562 0.22558594    ...    [ 0.859375    1.9765625   0.54296875 ...  1.109375    0.05639648      0.6796875 ]    [0.29101562 1.9921875  1.734375   ...  1.2265625   0.14453125     0.53125   ]    [ 1.6484375  0.40820312 0.828125   ... 0.265625   0.28320312     0.43164062]]   [[0.27539062 1.8671875  0.078125   ...  0.515625    0.90625     2.453125  ]    [ 0.17773438 0.11572266  0.5390625  ... 0.5546875  0.40625      0.9765625 ]    [0.05395508 0.00325012 0.08691406 ... 0.8046875  0.03979492      0.07666016]    ...    [0.3359375   0.87890625 1.453125   ...  1.1328125   0.46875      0.65625   ]    [ 0.30273438  0.546875    0.11083984 ...  0.98828125 0.10791016      1.4375    ]    [ 1.4375     0.46484375  1.71875    ...  1.265625    0.21386719     0.70703125]]]] (2, 1000, 32, 128) real kernel Traceback (most recent call last):   File ""/home/hanq/llama/test_jax.py"", line 20, in      print(attention.mha(xq, xk, xv, None))   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/attention.py"", line 216, in mha     return pl.pallas_call(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 410, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: Internal TPU kernel compiler error: Loading elements out of bounds The MLIR operation involved:   %209 = ""vector.load""(%14, %9, %0, %9, %9) : (memref>, index, index, index, index) > vector Please report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/hanq/llama/test_jax.py"", line 20, in      print(attention.mha(xq, xk, xv, None))  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/hanq/llama/test_jax.py"", line 20, in      print(attention.mha(xq, xk, xv, None))   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 88, in pallas_call_tpu_lowering_rule     return mlir.lower_fun(_lower_fun, multiple_results=True)(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 79, in _lower_fun     return mosaic.as_tpu_kernel(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 408, in as_tpu_kernel     lowered_module_asm, constants = _lower_tpu_kernel(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 335, in _lower_tpu_kernel     _run_pass_pipeline(pipeline, module, ""infer vector layout"")   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 264, in _run_pass_pipeline     raise RuntimeError(""\n"".join(msg)) from None RuntimeError: Internal TPU kernel compiler error: Loading elements out of bounds The MLIR operation involved:   %209 = ""vector.load""(%14, %9, %0, %9, %9) : (memref>, index, index, index, index) > vector Please report a bug at: https://github.com/google/jax/issues/new?assignees=apaszke ```  What jax/jaxlib version are you using? jax==0.4.21.dev20231117 jaxlib==0.4.21.dev20231117  Which accelerator(s) are you using? TPU  Additional system info Python 3.10.2 Uname=Linux t1vncfe84bb3w0 5.19.01022gcp CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux  NVIDIA GPU info _No response_",2023-11-18T01:18:22Z,bug TPU,closed,0,16,https://github.com/jax-ml/jax/issues/18590,Can you try the version in jax/experimental/ops/tpu/flash_attention.py?,"Sure, updated test script to  ```python hanqncfe84bb3w0:~/llama$ cat test_jax.py import jax import jax.numpy as jnp from jax.experimental.pallas.ops import attention from jax.experimental.pallas.ops.tpu import flash_attention bs = 2 seqlen = 1000 n_heads = 128 dim = 512 rng = jax.random.PRNGKey(0) xq = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) xk = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) xv = jax.random.normal(rng, (bs, seqlen, n_heads, dim)) print('reference') res = flash_attention.mha_reference(xq, xk, xv, None) print(res) print(res.shape) print('real kernel') print(flash_attention.flash_attention(xq, xk, xv, None, debug=True)) ``` Got ``` hanqncfe84bb3w0:~/llama$ python test_jax.py reference (2, 1000, 128, 512) real kernel { lambda ; a:Ref{float32[1,1,128,512]} b:Ref{float32[1,1,128,512]} c:Ref{float32[1,1,128,512]}     d:Ref{float32[1,1,128,512]}. let     e:f32[128,512] >, %arg5: memref>, %arg6: memref>, %arg7: memref>) attributes {dimension_semantics = [tpu.dimension_semantics, tpu.dimension_semantics, tpu.dimension_semantics, tpu.dimension_semantics], iteration_bounds = array, scalar_prefetch = 0 : i64, scratch_operands = 0 : i64, window_params = [{transform_indices = , window_bounds = array}, {transform_indices = , window_bounds = array}, {transform_indices = , window_bounds = array}, {transform_indices = , window_bounds = array}]} {     %c0 = arith.constant 0 : index     %c0_0 = arith.constant 0 : index     %c0_1 = arith.constant 0 : index     %c0_2 = arith.constant 0 : index     %0 = vector.load %arg4[%c0, %c0_0, %c0_1, %c0_2] : memref>, vector     %1 = vector.shape_cast %0 : vector to vector     %c0_3 = arith.constant 0 : index     %c0_4 = arith.constant 0 : index     %c0_5 = arith.constant 0 : index     %c0_6 = arith.constant 0 : index     %2 = vector.load %arg5[%c0_3, %c0_4, %c0_5, %c0_6] : memref>, vector     %3 = vector.shape_cast %2 : vector to vector     %cst = arith.constant dense : vector     %4 = tpu.matmul %1, %3, %cst {transpose_rhs = true} : vector, vector, vector > vector     %cst_7 = arith.constant dense : vector     %5 = vector.multi_reduction , %4, %cst_7 [1] : vector to vector     %6 = vector.shape_cast %5 : vector to vector     %15 = vector.shape_cast %14 : vector to vector     %cst_13 = arith.constant dense : vector     %16 = tpu.matmul %13, %15, %cst_13 : vector, vector, vector > vector     %c0_14 = arith.constant 0 : index     %c0_15 = arith.constant 0 : index     %c0_16 = arith.constant 0 : index     %c0_17 = arith.constant 0 : index     %17 = vector.load %arg7[%c0_14, %c0_15, %c0_16, %c0_17] : memref>, vector     %18 = vector.shape_cast %17 : vector to vector     %19 = vector.shape_cast %16 : vector to vector     vector.store %19, %arg7[%c0_14, %c0_15, %c0_16, %c0_17] : memref>, vector     return   }   func.func (%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32) > (i32, i32, i32, i32) {     %c0_i32 = arith.constant 0 : i32     return %arg0, %arg1, %arg2, %c0_i32 : i32, i32, i32, i32   }   func.func (%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32) > (i32, i32, i32, i32) {     %c0_i32 = arith.constant 0 : i32     return %arg0, %arg1, %arg3, %c0_i32 : i32, i32, i32, i32   }   func.func (%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32) > (i32, i32, i32, i32) {     %c0_i32 = arith.constant 0 : i32     return %arg0, %arg1, %arg3, %c0_i32 : i32, i32, i32, i32   }   func.func (%arg0: i32, %arg1: i32, %arg2: i32, %arg3: i32) > (i32, i32, i32, i32) {     %c0_i32 = arith.constant 0 : i32     return %arg0, %arg1, %arg2, %c0_i32 : i32, i32, i32, i32   } } Traceback (most recent call last):   File ""/home/hanq/llama/test_jax.py"", line 21, in      print(flash_attention.flash_attention(xq, xk, xv, None, debug=True))   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 181, in flash_attention     return _flash_attention(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 199, in _flash_attention     return _flash_attention_impl(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/experimental/pallas/ops/tpu/flash_attention.py"", line 729, in _flash_attention_impl     o, *aux = pl.pallas_call(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/pallas_call.py"", line 410, in wrapped     out_flat = pallas_call_p.bind( jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: Expected a tiled memref The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/hanq/llama/test_jax.py"", line 21, in      print(flash_attention.flash_attention(xq, xk, xv, None, debug=True))   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 88, in pallas_call_tpu_lowering_rule     return mlir.lower_fun(_lower_fun, multiple_results=True)(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 79, in _lower_fun     return mosaic.as_tpu_kernel(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 408, in as_tpu_kernel     lowered_module_asm, constants = _lower_tpu_kernel(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 347, in _lower_tpu_kernel     apply_vector_layout.apply(module, hardware_generation) The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/hanq/llama/test_jax.py"", line 21, in      print(flash_attention.flash_attention(xq, xk, xv, None, debug=True))   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 88, in pallas_call_tpu_lowering_rule     return mlir.lower_fun(_lower_fun, multiple_results=True)(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/pallas/mosaic/pallas_call_registration.py"", line 79, in _lower_fun     return mosaic.as_tpu_kernel(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 408, in as_tpu_kernel     lowered_module_asm, constants = _lower_tpu_kernel(   File ""/home/hanq/.local/lib/python3.10/sitepackages/jax/_src/tpu_custom_call.py"", line 347, in _lower_tpu_kernel     apply_vector_layout.apply(module, hardware_generation)   File ""/home/hanq/.local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py"", line 1628, in apply     apply_layout_func(ctx, f)   File ""/home/hanq/.local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py"", line 1535, in apply_layout_func     apply_layout_block(ctx, entry_block)   File ""/home/hanq/.local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py"", line 1541, in apply_layout_block     apply_layout_op(ctx, op)   File ""/home/hanq/.local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py"", line 1616, in apply_layout_op     rule(ctx, op, layout_in, layout_out)   File ""/home/hanq/.local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py"", line 2572, in _vector_load_rule     memref_tiling = get_memref_tiling(op.base)   File ""/home/hanq/.local/lib/python3.10/sitepackages/jaxlib/mosaic/python/apply_vector_layout.py"", line 3498, in get_memref_tiling     raise RuntimeError(""Expected a tiled memref"") RuntimeError: Expected a tiled memref ``` Without the debug = True would get the same exception but without the logs of MLIR; so I pasted the one with debug = True.",Another data point: if I add `interpret=True` to `pl.pallas_call` inside of the flash_attention.py; then it passes.,What TPU version are you using?,I think you'll also need a nightly jaxlib for this to work.,"Updated today's nightlies:  jax==0.4.21.dev20231118 jaxlib==0.4.21.dev20231118 libtpunightly @ https://storage.googleapis.com/cloudtputpuvmartifacts/wheels/libtpunightly/libtpu_nightly0.1.dev20231118py3noneany.whl Issue still presists. (Note, in today's nightlies, it will first give `KeyError: 'xla_mosaic_dump_to'`; I commented that function out and then it will give back the same `RuntimeError: Expected a tiled memref` error.",One thing I see is that you need to swap the num_heads and seqlen dimension for TPU. Could you try that? Unsure of it will fix the error but worth trying.,"Tried, doesnt work. Why would it be differnt anyways? both `seqlen` and `n_heads` are just integers; and flash_attention should work bof both (seqlen=1000, n_heads=32) and (seqlen=32, n_heads=1000).  I am curious if you can repro my error or not. If not, I'd love to know your environment setup so I can use that exact one (I don't care of using the newest Jax, I just need a version that works). Thanks!","> Why would it be differnt anyways? In pallas, dimension order has physical implication. On TPUs, we need to pick some two dimensions to be layed out as vector register dimensions and the rest will be unrolled across many vector registers. In the case of flash attention, we lay the seqlen and head_dim across register dimensions and unroll over bs, heads. One last thing to try before I can do a more thorough investigation tomorrow would be to use seqlen=1024. Mosaic doesn't support arbitrary padding on all dimensions quite yet so it's worth trying powers of 2 for the last two dimensions. head_dim of 128 is already fine.","yes, the issue still persists with `seqlen=1024`. Just to understand more, you are saying the flipping variables `seqlen` and `n_heads` matter because the kernel might expect `seqlen > n_heads`? Because the name of python variables  definitely shouldn't matter.","Okay, I'll do a more thorough investigation tomorrow. Wrt to dimension order, the kernel expects a sequence length dimension (ie something usually bigger than number of heads) in the second to last position.","I was able to run it successfully on a Cloud TPU VM (v5e) with these versions: ``` abslpy==2.0.0 certifi==2023.11.17 charsetnormalizer==3.3.2 idna==3.6 e git+https://github.com/google/jax.gitegg=jax jaxlib==0.4.21.dev20231127 libtpunightly @ https://storage.googleapis.com/cloudtputpuvmartifacts/wheels/libtpunightly/libtpu_nightly0.1.dev20231127py3noneany.whlsha256=9dc8214ff9a50ceec13cd93df7053106a8750bc1df4c7edc351794f436753756 mldtypes==0.3.1 numpy==1.26.2 opteinsum==3.3.0 requests==2.31.0 scipy==1.11.4 urllib3==2.1.0 ``` I ran this script: ```python import jax import jax.numpy as jnp from jax.experimental.pallas.ops.tpu import flash_attention bs = 2 seqlen = 1024 n_heads = 32 dim = 128 rng = jax.random.PRNGKey(0) xq = jax.random.normal(rng, (bs, n_heads, seqlen, dim)) xk = jax.random.normal(rng, (bs, n_heads, seqlen, dim)) xv = jax.random.normal(rng, (bs, n_heads, seqlen, dim)) print('reference') res = flash_attention.mha_reference(xq, xk, xv, None) print(res) print(res.shape) print('real kernel') print(flash_attention.flash_attention(xq, xk, xv, None)) ```",Thanks! Confirmed that `jax @ git+https://github.com/google/jax.git` Also works on cloudtpu v48,"Reopening the issue, the flash attention still have the following 2 issues: 1. When `seqlen = 1` it gives `ValueError: block_q=128 should be smaller or equal to q_seq_len=1`. I understand that it probably it is by design at this point. However, I would like to ask for the support for the case of seqlen = 1 as this is the case during the `decode` phase for a typical LLM inference. 2. It's not faster than the reference implementation; which kinda of defeats the purpose of using a specialized kernel. Script used: ```python import time import jax import jax.numpy as jnp from jax.experimental.pallas.ops.tpu import flash_attention bs = 2 seqlen = 1024 n_heads = 1024 dim = 512 rng = jax.random.PRNGKey(0) xq = jax.random.normal(rng, (bs, n_heads, seqlen, dim)) xk = jax.random.normal(rng, (bs, n_heads, seqlen, dim)) xv = jax.random.normal(rng, (bs, n_heads, seqlen, dim)) print('real kernel') mha_real = jax.jit(flash_attention.flash_attention) for _ in range(4):     xq = jax.random.normal(rng, (bs, n_heads, seqlen, dim))     xk = jax.random.normal(rng, (bs, n_heads, seqlen, dim))     xv = jax.random.normal(rng, (bs, n_heads, seqlen, dim))     start = time.time()     res = jax.block_until_ready(mha_real(xq, xk, xv, None))     end = time.time()     print(end  start) print('reference') mha_ref = jax.jit(flash_attention.mha_reference) for _ in range(4):     xq = jax.random.normal(rng, (bs, n_heads, seqlen, dim))     xk = jax.random.normal(rng, (bs, n_heads, seqlen, dim))     xv = jax.random.normal(rng, (bs, n_heads, seqlen, dim))     start = time.time()     res = jax.block_until_ready(mha_ref(xq, xk, xv, None))     end = time.time()     print(end  start) print(res.shape) del res print(' diff ') res1 = jax.block_until_ready(mha_ref(xq, xk, xv, None)) res2 = jax.block_until_ready(mha_real(xq, xk, xv, None)) print('max diff', jnp.max(jnp.abs(res1  res2))) ``` Output ``` hanqncfe84bb3w0:~/llama$ python test_jax.py real kernel 0.29532814025878906 0.22074246406555176 0.22089743614196777 0.22095751762390137 reference 0.9379653930664062 0.1637272834777832 0.16379451751708984 0.16376519203186035 (2, 1024, 1024, 512)  diff max diff 0.0 ``` The first run it's faster I guess it's faster to compile. But the subsequent runs are slower.","> When seqlen = 1 it gives ValueError: block_q=128 should be smaller or equal to q_seq_len=1. I understand that it probably it is by design at this point. However, I would like to ask for the support for the case of seqlen = 1 as this is the case during the decode phase for a typical LLM inference. You probably want a slightly different kernel for decoding. On a TPU, naively using a sequence length of 1 will result in an extremely padded matmul. We might want to do a matrix vector product or somethign else. > It's not faster than the reference implementation; which kinda of defeats the purpose of using a specialized kernel. You're seeing these results for 2 reasons: 1. You are using default block sizes, which tend to be slow. Try passing in `block_sizes` (https://github.com/google/jax/blob/c855bb0371fd7df3e2c33c0d153a23299b4f1988/jax/experimental/pallas/ops/tpu/flash_attention.pyL148) and sweep over larger ones. You should see significant performance improvements. 2. You are using a somewhat small sequence length. For sequence lengths <= 4096, XLA has some fusions that do something similar to flash attention so the expected improvement over XLA isn't that big. Once you go to 8k and above, you should see much bigger improvements.",Thanks Sharad! Yes I tried few block sizes indeed it got faster. Thanks for the pointers!
762,"以下是一个github上的jax下的一个issue, 标题是(If parameter to `get_aval` is AbstractValue return it.)， 内容是 (Hello,  While exploring the `abstracted_axes={0: ""n""}` I noticed that passing `AbstractValue`s as inputs to the `make_jaxpr` function will cause it to yield a `TypeError`. However, passing `AbstractValue`s as inputs to `make_jaxpr` is possible when `abstracted_axes=None`. To preserve this feature, this is a possible change. I believe passing `AbstractValue`s to `make_jaxpr` directly makes sense as a way to compile AOT via type signatures. EDIT: The following PR may also be relevant: https://github.com/google/jax/pull/18505)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,If parameter to `get_aval` is AbstractValue return it.,"Hello,  While exploring the `abstracted_axes={0: ""n""}` I noticed that passing `AbstractValue`s as inputs to the `make_jaxpr` function will cause it to yield a `TypeError`. However, passing `AbstractValue`s as inputs to `make_jaxpr` is possible when `abstracted_axes=None`. To preserve this feature, this is a possible change. I believe passing `AbstractValue`s to `make_jaxpr` directly makes sense as a way to compile AOT via type signatures. EDIT: The following PR may also be relevant: https://github.com/google/jax/pull/18505",2023-11-16T17:24:59Z,,closed,0,1,https://github.com/jax-ml/jax/issues/18561,"Thanks for the suggestion! But I don't think we want this change; `core.get_aval` should only be applied to objects representing JAX values, which in turn have JAX types. An `AbstractValue` is a JAX type, not a JAX value, so it doesn't fit. The right fix is probably in the calling code, not in `core.get_aval`."
3846,"以下是一个github上的jax下的一个issue, 标题是(Feature Request: A way to redefine mesh specifications within a shard_map)， 内容是 (There are situations where one way of labeling the same mesh can make certain communication patterns easier to code than others. However, there isn't really one mesh labeling strategy that is consistently easy to use across the space of all possible communication patterns. Currently, if a user or library developer wants to develop advanced communication patterns via `shard_map`, they'll be forced to either  a) Force end users to use only a predefined set of axis labels for certain libraries. If these are not consistent between libraries, then you can not use those libraries together. b) User explicitly passes axis labels for all ops. Both library developers and end users are forced to lug around passed axes names throughout their code. It's a large burden, and very fragile to change. This is what I propose instead: A new method `mesh_remap(fn: Callable, mesh: Mesh)` that can be used within any normal `shard_map`ed method. The basic use case looks like this: ```python devices = jax.devices() mesh_2d = Mesh(devices.reshape((16, 16)), ['x', 'y']) mesh_3d = Mesh(devices.reshape((8, 2, 16)), ['a', 'b', 'c']) (mesh_remap, mesh=mesh_3d) def gather_middle(arr):    arr is still the same 2D tile from f.    Nothing has changed. Not the shape nor the device    placement    We gather against the new middle 'b' mesh axis.   res == all_gather(arr, 'b', tiled=True)   assert arr.shape[0] == res.shape[0] // 2   return res (shard_map, mesh=mesh_2D, in_spec=P('x', 'y'), out_spec=P('x', 'y')) def f(arr):    arr is a simple 2D matrix sharded against a simple 2D mesh.   return gather_middle(arr) ``` Here, all we are doing when calling `gather_middle` is redefining how the mesh is labeled. `arr` is not communicated in any way until the `all_gather(...)` call. In this case, since we are gathering on the new `'b'` mesh axis, this all gather definition would not have been possible with the previous `mesh_2d` axes. The same code could potentially be used instead with a `with` block. Since we do not need to define input or output specification, wrapping a function is not required. We are only changing the global state of the mesh, so a with clause should theoretically suffice. ```python (shard_map, mesh=mesh_2D, in_spec=P('x', 'y'), out_spec=P('x', 'y')) def f(arr):   with mesh_remap(mesh_3D):     return all_gather(arr, 'b', tiled=True) ``` Which is best from a user or jaxpr standpoint is something that should likely be discussed. Alternative considerations: 1. Simply use really high dimensional meshes and and use axes tuples (for example: `all_gather(x,  axes=('i', 'j'))`) for larger groupings. In practice this solution has been what we've been doing. The above example could've instead been implemented like this  ```python (shard_map, mesh=mesh_3D, in_spec=P(('a', 'b'), 'c'), out_spec=P(('a', 'b'), 'c')) def f(arr):   return gather_middle(arr) ``` And this works plus or minus a few edge case bugs. This is enough for small benchmarks, but it's limited in flexibility, and you'll be forced to deal with many multiple more axes at any given time than is convenient to deal with. This solution also limits the communication strides you can use to what is was possible with your initial mesh definition. Library writers are also forced to deal with user defined mesh labels, and it can become a headache when doing automated communication pattern optimizations. 2. Use independent shard_maps and pipe the results between each method. Can work, but is fragile. If the partition spec / mesh alignment is not consistent you may unintentionally be inserting additional comms. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Feature Request: A way to redefine mesh specifications within a shard_map,"There are situations where one way of labeling the same mesh can make certain communication patterns easier to code than others. However, there isn't really one mesh labeling strategy that is consistently easy to use across the space of all possible communication patterns. Currently, if a user or library developer wants to develop advanced communication patterns via `shard_map`, they'll be forced to either  a) Force end users to use only a predefined set of axis labels for certain libraries. If these are not consistent between libraries, then you can not use those libraries together. b) User explicitly passes axis labels for all ops. Both library developers and end users are forced to lug around passed axes names throughout their code. It's a large burden, and very fragile to change. This is what I propose instead: A new method `mesh_remap(fn: Callable, mesh: Mesh)` that can be used within any normal `shard_map`ed method. The basic use case looks like this: ```python devices = jax.devices() mesh_2d = Mesh(devices.reshape((16, 16)), ['x', 'y']) mesh_3d = Mesh(devices.reshape((8, 2, 16)), ['a', 'b', 'c']) (mesh_remap, mesh=mesh_3d) def gather_middle(arr):    arr is still the same 2D tile from f.    Nothing has changed. Not the shape nor the device    placement    We gather against the new middle 'b' mesh axis.   res == all_gather(arr, 'b', tiled=True)   assert arr.shape[0] == res.shape[0] // 2   return res (shard_map, mesh=mesh_2D, in_spec=P('x', 'y'), out_spec=P('x', 'y')) def f(arr):    arr is a simple 2D matrix sharded against a simple 2D mesh.   return gather_middle(arr) ``` Here, all we are doing when calling `gather_middle` is redefining how the mesh is labeled. `arr` is not communicated in any way until the `all_gather(...)` call. In this case, since we are gathering on the new `'b'` mesh axis, this all gather definition would not have been possible with the previous `mesh_2d` axes. The same code could potentially be used instead with a `with` block. Since we do not need to define input or output specification, wrapping a function is not required. We are only changing the global state of the mesh, so a with clause should theoretically suffice. ```python (shard_map, mesh=mesh_2D, in_spec=P('x', 'y'), out_spec=P('x', 'y')) def f(arr):   with mesh_remap(mesh_3D):     return all_gather(arr, 'b', tiled=True) ``` Which is best from a user or jaxpr standpoint is something that should likely be discussed. Alternative considerations: 1. Simply use really high dimensional meshes and and use axes tuples (for example: `all_gather(x,  axes=('i', 'j'))`) for larger groupings. In practice this solution has been what we've been doing. The above example could've instead been implemented like this  ```python (shard_map, mesh=mesh_3D, in_spec=P(('a', 'b'), 'c'), out_spec=P(('a', 'b'), 'c')) def f(arr):   return gather_middle(arr) ``` And this works plus or minus a few edge case bugs. This is enough for small benchmarks, but it's limited in flexibility, and you'll be forced to deal with many multiple more axes at any given time than is convenient to deal with. This solution also limits the communication strides you can use to what is was possible with your initial mesh definition. Library writers are also forced to deal with user defined mesh labels, and it can become a headache when doing automated communication pattern optimizations. 2. Use independent shard_maps and pipe the results between each method. Can work, but is fragile. If the partition spec / mesh alignment is not consistent you may unintentionally be inserting additional comms. ",2023-11-15T03:53:52Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/18537
4598,"以下是一个github上的jax下的一个issue, 标题是(Test failures: numerical error in`tests/qdwh_test.py::QdwhTest::testQdwhWithOnRankDeficientInput5`)， 内容是 ( Description A test failure was found when upgrading jaxlib in NixOS This error is hardwaredependent. I can reproduce the error on an intel machine with i513400F, but the error doesn't occur on an AMD platform with Ryzen7 6850HS. The jaxlib binary is CPU only with MKL enabled at compile time. The two machines shared the same jaxlib binary, which implies that the bug depends on the underlying hardware.  The output of the failed test ```python __________________ QdwhTest.testQdwhWithOnRankDeficientInput5 __________________ [gw3] linux  Python 3.11.5 /nix/store/ffll6glz3gwx342z0ch8wx30p5cnqz1zpython33.11.5/bin/python3.11 self = , m = 10 n = 10, log_cond = 3.0     .sample_product(       [dict(m=m, n=n) for m, n in [(10, 10), (8, 8)]],       log_cond=np.linspace(1, 4, 4),     )     def testQdwhWithOnRankDeficientInput(self, m, n, log_cond):       """"""Tests qdwh with rankdeficient input.""""""       a = np.triu(np.ones((m, n))).astype(_QDWH_TEST_DTYPE)        Generates a rankdeficient input.       u, s, v = np.linalg.svd(a, full_matrices=False)       cond = 10**log_cond       s = jnp.linspace(cond, 1, min(m, n))       s = jnp.expand_dims(s.at[1].set(0), range(u.ndim  1))       a = (u * s) @ v       is_hermitian = _check_symmetry(a)       max_iterations = 15       actual_u, actual_h, _, _ = qdwh.qdwh(a, is_hermitian=is_hermitian,                                            max_iterations=max_iterations)       _, expected_h = osp_linalg.polar(a)        For rankdeficient matrix, `u` is not unique.       with self.subTest('Test h.'):         relative_diff_h = _compute_relative_diff(actual_h, expected_h)         np.testing.assert_almost_equal(relative_diff_h, 1E6, decimal=5)       with self.subTest('Test u.dot(h).'):         a_round_trip = _dot(actual_u, actual_h)         relative_diff_a = _compute_relative_diff(a_round_trip, a)         np.testing.assert_almost_equal(relative_diff_a, 1E6, decimal=5)       with self.subTest('Test orthogonality.'):         actual_results = _dot(actual_u.T.conj(), actual_u)         expected_results = np.eye(n) >       self.assertAllClose(             actual_results, expected_results, rtol=_QDWH_TEST_EPS, atol=1e6         ) tests/qdwh_test.py:202:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:1013: in assertAllClose     self.assertArraysAllClose(x, y, check_dtypes=False, atol=atol, rtol=rtol, jax/_src/test_util.py:978: in assertArraysAllClose     _assert_numpy_allclose(x, y, atol=atol, rtol=rtol, err_msg=err_msg) jax/_src/public_test_util.py:121: in _assert_numpy_allclose     np.testing.assert_allclose(a, b, **kw, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (.compare at 0x7ffe0d589580>, array([[ 0.983452  ,  0.03162566, 0.04389316,  0.0522...., 0., 1., 0., 0.],        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])) kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1.19209e07, atol=1e06', 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           AssertionError:  E           Not equal to tolerance rtol=1.19209e07, atol=1e06 E            E           Mismatched elements: 100 / 100 (100%) E           Max absolute difference: 0.18940455 E           Max relative difference: 0.18940455 E            x: array([[ 0.983452,  0.031626, 0.043893,  0.052261, 0.055985,  0.054734, E                   0.04862 ,  0.038186, 0.024359,  0.008367], E                  [ 0.031626,  0.939559,  0.083886, 0.099878,  0.106995, 0.104604,... E            y: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], E                  [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], E                  [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],... /nix/store/ffll6glz3gwx342z0ch8wx30p5cnqz1zpython33.11.5/lib/python3.11/contextlib.py:81: AssertionError =========================== short test summary info ============================ FAILED tests/qdwh_test.py::QdwhTest::testQdwhWithOnRankDeficientInput5  AssertionError: ```   What jax/jaxlib version are you using? v0.4.20 (refs/tags/jaxv0.4.20)  Which accelerator(s) are you using? CPU  Additional system info Python 3.11 NixOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Test failures: numerical error in`tests/qdwh_test.py::QdwhTest::testQdwhWithOnRankDeficientInput5`," Description A test failure was found when upgrading jaxlib in NixOS This error is hardwaredependent. I can reproduce the error on an intel machine with i513400F, but the error doesn't occur on an AMD platform with Ryzen7 6850HS. The jaxlib binary is CPU only with MKL enabled at compile time. The two machines shared the same jaxlib binary, which implies that the bug depends on the underlying hardware.  The output of the failed test ```python __________________ QdwhTest.testQdwhWithOnRankDeficientInput5 __________________ [gw3] linux  Python 3.11.5 /nix/store/ffll6glz3gwx342z0ch8wx30p5cnqz1zpython33.11.5/bin/python3.11 self = , m = 10 n = 10, log_cond = 3.0     .sample_product(       [dict(m=m, n=n) for m, n in [(10, 10), (8, 8)]],       log_cond=np.linspace(1, 4, 4),     )     def testQdwhWithOnRankDeficientInput(self, m, n, log_cond):       """"""Tests qdwh with rankdeficient input.""""""       a = np.triu(np.ones((m, n))).astype(_QDWH_TEST_DTYPE)        Generates a rankdeficient input.       u, s, v = np.linalg.svd(a, full_matrices=False)       cond = 10**log_cond       s = jnp.linspace(cond, 1, min(m, n))       s = jnp.expand_dims(s.at[1].set(0), range(u.ndim  1))       a = (u * s) @ v       is_hermitian = _check_symmetry(a)       max_iterations = 15       actual_u, actual_h, _, _ = qdwh.qdwh(a, is_hermitian=is_hermitian,                                            max_iterations=max_iterations)       _, expected_h = osp_linalg.polar(a)        For rankdeficient matrix, `u` is not unique.       with self.subTest('Test h.'):         relative_diff_h = _compute_relative_diff(actual_h, expected_h)         np.testing.assert_almost_equal(relative_diff_h, 1E6, decimal=5)       with self.subTest('Test u.dot(h).'):         a_round_trip = _dot(actual_u, actual_h)         relative_diff_a = _compute_relative_diff(a_round_trip, a)         np.testing.assert_almost_equal(relative_diff_a, 1E6, decimal=5)       with self.subTest('Test orthogonality.'):         actual_results = _dot(actual_u.T.conj(), actual_u)         expected_results = np.eye(n) >       self.assertAllClose(             actual_results, expected_results, rtol=_QDWH_TEST_EPS, atol=1e6         ) tests/qdwh_test.py:202:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:1013: in assertAllClose     self.assertArraysAllClose(x, y, check_dtypes=False, atol=atol, rtol=rtol, jax/_src/test_util.py:978: in assertArraysAllClose     _assert_numpy_allclose(x, y, atol=atol, rtol=rtol, err_msg=err_msg) jax/_src/public_test_util.py:121: in _assert_numpy_allclose     np.testing.assert_allclose(a, b, **kw, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (.compare at 0x7ffe0d589580>, array([[ 0.983452  ,  0.03162566, 0.04389316,  0.0522...., 0., 1., 0., 0.],        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])) kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1.19209e07, atol=1e06', 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           AssertionError:  E           Not equal to tolerance rtol=1.19209e07, atol=1e06 E            E           Mismatched elements: 100 / 100 (100%) E           Max absolute difference: 0.18940455 E           Max relative difference: 0.18940455 E            x: array([[ 0.983452,  0.031626, 0.043893,  0.052261, 0.055985,  0.054734, E                   0.04862 ,  0.038186, 0.024359,  0.008367], E                  [ 0.031626,  0.939559,  0.083886, 0.099878,  0.106995, 0.104604,... E            y: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], E                  [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], E                  [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],... /nix/store/ffll6glz3gwx342z0ch8wx30p5cnqz1zpython33.11.5/lib/python3.11/contextlib.py:81: AssertionError =========================== short test summary info ============================ FAILED tests/qdwh_test.py::QdwhTest::testQdwhWithOnRankDeficientInput5  AssertionError: ```   What jax/jaxlib version are you using? v0.4.20 (refs/tags/jaxv0.4.20)  Which accelerator(s) are you using? CPU  Additional system info Python 3.11 NixOS  NVIDIA GPU info _No response_",2023-11-15T02:42:50Z,bug contributions welcome needs info CPU,open,0,8,https://github.com/jax-ml/jax/issues/18535,Does this reproduce with an upstream `pip` build of jax/jaxlib?,> Does this reproduce with an upstream pip build of jax/jaxlib?  (or others) This could be tested relatively easily by building `python3Packages.jax.override { jaxlib = jaxlibbin; }` or something thereabouts since `python3Packages.jaxlibbin` is pulled directly from PyPI.,"Yes, I can reproduce this problem using `pip` build jax/jaxlib on that same machine (the i513400F one). The test is conducted in an archlinux container with a fresh venv environment and `pip install ""jax[cpu]""`, and follow the test using pytest instruction. And it gives the exact same result:   pytest output  ```python self = , m = 10, n = 10, log_cond = 3.0     .sample_product(       [dict(m=m, n=n) for m, n in [(10, 10), (8, 8)]],       log_cond=np.linspace(1, 4, 4),     )     def testQdwhWithOnRankDeficientInput(self, m, n, log_cond):       """"""Tests qdwh with rankdeficient input.""""""       a = np.triu(np.ones((m, n))).astype(_QDWH_TEST_DTYPE)        Generates a rankdeficient input.       u, s, v = np.linalg.svd(a, full_matrices=False)       cond = 10**log_cond       s = jnp.linspace(cond, 1, min(m, n))       s = jnp.expand_dims(s.at[1].set(0), range(u.ndim  1))       a = (u * s) @ v       is_hermitian = _check_symmetry(a)       max_iterations = 15       actual_u, actual_h, _, _ = qdwh.qdwh(a, is_hermitian=is_hermitian,                                            max_iterations=max_iterations)       _, expected_h = osp_linalg.polar(a)        For rankdeficient matrix, `u` is not unique.       with self.subTest('Test h.'):         relative_diff_h = _compute_relative_diff(actual_h, expected_h)         np.testing.assert_almost_equal(relative_diff_h, 1E6, decimal=5)       with self.subTest('Test u.dot(h).'):         a_round_trip = _dot(actual_u, actual_h)         relative_diff_a = _compute_relative_diff(a_round_trip, a)         np.testing.assert_almost_equal(relative_diff_a, 1E6, decimal=5)       with self.subTest('Test orthogonality.'):         actual_results = _dot(actual_u.T.conj(), actual_u)         expected_results = np.eye(n) >       self.assertAllClose(             actual_results, expected_results, rtol=_QDWH_TEST_EPS, atol=1e6         ) tests/qdwh_test.py:202:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:1013: in assertAllClose     self.assertArraysAllClose(x, y, check_dtypes=False, atol=atol, rtol=rtol, jax/_src/test_util.py:978: in assertArraysAllClose     _assert_numpy_allclose(x, y, atol=atol, rtol=rtol, err_msg=err_msg) jax/_src/public_test_util.py:121: in _assert_numpy_allclose     np.testing.assert_allclose(a, b, **kw, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (.compare at 0x7f51ae846b60>, array([[ 0.983452  ,  0.03162566, 0.04389316,  0.0522...., 0., 1., 0., 0.],        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])) kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1.19209e07, atol=1e06', 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           AssertionError:  E           Not equal to tolerance rtol=1.19209e07, atol=1e06 E            E           Mismatched elements: 100 / 100 (100%) E           Max absolute difference: 0.18940455 E           Max relative difference: 0.18940455 E            x: array([[ 0.983452,  0.031626, 0.043893,  0.052261, 0.055985,  0.054734, E                   0.04862 ,  0.038186, 0.024359,  0.008367], E                  [ 0.031626,  0.939559,  0.083886, 0.099878,  0.106995, 0.104604,... E            y: array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], E                  [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], E                  [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],... /usr/lib/python3.11/contextlib.py:81: AssertionError =================================================================================== short test summary info ==================================================================================== FAILED tests/qdwh_test.py::QdwhTest::testQdwhWithOnRankDeficientInput5  AssertionError: ```   pip show jax jaxlib ``` Name: jax Version: 0.4.20 Summary: Differentiate, compile, and transform Numpy code. Homepage: https://github.com/google/jax Author: JAX team Authoremail: jaxdev.com License: Apache2.0 Location: /home//venv/lib/python3.11/sitepackages Requires: mldtypes, numpy, numpy, opteinsum, scipy Requiredby:   Name: jaxlib Version: 0.4.20 Summary: XLA library for JAX Homepage: https://github.com/google/jax Author: JAX team Authoremail: jaxdev.com License: Apache2.0 Location: /home//venv/lib/python3.11/sitepackages Requires: mldtypes, numpy, scipy Requiredby:  ``` ","Do me another favor: use upstream `scipy` in this test as well and disable MKL or similar if you're using it? Some of the linear algebra routines used by JAX come from `scipy`, and in turn they might be provided by MKL if you have MKLenabled scipy installed.","Sorry for the late reply. The second test was already run with the upstream version of scipy, jax, jaxlib, and no intel MKL involved. 1. > A test failure was found when [upgrading jaxlib in NixOS]    Here I used the NixOS's build, jax/jaxlib/scipy are built through Nix, and MKL is enabled. 3. > The test is conducted in an archlinux container with a fresh venv environment and pip install ""jax[cpu]"", and follow the test using pytest instruction.     Here everything were from upstream. I started from a clean archlinux container with only python/pip, and no MKL installed, then I created a new python venv, and `pip install U ""jax[cpu]""`, `pip install r build/testrequirements.txt`. So it should be a pretty clean environment. Here is everything in that venv, listed by `pip list`, and all from upstream. ``` Package          Version   abslpy          2.0.0 attrs            23.1.0 build            1.0.3 cloudpickle      3.0.0 colorama         0.4.6 execnet          2.0.2 hypothesis       6.88.4 iniconfig        2.0.0 jax              0.4.20 jaxlib           0.4.20 markdownitpy   3.0.0 mdurl            0.1.2 mldtypes        0.3.1 numpy            1.26.2 opteinsum       3.3.0 packaging        23.2 Pillow           10.1.0 pip              23.2.1 pluggy           1.3.0 portpicker       1.6.0 psutil           5.9.6 Pygments         2.16.1 pyproject_hooks  1.0.0 pytest           7.4.3 pytestxdist     3.4.0 rich             13.6.0 scipy            1.11.3 setuptools       65.5.0 sortedcontainers 2.4.0 wheel            0.41.3 ```","Hmm. I can't reproduce this. I used Ubuntu 22.04, Python 3.11 (from the deadsnakes PPA), and I tried: * a GCP c3 VM (Intel Sapphire Rapids) * a GCP n2 VM (Intel Cascade Lake) * a GCP n1 VM (Intel Skylake) At this point, I'm stuck. If I can't reproduce it, I can't debug it. I don't have access to that particular desktop Intel chip. Does it reproduce with Nix on a Cloud VM of some kind?","If you shared the output of `lscpu` on that machine, it might give me a clue.","``` Architecture:            x86_64   CPU opmode(s):        32bit, 64bit   Address sizes:         39 bits physical, 48 bits virtual   Byte Order:            Little Endian CPU(s):                  16   Online CPU(s) list:   015 Vendor ID:               GenuineIntel   Model name:            13th Gen Intel(R) Core(TM) i513400F     CPU family:          6     Model:               191     Thread(s) per core:  2     Core(s) per socket:  10     Socket(s):           1     Stepping:            2     CPU(s) scaling MHz:  19%     CPU max MHz:         4600.0000     CPU min MHz:         800.0000     BogoMIPS:            4992.00     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts                           rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_dead                          line_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep                           bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp h                          wp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr ibt flush_l1d arch_capabilities Virtualization features:    Virtualization:        VTx Caches (sum of all):        L1d:                   416 KiB (10 instances)   L1i:                   448 KiB (10 instances)   L2:                    9.5 MiB (7 instances)   L3:                    20 MiB (1 instance) NUMA:                       NUMA node(s):          1   NUMA node0 CPU(s):     015 Vulnerabilities:            Gather data sampling:  Not affected   Itlb multihit:         Not affected   L1tf:                  Not affected   Mds:                   Not affected   Meltdown:              Not affected   Mmio stale data:       Not affected   Retbleed:              Not affected   Spec rstack overflow:  Not affected   Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl   Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization   Spectre v2:            Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSBeIBRS SW sequence   Srbds:                 Not affected   Tsx async abort:       Not affected ``` I hope it helps."
1415,"以下是一个github上的jax下的一个issue, 标题是(Avoid querying `named_shape` to support `DShapedArray`s.)， 内容是 (Hi, We are using JAX to compile Python to MLIR. We are currently exploring the implementation of dynamic tensor sizes support in JAX (i.e., `abstracted_axes`). I noticed that when using the option calling `make_jaxpr` with `abstracted_axes=something` and `return_shapes=True` an exception is raised. This is because `DShapedArray`s does not have a `named_shape` attribute which is needed in these lines of code: ```python 2447   (fun) 2448    2449   def make_jaxpr_f(*args, **kwargs):       ... snip ... 2465     if return_shape: 2466       out_avals, _ = unzip2(out_type) 2467       out_shapes_flat = [ 2468           ShapeDtypeStruct(a.shape, a.dtype, a.named_shape) for a in out_avals]  < Here 2469       return closed_jaxpr, tree_unflatten(out_tree(), out_shapes_flat) 2470     return closed_jaxpr ``` I am not too familiar with Dynamic shapes in JAX yet, but implementing the changes allowed me to return a meaningful shape with the following value: ``` ShapeDtypeStruct(shape=(InDBIdx(val=0),), dtype=int64) ``` which appears to be sufficient. Happy to hear your thoughts here. I would really like to be able to use `abstracted_axes` and `return_shapes=True`. Thank you! :))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Avoid querying `named_shape` to support `DShapedArray`s.,"Hi, We are using JAX to compile Python to MLIR. We are currently exploring the implementation of dynamic tensor sizes support in JAX (i.e., `abstracted_axes`). I noticed that when using the option calling `make_jaxpr` with `abstracted_axes=something` and `return_shapes=True` an exception is raised. This is because `DShapedArray`s does not have a `named_shape` attribute which is needed in these lines of code: ```python 2447   (fun) 2448    2449   def make_jaxpr_f(*args, **kwargs):       ... snip ... 2465     if return_shape: 2466       out_avals, _ = unzip2(out_type) 2467       out_shapes_flat = [ 2468           ShapeDtypeStruct(a.shape, a.dtype, a.named_shape) for a in out_avals]  < Here 2469       return closed_jaxpr, tree_unflatten(out_tree(), out_shapes_flat) 2470     return closed_jaxpr ``` I am not too familiar with Dynamic shapes in JAX yet, but implementing the changes allowed me to return a meaningful shape with the following value: ``` ShapeDtypeStruct(shape=(InDBIdx(val=0),), dtype=int64) ``` which appears to be sufficient. Happy to hear your thoughts here. I would really like to be able to use `abstracted_axes` and `return_shapes=True`. Thank you! :)",2023-11-13T21:10:56Z,,closed,0,3,https://github.com/jax-ml/jax/issues/18505,"Thanks for the suggestion! Actually, `named_shape` is kind of dead at the moment, so instead of adding it to a new place, I'd rather edit any place that expects the `named_shape` attribute so as not to expect it. Does that make sense?","Hi , Thanks for your feedback. There are several ways to fix this, I took the least invasive approach here, just modifying the `make_jaxpr_f` function to not assume that `out_avals` have the `shaped_name` attribute. Would you like me to go through all the code and change `${var}.named_shape` to a function that checks if `named_shape` is present or not? For the time being, the only change that is impeding our project is this line (and another PR I'll open to address the `get_aval` CC(If parameter to `get_aval` is AbstractValue return it.) from the caller of `get_aval`). Thanks!",I'm closing as this has already been implemented.
8419,"以下是一个github上的jax下的一个issue, 标题是(Use a Jacobi SVD solver for unbatched SVDs up to 1024x1024 on NVIDIA GPUs.)， 内容是 (Use a Jacobi SVD solver for unbatched SVDs up to 1024x1024 on NVIDIA GPUs. The unbatched Jacobi solver is faster for smallmoderate matrices, and the unbatched kernel doesn't have size restrictions. Timings on T4 GPU: Before:  Benchmark                  Time             CPU   Iterations  svd/m:1/n:1           263587 ns       242274 ns         2780 svd/m:2/n:1           335561 ns       298238 ns         2303 svd/m:5/n:1           337784 ns       299841 ns         2304 svd/m:10/n:1          339184 ns       300703 ns         2311 svd/m:100/n:1         359826 ns       320088 ns         2159 svd/m:500/n:1         376124 ns       338660 ns         2076 svd/m:800/n:1         375779 ns       335590 ns         2060 svd/m:1000/n:1        419171 ns       341487 ns         2072 svd/m:1/n:2           307564 ns       270663 ns         2544 svd/m:2/n:2           320928 ns       283601 ns         2487 svd/m:5/n:2           377373 ns       344228 ns         2035 svd/m:10/n:2          380557 ns       349412 ns         1953 svd/m:100/n:2         435465 ns       403496 ns         1722 svd/m:500/n:2         444610 ns       410913 ns         1680 svd/m:800/n:2         454493 ns       416495 ns         1665 svd/m:1000/n:2        492110 ns       420539 ns         1665 svd/m:1/n:5           307316 ns       275833 ns         2531 svd/m:2/n:5           374318 ns       341432 ns         2086 svd/m:5/n:5           512928 ns       470293 ns         1361 svd/m:10/n:5          589330 ns       537070 ns         1353 svd/m:100/n:5         620164 ns       580166 ns         1193 svd/m:500/n:5         636424 ns       593692 ns         1180 svd/m:800/n:5         635545 ns       595016 ns         1181 svd/m:1000/n:5        672443 ns       597387 ns         1115 svd/m:1/n:10          310013 ns       273998 ns         2520 svd/m:2/n:10          370451 ns       334489 ns         2105 svd/m:5/n:10          560037 ns       522223 ns         1274 svd/m:10/n:10         572868 ns       535388 ns         1304 svd/m:100/n:10        959802 ns       918258 ns          765 svd/m:500/n:10        955958 ns       909778 ns          758 svd/m:800/n:10        924104 ns       879512 ns          777 svd/m:1000/n:10       950140 ns       883493 ns          775 svd/m:1/n:100         351237 ns       315554 ns         2198 svd/m:2/n:100         426883 ns       390089 ns         1792 svd/m:5/n:100         601557 ns       564493 ns         1255 svd/m:10/n:100        920819 ns       880011 ns          787 svd/m:100/n:100      7902281 ns      7229220 ns           95 svd/m:500/n:100      9720727 ns      9040679 ns           79 svd/m:800/n:100      9856378 ns      8998050 ns           79 svd/m:1000/n:100     9721017 ns      9086414 ns           79 svd/m:1/n:500         371171 ns       334217 ns         2117 svd/m:2/n:500         449165 ns       411499 ns         1700 svd/m:5/n:500         620354 ns       581866 ns         1185 svd/m:10/n:500        892375 ns       847239 ns          833 svd/m:100/n:500      9564810 ns      8867540 ns           79 svd/m:500/n:500    111924035 ns    104078023 ns            7 svd/m:800/n:500    147777319 ns    142730412 ns            5 svd/m:1000/n:500   154205084 ns    149740209 ns            5 svd/m:1/n:800         372122 ns       334212 ns         2119 svd/m:2/n:800         456672 ns       419260 ns         1680 svd/m:5/n:800         691208 ns       626003 ns         1190 svd/m:10/n:800       1017694 ns       941480 ns          730 svd/m:100/n:800      9892683 ns      9091043 ns           76 svd/m:500/n:800    144134235 ns    139129722 ns            5 svd/m:800/n:800    342790246 ns    333299774 ns            2 svd/m:1000/n:800   432820082 ns    427978978 ns            2 svd/m:1/n:1000        372785 ns       335745 ns         1805 svd/m:2/n:1000        451946 ns       413341 ns         1668 svd/m:5/n:1000        618475 ns       577213 ns         1169 svd/m:10/n:1000       907729 ns       863335 ns          808 svd/m:100/n:1000     9868543 ns      9116870 ns           76 svd/m:500/n:1000   156777811 ns    152042065 ns            5 svd/m:800/n:1000   429704070 ns    424677592 ns            2 svd/m:1000/n:1000  654864311 ns    642693162 ns            1 After:  Benchmark                  Time             CPU   Iterations  svd/m:1/n:1           265980 ns       245433 ns         2791 svd/m:2/n:1           340203 ns       302783 ns         2288 svd/m:5/n:1           337807 ns       301916 ns         2286 svd/m:10/n:1          338064 ns       302441 ns         2297 svd/m:100/n:1         335444 ns       298440 ns         2327 svd/m:500/n:1         338025 ns       302096 ns         2272 svd/m:800/n:1         328382 ns       291740 ns         2252 svd/m:1000/n:1        397494 ns       310905 ns         2239 svd/m:1/n:2           310464 ns       274507 ns         2535 svd/m:2/n:2           319999 ns       284247 ns         2515 svd/m:5/n:2           373435 ns       335919 ns         2069 svd/m:10/n:2          376327 ns       339327 ns         2056 svd/m:100/n:2         385061 ns       349258 ns         2003 svd/m:500/n:2         392352 ns       355735 ns         1932 svd/m:800/n:2         410736 ns       370677 ns         1881 svd/m:1000/n:2        494326 ns       405603 ns         1721 svd/m:1/n:5           316735 ns       277292 ns         2538 svd/m:2/n:5           383748 ns       342218 ns         2077 svd/m:5/n:5           494204 ns       454309 ns         1476 svd/m:10/n:5          547017 ns       508184 ns         1371 svd/m:100/n:5         514537 ns       476761 ns         1460 svd/m:500/n:5         544656 ns       504877 ns         1381 svd/m:800/n:5         642590 ns       599314 ns         1159 svd/m:1000/n:5        706166 ns       621209 ns         1106 svd/m:1/n:10          310825 ns       274374 ns         2511 svd/m:2/n:10          381316 ns       344202 ns         2094 svd/m:5/n:10          565469 ns       526759 ns         1266 svd/m:10/n:10         576111 ns       537286 ns         1299 svd/m:100/n:10        653250 ns       613392 ns         1137 svd/m:500/n:10        690532 ns       645828 ns         1080 svd/m:800/n:10        763924 ns       723677 ns          959 svd/m:1000/n:10       940342 ns       855517 ns          818 svd/m:1/n:100         306134 ns       271533 ns         2526 svd/m:2/n:100         374680 ns       339298 ns         2071 svd/m:5/n:100         576926 ns       539062 ns         1228 svd/m:10/n:100        656806 ns       615171 ns         1123 svd/m:100/n:100      3295164 ns      3138621 ns          223 svd/m:500/n:100      4269347 ns      4166000 ns          168 svd/m:800/n:100      4656541 ns      4522247 ns          154 svd/m:1000/n:100     6479223 ns      6354578 ns          112 svd/m:1/n:500         329966 ns       289083 ns         2440 svd/m:2/n:500         407535 ns       366794 ns         1947 svd/m:5/n:500         567367 ns       522809 ns         1336 svd/m:10/n:500        712307 ns       657608 ns         1065 svd/m:100/n:500      4262986 ns      4169907 ns          167 svd/m:500/n:500     28824720 ns     28650258 ns           25 svd/m:800/n:500     29330139 ns     28677269 ns           25 svd/m:1000/n:500    30848037 ns     30089216 ns           23 svd/m:1/n:800         328620 ns       289181 ns         2329 svd/m:2/n:800         419052 ns       379483 ns         1876 svd/m:5/n:800         587366 ns       546979 ns         1269 svd/m:10/n:800        830762 ns       787923 ns          893 svd/m:100/n:800      4763633 ns      4595738 ns          152 svd/m:500/n:800     30447861 ns     29949714 ns           24 svd/m:800/n:800     94188958 ns     93488372 ns            8 svd/m:1000/n:800    94701529 ns     93394677 ns            7 svd/m:1/n:1000        351102 ns       313099 ns         2218 svd/m:2/n:1000        446543 ns       407807 ns         1708 svd/m:5/n:1000        661152 ns       616174 ns         1129 svd/m:10/n:1000       915743 ns       873397 ns          802 svd/m:100/n:1000     6434730 ns      6282779 ns          113 svd/m:500/n:1000    30244321 ns     29684290 ns           24 svd/m:800/n:1000    92727423 ns     91477078 ns            8 svd/m:1000/n:1000  169500709 ns    168358420 ns            4)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Use a Jacobi SVD solver for unbatched SVDs up to 1024x1024 on NVIDIA GPUs.,"Use a Jacobi SVD solver for unbatched SVDs up to 1024x1024 on NVIDIA GPUs. The unbatched Jacobi solver is faster for smallmoderate matrices, and the unbatched kernel doesn't have size restrictions. Timings on T4 GPU: Before:  Benchmark                  Time             CPU   Iterations  svd/m:1/n:1           263587 ns       242274 ns         2780 svd/m:2/n:1           335561 ns       298238 ns         2303 svd/m:5/n:1           337784 ns       299841 ns         2304 svd/m:10/n:1          339184 ns       300703 ns         2311 svd/m:100/n:1         359826 ns       320088 ns         2159 svd/m:500/n:1         376124 ns       338660 ns         2076 svd/m:800/n:1         375779 ns       335590 ns         2060 svd/m:1000/n:1        419171 ns       341487 ns         2072 svd/m:1/n:2           307564 ns       270663 ns         2544 svd/m:2/n:2           320928 ns       283601 ns         2487 svd/m:5/n:2           377373 ns       344228 ns         2035 svd/m:10/n:2          380557 ns       349412 ns         1953 svd/m:100/n:2         435465 ns       403496 ns         1722 svd/m:500/n:2         444610 ns       410913 ns         1680 svd/m:800/n:2         454493 ns       416495 ns         1665 svd/m:1000/n:2        492110 ns       420539 ns         1665 svd/m:1/n:5           307316 ns       275833 ns         2531 svd/m:2/n:5           374318 ns       341432 ns         2086 svd/m:5/n:5           512928 ns       470293 ns         1361 svd/m:10/n:5          589330 ns       537070 ns         1353 svd/m:100/n:5         620164 ns       580166 ns         1193 svd/m:500/n:5         636424 ns       593692 ns         1180 svd/m:800/n:5         635545 ns       595016 ns         1181 svd/m:1000/n:5        672443 ns       597387 ns         1115 svd/m:1/n:10          310013 ns       273998 ns         2520 svd/m:2/n:10          370451 ns       334489 ns         2105 svd/m:5/n:10          560037 ns       522223 ns         1274 svd/m:10/n:10         572868 ns       535388 ns         1304 svd/m:100/n:10        959802 ns       918258 ns          765 svd/m:500/n:10        955958 ns       909778 ns          758 svd/m:800/n:10        924104 ns       879512 ns          777 svd/m:1000/n:10       950140 ns       883493 ns          775 svd/m:1/n:100         351237 ns       315554 ns         2198 svd/m:2/n:100         426883 ns       390089 ns         1792 svd/m:5/n:100         601557 ns       564493 ns         1255 svd/m:10/n:100        920819 ns       880011 ns          787 svd/m:100/n:100      7902281 ns      7229220 ns           95 svd/m:500/n:100      9720727 ns      9040679 ns           79 svd/m:800/n:100      9856378 ns      8998050 ns           79 svd/m:1000/n:100     9721017 ns      9086414 ns           79 svd/m:1/n:500         371171 ns       334217 ns         2117 svd/m:2/n:500         449165 ns       411499 ns         1700 svd/m:5/n:500         620354 ns       581866 ns         1185 svd/m:10/n:500        892375 ns       847239 ns          833 svd/m:100/n:500      9564810 ns      8867540 ns           79 svd/m:500/n:500    111924035 ns    104078023 ns            7 svd/m:800/n:500    147777319 ns    142730412 ns            5 svd/m:1000/n:500   154205084 ns    149740209 ns            5 svd/m:1/n:800         372122 ns       334212 ns         2119 svd/m:2/n:800         456672 ns       419260 ns         1680 svd/m:5/n:800         691208 ns       626003 ns         1190 svd/m:10/n:800       1017694 ns       941480 ns          730 svd/m:100/n:800      9892683 ns      9091043 ns           76 svd/m:500/n:800    144134235 ns    139129722 ns            5 svd/m:800/n:800    342790246 ns    333299774 ns            2 svd/m:1000/n:800   432820082 ns    427978978 ns            2 svd/m:1/n:1000        372785 ns       335745 ns         1805 svd/m:2/n:1000        451946 ns       413341 ns         1668 svd/m:5/n:1000        618475 ns       577213 ns         1169 svd/m:10/n:1000       907729 ns       863335 ns          808 svd/m:100/n:1000     9868543 ns      9116870 ns           76 svd/m:500/n:1000   156777811 ns    152042065 ns            5 svd/m:800/n:1000   429704070 ns    424677592 ns            2 svd/m:1000/n:1000  654864311 ns    642693162 ns            1 After:  Benchmark                  Time             CPU   Iterations  svd/m:1/n:1           265980 ns       245433 ns         2791 svd/m:2/n:1           340203 ns       302783 ns         2288 svd/m:5/n:1           337807 ns       301916 ns         2286 svd/m:10/n:1          338064 ns       302441 ns         2297 svd/m:100/n:1         335444 ns       298440 ns         2327 svd/m:500/n:1         338025 ns       302096 ns         2272 svd/m:800/n:1         328382 ns       291740 ns         2252 svd/m:1000/n:1        397494 ns       310905 ns         2239 svd/m:1/n:2           310464 ns       274507 ns         2535 svd/m:2/n:2           319999 ns       284247 ns         2515 svd/m:5/n:2           373435 ns       335919 ns         2069 svd/m:10/n:2          376327 ns       339327 ns         2056 svd/m:100/n:2         385061 ns       349258 ns         2003 svd/m:500/n:2         392352 ns       355735 ns         1932 svd/m:800/n:2         410736 ns       370677 ns         1881 svd/m:1000/n:2        494326 ns       405603 ns         1721 svd/m:1/n:5           316735 ns       277292 ns         2538 svd/m:2/n:5           383748 ns       342218 ns         2077 svd/m:5/n:5           494204 ns       454309 ns         1476 svd/m:10/n:5          547017 ns       508184 ns         1371 svd/m:100/n:5         514537 ns       476761 ns         1460 svd/m:500/n:5         544656 ns       504877 ns         1381 svd/m:800/n:5         642590 ns       599314 ns         1159 svd/m:1000/n:5        706166 ns       621209 ns         1106 svd/m:1/n:10          310825 ns       274374 ns         2511 svd/m:2/n:10          381316 ns       344202 ns         2094 svd/m:5/n:10          565469 ns       526759 ns         1266 svd/m:10/n:10         576111 ns       537286 ns         1299 svd/m:100/n:10        653250 ns       613392 ns         1137 svd/m:500/n:10        690532 ns       645828 ns         1080 svd/m:800/n:10        763924 ns       723677 ns          959 svd/m:1000/n:10       940342 ns       855517 ns          818 svd/m:1/n:100         306134 ns       271533 ns         2526 svd/m:2/n:100         374680 ns       339298 ns         2071 svd/m:5/n:100         576926 ns       539062 ns         1228 svd/m:10/n:100        656806 ns       615171 ns         1123 svd/m:100/n:100      3295164 ns      3138621 ns          223 svd/m:500/n:100      4269347 ns      4166000 ns          168 svd/m:800/n:100      4656541 ns      4522247 ns          154 svd/m:1000/n:100     6479223 ns      6354578 ns          112 svd/m:1/n:500         329966 ns       289083 ns         2440 svd/m:2/n:500         407535 ns       366794 ns         1947 svd/m:5/n:500         567367 ns       522809 ns         1336 svd/m:10/n:500        712307 ns       657608 ns         1065 svd/m:100/n:500      4262986 ns      4169907 ns          167 svd/m:500/n:500     28824720 ns     28650258 ns           25 svd/m:800/n:500     29330139 ns     28677269 ns           25 svd/m:1000/n:500    30848037 ns     30089216 ns           23 svd/m:1/n:800         328620 ns       289181 ns         2329 svd/m:2/n:800         419052 ns       379483 ns         1876 svd/m:5/n:800         587366 ns       546979 ns         1269 svd/m:10/n:800        830762 ns       787923 ns          893 svd/m:100/n:800      4763633 ns      4595738 ns          152 svd/m:500/n:800     30447861 ns     29949714 ns           24 svd/m:800/n:800     94188958 ns     93488372 ns            8 svd/m:1000/n:800    94701529 ns     93394677 ns            7 svd/m:1/n:1000        351102 ns       313099 ns         2218 svd/m:2/n:1000        446543 ns       407807 ns         1708 svd/m:5/n:1000        661152 ns       616174 ns         1129 svd/m:10/n:1000       915743 ns       873397 ns          802 svd/m:100/n:1000     6434730 ns      6282779 ns          113 svd/m:500/n:1000    30244321 ns     29684290 ns           24 svd/m:800/n:1000    92727423 ns     91477078 ns            8 svd/m:1000/n:1000  169500709 ns    168358420 ns            4",2023-11-13T19:01:16Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18503
786,"以下是一个github上的jax下的一个issue, 标题是(jax.eval_shape silence the typing annotations)， 内容是 (`jax.eval_shape` is badly annotated, so users loose typechecking and autocomplete on returned input. Current anntations: ```python def eval_shape(fun: Callable, *args, **kwargs) ``` Should be instead: ```python P = ParamSpec('P') OutT = TypeVar('OutT') def eval_shape(fun: Callable[P, OutT], *args: P.args, **kwargs: P.kwargs) > OutT: ``` The `ParamSpec` is less important, but loosing the type checking on the function output is quite annoying as there's no more autocomplete on the output: ``` x = jax.eval_shape(my_fn) x.attr   < No autocomplete here :'( ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.eval_shape silence the typing annotations,"`jax.eval_shape` is badly annotated, so users loose typechecking and autocomplete on returned input. Current anntations: ```python def eval_shape(fun: Callable, *args, **kwargs) ``` Should be instead: ```python P = ParamSpec('P') OutT = TypeVar('OutT') def eval_shape(fun: Callable[P, OutT], *args: P.args, **kwargs: P.kwargs) > OutT: ``` The `ParamSpec` is less important, but loosing the type checking on the function output is quite annoying as there's no more autocomplete on the output: ``` x = jax.eval_shape(my_fn) x.attr   < No autocomplete here :'( ```",2023-11-13T18:43:02Z,enhancement,closed,0,7,https://github.com/jax-ml/jax/issues/18502,"Thanks! I think this annotation predates the availability of `typing.ParamSpec`. However, there's a bit of a problem here: `eval_shape` does not return the same type as the input function, rather it returns the same pytree structure, with all array & scalar entries replaced by `ShapeDtypeStruct`. I'm not sure that Python type annotations are expressive enough to actually annotate that behavior. What do you think?","Thank you for the answer. I agree that python annotations are not expressive enough, however this seems a case where practicality should beat purity. ```python .struct.dataclass class Batch:   img: Float['h w c']   label: Float['h w c'] def fn() > Batch:   ... type(fn()) is Batch type(jax.eval_shape(fn)) is Batch ``` I think loosing autocomplete has quite a big impact on productivity. In our codebase, we reduce friction to the minimum and forcing users to inspect the original function and having to navigate multiple levels in the codebase to understand the structure.","Sure, we can give it a try, but fundamentally the proposed annotation is incorrect, so if it breaks too many downstream packages we won't be able to add it.","I expect this will break code. Not because of the `jax.Array` vs `ShapeDtypeStruct`, but because it will uncover actual pytype error that were previously hidden. I recently sent cl/582235119 to fix annotations for `.named_call` (which should be a noop), but this exposed many bad calls that were ignored by pytype because decorated function were loosing their type checking. It should also be possible to support the simple cases of simple nested structures with overload: ```python PyTree = _T  tuple['PyTree[_T]',...]  def eval_shape(fun: Callable[P, jax.Array], *args: P.args, **kwargs: P.kwargs) > jax.ShapeDTypeStruct:   ...  def eval_shape(fun: Callable[P, PyTree[jax.Array]], *args: P.args, **kwargs: P.kwargs) > PyTree[jax.ShapeDTypeStruct]]:   ...  def eval_shape(fun: Callable[P, OutT], *args: P.args, **kwargs: P.kwargs) > OutT:   ... ``` But this might add too much complexity","That works for the simplest cases, but arbitrary types can be registered as pytrees at runtime so fundamentally there's no way to correctly annotate PyTrees in Python's type system.","I don't think this will help JAX itself, but if it helps any thirdparty projects: the next jaxtyping release will have enough to make annotating this possible. ```python from collections.abc import Callable from typing import TypeAlias import jax from jaxtyping import ArrayLike, PyTree ArrayOut: TypeAlias = PyTree[ArrayLike, ""T""] StructOut: TypeAlias = PyTree[jax.ShapeDtypeStruct, ""T""] def eval_shape(Callable[..., ArrayOut], ...) > StructOut:     ... ``` where the `""T""` is bound to the PyTree structure, so both input and output must have matching structures. (And the `ArrayLike`  / `ShapeDtypeStruct` indicate the leaf types as usual.) (Actually, the next release will even make it possible to annotate that the array shapes match too: ```python ArrayOut: TypeAlias = PyTree[Shaped[ArrayLike, ""?*shape""], ""T""] StructOut: TypeAlias = PyTree[Shaped[jax.ShapeDtypeStruct, ""?*shape""], ""T""] ``` but that one is probably unreadable magic unless you really like your type annotations.)","I'm going to close this; I don't think it's really possible to do better than the current annotation, given the fact that pytree types are not statically resolveable."
2191,"以下是一个github上的jax下的一个issue, 标题是(libtpu.so present but jax fails with "".local/lib/python3.9/site-packages/libtpu/libtpu.so: cannot open shared object file: No such file or directory"")， 内容是 ( Description i just got my hands on an usb coral tpu and wanted to try JAX, however i can't seem to get it to work. I get: ```Exception occurred during processing of request from ('127.0.0.1', 45428) Traceback (most recent call last):   File ""/home/username/.local/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 623, in backends     backend = _init_backend(platform)   File ""/home/username/.local/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 734, in _init_backend     backend = registration.factory()   File ""/home/username/.local/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 139, in tpu_client_timer_callback     client = xla_client.make_tpu_client(_get_tpu_library_path())   type: ignore   File ""/home/username/.local/lib/python3.9/sitepackages/jaxlib/xla_client.py"", line 188, in make_tpu_client     load_pjrt_plugin_dynamically('tpu', library_path or 'libtpu.so')   File ""/home/username/.local/lib/python3.9/sitepackages/jaxlib/xla_client.py"", line 144, in load_pjrt_plugin_dynamically     return _xla.load_pjrt_plugin(plugin_name, library_path) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to open /home/username/.local/lib/python3.9/sitepackages/libtpu/libtpu.so: /home/username/.local/lib/python3.9/sitepackages/libtpu/libtpu.so: cannot open shared object file: No such file or directory ``` however libtpu.so is present at that location: ```username:/var/www/temp $ ls /home/username/.local/lib/python3.9/sitepackages/libtpu/libtpu.so rwxrxrx 1 username username 265M Nov 10 12:54 /home/username/.local/lib/python3.9/sitepackages/libtpu/libtpu.so ``` of note, i'm just trying things out and did not setup a venv  What jax/jaxlib version are you using? jax0.4.20  Which accelerator(s) are you using? TPU  Additional system info Raspberry Pi 4, Debian GNU/Linux 11 (bullseye) 64bit  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"libtpu.so present but jax fails with "".local/lib/python3.9/site-packages/libtpu/libtpu.so: cannot open shared object file: No such file or directory"""," Description i just got my hands on an usb coral tpu and wanted to try JAX, however i can't seem to get it to work. I get: ```Exception occurred during processing of request from ('127.0.0.1', 45428) Traceback (most recent call last):   File ""/home/username/.local/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 623, in backends     backend = _init_backend(platform)   File ""/home/username/.local/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 734, in _init_backend     backend = registration.factory()   File ""/home/username/.local/lib/python3.9/sitepackages/jax/_src/xla_bridge.py"", line 139, in tpu_client_timer_callback     client = xla_client.make_tpu_client(_get_tpu_library_path())   type: ignore   File ""/home/username/.local/lib/python3.9/sitepackages/jaxlib/xla_client.py"", line 188, in make_tpu_client     load_pjrt_plugin_dynamically('tpu', library_path or 'libtpu.so')   File ""/home/username/.local/lib/python3.9/sitepackages/jaxlib/xla_client.py"", line 144, in load_pjrt_plugin_dynamically     return _xla.load_pjrt_plugin(plugin_name, library_path) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to open /home/username/.local/lib/python3.9/sitepackages/libtpu/libtpu.so: /home/username/.local/lib/python3.9/sitepackages/libtpu/libtpu.so: cannot open shared object file: No such file or directory ``` however libtpu.so is present at that location: ```username:/var/www/temp $ ls /home/username/.local/lib/python3.9/sitepackages/libtpu/libtpu.so rwxrxrx 1 username username 265M Nov 10 12:54 /home/username/.local/lib/python3.9/sitepackages/libtpu/libtpu.so ``` of note, i'm just trying things out and did not setup a venv  What jax/jaxlib version are you using? jax0.4.20  Which accelerator(s) are you using? TPU  Additional system info Raspberry Pi 4, Debian GNU/Linux 11 (bullseye) 64bit  NVIDIA GPU info _No response_",2023-11-10T19:08:18Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18486,"Ah! `libtpu` is for Google's Cloud TPU VM product (https://cloud.google.com/tpu), which is a datacenter accelerator.  Cloud TPUs are a different thing to ""Edge TPU""s, which are a very different kind of TPU for lowpower edge devices. JAX doesn't support edge TPU at the moment, I'm afraid! The error loading the `.so` file is because it is the wrong architecture: it's an x8664 library, but the Raspberry Pi is an ARM machine. Hope that helps! I'm sorry I don't have better news. ",oops. *facepalm* i was hoping to accelerate numpy operations on large datasets. I guess i didn't read enough beforehand. Another search tells me the tensorflow package might offer what i need.  Thanks!
1235,"以下是一个github上的jax下的一个issue, 标题是(One context manager with all of the flags?)， 内容是 (With the recent announcement on RNGs, I just found out about `jax.threefry_partitionable`.  There are now quite a few flags, each with its own context manager.  I love that they're set with context managers, but they're hard to discover unless you pay really close attention to all of the announcements.  Have you considered adding one context manager for all of the flags?  Something like: ```python def jax_context_manager(enable_x64: bool,                         enable_custom_prng: bool,                         threefry_partitionable: bool,                         cache_dir: str                         ) > ContextManager[None, None, None]:     ... ``` This would make it: * easy to discover flags, * encourage users to set them all at the same time, and * ensure uniformity (`cache_dir` is not a context manager and instead raises if you call it with different values, `threefry_partitionable` takes a Boolean flag whereas `enable_custom_prng` doesn't even though they could have been coded the same way).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,One context manager with all of the flags?,"With the recent announcement on RNGs, I just found out about `jax.threefry_partitionable`.  There are now quite a few flags, each with its own context manager.  I love that they're set with context managers, but they're hard to discover unless you pay really close attention to all of the announcements.  Have you considered adding one context manager for all of the flags?  Something like: ```python def jax_context_manager(enable_x64: bool,                         enable_custom_prng: bool,                         threefry_partitionable: bool,                         cache_dir: str                         ) > ContextManager[None, None, None]:     ... ``` This would make it: * easy to discover flags, * encourage users to set them all at the same time, and * ensure uniformity (`cache_dir` is not a context manager and instead raises if you call it with different values, `threefry_partitionable` takes a Boolean flag whereas `enable_custom_prng` doesn't even though they could have been coded the same way).",2023-11-10T16:53:31Z,enhancement,open,0,5,https://github.com/jax-ml/jax/issues/18483,"Related to https://github.com/google/jax/issues/17571, another advantage would be to have an environment variable that when set makes it an error to use Jax outside this context manager.  This would prevent Jax from being initialized outside threads, and make it easy to work with threads.  Otherwise, if Jax gets initialized, then it breaks when the thread is spawned and Jax is used within the thread. What I did previously was put an assert in Jax's internal initialization code and find any initializations that happened before my thread started.  This flag would make that unnecessary.","I agree the current configuration story is not ideal. If the main problem is discoverability, do you think improving the docs would help? On a related note, I started an internal refactoring a few weeks ago, which should eventually translate to meaningful improvements in the public API. The current thinking is * JAX has two distinct kinds of configuration options: _flags_, set at most once and never changed, and _state objects_, which can be modified any number of times both globally (via `config.update()`) and threadlocally via a context manager. * We want to clearly separate these on the type level and make the corresponding lookup and update APIs statically typed as well. That should help with catching typos (e.g. `jax.config.enable_custom_rng`) statically and also improve discoverability in editors/IDEs.","I'm not opposed to the idea of having a `config.update()`like API which updates state objects jointly. I would probably prefer to have just one way of doing threadlocal updates, though. So, we would potentially need to deprecate the current context managers in favor of the new API. I also like that we can get static type errors whenever flags are used for threadlocal updates (assuming the changes from my previous message materialize). E..g ```python  !!!  with jax.config.thread_local_update(     jax.config.platforms=[""foo"", ""bar""], )   ```","> do you think improving the docs would help? My mistake, I didn't see that they were all collected into section.  That said, what's going on here?  Is that `config.update`? > I'm not opposed to the idea of having a `config.update()`like API which updates state objects jointly.  Yeah, that would probably be good.  But, I'm going to use the context manager interface since I do use threads in some invocations, so this wouldn't help me.  I also find context managers more elegant. > I would probably prefer to have just one way of doing threadlocal updates, though. So, we would potentially need to deprecate the current context managers in favor of the new API. Fair enough.  I understand that there might be advantages of the current API. A couple other reasons to favor a single context manager: * it would allow you to have grouped flags.  E.g., `future=True` could turn on `enable_custom_vjp_by_custom_transpose` and `enable_custom_prng`; or `check_all=True` could enable `check_nans`, `check_infs`, `check_tracer_leaks`, etc. * it keeps the API smaller, which is one of the benefits of Jax. > I also like that we can get static type errors Yeah, I agree!","Also, `enable_x64` and `enable_debug_logging` (only exposed from the command line?) seem to be missing from the configuration section?"
901,"以下是一个github上的jax下的一个issue, 标题是(Implement more efficient `jax.block_until_ready(x)` in C++)， 内容是 (Implement more efficient `jax.block_until_ready(x)` in C++ The current implementation synchronously calls `ArrayImpl.block_until_ready()` one by one. This is suboptimal when it's not cheap to query the readiness of an array. Also, calling `x.block_until_ready()` causes GIL to be acquired/released repeatedly. To address this issue, this CL introduces a C++ implementation of `jax.block_until_ready(x)` that uses IFRT's `Array::GetReadyFuture()` to asynchronously query the readiness of all arrays and wait for them once. To preserve the previous behavior, the C++ implementation also has a slow path for any nonPyArray objects that implement `block_until_ready`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Implement more efficient `jax.block_until_ready(x)` in C++,"Implement more efficient `jax.block_until_ready(x)` in C++ The current implementation synchronously calls `ArrayImpl.block_until_ready()` one by one. This is suboptimal when it's not cheap to query the readiness of an array. Also, calling `x.block_until_ready()` causes GIL to be acquired/released repeatedly. To address this issue, this CL introduces a C++ implementation of `jax.block_until_ready(x)` that uses IFRT's `Array::GetReadyFuture()` to asynchronously query the readiness of all arrays and wait for them once. To preserve the previous behavior, the C++ implementation also has a slow path for any nonPyArray objects that implement `block_until_ready`.",2023-11-10T03:19:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18474
1988,"以下是一个github上的jax下的一个issue, 标题是(operation gpusolverDnCreate(&handle) failed: cuSolver has not been initialized)， 内容是 ( Description After installing jax as follows: ``` python3 m pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` I get the following error: ``` $ python3 c ""from jax import numpy as jnp; jnp.linalg.qr(jnp.zeros([2, 2]))"" Traceback (most recent call last):   File """", line 1, in    File ""/marvel/home/cgmartin/mambaforge/lib/python3.10/sitepackages/jax/_src/numpy/linalg.py"", line 598, in qr     q, r = lax_linalg.qr(a, full_matrices=full_matrices) jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver has not been initialized The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/marvel/home/cgmartin/mambaforge/lib/python3.10/sitepackages/jaxlib/gpu_solver.py"", line 127, in _geqrf_hlo     lwork, opaque = gpu_solver.build_geqrf_descriptor( RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver has not been initialized ```  What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.10, CentOS Linux 7 (Core)  NVIDIA GPU info ``` Thu Nov  9 17:28:05 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,operation gpusolverDnCreate(&handle) failed: cuSolver has not been initialized," Description After installing jax as follows: ``` python3 m pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` I get the following error: ``` $ python3 c ""from jax import numpy as jnp; jnp.linalg.qr(jnp.zeros([2, 2]))"" Traceback (most recent call last):   File """", line 1, in    File ""/marvel/home/cgmartin/mambaforge/lib/python3.10/sitepackages/jax/_src/numpy/linalg.py"", line 598, in qr     q, r = lax_linalg.qr(a, full_matrices=full_matrices) jax._src.source_info_util.JaxStackTraceBeforeTransformation: RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver has not been initialized The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/marvel/home/cgmartin/mambaforge/lib/python3.10/sitepackages/jaxlib/gpu_solver.py"", line 127, in _geqrf_hlo     lwork, opaque = gpu_solver.build_geqrf_descriptor( RuntimeError: jaxlib/gpu/solver_kernels.cc:45: operation gpusolverDnCreate(&handle) failed: cuSolver has not been initialized ```  What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.10, CentOS Linux 7 (Core)  NVIDIA GPU info ``` Thu Nov  9 17:28:05 2023        ++  ++ ```",2023-11-09T22:28:45Z,bug needs info NVIDIA GPU,closed,0,10,https://github.com/jax-ml/jax/issues/18471,"That return code from `cusolverDnCreate` means ""cusolver initialization failed"". My best guess: is it possible you have multiple copies of cusolver installed and JAX is finding the wrong one? Is there another copy in `/usr/local/cuda` perhaps?", Here are the directory contents around that path. Let me what other commands you'd like me to run for diagnosis. ``` $ ls /usr/local bin   cuda11	 cuda11.4  etc    include  lib64    sbin   src cuda  cuda11.0  dcgm	    games  lib	    libexec  share $ ls /usr/local/cuda bin				gds	 nsightee_plugins  src computesanitizer		include  nvml		   targets CUDA_Toolkit_Release_Notes.txt	lib64	 nvvm		   tools DOCS				libnvvp  README		   version.json EULA.txt			LICENSE  samples extras				man	 share $ ls /usr/local/cuda/targets x86_64linux $ ls /usr/local/cuda/targets/x86_64linux include  lib $ ls /usr/local/cuda/targets/x86_64linux/lib libaccinj64.so		      libnppial.so.11.4.0.110 libaccinj64.so.11.4	      libnppial_static.a libaccinj64.so.11.4.120       libnppicc.so libcublasLt.so		      libnppicc.so.11 libcublasLt.so.11	      libnppicc.so.11.4.0.110 libcublasLt.so.11.6.5.2       libnppicc_static.a libcublasLt_static.a	      libnppidei.so libcublas.so		      libnppidei.so.11 libcublas.so.11		      libnppidei.so.11.4.0.110 libcublas.so.11.6.5.2	      libnppidei_static.a libcublas_static.a	      libnppif.so libcudadevrt.a		      libnppif.so.11 libcudart.so		      libnppif.so.11.4.0.110 libcudart.so.11.0	      libnppif_static.a libcudart.so.11.4.148	      libnppig.so libcudart_static.a	      libnppig.so.11 libcufft.so		      libnppig.so.11.4.0.110 libcufft.so.10		      libnppig_static.a libcufft.so.10.5.2.100	      libnppim.so libcufft_static.a	      libnppim.so.11 libcufft_static_nocallback.a  libnppim.so.11.4.0.110 libcufftw.so		      libnppim_static.a libcufftw.so.10		      libnppist.so libcufftw.so.10.5.2.100       libnppist.so.11 libcufftw_static.a	      libnppist.so.11.4.0.110 libcufile_rdma.so	      libnppist_static.a libcufile_rdma.so.1	      libnppisu.so libcufile_rdma.so.1.0.2       libnppisu.so.11 libcufile_rdma_static.a       libnppisu.so.11.4.0.110 libcufile.so		      libnppisu_static.a libcufile.so.0		      libnppitc.so libcufile.so.1.0.2	      libnppitc.so.11 libcufile_static.a	      libnppitc.so.11.4.0.110 libcufilt.a		      libnppitc_static.a libcuinj64.so		      libnpps.so libcuinj64.so.11.4	      libnpps.so.11 libcuinj64.so.11.4.120	      libnpps.so.11.4.0.110 libculibos.a		      libnpps_static.a libcupti.so		      libnvblas.so libcupti.so.11.4	      libnvblas.so.11 libcupti.so.2021.2.2	      libnvblas.so.11.6.5.2 libcupti_static.a	      libnvjpeg.so libcurand.so		      libnvjpeg.so.11 libcurand.so.10		      libnvjpeg.so.11.5.2.120 libcurand.so.10.2.5.120       libnvjpeg_static.a libcurand_static.a	      libnvperf_host.so libcusolverMg.so	      libnvperf_host_static.a libcusolverMg.so.11	      libnvperf_target.so libcusolverMg.so.11.2.0.120   libnvptxcompiler_static.a libcusolver.so		      libnvrtcbuiltins.so libcusolver.so.11	      libnvrtcbuiltins.so.11.4 libcusolver.so.11.2.0.120     libnvrtcbuiltins.so.11.4.152 libcusolver_static.a	      libnvrtc.so libcusparse.so		      libnvrtc.so.11.2 libcusparse.so.11	      libnvrtc.so.11.4.152 libcusparse.so.11.6.0.120     libnvToolsExt.so libcusparse_static.a	      libnvToolsExt.so.1 liblapack_static.a	      libnvToolsExt.so.1.0.0 libmetis_static.a	      libOpenCL.so libnppc.so		      libOpenCL.so.1 libnppc.so.11		      libOpenCL.so.1.0 libnppc.so.11.4.0.110	      libOpenCL.so.1.0.0 libnppc_static.a	      libpcsamplingutil.so libnppial.so		      stubs libnppial.so.11 ```," can you try updating your CUDA version to `>=11.8` and see if the issue persists? Currently it looks like it is using `cusolver` from the `/usr/local/cuda11.4` install, and that may be causing the conflict. Note that JAX expects `cusolver>=11.4.1` while the `cuda11.4` toolkit provides `cusolver=11.2.0`. It can be a bit confusing since `cusolver` is versioned **separately** from that of the toolkit as a whole. I wonder if this is another instance where we ought to try and prefer pipinstalled CUDA tools if possible Edit: Actually, looking again, it should already prefer the pipinstalled `cusolver`, which is guaranteed compatible. For some extra details, would you be able to share the output of `pip list | grep nvidia`?",  ```sh $ python3 m pip list | grep nvidia nvidiacublascu11           11.11.3.6 nvidiacublascu12           12.1.3.1 nvidiacudacupticu11       11.8.87 nvidiacudanvcccu11        11.8.89 nvidiacudanvcccu12        12.1.105 nvidiacudaruntimecu11     11.8.89 nvidiacudaruntimecu12     12.1.105 nvidiacudnncu11            8.9.0.131 nvidiacudnncu12            8.9.0.131 nvidiacufftcu11            10.9.0.58 nvidiacufftcu12            11.0.2.54 nvidiacusolvercu11         11.4.1.48 nvidiacusolvercu12         11.4.5.107 nvidiacusparsecu11         11.7.5.86 nvidiacusparsecu12         12.1.0.106 nvidiancclcu11             2.19.3 nvidianvjitlinkcu12        12.1.105 ```,One possible problem is you have both `cu11` and `cu12` packages installed. These cannot coexist! They will stomp over each other's files. Can you try in a fresh virtualenv?, What's the recommended way to pip uninstall one of the two `cu` versions? Just manually pip uninstalling the packages one by one?," I'd recommend a fresh virtualenv, but if you want to try removing manually, I'd remove *all* of the packages listed above and then reinstall `jax[cuda11_pip]` (or cuda 12, if you prefer, but only one). LMK if that works!", Looks like the error stopped occurring.,Great! Closing.,">  I'd recommend a fresh virtualenv, but if you want to try removing manually, I'd remove _all_ of the packages listed above and then reinstall `jax[cuda11_pip]` (or cuda 12, if you prefer, but only one). LMK if that works! how can i remove all packages  ??"
868,"以下是一个github上的jax下的一个issue, 标题是(Using CPU array raise Disallowed host-to-device transfer)， 内容是 ( Description When the array is on CPU backend, simple ops trigger hosttodevice transfer. Reproduction: ```python x = jax.device_put(0, jax.devices('cpu')[0]) with jax.transfer_guard('disallow'):   x + 0   XlaRuntimeError ``` `x + 0` raise: ``` XlaRuntimeError: INVALID_ARGUMENT: Disallowed hosttodevice transfer: aval=ShapedArray(int32[]), dst_sharding=GSPMDSharding({replicated}) ``` It looks like `0` gets converted to REPLICATED array before, so `x + 0` trigger a transfer. Might be related to: * https://github.com/google/jax/issues/16002 * https://github.com/google/jax/issues/16602  What jax/jaxlib version are you using? HEAD)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Using CPU array raise Disallowed host-to-device transfer," Description When the array is on CPU backend, simple ops trigger hosttodevice transfer. Reproduction: ```python x = jax.device_put(0, jax.devices('cpu')[0]) with jax.transfer_guard('disallow'):   x + 0   XlaRuntimeError ``` `x + 0` raise: ``` XlaRuntimeError: INVALID_ARGUMENT: Disallowed hosttodevice transfer: aval=ShapedArray(int32[]), dst_sharding=GSPMDSharding({replicated}) ``` It looks like `0` gets converted to REPLICATED array before, so `x + 0` trigger a transfer. Might be related to: * https://github.com/google/jax/issues/16002 * https://github.com/google/jax/issues/16602  What jax/jaxlib version are you using? HEAD",2023-11-08T16:04:41Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/18436,The problem here is that the `0` is on host. There will still be a host > device transfer in the sense of sending `0` from host to CPU device 0. So I would say this is working as expected.
2625,"以下是一个github上的jax下的一个issue, 标题是(Inconsistent behaviour of jited and non jited array assignment function on some `PositionalSharding` topologies.)， 内容是 ( Description We are trying to implement `PositionalSharding` in a minimal case where we are are shifting values in an array and assigning new ones. We have observed that when jiting this `assign` function the outputs are different and incorrect when `array` has a `(8,1)` or `(2, 4)` shard topology. The sharding does not effect the unjited function. From inspection of the jited outputs on the topologies that fail, we have noticed the values from the right hand side of the initial  array end up at index `7` i.e. `jitted_assign(array, value)[7] == array[1]`. The correct behaviour is `assign(array, value)[7] == array[8]`. ```python import os  Make 8 CPU devices os.environ[""XLA_FLAGS""] = 'xla_force_host_platform_device_count=8' import jax.numpy as jnp import jax.random from jax.experimental import mesh_utils from jax.sharding import PositionalSharding def assign(array, value):     """"""Shift the array to the left by one and set the rightmost column to value.""""""     shifted = array.at[:1, :].set(array[1:, :])     return shifted.at[1, :].set(jnp.ones(array.shape[1]) * value) def main():     value = 1.456     N = 16     array = jax.random.uniform(jax.random.PRNGKey(1241), (N, N))     devices = mesh_utils.create_device_mesh((8,))     sharding = PositionalSharding(devices)     jitted_assign = jax.jit(assign)      This works     array = jax.device_put(array, sharding.reshape(1, 8))     assert jnp.allclose(assign(array, value), jitted_assign(array, value))      Correctly shifted output     assert jnp.allclose(assign(array, value)[7], array[8])     array = jax.device_put(array, sharding.reshape(8, 1))      This should not pass     assert jnp.allclose(jitted_assign(array, value)[7], array[1])      This fails     assert jnp.allclose(assign(array, value), jitted_assign(array, value))      This fails     array = jax.device_put(array, sharding.reshape(2, 4))     assert jnp.allclose(assign(array, value), jitted_assign(array, value)) if __name__ == '__main__':     main() ``` Any help to explain what is going on would be very appreciated, whether this is a bug or me just not understanding how sharding is expected to work.  What jax/jaxlib version are you using? jax==0.4.16 jaxlib==0.4.16  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, Ubuntu 22.04.3 LTS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Inconsistent behaviour of jited and non jited array assignment function on some `PositionalSharding` topologies.," Description We are trying to implement `PositionalSharding` in a minimal case where we are are shifting values in an array and assigning new ones. We have observed that when jiting this `assign` function the outputs are different and incorrect when `array` has a `(8,1)` or `(2, 4)` shard topology. The sharding does not effect the unjited function. From inspection of the jited outputs on the topologies that fail, we have noticed the values from the right hand side of the initial  array end up at index `7` i.e. `jitted_assign(array, value)[7] == array[1]`. The correct behaviour is `assign(array, value)[7] == array[8]`. ```python import os  Make 8 CPU devices os.environ[""XLA_FLAGS""] = 'xla_force_host_platform_device_count=8' import jax.numpy as jnp import jax.random from jax.experimental import mesh_utils from jax.sharding import PositionalSharding def assign(array, value):     """"""Shift the array to the left by one and set the rightmost column to value.""""""     shifted = array.at[:1, :].set(array[1:, :])     return shifted.at[1, :].set(jnp.ones(array.shape[1]) * value) def main():     value = 1.456     N = 16     array = jax.random.uniform(jax.random.PRNGKey(1241), (N, N))     devices = mesh_utils.create_device_mesh((8,))     sharding = PositionalSharding(devices)     jitted_assign = jax.jit(assign)      This works     array = jax.device_put(array, sharding.reshape(1, 8))     assert jnp.allclose(assign(array, value), jitted_assign(array, value))      Correctly shifted output     assert jnp.allclose(assign(array, value)[7], array[8])     array = jax.device_put(array, sharding.reshape(8, 1))      This should not pass     assert jnp.allclose(jitted_assign(array, value)[7], array[1])      This fails     assert jnp.allclose(assign(array, value), jitted_assign(array, value))      This fails     array = jax.device_put(array, sharding.reshape(2, 4))     assert jnp.allclose(assign(array, value), jitted_assign(array, value)) if __name__ == '__main__':     main() ``` Any help to explain what is going on would be very appreciated, whether this is a bug or me just not understanding how sharding is expected to work.  What jax/jaxlib version are you using? jax==0.4.16 jaxlib==0.4.16  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, Ubuntu 22.04.3 LTS  NVIDIA GPU info _No response_",2023-11-08T14:54:52Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/18434,Can you try on the latest jax and jaxlib version?,"I get the same result after upgrading to: ``` jax==0.4.20 jaxlib==0.4.20 ``` Stack trace: ``` Traceback (most recent call last):   File ""/home/laurence/2dnavierstokes/solver/sharding_error.py"", line 49, in      main()   File ""/home/laurence/2dnavierstokes/solver/sharding_error.py"", line 41, in main     assert jnp.allclose(assign(array, value), jitted_assign(array, value)) AssertionError ```","Any ideas  ? Is there someway I can shard without having to use anything experimental, if there is a stability issue there?"
3237,"以下是一个github上的jax下的一个issue, 标题是(Performance regression for reduction on GPU)， 内容是 ( Description I've observed a performance regression moving from jax 0.4.14 to 0.4.16 in a rejection sampling while loop. Here's a simplified version of the code that exhibits the same behavior: ```python import timeit import jax import jax.numpy as jnp def rejection_sample(sample_fn, accept_fn, *, rng_key):     def loop_body(state):         (rng_key, sample, accepted_mask) = state         rng_key, subkey = jax.random.split(rng_key)         new_sample = sample_fn(subkey)         sample = jax.tree_map(             lambda x, x_new: jnp.where(accepted_mask, x, x_new),             sample, new_sample)         accepted_mask = jnp.logical_or(accept_fn(new_sample),                                        accepted_mask)         return rng_key, sample, accepted_mask     def loop_cond(state):         _, _, accepted_mask = state         return jnp.logical_not(jnp.all(accepted_mask))     rng_key, subkey = jax.random.split(rng_key)     first_sample = sample_fn(subkey)     first_accepted = accept_fn(first_sample)     _, sample, _ = jax.lax.while_loop(loop_cond, loop_body,                                       (rng_key, first_sample,                                        first_accepted))     return sample def sample(shape, *, rng_key):     def sample_fn(rng_key):         return jax.random.uniform(rng_key, minval=0.0, maxval=1.0,                                   shape=shape)     def accept_fn(sample):         return sample < 0.7     return rejection_sample(sample_fn, accept_fn, rng_key=rng_key) L = 16 Lt = 48 rng_key = jax.random.PRNGKey(0) shape = (L, L, L, Lt)  Force compilation before timing sample_jit = jax.jit(lambda key: sample(shape, rng_key=key)) rng_key, subkey = jax.random.split(rng_key) x0 = sample_jit(subkey).block_until_ready() rng_key, subkey = jax.random.split(rng_key) dt = timeit.timeit(lambda: sample_jit(subkey).block_until_ready(),                    number=100) print(dt) ``` Running this under several different environments yields ``` Using jax0.4.10 0.07284442099989974 Using jax0.4.12 0.08756969699970796 Using jax0.4.14 0.10284504599985667 Using jax0.4.16 7.349518729002739 Using jax0.4.20 8.398614255998837 ``` I.e. the newer version is ~100x slower (in fact, there seems to be a consistent increase in runtime from version to version). I see similar behavior on two different types NVIDIA GPUs (A100 and RTX 2080 Ti). If I force everything to run on CPU, the timings become more inconsistent, but generally consistent with each other, so this likely a GPUspecific issue: ``` Using jax0.4.10 (cpu) 5.222352617987781 Using jax0.4.12 (cpu) 4.833697886992013 Using jax0.4.14 (cpu) 3.9580981749895727 Using jax0.4.16 (cpu) 5.534542325010989 Using jax0.4.20 (cpu) 4.845710285997484 ``` Potentially related issues: https://github.com/google/jax/issues/16661 https://github.com/google/jax/issues/16663  What jax/jaxlib version are you using? Several (see above)  Which accelerator(s) are you using? CPU, GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Performance regression for reduction on GPU," Description I've observed a performance regression moving from jax 0.4.14 to 0.4.16 in a rejection sampling while loop. Here's a simplified version of the code that exhibits the same behavior: ```python import timeit import jax import jax.numpy as jnp def rejection_sample(sample_fn, accept_fn, *, rng_key):     def loop_body(state):         (rng_key, sample, accepted_mask) = state         rng_key, subkey = jax.random.split(rng_key)         new_sample = sample_fn(subkey)         sample = jax.tree_map(             lambda x, x_new: jnp.where(accepted_mask, x, x_new),             sample, new_sample)         accepted_mask = jnp.logical_or(accept_fn(new_sample),                                        accepted_mask)         return rng_key, sample, accepted_mask     def loop_cond(state):         _, _, accepted_mask = state         return jnp.logical_not(jnp.all(accepted_mask))     rng_key, subkey = jax.random.split(rng_key)     first_sample = sample_fn(subkey)     first_accepted = accept_fn(first_sample)     _, sample, _ = jax.lax.while_loop(loop_cond, loop_body,                                       (rng_key, first_sample,                                        first_accepted))     return sample def sample(shape, *, rng_key):     def sample_fn(rng_key):         return jax.random.uniform(rng_key, minval=0.0, maxval=1.0,                                   shape=shape)     def accept_fn(sample):         return sample < 0.7     return rejection_sample(sample_fn, accept_fn, rng_key=rng_key) L = 16 Lt = 48 rng_key = jax.random.PRNGKey(0) shape = (L, L, L, Lt)  Force compilation before timing sample_jit = jax.jit(lambda key: sample(shape, rng_key=key)) rng_key, subkey = jax.random.split(rng_key) x0 = sample_jit(subkey).block_until_ready() rng_key, subkey = jax.random.split(rng_key) dt = timeit.timeit(lambda: sample_jit(subkey).block_until_ready(),                    number=100) print(dt) ``` Running this under several different environments yields ``` Using jax0.4.10 0.07284442099989974 Using jax0.4.12 0.08756969699970796 Using jax0.4.14 0.10284504599985667 Using jax0.4.16 7.349518729002739 Using jax0.4.20 8.398614255998837 ``` I.e. the newer version is ~100x slower (in fact, there seems to be a consistent increase in runtime from version to version). I see similar behavior on two different types NVIDIA GPUs (A100 and RTX 2080 Ti). If I force everything to run on CPU, the timings become more inconsistent, but generally consistent with each other, so this likely a GPUspecific issue: ``` Using jax0.4.10 (cpu) 5.222352617987781 Using jax0.4.12 (cpu) 4.833697886992013 Using jax0.4.14 (cpu) 3.9580981749895727 Using jax0.4.16 (cpu) 5.534542325010989 Using jax0.4.20 (cpu) 4.845710285997484 ``` Potentially related issues: https://github.com/google/jax/issues/16661 https://github.com/google/jax/issues/16663  What jax/jaxlib version are you using? Several (see above)  Which accelerator(s) are you using? CPU, GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info _No response_",2023-11-07T19:02:35Z,bug performance XLA NVIDIA GPU,closed,0,8,https://github.com/jax-ml/jax/issues/18424,Can you try updating to the latest version of JAX? I believe jaxlib==0.4.16 has a known performance issue that may be related: https://github.com/google/jax/pull/17724,"The latest version I've tried is 0.4.20, which is the most recent release as far as I can tell. The issue is still present there. The issue you linked inspired me to dig a bit deeper, and after some further investigation it appears the issue is not actually in the while loop, but rather in the condition. Specifically the combination `jnp.logical_not(jnp.all(...))` seems to be the slow part. This simplifies things a bit: ```python import timeit import jax import jax.numpy as jnp .jit def f(x):     return jnp.logical_not(jnp.all(x < 0.7)) .jit def g(x):     return jnp.any(jnp.logical_not(x < 0.7)) .jit def h(x):     return jnp.all(x < 0.7) shape = (16**3 * 48,) rng_key = jax.random.PRNGKey(0) x = jax.random.uniform(rng_key, minval=0.0, maxval=1.0,                        shape=shape)  Compile f(x) g(x) h(x) dt = timeit.timeit(lambda: f(x).block_until_ready(),                    number=1000) print(f""not(all(x)): {dt}"") dt = timeit.timeit(lambda: g(x).block_until_ready(),                    number=1000) print(f""any(not(x)): {dt}"") dt = timeit.timeit(lambda: h(x).block_until_ready(),                    number=1000) print(f""all(x): {dt}"") ``` Running this I get ``` Using jax0.4.10 not(all(x)): 0.04904459699173458 any(not(x)): 0.040804433010634966 all(x): 0.03323979300330393 Using jax0.4.12 not(all(x)): 0.06948921899311244 any(not(x)): 0.041380729991942644 all(x): 0.03729355499672238 Using jax0.4.14 not(all(x)): 0.07314369500090834 any(not(x)): 0.065828587001306 all(x): 0.06242232900694944 Using jax0.4.16 not(all(x)): 5.161174478009343 any(not(x)): 0.1380664679891197 all(x): 0.11088073400605936 Using jax0.4.20 not(all(x)): 3.935028567997506 any(not(x)): 0.11883646699425299 all(x): 0.09598468799958937 ``` I'm not sure if this is the same as CC(Added `serial_dot_products` benchmark), but it does sound plausible.",I can't reproduce this on my (CPU) machine: ``` not(all(x)): 0.031113745993934572 any(not(x)): 0.029049667005892843 all(x): 0.029495747003238648 ``` On what platform does this issue occur?,"The issue is only present on GPU  in particular I've seen this with 2 different Nvidia GPUs (A100 and RTX 2080 Ti), and also two different CUDA versions (11.8 and 12.0).","Thanks, it looks like XLA's code generation regressed for this reduction. Filed https://github.com/openxla/xla/issues/7152","We have found that the regression came from reduction epilogue fusion which was implemented in that time frame when the regression occurred. We allowed to create a fusion that is actually not supported by the reduction emitter, so the reduction got emitted via the loop emitter as elementwise reduction, which is quite a bit slower in this case. I am about to submit a fix where we do not allow this fusion, so the reduce will still be the root of the fusion.",https://github.com/openxla/xla/commit/cc5307ec4f968a80aecaeb0febed2a65789ab713 was the fix for this,Closing because the XLA issue was fixed! Please try it out.
2434,"以下是一个github上的jax下的一个issue, 标题是(Setting values in a matrix by broadcasting slower than using a loop after jit compilation)， 内容是 ( Description When inserting an array of values into a matrix using a for loop seems to run faster than setting the values by broadcasting after compilation. The compilation of the for loop is much slower as expected. This issue only seems to show up when using ""jax_enable_x64"" as when I remove that and use the default dtype for the matrices the broadcasting solution is faster than the loop as expected.  ```python import jax import jax.numpy as jnp from time import time from jax.config import config config.update(""jax_enable_x64"", True) npart = 50 npair = npart * (npart  1) k = jnp.arange(npair) ip = jnp.empty(npair, dtype=int) jp = jnp.empty(npair, dtype=int) k = 0 for i in range(npart):     for j in range(npart):         if (i != j):             ip = ip.at[k].set(i)             jp = jp.at[k].set(j)             k += 1 key = jax.random.PRNGKey(17) key, key_input = jax.random.split(key) .jit def loop_set(pf):     Rho = jnp.zeros((npart+1, npart+1), dtype='complex128')     for k in range(npair):         Rho = Rho.at[ip[k], jp[k]].set(pf[k])     return Rho .jit def broadcast_set(pf):     Rho = jnp.zeros((npart+1, npart+1), dtype='complex128')     Rho = Rho.at[ip, jp].set(pf)     return Rho pf = jax.random.normal(key_input, shape=[npair]) key, key_input = jax.random.split(key) t = time() a = loop_set(pf) a.block_until_ready() print(f'loop_set compile time = {time()t} (s)') pf = jax.random.normal(key_input, shape=[npair]) key, key_input = jax.random.split(key) t = time() a = broadcast_set(pf) a.block_until_ready() print(f'broadcast_set compile time = {time()t} (s)') pf = jax.random.normal(key_input, shape=[npair]) key, key_input = jax.random.split(key) t = time() a = loop_set(pf) a.block_until_ready() print(f'loop_set run time = {time()t} (s)') pf = jax.random.normal(key_input, shape=[npair]) key, key_input = jax.random.split(key) t = time() a = broadcast_set(pf) a.block_until_ready() print(f'broadcast_set run time = {time()t} (s)') ```  What jax/jaxlib version are you using? jax v0.4.13, jaxlib v0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Linux  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Setting values in a matrix by broadcasting slower than using a loop after jit compilation," Description When inserting an array of values into a matrix using a for loop seems to run faster than setting the values by broadcasting after compilation. The compilation of the for loop is much slower as expected. This issue only seems to show up when using ""jax_enable_x64"" as when I remove that and use the default dtype for the matrices the broadcasting solution is faster than the loop as expected.  ```python import jax import jax.numpy as jnp from time import time from jax.config import config config.update(""jax_enable_x64"", True) npart = 50 npair = npart * (npart  1) k = jnp.arange(npair) ip = jnp.empty(npair, dtype=int) jp = jnp.empty(npair, dtype=int) k = 0 for i in range(npart):     for j in range(npart):         if (i != j):             ip = ip.at[k].set(i)             jp = jp.at[k].set(j)             k += 1 key = jax.random.PRNGKey(17) key, key_input = jax.random.split(key) .jit def loop_set(pf):     Rho = jnp.zeros((npart+1, npart+1), dtype='complex128')     for k in range(npair):         Rho = Rho.at[ip[k], jp[k]].set(pf[k])     return Rho .jit def broadcast_set(pf):     Rho = jnp.zeros((npart+1, npart+1), dtype='complex128')     Rho = Rho.at[ip, jp].set(pf)     return Rho pf = jax.random.normal(key_input, shape=[npair]) key, key_input = jax.random.split(key) t = time() a = loop_set(pf) a.block_until_ready() print(f'loop_set compile time = {time()t} (s)') pf = jax.random.normal(key_input, shape=[npair]) key, key_input = jax.random.split(key) t = time() a = broadcast_set(pf) a.block_until_ready() print(f'broadcast_set compile time = {time()t} (s)') pf = jax.random.normal(key_input, shape=[npair]) key, key_input = jax.random.split(key) t = time() a = loop_set(pf) a.block_until_ready() print(f'loop_set run time = {time()t} (s)') pf = jax.random.normal(key_input, shape=[npair]) key, key_input = jax.random.split(key) t = time() a = broadcast_set(pf) a.block_until_ready() print(f'broadcast_set run time = {time()t} (s)') ```  What jax/jaxlib version are you using? jax v0.4.13, jaxlib v0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Linux  NVIDIA GPU info ``` ++  ++ ```",2023-11-07T16:19:53Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/18420,"Hi  thanks for the report! When I run your code on a Colab T4 GPU I see this: ``` loop_set compile time = 53.260170459747314 (s) broadcast_set compile time = 0.1718456745147705 (s) loop_set run time = 0.022067546844482422 (s) broadcast_set run time = 0.02761054039001465 (s) ``` This is not all that different than what I'd expect for code like this: the compilation of the `for`loop version takes a long time becuase JAX tracing unrolls python loops. In general, if you find yourself looping over array values, you should expect  compilation time to grow approximately quadratically with the number of iterations: this is becuse the compiler must consider each statement in sequence and use heuristics to figure out how to execute it efficiently within the program. On the other hand, a single broadcastbased approach will have relatively quick compilation, because there are many fewer statements for the compiler to consider. The flipside of this is that loopbased implementations give the compiler a lot of freedom to select an efficient implementation, so the resulting function can be a bit faster than if the computation is expressed in terms of single broadcasted operations, which lower to predefined kernels. In your case, the fact that the predefined 64bit kernel is slower, while the predefined 32bit kernel is faster, probably speaks to the kind of operations your hardware was built for (64bit operations are often much slower than 32bit on modern GPUs). All of that seems consistent with the results you're seeing in your experiment.","I didn't realize the loopbased implementation would give the compiler more freedom. In my case I see the ""loop_set run time"" is quite a bit faster than the ""broadcast_set run time""  ``` loop_set compile time = 44.53834414482117 (s) broadcast_set compile time = 0.13554906845092773 (s) loop_set run time = 0.009951353073120117 (s) broadcast_set run time = 0.025753021240234375 (s) ``` From your explanation it seems this difference just comes down to what the hardware is built for? Thank you for the thorough and quick response!","Yeah, it's a combination of what operations the hardware can do efficiently, and what sequences of operations the XLA compiler has heuristics for. If you're curious about the details, you can dig deeper by printing out the optimized HLO using the Ahead of time compilation APIs, but the HLO for your loopbased approach is going to be very long!",I'm going to close this as I think the question is addressed – thanks!
4398,"以下是一个github上的jax下的一个issue, 标题是(TPU V3-32 jax profiling example failing)， 内容是 ( Description Hello, I tried running the https://jax.readthedocs.io/en/latest/profiling.html example: ```python with jax.profiler.trace(""/tmp/jaxtrace"", create_perfetto_link=True):    Run the operations to be profiled   key = jax.random.PRNGKey(0)   x = jax.random.normal(key, (5000, 5000))   y = x @ x   y.block_until_ready() ``` and received  the following error: ```bash 20231106 17:01:02.021973: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231106 17:01:02.022024: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231106 17:01:02.022078: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231106 17:01:02.188791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231106 17:01:02.188843: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231106 17:01:02.188898: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231106 17:01:02.191595: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231106 17:01:02.191643: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231106 17:01:02.191697: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231106 17:01:02.234758: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231106 17:01:02.234804: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231106 17:01:02.234864: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231106 17:01:03.803584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20231106 17:01:03.961014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20231106 17:01:03.951421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20231106 17:01:04.057704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT ``` Here is the command I used to create the tpu nodes: ```bash gcloud compute tpus tpuvm create nodev332euw4a project myprjname \       zone=europewest4a \       acceleratortype=v332 \       version=tpuubuntu2204base \       preemptible gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install tensorflow tensorboardpluginprofile"" gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html"" ``` Any advice on what is going wrong and how to fix this? Best Regards, AI  What jax/jaxlib version are you using? jax 0.4.19, jaxlib 0.4.19  Which accelerator(s) are you using? TPU  Additional system info tpuubuntu2204base  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,TPU V3-32 jax profiling example failing," Description Hello, I tried running the https://jax.readthedocs.io/en/latest/profiling.html example: ```python with jax.profiler.trace(""/tmp/jaxtrace"", create_perfetto_link=True):    Run the operations to be profiled   key = jax.random.PRNGKey(0)   x = jax.random.normal(key, (5000, 5000))   y = x @ x   y.block_until_ready() ``` and received  the following error: ```bash 20231106 17:01:02.021973: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231106 17:01:02.022024: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231106 17:01:02.022078: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231106 17:01:02.188791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231106 17:01:02.188843: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231106 17:01:02.188898: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231106 17:01:02.191595: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231106 17:01:02.191643: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231106 17:01:02.191697: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231106 17:01:02.234758: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231106 17:01:02.234804: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231106 17:01:02.234864: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231106 17:01:03.803584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20231106 17:01:03.961014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20231106 17:01:03.951421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20231106 17:01:04.057704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT ``` Here is the command I used to create the tpu nodes: ```bash gcloud compute tpus tpuvm create nodev332euw4a project myprjname \       zone=europewest4a \       acceleratortype=v332 \       version=tpuubuntu2204base \       preemptible gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install tensorflow tensorboardpluginprofile"" gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html"" ``` Any advice on what is going wrong and how to fix this? Best Regards, AI  What jax/jaxlib version are you using? jax 0.4.19, jaxlib 0.4.19  Which accelerator(s) are you using? TPU  Additional system info tpuubuntu2204base  NVIDIA GPU info _No response_",2023-11-06T17:13:21Z,bug needs info TPU,open,0,8,https://github.com/jax-ml/jax/issues/18408,These errors usually occur when there's a conflict with multiple registrations of the same plugin.  Try checking if there are any duplicate registrations or conflicts in your code or environment.,"Hi  , Can you please help understand you suggestion? For more context here is the code I ran   ```python  The following code snippet will be run on all TPU hosts import sys import jax import jax.numpy as jnp from jax import pmap import timeit import time with jax.profiler.trace(""/tmp/jaxtrace"", create_perfetto_link=True):    Run the operations to be profiled   key = jax.random.PRNGKey(0)   x = jax.random.normal(key, (5000, 5000))   y = x @ x   y.block_until_ready() ``` And then after you suggested multiple registrations of the same plugin, I ran the below code   ```python import sys import jax import jax.numpy as jnp from jax import pmap import timeit import time if(jax.process_index() == 0):   with jax.profiler.trace(""/tmp/jaxtrace"", create_perfetto_link=True):      Run the operations to be profiled     key = jax.random.PRNGKey(0)     x = jax.random.normal(key, (5000, 5000))     y = x @ x     y.block_until_ready() ``` any idea what is going wrong? Best regards, AI","The errors you are getting are related to TPU driver and TPU runtime not being installed. You can install them using the following commands:  ` gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install tensorflowtpu"" gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install tensorflowio"" `  Once you have installed the TPU driver and TPU runtime, you can try running the code again.﻿ Hope it work","Hi  , Thank you for the suggestion. I created a new node and ran the above commands, still I got the same error: ```bash 20231107 18:54:28.834628: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231107 18:54:28.834673: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231107 18:54:28.834737: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231107 18:54:28.893309: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231107 18:54:28.893356: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231107 18:54:28.893410: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231107 18:54:28.905076: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231107 18:54:28.905124: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231107 18:54:28.905178: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231107 18:54:28.949548: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 20231107 18:54:28.949598: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 20231107 18:54:28.949652: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 20231107 18:54:30.375943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20231107 18:54:30.482016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20231107 18:54:30.482336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20231107 18:54:30.558450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT ``` Not sure what the issue is. I am finding it hard to debugfrom the error messages as well. Any idea what can be tried next? Best regards, AI","Your TensorRT installation is wrong I guess. To install TensorRT manually, you can follow the instructions in the TensorRT documentation. Just try it but after installation you will find some version mismatch issues so it's gonna be tough stuff. Gud luck!","Moreover I'm getting this output on T4 ``` Open URL in browser: https://ui.perfetto.dev/!/?url=http://127.0.0.1:9001/perfetto_trace.json.gz ``` On TPU I'm getting this error ``` WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)   ValueError Traceback (most recent call last)  in () 7 8 if(jax.process_index() == 0): > 9 with jax.profiler.trace(""/tmp/jaxtrace"", create_perfetto_link=True): 10  Run the operations to be profiled 11 key = jax.random.PRNGKey(0)  3 frames /usr/local/lib/python3.10/distpackages/jax/_src/profiler.py in _write_perfetto_trace_file(log_dir) 133 trace_jsons = glob.glob(os.path.join(latest_folder, ""*.trace.json.gz"")) 134 if len(trace_jsons) != 1: > 135 raise ValueError(f""Invalid trace folder: {latest_folder}"") 136 trace_json, = trace_jsons 137 ValueError: Invalid trace folder: /tmp/jaxtrace/plugins/profile/2023_11_08_05_24_06 ``` But not the one you mentioned","  What actually goes wrong? Is it just those errors are spat out? Those look benign to me, since they are related to `tensorflow` trying to initialize GPU things, which won't work, because you don't have a GPU on a TPU machine. You can probably work around by installing `tensorflowcpu` instead. Does the profile itself work if you leave aside those errors?",I had the same issue.
776,"以下是一个github上的jax下的一个issue, 标题是(Add comb function to jax._src.scipy.special)， 内容是 (Here, we add the comb function to jax.scipy.special. This implementation provides an approximate and efficient calculation, while also handling the exact case. As in scipy, we also handle the case where the number of repetitions is computed. Finally, we add a test in the `tests/lax_scipy_special_functions_test.py` file to maintain testing coverage. This is my first PR to jax. Although I tried to follow the contributing guidelines closely, please let me know if there's anything I missed.  Thanks! Resolves: CC([scipy] Add scipy.special.comb to JAX) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add comb function to jax._src.scipy.special,"Here, we add the comb function to jax.scipy.special. This implementation provides an approximate and efficient calculation, while also handling the exact case. As in scipy, we also handle the case where the number of repetitions is computed. Finally, we add a test in the `tests/lax_scipy_special_functions_test.py` file to maintain testing coverage. This is my first PR to jax. Although I tried to follow the contributing guidelines closely, please let me know if there's anything I missed.  Thanks! Resolves: CC([scipy] Add scipy.special.comb to JAX) ",2023-11-04T18:38:34Z,,open,0,5,https://github.com/jax-ml/jax/issues/18389,"Thanks for the review  ! Hm, unsure why this PR looks so messy now, but I added in the changes that you suggested with the exception of my question on `exact`.  Please let me know if I should create a new PR that's less messy, or if this is ok. ","You should be able to fix the commit history with something like this (assuming upstream points to the main jax repo, and origin points to your fork: ``` $ git remote v   the output should look something like this: origin	git.com:Phionx/jax.git (fetch) origin	git.com:Phionx/jax.git (push) upstream	git.com:google/jax.git (fetch) upstream	git.com:google/jax.git (push) $ git checkout main $ git pull upstream main $ git checkout addingscipyspecialcomb $ git rebase main $ git push origin +addingscipyspecialcomb ```",Hi  let me know if you'd like to continue working on this!,"Hi , Thanks for the ping! I've made changes based on your most recent suggestions.  Please let me know if this looks good to you or if there are any other changes for me to add. Thanks for your review!",Can I bump this? Am looking to migrate some bezier curve code from scipy to jax to see if jax can be used to fit composite bezier curves efficiently
961,"以下是一个github上的jax下的一个issue, 标题是(Inaccurate output when converting bool array to bfloat16 )， 内容是 ( Description ```python import jax import numpy as np np.random.seed(42) x = (np.random.rand(2, 2048) * 1024).astype(np.int32) x = jax.numpy.asarray(x) print(x) print(x > 100) print(jnp.sum((x > 100).astype(jax.numpy.int32), 1, keepdims=True)) print(jnp.sum((x > 100).astype(jax.numpy.bfloat16), 1, keepdims=True)) ``` Outputs: ``` [[383 973 749 ... 740  69 724]  [556  83 469 ... 951 339 476]] [[ True  True  True ...  True False  True]  [ True False  True ...  True  True  True]] [[1828]  [1837]] [[1824]  [1840]] ``` Note the difference in the sum!  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Inaccurate output when converting bool array to bfloat16 ," Description ```python import jax import numpy as np np.random.seed(42) x = (np.random.rand(2, 2048) * 1024).astype(np.int32) x = jax.numpy.asarray(x) print(x) print(x > 100) print(jnp.sum((x > 100).astype(jax.numpy.int32), 1, keepdims=True)) print(jnp.sum((x > 100).astype(jax.numpy.bfloat16), 1, keepdims=True)) ``` Outputs: ``` [[383 973 749 ... 740  69 724]  [556  83 469 ... 951 339 476]] [[ True  True  True ...  True False  True]  [ True False  True ...  True  True  True]] [[1828]  [1837]] [[1824]  [1840]] ``` Note the difference in the sum!  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-11-04T04:40:54Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18387,"You closed this so you've probably figured it out, but in case others stumble across this: the reason for this is that bfloat16 is incapable of representing the correct output in this case. For example: ```python >>> from ml_dtypes import bfloat16, finfo >>> bfloat16(1828) 1824 ``` Using `numpy.nextafter`, we can see that the next representable bfloat16 value after 1824 is 1832: ```python >>> import numpy as np >>> np.nextafter(bfloat16(1824), bfloat16(2000)) 1832 ``` You should always keep this in mind when working with `bfloat16` values: they encode only an 8bit significand, so computational results will only be accurate to about 1 part in 256. This is by design: `bfloat16` was specifically created for situations where numerical accuracy can be discarded in favor of more efficient computation. "
2562,"以下是一个github上的jax下的一个issue, 标题是(Segfault on multiple gpus (sharding+scan+select))， 内容是 ( Description While running some complicated code on multiple gpus I was consistently getting segmentation faults whenever I was jitting my code. I was able to condense it down to the following reproducer: ```python import jax import jax.numpy as jnp from functools import partial pos_shardings=jax.sharding.PositionalSharding(jax.devices()) k = jax.random.split(jax.random.PRNGKey(123), 2) p = jax.jit(partial(jax.random.uniform, shape=(8,)), out_shardings=pos_shardings.replicate())(k[0]) w = jax.jit(partial(jax.random.uniform, shape=(128,)), out_shardings=pos_shardings)(k[1]) do_select=True  segfault below do_select=False  works def scan_eval(f, w_batched, p):     x0 = jax.tree_map(lambda x: x[0], w_batched)     carry_init = True, jnp.zeros_like(jax.eval_shape(f, x0, p))     def f_(carry, x):         is_first, y_carry = carry         y = f(x, p)         if do_select:             y_reduce = jax.lax.select(is_first, y, y_carry + y)         else:             y_reduce = y_carry + y         return (False, y_reduce), None     (_, res), _1 = jax.lax.scan(f_, carry_init, w_batched, unroll=1)     return res def f(w, p):     return p+w.sum() def test(w,p):     w_batched = w.reshape(16, 1)     res = scan_eval(f, w_batched, p)     return res print('t', test(w, p).sum())  works print('tj', jax.jit(test)(w, p).sum())  segfault on 2 gpus (if do_select=True code path above is executed) ``` output: ``` t 603.76337 [1]    157335 segmentation fault  python3 segfault.py ```  side remark: actually I would like to run this inside of a shard map, which segfaults the same way: ```python from jax.sharding import Mesh, PartitionSpec as P from jax.experimental.shard_map import shard_map mesh = Mesh(jax.devices(), axis_names=(""i"")) in_specs = P(""i""), P() out_specs = P() (shard_map, mesh=mesh, in_specs=in_specs, out_specs=out_specs) def test_shardmap(w, p):     w_batched = w.reshape(16, 1)     res = scan_eval(f, w_batched, p)     return jax.lax.psum(res,'i') print('s', test_shardmap(w, p).sum())  works print('sj', jax.jit(test_shardmap)(w, p).sum())  segfault on 2 gpus (if do_select=True code path above is executed) ```   What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20+cuda12.cudnn89  Which accelerator(s) are you using? 2 x GPU  Additional system info Python 3.9.2, Debian 11  NVIDIA GPU info ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Segfault on multiple gpus (sharding+scan+select)," Description While running some complicated code on multiple gpus I was consistently getting segmentation faults whenever I was jitting my code. I was able to condense it down to the following reproducer: ```python import jax import jax.numpy as jnp from functools import partial pos_shardings=jax.sharding.PositionalSharding(jax.devices()) k = jax.random.split(jax.random.PRNGKey(123), 2) p = jax.jit(partial(jax.random.uniform, shape=(8,)), out_shardings=pos_shardings.replicate())(k[0]) w = jax.jit(partial(jax.random.uniform, shape=(128,)), out_shardings=pos_shardings)(k[1]) do_select=True  segfault below do_select=False  works def scan_eval(f, w_batched, p):     x0 = jax.tree_map(lambda x: x[0], w_batched)     carry_init = True, jnp.zeros_like(jax.eval_shape(f, x0, p))     def f_(carry, x):         is_first, y_carry = carry         y = f(x, p)         if do_select:             y_reduce = jax.lax.select(is_first, y, y_carry + y)         else:             y_reduce = y_carry + y         return (False, y_reduce), None     (_, res), _1 = jax.lax.scan(f_, carry_init, w_batched, unroll=1)     return res def f(w, p):     return p+w.sum() def test(w,p):     w_batched = w.reshape(16, 1)     res = scan_eval(f, w_batched, p)     return res print('t', test(w, p).sum())  works print('tj', jax.jit(test)(w, p).sum())  segfault on 2 gpus (if do_select=True code path above is executed) ``` output: ``` t 603.76337 [1]    157335 segmentation fault  python3 segfault.py ```  side remark: actually I would like to run this inside of a shard map, which segfaults the same way: ```python from jax.sharding import Mesh, PartitionSpec as P from jax.experimental.shard_map import shard_map mesh = Mesh(jax.devices(), axis_names=(""i"")) in_specs = P(""i""), P() out_specs = P() (shard_map, mesh=mesh, in_specs=in_specs, out_specs=out_specs) def test_shardmap(w, p):     w_batched = w.reshape(16, 1)     res = scan_eval(f, w_batched, p)     return jax.lax.psum(res,'i') print('s', test_shardmap(w, p).sum())  works print('sj', jax.jit(test_shardmap)(w, p).sum())  segfault on 2 gpus (if do_select=True code path above is executed) ```   What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20+cuda12.cudnn89  Which accelerator(s) are you using? 2 x GPU  Additional system info Python 3.9.2, Debian 11  NVIDIA GPU info ``` ++  ++++ ```",2023-11-03T22:00:09Z,bug XLA NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/18384,Looks like an XLA bug. Filed https://github.com/openxla/xla/issues/7155,Hi   I tried to execute the mentioned code on cloud VM having 4 T4 GPUs with JAX version 0.4.26 and it executed without any error. Please find the attached screenshot for reference.  Could you please verify the same and let us know if the issue resolved or still exist. Thank you.,> Hi  >  > I tried to execute the mentioned code on cloud VM having 4 T4 GPUs with JAX version 0.4.26 and it executed without any error. Please find the attached screenshot for reference. >  >  > Could you please verify the same and let us know if the issue resolved or still exist. >  > Thank you. I can confirm that this seems to be fixed now.
466,"以下是一个github上的jax下的一个issue, 标题是([XLA:GPU] Consider Triton for all non-pure GEMM fusions)， 内容是 ([XLA:GPU] Consider Triton for all nonpure GEMM fusions This is a big step toward enabling xla_gpu_triton_gemm_any by default. It shows about 1.05x geomean speedup on internal benchmarks (comparable to xla_gpu_triton_gemm_any=true).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,[XLA:GPU] Consider Triton for all non-pure GEMM fusions,[XLA:GPU] Consider Triton for all nonpure GEMM fusions This is a big step toward enabling xla_gpu_triton_gemm_any by default. It shows about 1.05x geomean speedup on internal benchmarks (comparable to xla_gpu_triton_gemm_any=true).,2023-11-03T15:28:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18371
6753,"以下是一个github上的jax下的一个issue, 标题是([Pallas] GPU: CUDA error when kernel launch dimensions >= 65536 in Y or Z)， 内容是 ( Description The following code (adapted from the pallas quickstart) results in a CUDA error, because Pallas tries to launch the resulting CUDA kernel with too large a grid in the Y dimension. ```python import jax from jax import random from jax.experimental import pallas as pl def add_vectors_kernel(x_ref, y_ref, o_ref):   x, y = x_ref[...], y_ref[...]   o_ref[...] = x + y .jit def add_vectors(x: jax.Array, y: jax.Array) > jax.Array:   return pl.pallas_call(add_vectors_kernel,                         out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)                         )(x, y)  add batch dimension add_vectors = jax.vmap(add_vectors)  Standard [N, C] succeeds because leading dimension is mapped to X dim in launch grid print(add_vectors(random.uniform(random.PRNGKey(5), (65536, 64)),                   random.uniform(random.PRNGKey(6), (65536, 64))))  Adding another batch dimension add_vectors = jax.vmap(add_vectors)  Fails, because now N = 65536 is mapped to the Y dim in launch grid print(add_vectors(random.uniform(random.PRNGKey(5), (4, 65536, 64)),                   random.uniform(random.PRNGKey(6), (4, 65536, 64)))) ``` On my machine (jax & jaxlib head, jaxtriton 1f41ec2 and tritonnightly2.1.0.dev20231014192330, cuda 12), the above code prints the following: ``` Launching [N, C] [N, C] success 0.9997495412826538 Launching [P, N, C] 20231102 14:19:11.657404: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: jaxlib/gpu/triton_kernels.cc:259: operation gpuLaunchKernel( kernel, grid[0], grid[1], grid[2], block_dim_x_, 1, 1, shared_mem_bytes_, stream, params, nullptr) failed: CUDA_ERROR_INVALID_VALUE 20231102 14:19:11.657424: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2716] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/triton_kernels.cc:259: operation gpuLaunchKernel( kernel, grid[0], grid[1], grid[2], block_dim_x_, 1, 1, shared_mem_bytes_, stream, params, nullptr) failed: CUDA_ERROR_INVALID_VALUE; current tracing scope: customcall.0; current profiling annotation: XlaModule:hlo_module=jit_add_vectors,program_id=6. jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""pallas_error.py"", line 29, in      r = add_vectors(random.uniform(random.PRNGKey(5), (4, 65536, 64)), jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/triton_kernels.cc:259: operation gpuLaunchKernel( kernel, grid[0], grid[1], grid[2], block_dim_x_, 1, 1, shared_mem_bytes_, stream, params, nullptr) failed: CUDA_ERROR_INVALID_VALUE; current tracing scope: customcall.0; current profiling annotation: XlaModule:hlo_module=jit_add_vectors,program_id=6. ``` For context, I'm hitting this issue in my populationbasedtraining codebase, where I'm training an ensemble of models simultaneously using jax.vmap over my update loop (this expands the training loop inputs from shape [N, C] to [P, N, C], where P is my population size and P = 65536. The core issue is twofold: 1. The pallas_call batching rule internally *prepends* the vmapped dimension to GridMapping 2. When mapping this to a 3D CUDA grid, `_process_grid_to_3d_grid` just assigns the GridMapping dims from left to right to X, Y, and Z in the kernel launch. The problem is that the CUDA API specifies that the max dimensions for a launch are [2^31  1, 65535, 65535] for the 3D grid. I believe the intent is that adjacent threadblocks will be assigned along the x dimension first (like the inner loop in a triple nested for loop over z, y, then x).  Given these semantics, I think it would actually make the most sense for the pallas_call batching rule to *append* the vmapped dimensions to the GridMapping rather than prepending them, but I don't know if that would have negative implications for the TPU backend. That would result in a 2D kernel launch of size [65536, 4] in my above example. For >3 grid dimensions, `_process_grid_to_3d_grid` could collapse the additional dimensions into the Z launch dimension rather than X, although something would need to be done if the resulting Z dimension was greater than 65535. To sidestep all these issues in my own repo and avoid changing the pallas_call batching rule, I've replaced `_process_grid_to_3d_grid` with an implementation that just collapses all the dimensions into X, resulting in a one dimensional kernel launch, leveraging the fact that the X launch dimension can go all the way up to 2^31  1: ```python def _process_grid_to_3d_grid(builder, grid_mapping: GridMapping):   if len(grid_mapping.grid) == 0:     return grid_mapping.grid, []   total_axis_size = np.prod(grid_mapping.grid)   new_grid = (total_axis_size,)   def make_subidx(cur_idx, s):     return (         cur_idx.__floordiv__(s, _builder=builder),         cur_idx.__mod__(s, _builder=builder),     )   grid0 = tl.program_id(0, _builder=builder)   out_indices = [0] * len(grid_mapping.grid)    Preserve existing ordering for nonmapped dims   for i, s in enumerate(grid_mapping.grid):     if i in grid_mapping.mapped_dims:       continue     grid0, out_indices[i] = make_subidx(grid0, s)    For mapped dims, iterate the inner mapped dims to the outer dims. This     follows the pallas_call batching rule that prepends the vmapped dimension.    Intuitively, it makes sense to iterate over the inner mapped dimensions first.   for dim in reversed(grid_mapping.mapped_dims):     s = grid_mapping.grid[dim]     grid0, out_indices[dim] = make_subidx(grid0, s)   return new_grid, out_indices ``` This fixes the above crash for me (and all the pallas tests still pass). The downside of this approach is that the resulting kernel has to do a bit more indexing math. I haven't checked for a performance impact yet (although I doubt it would be significant). Please let me know if I should open a pull request with this solution (or I'd be happy to implement it differently if some other strategy is more desirable).  What jax/jaxlib version are you using? jax c8b7c1b80 (HEAD), jaxlib c8b7c1b80  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info ``` ++  +++ + ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[Pallas] GPU: CUDA error when kernel launch dimensions >= 65536 in Y or Z," Description The following code (adapted from the pallas quickstart) results in a CUDA error, because Pallas tries to launch the resulting CUDA kernel with too large a grid in the Y dimension. ```python import jax from jax import random from jax.experimental import pallas as pl def add_vectors_kernel(x_ref, y_ref, o_ref):   x, y = x_ref[...], y_ref[...]   o_ref[...] = x + y .jit def add_vectors(x: jax.Array, y: jax.Array) > jax.Array:   return pl.pallas_call(add_vectors_kernel,                         out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)                         )(x, y)  add batch dimension add_vectors = jax.vmap(add_vectors)  Standard [N, C] succeeds because leading dimension is mapped to X dim in launch grid print(add_vectors(random.uniform(random.PRNGKey(5), (65536, 64)),                   random.uniform(random.PRNGKey(6), (65536, 64))))  Adding another batch dimension add_vectors = jax.vmap(add_vectors)  Fails, because now N = 65536 is mapped to the Y dim in launch grid print(add_vectors(random.uniform(random.PRNGKey(5), (4, 65536, 64)),                   random.uniform(random.PRNGKey(6), (4, 65536, 64)))) ``` On my machine (jax & jaxlib head, jaxtriton 1f41ec2 and tritonnightly2.1.0.dev20231014192330, cuda 12), the above code prints the following: ``` Launching [N, C] [N, C] success 0.9997495412826538 Launching [P, N, C] 20231102 14:19:11.657404: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: jaxlib/gpu/triton_kernels.cc:259: operation gpuLaunchKernel( kernel, grid[0], grid[1], grid[2], block_dim_x_, 1, 1, shared_mem_bytes_, stream, params, nullptr) failed: CUDA_ERROR_INVALID_VALUE 20231102 14:19:11.657424: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2716] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/triton_kernels.cc:259: operation gpuLaunchKernel( kernel, grid[0], grid[1], grid[2], block_dim_x_, 1, 1, shared_mem_bytes_, stream, params, nullptr) failed: CUDA_ERROR_INVALID_VALUE; current tracing scope: customcall.0; current profiling annotation: XlaModule:hlo_module=jit_add_vectors,program_id=6. jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""pallas_error.py"", line 29, in      r = add_vectors(random.uniform(random.PRNGKey(5), (4, 65536, 64)), jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/triton_kernels.cc:259: operation gpuLaunchKernel( kernel, grid[0], grid[1], grid[2], block_dim_x_, 1, 1, shared_mem_bytes_, stream, params, nullptr) failed: CUDA_ERROR_INVALID_VALUE; current tracing scope: customcall.0; current profiling annotation: XlaModule:hlo_module=jit_add_vectors,program_id=6. ``` For context, I'm hitting this issue in my populationbasedtraining codebase, where I'm training an ensemble of models simultaneously using jax.vmap over my update loop (this expands the training loop inputs from shape [N, C] to [P, N, C], where P is my population size and P = 65536. The core issue is twofold: 1. The pallas_call batching rule internally *prepends* the vmapped dimension to GridMapping 2. When mapping this to a 3D CUDA grid, `_process_grid_to_3d_grid` just assigns the GridMapping dims from left to right to X, Y, and Z in the kernel launch. The problem is that the CUDA API specifies that the max dimensions for a launch are [2^31  1, 65535, 65535] for the 3D grid. I believe the intent is that adjacent threadblocks will be assigned along the x dimension first (like the inner loop in a triple nested for loop over z, y, then x).  Given these semantics, I think it would actually make the most sense for the pallas_call batching rule to *append* the vmapped dimensions to the GridMapping rather than prepending them, but I don't know if that would have negative implications for the TPU backend. That would result in a 2D kernel launch of size [65536, 4] in my above example. For >3 grid dimensions, `_process_grid_to_3d_grid` could collapse the additional dimensions into the Z launch dimension rather than X, although something would need to be done if the resulting Z dimension was greater than 65535. To sidestep all these issues in my own repo and avoid changing the pallas_call batching rule, I've replaced `_process_grid_to_3d_grid` with an implementation that just collapses all the dimensions into X, resulting in a one dimensional kernel launch, leveraging the fact that the X launch dimension can go all the way up to 2^31  1: ```python def _process_grid_to_3d_grid(builder, grid_mapping: GridMapping):   if len(grid_mapping.grid) == 0:     return grid_mapping.grid, []   total_axis_size = np.prod(grid_mapping.grid)   new_grid = (total_axis_size,)   def make_subidx(cur_idx, s):     return (         cur_idx.__floordiv__(s, _builder=builder),         cur_idx.__mod__(s, _builder=builder),     )   grid0 = tl.program_id(0, _builder=builder)   out_indices = [0] * len(grid_mapping.grid)    Preserve existing ordering for nonmapped dims   for i, s in enumerate(grid_mapping.grid):     if i in grid_mapping.mapped_dims:       continue     grid0, out_indices[i] = make_subidx(grid0, s)    For mapped dims, iterate the inner mapped dims to the outer dims. This     follows the pallas_call batching rule that prepends the vmapped dimension.    Intuitively, it makes sense to iterate over the inner mapped dimensions first.   for dim in reversed(grid_mapping.mapped_dims):     s = grid_mapping.grid[dim]     grid0, out_indices[dim] = make_subidx(grid0, s)   return new_grid, out_indices ``` This fixes the above crash for me (and all the pallas tests still pass). The downside of this approach is that the resulting kernel has to do a bit more indexing math. I haven't checked for a performance impact yet (although I doubt it would be significant). Please let me know if I should open a pull request with this solution (or I'd be happy to implement it differently if some other strategy is more desirable).  What jax/jaxlib version are you using? jax c8b7c1b80 (HEAD), jaxlib c8b7c1b80  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info ``` ++  +++ + ```",2023-11-02T21:55:43Z,bug NVIDIA GPU pallas,closed,0,2,https://github.com/jax-ml/jax/issues/18361,"Thanks for the thorough investigation. The batch dimensions do need to be major dimensions on the TPU, so I don't think changing the batching rule is a good idea. Handling this issue in `_process_grid_to_3d_grid` makes sense. I'm wondering if we can end up with a solution in between yours and the current one. Rather than folding all the thread indices into a 1d grid, could we treat the `x` dimension in the grid as the 'catch all' dimension (aka use a reversed CUDA launch grid). Are there any downsides to that approach?","Thanks for the fast response: I've submitted a PR (linked above) that implements roughly your proposed solution. It has to be a bit more complicated, because you can hit easily hit this bug even with less than 3 axes. I've added code that collapses >3 dimensions into X, and then also will collapse further if the resulting Y or Z dimensions are too large."
336,"以下是一个github上的jax下的一个issue, 标题是(Add a test for double donation.)， 内容是 (Add a test for double donation. The underlying issue was fixed some time ago. Fixes https://github.com/google/jax/issues/9635)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add a test for double donation.,Add a test for double donation. The underlying issue was fixed some time ago. Fixes https://github.com/google/jax/issues/9635,2023-11-02T20:55:25Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18359
1549,"以下是一个github上的jax下的一个issue, 标题是(Unimplemented ""discharged_consts"" when discharging while loop effects)， 内容是 ( Description The test below, when added into `pallas_test.py`, yields the error: ``` jax/_src/lax/control_flow/loops.py"", line 1739, in _while_discharge_rule     if discharged_consts: raise NotImplementedError(discharged_consts)   changed this line NotImplementedError: [array([[0.]], dtype=float32)] ``` Test case: ```   def test_while_loop_discharge_consts(self):     start = jnp.int32([[0, 10], [15, 20]])     ntrips = jnp.int32(         random.uniform(random.PRNGKey(2), (8, 8), minval=5., maxval=10.))     .partial(         self.pallas_call,         grid=(2, 2),         in_specs=[             pl.BlockSpec(lambda i, j: (i, j), (1, 1)),   start             pl.BlockSpec(lambda i, j: (i, j), (4, 4)),   ntrips         ],         out_shape=jax.ShapeDtypeStruct((50, 2), jnp.float32),         out_specs=pl.BlockSpec(lambda i, j: (0, 0), (50, 2)))     def test_fn(start, ntrips, out):       start = start[0, 0]       ntrips = ntrips[:, :]       def body(_, arg):         i, = arg         out[i, 1] = jnp.zeros([])         return i  1,       jax.lax.fori_loop(0, ntrips.max(), body, (ntrips.max()  1 + start,))     test_fn(start, ntrips) ```  What jax/jaxlib version are you using? Google internal  Which accelerator(s) are you using? GPU  Additional system info Google internal  NVIDIA GPU info A100)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Unimplemented ""discharged_consts"" when discharging while loop effects"," Description The test below, when added into `pallas_test.py`, yields the error: ``` jax/_src/lax/control_flow/loops.py"", line 1739, in _while_discharge_rule     if discharged_consts: raise NotImplementedError(discharged_consts)   changed this line NotImplementedError: [array([[0.]], dtype=float32)] ``` Test case: ```   def test_while_loop_discharge_consts(self):     start = jnp.int32([[0, 10], [15, 20]])     ntrips = jnp.int32(         random.uniform(random.PRNGKey(2), (8, 8), minval=5., maxval=10.))     .partial(         self.pallas_call,         grid=(2, 2),         in_specs=[             pl.BlockSpec(lambda i, j: (i, j), (1, 1)),   start             pl.BlockSpec(lambda i, j: (i, j), (4, 4)),   ntrips         ],         out_shape=jax.ShapeDtypeStruct((50, 2), jnp.float32),         out_specs=pl.BlockSpec(lambda i, j: (0, 0), (50, 2)))     def test_fn(start, ntrips, out):       start = start[0, 0]       ntrips = ntrips[:, :]       def body(_, arg):         i, = arg         out[i, 1] = jnp.zeros([])         return i  1,       jax.lax.fori_loop(0, ntrips.max(), body, (ntrips.max()  1 + start,))     test_fn(start, ntrips) ```  What jax/jaxlib version are you using? Google internal  Which accelerator(s) are you using? GPU  Additional system info Google internal  NVIDIA GPU info A100",2023-11-02T19:23:53Z,bug pallas,open,0,1,https://github.com/jax-ml/jax/issues/18357,This looks related to CC(Add discharge rules for scan/while); assigning  
5182,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] InconclusiveDimensionOperation for scatter op with leading vmapped symbolic dimension)， 内容是 ( Description Running into an issue when trying to use scatter operations inside a converted batch jax function when gradient is required. Is there a recommended work around if I have an unknown batch dimension? Repro ``` import jax from jax.experimental import jax2tf import tensorflow as tf def func(x):     inds = jp.array([0, 2, 4], dtype=jp.int32)     updates = jp.array([100., 101., 102.], dtype=jp.float32)     x = x.at[jp.array(inds)].set(updates)     return x b = 10 x = tf.random.uniform((b, 5)) with tf.GradientTape() as tape:     tape.watch(x)     func_convert = jax2tf.convert(jax.vmap(func), polymorphic_shapes=['(b, _)'])     res = func_convert(x)     loss = tf.reduce_mean(res) grads = tape.gradient(loss, x) ``` Results in error ``` File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/export/export.py:850, in _get_vjp_fun..fun_vjp_jax(*args_and_out_cts_flat_jax)     846   return res_flat     848 args_flat_jax, out_cts_flat_jax = util.split_list(args_and_out_cts_flat_jax,     849                                                   [len(in_avals)]) > 850 _, pullback_jax = jax.vjp(flattened_primal_fun_jax, *args_flat_jax)     851 return pullback_jax(out_cts_flat_jax)     [... skipping hidden 7 frame] File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/export/export.py:844, in _get_vjp_fun..fun_vjp_jax..flattened_primal_fun_jax(*args_flat)     842 def flattened_primal_fun_jax(*args_flat):     843   args, kwargs = in_tree.unflatten(args_flat) > 844   res = primal_fun(*args, **kwargs)     845   res_flat, _ = tree_util.tree_flatten(res)     846   return res_flat     [... skipping hidden 3 frame] Cell In[628], line 8, in func(x)       6 inds = jp.array([0, 2, 4], dtype=jp.int32)       7 updates = jp.array([100., 101., 102.], dtype=jp.float32) > 8 x = x.at[jp.array(inds)].set(updates)       9 return x File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/_src/numpy/array_methods.py:482, in _IndexUpdateRef.set(self, values, indices_are_sorted, unique_indices, mode)     473 def set(self, values, *, indices_are_sorted=False, unique_indices=False,     474         mode=None):     475   """"""Pure equivalent of ``x[idx] = y``.     476      477   Returns the value of ``x`` that would result from the NumPystyle    (...)     480   See :mod:`jax.ops` for details.     481   """""" > 482   return scatter._scatter_update(self.array, self.index, values, lax.scatter,     483                                  indices_are_sorted=indices_are_sorted,     484                                  unique_indices=unique_indices, mode=mode) File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/_src/ops/scatter.py:78, in _scatter_update(x, idx, y, scatter_op, indices_are_sorted, unique_indices, mode, normalize_indices)      75  XLA gathers and scatters are very similar in structure; the scatter logic      76  is more or less a transpose of the gather equivalent.      77 treedef, static_idx, dynamic_idx = jnp._split_index_for_jit(idx, x.shape) > 78 return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,      79                      indices_are_sorted, unique_indices, mode,      80                      normalize_indices) File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/_src/ops/scatter.py:125, in _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, normalize_indices)     118  Transpose the gather dimensions into scatter dimensions (cf.     119  lax._gather_transpose_rule)     120 dnums = lax.ScatterDimensionNumbers(     121   update_window_dims=indexer.dnums.offset_dims,     122   inserted_window_dims=indexer.dnums.collapsed_slice_dims,     123   scatter_dims_to_operand_dims=indexer.dnums.start_index_map     124 ) > 125 out = scatter_op(     126   x, indexer.gather_indices, y, dnums,     127   indices_are_sorted=indexer.indices_are_sorted or indices_are_sorted,     128   unique_indices=indexer.unique_indices or unique_indices,     129   mode=mode)     130 return lax_internal._convert_element_type(out, dtype, weak_type)     [... skipping hidden 9 frame] File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/export/shape_poly.py:587, in _DimExpr.__int__(self)     585   return op.index(next(iter(self._coeffs.values())))     586 else: > 587   raise InconclusiveDimensionOperation(f""Symbolic dimension '{self}' used in a context that requires a constant"") InconclusiveDimensionOperation: Symbolic dimension 'b' used in a context that requires a constant This error arises for comparison operations with shapes that are nonconstant, and the result of the operation cannot be represented as a boolean value for all values of the symbolic dimensions involved. ```  What jax/jaxlib version are you using? jax v0.4.17, jaxlib v0.4.17, tensorflow v2.14.0  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] InconclusiveDimensionOperation for scatter op with leading vmapped symbolic dimension," Description Running into an issue when trying to use scatter operations inside a converted batch jax function when gradient is required. Is there a recommended work around if I have an unknown batch dimension? Repro ``` import jax from jax.experimental import jax2tf import tensorflow as tf def func(x):     inds = jp.array([0, 2, 4], dtype=jp.int32)     updates = jp.array([100., 101., 102.], dtype=jp.float32)     x = x.at[jp.array(inds)].set(updates)     return x b = 10 x = tf.random.uniform((b, 5)) with tf.GradientTape() as tape:     tape.watch(x)     func_convert = jax2tf.convert(jax.vmap(func), polymorphic_shapes=['(b, _)'])     res = func_convert(x)     loss = tf.reduce_mean(res) grads = tape.gradient(loss, x) ``` Results in error ``` File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/export/export.py:850, in _get_vjp_fun..fun_vjp_jax(*args_and_out_cts_flat_jax)     846   return res_flat     848 args_flat_jax, out_cts_flat_jax = util.split_list(args_and_out_cts_flat_jax,     849                                                   [len(in_avals)]) > 850 _, pullback_jax = jax.vjp(flattened_primal_fun_jax, *args_flat_jax)     851 return pullback_jax(out_cts_flat_jax)     [... skipping hidden 7 frame] File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/export/export.py:844, in _get_vjp_fun..fun_vjp_jax..flattened_primal_fun_jax(*args_flat)     842 def flattened_primal_fun_jax(*args_flat):     843   args, kwargs = in_tree.unflatten(args_flat) > 844   res = primal_fun(*args, **kwargs)     845   res_flat, _ = tree_util.tree_flatten(res)     846   return res_flat     [... skipping hidden 3 frame] Cell In[628], line 8, in func(x)       6 inds = jp.array([0, 2, 4], dtype=jp.int32)       7 updates = jp.array([100., 101., 102.], dtype=jp.float32) > 8 x = x.at[jp.array(inds)].set(updates)       9 return x File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/_src/numpy/array_methods.py:482, in _IndexUpdateRef.set(self, values, indices_are_sorted, unique_indices, mode)     473 def set(self, values, *, indices_are_sorted=False, unique_indices=False,     474         mode=None):     475   """"""Pure equivalent of ``x[idx] = y``.     476      477   Returns the value of ``x`` that would result from the NumPystyle    (...)     480   See :mod:`jax.ops` for details.     481   """""" > 482   return scatter._scatter_update(self.array, self.index, values, lax.scatter,     483                                  indices_are_sorted=indices_are_sorted,     484                                  unique_indices=unique_indices, mode=mode) File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/_src/ops/scatter.py:78, in _scatter_update(x, idx, y, scatter_op, indices_are_sorted, unique_indices, mode, normalize_indices)      75  XLA gathers and scatters are very similar in structure; the scatter logic      76  is more or less a transpose of the gather equivalent.      77 treedef, static_idx, dynamic_idx = jnp._split_index_for_jit(idx, x.shape) > 78 return _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,      79                      indices_are_sorted, unique_indices, mode,      80                      normalize_indices) File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/_src/ops/scatter.py:125, in _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, normalize_indices)     118  Transpose the gather dimensions into scatter dimensions (cf.     119  lax._gather_transpose_rule)     120 dnums = lax.ScatterDimensionNumbers(     121   update_window_dims=indexer.dnums.offset_dims,     122   inserted_window_dims=indexer.dnums.collapsed_slice_dims,     123   scatter_dims_to_operand_dims=indexer.dnums.start_index_map     124 ) > 125 out = scatter_op(     126   x, indexer.gather_indices, y, dnums,     127   indices_are_sorted=indexer.indices_are_sorted or indices_are_sorted,     128   unique_indices=indexer.unique_indices or unique_indices,     129   mode=mode)     130 return lax_internal._convert_element_type(out, dtype, weak_type)     [... skipping hidden 9 frame] File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/export/shape_poly.py:587, in _DimExpr.__int__(self)     585   return op.index(next(iter(self._coeffs.values())))     586 else: > 587   raise InconclusiveDimensionOperation(f""Symbolic dimension '{self}' used in a context that requires a constant"") InconclusiveDimensionOperation: Symbolic dimension 'b' used in a context that requires a constant This error arises for comparison operations with shapes that are nonconstant, and the result of the operation cannot be represented as a boolean value for all values of the symbolic dimensions involved. ```  What jax/jaxlib version are you using? jax v0.4.17, jaxlib v0.4.17, tensorflow v2.14.0  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_",2023-11-01T20:47:41Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18348,"I will look into this, it seems that jvp(scatter) with shape polymorphism is not working properly. I looked at the code a bit, and there is a simpler code path is the update indices are unique. If you are willing to promise that, then use `set(updates, unique_indices=True)` and I think then it should work."
4600,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Unable to wrap tf.function + convert + call_tf when using tf.Variable / stateful call_tf)， 内容是 ( Description Having trouble nesting a call_tf inside a converted jax function when using a stateful transform / tf.Variable. ``` import jax from jax.experimental import jax2tf import tensorflow as tf layer = tf.keras.layers.Dense(10, dtype=tf.float32) layer.build((None, 8)) def func_tf(x):     return layer(x) def func_jax(x):   return jax.numpy.sin(jax2tf.call_tf(func_tf)(x)) .function(autograph=False, jit_compile=False) def outer_func_tf(x):     return jax2tf.convert(func_jax)(x) x = tf.random.uniform((10, 8)) outer_func_tf(x) ``` Error is: ``` WARNING:absl:call_tf works best with a TensorFlow function that does not capture variables or tensors from the context. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdlimitationsofcall_tf for a discussion. The following captures were found [>, >]  NotImplementedError                       Traceback (most recent call last) Cell In[163], line 21      19  Calls `cos_tf` in TF eager mode      20 x = tf.random.uniform((10, 8)) > 21 outer_func_tf(x) File ~/mambaforge/envs//lib/python3.10/sitepackages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback..error_handler(*args, **kwargs)     151 except Exception as e:     152   filtered_tb = _process_traceback_frames(e.__traceback__) > 153   raise e.with_traceback(filtered_tb) from None     154 finally:     155   del filtered_tb Cell In[163], line 17, in outer_func_tf(x)      15 .function(autograph=False, jit_compile=False)      16 def outer_func_tf(x): > 17     return jax2tf.convert(func_jax)(x) File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:402, in convert..converted_fun_tf(*args_tf, **kwargs_tf)     396   impl = GraphSerializationImpl(     397       fun_jax,     398       args_specs=args_specs, kwargs_specs=kwargs_specs,     399       args_flat_tf=args_flat_tf,     400       enable_xla=enable_xla)     401 try: > 402   impl.before_conversion()     404   outs_tree: tree_util.PyTreeDef = None   type: ignore     405   if with_gradient: File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:503, in NativeSerializationImpl.before_conversion(self)     500   _thread_local_state.call_tf_concrete_function_list = _prev_func_list     502 self._restore_context = _restore_context > 503 self.exported = export.export(     504     self.fun_jax,     505     lowering_platform=self.lowering_platform,     506     disabled_checks=self.native_serialization_disabled_checks     507 )(*self.args_specs, **self.kwargs_specs) File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/export/export.py:422, in export..do_export(*args_specs, **kwargs_specs)     420 prev_enable_shape_assertions = shape_poly.thread_local_state.enable_shape_assertions     421 shape_poly.thread_local_state.enable_shape_assertions = enable_shape_assertions > 422 lowered = wrapped_fun_jax.lower(     423     *args_specs, **kwargs_specs,     424     _experimental_lowering_parameters=mlir.LoweringParameters(     425       platforms=lowering_platforms,     426     ))     428 lowering = lowered._lowering   type: ignore     429 _check_lowering(lowering)     [... skipping hidden 12 frame] File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/jax2tf/call_tf.py:497, in _call_tf_lowering(ctx, platform, function_flat_tf, args_flat_sig_tf, has_side_effects, ordered, call_tf_graph, output_avals, *args_op, **_)     494     else:     495       captured_inputs.append(inp) > 497 captured_ops = tuple(     498     mlir.ir_constant(np.asarray(inp))     499     for inp in captured_inputs     500 )     502 if call_tf_graph:     503   with jax2tf_internal.inside_call_tf(): File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/jax2tf/call_tf.py:498, in (.0)     494     else:     495       captured_inputs.append(inp)     497 captured_ops = tuple( > 498     mlir.ir_constant(np.asarray(inp))     499     for inp in captured_inputs     500 )     502 if call_tf_graph:     503   with jax2tf_internal.inside_call_tf(): NotImplementedError: numpy() is only available when eager execution is enabled. ```  What jax/jaxlib version are you using? jax v0.4.17, jaxlib v0.4.17, tensorflow v2.14.0  Which accelerator(s) are you using? CPU  Additional system info Apple M1 Max  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Unable to wrap tf.function + convert + call_tf when using tf.Variable / stateful call_tf," Description Having trouble nesting a call_tf inside a converted jax function when using a stateful transform / tf.Variable. ``` import jax from jax.experimental import jax2tf import tensorflow as tf layer = tf.keras.layers.Dense(10, dtype=tf.float32) layer.build((None, 8)) def func_tf(x):     return layer(x) def func_jax(x):   return jax.numpy.sin(jax2tf.call_tf(func_tf)(x)) .function(autograph=False, jit_compile=False) def outer_func_tf(x):     return jax2tf.convert(func_jax)(x) x = tf.random.uniform((10, 8)) outer_func_tf(x) ``` Error is: ``` WARNING:absl:call_tf works best with a TensorFlow function that does not capture variables or tensors from the context. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdlimitationsofcall_tf for a discussion. The following captures were found [>, >]  NotImplementedError                       Traceback (most recent call last) Cell In[163], line 21      19  Calls `cos_tf` in TF eager mode      20 x = tf.random.uniform((10, 8)) > 21 outer_func_tf(x) File ~/mambaforge/envs//lib/python3.10/sitepackages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback..error_handler(*args, **kwargs)     151 except Exception as e:     152   filtered_tb = _process_traceback_frames(e.__traceback__) > 153   raise e.with_traceback(filtered_tb) from None     154 finally:     155   del filtered_tb Cell In[163], line 17, in outer_func_tf(x)      15 .function(autograph=False, jit_compile=False)      16 def outer_func_tf(x): > 17     return jax2tf.convert(func_jax)(x) File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:402, in convert..converted_fun_tf(*args_tf, **kwargs_tf)     396   impl = GraphSerializationImpl(     397       fun_jax,     398       args_specs=args_specs, kwargs_specs=kwargs_specs,     399       args_flat_tf=args_flat_tf,     400       enable_xla=enable_xla)     401 try: > 402   impl.before_conversion()     404   outs_tree: tree_util.PyTreeDef = None   type: ignore     405   if with_gradient: File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:503, in NativeSerializationImpl.before_conversion(self)     500   _thread_local_state.call_tf_concrete_function_list = _prev_func_list     502 self._restore_context = _restore_context > 503 self.exported = export.export(     504     self.fun_jax,     505     lowering_platform=self.lowering_platform,     506     disabled_checks=self.native_serialization_disabled_checks     507 )(*self.args_specs, **self.kwargs_specs) File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/export/export.py:422, in export..do_export(*args_specs, **kwargs_specs)     420 prev_enable_shape_assertions = shape_poly.thread_local_state.enable_shape_assertions     421 shape_poly.thread_local_state.enable_shape_assertions = enable_shape_assertions > 422 lowered = wrapped_fun_jax.lower(     423     *args_specs, **kwargs_specs,     424     _experimental_lowering_parameters=mlir.LoweringParameters(     425       platforms=lowering_platforms,     426     ))     428 lowering = lowered._lowering   type: ignore     429 _check_lowering(lowering)     [... skipping hidden 12 frame] File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/jax2tf/call_tf.py:497, in _call_tf_lowering(ctx, platform, function_flat_tf, args_flat_sig_tf, has_side_effects, ordered, call_tf_graph, output_avals, *args_op, **_)     494     else:     495       captured_inputs.append(inp) > 497 captured_ops = tuple(     498     mlir.ir_constant(np.asarray(inp))     499     for inp in captured_inputs     500 )     502 if call_tf_graph:     503   with jax2tf_internal.inside_call_tf(): File ~/mambaforge/envs//lib/python3.10/sitepackages/jax/experimental/jax2tf/call_tf.py:498, in (.0)     494     else:     495       captured_inputs.append(inp)     497 captured_ops = tuple( > 498     mlir.ir_constant(np.asarray(inp))     499     for inp in captured_inputs     500 )     502 if call_tf_graph:     503   with jax2tf_internal.inside_call_tf(): NotImplementedError: numpy() is only available when eager execution is enabled. ```  What jax/jaxlib version are you using? jax v0.4.17, jaxlib v0.4.17, tensorflow v2.14.0  Which accelerator(s) are you using? CPU  Additional system info Apple M1 Max  NVIDIA GPU info _No response_",2023-10-30T00:34:28Z,bug,open,0,9,https://github.com/jax-ml/jax/issues/18315,"I found that `keras_core` (I believe soon to be Keras 3) provides a workaround for my example. In particular, the `stateless_call` method of keras_core layers makes it possible.  Here is a contrived example: ``` import os os.environ[""KERAS_BACKEND""] = ""tensorflow"" import jax from jax.experimental import jax2tf import tensorflow as tf import keras_core class Jax2TFLayer(keras_core.layers.Layer):     def __init__(self, units, *args, **kwargs):         super().__init__(*args, **kwargs)         self._layer = keras_core.layers.Dense(units)     def build(self, input_shape):         super().build(input_shape)         self._layer.build(input_shape)     def _call_tf(self, x, trainable_variables, non_trainable_variables):         return self._layer.stateless_call(trainable_variables, non_trainable_variables, x)[0]       def _call_jax(self, x, trainable_variables, non_trainable_variables):         length = 100         def f(x, i):             out = jax2tf.call_tf(self._call_tf)(x, trainable_variables, non_trainable_variables)             return x, out         return jax.lax.scan(f, x, jp.arange(length), length=length)     .function(autograph=False, jit_compile=True)     def call(self, x):         return jax2tf.convert(self._call_jax)(             x, [v.value for v in self._layer.trainable_variables], [v.value for v in self._layer.non_trainable_variables]         )[1] inp = tf.random.uniform((10, 16)) out = layer(inp) ``` This can work great for my use case! but can't use it just yet as I understand polymorphism in call_tf doesn't work with native_serialization which I currently require","I will take a look at this repro, but in general the call_tf mechanism works best when there are not tf.Variables captured in the called function. It is best to refactor the TF code to pass the variable values in and out of the TF function.","I looked at the original repro. The problem here is that when call_tf processes `func_tf` is notices that `func_tf` captures two `tf.Variables`, and it tries to read the values of those variables using `.numpy()`. This works if the code runs in eager mode, but does not work when the outer code is under `tf.function`. You can verify this if you remove `tf.function` from your repro.  Can you please take a look if there is some workaround? ",Sure. > and it tries to read the values of those variables using .numpy() Could you point me where it call `.numpy()` in keras lib ? I will try to see if we can remove it.,The call to `numpy` happens as part of `np.asarray(inp)` here. This works if we are in eager mode bug fails under `tf.function`,"  As  's suggestion, the best way is to rewrite the tf function so it has on captured tf.Variables.    Another choice is https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly but it has performance penalty. I test it on colab https://shorturl.at/bkwHJ and it works. By the way, I try to use same way on jax2tf.py but it is much complicated.   , do you know where is the best location insert this context_manager call ? Thanks","I do not think that we should change jax2tf to force running of TF functions eagerly. If the user starts with a `tf.keras.layer`, which references variables, is it possible to split that function into a set of variable values and a function that takes the variables as inputs?","It would be nice if there were some other way of resolving this. I am loading a saved model (originally JAX code) to run inside more JAX code, which I then want to export into another savedmodel, but it is tricky to use call_tf on the inner savedmodel while also passing in all variables / etc it uses (since they are tf tensors).","For resolving this when functions are not run eagerly, what about modifying _call_tf_lowering to grab the value of variables? This is what I had to do to get my use case working for https://github.com/google/jax/issues/11753 ```python   if tf.executing_eagerly():     np_captured_inputs = [np.asarray(inp) for inp in captured_inputs]   else:     if captured_inputs:       with tf.compat.v1.Session(graph=captured_inputs[0].graph) as sess:           sess.run([v.initializer for v in captured_inputs])           np_captured_inputs = sess.run(captured_inputs)     else:       np_captured_inputs = []   captured_ops = tuple(       mlir.ir_constant(inp)       for inp in np_captured_inputs   ) ```"
715,"以下是一个github上的jax下的一个issue, 标题是([XlaCallModule] Fixes for serialization version 9.)， 内容是 ([XlaCallModule] Fixes for serialization version 9. In version 9, the main function of a serialized module may contain token arguments and outputs. Those do not correspond to actual XlaCallModule op inputs and outputs. In cl/577032011 we had adjusted the input_shapes for the call to RefineDynamicShapes from xla_call_module_op. Here we move the adjustment to input_shapes inside the RefineDynamicShapes, so that it takes effect for all call sites, including those from shape_inference.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[XlaCallModule] Fixes for serialization version 9.,"[XlaCallModule] Fixes for serialization version 9. In version 9, the main function of a serialized module may contain token arguments and outputs. Those do not correspond to actual XlaCallModule op inputs and outputs. In cl/577032011 we had adjusted the input_shapes for the call to RefineDynamicShapes from xla_call_module_op. Here we move the adjustment to input_shapes inside the RefineDynamicShapes, so that it takes effect for all call sites, including those from shape_inference.",2023-10-28T14:33:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18312
2429,"以下是一个github上的jax下的一个issue, 标题是(``custom_jvp`` of ``while_loop`` failes on reverse mode.)， 内容是 ( Description I'm working on a library for numerical quadrature in JAX, with derivatives defined via Leibniz rule. I've defined ``custom_jvp`` for my quadrature functions and it works fine in forward mode, but when trying reverse mode AD I get a ``NotImplementedError`` ```python .partial(jax.custom_jvp, nondiff_argnums=(0,)) def dummy_integrate(fun, a, step, *args):     def condfun(state):         x, f, fx = state         return abs(fx)/abs(f) > 1e2     def bodyfun(state):         x, f, fx = state         x += step         fx = fun(x, *args)         f += fx         return x, f, fx     return jax.lax.while_loop(condfun, bodyfun, (a,fun(a, *args), np.inf))[1] .defjvp    def _dummy_integrate_jvp(fun, primals, tangents):     a, step = primals[:2]     args = primals[2:]     adot, stepdot = tangents[:2]     argsdot = tangents[2:]     f1 = dummy_integrate(fun, *primals)      ignoring boundary terms, derivative of integral is integral of derivative     def df(x, *args):         return jax.jvp(fun, (x, *args), (jnp.zeros_like(x), *argsdot))[1]     f2 = dummy_integrate(df, *primals)     return f1,  f2 fun = lambda x, c : jnp.exp(c*x) a = 1. step = 0.01 c = 1.2 def bar(c):     return dummy_integrate(fun, a, step, c)  this runs fine jax.jacfwd(bar)(1.2)  this fails with the error below jax.jacrev(bar)(1.2) ``` The error: ``` File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py:481, in JaxprTrace.post_process_custom_jvp_call(self, out_tracers, _)     477 def post_process_custom_jvp_call(self, out_tracers, _):     478    This path should only be reachable if we expose a partial eval API     479    unrelated to autodiff, since we raise an error when differentiation with     480    respect to values over which a custom_jvp function closes is detected. > 481   raise NotImplementedError NotImplementedError:  ``` As far as I understand, reverse mode should work here, since the jvp is defined in terms of calls to the primal function. Is there something I'm missing?  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,``custom_jvp`` of ``while_loop`` failes on reverse mode.," Description I'm working on a library for numerical quadrature in JAX, with derivatives defined via Leibniz rule. I've defined ``custom_jvp`` for my quadrature functions and it works fine in forward mode, but when trying reverse mode AD I get a ``NotImplementedError`` ```python .partial(jax.custom_jvp, nondiff_argnums=(0,)) def dummy_integrate(fun, a, step, *args):     def condfun(state):         x, f, fx = state         return abs(fx)/abs(f) > 1e2     def bodyfun(state):         x, f, fx = state         x += step         fx = fun(x, *args)         f += fx         return x, f, fx     return jax.lax.while_loop(condfun, bodyfun, (a,fun(a, *args), np.inf))[1] .defjvp    def _dummy_integrate_jvp(fun, primals, tangents):     a, step = primals[:2]     args = primals[2:]     adot, stepdot = tangents[:2]     argsdot = tangents[2:]     f1 = dummy_integrate(fun, *primals)      ignoring boundary terms, derivative of integral is integral of derivative     def df(x, *args):         return jax.jvp(fun, (x, *args), (jnp.zeros_like(x), *argsdot))[1]     f2 = dummy_integrate(df, *primals)     return f1,  f2 fun = lambda x, c : jnp.exp(c*x) a = 1. step = 0.01 c = 1.2 def bar(c):     return dummy_integrate(fun, a, step, c)  this runs fine jax.jacfwd(bar)(1.2)  this fails with the error below jax.jacrev(bar)(1.2) ``` The error: ``` File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py:481, in JaxprTrace.post_process_custom_jvp_call(self, out_tracers, _)     477 def post_process_custom_jvp_call(self, out_tracers, _):     478    This path should only be reachable if we expose a partial eval API     479    unrelated to autodiff, since we raise an error when differentiation with     480    respect to values over which a custom_jvp function closes is detected. > 481   raise NotImplementedError NotImplementedError:  ``` As far as I understand, reverse mode should work here, since the jvp is defined in terms of calls to the primal function. Is there something I'm missing?  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-10-28T02:24:54Z,bug,open,0,8,https://github.com/jax-ml/jax/issues/18311,"You might be interested in this, https://github.com/patrickkidger/equinox/blob/main/equinox/internal/_loop/loop.py","I seem to get the same error when using scan, so it may not be unique to while loops.","This is interesting. I have no problems using while_loop with custom JVP rule in my case but don't know what is happening in your case (I reproduced the error). Maybe using `dummy_integrate` twice in `_dummy_integrate_jvp` caused the error. I defined `dummy_integrate2`, which is exactly the same as `dummy_integrate` and replaced the second `dummy_integrate` with `dummy_integrate2`. I got the following error: ` ValueError: Reversemode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop. ` This was interesting because I thought JAX does not differentiate `_dummy_integrate_jvp` function!","  I think the issue you're seeing here is because the `df` function (defined inside the custom JVP rule) closes over `argsdot`, but is then passed as a `nondiff_argnum` to `dummy_integrate` (the one that returns `f2`). When using a `custom_jvp` or `custom_vjp` rule, you must ensure any functionvalued arguments don't close over any tracers. (Typically I'd recommend always making them global functions as a way to avoid any possibility of this.) You should rewrite things so that the `argsdot` tracers are passed as formal arguments.   Right, you're now bumping into the second issue with the original code (and which f0uriest will hit after having fixed the above issue)! Explaining this one requires knowing a bit about JAX internals. Buckle up, this gets a bit complicated. JAX performs VJPs (backpropagation) by first figuring out what the tangent part of a JVP rule looks like, and then transposing it (loosely speaking, the ""transpose"" here is the ""run it backwards"" part of backpropagation). In terms of public JAX APIs, what this corresponds to is that `jax.grad` is built by applying  `jax.linear_transpose` to the tangent inputs and outputs of `jax.jvp`. Unfortunately, `jax.lax.while_loop` does not support transposition. Backpropagating through a while loop would require saving the result of every step. As it's a while loop, the number of steps is not known at compile time. That means the amount of memory needed is not known at compile time. And the XLA compiler only performs static memory allocation. Thus, no backpropagating through `jax.lax.while_loop`. The error message you're seeing is to help catch the common case when `jax.grad(jax.lax.while_loop)`, i.e. basically `jax.linear_tranpose(jax.jvp(jax.lax.while_loop))`. In the case of this example, then as you note, we're already inside the JVP rule. Thus what we're actually doing is `jax.linear_transpose(jax.lax.while_loop)`. This is an equally impossible operation to perform, it's just that the error message is only designed to help with the common error described in the previous paragraph.  Phew! Okay, what are the possible fixes? You've got a few possible options: 1) write a custom JAX primitive. This will allow you to define both JVP and transposition rules to your heart's content. 2) fix the issue I first described, then use `jax.custom_vjp`. This won't allow you to perform forwardmode autodiff, though. FWIW, this highlights a usecase for CC(custom_vjp now supports jvps), which adds support for jvpof`custom_vjp`. If the approach there can be accepted + the PR finished off, then you'll be able to do use `custom_vjp` without having to sacrifice support for JVPs.","kidger Thank you, it was very educational! Hope your PR or something similar will be approved. I also want to have a way to define both vjp and jvp for largescale inverse problems.",Thanks for all the information.,"Thanks for all the help kidger. I finally got around to working a bit more on this and running into what might be a related problem. I've fixed `df` to not close over anything, and I'm using `scan` instead of `while_loop` so I think it should be fine to transpose. Updated code: ```python def bounded_while_loop(condfun, bodyfun, init_val, bound):     """"""While loop for bounded number of iterations, implemented using cond and scan.""""""      could do some fancy stuff with checkpointing here like in equinox but the loops      in quadax usually only do ~100 iterations max so probably not worth it.     def scanfun(state, *args):         return jax.lax.cond(condfun(state), bodyfun, lambda x: x, state), None     return jax.lax.scan(scanfun, init_val, None, bound)[0] .partial(jax.custom_jvp, nondiff_argnums=(0,1,2)) def dummy_integrate(fun, a, step, args):     def condfun(state):         x, f, fx = state         return abs(fx)/abs(f) > 1e2     def bodyfun(state):         x, f, fx = state         x += step         fx = fun(x, *args)         f += fx         return x, f, fx     return bounded_while_loop(condfun, bodyfun, (a,fun(a, *args), np.inf), bound=100)[1] .defjvp    def _dummy_integrate_jvp(fun, a, step, primals, tangents):     assert len(primals) == len(tangents) == 1     args = primals[0]      argsdot = tangents[0]     assert isinstance(args, tuple)     assert isinstance(argsdot, tuple)     assert len(args) == len(argsdot) == 1     assert args[0] == 1.2     assert a == 1     assert step == 0.01     f1 = dummy_integrate(fun, a, step, args)      ignoring boundary terms, derivative of integral is integral of derivative     def df(x, vargs, vargsdot):         return jax.jvp(fun, (x, *vargs), (jnp.zeros_like(x), *vargsdot))[1]     f2 = dummy_integrate(df, a, step, (args, argsdot))     return f1,  f2 fun = lambda x, c : jnp.exp(c*x) a = 1. step = 0.01 c = 1.2 def bar(c):     return dummy_integrate(fun, a, step, (c,))  this runs fine jax.jacfwd(bar)(1.2)  this fails with the error below jax.jacrev(bar)(1.2) ``` I'm now getting an assertion error from `_scan_transpose` about undefined primals: ``` JaxStackTraceBeforeTransformation: AssertionError The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: AssertionError                            Traceback (most recent call last) Cell In[23], line 63      60 jax.jacfwd(bar)(1.2)      62  this fails with the error below > 63 jax.jacrev(bar)(1.2) File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/api.py:901, in jacrev..jacfun(*args, **kwargs)     899   y, pullback, aux = _vjp(f_partial, *dyn_args, has_aux=True)     900 tree_map(partial(_check_output_dtype_jacrev, holomorphic), y) > 901 jac = vmap(pullback)(_std_basis(y))     902 jac = jac[0] if isinstance(argnums, int) else jac     903 example_args = dyn_args[0] if isinstance(argnums, int) else dyn_args     [... skipping hidden 8 frame] File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/lax/control_flow/loops.py:707, in _scan_transpose(cts, reverse, length, num_consts, num_carry, jaxpr, linear, unroll, _split_transpose, *args)     705 ires, _ = split_list(consts, [num_ires])     706 _, eres = split_list(xs, [sum(xs_lin)]) > 707 assert not any(ad.is_undefined_primal(r) for r in ires)     708 assert not any(ad.is_undefined_primal(r) for r in eres)     710 carry_avals, y_avals = split_list(jaxpr.out_avals, [num_carry]) AssertionError:  ``` any ideas?","Hmm. So 'undefined primals' actually refer to the tangents  here, `argsdot`  whose values aren't available when transposing. Somehow the JVP rule of the scan is saving such an undefined primal as one of its residual values (those values that the forward pass saves for the backward pass). If you figure this out then I'd be curious to know the answer!"
2369,"以下是一个github上的jax下的一个issue, 标题是(JAX CUDA incompatible with PyTorch in Python 3.11)， 内容是 ( Description Install ipython, torch, and jax on a fresh python 3.11.3 environment: ```bash pip install U ipython torch ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Fire up ipython and try to run a simple command: ``` In [1]: import jax.numpy as jnp In [2]: jnp.linspace(1, 1, 30) WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1698270342.343097   19654 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.  XlaRuntimeError                           Traceback (most recent call last) Cell In[2], line 1 > 1 jnp.linspace(1, 1, 30) File ~/.pyenv/versions/3.11.3/lib/python3.11/sitepackages/jax/_src/numpy/lax_numpy.py:2396, in linspace(start, stop, num, endpoint, retstep, dtype, axis)    2394 num = core.concrete_or_error(operator.index, num, ""'num' argument of jnp.linspace"")    2395 axis = core.concrete_or_error(operator.index, axis, ""'axis' argument of jnp.linspace"") > 2396 return _linspace(start, stop, num, endpoint, retstep, dtype, axis)     [... skipping hidden 14 frame] File ~/.pyenv/versions/3.11.3/lib/python3.11/sitepackages/jax/_src/compiler.py:251, in backend_compile(backend, module, options, host_callbacks)     246   return backend.compile(built_c, compile_options=options,     247                          host_callbacks=host_callbacks)     248  Some backends don't have `host_callbacks` option yet     249  TODO(sharadmv): remove this fallback when all backends allow `compile`     250  to take in `host_callbacks` > 251 return backend.compile(built_c, compile_options=options) XlaRuntimeError: FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas version In [3]: ``` `which ptxas`: no results `nvidiasmi`: CUDA Version 12.2, Driver 535.113.01, NVIDIA GeForce GTX 4090 `uname a`: Linux 6.5.8_1 (Void LInux) `pip list  ++++ ``` Torch still works in this installation. But if I install JAX without torch, it updates some of the libraries and then torch fails to load because of something cuda link related. Can test this out in a separate environment if the details of this are useful.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX CUDA incompatible with PyTorch in Python 3.11," Description Install ipython, torch, and jax on a fresh python 3.11.3 environment: ```bash pip install U ipython torch ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Fire up ipython and try to run a simple command: ``` In [1]: import jax.numpy as jnp In [2]: jnp.linspace(1, 1, 30) WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1698270342.343097   19654 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.  XlaRuntimeError                           Traceback (most recent call last) Cell In[2], line 1 > 1 jnp.linspace(1, 1, 30) File ~/.pyenv/versions/3.11.3/lib/python3.11/sitepackages/jax/_src/numpy/lax_numpy.py:2396, in linspace(start, stop, num, endpoint, retstep, dtype, axis)    2394 num = core.concrete_or_error(operator.index, num, ""'num' argument of jnp.linspace"")    2395 axis = core.concrete_or_error(operator.index, axis, ""'axis' argument of jnp.linspace"") > 2396 return _linspace(start, stop, num, endpoint, retstep, dtype, axis)     [... skipping hidden 14 frame] File ~/.pyenv/versions/3.11.3/lib/python3.11/sitepackages/jax/_src/compiler.py:251, in backend_compile(backend, module, options, host_callbacks)     246   return backend.compile(built_c, compile_options=options,     247                          host_callbacks=host_callbacks)     248  Some backends don't have `host_callbacks` option yet     249  TODO(sharadmv): remove this fallback when all backends allow `compile`     250  to take in `host_callbacks` > 251 return backend.compile(built_c, compile_options=options) XlaRuntimeError: FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas version In [3]: ``` `which ptxas`: no results `nvidiasmi`: CUDA Version 12.2, Driver 535.113.01, NVIDIA GeForce GTX 4090 `uname a`: Linux 6.5.8_1 (Void LInux) `pip list  ++++ ``` Torch still works in this installation. But if I install JAX without torch, it updates some of the libraries and then torch fails to load because of something cuda link related. Can test this out in a separate environment if the details of this are useful.",2023-10-25T21:49:53Z,bug needs info NVIDIA GPU,closed,0,14,https://github.com/jax-ml/jax/issues/18281,"Torch and JAX have incompatible CUDA 12 versions. If you want to mix both, use the CUDA 11.8 packages of both.","I'd also recommend: start over in a fresh virtual environment, or at least make sure you don't have multiple CUDA major version packages installed (they will conflict).","Ok, thank you.","Even using CUDA 11.8 (fresh environment), I get the following when I try to do `jnp.linspace(1, 1, 30)`:  ``` CUDA backend failed to initialize: Unable to load cuFFT. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ```","Can you clarify what packages you now have installed (`pip list`)? Can you also confirm that you started from a fresh virtual environment? (You had both cuda 12 and cuda 11 packages installed before and they will *overwrite* each other's files. I'm concerned you might have an installation that still has some cuda 12 libraries installed, and the easiest way to prevent that is to start with a fresh virtual environment.)","Here is a reproduction script ```bash  install fresh env λ pyenv uninstall 3.11.3   removes python and all its associated packages λ pyenv install 3.11.3  freshly compiles a new binary λ pyenv global 3.11.3  makes sure pip points to this fresh install  install according to https://pytorch.org/getstarted/locally/ λ pip install torch torchvision torchaudio indexurl https://download.pytorch.org/whl/cu118  install according to https://jax.readthedocs.io/en/latest/installation.htmlnvidiagpu λ pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  pip list λ pip list jax' jax                      0.4.19 jaxlib                   0.4.19+cuda11.cudnn86 nvidiacudacupticu11   11.8.87 nvidiacudanvcccu11    11.8.89 nvidiacudanvrtccu11   11.7.99 nvidiacudaruntimecu11 11.8.89 torch                    2.1.0+cu118 torchaudio               2.1.0+cu118 torchmetrics             0.11.4 torchvision              0.16.0+cu118  test λ python c ""import jax.numpy as jnp; jnp.linspace(1, 1, 20)"" ::: CUDA backend failed to initialize: Unable to load cuFFT. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ```","I'm also getting an error running this script for PyMC, which reports: ``` AttributeError: module 'jax.numpy' has no attribute 'product' ```","Note that `jax.numpy.product` was removed in JAX v0.4.16; see CHANGELOG: jax v0.4.16. This was following a similar deprecation of `numpy.product`. You'll either have to change your script to use `jax.numpy.prod`, or use JAX version 0.4.15 or older.","Ok that's fine, but I still can't get JAX to load CUDA; that was just intended to show it can't initialize the backend; linspace still does not work","Something seems broken with your installation. In particular, you are missing many of the packages declared as direct dependencies of `jax[cuda11_pip]`: https://github.com/google/jax/blob/57e33dc3b553fc49095090d0389b7cbf0e694728/setup.pyL115 JAX declares a dependency on `nvidiacufftcu11>=10.9`, which appears to be missing from your installation. That would explain why JAX can't find cufft. I have no idea how this could happen: JAX clearly declares this as a dependency. Try reinstalling `jax[cuda11_pip]`? If that doesn't work, try manually installing the packages listed in the link above?","It does include it...`nvidia` wasn't in the regex, only `cuda`; here is the list filtered to include all nvidia packages, as well as the issue to confirm its the same environment: ``` λ pip list nvidia' jax                      0.4.19 jaxlib                   0.4.19+cuda11.cudnn86 nvidiacublascu11       11.11.3.6 nvidiacudacupticu11   11.8.87 nvidiacudanvcccu11    11.8.89 nvidiacudanvrtccu11   11.7.99 nvidiacudaruntimecu11 11.8.89 nvidiacudnncu11        8.9.4.25 nvidiacufftcu11        10.9.0.58 nvidiacurandcu11       10.2.10.91 nvidiacusolvercu11     11.4.0.1 nvidiacusparsecu11     11.7.4.91 nvidiancclcu11         2.19.3 nvidianvtxcu11         11.7.91 pytorchlightning        2.1.0 torch                    2.1.0+cu118 torch_geometric          2.4.0 torchaudio               2.1.0+cu118 torchmetrics             0.11.4 torchvision              0.16.0+cu118 17:02 danj main /data/repos/priorvgae/pyro λ ipy [TerminalIPythonApp] Loading IPython extension: storemagic [TerminalIPythonApp] Loading IPython extension: autoreload [TerminalIPythonApp] Running code in user namespace: %autoreload 2 [TerminalIPythonApp] Running code in user namespace: import os [TerminalIPythonApp] Running code in user namespace: import sys [TerminalIPythonApp] Running code in user namespace: import numpy as np [TerminalIPythonApp] Running code in user namespace: import pandas as pd [TerminalIPythonApp] Running code in user namespace: from pathlib import Path In [1]: import jax.numpy as jnp In [2]: jnp.linspace(1, 1, 30) CUDA backend failed to initialize: Unable to load cuFFT. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Out[2]: Array([1.        , 0.93103445, 0.862069  , 0.79310346, 0.7241379 ,        0.65517247, 0.5862069 , 0.51724136, 0.44827583, 0.37931037,        0.31034482, 0.24137929, 0.17241383, 0.10344827, 0.03448275,         0.03448275,  0.10344827,  0.1724138 ,  0.24137929,  0.31034482,         0.37931037,  0.44827583,  0.51724136,  0.5862069 ,  0.65517235,         0.7241379 ,  0.79310346,  0.862069  ,  0.93103445,  1.        ],      dtype=float32) In [3]: ```","I've also confirmed this on two machines, both with NVIDIA driver version 535.113.01 and CUDA 12.2. I also can't seem to get logs: ``` λ TF_CPP_MIN_LOG_LEVEL=0 python c ""import jax.numpy as jnp; jnp.linspace(1, 1, 20)"" ::: CUDA backend failed to initialize: Unable to load cuFFT. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ```","I'm not able to reproduce this. I made a *fresh* GCP VM (1 NVIDIA T4 GPU, 16 vCPUs, Ubuntu 22.04 LTS), and did exactly this: ``` sudo apt update sudo apt install nvidiadriver535 python3.10venv sudo reboot python3.10 m venv myenv source myenv/bin/activate pip install torch torchvision torchaudio indexurl https://download.pytorch.org/whl/cu118 pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html python c ""import jax.numpy as jnp; jnp.linspace(1, 1, 20)"" ``` and everything worked fine. Do you have any other CUDA installations on this machine, e.g., in `/usr/local/cuda`? Are any CUDA libraries in `LD_LIBRARY_PATH`?","There aren't, but I was able to manually uninstall and reinstall the offending packages. It appears as though the issue was with pip  it wasn't properly upgrading the packages  somehow it was glossing over packages if they satisfied the requirements (even if they weren't properly installed)."
1214,"以下是一个github上的jax下的一个issue, 标题是(MLIR translation rule for primitive 'pallas_call' not found for platform cuda)， 内容是 ( Description I am using jax_triton for blocksparse matmul kernels. With the installation instructions on jax_triton main, I face no issues. My environment from July 2023 installed    jaxlib v0.4.15.dev20230802+cuda12.cudnn89  jax v0.4.15  jaxtriton v0.1.4 Since pallas was moved to jax.experimental, I want to get rid of the jaxtriton requirement and work with jax/jaxlib only. When trying to run a kernel from jax.experimental.pallas.pallas_call, I get the above mentioned error ``` /jax/_src/interpreters/mlir.py"", line 1389, in jaxpr_subcomp     raise NotImplementedError( NotImplementedError: MLIR translation rule for primitive 'pallas_call' not found for platform cuda ``` Tested with jax/jaxlib versions 0.4.17 to 0.4.19. Is this a system related issue or an issue with jax?  What jax/jaxlib version are you using? jax v0.4.19 jaxlib v0.4.19  Which accelerator(s) are you using? GPU  Additional system info Python 3.12  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,MLIR translation rule for primitive 'pallas_call' not found for platform cuda," Description I am using jax_triton for blocksparse matmul kernels. With the installation instructions on jax_triton main, I face no issues. My environment from July 2023 installed    jaxlib v0.4.15.dev20230802+cuda12.cudnn89  jax v0.4.15  jaxtriton v0.1.4 Since pallas was moved to jax.experimental, I want to get rid of the jaxtriton requirement and work with jax/jaxlib only. When trying to run a kernel from jax.experimental.pallas.pallas_call, I get the above mentioned error ``` /jax/_src/interpreters/mlir.py"", line 1389, in jaxpr_subcomp     raise NotImplementedError( NotImplementedError: MLIR translation rule for primitive 'pallas_call' not found for platform cuda ``` Tested with jax/jaxlib versions 0.4.17 to 0.4.19. Is this a system related issue or an issue with jax?  What jax/jaxlib version are you using? jax v0.4.19 jaxlib v0.4.19  Which accelerator(s) are you using? GPU  Additional system info Python 3.12  NVIDIA GPU info ``` ++  ++ ```",2023-10-25T16:40:31Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18275,You will still need jax_triton. We are investigating some solutions on how to remove its dependency but they will not materialize for some time.,Thanks for letting me know. Looking forward to see it in the future.
1837,"以下是一个github上的jax下的一个issue, 标题是(jax.nn.initializers. Should JAX prefer user-requested dtype over scale parameter?)， 内容是 ( Description Hi,  This is both sort of a bug report or clarification on the semantics of `jax.nn.initializers`. Should JAX prefer the dtype specified by users over the parameter passed to the initializers? Consider the following snippet. ```python import os  os.environ[""JAX_DEFAULT_DTYPE_BITS""] = ""32"" os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""] = ""0.7"" os.environ[""JAX_ENABLE_X64""] = ""1"" import jax import jax.numpy as jnp import numpy as np _default_kernel_init = jax.nn.initializers.orthogonal(np.sqrt(2), dtype=jnp.float32) param = _default_kernel_init(jax.random.PRNGKey(0), (5,5), jnp.float32) print(param.dtype)  float64 ``` Here the scale is specified as a NumPy array and by defaults it uses float64. Due to the way the initializer is implemented, the user requested dtype may be upcasted to float64. Is this the intended behavior?  Some additional context, this is not the behavior used in some NN libraries. For example, in dmhaiku, see  https://github.com/googledeepmind/dmhaiku/blob/402a701364201dbfabd0e93faee16a201dd48a9c/haiku/_src/initializers.pyL299 the scale will be converted to have the same dtype as the user requested one. Within JAX, I think the variance_scaling initializer also works that way. I created a PR https://github.com/google/jax/pull/18266 trying to fix this, but I wasn't sure how to structure the test, where to put it and how to enable test it under the condition that x64 is enabled.  What jax/jaxlib version are you using? HEAD  Which accelerator(s) are you using? CPU/GPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.nn.initializers. Should JAX prefer user-requested dtype over scale parameter?," Description Hi,  This is both sort of a bug report or clarification on the semantics of `jax.nn.initializers`. Should JAX prefer the dtype specified by users over the parameter passed to the initializers? Consider the following snippet. ```python import os  os.environ[""JAX_DEFAULT_DTYPE_BITS""] = ""32"" os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""] = ""0.7"" os.environ[""JAX_ENABLE_X64""] = ""1"" import jax import jax.numpy as jnp import numpy as np _default_kernel_init = jax.nn.initializers.orthogonal(np.sqrt(2), dtype=jnp.float32) param = _default_kernel_init(jax.random.PRNGKey(0), (5,5), jnp.float32) print(param.dtype)  float64 ``` Here the scale is specified as a NumPy array and by defaults it uses float64. Due to the way the initializer is implemented, the user requested dtype may be upcasted to float64. Is this the intended behavior?  Some additional context, this is not the behavior used in some NN libraries. For example, in dmhaiku, see  https://github.com/googledeepmind/dmhaiku/blob/402a701364201dbfabd0e93faee16a201dd48a9c/haiku/_src/initializers.pyL299 the scale will be converted to have the same dtype as the user requested one. Within JAX, I think the variance_scaling initializer also works that way. I created a PR https://github.com/google/jax/pull/18266 trying to fix this, but I wasn't sure how to structure the test, where to put it and how to enable test it under the condition that x64 is enabled.  What jax/jaxlib version are you using? HEAD  Which accelerator(s) are you using? CPU/GPU  Additional system info Linux  NVIDIA GPU info _No response_",2023-10-24T23:47:36Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/18267,"Hi   It appears that this issue has been resolved in JAX 0.4.26. I tested the provided code with multiple JAX versions on colab CPU, GPU and TPU. From JAX 0.4.26 onwards, JAX prefers userrequested dtype when using `jax.nn.initializers`: ```python import os os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""] = ""0.7"" os.environ[""JAX_ENABLE_X64""] = ""1"" import jax import jax.numpy as jnp import numpy as np print(jax.__version__, jax.devices()) _default_kernel_init = jax.nn.initializers.orthogonal(np.sqrt(2), dtype=jnp.float32) param = _default_kernel_init(jax.random.PRNGKey(0), (5,5), jnp.float32) print(param.dtype) ``` Output: ``` 0.4.26 [CpuDevice(id=0)] float32 ``` Attaching the gist for reference. Thank you."
1249,"以下是一个github上的jax下的一个issue, 标题是(Instantiating a very large jax.Array according to a Sharding)， 内容是 (I'd like to initialize an array according to a sharding, rather than initializing it on the default device and then moving it to the sharding. This is required when trying to instantiate arrays that are larger than a single GPU/TPU memory but smaller that many GPU/TPUs combined memory. I'm building an on device replay buffer to work with Podracer style architectures.  In the cases where an algorithm requires a large replay buffer (ApeXDQN, MuZero, Muesli) that replay buffer will need to be instantiated according to a sharding to prevent OOM errors.   There are implementations of the tooling I'm talking about brax, dejax and very recently flashbax. These implementations work well with stateless environments as you can just pmap over the training loop to shard the replay buffer across devices, effectively increasing buffer size. However this doesn't make use of the sharding tooling available through the new unified jax.Array API. Similar to https://github.com/google/jax/issues/4221issue695968528.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Instantiating a very large jax.Array according to a Sharding,"I'd like to initialize an array according to a sharding, rather than initializing it on the default device and then moving it to the sharding. This is required when trying to instantiate arrays that are larger than a single GPU/TPU memory but smaller that many GPU/TPUs combined memory. I'm building an on device replay buffer to work with Podracer style architectures.  In the cases where an algorithm requires a large replay buffer (ApeXDQN, MuZero, Muesli) that replay buffer will need to be instantiated according to a sharding to prevent OOM errors.   There are implementations of the tooling I'm talking about brax, dejax and very recently flashbax. These implementations work well with stateless environments as you can just pmap over the training loop to shard the replay buffer across devices, effectively increasing buffer size. However this doesn't make use of the sharding tooling available through the new unified jax.Array API. Similar to https://github.com/google/jax/issues/4221issue695968528.",2023-10-24T21:05:59Z,enhancement,closed,1,8,https://github.com/jax-ml/jax/issues/18263,"You can try this: ``` .jit def f():   x = jnp.arange(8)   return jax.lax.with_sharding_constraint(x, sharding) f() ``` Another way is this: ``` (jax.jit, out_shardings=sharding) def f():   return jnp.arange(8) ``` This will instantiate `x` directly on the devices as a sharded Array. In other words, `x` will never be on the default device. The above 2 ways are similar but just differ in style and taste. I personally like the first way.","That worked like a charm thank you. Follow up question: Does donate_argnums in jit not play well with the sharding API? I can't seem to implement donate_argnums in a way where the donated buffer is useable. I keep getting:  ``` ""/admin/homewillb/cleanba2/venv39/lib/python3.9/sitepackages/jax/_src/interpreters/mlir.py:766: UserWarning: Some donated buffers were not usable: ShapedArray(uint8[5000,2,4,84,84]), ShapedArray(uint8[5000,2]), ShapedArray(float32[5000,2]), ShapedArray(uint8[5000,2]). See an explanation at https://jax.readthedocs.io/en/latest/faq.htmlbufferdonation.   warnings.warn(""Some donated buffers were not usable:"" ``` The documentation still references deprecated pjit, which makes me ask.","You need to set out_shardings too so then jit will donate properly. donation works by looking at the sharded shape. If you don't specify out_shardings, we don't know what the sharding is going to be until after compilation and that's too late in the stack to set donation bits. There is a fix for this but I just need some time to get it submitted. Until then, you can set out_shardings :)","hmmm I very well could be misusing but here is a minimal example that doesn't use the buffers:  ``` import os os.environ[""XLA_FLAGS""] = 'xla_force_host_platform_device_count=2'  Use 8 CPU devices from functools import partial import jax import jax.numpy as jnp from jax.sharding import PositionalSharding devices = jax.devices() sharding = PositionalSharding(devices) (jax.jit, donate_argnums=0) def insert(rb_state, update):     rb_state = jax.lax.dynamic_update_slice_in_dim(rb_state, update, 0, axis=0)     return jax.lax.with_sharding_constraint(rb_state, sharding) rb_state = jnp.zeros((100,)) rb_state = jax.device_put(rb_state, sharding) update = jnp.ones((10,)) update = jax.device_put(update, sharding) insert(rb_state, update) ```",I think I'm conflating `jax.lax.with_sharding_constraint` with explicitly passing the argument `out_shardings`... However the output of the jitted function is a pytree. Can I pass a pytree of shards to `out_shardings`?,"Yeah, pass in the out_shardings to jit instead of `wsc`. I guess that's one advantage of using out_shardings. > Can I pass a pytree of shards to out_shardings? Yeah I'll fix this though so this never happens again.","THis works for me ``` In [3]: from functools import partial    ...:    ...: import jax    ...: import jax.numpy as jnp    ...: from jax.sharding import PositionalSharding    ...:    ...: devices = jax.devices()    ...: sharding = PositionalSharding(devices)    ...:    ...: (jax.jit, donate_argnums=0, out_shardings=sharding)    ...: def insert(rb_state, update):    ...:     rb_state = jax.lax.dynamic_update_slice_in_dim(rb_state, update, 0, axis=0)    ...:     return rb_state    ...:    ...: rb_state = jnp.zeros((100,))    ...: rb_state = jax.device_put(rb_state, sharding)    ...:    ...: update = jnp.ones((10,))    ...: update = jax.device_put(update, sharding)    ...:    ...: insert(rb_state, update) Out[3]: Array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32) ```","Likewise, I should have closed on my last comment. Greatly appreciate your help!"
6562,"以下是一个github上的jax下的一个issue, 标题是(Repeatedly building JAX causes string substitution failures)， 内容是 ( Description When developing JAX locally one often wants to repeatedly compile jaxlib etc. https://github.com/NVIDIA/JAXToolbox/blob/c50839183fb69d20ab946cd7312521d796dc2c53/.github/container/buildjax.shL272L273 is a wrapper for triggering this build. Executing it twice in succession yields an error: ``` + pip disablepipversioncheck install . Processing /opt/jaxsource   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done Requirement already satisfied: mldtypes>=0.2.0 in /usr/local/lib/python3.10/distpackages (from jax==0.4.20.dev20231023+ga4fd1097b) (0.3.1) Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/distpackages (from jax==0.4.20.dev20231023+ga4fd1097b) (1.26.1) Requirement already satisfied: opteinsum in /usr/local/lib/python3.10/distpackages (from jax==0.4.20.dev20231023+ga4fd1097b) (3.3.0) Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/distpackages (from jax==0.4.20.dev20231023+ga4fd1097b) (1.11.3) Building wheels for collected packages: jax   Building wheel for jax (pyproject.toml) ... error   error: subprocessexitedwitherror   × Building wheel for jax (pyproject.toml) did not run successfully.   │ exit code: 1   ╰─> [50 lines of output]       running bdist_wheel       running build       running build_py       Traceback (most recent call last):         File ""/usr/local/lib/python3.10/distpackages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in            main()         File ""/usr/local/lib/python3.10/distpackages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main           json_out['return_val'] = hook(**hook_input['kwargs'])         File ""/usr/local/lib/python3.10/distpackages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 251, in build_wheel           return _build_backend().build_wheel(wheel_directory, config_settings,         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/build_meta.py"", line 434, in build_wheel           return self._build_with_temp_dir(         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/build_meta.py"", line 419, in _build_with_temp_dir           self.run_setup()         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/build_meta.py"", line 341, in run_setup           exec(code, locals())         File """", line 63, in          File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/__init__.py"", line 103, in setup           return distutils.core.setup(**attrs)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/core.py"", line 185, in setup           return run_commands(dist)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/core.py"", line 201, in run_commands           dist.run_commands()         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/dist.py"", line 969, in run_commands           self.run_command(cmd)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/dist.py"", line 989, in run_command           super().run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/dist.py"", line 988, in run_command           cmd_obj.run()         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/wheel/bdist_wheel.py"", line 364, in run           self.run_command(""build"")         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/cmd.py"", line 318, in run_command           self.distribution.run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/dist.py"", line 989, in run_command           super().run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/dist.py"", line 988, in run_command           cmd_obj.run()         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/command/build.py"", line 131, in run           self.run_command(cmd_name)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/cmd.py"", line 318, in run_command           self.distribution.run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/dist.py"", line 989, in run_command           super().run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/dist.py"", line 988, in run_command           cmd_obj.run()         File ""/opt/jaxsource/jax/version.py"", line 108, in run           _write_version(os.path.join(self.build_lib, pkg_source_path,         File ""/opt/jaxsource/jax/version.py"", line 95, in _write_version           raise RuntimeError(f""Build: could not find {old_version_string!r} in {fname}"")       RuntimeError: Build: could not find '_release_version: str | None = None' in build/lib/jax/version.py       [end of output]   note: This error originates from a subprocess, and is likely not a problem with pip.   ERROR: Failed building wheel for jax Failed to build jax ERROR: Could not build wheels for jax, which is required to install pyproject.tomlbased projects ``` which can be avoided by running ``` cp ./jaxsource/jax/version.py ./jaxsource/build/lib/jax/version.py ``` before every build. These two files differ: ```diff 27c27  _release_version: str = '0.4.20.dev20231023+ga4fd1097b' 89c89    old_version_string = ""_release_version: str = '0.4.20.dev20231023+ga4fd1097b'"" ``` it's annoying to have to do this every time. It seems reasonable to expect that rebuilding jax with no changes should be a ~noop, not something that triggers an error, but perhaps this `buildjax.sh` script is doing something that it shouldn't?  What jax/jaxlib version are you using? development branch (commit a4fd1097b64448d37ddcb7bb2a2f4a488322bdf4)  Which accelerator(s) are you using? GPU (but N/A)  Additional system info Python 3.10, Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Repeatedly building JAX causes string substitution failures," Description When developing JAX locally one often wants to repeatedly compile jaxlib etc. https://github.com/NVIDIA/JAXToolbox/blob/c50839183fb69d20ab946cd7312521d796dc2c53/.github/container/buildjax.shL272L273 is a wrapper for triggering this build. Executing it twice in succession yields an error: ``` + pip disablepipversioncheck install . Processing /opt/jaxsource   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done Requirement already satisfied: mldtypes>=0.2.0 in /usr/local/lib/python3.10/distpackages (from jax==0.4.20.dev20231023+ga4fd1097b) (0.3.1) Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/distpackages (from jax==0.4.20.dev20231023+ga4fd1097b) (1.26.1) Requirement already satisfied: opteinsum in /usr/local/lib/python3.10/distpackages (from jax==0.4.20.dev20231023+ga4fd1097b) (3.3.0) Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/distpackages (from jax==0.4.20.dev20231023+ga4fd1097b) (1.11.3) Building wheels for collected packages: jax   Building wheel for jax (pyproject.toml) ... error   error: subprocessexitedwitherror   × Building wheel for jax (pyproject.toml) did not run successfully.   │ exit code: 1   ╰─> [50 lines of output]       running bdist_wheel       running build       running build_py       Traceback (most recent call last):         File ""/usr/local/lib/python3.10/distpackages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 353, in            main()         File ""/usr/local/lib/python3.10/distpackages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 335, in main           json_out['return_val'] = hook(**hook_input['kwargs'])         File ""/usr/local/lib/python3.10/distpackages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 251, in build_wheel           return _build_backend().build_wheel(wheel_directory, config_settings,         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/build_meta.py"", line 434, in build_wheel           return self._build_with_temp_dir(         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/build_meta.py"", line 419, in _build_with_temp_dir           self.run_setup()         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/build_meta.py"", line 341, in run_setup           exec(code, locals())         File """", line 63, in          File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/__init__.py"", line 103, in setup           return distutils.core.setup(**attrs)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/core.py"", line 185, in setup           return run_commands(dist)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/core.py"", line 201, in run_commands           dist.run_commands()         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/dist.py"", line 969, in run_commands           self.run_command(cmd)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/dist.py"", line 989, in run_command           super().run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/dist.py"", line 988, in run_command           cmd_obj.run()         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/wheel/bdist_wheel.py"", line 364, in run           self.run_command(""build"")         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/cmd.py"", line 318, in run_command           self.distribution.run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/dist.py"", line 989, in run_command           super().run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/dist.py"", line 988, in run_command           cmd_obj.run()         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/command/build.py"", line 131, in run           self.run_command(cmd_name)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/cmd.py"", line 318, in run_command           self.distribution.run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/dist.py"", line 989, in run_command           super().run_command(command)         File ""/tmp/pipbuildenvgiv0hxwa/overlay/local/lib/python3.10/distpackages/setuptools/_distutils/dist.py"", line 988, in run_command           cmd_obj.run()         File ""/opt/jaxsource/jax/version.py"", line 108, in run           _write_version(os.path.join(self.build_lib, pkg_source_path,         File ""/opt/jaxsource/jax/version.py"", line 95, in _write_version           raise RuntimeError(f""Build: could not find {old_version_string!r} in {fname}"")       RuntimeError: Build: could not find '_release_version: str | None = None' in build/lib/jax/version.py       [end of output]   note: This error originates from a subprocess, and is likely not a problem with pip.   ERROR: Failed building wheel for jax Failed to build jax ERROR: Could not build wheels for jax, which is required to install pyproject.tomlbased projects ``` which can be avoided by running ``` cp ./jaxsource/jax/version.py ./jaxsource/build/lib/jax/version.py ``` before every build. These two files differ: ```diff 27c27  _release_version: str = '0.4.20.dev20231023+ga4fd1097b' 89c89    old_version_string = ""_release_version: str = '0.4.20.dev20231023+ga4fd1097b'"" ``` it's annoying to have to do this every time. It seems reasonable to expect that rebuilding jax with no changes should be a ~noop, not something that triggers an error, but perhaps this `buildjax.sh` script is doing something that it shouldn't?  What jax/jaxlib version are you using? development branch (commit a4fd1097b64448d37ddcb7bb2a2f4a488322bdf4)  Which accelerator(s) are you using? GPU (but N/A)  Additional system info Python 3.10, Linux  NVIDIA GPU info _No response_",2023-10-24T14:15:40Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/18252,"This tells me that your build process is finding the JAX sources in the output of the previous build, rather than from the actual source. This seems problematic for many reasons, and so the error is arguably a good thing! I’d recommend deleting the previous build before making a new one, or otherwise ensuring that you’re building from source rather than from the previous build artifact.","Note the repro here is, from the JAX source tree: ``` pip install . pip install . ``` I would have expected that to work?","Yeah, indeed – do you know the mechanism by which the new build process reads from the previous build artifact? Is that intended behavior by `pip`? If it's intended, we should adjust our buildtime source patching script to account for that. If it's not intended, we should figure out why it's happening.","It seems like we just leave some files under`build/` (which is coincidentally our directory containing other things, sigh), which if you delete them the problem goes away. I'm not sure why `pip` doesn't remove the previous state when doing this.",Same issue here. Maybe we can add a if check on `build.py` to check and remove `build/bdist.linuxx86_64` `build/lib/` and `build/__pycache__/` ?,"The thing that's more concerning to me here is that when you run `pip install .`, the contents of the `build/` directory are used instead of the actual source you're trying to install. That seems like a bug in `pip`, and I'd like to understand it rather than work around it. The error you're seeing here is reflecting a real build issue: `pip` is building the package using the wrong set of files!","OK, I'll investigate further before doing anything else with https://github.com/NVIDIA/JAXToolbox/pull/396.","> The thing that's more concerning to me here is that when you run pip install ., the contents of the build/ directory are used instead of the actual source you're trying to install. Can you clarify what you meant by this? As far as I can see, the code that's being **executed** is from the source directory. i.e. in the traceback in the issue, I see `/opt/jaxsource/jax/version.py` (without a `build/`). It seems that by default the `build_py` step only copies files if the source is newer than the destination (or if the destination doesn't exist). In our case, the build scripts modify the destination copy `build/version.py` and leave the source copy `version.py` alone, so the destination is newer and subsequent installs do not update it. This doesn't seem unreasonable. The error then follows because the build scripts only know how to modify an unmodified copy, they cannot update an alreadymodified `version.py`. In light of all of this, I think the workaround of https://github.com/NVIDIA/JAXToolbox/pull/396 is valid. As for an actual fix, the leastmagic solution would probably be to not modify files that are notionally copies from the source directory in the pip build directory... Maybe a more pragmatic / less invasive suggestion is to explicitly remove the build tree copy during the build if it exists, i.e. something like ```diff  a/jax/version.py +++ b/jax/version.py @@ 103,10 +103,13 @@ def _get_cmdclass(pkg_source_path):    class _build_py(build_py_orig):      def run(self): +      _version_py_in_build = os.path.join(self.build_lib, pkg_source_path, +                                          os.path.basename(__file__)) +      if os.path.isfile(_version_py_in_build): +          os.unlink(_version_py_in_build)        super().run()        if _release_version is None:         _write_version(os.path.join(self.build_lib, pkg_source_path,                                     os.path.basename(__file__))) +        _write_version(_version_py_in_build)    class _sdist(sdist_orig):      def make_release_tree(self, base_dir, files): ``` This seems preferable to ""fixing"" the modification logic to cope with alreadymodified files. Thoughts, ?","> It seems that by default the build_py step only copies files if the source is newer than the destination I see – that would explain the issue. > As for an actual fix, the leastmagic solution would probably be to not modify files that are notionally copies from the source directory in the pip build directory... Sure  if you have a suggestion for how to embed buildtime version strings in the package distributions without doing this, I'd be happy to hear it! For what it's worth, the approach we use here (overriding `build_py` to overwrite the version file) is similar to the one used by standard tools like versioneer, though the details differ. Regarding the proposed fix – this looks reasonable to me, assuming the file is copied from the source directory to the build directory during `super().run()`. Would you like to put together a PR?","I opened https://github.com/google/jax/pull/18746 with essentially the workaround above, please take a look  :)"
1696,"以下是一个github上的jax下的一个issue, 标题是(sparse sparse matmul exhausts memory; even when dense version works fine)， 内容是 ( Description Given two BCOO matrices: ``` a = jax.experimental.sparse.random_bcoo(jax.random.PRNGKey(42), shape=(256, 1024)).astype(jnp.int16) b = jax.experimental.sparse.random_bcoo(jax.random.PRNGKey(43), shape=(256, 1024)).astype(jnp.int16) ```` Sparse matmul, implemented with the generalized dot operation  ``` jax.experimental.sparse.bcoo_dot_general(a, b, dimension_numbers=(((1,), (1,)), ((), ()))) ```` runs out of memory:  ``` XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating 175922363761 bytes. ``` Similarly, implementing the matmul by sparsifying (sp?) the infix operator,  ``` jax.experimental.sparse.sparsify(lambda x, y: x @ y.T)(a, b) ``` crashes the interpreter. Multiplying the dense version of these matrices, with  ``` jax.lax.dot_general(a.todense(), b.todense(), (((1,), (1,)), ((), ()))) ``` or,  ``` a.todense() @ b.todense().T ``` goes through just fine (the former is fast, too; kudos).  Any idea why the sparse version isn't working? Maybe I've constructed the BCOOs improperly (I'm still confused by the n_batch property, so maybe toggling it would help?).  EDIT: Tried straight up sparse sparse matmul via,  ``` a @ b.T ``` but got,  ``` XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating 175922363761 bytes. ```  What jax/jaxlib version are you using? jax v0.4.18  Which accelerator(s) are you using? CPU   Additional system info python 3.10.13, ubuntu 20.04  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,sparse sparse matmul exhausts memory; even when dense version works fine," Description Given two BCOO matrices: ``` a = jax.experimental.sparse.random_bcoo(jax.random.PRNGKey(42), shape=(256, 1024)).astype(jnp.int16) b = jax.experimental.sparse.random_bcoo(jax.random.PRNGKey(43), shape=(256, 1024)).astype(jnp.int16) ```` Sparse matmul, implemented with the generalized dot operation  ``` jax.experimental.sparse.bcoo_dot_general(a, b, dimension_numbers=(((1,), (1,)), ((), ()))) ```` runs out of memory:  ``` XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating 175922363761 bytes. ``` Similarly, implementing the matmul by sparsifying (sp?) the infix operator,  ``` jax.experimental.sparse.sparsify(lambda x, y: x @ y.T)(a, b) ``` crashes the interpreter. Multiplying the dense version of these matrices, with  ``` jax.lax.dot_general(a.todense(), b.todense(), (((1,), (1,)), ((), ()))) ``` or,  ``` a.todense() @ b.todense().T ``` goes through just fine (the former is fast, too; kudos).  Any idea why the sparse version isn't working? Maybe I've constructed the BCOOs improperly (I'm still confused by the n_batch property, so maybe toggling it would help?).  EDIT: Tried straight up sparse sparse matmul via,  ``` a @ b.T ``` but got,  ``` XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory allocating 175922363761 bytes. ```  What jax/jaxlib version are you using? jax v0.4.18  Which accelerator(s) are you using? CPU   Additional system info python 3.10.13, ubuntu 20.04  NVIDIA GPU info _No response_",2023-10-24T03:25:38Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18249,Hi  thanks for the report. This is a duplicate of CC(BCOO sparsedense matrixmatrix products produce high memory usage due to nonzero entry copies) – there is some discussion of the issue there.,Oops. My bad. 
1359,"以下是一个github上的jax下的一个issue, 标题是(`BCOO.fromdense` is not compatible with `jax.vmap`)， 内容是 ( Description Something unrelated that I bumped into whilst investigating CC(`BCOO` is not compatible with `jax.vmap`): ```python import jax import jax.numpy as jnp from jax.experimental import sparse matrix = jax.vmap(sparse.BCOO.fromdense)(jnp.arange(16.).reshape(1, 4, 4))    File "".../jax/experimental/sparse/util.py"", line 106, in _count_stored_elements      return int(_count_stored_elements_per_batch(mat, n_batch, n_dense).max(initial=0))             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  jax.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with  shape int32[].  The problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or  `jnp.array(x, int)` instead. ``` I know about the `fromdense(..., n_batch=...)` argment, but I think it'd be reasonable for `fromdense` to occur within traced code.  What jax/jaxlib version are you using? JAX 0.4.19  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`BCOO.fromdense` is not compatible with `jax.vmap`," Description Something unrelated that I bumped into whilst investigating CC(`BCOO` is not compatible with `jax.vmap`): ```python import jax import jax.numpy as jnp from jax.experimental import sparse matrix = jax.vmap(sparse.BCOO.fromdense)(jnp.arange(16.).reshape(1, 4, 4))    File "".../jax/experimental/sparse/util.py"", line 106, in _count_stored_elements      return int(_count_stored_elements_per_batch(mat, n_batch, n_dense).max(initial=0))             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  jax.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: traced array with  shape int32[].  The problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or  `jnp.array(x, int)` instead. ``` I know about the `fromdense(..., n_batch=...)` argment, but I think it'd be reasonable for `fromdense` to occur within traced code.  What jax/jaxlib version are you using? JAX 0.4.19  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-10-23T22:35:57Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/18245,"This is working as intended. You need to pass a static value to `nse` in order to use `BCOO.fromdense` within `vmap` and other JAX transformations, because otherwise the output arrays have datadependent size.","Ah, gotcha. In that case I think treat this as a report that the error message could be improved.","Thanks  I think the intent is that it would hit this line and raise a useful error, but it's clearly not doing that: https://github.com/google/jax/blob/20e583834ee8e8dc6e4d0c43d2eb86cfe9428f58/jax/experimental/sparse/bcoo.pyL289 This is the intended error message: https://github.com/google/jax/blob/20e583834ee8e8dc6e4d0c43d2eb86cfe9428f58/jax/experimental/sparse/bcoo.pyL244L248"
676,"以下是一个github上的jax下的一个issue, 标题是(Cloudpickle and deepcopy support for Jaxprs)， 内容是 (Fixes CC(Allow Jaxprs to be cloudpickleable)  Needed to implement the methods `__reduce_ex__` and `__deepcopy__` on `Primitive` and `SourceInfo`. Added unit tests in a new file `copying_test.py`. This is generally useful in distributed environments, i.e., I can make a transform to create a `shard_map`ed jaxpr, send this jaxpr to each of my worker nodes over the network via Ray / Dask (both of which use cloudpickle), and then just execute the jaxprs.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cloudpickle and deepcopy support for Jaxprs,"Fixes CC(Allow Jaxprs to be cloudpickleable)  Needed to implement the methods `__reduce_ex__` and `__deepcopy__` on `Primitive` and `SourceInfo`. Added unit tests in a new file `copying_test.py`. This is generally useful in distributed environments, i.e., I can make a transform to create a `shard_map`ed jaxpr, send this jaxpr to each of my worker nodes over the network via Ray / Dask (both of which use cloudpickle), and then just execute the jaxprs.",2023-10-23T22:10:52Z,,closed,0,14,https://github.com/jax-ml/jax/issues/18243,Classic case of works on my machine. Not sure what's wrong with the docs build,"> Jitted functions are already pickleable. Not for my usecase. The jaxpr is still picked up in the closure, making the pickle impossible. ```python >>> import jax >>> import cloudpickle >>> def f(a): ...     return a + a ...  >>> j = jax.make_jaxpr(f)(1) >>> cloudpickle.cloudpickle.dumps(j) Traceback (most recent call last):   File """", line 1, in    File ""/home/chase/anaconda3/envs/python311/lib/python3.11/sitepackages/cloudpickle/cloudpickle.py"", line 1479, in dumps     cp.dump(obj)   File ""/home/chase/anaconda3/envs/python311/lib/python3.11/sitepackages/cloudpickle/cloudpickle.py"", line 1245, in dump     return super().dump(obj)            ^^^^^^^^^^^^^^^^^ TypeError: cannot pickle 'jaxlib.xla_extension.Traceback' object >>> >>> jitted = jax.jit(jax.core.jaxpr_as_fun(j)) >>> cloudpickle.cloudpickle.dumps(jitted) Traceback (most recent call last):   File """", line 1, in    File ""/home/chase/anaconda3/envs/python311/lib/python3.11/sitepackages/cloudpickle/cloudpickle.py"", line 1479, in dumps     cp.dump(obj)   File ""/home/chase/anaconda3/envs/python311/lib/python3.11/sitepackages/cloudpickle/cloudpickle.py"", line 1245, in dump     return super().dump(obj)            ^^^^^^^^^^^^^^^^^ TypeError: cannot pickle 'jaxlib.xla_extension.Traceback' object ``` `lower(...)` isn't pickleable either.  ```python >>> s = jax.jit(jax.core.jaxpr_as_fun(j)).lower(1) An NVIDIA GPU may be present on this machine, but a CUDAenabled jaxlib is not installed. Falling back to cpu. >>> s  >>> cloudpickle.cloudpickle.dumps(s) Traceback (most recent call last):   File """", line 1, in    File ""/home/chase/anaconda3/envs/clean11/lib/python3.11/sitepackages/cloudpickle/cloudpickle.py"", line 1479, in dumps     cp.dump(obj)   File ""/home/chase/anaconda3/envs/clean11/lib/python3.11/sitepackages/cloudpickle/cloudpickle.py"", line 1245, in dump     return super().dump(obj)            ^^^^^^^^^^^^^^^^^ TypeError: cannot pickle 'jaxlib.mlir._mlir_libs._mlir.ir.Module' object ```","That wasn't exactly the question, though. You can pickle `jit(f)`, no problem. Why is the jaxpr the right thing to pickle?","> Why is the jaxpr the right thing to pickle? Because my goal is to be able to transform `f` _before_ sending it off to the workers. I can not do that with just the original definition of `f` unless I do that transform instead on every single worker, which is not scalable for my goals. ","> which is not scalable for my goals. For example: if my transform is a randomized optimization, I would need to ensure all of the workers came to the exact same solution. Possible, but incredibly fragile. Solving this first on the client before dispatching prevents this issue forever. ","I want jaxprs to be able to be passed around, modified, and executed using standard distributed python tooling. This could have a wide range of applications beyond just pickling `jit(f)` You could imagine the jaxprs being used with a Ray / Dask server just for the _compiler optimization_ and not even the actual execution. Something like: ```python  Use the remote cluster to optimize your jaxpr. jxpr = best_of([optimize.remote(f, seed) for seed in initial_seeds], my_metrics)  Execute the jaxpr locally on your GPU. jax.jit(jaxpr_as_fun(jxpr))(my_args) ```","I think we can imagine similar such things (in fact,  had a branch doing something similar for a Ray prototype a while back, though we never merged it). While we all broadly agree that this can be potentially useful, it's not free, hence the questions about alternatives. For your immediate, concrete use case, does pickling `jit(f)` suffice? Could you do anything else short of pickling jaxpr?",">  For your immediate, concrete use case, does pickling jit(f) suffice? Sadly not really, I've tried and it's usually pretty painful. > Could you do anything else short of pickling jaxpr? This is the setup I want to solve: I have a jax function, `f`, and I have a transform `trfm`, which can be expensive, and possibly non deterministic. I want to execute `trfm(f)(args...)` in an SPMD fashion on a distributed `mesh` I've come up with several ideas to do this 1. Make all of the transforms execute on the workers.       * This is what is recommended currently in JAX.      * This is reasonable for welldefined, deterministic transforms (i.e., `grad, vmap, vjp`), but can become difficult if `trfm` needs to do expensive optimization searches, or if any randomization is used.       * When developers create their own custom transform as `jax.extend` evolves, they're going to have a bad time ~if~ when they need to debug a nondeterminism bug across a cluster.  2. Make some kind of separate IR that is pickled instead of the `Jaxpr`s.      * I don't think anyone wants to support this lol.  3. Support pickling `Jaxprs`.       * This PR.       * In this setup, our `Jaxpr` can be derived locally if `trfm` is randomized or complicated. We then dispatch this jaxpr with Ray or Dask (via cloudpickle) to the entire mesh.       * All nodes have the exact same `Jaxpr`, so when you run `jit(jaxpr_as_fun(jaxpr))(arg...)` on all nodes simultaneously, the chance of bugs related to nodes running mismatching SPMD binaries drops significantly.  Honestly, these are the only solutions I could think of. I've had to both 1) and 2) at various times in the past and they always are very fragile. If instead I could have 3) JustWork™, it would significantly simplify a lot of the dispatching infrastructure for autopartitioning work.  There could be a forth even easier solution I am missing, but I haven't found it yet. Ideas are welcomed!  >  While we all broadly agree that this can be potentially useful, it's not free, hence the questions about alternatives Nothing is ever free, but what is the cost we're trying to avoid here? We have unit tests that will catch obvious problems quickly, and I am happy to be the one responsible to fix issues related to this (I'll probably be the one hitting issues the most anyway lol).  I can see the argument against adding another global dictionary to manage, but we already use similar global dictionaries for `vmap`, `jit`, `grad`, and well, basically everything! It's not a weird thing to see in the JAX codebase.  I can also see an argument against the name strings being used for infrastructure. I also don't like this either, but the inclusion of `namespace` and possibly also including the `jax.__version__` (I should add this...), should be enough to avoid conflicts/compatibility issues.  There are no name conflicts as it stands today (at least in OSS land), and again issues could likely be caught quickly with good unit tests. So the cost is:  * Manage 40 new LOC, a single extra global dictionary, and a few unit tests.  * Risk that we add new attributes that are not pickleable in the future and have to deal with them.    * Unit tests will likely catch it, and you can `None` them out in a `__reduce_ex__` method like I did with `Traceback`. Annoying but not terrible.  The value:  * Ray and Dask are automatically fully compatible with Jaxprs.  * Unlocks distributed jaxpr optimizations and dispatch. In my opinion it's super worth it. ","At one point not long ago,  made executables experimentally serializable by relying on pickle's persistent ID mechanism. See: https://github.com/google/jax/blob/cd177fd5663e1f25c94e76e6babf6d676c8f5c50/jax/experimental/serialize_executable.pyL62L91 Could something similar be useful here, in particular to decouple a bit from the jax core type definitions (especially if we want this actually decoupled at first)? https://docs.python.org/3/library/pickle.htmlpersistenceofexternalobjects https://docs.python.org/3/library/pickle.htmldispatchtables https://docs.python.org/3/library/pickle.htmlcustomreductionfortypesfunctionsandotherobjects", please take a look at the latest implementation. I think it should be a much more agreeable solution than what I had before.,"With https://github.com/google/jax/pull/18243issuecomment1781294233, why can't this live in your experimental project? In other words, pickling jaxprs doesn't have to live in JAX with the above approach I think :) I would recommend that you try out what's recommended in the above comment and see if that works?",Should we instead be serializing the stablehlo if you want it posttransform?,"> Should we instead be serializing the stablehlo if you want it posttransform? I want to stay in Jaxpr / python land. The serialization is less important than its compatibility with standard python cloud tooling.  > why can't this live in your experimental project? I can do anything I want internally, but I think this is valuable enough to the larger OSS community for it to exist and work easily.",Closing as stale. 
2769,"以下是一个github上的jax下的一个issue, 标题是(jax.lax.psum hangs when using TPU v3-32 )， 内容是 ( Description Greetings, I am attempting to run allreduce (jax.lax.psum) on TPU v332.  Here is the command I used to create the tpu nodes:: ```bash gcloud compute tpus tpuvm create nodev332euw4a project synergyexp \       zone=europewest4a \       acceleratortype=v332 \       version=tpuubuntu2204base \       preemptible gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html"" ``` I have been using this python script to do the all reduce: ```python  jax_allreduce_v3_32.py  The following code snippet will be run on all TPU hosts import sys import jax import jax.numpy as jnp from jax import pmap import timeit import time size_of_allreduce_in_bytes = [512] size_of_allreduce_in_bytes = [1073741824] iterations = 1 for ss in size_of_allreduce_in_bytes:     device_count = jax.device_count()     x = jnp.arange(ss/(4*device_count))     x1 = [x,x,x,x,x,x,x,x]  for 8 TPUs     x2 = jnp.array(x1)     print(f""running collective of size = {x2.nbytes}"")      The number of TPU cores attached to this host     local_device_count = jax.local_device_count()      The psum is performed over all mapped devices across the Pod      xs = jax.numpy.ones(jax.local_device_count())     start_time = time.time()     for i in range(iterations):         r = jax.pmap(lambda xx: jax.lax.psum(xx, 'i'), axis_name='i')(x2)     end_time = time.time()      Print from a single host to avoid duplicated output     if jax.process_index() == 0:         print(f""execution_time = {(end_timestart_time)/iterations}"")         print('global device count:', jax.device_count())         print('local device count:', jax.local_device_count())         print('pmap result:', r) ``` ```bash $ gcloud compute tpus tpuvm ssh nodev332euw4a \     zone=europewest4a \     worker=all \     command=""python3 jax_allreduce_v3_32.py"" Using ssh batch size of 1. Attempting to SSH into 1 nodes with a total of 4 workers. SSH: Attempting to connect to worker 0... SSH: Attempting to connect to worker 1... SSH: Attempting to connect to worker 2... SSH: Attempting to connect to worker 3... running collective of size = 128 running collective of size = 128 running collective of size = 128 ``` and it just hangs there indefinitely.  Any advice on what is going wrong and how to fix this? Best regards, AI  What jax/jaxlib version are you using? jax 0.4.19, jaxlib 0.4.19  Which accelerator(s) are you using? TPU  Additional system info tpuubuntu2204base  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jax.lax.psum hangs when using TPU v3-32 ," Description Greetings, I am attempting to run allreduce (jax.lax.psum) on TPU v332.  Here is the command I used to create the tpu nodes:: ```bash gcloud compute tpus tpuvm create nodev332euw4a project synergyexp \       zone=europewest4a \       acceleratortype=v332 \       version=tpuubuntu2204base \       preemptible gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html"" ``` I have been using this python script to do the all reduce: ```python  jax_allreduce_v3_32.py  The following code snippet will be run on all TPU hosts import sys import jax import jax.numpy as jnp from jax import pmap import timeit import time size_of_allreduce_in_bytes = [512] size_of_allreduce_in_bytes = [1073741824] iterations = 1 for ss in size_of_allreduce_in_bytes:     device_count = jax.device_count()     x = jnp.arange(ss/(4*device_count))     x1 = [x,x,x,x,x,x,x,x]  for 8 TPUs     x2 = jnp.array(x1)     print(f""running collective of size = {x2.nbytes}"")      The number of TPU cores attached to this host     local_device_count = jax.local_device_count()      The psum is performed over all mapped devices across the Pod      xs = jax.numpy.ones(jax.local_device_count())     start_time = time.time()     for i in range(iterations):         r = jax.pmap(lambda xx: jax.lax.psum(xx, 'i'), axis_name='i')(x2)     end_time = time.time()      Print from a single host to avoid duplicated output     if jax.process_index() == 0:         print(f""execution_time = {(end_timestart_time)/iterations}"")         print('global device count:', jax.device_count())         print('local device count:', jax.local_device_count())         print('pmap result:', r) ``` ```bash $ gcloud compute tpus tpuvm ssh nodev332euw4a \     zone=europewest4a \     worker=all \     command=""python3 jax_allreduce_v3_32.py"" Using ssh batch size of 1. Attempting to SSH into 1 nodes with a total of 4 workers. SSH: Attempting to connect to worker 0... SSH: Attempting to connect to worker 1... SSH: Attempting to connect to worker 2... SSH: Attempting to connect to worker 3... running collective of size = 128 running collective of size = 128 running collective of size = 128 ``` and it just hangs there indefinitely.  Any advice on what is going wrong and how to fix this? Best regards, AI  What jax/jaxlib version are you using? jax 0.4.19, jaxlib 0.4.19  Which accelerator(s) are you using? TPU  Additional system info tpuubuntu2204base  NVIDIA GPU info _No response_",2023-10-22T15:45:10Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/18224,"Looks like one of the workers failed to even start for some reason. Your script worked for me (I only modified the print stmt to print out the worker id). ``` Using ssh batch size of 1. Attempting to SSH into 1 nodes with a total of 4 workers. SSH: Attempting to connect to worker 0... SSH: Attempting to connect to worker 1... SSH: Attempting to connect to worker 2... SSH: Attempting to connect to worker 3... running collective of size = 128 for worker: 2 running collective of size = 128 for worker: 3 running collective of size = 128 for worker: 1 running collective of size = 128 for worker: 0 execution_time = 0.276763916015625 global device count: 32 local device count: 8 pmap result: [[ 0. 32. 64. 96.]  [ 0. 32. 64. 96.]  [ 0. 32. 64. 96.]  [ 0. 32. 64. 96.]  [ 0. 32. 64. 96.]  [ 0. 32. 64. 96.]  [ 0. 32. 64. 96.]  [ 0. 32. 64. 96.]] ``` you can also try putting: print(""Initialization complete: %s"" % jax.process_index()) before anything else to see if all the processes have initialized properly. Consider looking through (or attaching) the logs for each process to see if there is some problem initializing: `gcloud compute tpus tpuvm ssh nodev332euw4a     zone=europewest4a     worker=all     command=""cat /tmp/tpu_logs/tpu_driver.INFO""`","Hi , Thank you for your response! I tried it again and it did work. However, when I increased the ``` iterations = 1``` to ``` iterations = 1000``` It hangs and terminates with this message: ``` running collective of size = 128 running collective of size = 128 running collective of size = 128  Command execution on worker 0 failed with exit status 1. Continuing. ``` Here is the log I collected as you suggested: temp.log any idea why this is happening? For context, I am running the iteration 1000 times so that I get an averaged time of the collective communication execution.  Best regards, AI","Hi, another possibility is that you're early exiting from one of the tasks. Consider fetching the final pmap result on all tasks in order to ensure that all tasks have finished the pmap result. Also consider lifting the pmap like so: ``` pmapped_fn = jax.pmap(lambda xx: jax.lax.psum(xx, 'i'), axis_name='i') ...      for i in range(iterations):         r = pmapped_fn(x2) ``` I'm noticing in your log that there are many recompiles.","Hi  , Thank you so much for the suggestion! it looks like it is working! I am new this, so please forgive me if my doubt is too basic: How is this logic executing: ```python pmapped_fn = jax.pmap(lambda xx: jax.lax.psum(xx, 'i'), axis_name='i') for i in range(iterations):         r = pmapped_fn(x2) ``` is the pmap executing `iterations` times or , is the result being assigned to `r` `iterations` times I ask because here is the execution times (in s): 1. iteration = 1        execution_time = 0.27577686309814453 2. iteration = 100    execution_time = 0.003425896167755127 3. iteration = 1000  execution_time = 0.000904909372329712 Best Regards, AI","Because jax uses a jit compilation where pmapped_fn is lazily compiled on the first step, we can expect that the time breakdown is approximately: `jit_time + n_steps * (max(dispatch_overhead, on_device_time))` Because your ondevice function is trivial (just a allreduce of a trivial value), this means that your code is: `jit_time + n_steps * dispatch_overhead`. From your numbers, it looks like the jit time is roughly ~270ms and the dispatch_overhead is ~600us. You can see more with: https://jax.readthedocs.io/en/latest/profiling.html (This will also give you a better estimate of the on_device_time). For performance, you want your entire model step update (fwd + bwd + reducegradients + optimizer apply) to be a single training step. I would also recommend switching to the jit API with something like this (This will trigger the SPMD partitioner and make it easier to try different shardings in the future): ``` mesh = jax.sharding.Mesh(jax.devices(), (""data"",)) weight_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec()) data_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(""data"")) .partial(jax.jit, in_shardings=(weight_sharding, data_sharding, data_sharding), out_shardings=(weight_sharding)) def fake_update_step(weights, x, y):   def fake_loss_fn(weights, x):     x = x @ weights     return ((x  y) ** 2).sum()   loss, grads = jax.value_and_grad(fake_loss_fn)(weights, x)   weights = learning_rate * grads   return weights, loss weights = init_weights_somehow() for x, y in dataset:   weights, _ = fake_update_step(weights, x, y) ```","Thank you so much for the detailed solution! I will take a look into the suggested options.  Best regards, AI","Hi , I had a question regarding  the formula `jit_time + n_steps * dispatch_overhead` How are you calculating `n_steps`? I believe it depends on the type of allreduce I guess my question is what type of allreduce is performed by `jax.lax.psum`? is it Ring, Binary tree, halvingdoubling or something else?"
899,"以下是一个github上的jax下的一个issue, 标题是([export] Add jax.global_constant MLIR attributes for dimension variable arguments)， 内容是 (In presence of shape polymorphism and multiplatorm lowering we pass the global values for the dimension variables and the platform index to all inner HLO functions. At the moment, prior to compilation we run a shape refinement pass to infer which of the arguments of a function carry such global values and to constantfold those values. This inference can yield false positives, e.g., when a userdefined function is called with a constant int32 as the first argument. With this change we do not need to infer anymore the arguments that carry global constants. This is in preparation for a more reliable implementation of shape refinement.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[export] Add jax.global_constant MLIR attributes for dimension variable arguments,"In presence of shape polymorphism and multiplatorm lowering we pass the global values for the dimension variables and the platform index to all inner HLO functions. At the moment, prior to compilation we run a shape refinement pass to infer which of the arguments of a function carry such global values and to constantfold those values. This inference can yield false positives, e.g., when a userdefined function is called with a constant int32 as the first argument. With this change we do not need to infer anymore the arguments that carry global constants. This is in preparation for a more reliable implementation of shape refinement.",2023-10-20T01:54:47Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18202
818,"以下是一个github上的jax下的一个issue, 标题是(Support older TPU which does not have get_library_path.)， 内容是 (Support older TPU which does not have get_library_path. This should fix the CI failure with older TPU (the oldest supported TPU should be updated to 20230912 as well). Tested with: ```  pip install pre libtpunightly==0.1.dev.20230912 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html  ENABLE_PJRT_COMPATIBILITY=true python c ""import jax; print(jax.devices());""  ENABLE_PJRT_COMPATIBILITY=true python tests/api_test.py ``` FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/google/jax/pull/18168 from superbobry:noconfigimport 107930425937d9e9799d96506272eb9fb8389b30)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Support older TPU which does not have get_library_path.,"Support older TPU which does not have get_library_path. This should fix the CI failure with older TPU (the oldest supported TPU should be updated to 20230912 as well). Tested with: ```  pip install pre libtpunightly==0.1.dev.20230912 f https://storage.googleapis.com/jaxreleases/libtpu_releases.html  ENABLE_PJRT_COMPATIBILITY=true python c ""import jax; print(jax.devices());""  ENABLE_PJRT_COMPATIBILITY=true python tests/api_test.py ``` FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/google/jax/pull/18168 from superbobry:noconfigimport 107930425937d9e9799d96506272eb9fb8389b30",2023-10-19T16:53:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18192
316,"以下是一个github上的jax下的一个issue, 标题是([random] make PRNG impl attributes private)， 内容是 (We don't want users to rely on these – instead they should use APIs like `jax.random.key_impl`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[random] make PRNG impl attributes private,We don't want users to rely on these – instead they should use APIs like `jax.random.key_impl`,2023-10-17T21:43:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18161
475,"以下是一个github上的jax下的一个issue, 标题是(Set public module for exported jax.dtypes APIs)， 内容是 (Before: ```python >>> jax.dtypes.prng_key jax._src.dtypes.prng_key ``` After: ```python >>> jax.dtypes.prng_key jax.dtypes.prng_key ``` I'm beginning to look at parameterized dtype annotations, and the `_src` in the representation was annoying me 😁 )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Set public module for exported jax.dtypes APIs,"Before: ```python >>> jax.dtypes.prng_key jax._src.dtypes.prng_key ``` After: ```python >>> jax.dtypes.prng_key jax.dtypes.prng_key ``` I'm beginning to look at parameterized dtype annotations, and the `_src` in the representation was annoying me 😁 ",2023-10-17T20:44:31Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18159
748,"以下是一个github上的jax下的一个issue, 标题是([XlaCallModule] Drop support for dim_args_spec attribute.)， 内容是 ([XlaCallModule] Drop support for dim_args_spec attribute. This attribute was used to support shape polymorphism in versions up to and including version 4. Starting on March 28th 2023 with JAX version 0.4.6 we stopped using this attribute. We are now beyond the 6 month backward compatibility version and we drop support for this attribute. We also increase the minimum supported serialization version to 5. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[XlaCallModule] Drop support for dim_args_spec attribute.,[XlaCallModule] Drop support for dim_args_spec attribute. This attribute was used to support shape polymorphism in versions up to and including version 4. Starting on March 28th 2023 with JAX version 0.4.6 we stopped using this attribute. We are now beyond the 6 month backward compatibility version and we drop support for this attribute. We also increase the minimum supported serialization version to 5. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions,2023-10-16T18:20:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18136
1644,"以下是一个github上的jax下的一个issue, 标题是(Gradient of `jax.random.dirichlet` with small `alpha` element gives `nan`)， 内容是 ( Description Hi, Thanks for building this awesome library! I am still amazed that you can take derivatives of random variables like the Dirichlet distribution, while JAX takes care of the reparameterisations. 🤯 While playing around, I came across an edge case where taking the derivative of a `jax.random.dirichlet` sample gives a `nan`. ```python import jax import jax.numpy as jnp def sum_square(a, key):     y = jax.random.dirichlet(key, a)     return jnp.sum(y**2) alpha = jnp.array([[2.4587069e37, 2.3883429e+00, 3.9916661e+00, 7.5055346e02,          4.4362344e02, 3.1457476e02]], dtype=jnp.float32) grads = jax.grad(sum_square)(alpha, jax.random.key(0))  Output grads:  Array([[           nan, 1.3312605e01,  4.2917587e02, 4.4759928e04,          2.5203576e07, 5.7851091e33]], dtype=float32) ``` It is probably related to the fact that, in this case, `jax.random.dirichlet` samples an exact zero: ```python jax.random.dirichlet(jax.random.key(0), alpha)  Output:  Array([[0.0000000e+00, 1.3876437e01, 8.6123359e01, 2.0013299e06,         3.7550779e10, 1.4884029e36]], dtype=float32) ``` I suppose this is a bug, right? Let me know if there is anything I can do to further clarify. Kind regards, Hylke  What jax/jaxlib version are you using? jax0.4.18/jaxlib0.4.18  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.2, Ubuntu 23.04  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Gradient of `jax.random.dirichlet` with small `alpha` element gives `nan`," Description Hi, Thanks for building this awesome library! I am still amazed that you can take derivatives of random variables like the Dirichlet distribution, while JAX takes care of the reparameterisations. 🤯 While playing around, I came across an edge case where taking the derivative of a `jax.random.dirichlet` sample gives a `nan`. ```python import jax import jax.numpy as jnp def sum_square(a, key):     y = jax.random.dirichlet(key, a)     return jnp.sum(y**2) alpha = jnp.array([[2.4587069e37, 2.3883429e+00, 3.9916661e+00, 7.5055346e02,          4.4362344e02, 3.1457476e02]], dtype=jnp.float32) grads = jax.grad(sum_square)(alpha, jax.random.key(0))  Output grads:  Array([[           nan, 1.3312605e01,  4.2917587e02, 4.4759928e04,          2.5203576e07, 5.7851091e33]], dtype=float32) ``` It is probably related to the fact that, in this case, `jax.random.dirichlet` samples an exact zero: ```python jax.random.dirichlet(jax.random.key(0), alpha)  Output:  Array([[0.0000000e+00, 1.3876437e01, 8.6123359e01, 2.0013299e06,         3.7550779e10, 1.4884029e36]], dtype=float32) ``` I suppose this is a bug, right? Let me know if there is anything I can do to further clarify. Kind regards, Hylke  What jax/jaxlib version are you using? jax0.4.18/jaxlib0.4.18  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.2, Ubuntu 23.04  NVIDIA GPU info _No response_",2023-10-16T13:46:02Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/18129,"Hi  thanks for the report! I think this doesn't necessarily have to do with `dirichlet`, but rather with the fact that the function $f(x) = \sqrt{x^2}$ doesn't have welldefined gradients as $x\to 0$: ```python >>> jax.grad(lambda x: jax.numpy.sqrt(x ** 2))(0.0) Array(nan, dtype=float32, weak_type=True) ``` Given this, I think the example is working as expected, and if you want different behavior near `x=0` you'll need to use a different functional form. What do you think?","Thanks for your response.  I think I don't completely follow you. Where does `sqrt` enter in the equation? Is `jax.random.dirichlet` using `jax.numpy.sqrt` under the hood?   Thanks in advance, Hylke","Oh, sorry, I thought you were computing a norm. Let me look closer at this...","It looks like you're right that there's something in the `dirichlet` implementation that leads to `NaN` gradients for very small values of `alpha`. Here's a shorter repro: ```python def f(a, key):   return jax.random.dirichlet(key, a[None])[0] key = jax.random.key(0) alpha = 1E37 jax.grad(f)(alpha, key)  NaN ```"
1628,"以下是一个github上的jax下的一个issue, 标题是(todense() on jax sparse COO returning only zeros)， 内容是 ( Description I was testing the jax.experimental.sparse library and found that todense() method on the COO sparse matrix is returning a vector of zeros. Code to reproduce (https://colab.research.google.com/drive/1_xxQEqEl5c9NMvYYe8zKsqOS6LJ_V_9P?usp=sharing): ```python coo_shape = (6,1) python_coo = {1:10, 3:20, 5:20}  to array py_indices = list(python_coo.keys()) py_data = list(python_coo.values()) indices = jnp.array([py_indices, [1]*len(py_indices)]).T data = jnp.array(py_data, dtype=jnp.float32) vec_coo = sparse.BCOO((data, indices), shape=coo_shape) vec_coo.todense()  outputs a vector full of zeros Array([[0.],        [0.],        [0.],        [0.],        [0.],        [0.]], dtype=float32) ``` Currently, I am using a workaround with jax.lax.scatter: ```python def vec_coo_todense_wscatter(coo):   indices = coo.indices[:,0]   dnums = jax.lax.ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,))   return jax.lax.scatter(jnp.zeros((coo.shape[0],)), jnp.expand_dims(indices, axis=1), coo.data, dimension_numbers=dnums)  returns a dense vector with the correct data vec_coo_todense_wscatter(vec_coo) Array([ 0., 10.,  0., 20.,  0., 20.], dtype=float32) ```  What jax/jaxlib version are you using? v0.4.16  Which accelerator(s) are you using? Tested on CPU and GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,todense() on jax sparse COO returning only zeros," Description I was testing the jax.experimental.sparse library and found that todense() method on the COO sparse matrix is returning a vector of zeros. Code to reproduce (https://colab.research.google.com/drive/1_xxQEqEl5c9NMvYYe8zKsqOS6LJ_V_9P?usp=sharing): ```python coo_shape = (6,1) python_coo = {1:10, 3:20, 5:20}  to array py_indices = list(python_coo.keys()) py_data = list(python_coo.values()) indices = jnp.array([py_indices, [1]*len(py_indices)]).T data = jnp.array(py_data, dtype=jnp.float32) vec_coo = sparse.BCOO((data, indices), shape=coo_shape) vec_coo.todense()  outputs a vector full of zeros Array([[0.],        [0.],        [0.],        [0.],        [0.],        [0.]], dtype=float32) ``` Currently, I am using a workaround with jax.lax.scatter: ```python def vec_coo_todense_wscatter(coo):   indices = coo.indices[:,0]   dnums = jax.lax.ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,))   return jax.lax.scatter(jnp.zeros((coo.shape[0],)), jnp.expand_dims(indices, axis=1), coo.data, dimension_numbers=dnums)  returns a dense vector with the correct data vec_coo_todense_wscatter(vec_coo) Array([ 0., 10.,  0., 20.,  0., 20.], dtype=float32) ```  What jax/jaxlib version are you using? v0.4.16  Which accelerator(s) are you using? Tested on CPU and GPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-10-15T20:34:52Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/18124,"Update: The ""bug"" that I was experience was related to my indices and the shape of the matrix i believe: ```python indices = jnp.array([py_indices, [1]*len(py_indices)]).T shape = (6,1) ``` As you can see I am wrongly setting the column indices to ""1"" instead of ""0"".  Shouldn't this be impossible when I create a COO matrix? I am attempting to index column 1, but based on the shape, this seems unfeasible. Shouldn't this throw an error?","I think you meant to use this: ```python indices = jnp.array([py_indices, [0]*len(py_indices)]).T ``` For an axis of size 1, an index value 1 is outofbounds (indexing in Python is generally zerobased), and BCOO uses outofbound indexing to indicate padded values that don't affect the array.","Exactly, . As mentioned above, I realized that later. However, shouldn't jax have thrown an error by validating outofbounds?","As I mentioned, JAX's `BCOO` matrix uses outofbound indices to mark padded values in its representation, so the data you passed is valid, it's just that it represents a different matrix than you intended it to represent. But in general, JAX tends not to error on outofbound indices for good reason; see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmloutofboundsindexing",Ty for the explanation :) Really appreciated the link.  Closing the issue
508,"以下是一个github上的jax下的一个issue, 标题是([XlaCallModule] Allow i64 platform index arguments.)， 内容是 ([XlaCallModule] Allow i64 platform index arguments. Previously, for multiplatform serialization the platform index argument was required to be an i32. Now we allow also i64, just like we do for dimension variables. This flexibility is useful for JAX when running in 64bit mode.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[XlaCallModule] Allow i64 platform index arguments.,"[XlaCallModule] Allow i64 platform index arguments. Previously, for multiplatform serialization the platform index argument was required to be an i32. Now we allow also i64, just like we do for dimension variables. This flexibility is useful for JAX when running in 64bit mode.",2023-10-13T21:26:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18114
13512,"以下是一个github上的jax下的一个issue, 标题是(Crash due to out-of-range index access)， 内容是 ( Discussed in https://github.com/google/jax/discussions/18103  Originally posted by **DanPuzzuoli** October 13, 2023 I'm trying to run a jit compiled gradient and I'm getting the following error: ```   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 177, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 256, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(                                                  ^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 167, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py"", line 2657, in bind     return self.bind_with_trace(top_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py"", line 389, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py"", line 869, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1212, in _pjit_call_impl     return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1196, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(                          ^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1132, in _pjit_call_impl_python     lowering_parameters=mlir.LoweringParameters()).compile()                                                    ^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2276, in compile     executable = UnloadedMeshExecutable.from_hlo(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2624, in from_hlo     xla_executable, compile_options = _cached_compilation(                                       ^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2531, in _cached_compilation     xla_executable = compiler.compile_or_get_cached(                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 294, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 256, in backend_compile     return backend.compile(built_c, compile_options=options)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ IndexError: vector ``` This error does not get raised if I don't try to `jit` the gradient function, which makes it difficult to track down what's causing the error. I'm still trying to find a minimal example, but wanted to ask here in case anyone has any insight. On my Macbook this produces: ``` libc++abi: terminating due to uncaught exception of type std::out_of_range: vector Abort trap: 6 ``` with this lldb backtrace: ``` (lldb) bt * thread CC(Python 3 compatibility issues), queue = 'com.apple.mainthread', stop reason = signal SIGABRT   * frame CC(未找到相关数据): 0x000000018d178744 libsystem_kernel.dylib`__pthread_kill + 8     frame CC(Python 3 compatibility issues): 0x000000018d1afc28 libsystem_pthread.dylib`pthread_kill + 288     frame CC(Explicit tuples are not valid function parameters in Python 3): 0x000000018d0bdae8 libsystem_c.dylib`abort + 180     frame CC(Undefined name: from ..core import JaxTuple): 0x000000018d168b84 libc++abi.dylib`abort_message + 132     frame CC(Undefined name: from six.moves import xrange): 0x000000018d1583b4 libc++abi.dylib`demangling_terminate_handler() + 320     frame CC(Building on OSX with CUDA): 0x000000018ce2ee68 libobjc.A.dylib`_objc_terminate() + 160     frame CC(Made a shim to handle configuration without having absl parse flags): 0x000000018d167f48 libc++abi.dylib`std::__terminate(void (*)()) + 16     frame CC(Quickish check): 0x000000018d16ad34 libc++abi.dylib`__cxxabiv1::failed_throw(__cxxabiv1::__cxa_exception*) + 36     frame CC(Quickish check): 0x000000018d16ace0 libc++abi.dylib`__cxa_throw + 140     frame CC(Adding quickstart notebook, and corresponding gitignore rules): 0x000000018d0e371c libc++.1.dylib`std::__1::__throw_out_of_rangeabi:v15006 + 72     frame CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.): 0x000000018d0e7318 libc++.1.dylib`std::__1::__vector_base_common::__throw_out_of_range() const + 24     frame CC(Split out `jax` and `jaxlib` packages): 0x000000015defc80c xla_extension.so`std::__1::__vector_base>::__throw_out_of_range() const + 12     frame CC(Update the quickstart notebook.): 0x000000015defb1c8 xla_extension.so`xla::Shape::tuple_shapes(int) const + 72     frame CC(Fixing logo size so resize is not required): 0x000000015df03444 xla_extension.so`xla::ShapeUtil::GetSubshape(xla::Shape const&, absl::lts_20230125::Span) + 72     frame CC(Add copyright notice to quickstart notebook.): 0x000000015de14c80 xla_extension.so`xla::MutableLiteralBase::CopyFrom(xla::LiteralSlice const&, xla::ShapeIndex const&, xla::ShapeIndex const&, bool) + 156     frame CC(rename in_bdims, out_bdims > in_axes, out_axes): 0x000000015ad22a50 xla_extension.so`xla::HloEvaluator::HandleGetTupleElement(xla::HloInstruction const*) + 744     frame CC(Add wheelbuilding scripts): 0x000000015db94378 xla_extension.so`absl::lts_20230125::Status xla::HloInstruction::Accept(xla::DfsHloVisitorBase*, bool, bool, bool) + 1192     frame CC(Implement np.repeat for scalar repeats.): 0x0000000159fcc41c xla_extension.so`absl::lts_20230125::Status xla::HloComputation::Accept(xla::DfsHloVisitorBase*) const + 388     frame CC(Populate readme): 0x000000015ad0c2bc xla_extension.so`xla::HloEvaluator::Evaluate(xla::HloComputation const&, absl::lts_20230125::Span) + 936     frame CC(Notebook showing how to write gufuncs with vmap): 0x000000015ad25140 xla_extension.so`xla::HloEvaluator::HandleConditional(xla::HloInstruction const*) + 304     frame CC(Fix link in gufuncs notebook): 0x000000015ad0d334 xla_extension.so`xla::HloEvaluator::EvaluateInternal(xla::HloInstruction const*, xla::ShapeIndex const&, bool) + 512     frame CC(Typo): 0x000000015ad0cfd0 xla_extension.so`xla::HloEvaluator::Evaluate(xla::HloInstruction const*, bool) + 228     frame CC(differention > differentiation): 0x000000015ad0da84 xla_extension.so`xla::HloEvaluator::TryEvaluate(xla::HloInstruction const*, xla::Literal*, bool) + 44     frame CC(Typo, Python parens): 0x000000015ac67538 xla_extension.so`xla::HloConstantFolding::Run(xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 732     frame CC(attempt to centerjustify the jax logo in readme): 0x000000015afc07dc xla_extension.so`xla::HloPassPipeline::RunHelper(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 52     frame CC(Barebones neural network and data loading example notebook): 0x000000015afc05d0 xla_extension.so`absl::lts_20230125::StatusOr xla::HloPassPipeline::RunPassesInternal(xla::HloModule*, xla::DebugOptions const&, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&)::'lambda'(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&)::operator()(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) const + 56     frame CC(fix symbolic zero handling in concat transpose): 0x000000015afbe178 xla_extension.so`absl::lts_20230125::StatusOr xla::HloPassPipeline::RunPassesInternal(xla::HloModule*, xla::DebugOptions const&, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 912     frame CC(Cloud TPU Support): 0x000000015afbdcd0 xla_extension.so`xla::HloPassPipeline::Run(xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 100     frame CC(examples/datasets.py doesn’t work in python3): 0x000000015a485df8 xla_extension.so`xla::HloPassFix::RunOnChangedComputationsOnce(xla::HloModule*, xla::HloPassInterface::RunState*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 68     frame CC(Add support for `np.trace` ): 0x000000015a485aac xla_extension.so`xla::HloPassFix::RunToFixPoint(xla::HloModule*, xla::HloPassInterface::RunState*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 144     frame CC(Error on NaN?): 0x000000015a485448 xla_extension.so`xla::HloPassFix::Run(xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 404     frame CC(Bug in examples?): 0x000000015afc07dc xla_extension.so`xla::HloPassPipeline::RunHelper(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 52     frame CC(Fix the bug in classifier example, batching_test and README): 0x000000015afc05d0 xla_extension.so`absl::lts_20230125::StatusOr xla::HloPassPipeline::RunPassesInternal(xla::HloModule*, xla::DebugOptions const&, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&)::'lambda'(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&)::operator()(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) const + 56     frame CC(Broadcasting of size0 dimensions not implemented): 0x000000015afbe178 xla_extension.so`absl::lts_20230125::StatusOr xla::HloPassPipeline::RunPassesInternal(xla::HloModule*, xla::DebugOptions const&, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 912     frame CC(minor spelling tweaks): 0x000000015afbdcd0 xla_extension.so`xla::HloPassPipeline::Run(xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 100     frame CC(CUDA90 and py3 ): 0x000000015a46d7c4 xla_extension.so`xla::cpu::CpuCompiler::RunHloPassesThroughLayoutAssn(xla::HloModule*, bool, xla::cpu::LLVMTargetMachineFeatures*, bool) + 5368     frame CC(add dot_general batching rule): 0x000000015a4702b4 xla_extension.so`xla::cpu::CpuCompiler::RunHloPasses(xla::HloModule*, bool, llvm::TargetMachine*, bool) + 100     frame CC(np.einsum support): 0x000000015a470508 xla_extension.so`xla::cpu::CpuCompiler::RunHloPasses(std::__1::unique_ptr>, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&) + 468     frame CC(Require protobuf 3.6.0 or later): 0x000000015a3fcaf8 xla_extension.so`xla::TfrtCpuClient::Compile(xla::XlaComputation const&, xla::CompileOptions) + 1600 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Crash due to out-of-range index access," Discussed in https://github.com/google/jax/discussions/18103  Originally posted by **DanPuzzuoli** October 13, 2023 I'm trying to run a jit compiled gradient and I'm getting the following error: ```   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 177, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 256, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(                                                  ^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 167, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py"", line 2657, in bind     return self.bind_with_trace(top_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py"", line 389, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py"", line 869, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1212, in _pjit_call_impl     return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1196, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(                          ^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1132, in _pjit_call_impl_python     lowering_parameters=mlir.LoweringParameters()).compile()                                                    ^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2276, in compile     executable = UnloadedMeshExecutable.from_hlo(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2624, in from_hlo     xla_executable, compile_options = _cached_compilation(                                       ^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2531, in _cached_compilation     xla_executable = compiler.compile_or_get_cached(                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 294, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/compiler.py"", line 256, in backend_compile     return backend.compile(built_c, compile_options=options)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ IndexError: vector ``` This error does not get raised if I don't try to `jit` the gradient function, which makes it difficult to track down what's causing the error. I'm still trying to find a minimal example, but wanted to ask here in case anyone has any insight. On my Macbook this produces: ``` libc++abi: terminating due to uncaught exception of type std::out_of_range: vector Abort trap: 6 ``` with this lldb backtrace: ``` (lldb) bt * thread CC(Python 3 compatibility issues), queue = 'com.apple.mainthread', stop reason = signal SIGABRT   * frame CC(未找到相关数据): 0x000000018d178744 libsystem_kernel.dylib`__pthread_kill + 8     frame CC(Python 3 compatibility issues): 0x000000018d1afc28 libsystem_pthread.dylib`pthread_kill + 288     frame CC(Explicit tuples are not valid function parameters in Python 3): 0x000000018d0bdae8 libsystem_c.dylib`abort + 180     frame CC(Undefined name: from ..core import JaxTuple): 0x000000018d168b84 libc++abi.dylib`abort_message + 132     frame CC(Undefined name: from six.moves import xrange): 0x000000018d1583b4 libc++abi.dylib`demangling_terminate_handler() + 320     frame CC(Building on OSX with CUDA): 0x000000018ce2ee68 libobjc.A.dylib`_objc_terminate() + 160     frame CC(Made a shim to handle configuration without having absl parse flags): 0x000000018d167f48 libc++abi.dylib`std::__terminate(void (*)()) + 16     frame CC(Quickish check): 0x000000018d16ad34 libc++abi.dylib`__cxxabiv1::failed_throw(__cxxabiv1::__cxa_exception*) + 36     frame CC(Quickish check): 0x000000018d16ace0 libc++abi.dylib`__cxa_throw + 140     frame CC(Adding quickstart notebook, and corresponding gitignore rules): 0x000000018d0e371c libc++.1.dylib`std::__1::__throw_out_of_rangeabi:v15006 + 72     frame CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.): 0x000000018d0e7318 libc++.1.dylib`std::__1::__vector_base_common::__throw_out_of_range() const + 24     frame CC(Split out `jax` and `jaxlib` packages): 0x000000015defc80c xla_extension.so`std::__1::__vector_base>::__throw_out_of_range() const + 12     frame CC(Update the quickstart notebook.): 0x000000015defb1c8 xla_extension.so`xla::Shape::tuple_shapes(int) const + 72     frame CC(Fixing logo size so resize is not required): 0x000000015df03444 xla_extension.so`xla::ShapeUtil::GetSubshape(xla::Shape const&, absl::lts_20230125::Span) + 72     frame CC(Add copyright notice to quickstart notebook.): 0x000000015de14c80 xla_extension.so`xla::MutableLiteralBase::CopyFrom(xla::LiteralSlice const&, xla::ShapeIndex const&, xla::ShapeIndex const&, bool) + 156     frame CC(rename in_bdims, out_bdims > in_axes, out_axes): 0x000000015ad22a50 xla_extension.so`xla::HloEvaluator::HandleGetTupleElement(xla::HloInstruction const*) + 744     frame CC(Add wheelbuilding scripts): 0x000000015db94378 xla_extension.so`absl::lts_20230125::Status xla::HloInstruction::Accept(xla::DfsHloVisitorBase*, bool, bool, bool) + 1192     frame CC(Implement np.repeat for scalar repeats.): 0x0000000159fcc41c xla_extension.so`absl::lts_20230125::Status xla::HloComputation::Accept(xla::DfsHloVisitorBase*) const + 388     frame CC(Populate readme): 0x000000015ad0c2bc xla_extension.so`xla::HloEvaluator::Evaluate(xla::HloComputation const&, absl::lts_20230125::Span) + 936     frame CC(Notebook showing how to write gufuncs with vmap): 0x000000015ad25140 xla_extension.so`xla::HloEvaluator::HandleConditional(xla::HloInstruction const*) + 304     frame CC(Fix link in gufuncs notebook): 0x000000015ad0d334 xla_extension.so`xla::HloEvaluator::EvaluateInternal(xla::HloInstruction const*, xla::ShapeIndex const&, bool) + 512     frame CC(Typo): 0x000000015ad0cfd0 xla_extension.so`xla::HloEvaluator::Evaluate(xla::HloInstruction const*, bool) + 228     frame CC(differention > differentiation): 0x000000015ad0da84 xla_extension.so`xla::HloEvaluator::TryEvaluate(xla::HloInstruction const*, xla::Literal*, bool) + 44     frame CC(Typo, Python parens): 0x000000015ac67538 xla_extension.so`xla::HloConstantFolding::Run(xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 732     frame CC(attempt to centerjustify the jax logo in readme): 0x000000015afc07dc xla_extension.so`xla::HloPassPipeline::RunHelper(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 52     frame CC(Barebones neural network and data loading example notebook): 0x000000015afc05d0 xla_extension.so`absl::lts_20230125::StatusOr xla::HloPassPipeline::RunPassesInternal(xla::HloModule*, xla::DebugOptions const&, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&)::'lambda'(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&)::operator()(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) const + 56     frame CC(fix symbolic zero handling in concat transpose): 0x000000015afbe178 xla_extension.so`absl::lts_20230125::StatusOr xla::HloPassPipeline::RunPassesInternal(xla::HloModule*, xla::DebugOptions const&, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 912     frame CC(Cloud TPU Support): 0x000000015afbdcd0 xla_extension.so`xla::HloPassPipeline::Run(xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 100     frame CC(examples/datasets.py doesn’t work in python3): 0x000000015a485df8 xla_extension.so`xla::HloPassFix::RunOnChangedComputationsOnce(xla::HloModule*, xla::HloPassInterface::RunState*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 68     frame CC(Add support for `np.trace` ): 0x000000015a485aac xla_extension.so`xla::HloPassFix::RunToFixPoint(xla::HloModule*, xla::HloPassInterface::RunState*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 144     frame CC(Error on NaN?): 0x000000015a485448 xla_extension.so`xla::HloPassFix::Run(xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 404     frame CC(Bug in examples?): 0x000000015afc07dc xla_extension.so`xla::HloPassPipeline::RunHelper(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 52     frame CC(Fix the bug in classifier example, batching_test and README): 0x000000015afc05d0 xla_extension.so`absl::lts_20230125::StatusOr xla::HloPassPipeline::RunPassesInternal(xla::HloModule*, xla::DebugOptions const&, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&)::'lambda'(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&)::operator()(xla::HloPassInterface*, xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) const + 56     frame CC(Broadcasting of size0 dimensions not implemented): 0x000000015afbe178 xla_extension.so`absl::lts_20230125::StatusOr xla::HloPassPipeline::RunPassesInternal(xla::HloModule*, xla::DebugOptions const&, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 912     frame CC(minor spelling tweaks): 0x000000015afbdcd0 xla_extension.so`xla::HloPassPipeline::Run(xla::HloModule*, absl::lts_20230125::flat_hash_set>, absl::lts_20230125::container_internal::StringHash, absl::lts_20230125::container_internal::StringEq, std::__1::allocator>>> const&) + 100     frame CC(CUDA90 and py3 ): 0x000000015a46d7c4 xla_extension.so`xla::cpu::CpuCompiler::RunHloPassesThroughLayoutAssn(xla::HloModule*, bool, xla::cpu::LLVMTargetMachineFeatures*, bool) + 5368     frame CC(add dot_general batching rule): 0x000000015a4702b4 xla_extension.so`xla::cpu::CpuCompiler::RunHloPasses(xla::HloModule*, bool, llvm::TargetMachine*, bool) + 100     frame CC(np.einsum support): 0x000000015a470508 xla_extension.so`xla::cpu::CpuCompiler::RunHloPasses(std::__1::unique_ptr>, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&) + 468     frame CC(Require protobuf 3.6.0 or later): 0x000000015a3fcaf8 xla_extension.so`xla::TfrtCpuClient::Compile(xla::XlaComputation const&, xla::CompileOptions) + 1600 ```",2023-10-13T17:53:27Z,,closed,0,1,https://github.com/jax-ml/jax/issues/18106,"openxla/xla CC([jax2tf] Fix bug in dot_general.) should have fixed this, and the fix should be present in the next jaxlib."
575,"以下是一个github上的jax下的一个issue, 标题是(Setup compatibility testing to support the oldest supported libtpu ve…)， 内容是 (…rsion (12 weeks)  A new matrix option was added to jaxlibversion matrix  Expected to immediately fail due to strict jaxlib version check (requires libtpu to have the same API verison as jaxlib, fails otherwise)  Due to expected failure, so chat will be sent when the jaxlibversion matrix is ""nightly+oldest_supported_libtpu"")请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,Setup compatibility testing to support the oldest supported libtpu ve…,"…rsion (12 weeks)  A new matrix option was added to jaxlibversion matrix  Expected to immediately fail due to strict jaxlib version check (requires libtpu to have the same API verison as jaxlib, fails otherwise)  Due to expected failure, so chat will be sent when the jaxlibversion matrix is ""nightly+oldest_supported_libtpu""",2023-10-11T22:05:17Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/18070, cc,"Looks good! We should test this to double check it works, but we can commit and then try :)"
3734,"以下是一个github上的jax下的一个issue, 标题是(Cannot call custom primitive with ordered effect twice in JVP rule)， 内容是 ( Description We are currently migrating mpi4jax from manual XLA tokens to the new ordered effects. The only remaining test failures are related to our JVP rules which call the underlying primitive twice (once on the primal and once on the tangent). Commenting out one of the `.bind` calls in the JVP rule works. Reproducer: ```python import functools from jax.core import Primitive, ShapedArray from jax.interpreters import ad, xla, mlir from jax._src.effects import (     Effect,     lowerable_effects,     ordered_effects,     control_flow_allowed_effects,     custom_derivatives_allowed_effects, )  Register an ordered effect class EffectType(Effect):     pass lowerable_effects.add_type(EffectType) ordered_effects.add_type(EffectType) control_flow_allowed_effects.add_type(EffectType) custom_derivatives_allowed_effects.add_type(EffectType) ordered_effect = EffectType()  Define custom Jax primitive custom_p = Primitive(""custom"") custom_impl = functools.partial(xla.apply_primitive, custom_p) def custom(x):     return custom_p.bind(x) def custom_xla_encode_cpu(ctx, x):     token = ctx.tokens_in.get(ordered_effect)[0]     ctx.set_tokens_out(mlir.TokenSet({ordered_effect: (token,)}))     return [x] def custom_abstract_eval(xs):     return ShapedArray(xs.shape, xs.dtype), {ordered_effect} def custom_value_and_jvp(in_args, tan_args):     (x,) = in_args     (x_tan,) = tan_args     val = custom_p.bind(x)     jvp = custom_p.bind(x_tan)     return val, jvp def custom_transpose_rule(x_tan, *x_args):     res = custom_p.bind(x_tan)     return (res,) custom_p.def_impl(custom_impl) custom_p.def_effectful_abstract_eval(custom_abstract_eval) ad.primitive_jvps[custom_p] = custom_value_and_jvp ad.primitive_transposes[custom_p] = custom_transpose_rule mlir.register_lowering(custom_p, custom_xla_encode_cpu, platform=""cpu"") if __name__ == ""__main__"":     import jax     x = jax.numpy.zeros(100)     def testfun(x):         return custom(x).sum()     jax.grad(testfun)(x) ``` Error: ```bash $ python bug.py Traceback (most recent call last):   File ""/home/dion/codes/mpi4jax/bug.py"", line 74, in      jax.grad(testfun)(x)   File ""/home/dion/codes/mpi4jax/bug.py"", line 72, in testfun     return custom(x).sum()   File ""/home/dion/codes/mpi4jax/bug.py"", line 31, in custom     return custom_p.bind(x)   File ""/home/dion/codes/mpi4jax/bug.py"", line 49, in custom_value_and_jvp     jvp = custom_p.bind(x_tan) jax._src.source_info_util.JaxStackTraceBeforeTransformation: ValueError: INVALID_ARGUMENT: Execution supplied 1 buffers but compiled program expected 2 buffers The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/dion/codes/mpi4jax/bug.py"", line 74, in      jax.grad(testfun)(x)   File ""/home/dion/codes/mpi4jax/bug.py"", line 54, in custom_transpose_rule     res = custom_p.bind(x_tan) ValueError: INVALID_ARGUMENT: Execution supplied 1 buffers but compiled program expected 2 buffers ```  What jax/jaxlib version are you using? 0.4.18  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot call custom primitive with ordered effect twice in JVP rule," Description We are currently migrating mpi4jax from manual XLA tokens to the new ordered effects. The only remaining test failures are related to our JVP rules which call the underlying primitive twice (once on the primal and once on the tangent). Commenting out one of the `.bind` calls in the JVP rule works. Reproducer: ```python import functools from jax.core import Primitive, ShapedArray from jax.interpreters import ad, xla, mlir from jax._src.effects import (     Effect,     lowerable_effects,     ordered_effects,     control_flow_allowed_effects,     custom_derivatives_allowed_effects, )  Register an ordered effect class EffectType(Effect):     pass lowerable_effects.add_type(EffectType) ordered_effects.add_type(EffectType) control_flow_allowed_effects.add_type(EffectType) custom_derivatives_allowed_effects.add_type(EffectType) ordered_effect = EffectType()  Define custom Jax primitive custom_p = Primitive(""custom"") custom_impl = functools.partial(xla.apply_primitive, custom_p) def custom(x):     return custom_p.bind(x) def custom_xla_encode_cpu(ctx, x):     token = ctx.tokens_in.get(ordered_effect)[0]     ctx.set_tokens_out(mlir.TokenSet({ordered_effect: (token,)}))     return [x] def custom_abstract_eval(xs):     return ShapedArray(xs.shape, xs.dtype), {ordered_effect} def custom_value_and_jvp(in_args, tan_args):     (x,) = in_args     (x_tan,) = tan_args     val = custom_p.bind(x)     jvp = custom_p.bind(x_tan)     return val, jvp def custom_transpose_rule(x_tan, *x_args):     res = custom_p.bind(x_tan)     return (res,) custom_p.def_impl(custom_impl) custom_p.def_effectful_abstract_eval(custom_abstract_eval) ad.primitive_jvps[custom_p] = custom_value_and_jvp ad.primitive_transposes[custom_p] = custom_transpose_rule mlir.register_lowering(custom_p, custom_xla_encode_cpu, platform=""cpu"") if __name__ == ""__main__"":     import jax     x = jax.numpy.zeros(100)     def testfun(x):         return custom(x).sum()     jax.grad(testfun)(x) ``` Error: ```bash $ python bug.py Traceback (most recent call last):   File ""/home/dion/codes/mpi4jax/bug.py"", line 74, in      jax.grad(testfun)(x)   File ""/home/dion/codes/mpi4jax/bug.py"", line 72, in testfun     return custom(x).sum()   File ""/home/dion/codes/mpi4jax/bug.py"", line 31, in custom     return custom_p.bind(x)   File ""/home/dion/codes/mpi4jax/bug.py"", line 49, in custom_value_and_jvp     jvp = custom_p.bind(x_tan) jax._src.source_info_util.JaxStackTraceBeforeTransformation: ValueError: INVALID_ARGUMENT: Execution supplied 1 buffers but compiled program expected 2 buffers The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/dion/codes/mpi4jax/bug.py"", line 74, in      jax.grad(testfun)(x)   File ""/home/dion/codes/mpi4jax/bug.py"", line 54, in custom_transpose_rule     res = custom_p.bind(x_tan) ValueError: INVALID_ARGUMENT: Execution supplied 1 buffers but compiled program expected 2 buffers ```  What jax/jaxlib version are you using? 0.4.18  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_",2023-10-11T20:36:17Z,bug,closed,1,4,https://github.com/jax-ml/jax/issues/18068,"Funnily enough, it works if I add an (unused) token argument to the primitive: ```python import os os.environ[""JAX_PLATFORM_NAME""] = ""cpu"" import functools from jax.lax import create_token from jax.core import Primitive, ShapedArray from jax.interpreters import ad, xla, mlir from jax._src.effects import (     Effect,     lowerable_effects,     ordered_effects,     control_flow_allowed_effects,     custom_derivatives_allowed_effects, )  Register an ordered effect class EffectType(Effect):     pass lowerable_effects.add_type(EffectType) ordered_effects.add_type(EffectType) control_flow_allowed_effects.add_type(EffectType) custom_derivatives_allowed_effects.add_type(EffectType) ordered_effect = EffectType()  Define custom Jax primitive custom_p = Primitive(""custom"") custom_impl = functools.partial(xla.apply_primitive, custom_p) def custom(x):     token = create_token()   < new     return custom_p.bind(x, token) def custom_xla_encode_cpu(ctx, x, token):     del token   < throw away input argument     token = ctx.tokens_in.get(ordered_effect)[0]     ctx.set_tokens_out(mlir.TokenSet({ordered_effect: (token,)}))     return [x] def custom_abstract_eval(xs, token):     return ShapedArray(xs.shape, xs.dtype), {ordered_effect} def custom_value_and_jvp(in_args, tan_args):     (x, token) = in_args     (x_tan, *_) = tan_args     val = custom_p.bind(x, token)     jvp = custom_p.bind(x_tan, token)     return val, jvp def custom_transpose_rule(x_tan, *x_args):     _, token = x_args     res = custom_p.bind(x_tan, token)     return (res, ad.Zero.from_value(token)) custom_p.def_impl(custom_impl) custom_p.def_effectful_abstract_eval(custom_abstract_eval) ad.primitive_jvps[custom_p] = custom_value_and_jvp ad.primitive_transposes[custom_p] = custom_transpose_rule mlir.register_lowering(custom_p, custom_xla_encode_cpu, platform=""cpu"") if __name__ == ""__main__"":     import jax     x = jax.numpy.zeros(100)     def testfun(x):         return custom(x).sum()     testfun(x)     jax.grad(testfun)(x)     print(""ok"") ``` ```bash $ python foo.py ok ```","I would like to second this issue, but I think it happens any time the primitive is called twice for the same abstract values, not just for jvp rules. Taking a look into `apply_primitive` (line ~121 in jax/_src/dispatch.py), the error can also be raised at `return compiled_fun(*args)` by simply calling it twice in a row (with the exact same local `args`). The second call appears to not get some internally prepended token argument that the first one gets, and says there is one less argument than expected. So it seems to be some statefullness to the call wrapper that gets used up, and the wrapper is cached so it can't be called again. Also looking at the method `xla_primitive_callable` (line ~144 in jax/_src/dispatch.py) that creates the wrapper, there's the branching statement where it either binds the call to `compiled.create_cpp_call_for_apply_primitive` versus `compiled.unsafe_call`. Forcing it to use `compiled.unsafe_call` does *not* cause the same error. I can also confirm that adding dummy argument seems to work around it, as in it no longer raises the error, but it still makes me uneasy because that doesn't seem selfconsistent.","Hi   It looks like this issue has been resolved in the latest JAX versions. I executed the mentioned code without any errors in Colab using JAX version 0.4.23: ```Python import jax print(f""jax version: {jax.__version__}"") ``` Output: `jax version: 0.4.23 ` ```Python import functools from jax.core import Primitive, ShapedArray from jax.interpreters import ad, xla, mlir from jax._src.effects import (     Effect,     lowerable_effects,     ordered_effects,     control_flow_allowed_effects,     custom_derivatives_allowed_effects, )  Register an ordered effect class EffectType(Effect):     pass lowerable_effects.add_type(EffectType) ordered_effects.add_type(EffectType) control_flow_allowed_effects.add_type(EffectType) custom_derivatives_allowed_effects.add_type(EffectType) ordered_effect = EffectType()  Define custom Jax primitive custom_p = Primitive(""custom"") custom_impl = functools.partial(xla.apply_primitive, custom_p) def custom(x):     return custom_p.bind(x) def custom_xla_encode_cpu(ctx, x):     token = ctx.tokens_in.get(ordered_effect)[0]     ctx.set_tokens_out(mlir.TokenSet({ordered_effect: (token,)}))     return [x] def custom_abstract_eval(xs):     return ShapedArray(xs.shape, xs.dtype), {ordered_effect} def custom_value_and_jvp(in_args, tan_args):     (x,) = in_args     (x_tan,) = tan_args     val = custom_p.bind(x)     jvp = custom_p.bind(x_tan)     return val, jvp def custom_transpose_rule(x_tan, *x_args):     res = custom_p.bind(x_tan)     return (res,) custom_p.def_impl(custom_impl) custom_p.def_effectful_abstract_eval(custom_abstract_eval) ad.primitive_jvps[custom_p] = custom_value_and_jvp ad.primitive_transposes[custom_p] = custom_transpose_rule mlir.register_lowering(custom_p, custom_xla_encode_cpu, platform=""cpu"") if __name__ == ""__main__"":     import jax     x = jax.numpy.zeros(100)     def testfun(x):         return custom(x).sum()     jax.grad(testfun)(x)     print(""ok"") ``` Output: `ok ` Kindly find the gist for your reference.","Wow, thanks  ! I also ran your repro and verified it works."
786,"以下是一个github上的jax下的一个issue, 标题是(Batched/Parallelised `make_array_from_callback` for array trees)， 内容是 (When using array trees (e.g. dictionary of arrays) as input to jitted functions, one can use `make_array_from_callback` within a `jax.tree_map` call to transform arrays in jax arrays. For example ``` data =  def to_jax_array(x):     return jax.make_array_from_callback(...) data = jax.tree_map(_to_jax_array, data) ``` This turns out to be somewhat slow and would be nice to have `make_array_from_callback` accepting array trees, pushing down the stack tree traversing and `device_put`s to amortise and possibly parallelise the underlying ops.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Batched/Parallelised `make_array_from_callback` for array trees,"When using array trees (e.g. dictionary of arrays) as input to jitted functions, one can use `make_array_from_callback` within a `jax.tree_map` call to transform arrays in jax arrays. For example ``` data =  def to_jax_array(x):     return jax.make_array_from_callback(...) data = jax.tree_map(_to_jax_array, data) ``` This turns out to be somewhat slow and would be nice to have `make_array_from_callback` accepting array trees, pushing down the stack tree traversing and `device_put`s to amortise and possibly parallelise the underlying ops.",2023-10-11T14:11:12Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/18058
348,"以下是一个github上的jax下的一个issue, 标题是([random] add shaped_abstractify handler for custom PRNG key)， 内容是 (I confirmed with a `breakpoint()` that with this change keyarrays no longer go via `shaped_abstractify_slow`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[random] add shaped_abstractify handler for custom PRNG key,I confirmed with a `breakpoint()` that with this change keyarrays no longer go via `shaped_abstractify_slow`.,2023-10-10T23:16:39Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/18052
805,"以下是一个github上的jax下的一个issue, 标题是(JAX and TORCH)， 内容是 ( Description When I only pip the latesd jax with cuda(pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html), I can use the jax with gpu.But when I pip the torch(pip install torch) later, Ican't use the jax with gpu,it remind me that cuda or cusolver's version is older than jax's.Why? Can Older jax version avoid it?Then how can I pip the jax[cuda] with relevant version?   What jax/jaxlib version are you using? jax0.4.18 jaxlib0.4.18+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info 3.10.9/Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX and TORCH," Description When I only pip the latesd jax with cuda(pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html), I can use the jax with gpu.But when I pip the torch(pip install torch) later, Ican't use the jax with gpu,it remind me that cuda or cusolver's version is older than jax's.Why? Can Older jax version avoid it?Then how can I pip the jax[cuda] with relevant version?   What jax/jaxlib version are you using? jax0.4.18 jaxlib0.4.18+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info 3.10.9/Linux  NVIDIA GPU info _No response_",2023-10-10T10:17:39Z,bug NVIDIA GPU,closed,9,34,https://github.com/jax-ml/jax/issues/18032,"That's correct. The current releases of PyTorch and JAX have incompatible CUDA version dependencies. I reported this issue to the PyTorch developers a while back, but there has been no interest in relaxing their CUDA version dependencies. My recommendations: * use a different virtualenv for PyTorch and JAX. This is the simplest solution and probably the best. * if for some reason you really want PyTorch and JAX in the same virtualenv, install the CPU version of one of them and the CUDA version of the other. That avoids any CUDA version conflicts. * it may work to simply install JAX after PyTorch, since JAX wants a newer CUDA version than PyTorch's current release does, and in practice NVIDIA's CUDA releases are backwards compatible. I'm not sure if PyTorch enforces a version check, but if not it's highly likely this will work. But I don't think the PyTorch developers support it. * another solution is to install the CUDA version needed for one of the two, and build the other one from source. For example, JAX will happily build from source with an older CUDA release, it's just the binary distribution that requires a CUDA version matching the version against which it was built. Does that resolve your problem? Hope that helps!","This is quite annoying (and inconvenient) now that people have written torch2jax functionality which allows GPUaccelerated interaction, https://github.com/samuela/torch2jax https://github.com/rdyro/torch2jax","Hi , I've been experimenting the simultaneous usage of Torch and JAX for a while. I'm currently working in a Docker container in which they both work on GPU. JAX was installed according to the official documentation as: ```sh pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` I leave here the Conda YAML of the environment, there will probably be some extra packages, but I hope this can help:  conda environment ```yaml name: base channels:    nvidia    condaforge    defaults dependencies:    _libgcc_mutex=0.1=conda_forge    _openmp_mutex=4.5=2_gnu    abslpy=1.4.0=py310h06a4308_0    alsalib=1.2.10=hd590300_0    appdirs=1.4.4=pyhd3eb1b0_0    asttokens=2.0.5=pyhd3eb1b0_0    attr=2.5.1=h166bdaf_1    backcall=0.2.0=pyhd3eb1b0_0    binutils=2.40=hdd6e379_0    binutils_impl_linux64=2.40=hf600244_0    binutils_linux64=2.40=hbdbef99_2    blas=1.0=openblas    boltons=23.0.0=pyhd8ed1ab_0    brotli=1.0.9=he6710b0_2    brotlipython=1.1.0=py310hc6cd4ac_0    bzip2=1.0.8=h7f98852_4    cares=1.19.1=hd590300_0    ccompiler=1.6.0=hd590300_0    cacertificates=2023.7.22=hbcca054_0    cairo=1.16.0=hb05425b_5    certifi=2023.7.22=pyhd8ed1ab_0    cffi=1.15.1=py310h255011f_3    charsetnormalizer=3.2.0=pyhd8ed1ab_0    chex=0.1.5=py310h06a4308_0    click=8.0.4=py310h06a4308_0    colorama=0.4.6=pyhd8ed1ab_0    coloredlogs=15.0.1=py310h06a4308_1    compilers=1.6.0=ha770c72_0    conda=23.3.1=py310hff52083_0    condapackagehandling=2.2.0=pyh38be061_0    condapackagestreaming=0.9.0=pyhd8ed1ab_0    contourpy=1.0.5=py310hdb19cb5_0    cryptography=41.0.3=py310h75e40e8_0    cudanvcc=11.3.58=h2467b9f_0    cudaversion=11.8=h70ddcb2_2    cudatoolkit=11.8.0=h4ba93d1_12    cudnn=8.9.2.26=cuda11_0    cupti=11.8.0=he078b1a_0    cxxcompiler=1.6.0=h00ab1b0_0    cycler=0.11.0=pyhd3eb1b0_0    dbus=1.13.18=hb2f20db_0    deap=1.4.1=py310h7cbd5c2_0    decorator=5.1.1=pyhd3eb1b0_0    dmtree=0.1.7=py310h6a678d5_1    dockerpycreds=0.4.0=pyhd3eb1b0_0    docstring_parser=0.15=pyhd8ed1ab_0    exceptiongroup=1.0.4=py310h06a4308_0    executing=0.8.3=pyhd3eb1b0_0    expat=2.5.0=h6a678d5_0    filelock=3.9.0=py310h06a4308_0    flax=0.6.1=pyhd8ed1ab_1    fmt=9.1.0=h924138e_0    fontttfdejavusansmono=2.37=hd3eb1b0_0    fontttfinconsolata=2.001=hcb22688_0    fontttfsourcecodepro=2.030=hd3eb1b0_0    fontttfubuntu=0.83=h8b1ccd4_0    fontconfig=2.14.2=h14ed4e7_0    fontsanaconda=1=h8fa9717_0    fontscondaecosystem=1=hd3eb1b0_0    fonttools=4.25.0=pyhd3eb1b0_0    fortrancompiler=1.6.0=heb67821_0    freetype=2.12.1=h4a9f257_0    frozendict=2.3.8=py310h2372a71_0    gcc=12.3.0=h8d2909c_2    gcc_impl_linux64=12.3.0=he2b93b0_1    gcc_linux64=12.3.0=h76fc315_2    gettext=0.21.1=h27087fc_0    gfortran=12.3.0=h499e0f7_2    gfortran_impl_linux64=12.3.0=hfcedea8_1    gfortran_linux64=12.3.0=h7fe76b4_2    gitdb=4.0.7=pyhd3eb1b0_0    gitpython=3.1.30=py310h06a4308_0    glib=2.78.0=hfc55251_0    glibtools=2.78.0=hfc55251_0    gmp=6.2.1=h295c915_3    gmpy2=2.1.2=py310heeb90bb_0    graphite2=1.3.14=h295c915_1    gstpluginsbase=1.22.5=h8e1006c_1    gstreamer=1.22.5=h98fc4e7_1    gxx=12.3.0=h8d2909c_2    gxx_impl_linux64=12.3.0=he2b93b0_1    gxx_linux64=12.3.0=h8a814eb_2    harfbuzz=8.2.0=h3d44ed6_0    humanfriendly=10.0=py310h06a4308_1    icu=73.2=h59595ed_0    idna=3.4=pyhd8ed1ab_0    intelopenmp=2023.1.0=hdb19cb5_46305    ipython=8.15.0=py310h06a4308_0    jaxdataclasses=1.5.1=pyhd8ed1ab_0    jaxlie=1.3.3=pyhd8ed1ab_0    jedi=0.18.1=py310h06a4308_1    jinja2=3.1.2=py310h06a4308_0    jsonpatch=1.32=pyhd8ed1ab_0    jsonpointer=2.4=py310hff52083_0    kernelheaders_linux64=2.6.32=he073ed8_16    keyutils=1.6.1=h166bdaf_0    kiwisolver=1.4.4=py310h6a678d5_0    krb5=1.21.2=h659d440_0    lame=3.100=h7b6447c_0    lcms2=2.15=h7f713cb_2    ld_impl_linux64=2.40=h41732ed_0    lerc=4.0.0=h27087fc_0    libarchive=3.6.2=h039dbb9_1    libcap=2.69=h0f662aa_0    libclang=15.0.7=default_h7634d5b_3    libclang13=15.0.7=default_h9986a30_3    libcups=2.3.3=h4637d8d_4    libcurl=8.3.0=hca28451_0    libdeflate=1.19=hd590300_0    libedit=3.1.20191231=he28a2e2_2    libev=4.33=h516909a_1    libevent=2.1.12=hdbd6064_1    libexpat=2.5.0=hcb278e6_1    libffi=3.4.2=h7f98852_5    libflac=1.4.3=h59595ed_0    libgccdevel_linux64=12.3.0=h8bca6fd_1    libgccng=13.2.0=h807b86a_0    libgcrypt=1.10.1=h166bdaf_0    libgfortranng=13.2.0=h69a702a_1    libgfortran5=13.2.0=ha4646dd_1    libglib=2.78.0=hebfc3b9_0    libgomp=13.2.0=h807b86a_0    libgpgerror=1.47=h71f35ed_0    libiconv=1.17=h166bdaf_0    libjpegturbo=2.1.5.1=hd590300_1    libllvm15=15.0.7=h5cf9203_3    libmamba=1.2.0=hcea66bb_0    libmambapy=1.2.0=py310h1428755_0    libnghttp2=1.52.0=h61bc06f_0    libnsl=2.0.0=h7f98852_0    libogg=1.3.5=h27cfd23_1    libopenblas=0.3.21=h043d6bf_0    libopus=1.3.1=h7b6447c_0    libpng=1.6.39=h5eee18b_0    libpq=15.4=hfc447b1_0    libprotobuf=3.20.3=he621ea3_0    libsanitizer=12.3.0=h0f45ef3_1    libsndfile=1.2.2=hbc2eb40_0    libsolv=0.7.24=hfc55251_4    libsqlite=3.43.0=h2797004_0    libssh2=1.11.0=h0841786_0    libstdcxxdevel_linux64=12.3.0=h8bca6fd_1    libstdcxxng=13.2.0=h7e041cc_0    libsystemd0=254=h3516f8a_0    libtiff=4.6.0=h29866fb_1    libuuid=2.38.1=h0b41bf4_0    libvorbis=1.3.7=h7b6447c_0    libwebpbase=1.3.2=h5eee18b_0    libxcb=1.15=h7f8727e_0    libxkbcommon=1.5.0=h5d7e998_3    libxml2=2.11.5=h232c23b_1    libzlib=1.2.13=hd590300_5    lz4c=1.9.4=hcb278e6_0    lzo=2.10=h516909a_1000    magma=2.7.1=h2c23e93_0    mamba=1.2.0=py310h51d5547_0    markdownitpy=2.2.0=py310h06a4308_1    markupsafe=2.1.1=py310h7f8727e_0    mashumaro=3.6=py310h06a4308_0    matplotlib=3.7.2=py310h06a4308_0    matplotlibbase=3.7.2=py310h1128e8f_0    matplotlibinline=0.1.6=py310h06a4308_0    mdurl=0.1.0=py310h06a4308_0    mkl=2023.1.0=h213fc3f_46343    mpc=1.1.0=h10f8cd9_1    mpfr=4.0.2=hb69a4c5_1    mpg123=1.31.3=hcb278e6_0    mpmath=1.3.0=py310h06a4308_0    msgpackpython=1.0.3=py310hd09550d_0    munkres=1.1.4=py_0    mysqlcommon=8.0.33=hf1915f5_4    mysqllibs=8.0.33=hca2cd23_4    ncurses=6.4=hcb278e6_0    networkx=3.1=py310h06a4308_0    ninja=1.10.2=h06a4308_5    ninjabase=1.10.2=hd09550d_5    nspr=4.35=h6a678d5_0    nss=3.92=h1d7d5a4_0    numpy=1.25.2=py310heeff2f4_0    numpybase=1.25.2=py310h8a23956_0    openjpeg=2.5.0=h488ebb8_3    openssl=3.1.2=hd590300_0    opt_einsum=3.3.0=pyhd3eb1b0_1    optax=0.1.4=py310h06a4308_0    overrides=7.4.0=pyhd8ed1ab_0    packaging=23.1=pyhd8ed1ab_0    parso=0.8.3=pyhd3eb1b0_0    pathtools=0.1.2=pyhd3eb1b0_1    pcre2=10.40=hc3806b6_0    pexpect=4.8.0=pyhd3eb1b0_3    pickleshare=0.7.5=pyhd3eb1b0_1003    pillow=10.0.1=py310h29da1c1_0    pip=23.2.1=pyhd8ed1ab_0    pixman=0.40.0=h7f8727e_1    pluggy=1.3.0=pyhd8ed1ab_0    ply=3.11=py310h06a4308_0    pptree=3.1=pyhd8ed1ab_0    prompttoolkit=3.0.36=py310h06a4308_0    protobuf=3.20.3=py310h6a678d5_0    psutil=5.9.0=py310h5eee18b_0    ptyprocess=0.7.0=pyhd3eb1b0_2    pulseaudioclient=16.1=hb77b528_5    pure_eval=0.2.2=pyhd3eb1b0_0    pybind11abi=4=hd8ed1ab_3    pycosat=0.6.4=py310h5764c6d_1    pycparser=2.21=pyhd8ed1ab_0    pygments=2.15.1=py310h06a4308_1    pyopenssl=23.2.0=pyhd8ed1ab_1    pyparsing=3.0.9=py310h06a4308_0    pyqt=5.15.9=py310h04931ad_4    pyqt5sip=12.12.2=py310hc6cd4ac_4    pysocks=1.7.1=pyha2e5f31_6    python=3.10.8=h4a9ceb5_0_cpython    pythondateutil=2.8.2=pyhd3eb1b0_0    python_abi=3.10=3_cp310    pytorch=2.0.1=gpu_cuda118py310h7799f5a_0    pyyaml=6.0=py310h5eee18b_1    qtmain=5.15.8=hc47bfe8_16    readline=8.2=h8228510_1    reproc=14.2.4=h0b41bf4_0    reproccpp=14.2.4=hcb278e6_0    requests=2.31.0=pyhd8ed1ab_0    rich=13.3.5=py310h06a4308_0    ruamel.yaml=0.17.32=py310h2372a71_0    ruamel.yaml.clib=0.2.7=py310h1fa729e_1    scipy=1.11.1=py310heeff2f4_0    sentrysdk=1.9.0=py310h06a4308_0    setproctitle=1.2.2=py310h7f8727e_0    setuptools=68.2.2=pyhd8ed1ab_0    shtab=1.6.4=pyhd8ed1ab_1    sip=6.7.11=py310hc6cd4ac_0    six=1.16.0=pyhd3eb1b0_1    smmap=4.0.0=pyhd3eb1b0_0    stack_data=0.2.0=pyhd3eb1b0_0    sympy=1.11.1=py310h06a4308_0    sysroot_linux64=2.12=he073ed8_16    tbb=2021.8.0=hdb19cb5_0    tk=8.6.12=h27826a3_0    toml=0.10.2=pyhd3eb1b0_0    tomli=2.0.1=py310h06a4308_0    toolz=0.12.0=pyhd8ed1ab_0    tornado=6.3.2=py310h5eee18b_0    tqdm=4.66.1=pyhd8ed1ab_0    traitlets=5.7.1=py310h06a4308_0    typingextensions=4.7.1=py310h06a4308_0    typing_extensions=4.7.1=py310h06a4308_0    typing_utils=0.1.0=pyhd8ed1ab_0    tyro=0.5.7=pyhd8ed1ab_0    tzdata=2023c=h71feb2d_0    urllib3=2.0.4=pyhd8ed1ab_0    wandb=0.15.10=pyhd8ed1ab_0    wcwidth=0.2.5=pyhd3eb1b0_0    wheel=0.41.2=pyhd8ed1ab_0    xcbutil=0.4.0=hd590300_1    xcbutilimage=0.4.0=h8ee46fc_1    xcbutilkeysyms=0.4.0=h8ee46fc_1    xcbutilrenderutil=0.3.9=hd590300_1    xcbutilwm=0.4.1=h8ee46fc_1    xkeyboardconfig=2.39=hd590300_0    xorgkbproto=1.0.7=h7f98852_1002    xorglibice=1.1.1=hd590300_0    xorglibsm=1.2.4=h7391055_0    xorglibx11=1.8.6=h8ee46fc_0    xorglibxau=1.0.11=hd590300_0    xorglibxext=1.3.4=h0b41bf4_2    xorglibxrender=0.9.11=hd590300_0    xorgrenderproto=0.11.1=h7f98852_1002    xorgxextproto=7.3.0=h0b41bf4_1003    xorgxf86vidmodeproto=2.3.1=h7f98852_1002    xorgxproto=7.0.31=h27cfd23_1007    xz=5.2.6=h166bdaf_0    yaml=0.2.5=h7b6447c_0    yamlcpp=0.7.0=h27087fc_2    zlib=1.2.13=hd590300_5    zstandard=0.19.0=py310h5764c6d_0    zstd=1.5.5=hfc55251_0    pip:        jax==0.4.18        jaxlib==0.4.18+cuda12.cudnn89        mldtypes==0.3.1        nvidiacublascu12==12.2.5.6        nvidiacudacupticu12==12.2.142        nvidiacudanvcccu12==12.2.140        nvidiacudanvrtccu12==12.2.140        nvidiacudaruntimecu12==12.2.140        nvidiacudnncu12==8.9.4.25        nvidiacufftcu12==11.0.8.103        nvidiacusolvercu12==11.5.2.141        nvidiacusparsecu12==12.1.2.141        nvidiancclcu12==2.18.3        nvidianvjitlinkcu12==12.2.140 prefix: /conda ```   Python 3.10.8  Ubuntu 22.04  jax==0.4.18  jaxlib==0.4.18+cuda12.cudnn89  Driver Version: 525.125.06  CUDA Version: 12.0","Thank you for your all help. For some reason I can't experience it now,but I'll try it soon and reply you.","ok people, this has been a 1 day nightmare. But finally got this to work on an H100 machine with cuda 12.2, without sudo.  First install Cuda 12.2 (this was already there for me)  Then install Cudnn 8.9 through the official website, using the tar option: https://docs.nvidia.com/deeplearning/cudnn/installguide/index.htmlinstalllinuxtar  then follow what this guy did to build magma: https://github.com/huggingface/autotrainadvanced/issues/281issuecomment1740762360 then install pytorch from source as that post says!!!! and bualaaa","No promises, but informally we're going to try to keep at least one JAX release have a version that is also released with PyTorch. Right now, that's the CUDA 11.8 release of JAX. It's not a guarantee, though; it might happen that for some JAX and Pytorch versions there's no intersecting CUDA version.","I hit a similar issue when installing pytorch and jax into the same conda environment: when torch is loaded first, `jax.devices()` will list only cpu devices. A short summary of diagnosis: It turns out that torch is built against cudnn version 8.7 while jaxlib is built against cudnn version 8.8 leading to an exception when executing `jax._src.xla_bridge._check_cuda_versions()`. Here follows a reproducer: ```sh mamba create n testpytorchjax pytorch pytorchcuda=11.8 jaxlib=*=*cuda118* jax c pytorch c nvidia nochannelpriority y mamba activate testpytorchjax ``` (note: using strict channel priority would lead to a mamba solver problem). Import torch before checking jax.devices: ```python >>> import torch >>> import jax >>> jax.devices() CUDA backend failed to initialize: Found cuDNN version 8700, but JAX was built against version 8800, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) [CpuDevice(id=0)] ``` Import torch after checking jax.devices: ```python >>> import jax >>> jax.devices() [cuda(id=0), cuda(id=1)] >>> import torch >>> jax.__version__ '0.4.23' >>> torch.__version__ '2.1.2' >>> from torch._C import _cudnn >>> _cudnn.getCompileVersion() (8, 7, 0) ``` Notices that the result of `jaxlib.cuda._versions.cudnn_get_version()` depends on whether `torch` was imported before or after calling `jaxlib.cuda._versions.cudnn_get_version`: ```python >>> import jaxlib.cuda._versions >>> jaxlib.cuda._versions.cudnn_get_version() 8902 >>> import torch >>> jaxlib.cuda._versions.cudnn_get_version() 8902 ``` vs ```python >>> import torch >>> import jaxlib.cuda._versions >>> jaxlib.cuda._versions.cudnn_get_version() 8700 ``` that qualifies as an incompatible linkage issue: since libcudnn is dynamically loaded, the result of cudnnGetVersion ought to give the version of loaded library and not of the version of the library that a software was built against. The behavior above suggests that torch was linked with libcudnn statically. A possible resolution: Note that cuDNN minor releases are backward compatible with applications built against the same or earlier minor release. Hence, as long as jaxlib and torch are built against libcudnn with the same major version (8), the jax version check ought to ignore cudnn minor versions. Here is a patch: ``` diff git a/jax/_src/xla_bridge.py b/jax/_src/xla_bridge.py index 7977f6329..17c14bc5a 100644  a/jax/_src/xla_bridge.py +++ b/jax/_src/xla_bridge.py @@ 263,7 +263,7 @@ def _check_cuda_versions():        cuda_versions.cudnn_build_version,         NVIDIA promise both backwards and forwards compatibility for cuDNN patch         versions: https://docs.nvidia.com/deeplearning/cudnn/developerguide/index.htmlapicompat       scale_for_comparison=100, +      scale_for_comparison=1000,    )    _version_check(""cuFFT"", cuda_versions.cufft_get_version,                   cuda_versions.cufft_build_version, ```","> No promises, but informally we're going to try to keep at least one JAX release have a version that is also released with PyTorch. Right now, that's the CUDA 11.8 release of JAX. The latest version pair I could find that were compatible with each other were `jax[cuda11pip,cuda11_pip]==0.4.10` and `torch==2.2.1+cu118`. The main conflict in later versions for jax is for cudnn, which want `>8.8`, but torch wants `==8.7`. One way to check this would be: ```bash cat > requirements.in <<EOF findlinks https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html extraindexurl=https://download.pytorch.org/whl jax[cuda11_pip] torch==2.2.1+cu118 EOF pipcompile  Check the contents of requirements.txt. ```","A workaround that works better for us is to use CUDA 11 with Jax, but CUDA 12 with Torch. So basically `jax[cuda11_pip]` and `torch` in our requirements file works for us.","> A workaround that works better for us is to use CUDA 11 with Jax, but CUDA 12 with Torch. How did you get this to work? I'm using conda, but after installing `pytorchcuda=12.1` I get the following error from JAX: ``` E   RuntimeError: Unable to initialize backend 'cuda': Unable to load CUDA. Is it installed? (set JAX_PLATFORMS='' to automatically choose an available backend) ```","We did not have to do anything special. Just installed the two packages in a clean env, and both worked.","The only way I was able to solve the environment with both JAX and PyTorch on CUDA12 was to install some packages from the nvidia channel: ```console mamba create n jaxTorch jaxlib pytorch cudanvcc c condaforge c nvidia c pytorch ``` ```sh >>> import torch >>> import jax >>> torch.cuda.is_available() True >>> jax.devices() [cuda(id=0)] >>> import jaxlib.cuda._versions >>> jaxlib.cuda._versions.cudnn_get_version() 8902 >>> torch._C._cudnn.getCompileVersion() (8, 9, 2) ```  conda list ```yaml  Name                    Version                   Build  Channel _libgcc_mutex             0.1                 conda_forge    condaforge _openmp_mutex             4.5                  2_kmp_llvm    condaforge _sysroot_linux64_curr_repodata_hack 3                   h69a702a_14    condaforge binutils_impl_linux64    2.40                 hf600244_0    condaforge binutils_linux64         2.40                 hdade7a5_3    condaforge blas                      2.116                       mkl    condaforge blasdevel                3.9.0            16_linux64_mkl    condaforge bzip2                     1.0.8                hd590300_5    condaforge cares                    1.27.0               hd590300_0    condaforge cacertificates           2024.2.2             hbcca054_0    condaforge cudacccl_linux64        12.1.109             ha770c72_0    condaforge cudacudart               12.1.105             hd3aeb46_0    condaforge cudacudartdev           12.1.105             hd3aeb46_0    condaforge cudacudartdev_linux64  12.1.105             h59595ed_0    condaforge cudacudartstatic        12.1.105             hd3aeb46_0    condaforge cudacudartstatic_linux64 12.1.105             h59595ed_0    condaforge cudacudart_linux64      12.1.105             h59595ed_0    condaforge cudacupti                12.1.105             h59595ed_0    condaforge cudadriverdev_linux64  12.1.105             h59595ed_0    condaforge cudalibraries            12.1.0                        0    nvidia cudanvcc                 12.1.105             hcdd1206_1    condaforge cudanvccdev_linux64    12.1.105             ha770c72_0    condaforge cudanvccimpl            12.1.105             hd3aeb46_0    condaforge cudanvcctools           12.1.105             hd3aeb46_0    condaforge cudanvcc_linux64        12.1.105             h8a487aa_1    condaforge cudanvrtc                12.1.105             hd3aeb46_0    condaforge cudanvtx                 12.1.105             h59595ed_0    condaforge cudaopencl               12.1.105             h59595ed_0    condaforge cudaruntime              12.1.0                        0    nvidia cudaversion              12.1                 h1d6eff3_3    condaforge cudnn                     8.9.7.29             h092f7fd_3    condaforge filelock                  3.13.3             pyhd8ed1ab_0    condaforge gcc_impl_linux64         12.3.0               he2b93b0_5    condaforge gcc_linux64              12.3.0               h6477408_3    condaforge gxx_impl_linux64         12.3.0               he2b93b0_5    condaforge gxx_linux64              12.3.0               h4a1b8e8_3    condaforge icu                       73.2                 h59595ed_0    condaforge importlibmetadata        7.1.0              pyha770c72_0    condaforge importlib_metadata        7.1.0                hd8ed1ab_0    condaforge jax                       0.4.25             pyhd8ed1ab_0    condaforge jaxlib                    0.4.23          cuda120py312hc008a70_200    condaforge jinja2                    3.1.3              pyhd8ed1ab_0    condaforge kernelheaders_linux64   3.10.0              h4a8ded7_14    condaforge ld_impl_linux64          2.40                 h41732ed_0    condaforge libabseil                 20240116.1      cxx17_h59595ed_2    condaforge libblas                   3.9.0            16_linux64_mkl    condaforge libcblas                  3.9.0            16_linux64_mkl    condaforge libcublas                 12.1.0.26                     0    nvidia libcufft                  11.0.2.4                      0    nvidia libcufile                 1.6.1.9              hd3aeb46_0    condaforge libcurand                 10.3.2.106           hd3aeb46_0    condaforge libcusolver               11.4.4.55                     0    nvidia libcusparse               12.0.2.55                     0    nvidia libexpat                  2.6.2                h59595ed_0    condaforge libffi                    3.4.2                h7f98852_5    condaforge libgccdevel_linux64     12.3.0             h8bca6fd_105    condaforge libgccng                 13.2.0               h807b86a_5    condaforge libgfortranng            13.2.0               h69a702a_5    condaforge libgfortran5              13.2.0               ha4646dd_5    condaforge libgomp                   13.2.0               h807b86a_5    condaforge libgrpc                   1.62.1               h15f2491_0    condaforge libhwloc                  2.9.3           default_h554bfaf_1009    condaforge libiconv                  1.17                 hd590300_2    condaforge liblapack                 3.9.0            16_linux64_mkl    condaforge liblapacke                3.9.0            16_linux64_mkl    condaforge libnpp                    12.0.2.50                     0    nvidia libnsl                    2.0.1                hd590300_0    condaforge libnvjitlink              12.1.105             hd3aeb46_0    condaforge libnvjpeg                 12.1.1.14                     0    nvidia libprotobuf               4.25.3               h08a7969_0    condaforge libre211                 2023.09.01           h5a48ba9_2    condaforge libsanitizer              12.3.0               h0f45ef3_5    condaforge libsqlite                 3.45.2               h2797004_0    condaforge libstdcxxdevel_linux64  12.3.0             h8bca6fd_105    condaforge libstdcxxng              13.2.0               h7e041cc_5    condaforge libuuid                   2.38.1               h0b41bf4_0    condaforge libxcrypt                 4.4.36               hd590300_1    condaforge libxml2                   2.12.6               h232c23b_1    condaforge libzlib                   1.2.13               hd590300_5    condaforge llvmopenmp               15.0.7               h0cdce71_0    condaforge markupsafe                2.1.5           py312h98912ed_0    condaforge mkl                       2022.1.0           h84fe81f_915    condaforge mkldevel                 2022.1.0           ha770c72_916    condaforge mklinclude               2022.1.0           h84fe81f_915    condaforge ml_dtypes                 0.3.2           py312hfb8ada1_0    condaforge mpmath                    1.3.0              pyhd8ed1ab_0    condaforge nccl                      2.20.5.1             h3a97aeb_0    condaforge ncurses                   6.4.20240210         h59595ed_0    condaforge networkx                  3.2.1              pyhd8ed1ab_0    condaforge numpy                     1.26.4          py312heda63a1_0    condaforge oclicd                   2.3.2                hd590300_1    condaforge openssl                   3.2.1                hd590300_1    condaforge opteinsum                3.3.0                hd8ed1ab_2    condaforge opt_einsum                3.3.0              pyhc1e730c_2    condaforge pip                       24.0               pyhd8ed1ab_0    condaforge python                    3.12.2          hab00c5b_0_cpython    condaforge python_abi                3.12                    4_cp312    condaforge pytorch                   2.2.1           py3.12_cuda12.1_cudnn8.9.2_0    pytorch pytorchcuda              12.1                 ha16c6d3_5    pytorch pytorchmutex             1.0                        cuda    pytorch pyyaml                    6.0.1           py312h98912ed_1    condaforge re2                       2023.09.01           h7f4b329_2    condaforge readline                  8.2                  h8228510_1    condaforge scipy                     1.12.0          py312heda63a1_2    condaforge setuptools                69.2.0             pyhd8ed1ab_0    condaforge sympy                     1.12               pyh04b8f61_3    condaforge sysroot_linux64          2.17                h4a8ded7_14    condaforge tbb                       2021.11.0            h00ab1b0_1    condaforge tk                        8.6.13          noxft_h4845f30_101    condaforge typing_extensions         4.10.0             pyha770c72_0    condaforge tzdata                    2024a                h0c530f3_0    condaforge wheel                     0.43.0             pyhd8ed1ab_0    condaforge xz                        5.2.6                h166bdaf_0    condaforge yaml                      0.2.5                h7f98852_2    condaforge zipp                      3.17.0             pyhd8ed1ab_0    condaforge ```  fyi ","> The only way I was able to solve the environment with both JAX and PyTorch on CUDA12 was to install some packages from the nvidia channel: FYI, at the moment it is not possible to get both jax and pytorch with cuda 12 only using condaforge dependencies for this reason (I pinned several dependencies to get a clearer error): ~~~ traversaro:~$ mamba create n jaxtorchcuda pytorch==2.1.2=*cuda* jaxlib==0.4.23=*cuda* jax cudaversion=12.* python==3.11.* cudatoolkit==12.* Looking for: ['pytorch==2.1.2[build=*cuda*]', 'jaxlib==0.4.23[build=*cuda*]', 'jax', 'cudaversion=12', 'python=3.11', 'cudatoolkit=12'] condaforge/linux64                                        Using cache condaforge/noarch                                          Using cache Could not solve for environment specs The following packages are incompatible ├─ cudaversion 12**  is installable with the potential options │  ├─ cudaversion [12.012.4.* , which can be installed; ├─ cudatoolkit 12**  does not exist (perhaps a typo or a missing channel); ├─ jaxlib 0.4.23 *cuda* is installable with the potential options │  ├─ jaxlib 0.4.23 would require │  │  └─ cudatoolkit >=11.8,=1.62.1,=4.25.3,=11.8,=1.59.3,=4.24.4,=4.25.1,=4.25.1,=11.8,=4.25.1,=11.8,=4.25.1,=12.0,=4.25.1,=12.0,=4.24.4,=12.0,=4.25.1,<4.25.2.0a0 , which conflicts with any installable versions previously reported. ~~~ Once a condaforge pytorch version gets compiled with libprotobuf==4.25.3 (i.e. https://github.com/condaforge/pytorchcpufeedstock/pull/228 is ready and merged, big thanks to who the pytorch and jax condaforge mantainers) it should be possible to install both jax and pytorch with cuda enabled and using cuda 12 just with condaforge packages.","JAX 0.4.26 relaxed our CUDA version dependencies so the minimum CUDA version for JAX is 12.1. This is a version also supported by PyTorch. Try it out! We're going to try to make sure our supported version range overlaps with at least one PyTorch release. We dropped support for CUDA 11, note.","> > The only way I was able to solve the environment with both JAX and PyTorch on CUDA12 was to install some packages from the nvidia channel: >  > FYI, at the moment it is not possible to get both jax and pytorch with cuda 12 only using condaforge dependencies for this reason (I pinned several dependencies to get a clearer error): After a bunch of fixes from both jax and pytorch mantainers, now (late May 2024) it is possible to just install jax and pytorch from condaforge on Linux and out of the box they will work with GPU/CUDA support without the need to use any other conda channel: ~~~ $ conda create c condaforge n jaxpytorch pytorch jax $ conda activate jaxpytorch $ python Python 3.12.3  (main, Apr 15 2024, 18:38:13) [GCC 12.3.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import torch >>> import jax >>> torch.cuda.is_available() True >>> jax.devices() [cuda(id=0)] >>> ~~~ If for some reason this command does not install the cudaenabled jax, perhaps you are still using the classic conda solver, in that case you can force the installation of cudaenabled jax and pytorch with: ~~~ conda create n condaforge n jaxpytorch pytorch=*=cuda* jax jaxlib=*=cuda* ~~~ However, this is not necessary if you are using a recent conda install that defaults to use the `condalibmambasolver`, see https://www.anaconda.com/blog/afastercondaforagrowingcommunity .   conda list for reference  ~~~ (jaxpytorch) traversaro:~$ conda list  packages in environment at /home/traversaro/miniforge3/envs/jaxpytorch:   Name                    Version                   Build  Channel _libgcc_mutex             0.1                 conda_forge    condaforge _openmp_mutex             4.5                  2_kmp_llvm    condaforge _sysroot_linux64_curr_repodata_hack 3                   h69a702a_14    condaforge binutils_impl_linux64    2.40                 ha1999f0_1    condaforge binutils_linux64         2.40                 hdade7a5_3    condaforge bzip2                     1.0.8                hd590300_5    condaforge cares                    1.28.1               hd590300_0    condaforge cacertificates           2024.2.2             hbcca054_0    condaforge cudacccl_linux64        12.5.39              ha770c72_0    condaforge cudacrtdev_linux64     12.5.40              ha770c72_0    condaforge cudacrttools            12.5.40              ha770c72_0    condaforge cudacudart               12.5.39              he02047a_0    condaforge cudacudartdev           12.5.39              he02047a_0    condaforge cudacudartdev_linux64  12.5.39              h85509e4_0    condaforge cudacudartstatic        12.5.39              he02047a_0    condaforge cudacudartstatic_linux64 12.5.39              h85509e4_0    condaforge cudacudart_linux64      12.5.39              h85509e4_0    condaforge cudacupti                12.5.39              he02047a_0    condaforge cudadriverdev_linux64  12.5.39              h85509e4_0    condaforge cudanvcc                 12.5.40              hcdd1206_0    condaforge cudanvccdev_linux64    12.5.40              ha770c72_0    condaforge cudanvccimpl            12.5.40              hd3aeb46_0    condaforge cudanvcctools           12.5.40              hd3aeb46_0    condaforge cudanvcc_linux64        12.5.40              h8a487aa_0    condaforge cudanvrtc                12.5.40              he02047a_0    condaforge cudanvtx                 12.5.39              he02047a_0    condaforge cudanvvmdev_linux64    12.5.40              ha770c72_0    condaforge cudanvvmimpl            12.5.40              h59595ed_0    condaforge cudanvvmtools           12.5.40              h59595ed_0    condaforge cudaversion              12.5                 hd4f0392_3    condaforge cudnn                     8.9.7.29             h092f7fd_3    condaforge filelock                  3.14.0             pyhd8ed1ab_0    condaforge fsspec                    2024.5.0           pyhff2d567_0    condaforge gcc_impl_linux64         12.3.0               h58ffeeb_7    condaforge gcc_linux64              12.3.0               h6477408_3    condaforge gmp                       6.3.0                h59595ed_1    condaforge gmpy2                     2.1.5           py312h1d5cde6_1    condaforge gxx_impl_linux64         12.3.0               h2a574ab_7    condaforge gxx_linux64              12.3.0               h4a1b8e8_3    condaforge icu                       73.2                 h59595ed_0    condaforge importlibmetadata        7.1.0              pyha770c72_0    condaforge importlib_metadata        7.1.0                hd8ed1ab_0    condaforge jax                       0.4.27             pyhd8ed1ab_0    condaforge jaxlib                    0.4.23          cuda120py312h6027bbc_202    condaforge jinja2                    3.1.4              pyhd8ed1ab_0    condaforge kernelheaders_linux64   3.10.0              h4a8ded7_14    condaforge ld_impl_linux64          2.40                 hf3520f5_1    condaforge libabseil                 20240116.2      cxx17_h59595ed_0    condaforge libblas                   3.9.0           22_linux64_openblas    condaforge libcblas                  3.9.0           22_linux64_openblas    condaforge libcublas                 12.5.2.13            he02047a_0    condaforge libcufft                  11.2.3.18            he02047a_0    condaforge libcurand                 10.3.6.39            he02047a_0    condaforge libcusolver               11.6.2.40            he02047a_0    condaforge libcusparse               12.4.1.24            he02047a_0    condaforge libexpat                  2.6.2                h59595ed_0    condaforge libffi                    3.4.2                h7f98852_5    condaforge libgccdevel_linux64     12.3.0             h0223996_107    condaforge libgccng                 13.2.0               h77fa898_7    condaforge libgfortranng            13.2.0               h69a702a_7    condaforge libgfortran5              13.2.0               hca663fb_7    condaforge libgomp                   13.2.0               h77fa898_7    condaforge libgrpc                   1.62.2               h15f2491_0    condaforge libhwloc                  2.10.0          default_h5622ce7_1001    condaforge libiconv                  1.17                 hd590300_2    condaforge liblapack                 3.9.0           22_linux64_openblas    condaforge libmagma                  2.7.2                h173bb3b_2    condaforge libmagma_sparse           2.7.2                h173bb3b_3    condaforge libnsl                    2.0.1                hd590300_0    condaforge libnvjitlink              12.5.40              he02047a_0    condaforge libopenblas               0.3.27          pthreads_h413a1c8_0    condaforge libprotobuf               4.25.3               h08a7969_0    condaforge libre211                 2023.09.01           h5a48ba9_2    condaforge libsanitizer              12.3.0               hb8811af_7    condaforge libsqlite                 3.45.3               h2797004_0    condaforge libstdcxxdevel_linux64  12.3.0             h0223996_107    condaforge libstdcxxng              13.2.0               hc0a3c3a_7    condaforge libtorch                  2.3.0           cuda120_h2b0da52_301    condaforge libuuid                   2.38.1               h0b41bf4_0    condaforge libuv                     1.48.0               hd590300_0    condaforge libxcrypt                 4.4.36               hd590300_1    condaforge libxml2                   2.12.7               hc051c1a_0    condaforge libzlib                   1.2.13               hd590300_5    condaforge llvmopenmp               18.1.6               ha31de31_0    condaforge markupsafe                2.1.5           py312h98912ed_0    condaforge mkl                       2023.2.0         h84fe81f_50496    condaforge ml_dtypes                 0.4.0           py312h1d6d2e6_1    condaforge mpc                       1.3.1                hfe3b2da_0    condaforge mpfr                      4.2.1                h9458935_1    condaforge mpmath                    1.3.0              pyhd8ed1ab_0    condaforge nccl                      2.21.5.1             h3a97aeb_0    condaforge ncurses                   6.5                  h59595ed_0    condaforge networkx                  3.3                pyhd8ed1ab_1    condaforge numpy                     1.26.4          py312heda63a1_0    condaforge openssl                   3.3.0                h4ab18f5_3    condaforge opteinsum                3.3.0                hd8ed1ab_2    condaforge opt_einsum                3.3.0              pyhc1e730c_2    condaforge pip                       24.0               pyhd8ed1ab_0    condaforge python                    3.12.3          hab00c5b_0_cpython    condaforge python_abi                3.12                    4_cp312    condaforge pytorch                   2.3.0           cuda120_py312h26b3cf7_301    condaforge re2                       2023.09.01           h7f4b329_2    condaforge readline                  8.2                  h8228510_1    condaforge scipy                     1.13.1          py312hc2bc53b_0    condaforge setuptools                70.0.0             pyhd8ed1ab_0    condaforge sleef                     3.5.1                h9b69904_2    condaforge sympy                     1.12            pypyh9d50eac_103    condaforge sysroot_linux64          2.17                h4a8ded7_14    condaforge tbb                       2021.12.0            h297d8ca_1    condaforge tk                        8.6.13          noxft_h4845f30_101    condaforge typing_extensions         4.11.0             pyha770c72_0    condaforge tzdata                    2024a                h0c530f3_0    condaforge wheel                     0.43.0             pyhd8ed1ab_1    condaforge xz                        5.2.6                h166bdaf_0    condaforge zipp                      3.17.0             pyhd8ed1ab_0    condaforge zstd                      1.5.6                ha6fb4c9_0    condaforge ~~~ ","Can someone please point out the correct version necessary to get pytorch and jax both with GPU support on CUDA 12 as of July 2024? I would prefer it to be a standard venv rather than a conda env, but either is fine."," totally by chance I follow this issue, but in general you may have more success in finding help by using official jax help channels (see https://jax.readthedocs.io/en/latest/beginner_guide.htmlfindinghelp), rather then posting in closed issues. More on topic, I have no idea about pip/venv with cuda, but for conda the procedure posted in https://github.com/google/jax/issues/18032issuecomment2132399059 is working fine for me (when I originally posted the message I forgot to add the `c condaforge` to ensure it works fine also on `anaconda` or `miniconda` installation of conda that use `defaults` instead of `condaforge`, I just fixed that to avoid confusion).    By change I just noticed that you added a 👎🏽 reaction to my previous comment, any reason for doing so? Just fyi, authors do not get (at least by default) notifications for post reactions.", I found that running your command with `conda` will install: `jaxlib             condaforge/linux64::jaxlib0.4.27cpu_py312h17e8b90_0` whereas with `mamba` the correct version is installed: `mamba create c condaforge n jaxpytorch pytorch jax` `jaxlib                                    0.4.27  cuda120py312h4008524_200   condaforge/linux64` Perhaps this is why you got 3 thumbs down,">  I found that running your command with `conda` will install: `jaxlib condaforge/linux64::jaxlib0.4.27cpu_py312h17e8b90_0` whereas with `mamba` the correct version is installed: `mamba create c condaforge n jaxpytorch pytorch jax` `jaxlib 0.4.27 cuda120py312h4008524_200 condaforge/linux64` Perhaps this is why you got 3 thumbs down   Interestingly, in my system with: ~~~ rootT0NQNLN:~ conda info      active environment : None             shell level : 0        user config file : /root/.condarc  populated config files : /root/miniforge3/.condarc                           /root/.condarc           conda version : 24.3.0     condabuild version : not installed          python version : 3.10.14.final.0                  solver : libmamba (default)        virtual packages : __archspec=1=skylake                           __conda=24.3.0=0                           __cuda=12.0=0                           __glibc=2.39=0                           __linux=5.15.153.1=0                           __unix=0=0        base environment : /root/miniforge3  (writable)       conda av data dir : /root/miniforge3/etc/conda   conda av metadata url : None            channel URLs : https://conda.anaconda.org/condaforge/linux64                           https://conda.anaconda.org/condaforge/noarch           package cache : /root/miniforge3/pkgs                           /root/.conda/pkgs        envs directories : /root/miniforge3/envs                           /root/.conda/envs                platform : linux64              useragent : conda/24.3.0 requests/2.31.0 CPython/3.10.14 Linux/5.15.153.1microsoftstandardWSL2 ubuntu/24.04 glibc/2.39 solver/libmamba condalibmambasolver/24.1.0 libmambapy/1.5.8                 UID:GID : 0:0              netrc file : None            offline mode : False ~~~ the command ~~~ conda create n condaforge n jaxpytorch pytorch jax ~~~ installs the cuda jax, but indeed: ~~~ conda create solver=classic n condaforge n jaxpytorch pytorch jax ~~~ installs cpu jax. Perhaps you are using an old conda version that is using the classic solver by default? (You can see this if you report the `conda info` output, see https://www.anaconda.com/blog/afastercondaforagrowingcommunity). However, even with the classic solver forcing the solver to install the cuda version of jaxlib and pytorch works as expected (even if the classic solver is much slower): ~~~ conda create solver=classic n condaforge n jaxpytorch pytorch=*=cuda* jax jaxlib=*=cuda* ~~~ I edited the original comment accordingly.","You are right, I'm using the classic solver: ```      active environment : base     active env location : /home/chovanec/miniconda3             shell level : 1        user config file : /home/chovanec/.condarc  populated config files : /home/chovanec/.condarc           conda version : 23.1.0     condabuild version : not installed          python version : 3.10.8.final.0        virtual packages : __archspec=1=x86_64                           __cuda=12.3=0                           __glibc=2.31=0                           __linux=5.15.153.1=0                           __unix=0=0        base environment : /home/chovanec/miniconda3  (writable)       conda av data dir : /home/chovanec/miniconda3/etc/conda   conda av metadata url : None            channel URLs : https://conda.anaconda.org/bioconda/linux64                           https://conda.anaconda.org/bioconda/noarch                           https://conda.anaconda.org/condaforge/linux64                           https://conda.anaconda.org/condaforge/noarch                           https://repo.anaconda.com/pkgs/main/linux64                           https://repo.anaconda.com/pkgs/main/noarch                           https://repo.anaconda.com/pkgs/r/linux64                           https://repo.anaconda.com/pkgs/r/noarch           package cache : /home/chovanec/miniconda3/pkgs                           /home/chovanec/.conda/pkgs        envs directories : /home/chovanec/miniconda3/envs                           /home/chovanec/.conda/envs                platform : linux64              useragent : conda/23.1.0 requests/2.28.1 CPython/3.10.8 Linux/5.15.153.1microsoftstandardWSL2 ubuntu/20.04.6 glibc/2.31                 UID:GID : 1000:1000              netrc file : None            offline mode : False ```","> The only way I was able to solve the environment with both JAX and PyTorch on CUDA12 was to install some packages from the nvidia channel: >  > ``` > mamba create n jaxTorch jaxlib pytorch cudanvcc c condaforge c nvidia c pytorch > ``` >  > ```shell > >>> import torch > >>> import jax > >>> torch.cuda.is_available() > True > >>> jax.devices() > [cuda(id=0)] > >>> import jaxlib.cuda._versions > >>> jaxlib.cuda._versions.cudnn_get_version() > 8902 > >>> torch._C._cudnn.getCompileVersion() > (8, 9, 2) > ``` >  > conda list > fyi  Thanks for the solution, however i have found a possible bug that the jax numpy cannot initialize an array which size is bigger than (2, 52, 10) with both jax and jaxlib version are 0.4.30, so i have to downgrade the jax version to 0.4.23 and then works just fine, so for the insurance, the command could be like ``` conda create n _env_name_ jaxlib=0.4.23 pytorch cudanvcc python=3.11 c condaforge c nvidia c pytorch ``` python 3.12 is too newer to some commonly used pkgs","Just a curiosity, are you actually getting any packages from the `nvidia` or `pytorch` channel? If `condaforge` channel is used and you are using strict priority, all the packages you get should come from `condaforge`, and so I guess you could drop the `c nvidia c pytorch` from your command. However, you can check this by calling `conda list` and checking from where packages are installed.","> Just a curiosity, are you actually getting any packages from the `nvidia` or `pytorch` channel? If `condaforge` channel is used and you are using strict priority, all the packages you get should come from `condaforge`, and so I guess you could drop the `c nvidia c pytorch` from your command. However, you can check this by calling `conda list` and checking from where packages are installed. I'm not sure, maybe later i can do a test,thx for the noticing","> Just a curiosity, are you actually getting any packages from the `nvidia` or `pytorch` channel? If `condaforge` channel is used and you are using strict priority, all the packages you get should come from `condaforge`, and so I guess you could drop the `c nvidia c pytorch` from your command. However, you can check this by calling `conda list` and checking from where packages are installed. sorry for the late reply, here is the outputs !image since the jax and jax cuda lib are manually reinstalled by the pypi,  i guess yes that the packages are privileged installed from condaforge :)","Not sure how you can can end up with jax/jaxlib installed via pypi if you just created the environment with `conda create n _env_name_ jaxlib=0.4.23 pytorch cudanvcc python=3.11 c condaforge c nvidia c pytorch`, but as a general comment if you are installing something with pip is a good idea not to install it via conda, to avoid conflicts.","> Not sure how you can can end up with jax/jaxlib installed via pypi if you just created the environment with `conda create n _env_name_ jaxlib=0.4.23 pytorch cudanvcc python=3.11 c condaforge c nvidia c pytorch`, but as a general comment if you are installing something with pip is a good idea not to install it via conda, to avoid conflicts. In my case, the conflicts comes from the torch and jaxlib stick to different cudnn version, formerly i didn't seek to condaforge to install the cudatoolkit compatible for both torch and jaxlib. i use the pip command from the official jax documentation btw.","Ok, but in that case it is probably a good idea not to install `jax` and `jaxlib` from conda, and only install it from pip.","> Ok, but in that case it is probably a good idea not to install `jax` and `jaxlib` from conda, and only install it from pip. i think the only reason for the `jax` and `'jaxlib` suffix is to make sure the condaforge could search and install a compatible cudnn version, i did not do the test, so for the insurance, i recommend to annoyingly reinstall `jax` and `jaxlib` from pip","> > Ok, but in that case it is probably a good idea not to install `jax` and `jaxlib` from conda, and only install it from pip. >  > i think the only reason for the `jax` and `'jaxlib` suffix is to make sure the condaforge could search and install a compatible cudnn version, i did not do the test, so for the insurance, i recommend to annoyingly reinstall `jax` and `jaxlib` from pip But conda has no idea which version of cudnn the jaxlib installed via pip requires. If you want to install cudnn (and even a specific version) with conda, just install cudnn, to avoid problems is tipically useful to avoid to install jax or jaxlib via conda if you are installing it via pip.","> > > Ok, but in that case it is probably a good idea not to install `jax` and `jaxlib` from conda, and only install it from pip. > >  > >  > > i think the only reason for the `jax` and `'jaxlib` suffix is to make sure the condaforge could search and install a compatible cudnn version, i did not do the test, so for the insurance, i recommend to annoyingly reinstall `jax` and `jaxlib` from pip >  > But conda has no idea which version of cudnn the jaxlib installed via pip requires. If you want to install cudnn (and even a specific version) with conda, just install cudnn, to avoid problems is tipically useful to avoid to install jax or jaxlib via conda if you are installing it via pip. you are right, accidentally i use the pip install, and it just found the cudnn version meets the requirement lol."
1213,"以下是一个github上的jax下的一个issue, 标题是(Bitwise negation bug/no support)， 内容是 ( Description Hey there, I've recently encountered an interesting problem when using bitwise negation (```~```) operation on booleans that are immediate results of the functions. Here is a sample code: ``` from jax import numpy as jnp f = lambda x: x > 5 ~f(0)   returns 1, equivalent to ~0 ~f(10)  returns 2, equivalent to ~1 ``` It looks like JAX (or XLA) incorrectly reduces the negation operation with (probably) an intermediate representation of function output, though I don't know. This bug is extremely annoying to debug when passing a function as an argument to another function, so I think it is quite important to fix this, or do something else that would mitigate the problem. Of course, if one knows about the bug, it is easy to fix it with ```jnp.logical_not``` instead of ```~```. Cheers  What jax/jaxlib version are you using? jax v0.4.18; jaxlib v0.4.18  Which accelerator(s) are you using? CPU  Additional system info 3.10.4; Ubuntu 22.04; Kernel 5.15.81  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bitwise negation bug/no support," Description Hey there, I've recently encountered an interesting problem when using bitwise negation (```~```) operation on booleans that are immediate results of the functions. Here is a sample code: ``` from jax import numpy as jnp f = lambda x: x > 5 ~f(0)   returns 1, equivalent to ~0 ~f(10)  returns 2, equivalent to ~1 ``` It looks like JAX (or XLA) incorrectly reduces the negation operation with (probably) an intermediate representation of function output, though I don't know. This bug is extremely annoying to debug when passing a function as an argument to another function, so I think it is quite important to fix this, or do something else that would mitigate the problem. Of course, if one knows about the bug, it is easy to fix it with ```jnp.logical_not``` instead of ```~```. Cheers  What jax/jaxlib version are you using? jax v0.4.18; jaxlib v0.4.18  Which accelerator(s) are you using? CPU  Additional system info 3.10.4; Ubuntu 22.04; Kernel 5.15.81  NVIDIA GPU info _No response_",2023-10-10T09:59:38Z,bug needs info,closed,0,2,https://github.com/jax-ml/jax/issues/18031,"Note your reproduction doesn't use JAX at all! You're importing JAX but not using it: ``` f = lambda x: x > 5 ~f(0)   returns 1, equivalent to ~0 ~f(10)  returns 2, equivalent to ~1 ``` Did you get the correct repro?","Oh wow, I reduced my example so much, that it looks like it is not a bug in jax, but a 'bug' in python lambdas. Sorry! I have a piece of code that allows passing both python functions, and pjit functions, hence the confusion: jax counterpart (when doing ```~f(jnp.array(0))``` works as expected, unexpected was the difference in behaviours with pure Python lambdas."
1461,"以下是一个github上的jax下的一个issue, 标题是(Unable to correct CUDA vs. JAX version mismatch)， 内容是 ( Description I'm developing on a HPC cluster where I don't have the ability to modify the CUDA version and I'm getting: `CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12020, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` Is it possible to specify the CUDA version for JAX to build against? Meaning doing something like `pip install U ""jax[cuda12010_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` My intuition is that this would be done by specifying a version of JAX CUDA that was released prior to CUDA 12.2 however the documentation is unclear on how to download `pip install U ""jax[cuda12_pip]""` while specifying the version. I've tried ` pip install U ""jax[cuda12_pip]""==0.3.25` which results in `WARNING: jax 0.3.25 does not provide the extra 'cuda12_pip'` causing the CPU version to be installed in favor of the CUDA version.  What jax/jaxlib version are you using? 0.4.18  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` nvidiasmi Mon Oct  9 23:02:06 2023        ++  ++```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unable to correct CUDA vs. JAX version mismatch," Description I'm developing on a HPC cluster where I don't have the ability to modify the CUDA version and I'm getting: `CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12020, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` Is it possible to specify the CUDA version for JAX to build against? Meaning doing something like `pip install U ""jax[cuda12010_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` My intuition is that this would be done by specifying a version of JAX CUDA that was released prior to CUDA 12.2 however the documentation is unclear on how to download `pip install U ""jax[cuda12_pip]""` while specifying the version. I've tried ` pip install U ""jax[cuda12_pip]""==0.3.25` which results in `WARNING: jax 0.3.25 does not provide the extra 'cuda12_pip'` causing the CPU version to be installed in favor of the CUDA version.  What jax/jaxlib version are you using? 0.4.18  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` nvidiasmi Mon Oct  9 23:02:06 2023        ++  ++```",2023-10-09T23:02:30Z,bug NVIDIA GPU,closed,14,22,https://github.com/jax-ml/jax/issues/18027,"If you run: ``` pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` the right version of CUDA should be installed by `pip`. Can you show the output of `pip list` after doing that? If you did that (it sounds like you did), my guess is that you have another copy of CUDA installed and that JAX is preferring it for some reason. Is there another copy of CUDA present in your `LD_LIBRARY_PATH` directories and/or `PATH` directories? It is not possible to ""override"" the CUDA version JAX uses, but it should be very easy to `pip install` the correct version, and I'd like to understand why that's not working in your case.","I have a similar issue. ``` $ python3 Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.numpy.array(1) CUDA backend failed to initialize: Unable to load cuSOLVER. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Array(1, dtype=int32, weak_type=True) ``` This is after doing ... ``` $ pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Defaulting to user installation because normal sitepackages is not writeable Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Requirement already satisfied: jax[cuda12_pip] in ./.local/lib/python3.10/sitepackages (0.4.18) Requirement already satisfied: mldtypes>=0.2.0 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (0.2.0) Requirement already satisfied: numpy>=1.22 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (1.26.0) Requirement already satisfied: opteinsum in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (3.3.0) Requirement already satisfied: scipy>=1.7 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (1.11.2) Requirement already satisfied: jaxlib==0.4.18+cuda12.cudnn89 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (0.4.18+cuda12.cudnn89) Requirement already satisfied: nvidiacublascu12>=12.2.5.6 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (12.2.5.6) Requirement already satisfied: nvidiacudacupticu12>=12.2.142 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (12.2.142) Requirement already satisfied: nvidiacudanvcccu12>=12.2.140 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (12.2.140) Requirement already satisfied: nvidiacudaruntimecu12>=12.2.140 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (12.2.140) Requirement already satisfied: nvidiacudnncu12>=8.9 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (8.9.2.26) Requirement already satisfied: nvidiacufftcu12>=11.0.8.103 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (11.0.8.103) Requirement already satisfied: nvidiacusolvercu12>=11.5.2 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (11.5.2.141) Requirement already satisfied: nvidiacusparsecu12>=12.1.2.141 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (12.1.2.141) Requirement already satisfied: nvidiancclcu12>=2.18.3 in ./.local/lib/python3.10/sitepackages (from jax[cuda12_pip]) (2.18.3) Requirement already satisfied: nvidianvjitlinkcu12 in ./.local/lib/python3.10/sitepackages (from nvidiacusolvercu12>=11.5.2>jax[cuda12_pip]) (12.1.105) ``` Running with LD_DEBUG=libs shows that it's getting the right libraries, with one oddity: `/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/libcusparse.so.12: error: symbol lookup error: undefined symbol: __nvJitLinkGetLinkedCubin_12_2, version libnvJitLink.so.12 (fatal)` ```>>> a = jax.numpy.array(1) [...]    3088857:	find library=libnvJitLink.so.12 [0]; searching    3088857:	 search path=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/glibchwcaps/x8664v3:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/glibchwcaps/x8664v2:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/tls/x86_64/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/tls/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/tls/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/tls:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/x86_64/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/glibchwcaps/x8664v3:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/glibchwcaps/x8664v2:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/tls/x86_64/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/tls/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/tls/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/tls:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/x86_64/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/glibchwcaps/x8664v3:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/glibchwcaps/x8664v2:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/x86_64/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/x86_64/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib		(RUNPATH from file /home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/_versions.so)    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/glibchwcaps/x8664v3/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/glibchwcaps/x8664v2/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/tls/x86_64/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/tls/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/tls/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/tls/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/x86_64/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/glibchwcaps/x8664v3/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/glibchwcaps/x8664v2/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/tls/x86_64/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/tls/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/tls/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/tls/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/x86_64/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/x86_64/libnvJitLink.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/libnvJitLink.so.12    3088857:	    3088857:	find library=libcusparse.so.12 [0]; searching    3088857:	 search path=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/glibchwcaps/x8664v3:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/glibchwcaps/x8664v2:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/x86_64/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/x86_64/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/x86_64:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib		(RUNPATH from file /home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/_versions.so)    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/glibchwcaps/x8664v3/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/glibchwcaps/x8664v2/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/x86_64/x86_64/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/x86_64/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/x86_64/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/tls/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/x86_64/x86_64/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/x86_64/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/x86_64/libcusparse.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/libcusparse.so.12    3088857:	    3088857:	find library=libcublas.so.12 [0]; searching    3088857:	 search path=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib		(RUNPATH from file /home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/_versions.so)    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/libcublas.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/libcublas.so.12    3088857:	    3088857:	find library=libcublasLt.so.12 [0]; searching    3088857:	 search path=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../nvjitlink/lib:/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib		(RUNPATH from file /home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/_versions.so)    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/libcublasLt.so.12    3088857:	  trying file=/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cublas/lib/libcublasLt.so.12    3088857:	    3088857:	/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/libcusparse.so.12: error: symbol lookup error: undefined symbol: __nvJitLinkGetLinkedCubin_12_2, version libnvJitLink.so.12 (fatal) ``` If I manually upgrade via  ``` pip install upgrade nvidianvjitlinkcu12 Defaulting to user installation because normal sitepackages is not writeable Requirement already satisfied: nvidianvjitlinkcu12 in ./.local/lib/python3.10/sitepackages (12.1.105) Collecting nvidianvjitlinkcu12   Using cached nvidia_nvjitlink_cu1212.2.140py3nonemanylinux1_x86_64.whl (20.2 MB) Installing collected packages: nvidianvjitlinkcu12   Attempting uninstall: nvidianvjitlinkcu12     Found existing installation: nvidianvjitlinkcu12 12.1.105     Uninstalling nvidianvjitlinkcu1212.1.105:       Successfully uninstalled nvidianvjitlinkcu1212.1.105 Successfully installed nvidianvjitlinkcu1212.2.140 ``` then the error message changes, but still isn't right... ``` 20231010 21:32:22.307087: W external/xla/xla/service/gpu/nvptx_compiler.cc:703] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.2.140). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. ``` which might be a different issue??","I figured it out, though, not entirely. Unfortunately I still don't understand what went wrong with  `pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` but I did the following.  1. `pip install U ""https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.18+cuda12.cudnn88cp38cp38manylinux2014_x86_64.whl` 2. `pip install U https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.18+cuda12.cudnn88cp38cp38manylinux2014_x86_64.whl` 3. `pip install nvidiacublascu12 nvidiacudanvcccu12 nvidiacudaruntimecu12 nvidiacudnncu12 nvidiacufftcu12 nvidiacusolvercu12 nvidiacusparsecu12` I'm getting this new error/warning  ``` jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Input file '/tmp/tempfileip26014452294e005929599386076e282529c9.cubin' newer than toolkit (122 vs 120) ``` Which can be resolved with `os.environ[""XLA_FLAGS""] = ""xla_gpu_force_compilation_parallelism=1""` I've noticed ~40% regression on performance on a baseline I have run before on the same machine. Any ideas on if this is related to the warning or an error I've made in installation?   hopefully the fix I outlined can get you up and running. You'll need to change the url in steps 1 and 2 for your setup, options are @ `https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`"," Your issue should be fixed by PR CC(Add a version constraint on nvidianvjitlinkcu12.). The issue is that NVIDIA's own pip packages don't have a necessary version constraint, so `nvidianvjitlinkcu12` doesn't get upgraded. It's not a direct dependency of JAX, rather a transitive dependency. The warning `20231010 21:32:22.307087: W external/xla/xla/service/gpu/nvptx_compiler.cc:703] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.2.140). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. ` can be ignored if you like. It does no harm, it only slows down compilation a bit. The fix for that one is to upgrade your NVIDIA *driver*, like the message says. But you don't have to; nothing will go wrong if you don't.","> If you run: >  > ``` > pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html > ``` >  > the right version of CUDA should be installed by `pip`. Can you show the output of `pip list` after doing that? >  > If you did that (it sounds like you did), my guess is that you have another copy of CUDA installed and that JAX is preferring it for some reason. Is there another copy of CUDA present in your `LD_LIBRARY_PATH` directories and/or `PATH` directories? >  > It is not possible to ""override"" the CUDA version JAX uses, but it should be very easy to `pip install` the correct version, and I'd like to understand why that's not working in your case. Doin this does not automatically install the right version... in my case I have CUDA version 12010,, when I use that command, it installed the latest one but its against 12030 CUda version... what version of Jax that supports CUDA version 12010?"," We don't support CUDA 12.1, only 12.3 or later. You need to upgrade, and the `pip` command above should install the newer version. If you're still having trouble, share the output of `pip list | grep nvidia`. Are the up to date CUDA packages installed in the `pip` output?",">  We don't support CUDA 12.1, only 12.3 or later. You need to upgrade, and the `pip` command above should install the newer version. If you're still having trouble, share the output of `pip list  grep nvidia nvidiacublascu12       12.1.3.1 nvidiacudacupticu12   12.1.105 nvidiacudanvcccu12    12.4.99 nvidiacudanvrtccu12   12.1.105 nvidiacudaruntimecu12 12.1.105 nvidiacudnncu12        8.9.2.26 nvidiacufftcu12        11.0.2.54 nvidiacurandcu12       10.3.2.106 nvidiacusolvercu12     11.4.5.107 nvidiacusparsecu12     12.1.0.106 nvidiancclcu12         2.19.3 nvidianvjitlinkcu12    12.3.101 nvidianvtxcu12         12.1.105 ``` let me know what I should do to make it work. thanks thanks :)","Hello! I encountered a similar issue while I tried to make a new conda environment with JAX cuda12. After ``` pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` I got the error messages below: CUDA backend failed to initialize: Found cuBLAS version 120205, but JAX was built against version 120304, which is newer.  The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Here is my output of the command ```pip list | grep nvidia``` : ``` nvidiacublascu12        12.4.2.65 nvidiacudacupticu12    12.4.99 nvidiacudanvcccu12     12.4.99 nvidiacudanvrtccu12    12.4.99 nvidiacudaruntimecu12  12.4.99 nvidiacudnncu12         8.9.7.29 nvidiacufftcu12         11.2.0.44 nvidiacusolvercu12      11.6.0.99 nvidiacusparsecu12      12.3.0.142 nvidiancclcu12          2.20.5 nvidianvjitlinkcu12     12.4.99 ``` Could you help me??  I found that my HPC cluster has automatically loaded a module of CUDA 12.2. ```module unload cuda122``` resolved the issue without downgrading JAX.","CUDA backend failed to initialize: Found cuBLAS version 120205, but JAX was built against version 120304, which is newer. I have encountered this issue and solved it by specifying an older version (`0.4.23`): ```bash pip install upgrade ""jax[cuda12_local]==0.4.23"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```","0.4.20 is works on me which `Found cuBLAS version 120103` same solution as   ```shell pip install upgrade ""jax[cuda12_pip]==0.4.20"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```",None of these solutions work for the Grace Hopper 200. ,In my case I had to downgrade the nvidia* pip pacakges from `12.4` to `12.3`. Though I really think that was the `nvidiacudnncu12`  package downgrade from `9.0.0.312` => `8.9.7.29` which fixed the  `CUDA backend failed to initialize: Unable to load cuDNN. Is it installed?` error for me. The simplest was to just run: ``` python3 m pip install tensorflow[andcuda] ``` Which did just that:    Before ```  pip list grep nvidia nvidiacublascu12           12.3.4.1 nvidiacudacupticu12       12.3.101 nvidiacudanvcccu12        12.3.107 nvidiacudanvrtccu12       12.3.107 nvidiacudaruntimecu12     12.3.101 nvidiacudnncu12            8.9.7.29 nvidiacufftcu12            11.0.12.1 nvidiacurandcu12           10.3.4.107 nvidiacusolvercu12         11.5.4.101 nvidiacusparsecu12         12.2.0.103 nvidiancclcu12             2.19.3 nvidianvjitlinkcu12        12.3.101 ``` ,> In my case I had to downgrade the nvidia* pip pacakges from `12.4` to `12.3`. Though I really think that was the `nvidiacudnncu12` package downgrade from `9.0.0.312` => `8.9.7.29` which fixed the `CUDA backend failed to initialize: Unable to load cuDNN. Is it installed?` error for me. >  > The simplest was to just run: >  > ``` > python3 m pip install tensorflow[andcuda] > ``` >  > Which did just that: >  > Before > After Worked like a charm! thanks!,"> In my case I had to downgrade the nvidia* pip pacakges from `12.4` to `12.3`. Though I really think that was the `nvidiacudnncu12` package downgrade from `9.0.0.312` => `8.9.7.29` which fixed the `CUDA backend failed to initialize: Unable to load cuDNN. Is it installed?` error for me. >  > The simplest was to just run: >  > ``` > python3 m pip install tensorflow[andcuda] > ``` >  > Which did just that: >  > Before > After I got the same `Unable to load cuDNN` error when installing Jax using  ``` pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` I was able to solve this issue just by doing this, thank you so much!","> In my case I had to downgrade the nvidia* pip pacakges from `12.4` to `12.3`. Though I really think that was the `nvidiacudnncu12` package downgrade from `9.0.0.312` => `8.9.7.29` which fixed the `CUDA backend failed to initialize: Unable to load cuDNN. Is it installed?` error for me. >  > The simplest was to just run: >  > ``` > python3 m pip install tensorflow[andcuda] > ``` >  > Which did just that: > Before > After This worked like hell, after 2 hours of work!!! Thank you  !!!",`pip install nvidiacudnncu12==8.9.7.29` is a more direct/simpler fix than `python3 m pip install tensorflow[andcuda]`,"Try set `LD_LIBRARY_PATH=""$YOUR_CUDA_PATH""` in the environment variables. Sometimes it's just jax cannot find the proper version. YOUR_CUDA_PATH can be for example `/usr/local/cuda12.1/lib64`","I'm having a similar issue with the newer version of jaxlib 0.4.26 and CUDA 12.1 and running ```python3 m pip install tensorflow[andcuda]```  doesn't do the trick. In this case, the error seems to be derived from `cuSPARSE` **When running an scVI model I get the following message:** ``` CUDA backend failed to initialize: Unable to use CUDA because of the following issues with CUDA components: Outdated cuSPARSE installation found. Version JAX was built against: 12200 Minimum supported: 12100 Installed version: 12002 The local installation version must be no lower than 12100. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) GPU available: True (cuda), used: True TPU available: False, using: 0 TPU cores IPU available: False, using: 0 IPUs HPU available: False, using: 0 HPUs LOCAL_RANK: 0  CUDA_VISIBLE_DEVICES: [0] ``` I find it confusing that the message says the CUDA backend failed to initialize, but then the trainer spits out `GPU available: True (cuda), used: True` which I think suggests cuda is still being used... Should I ignore this? Also, not sure what it means by `installed version 12002`. That doesn't track with the actual versions installed (see below). I'm developing in an HPC so I have limited options to change CUDA. I'm working with CUDA 12.1 and I need to have pytorch and jax in the same environment. The jax 0.4.26 released last month should be compatible with this version of CUDA, and I installed them as recommended by the developers using: ``` conda install pytorch torchvision torchaudio pytorchcuda=12.1 c pytorch c nvidia pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` **NVIDIA drivers** ``` nvidiasmi ++  grep jax`** ``` jax                           0.4.26 jaxlib                        0.4.26+cuda12.cudnn89 ``` Any thoughts/ideas are greatly appreciated!","> `pip install nvidiacudnncu12==8.9.7.29` is a more direct/simpler fix than `python3 m pip install tensorflow[andcuda]` Worked for me. I guess because I had `0.4.20+cuda12.cudnn89`, which didn't work with the `nvidiacudnncu129.1.0.70`. ","How can i slove this problem? ``` 20240513 15:55:10.465794: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.1 which is older than the ptxas CUDA version (12.4.131). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. (100, 6) (100, 21, 3) (100, 21) (100, 21) /home/l/anaconda3/envs/CFM/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.   self.pid = os.fork() ``` pip list  grep jax jax                      0.4.28 jaxlib                   0.4.28+cuda12.cudnn89", Please open a new discussion for this topic. I'm also not clear exactly which issue you're trying to solve.,"Used the `tensorflow[andcuda]` method from this thread to fix this problem when I originally ran into it a few months ago. Got this error again today while messing with drivers, and installing `tensorflow[andcuda]` didn't solve the problem. For a dirty fix with driver version 12.2, uninstalling `tensorflow` then running  ``` pip install extraindexurl https://pypi.nvidia.com tensorrtbindings==8.6.1 tensorrtlibs==8.6.1 pip install U tensorflow[andcuda]==2.15.0 ``` worked for me."
390,"以下是一个github上的jax下的一个issue, 标题是([JAX] Add an option `subset_by_index` that allows computing a contiguous subset of eigenvalues from eigh.)， 内容是 ([JAX] Add an option `subset_by_index` that allows computing a contiguous subset of eigenvalues from eigh.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[JAX] Add an option `subset_by_index` that allows computing a contiguous subset of eigenvalues from eigh.,[JAX] Add an option `subset_by_index` that allows computing a contiguous subset of eigenvalues from eigh.,2023-10-09T22:22:03Z,,closed,0,1,https://github.com/jax-ml/jax/issues/18026,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
753,"以下是一个github上的jax下的一个issue, 标题是(cuSOLVER version question)， 内容是 ( Description When to use jnp.array(),it remind that""CUDA backend failed to initialize: Found cuSOLVER version 11501, but JAX was built against version 11502, which is newer. The copy of cuSOLVER that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ""  What jax/jaxlib version are you using? jax0.4.18 jaxlib0.4.18+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info 3.10.9/Linux  NVIDIA GPU info Mon Oct  9 22:20:04 2023        ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,cuSOLVER version question," Description When to use jnp.array(),it remind that""CUDA backend failed to initialize: Found cuSOLVER version 11501, but JAX was built against version 11502, which is newer. The copy of cuSOLVER that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ""  What jax/jaxlib version are you using? jax0.4.18 jaxlib0.4.18+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info 3.10.9/Linux  NVIDIA GPU info Mon Oct  9 22:20:04 2023        ++  ++",2023-10-09T14:20:46Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18009,"That's correct. In the latest JAX releases, we added code to verify that your installed CUDA libraries are at least as new as the libraries against which JAX was built. You can fix by updating your copy of cusolver. That said, we're probably being too strict here since we're requiring matching patch versions, so I'll relax that version test.",Hi   May I ask how to update the cusolver? Download and install a new CUDA Toolkit? Is it correct?
3627,"以下是一个github上的jax下的一个issue, 标题是( Issue with JAX's frompyfunc and at methods when compared to NumPy)， 内容是 ( Description I encountered an issue when trying to use JAX's frompyfunc and at methods for a specific use case that works fine in NumPy. Below is the code snippet that demonstrates the issue:  ``` import jax.numpy as jnp def scalar_add(x, y):    emphasize that only scalar tracers will be passed to this function.   assert jnp.shape(x) == jnp.shape(y) == ()   return x + y add = jnp.frompyfunc(scalar_add, nin=2, nout=1, identity=0) x = jnp.ones((5, 3)) indices = jnp.array([0,4,2]) t = jnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) x = add.at(x, indices, t, inplace=False)  ``` Expected Behavior: The code above is expected to perform an addition operation using JAX's frompyfunc and at methods, similar to the behavior in NumPy. Actual Behavior: However, running this code results in the following error: ```  ValueError                                Traceback (most recent call last) [](https://localhost:8080/) in ()      17       18  add.accumulate(x)   accumulate() method is cumulative reduction > 19 z = add.at(x, indices, t, inplace=False)   at() method is similar to JAX's ndarray.at     [... skipping hidden 12 frame] 3 frames /usr/local/lib/python3.10/distpackages/jax/_src/numpy/ufunc_api.py in at(self, a, indices, b, inplace)     238       return self._at_via_scan(a, indices)     239     else: > 240       return self._at_via_scan(a, indices, b)     241      242   def _at_via_scan(self, a, indices, *args): /usr/local/lib/python3.10/distpackages/jax/_src/numpy/ufunc_api.py in _at_via_scan(self, a, indices, *args)     254       return a.at[indices].set(self._call(a.at[indices].get(), *args))     255  > 256     args = tuple(_broadcast_to(arg, shape).ravel() for arg in args)     257     indices = [idx if isinstance(idx, slice) else _broadcast_to(idx, shape).ravel() for idx in indices]     258  /usr/local/lib/python3.10/distpackages/jax/_src/numpy/ufunc_api.py in (.0)     254       return a.at[indices].set(self._call(a.at[indices].get(), *args))     255  > 256     args = tuple(_broadcast_to(arg, shape).ravel() for arg in args)     257     indices = [idx if isinstance(idx, slice) else _broadcast_to(idx, shape).ravel() for idx in indices]     258  /usr/local/lib/python3.10/distpackages/jax/_src/numpy/util.py in _broadcast_to(arr, shape)     397     shape_tail = shape[nlead:]     398     compatible = all(core.definitely_equal_one_of_dim(arr_d, [1, shape_d]) > 399                      for arr_d, shape_d in safe_zip(arr_shape, shape_tail))     400     if nlead < 0 or not compatible:     401       msg = ""Incompatible shapes for broadcasting: {} and requested shape {}"" ValueError: safe_zip() argument 2 is shorter than argument 1 ``` Additional Information: This code works as expected in NumPy using the equivalent NumPy functions. The issue appears to be specific to JAX's implementation.  Numpy seems to work fine with the following results. ``` array([[ 2.,  3.,  4.],        [ 1.,  1.,  1.],        [ 8.,  9., 10.],        [ 1.,  1.,  1.],        [ 5.,  6.,  7.]]) ``` Environment: JAX version: 0.4.16 Python version: 0.4.16+cuda11.cudnn86 Operating System: Colab Please let me know if any further information or clarification is needed to address this issue.  What jax/jaxlib version are you using? jax: 0.4.16, jaxlib: 0.4.16+cuda11.cudnn86  Which accelerator(s) are you using? GPU T4   Additional system info On Colab  NVIDIA GPU info nvidiasmi)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi, Issue with JAX's frompyfunc and at methods when compared to NumPy," Description I encountered an issue when trying to use JAX's frompyfunc and at methods for a specific use case that works fine in NumPy. Below is the code snippet that demonstrates the issue:  ``` import jax.numpy as jnp def scalar_add(x, y):    emphasize that only scalar tracers will be passed to this function.   assert jnp.shape(x) == jnp.shape(y) == ()   return x + y add = jnp.frompyfunc(scalar_add, nin=2, nout=1, identity=0) x = jnp.ones((5, 3)) indices = jnp.array([0,4,2]) t = jnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) x = add.at(x, indices, t, inplace=False)  ``` Expected Behavior: The code above is expected to perform an addition operation using JAX's frompyfunc and at methods, similar to the behavior in NumPy. Actual Behavior: However, running this code results in the following error: ```  ValueError                                Traceback (most recent call last) [](https://localhost:8080/) in ()      17       18  add.accumulate(x)   accumulate() method is cumulative reduction > 19 z = add.at(x, indices, t, inplace=False)   at() method is similar to JAX's ndarray.at     [... skipping hidden 12 frame] 3 frames /usr/local/lib/python3.10/distpackages/jax/_src/numpy/ufunc_api.py in at(self, a, indices, b, inplace)     238       return self._at_via_scan(a, indices)     239     else: > 240       return self._at_via_scan(a, indices, b)     241      242   def _at_via_scan(self, a, indices, *args): /usr/local/lib/python3.10/distpackages/jax/_src/numpy/ufunc_api.py in _at_via_scan(self, a, indices, *args)     254       return a.at[indices].set(self._call(a.at[indices].get(), *args))     255  > 256     args = tuple(_broadcast_to(arg, shape).ravel() for arg in args)     257     indices = [idx if isinstance(idx, slice) else _broadcast_to(idx, shape).ravel() for idx in indices]     258  /usr/local/lib/python3.10/distpackages/jax/_src/numpy/ufunc_api.py in (.0)     254       return a.at[indices].set(self._call(a.at[indices].get(), *args))     255  > 256     args = tuple(_broadcast_to(arg, shape).ravel() for arg in args)     257     indices = [idx if isinstance(idx, slice) else _broadcast_to(idx, shape).ravel() for idx in indices]     258  /usr/local/lib/python3.10/distpackages/jax/_src/numpy/util.py in _broadcast_to(arr, shape)     397     shape_tail = shape[nlead:]     398     compatible = all(core.definitely_equal_one_of_dim(arr_d, [1, shape_d]) > 399                      for arr_d, shape_d in safe_zip(arr_shape, shape_tail))     400     if nlead < 0 or not compatible:     401       msg = ""Incompatible shapes for broadcasting: {} and requested shape {}"" ValueError: safe_zip() argument 2 is shorter than argument 1 ``` Additional Information: This code works as expected in NumPy using the equivalent NumPy functions. The issue appears to be specific to JAX's implementation.  Numpy seems to work fine with the following results. ``` array([[ 2.,  3.,  4.],        [ 1.,  1.,  1.],        [ 8.,  9., 10.],        [ 1.,  1.,  1.],        [ 5.,  6.,  7.]]) ``` Environment: JAX version: 0.4.16 Python version: 0.4.16+cuda11.cudnn86 Operating System: Colab Please let me know if any further information or clarification is needed to address this issue.  What jax/jaxlib version are you using? jax: 0.4.16, jaxlib: 0.4.16+cuda11.cudnn86  Which accelerator(s) are you using? GPU T4   Additional system info On Colab  NVIDIA GPU info nvidiasmi",2023-10-08T05:09:23Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18004,Thanks for the report! I'm looking into it
1399,"以下是一个github上的jax下的一个issue, 标题是(Jax/Brax program Crashes When Using Jacrev, but not Jacfwd)， 内容是 ( Description I opened a parallel issue in Brax, but was looking for some general JAX knowledge. How do I troubleshoot Jax crashing in an intelligent way? I saw this issue mention throwing error messages rather than crashing, but that does not seem to help me.  Essentially I am trying to persistently cache jacobians of the step function in brax on GPU, but am finding that if I use jacrev, the program will crash anytime the code is run a second time (when it accesses the persistent cache) and will do so without any error message. However, if I compute the jacobian and cache it with jacfwd, the cache is accessed without issue. Provided below is a minimal example using brax + jax. To reproduce my issue: Inside of minimal.py set which_calc to either jacfwd or jacrev Run minimal.py  entire program will execute and ./cache will be made and populated Run minimal.py again  second print (line 63) will not execute and program will quit prematurely if using jacrev, but not jacfwd minimal.txt  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Using a NVIDIA GeForce RTX 4090)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Jax/Brax program Crashes When Using Jacrev, but not Jacfwd"," Description I opened a parallel issue in Brax, but was looking for some general JAX knowledge. How do I troubleshoot Jax crashing in an intelligent way? I saw this issue mention throwing error messages rather than crashing, but that does not seem to help me.  Essentially I am trying to persistently cache jacobians of the step function in brax on GPU, but am finding that if I use jacrev, the program will crash anytime the code is run a second time (when it accesses the persistent cache) and will do so without any error message. However, if I compute the jacobian and cache it with jacfwd, the cache is accessed without issue. Provided below is a minimal example using brax + jax. To reproduce my issue: Inside of minimal.py set which_calc to either jacfwd or jacrev Run minimal.py  entire program will execute and ./cache will be made and populated Run minimal.py again  second print (line 63) will not execute and program will quit prematurely if using jacrev, but not jacfwd minimal.txt  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Using a NVIDIA GeForce RTX 4090",2023-10-06T21:24:55Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/17993
1005,"以下是一个github上的jax下的一个issue, 标题是(identify PRNG schemes on key arrays; recognize them in key constructors)， 内容是 (This change covers another iteration on CC([random] wrap_key_data accepts impl=key.dtype.impl), following discussion there. Specifically: * Introduce `jax.random.key_impl`, which accepts a key array and returns a hashable identifier of its PRNG implementation. * Accept this identifier optionally as the `impl` argument to `jax.random.key` and `wrap_key_data`. This now works: ```python k1 = jax.random.key(72, impl='threefry2x32') impl = jax.random.key_impl(k1) k2 = jax.random.key(72, impl=impl) assert arrays_equal(k1, k2) assert k1.dtype == k2.dtype ``` This change also set up an internal PRNG registry and register builtin implementations, to simplify various places where we essentially reconstruct such a registry from scratch (such as in tests).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,identify PRNG schemes on key arrays; recognize them in key constructors,"This change covers another iteration on CC([random] wrap_key_data accepts impl=key.dtype.impl), following discussion there. Specifically: * Introduce `jax.random.key_impl`, which accepts a key array and returns a hashable identifier of its PRNG implementation. * Accept this identifier optionally as the `impl` argument to `jax.random.key` and `wrap_key_data`. This now works: ```python k1 = jax.random.key(72, impl='threefry2x32') impl = jax.random.key_impl(k1) k2 = jax.random.key(72, impl=impl) assert arrays_equal(k1, k2) assert k1.dtype == k2.dtype ``` This change also set up an internal PRNG registry and register builtin implementations, to simplify various places where we essentially reconstruct such a registry from scratch (such as in tests).",2023-10-06T15:16:10Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17983
1552,"以下是一个github上的jax下的一个issue, 标题是(jax_threefry_partitionable + rematerialization doesn't seem to be working together in distributed training)， 内容是 ( Description I have a transformer model where each transformer block is rematerialized. The model is distributed over multiple devices using jit. Each transformer block has dropout enabled. To prevent rng implementation from inserting synchronization operations I'm also enabling `jax_threefry_partitionable` as suggested in the doc. Problem is, `jax_threefry_partitionable` doesn't seem to play nicely with rematerialization. As soon as I enable dropout, I get GPU OOM because JAX decides to preserve huge arrays containing rng key per activation tensor component for each transformer block, despite them being rematerialized. It should be possible for jax to reconstruct this key array from a single key during rematerialization, but it doesn't seem to do that. I'm happy to provide a repoduction if you can confirm that this is unexpected behavior. If not, can you please suggest a workaround? Currently it doesn't seem possible to efficiently train large models with dropout. A relevant discussion with OOM error message example here: https://github.com/google/flax/discussions/3090  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info python3.10  NVIDIA GPU info ``` Fri Oct  6 14:22:06 2023 ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,jax_threefry_partitionable + rematerialization doesn't seem to be working together in distributed training," Description I have a transformer model where each transformer block is rematerialized. The model is distributed over multiple devices using jit. Each transformer block has dropout enabled. To prevent rng implementation from inserting synchronization operations I'm also enabling `jax_threefry_partitionable` as suggested in the doc. Problem is, `jax_threefry_partitionable` doesn't seem to play nicely with rematerialization. As soon as I enable dropout, I get GPU OOM because JAX decides to preserve huge arrays containing rng key per activation tensor component for each transformer block, despite them being rematerialized. It should be possible for jax to reconstruct this key array from a single key during rematerialization, but it doesn't seem to do that. I'm happy to provide a repoduction if you can confirm that this is unexpected behavior. If not, can you please suggest a workaround? Currently it doesn't seem possible to efficiently train large models with dropout. A relevant discussion with OOM error message example here: https://github.com/google/flax/discussions/3090  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info python3.10  NVIDIA GPU info ``` Fri Oct  6 14:22:06 2023 ++  ++ ```",2023-10-06T14:22:33Z,bug,open,9,16,https://github.com/jax-ml/jax/issues/17982,I strugle with the same issue.,"Also relevant for me, would be great to have it solved."," Hey, sorry for mentioning you directly, but this issue hasn't received any attention for several weeks. Can someone from the jax team please take a look? Thanks!"," Hey, sorry for mentioning you directly, but can someone take a look at this issue? It's a big blocker for me.","I've made a repro for this bug. Turns out it has nothing to do with `jax_threefry_partitionable`, perfectly repoducible without it. Repo was made for A100 80Gb, so tensor shapes might need to be adjusted for a GPU with a different amount of memory. `./repro.py` — will fail because without rematerialization it needs ~122.75 Gb of GPU RAM `./repro.py remat` — works perfectly fine with remat, because it now needs just 63Gb of GPU RAM `./repro.py remat dropoutrate 0.1` — OOMs again, requiring ~118Gb of GPU RAM. From looking at peak buffers it becomes clear that the dropout mask is not being rematerialized: tensors correponding to full dropout masks for different layers are occupying memory. ``` Peak buffers:         Buffer 1:                 Size: 512.00MiB                 Operator: op_name=""jit(train_step)/jit(main)/jvp(Model)/Block_5._apply_block/Block_5/Dropout_0/jit(_bernoulli)/jit(_uniform)/threefry2x32"" source_file=""/papyrax/./tools/repro.py"" source_line=24                 XLA Label: customcall                 Shape: u32[134217728]                 ==========================         Buffer 2:                 Size: 512.00MiB                 Operator: op_name=""jit(train_step)/jit(main)/jvp(Model)/Block_4._apply_block/Block_4/Dropout_0/jit(_bernoulli)/jit(_uniform)/threefry2x32"" source_file=""/papyrax/./tools/repro.py"" source_line=24                 XLA Label: customcall                 Shape: u32[134217728]                 ==========================         Buffer 3:                 Size: 512.00MiB                 Operator: op_name=""jit(train_step)/jit(main)/jvp(Model)/Block_4._apply_block/Block_4/Dropout_0/jit(_bernoulli)/jit(_uniform)/threefry2x32"" source_file=""/papyrax/./tools/repro.py"" source_line=24                 XLA Label: customcall                 Shape: u32[134217728]                 ==========================         Buffer 4:                 Size: 512.00MiB                 Operator: op_name=""jit(train_step)/jit(main)/jvp(Model)/Block_3._apply_block/Block_3/Dropout_0/jit(_bernoulli)/jit(_uniform)/threefry2x32"" source_file=""/papyrax/./tools/repro.py"" source_line=24                 XLA Label: customcall                 Shape: u32[134217728]                 ==========================         Buffer 5:                 Size: 512.00MiB                 Operator: op_name=""jit(train_step)/jit(main)/jvp(Model)/Block_3._apply_block/Block_3/Dropout_0/jit(_bernoulli)/jit(_uniform)/threefry2x32"" source_file=""/papyrax/./tools/repro.py"" source_line=24                 XLA Label: customcall                 Shape: u32[134217728]                 ========================== ... ``` Repro code: ```python import functools import click import flax import flax.linen as nn import flax.training.train_state import jax import jax.numpy as jnp import optax class Dropout(nn.Module):     rate: float     .compact     def __call__(self, inputs, rng):         if self.rate == 0.0:             return inputs         if self.rate == 1.0:             return jnp.zeros_like(inputs)         keep_prob = 1.0  self.rate         mask = jax.random.bernoulli(rng, p=keep_prob, shape=inputs.shape)         return jax.lax.select(mask, inputs / keep_prob, jnp.zeros_like(inputs)) class Block(nn.Module):     dim: int     dropout_rate: float     .compact     def __call__(self, input, rng):         scale = 32   We want large memory consumption without remat         emb = nn.Dense(features=self.dim * scale)(input)         emb = nn.relu(emb)         emb = Dropout(rate=self.dropout_rate)(emb, rng)         emb = nn.Dense(features=self.dim)(emb)         return emb class Model(nn.Module):     dim: int     dropout_rate: float     num_layers: int     remat: bool     .compact     def __call__(self, input, rng):         def _apply_block(block, block_input, rng):             return block(block_input, rng)         if self.remat:             _apply_block = nn.checkpoint(                 _apply_block,                 policy=jax.checkpoint_policies.nothing_saveable,                 prevent_cse=True,             )         emb = input         for _ in range(self.num_layers):             rng, block_rng = jax.random.split(rng)             block = Block(dim=self.dim, dropout_rate=self.dropout_rate)             emb = _apply_block(block, emb, block_rng)         return emb def loss_fn(params, train_state, batch, rng):     outputs = train_state.apply_fn(params, batch, rng)     return jnp.mean(outputs * outputs) .partial(jax.jit, donate_argnames=(""train_state"",)) def train_step(train_state, batch, rng):     grad_fn = jax.grad(loss_fn)     grad = grad_fn(train_state.params, train_state, batch, rng)     train_state = train_state.apply_gradients(grads=grad)     return train_state def make_batch(batch_size, dim):     return jnp.zeros(shape=(batch_size, dim), dtype=jnp.float32) .command() .option(""dim"", type=int, default=1024) .option(""batchsize"", type=int, default=8192) .option(""dropoutrate"", type=float, default=0.0) .option(""numlayers"", type=int, default=64) .option(""remat"", is_flag=True) def main(     dim: int,     batch_size: int,     dropout_rate: float,     num_layers: int,     remat: bool, ):     model = Model(         dim=dim, dropout_rate=dropout_rate, num_layers=num_layers, remat=remat     )     batch = make_batch(batch_size=batch_size, dim=dim)     rng = jax.random.PRNGKey(0)     params = model.init({""params"": rng}, batch, rng)     optimizer = optax.adam(learning_rate=1e3)     train_state = flax.training.train_state.TrainState.create(         apply_fn=model.apply, params=params, tx=optimizer     )     train_state = train_step(train_state, batch, rng) if __name__ == ""__main__"":     main() ```",Thanks for the repro!," sorry that this slipped through the cracks. Thanks for the pings, everyone. Can you check that this repros with jaxlib 0.4.20? IIRC there was one GPUspecific remat fix that happened recently, though I don't have a link to it at the moment. EDIT: https://github.com/openxla/xla/pull/6527","Thanks for the pointer! Unfortunately, it looks like the problem is still present with jaxlib==0.4.20","Thanks for checking. I think our next step is to try to repro on TPU, to see if it's GPUspecific. We can do that on our end.","Hey, any updates on this?","  Happy new year, gentlemen! Do you think 2024 is the year when this bug finally got fixed? ;) ",Ping!,"Hmm, looks like using `jax_default_prng_impl=rbg` fixes this issue.","> Hmm, looks like using `jax_default_prng_impl=rbg` fixes this issue. Thanks, this is a useful additional bit of info. This is still in our queue, but we haven't dug in yet. I understood your most recent comment to mean that you have a workaround. Is that right? At large scales, `jax_default_prng_impl=rbg` can be a good idea to try anyway, as it can drastically speed up compilation times.","> I understood your most recent comment to mean that you have a workaround. Is that right? Looks like it. Interestingly, it also seems to fix another rngrelated issue: https://github.com/google/jax/issues/19893 Btw, can you elaborate a bit on how does the `rng` implementation work when keys are sharded? E.g. does it require any additional communication?","On GPU, for a fixed key, I do not expect that sharded _number generation_ under `rbg` would require communication. E.g. I expect the following to print `False`: ```python import jax import jax.numpy as jnp .jit def f(key, x):   numbers = jax.random.uniform(key, x.shape)   return x + numbers key = jax.random.key(42) x_sharding = jax.sharding.PositionalSharding(jax.devices()) x = jax.device_put(jnp.arange(24.), x_sharding) f_exe = f.lower(key, x).compile() print('Communicating?', 'collectivepermute' in f_exe.as_text()) ``` (and the same if we check for other collectives in the HLO.) Meanwhile I also expect the output sharding of `f(key, x)` to be, e.g.: ``` PositionalSharding([{GPU 0} {GPU 1}], shape=(2,)) ``` when `jax.devices()` is a list of two GPUs. Your comment however asks ""when keys are sharded."" Do you mean that you are sharding a computation that vmaps a random number generation operation over a batch of keys (in the form of a sharded key array)? If so, then there's a current unrelated issue to watch specifically regarding `vmap` of `rbg` over keys, covered by CC(efficient untrue batching of `random_bit_generator`). The workaround there is not to `vmap` number generation over keys, but instead to hoist the generation step: draw the entire batch of random numbers from a single key outside of the vmapped function, and pass that in."
6599,"以下是一个github上的jax下的一个issue, 标题是(Error with random.PRNGKey)， 内容是 ( Description Error occurred when generate PRNGKey with jax.random  ``` from jax import random key = random.PRNGKey(0) ``` After calling the above lines  ``` Unexpected exception formatting exception. Falling back to standard exception Traceback (most recent call last):   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py"", line 3460, in run_code     exec(code_obj, self.user_global_ns, self.user_ns)   File ""/tmp/ipykernel_2242/4187220659.py"", line 2, in      key = random.PRNGKey(0)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/random.py"", line 133, in PRNGKey     if impl_spec is None:   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 267, in seed_with_impl        File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 575, in random_seed     return PRNGKeyArrayImpl(aval.dtype.impl, phys_result)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 343, in bind   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 346, in bind_with_trace     self.aval = aval   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 728, in process_primitive      raises a useful error on attempts to pickle a Tracer.   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 587, in random_seed_impl     physical_aval = keys_aval_to_base_arr_aval(aval)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 592, in random_seed_impl_base     return random_wrap(physical_result, impl=aval.dtype.impl)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 827, in threefry_seed     out_shape = (*keys_aval.shape, *shape)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 509, in shift_right_logical   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 343, in bind   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 346, in bind_with_trace     self.aval = aval   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 728, in process_primitive      raises a useful error on attempts to pickle a Tracer.   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 118, in apply_primitive     for s, o in zip(self.shardings, other.shardings))   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/util.py"", line 254, in wrapper     .lru_cache(max_size)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/util.py"", line 247, in cached     assert not new_lhs   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 202, in xla_primitive_callable   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 359, in _xla_callable_uncached      remove it once we bring the id_tap implementation into the core.   File ""/opt/conda/lib/python3.10/sitepackages/jax/interpreters/pxla.py"", line 3204, in compile   File ""/opt/conda/lib/python3.10/sitepackages/jax/interpreters/pxla.py"", line 3167, in _compile_unloaded   File ""/opt/conda/lib/python3.10/sitepackages/jax/interpreters/pxla.py"", line 3447, in from_hlo   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1044, in compile_or_get_cached   File ""/opt/conda/lib/python3.10/sitepackages/jax/experimental/compilation_cache/compilation_cache.py"", line 15, in      from jax._src.compilation_cache import (   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/compilation_cache.py"", line 28, in      from jax._src import cache_key   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/cache_key.py"", line 29, in      from jax._src.lib.mlir import passmanager as pm ImportError: cannot import name 'passmanager' from 'jax._src.lib.mlir' (/opt/conda/lib/python3.10/sitepackages/jax/_src/lib/mlir/__init__.py) During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py"", line 2057, in showtraceback     stb = self.InteractiveTB.structured_traceback(   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 1288, in structured_traceback     return FormattedTB.structured_traceback(   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 1177, in structured_traceback     return VerboseTB.structured_traceback(   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 1030, in structured_traceback     formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 960, in format_exception_as_a_whole     frames.append(self.format_record(record))   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 870, in format_record     frame_info.lines, Colors, self.has_colors, lvals   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 704, in lines     return self._sd.lines   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/utils.py"", line 144, in cached_property_wrapper     value = obj.__dict__[self.func.__name__] = self.func(obj)   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/core.py"", line 734, in lines     pieces = self.included_pieces   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/utils.py"", line 144, in cached_property_wrapper     value = obj.__dict__[self.func.__name__] = self.func(obj)   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/core.py"", line 681, in included_pieces     pos = scope_pieces.index(self.executing_piece)   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/utils.py"", line 144, in cached_property_wrapper     value = obj.__dict__[self.func.__name__] = self.func(obj)   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/core.py"", line 660, in executing_piece     return only(   File ""/home/jih201/.local/lib/python3.10/sitepackages/executing/executing.py"", line 190, in only     raise NotOneValueFound('Expected one value, found 0') executing.executing.NotOneValueFound: Expected one value, found 0 ```  What jax/jaxlib version are you using? jax v0.4.2  Which accelerator(s) are you using? GPU   Additional system info Python 3.10.9, OS(Linux)  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Error with random.PRNGKey," Description Error occurred when generate PRNGKey with jax.random  ``` from jax import random key = random.PRNGKey(0) ``` After calling the above lines  ``` Unexpected exception formatting exception. Falling back to standard exception Traceback (most recent call last):   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py"", line 3460, in run_code     exec(code_obj, self.user_global_ns, self.user_ns)   File ""/tmp/ipykernel_2242/4187220659.py"", line 2, in      key = random.PRNGKey(0)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/random.py"", line 133, in PRNGKey     if impl_spec is None:   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 267, in seed_with_impl        File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 575, in random_seed     return PRNGKeyArrayImpl(aval.dtype.impl, phys_result)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 343, in bind   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 346, in bind_with_trace     self.aval = aval   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 728, in process_primitive      raises a useful error on attempts to pickle a Tracer.   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 587, in random_seed_impl     physical_aval = keys_aval_to_base_arr_aval(aval)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 592, in random_seed_impl_base     return random_wrap(physical_result, impl=aval.dtype.impl)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/prng.py"", line 827, in threefry_seed     out_shape = (*keys_aval.shape, *shape)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 509, in shift_right_logical   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 343, in bind   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 346, in bind_with_trace     self.aval = aval   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/core.py"", line 728, in process_primitive      raises a useful error on attempts to pickle a Tracer.   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 118, in apply_primitive     for s, o in zip(self.shardings, other.shardings))   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/util.py"", line 254, in wrapper     .lru_cache(max_size)   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/util.py"", line 247, in cached     assert not new_lhs   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 202, in xla_primitive_callable   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 359, in _xla_callable_uncached      remove it once we bring the id_tap implementation into the core.   File ""/opt/conda/lib/python3.10/sitepackages/jax/interpreters/pxla.py"", line 3204, in compile   File ""/opt/conda/lib/python3.10/sitepackages/jax/interpreters/pxla.py"", line 3167, in _compile_unloaded   File ""/opt/conda/lib/python3.10/sitepackages/jax/interpreters/pxla.py"", line 3447, in from_hlo   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1044, in compile_or_get_cached   File ""/opt/conda/lib/python3.10/sitepackages/jax/experimental/compilation_cache/compilation_cache.py"", line 15, in      from jax._src.compilation_cache import (   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/compilation_cache.py"", line 28, in      from jax._src import cache_key   File ""/opt/conda/lib/python3.10/sitepackages/jax/_src/cache_key.py"", line 29, in      from jax._src.lib.mlir import passmanager as pm ImportError: cannot import name 'passmanager' from 'jax._src.lib.mlir' (/opt/conda/lib/python3.10/sitepackages/jax/_src/lib/mlir/__init__.py) During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/interactiveshell.py"", line 2057, in showtraceback     stb = self.InteractiveTB.structured_traceback(   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 1288, in structured_traceback     return FormattedTB.structured_traceback(   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 1177, in structured_traceback     return VerboseTB.structured_traceback(   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 1030, in structured_traceback     formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 960, in format_exception_as_a_whole     frames.append(self.format_record(record))   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 870, in format_record     frame_info.lines, Colors, self.has_colors, lvals   File ""/home/jih201/.local/lib/python3.10/sitepackages/IPython/core/ultratb.py"", line 704, in lines     return self._sd.lines   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/utils.py"", line 144, in cached_property_wrapper     value = obj.__dict__[self.func.__name__] = self.func(obj)   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/core.py"", line 734, in lines     pieces = self.included_pieces   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/utils.py"", line 144, in cached_property_wrapper     value = obj.__dict__[self.func.__name__] = self.func(obj)   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/core.py"", line 681, in included_pieces     pos = scope_pieces.index(self.executing_piece)   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/utils.py"", line 144, in cached_property_wrapper     value = obj.__dict__[self.func.__name__] = self.func(obj)   File ""/home/jih201/.local/lib/python3.10/sitepackages/stack_data/core.py"", line 660, in executing_piece     return only(   File ""/home/jih201/.local/lib/python3.10/sitepackages/executing/executing.py"", line 190, in only     raise NotOneValueFound('Expected one value, found 0') executing.executing.NotOneValueFound: Expected one value, found 0 ```  What jax/jaxlib version are you using? jax v0.4.2  Which accelerator(s) are you using? GPU   Additional system info Python 3.10.9, OS(Linux)  NVIDIA GPU info ``` ++  ++ ```",2023-10-05T17:41:38Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17960,"Thanks for the report! This looks like it is probably due to a mismatch between your jax version and your jaxlib version. On that, you report that you're using jax v0.4.2, but the traceback shows a file that did not exist until jax v0.4.15: https://github.com/google/jax/blob/jaxlibv0.4.15/jax/_src/cache_key.py. So you must actually be using a more recent version of JAX. So the place to start would be to make sure that (1) you're using the JAX version you intend to be using, and (2) the jaxlib version you have installed is compatible.","Thank you for the help, after correcting the Jaxlib version, the error is resolved. "
506,"以下是一个github上的jax下的一个issue, 标题是(Draft: Scaled Dot Product Attention API in JAX)， 内容是 (Attention mechanisms, particularly the Scaled Dot Product Attention, play a vital role in modern neural architectures, especially in transformers. This PR introduces a JAXbased Scaled Dot Product Attention API, providing users with an optimized, flexible, and easytouse interface.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Draft: Scaled Dot Product Attention API in JAX,"Attention mechanisms, particularly the Scaled Dot Product Attention, play a vital role in modern neural architectures, especially in transformers. This PR introduces a JAXbased Scaled Dot Product Attention API, providing users with an optimized, flexible, and easytouse interface.",2023-10-05T17:20:25Z,,closed,0,3,https://github.com/jax-ml/jax/issues/17957,"Thanks for this! Is there a way to test that XLA:GPU is lowering indeed this to the intended cuDNN calls? (I have some ideas for how to do that, and I can give them a shot if you'd like, but I wanted to check if you already had thoughts on this.)","> Thanks for this! >  > Is there a way to test that XLA:GPU is lowering indeed this to the intended cuDNN calls? (I have some ideas for how to do that, and I can give them a shot if you'd like, but I wanted to check if you already had thoughts on this.) I have some internal unit tests that I'm using to validate the lowering. But it'd be great if you can give it a shot :).",This project is replaced by https://github.com/google/jax/pull/18814 So closing this PR.
974,"以下是一个github上的jax下的一个issue, 标题是(Getting Array instead of DeviceArray  )， 内容是 ( Description Hi Team, I am trying to understand the `Asynchronous dispatch` link  and when executing the below program (in colab) mentioned in this page I am getting `Array` instead of `DeviceArray`.  My query is whats the difference between `Array` and `DeviceArray`? Secondly, do we need document update here? ``` import numpy as np import jax.numpy as jnp from jax import random x = random.uniform(random.PRNGKey(0), (1000, 1000))  Printing the result (i.e. evaluating `repr(result)` or `str(result)`)  will block until the value is ready. jnp.dot(x, x) + 3.  ```  What jax/jaxlib version are you using? jax    0.4.16 ,    jaxlib   0.4.16+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Mac  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Getting Array instead of DeviceArray  ," Description Hi Team, I am trying to understand the `Asynchronous dispatch` link  and when executing the below program (in colab) mentioned in this page I am getting `Array` instead of `DeviceArray`.  My query is whats the difference between `Array` and `DeviceArray`? Secondly, do we need document update here? ``` import numpy as np import jax.numpy as jnp from jax import random x = random.uniform(random.PRNGKey(0), (1000, 1000))  Printing the result (i.e. evaluating `repr(result)` or `str(result)`)  will block until the value is ready. jnp.dot(x, x) + 3.  ```  What jax/jaxlib version are you using? jax    0.4.16 ,    jaxlib   0.4.16+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Mac  NVIDIA GPU info _No response_",2023-10-04T21:23:31Z,documentation,closed,0,2,https://github.com/jax-ml/jax/issues/17935,We need to update that doc but DeviceArray was deleted in favor of Array. https://jax.readthedocs.io/en/latest/jax_array_migration.html goes into more details.,I think we can declare this fixed now CC(Asynchronous dispatch doc update regarding jax.Array migration) was merged.
2914,"以下是一个github上的jax下的一个issue, 标题是(Shard_map does not have a checkify rule)， 内容是 ( Description Trying to use checkify leads to following error: ```File ""/home/mohitkhatwani/maxtext/MaxText/train.py"", line 317, in main     train_loop(pyconfig.config)   File ""/home/mohitkhatwani/maxtext/MaxText/train.py"", line 278, in train_loop     err, (state, metrics, nextrng) = p_train_step(   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 177, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 1133, in checked_fun     error, out_flat = checkify_jaxpr(jaxpr, errors, init_error, *consts)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 393, in checkify_jaxpr     return checkify_jaxpr_flat(jaxpr.jaxpr, jaxpr.consts,   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 424, in checkify_jaxpr_flat     error, outvals = checkify_rule(error, enabled_errors,   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 904, in pjit_error_check     checked_jaxpr, out_tree, _ = jaxpr_to_checkify_jaxpr(jaxpr, enabled_errors,   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 750, in jaxpr_to_checkify_jaxpr     new_jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(fun, flat_err_and_in_vals)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2203, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2225, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/linear_util.py"", line 190, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 424, in checkify_jaxpr_flat     error, outvals = checkify_rule(error, enabled_errors,   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 343, in default_checkify_rule     return error, primitive.bind(*invals, **params) TypeError: ShardMapPrimitive.bind() got an unexpected keyword argument 'out_names' ```  What jax/jaxlib version are you using? jax,jaxlib: HEAD  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Shard_map does not have a checkify rule," Description Trying to use checkify leads to following error: ```File ""/home/mohitkhatwani/maxtext/MaxText/train.py"", line 317, in main     train_loop(pyconfig.config)   File ""/home/mohitkhatwani/maxtext/MaxText/train.py"", line 278, in train_loop     err, (state, metrics, nextrng) = p_train_step(   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 177, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 1133, in checked_fun     error, out_flat = checkify_jaxpr(jaxpr, errors, init_error, *consts)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 393, in checkify_jaxpr     return checkify_jaxpr_flat(jaxpr.jaxpr, jaxpr.consts,   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 424, in checkify_jaxpr_flat     error, outvals = checkify_rule(error, enabled_errors,   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 904, in pjit_error_check     checked_jaxpr, out_tree, _ = jaxpr_to_checkify_jaxpr(jaxpr, enabled_errors,   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 750, in jaxpr_to_checkify_jaxpr     new_jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(fun, flat_err_and_in_vals)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2203, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2225, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/linear_util.py"", line 190, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 424, in checkify_jaxpr_flat     error, outvals = checkify_rule(error, enabled_errors,   File ""/home/mohitkhatwani/.local/lib/python3.10/sitepackages/jax/_src/checkify.py"", line 343, in default_checkify_rule     return error, primitive.bind(*invals, **params) TypeError: ShardMapPrimitive.bind() got an unexpected keyword argument 'out_names' ```  What jax/jaxlib version are you using? jax,jaxlib: HEAD  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-10-03T19:14:18Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17903,"Hey! I want to reproduce this error. Can you share the details of your environment, testsetup (commands etc)?"
574,"以下是一个github上的jax下的一个issue, 标题是([export] Set the default export serialization version to 8.)， 内容是 ([export] Set the default export serialization version to 8. This version has been supported by XlaCallModule since July 21, 2023 and we are now past the forwardcompatibility window. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions Reverts ae81ac9cc21696a22b973b1eae6ce222c7318ba7)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[export] Set the default export serialization version to 8.,"[export] Set the default export serialization version to 8. This version has been supported by XlaCallModule since July 21, 2023 and we are now past the forwardcompatibility window. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions Reverts ae81ac9cc21696a22b973b1eae6ce222c7318ba7",2023-10-03T04:56:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17897
4229,"以下是一个github上的jax下的一个issue, 标题是(Getting NaN for gradients when computing the loss with jit compile)， 内容是 ( Description I am trying to get the sensitivity of the loss with respect to the model params by running the following minimum reproducible example. ```python newline code newline import jax import jax.numpy as jnp from jax.config import config import brax from brax.io import mjcf from brax.generalized import pipeline import equinox as eqx import optax from functools import partial from jax import config class SimpleModel(eqx.Module):     layers: list           list of the layers in the NN     extra_bias: jax.Array  extra bias on the output     def __init__(self, key):         key1, key2, key3 = jax.random.split(key, 3)          layers contain trainable parameters         self.layers = [             eqx.nn.Linear(2, 4, key=key1),             eqx.nn.Linear(4, 4, key=key2),             eqx.nn.Linear(4, 1, key=key3)         ]          extra_bias is also a trainable parameter         self.extra_bias = jax.numpy.ones(1)     .jit     def __call__(self, x):         for layer in self.layers[:1]:             x = jax.nn.elu(layer(x))         return self.layers1 + self.extra_bias (jax.jit, static_argnames=('timesteps')) def loss(model: SimpleModel, system: brax.System, state: brax.State, timesteps):     def step(i: int, carry: tuple):         system_s, state_s, model_s, x_pos_s = carry         x_pos_s = x_pos_s.at[i].add(state_s.x.pos[0][0])         x = jnp.array([state_s.x.pos[0][0], state_s.qd[0]])         force = model_s(x.transpose())         state_s = pipeline.step(system_s, state_s, force)         return (system_s, state_s, model_s, x_pos_s)     x_pos = jnp.zeros((timesteps, 1), dtype=jnp.float32)     carry = (system, state, model, x_pos)     system, state, model, x_pos = jax.lax.fori_loop(0, timesteps, step, carry)     targetposition = jnp.zeros((timesteps, 1))     loss_value = jnp.sum(1.0*jnp.abs(x_pos  targetposition)) / timesteps     return loss_value def main():      config.update(""jax_disable_jit"", True)       config.update(""jax_debug_nans"", True)     key = jax.random.PRNGKey(seed=0)     key, subkey = jax.random.split(key, 2)     model = SimpleModel(subkey)     lr = 0.001     opt = optax.adam(lr)     opt_state = opt.init(model)     simTime = 0.01     timeSteps = int(simTime / 0.001)     system: brax.System = mjcf.loads(                                     """"""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        """""".format(x=1.0, y=0.0, z=0.5))     qd = jnp.array([0.0])     state = jax.jit(pipeline.init)(system, system.init_q, qd)     for i in range(2):         l, grad = eqx.filter_value_and_grad(loss, has_aux=False)(model, system, state, timeSteps)         print(""Batch: "", i+1, "" Loss: "", l)         print(""Gradients of layer 0 weights:\n"", grad.layers[0].weight)         updates, opt_state = opt.update(grad, opt_state)         model = optax.apply_updates(model, updates) if __name__ == ""__main__"":     main() ```  When I run this with Jax jit compile I get NaN for the gradients, but when I implement config.update(""jax_disable_jit"", True) to disable the jit compile I am getting values for gradients, but it is very slow.  Can anyone help me shed some light as to why this is happening, and how I can compute gradients with the jit compiler and BRAX physics engine? I am currently raising this issue with the authors of BRAX as well.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Python Verion =3.11.5, Ubuntu=22.04  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Getting NaN for gradients when computing the loss with jit compile," Description I am trying to get the sensitivity of the loss with respect to the model params by running the following minimum reproducible example. ```python newline code newline import jax import jax.numpy as jnp from jax.config import config import brax from brax.io import mjcf from brax.generalized import pipeline import equinox as eqx import optax from functools import partial from jax import config class SimpleModel(eqx.Module):     layers: list           list of the layers in the NN     extra_bias: jax.Array  extra bias on the output     def __init__(self, key):         key1, key2, key3 = jax.random.split(key, 3)          layers contain trainable parameters         self.layers = [             eqx.nn.Linear(2, 4, key=key1),             eqx.nn.Linear(4, 4, key=key2),             eqx.nn.Linear(4, 1, key=key3)         ]          extra_bias is also a trainable parameter         self.extra_bias = jax.numpy.ones(1)     .jit     def __call__(self, x):         for layer in self.layers[:1]:             x = jax.nn.elu(layer(x))         return self.layers1 + self.extra_bias (jax.jit, static_argnames=('timesteps')) def loss(model: SimpleModel, system: brax.System, state: brax.State, timesteps):     def step(i: int, carry: tuple):         system_s, state_s, model_s, x_pos_s = carry         x_pos_s = x_pos_s.at[i].add(state_s.x.pos[0][0])         x = jnp.array([state_s.x.pos[0][0], state_s.qd[0]])         force = model_s(x.transpose())         state_s = pipeline.step(system_s, state_s, force)         return (system_s, state_s, model_s, x_pos_s)     x_pos = jnp.zeros((timesteps, 1), dtype=jnp.float32)     carry = (system, state, model, x_pos)     system, state, model, x_pos = jax.lax.fori_loop(0, timesteps, step, carry)     targetposition = jnp.zeros((timesteps, 1))     loss_value = jnp.sum(1.0*jnp.abs(x_pos  targetposition)) / timesteps     return loss_value def main():      config.update(""jax_disable_jit"", True)       config.update(""jax_debug_nans"", True)     key = jax.random.PRNGKey(seed=0)     key, subkey = jax.random.split(key, 2)     model = SimpleModel(subkey)     lr = 0.001     opt = optax.adam(lr)     opt_state = opt.init(model)     simTime = 0.01     timeSteps = int(simTime / 0.001)     system: brax.System = mjcf.loads(                                     """"""                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        """""".format(x=1.0, y=0.0, z=0.5))     qd = jnp.array([0.0])     state = jax.jit(pipeline.init)(system, system.init_q, qd)     for i in range(2):         l, grad = eqx.filter_value_and_grad(loss, has_aux=False)(model, system, state, timeSteps)         print(""Batch: "", i+1, "" Loss: "", l)         print(""Gradients of layer 0 weights:\n"", grad.layers[0].weight)         updates, opt_state = opt.update(grad, opt_state)         model = optax.apply_updates(model, updates) if __name__ == ""__main__"":     main() ```  When I run this with Jax jit compile I get NaN for the gradients, but when I implement config.update(""jax_disable_jit"", True) to disable the jit compile I am getting values for gradients, but it is very slow.  Can anyone help me shed some light as to why this is happening, and how I can compute gradients with the jit compiler and BRAX physics engine? I am currently raising this issue with the authors of BRAX as well.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Python Verion =3.11.5, Ubuntu=22.04  NVIDIA GPU info _No response_",2023-10-02T20:27:21Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/17891,I was able to solve the issue. It seems that the problem was in how I was initializing the initial state with Brax.,"Hi mander , how did you solve this issue? I also got this error while using Brax and Jax. It bugs me for a long time. I would appreciate it very much if you could share more information.","So I rewrote my system and how I initialized my initial state and it seemed to work for me. See the example below: ```python system: brax.System = mjcf.loads(                             """"""                                                                                                                                                                                                                                                                                                                                                                                                                                        """""") qd = jnp.zeros(system.qd_size()) state = jax.jit(pipeline.init)(system, system.init_q, qd) ``` ",Thanks for sharing! mander  I ended up using the spring backend to avoid the issue. 
4407,"以下是一个github上的jax下的一个issue, 标题是(Test failures on aarch64-darwin: `RuntimeWarning: divide by zero encountered in equal`)， 内容是 ( Description Running the test suite on aarch64darwin gives me 3 errors. ``` =================================== FAILURES =================================== ___________________________ LaxRandomTest.test_copy0 ___________________________ self =  make_key =      .parameters([{'make_key': ctor} for ctor in KEY_CTORS])     def test_copy(self, make_key):       key = make_key(8459302) >     self.assertArraysEqual(key, key.copy()) tests/random_test.py:1713:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:963: in assertArraysEqual     np.testing.assert_array_equal(x, y, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (, Array((), dtype=key) overlaying: [      0 8459302], Array((), dtype=key) overlaying: [      0 8459302]) kwds = {'err_msg': '', 'header': 'Arrays are not equal', 'strict': False, 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           RuntimeWarning: divide by zero encountered in equal /nix/store/g5cm6iik6p4k39cj9k7a6sg2p09hl7wfpython33.10.12/lib/python3.10/contextlib.py:79: RuntimeWarning _____________________ LaxRandomWithRBGPRNGTest.test_copy0 ______________________ self =  make_key =      .parameters([{'make_key': ctor} for ctor in KEY_CTORS])     def test_copy(self, make_key):       key = make_key(8459302) >     self.assertArraysEqual(key, key.copy()) tests/random_test.py:1713:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:963: in assertArraysEqual     np.testing.assert_array_equal(x, y, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (, Array((), dtype=key) overlaying: [      0 8459302       0 8459302], Array((), dtype=key) overlaying: [      0 8459302       0 8459302]) kwds = {'err_msg': '', 'header': 'Arrays are not equal', 'strict': False, 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           RuntimeWarning: divide by zero encountered in equal /nix/store/g5cm6iik6p4k39cj9k7a6sg2p09hl7wfpython33.10.12/lib/python3.10/contextlib.py:79: RuntimeWarning __________________ LaxRandomWithUnsafeRBGPRNGTest.test_copy0 ___________________ self =  make_key =      .parameters([{'make_key': ctor} for ctor in KEY_CTORS])     def test_copy(self, make_key):       key = make_key(8459302) >     self.assertArraysEqual(key, key.copy()) tests/random_test.py:1713:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:963: in assertArraysEqual     np.testing.assert_array_equal(x, y, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (, Array((), dtype=key) overlaying: [      0 8459302       0 8459302], Array((), dtype=key) overlaying: [      0 8459302       0 8459302]) kwds = {'err_msg': '', 'header': 'Arrays are not equal', 'strict': False, 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           RuntimeWarning: divide by zero encountered in equal /nix/store/g5cm6iik6p4k39cj9k7a6sg2p09hl7wfpython33.10.12/lib/python3.10/contextlib.py:79: RuntimeWarning =========================== short test summary info ============================ FAILED tests/random_test.py::LaxRandomTest::test_copy0  RuntimeWarning: divide by zero encountered in equal FAILED tests/random_test.py::LaxRandomWithRBGPRNGTest::test_copy0  RuntimeWarning: divide by zero encountered in equal FAILED tests/random_test.py::LaxRandomWithUnsafeRBGPRNGTest::test_copy0  RuntimeWarning: divide by zero encountered in equal == 3 failed, 16331 passed, 2549 skipped, 30 deselected in 8827.06s (2:27:07) === ```  What jax/jaxlib version are you using? jax 0.4.16 (refs/tags/jaxv0.4.16), the jaxlib wheel from PyPI v0.4.16.  Which accelerator(s) are you using? CPU  Additional system info macOS aarch64darwin, M1 Pro  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Test failures on aarch64-darwin: `RuntimeWarning: divide by zero encountered in equal`," Description Running the test suite on aarch64darwin gives me 3 errors. ``` =================================== FAILURES =================================== ___________________________ LaxRandomTest.test_copy0 ___________________________ self =  make_key =      .parameters([{'make_key': ctor} for ctor in KEY_CTORS])     def test_copy(self, make_key):       key = make_key(8459302) >     self.assertArraysEqual(key, key.copy()) tests/random_test.py:1713:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:963: in assertArraysEqual     np.testing.assert_array_equal(x, y, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (, Array((), dtype=key) overlaying: [      0 8459302], Array((), dtype=key) overlaying: [      0 8459302]) kwds = {'err_msg': '', 'header': 'Arrays are not equal', 'strict': False, 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           RuntimeWarning: divide by zero encountered in equal /nix/store/g5cm6iik6p4k39cj9k7a6sg2p09hl7wfpython33.10.12/lib/python3.10/contextlib.py:79: RuntimeWarning _____________________ LaxRandomWithRBGPRNGTest.test_copy0 ______________________ self =  make_key =      .parameters([{'make_key': ctor} for ctor in KEY_CTORS])     def test_copy(self, make_key):       key = make_key(8459302) >     self.assertArraysEqual(key, key.copy()) tests/random_test.py:1713:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:963: in assertArraysEqual     np.testing.assert_array_equal(x, y, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (, Array((), dtype=key) overlaying: [      0 8459302       0 8459302], Array((), dtype=key) overlaying: [      0 8459302       0 8459302]) kwds = {'err_msg': '', 'header': 'Arrays are not equal', 'strict': False, 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           RuntimeWarning: divide by zero encountered in equal /nix/store/g5cm6iik6p4k39cj9k7a6sg2p09hl7wfpython33.10.12/lib/python3.10/contextlib.py:79: RuntimeWarning __________________ LaxRandomWithUnsafeRBGPRNGTest.test_copy0 ___________________ self =  make_key =      .parameters([{'make_key': ctor} for ctor in KEY_CTORS])     def test_copy(self, make_key):       key = make_key(8459302) >     self.assertArraysEqual(key, key.copy()) tests/random_test.py:1713:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:963: in assertArraysEqual     np.testing.assert_array_equal(x, y, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (, Array((), dtype=key) overlaying: [      0 8459302       0 8459302], Array((), dtype=key) overlaying: [      0 8459302       0 8459302]) kwds = {'err_msg': '', 'header': 'Arrays are not equal', 'strict': False, 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           RuntimeWarning: divide by zero encountered in equal /nix/store/g5cm6iik6p4k39cj9k7a6sg2p09hl7wfpython33.10.12/lib/python3.10/contextlib.py:79: RuntimeWarning =========================== short test summary info ============================ FAILED tests/random_test.py::LaxRandomTest::test_copy0  RuntimeWarning: divide by zero encountered in equal FAILED tests/random_test.py::LaxRandomWithRBGPRNGTest::test_copy0  RuntimeWarning: divide by zero encountered in equal FAILED tests/random_test.py::LaxRandomWithUnsafeRBGPRNGTest::test_copy0  RuntimeWarning: divide by zero encountered in equal == 3 failed, 16331 passed, 2549 skipped, 30 deselected in 8827.06s (2:27:07) === ```  What jax/jaxlib version are you using? jax 0.4.16 (refs/tags/jaxv0.4.16), the jaxlib wheel from PyPI v0.4.16.  Which accelerator(s) are you using? CPU  Additional system info macOS aarch64darwin, M1 Pro  NVIDIA GPU info _No response_",2023-09-29T22:23:36Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/17867,"Just ran again and I'm seeing another failure. Perhaps they're nondeterministic? ``` ___________________________ KeyArrayTest.test_async ____________________________ self =      def test_async(self):       key = self.make_keys(10) >     self.assertArraysEqual(key, key.block_until_ready()) tests/random_test.py:2169:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:963: in assertArraysEqual     np.testing.assert_array_equal(x, y, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (, Array((10,), dtype=key) overlaying: [[ 0 28]  [ 0 29]  [ 0 30]  [ 0 31]  [ 0 32]  [ 0 33...dtype=key) overlaying: [[ 0 28]  [ 0 29]  [ 0 30]  [ 0 31]  [ 0 32]  [ 0 33]  [ 0 34]  [ 0 35]  [ 0 36]  [ 0 37]]) kwds = {'err_msg': '', 'header': 'Arrays are not equal', 'strict': False, 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           RuntimeWarning: divide by zero encountered in equal /nix/store/g5cm6iik6p4k39cj9k7a6sg2p09hl7wfpython33.10.12/lib/python3.10/contextlib.py:79: RuntimeWarning ```","""ANOTHER ONE""  DJ Khaled ``` _________________________ KeyArrayTest.test_device_put _________________________ self =      def test_device_put(self):       device = jax.devices()[0]       keys = self.make_keys(4)       keys_on_device = jax.device_put(keys, device) >     self.assertArraysEqual(keys, keys_on_device) tests/random_test.py:2051:  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jax/_src/test_util.py:963: in assertArraysEqual     np.testing.assert_array_equal(x, y, err_msg=err_msg) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  args = (, Array((4,), dtype=key) overlaying: [[ 0 28]  [ 0 29]  [ 0 30]  [ 0 31]], Array((4,), dtype=key) overlaying: [[ 0 28]  [ 0 29]  [ 0 30]  [ 0 31]]) kwds = {'err_msg': '', 'header': 'Arrays are not equal', 'strict': False, 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           RuntimeWarning: divide by zero encountered in equal /nix/store/g5cm6iik6p4k39cj9k7a6sg2p09hl7wfpython33.10.12/lib/python3.10/contextlib.py:79: RuntimeWarning =========================== short test summary info ============================ FAILED tests/random_test.py::KeyArrayTest::test_device_put  RuntimeWarning: divide by zero encountered in equal ==== 1 failed, 1165 passed, 100 skipped, 5 deselected in 267.50s (0:04:27) ===== ``` EDIT: ok turns out there's a bunch of these: ```        > FAILED tests/random_test.py::KeyArrayTest::test_make_array_from_callback  RuntimeWarning: divide by zero encountered in equal        > FAILED tests/random_test.py::KeyArrayTest::test_make_array_from_single_device_arrays  RuntimeWarning: divide by zero encountered in equal        > FAILED tests/random_test.py::JnpWithKeyArrayTest::test_array  RuntimeWarning: divide by zero encountered in equal ```","Thanks for the report. Out of curiosity, what Python version are you using? I suspect these may be versiondependent.","It looks like this is similar to https://github.com/numpy/numpy/issues/18992, but for dividebyzero warnings rather than overflow warnings.","Looking closer, it seems like it has something to do with the interaction between `np.asanyarray` and custom PRNG keys: ```python import jax import numpy as np key = np.asanyarray(jax.random.key(0)) key == key   RuntimeWarning: divide by zero encountered in equal ``` But I'm only seeing this on Python 3.9, which is probably why this isn't getting caught by our CI.","Interesting, I'm on Python 3.10.12... not sure why it's getting picked up only on some versions","I'm still not sure the root cause of this, but the warnings boil down to this: ```python In [1]: import numpy as np In [2]: import jax.numpy as jnp In [3]: x = np.array([0], dtype=object) In [4]: x[0] = jnp.int32(1) In [5]: x == x :1: RuntimeWarning: divide by zero encountered in equal   x == x Out[5]: array([ True]) ``` That is, if you ever call `==` on an object array that contains JAX arrays, numpy raises this warning on some platforms. I have some fixes in CC(random_test: fix deprecation warnings for key tests) & CC(api_test: fix platformdependent deprecation warning)"
565,"以下是一个github上的jax下的一个issue, 标题是(Random Bernoulli allocates massive amounts of unnecessary memory  )， 内容是 (I am running jax.random.bernoulli to run 64*64 trials for 4 million different mean values. However, this consumes ~800GB of VRAM. It probably shouldn't. ``` external/tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 207.07GiB (rounded to 222343890432)requested by op ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Random Bernoulli allocates massive amounts of unnecessary memory  ,"I am running jax.random.bernoulli to run 64*64 trials for 4 million different mean values. However, this consumes ~800GB of VRAM. It probably shouldn't. ``` external/tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 207.07GiB (rounded to 222343890432)requested by op ```",2023-09-29T20:50:23Z,enhancement,closed,0,6,https://github.com/jax-ml/jax/issues/17865,"Thanks for the report! This certainly seems like a lot of memory, but you should keep in mind that just storing `64*64*4000000` float32 values will take ~60GB (64 * 64 * 4000000 * 4 bytes), and `random.bernoulli` needs several times this memory to perform the operations it uses to go from random bits to bernoullidistributed floats. You can get some sense of what's involved by looking at the jaxpr for this operation: ```python In [1]: import jax In [2]: jax.make_jaxpr(lambda key: jax.random.bernoulli(key, 0.5, (64, 64, 4000000)))(jax.random.key(0)) Out[2]:  { lambda ; a:key[]. let     b:bool[64,64,4000000] = pjit[       jaxpr={ lambda ; c:key[] d:f32[]. let           e:f32[64,64,4000000] = pjit[             jaxpr={ lambda ; f:key[] g:f32[] h:f32[]. let                 i:f32[] = convert_element_type[                   new_dtype=float32                   weak_type=False                 ] g                 j:f32[] = convert_element_type[                   new_dtype=float32                   weak_type=False                 ] h                 k:f32[1,1,1] = broadcast_in_dim[                   broadcast_dimensions=()                   shape=(1, 1, 1)                 ] i                 l:f32[1,1,1] = broadcast_in_dim[                   broadcast_dimensions=()                   shape=(1, 1, 1)                 ] j                 m:u32[64,64,4000000] = random_bits[                   bit_width=32                   shape=(64, 64, 4000000)                 ] f                 n:u32[64,64,4000000] = shift_right_logical m 9                 o:u32[64,64,4000000] = or n 1065353216                 p:f32[64,64,4000000] = bitcast_convert_type[new_dtype=float32] o                 q:f32[64,64,4000000] = sub p 1.0                 r:f32[1,1,1] = sub l k                 s:f32[64,64,4000000] = mul q r                 t:f32[64,64,4000000] = add s k                 u:f32[64,64,4000000] = max k t               in (u,) }             name=_uniform           ] c 0.0 1.0           v:bool[64,64,4000000] = lt e d         in (v,) }       name=_bernoulli     ] a 0.5   in (b,) } ``` Since each `64,64,4000000` value occupies ~60GB, it's not unreasonable that the whole operation, with several individual computations on intermediate arrays of this size, would require several hundred gigabytes. If you need to be more memory efficient, you might try generating your bernoulli samples in batches, within `lax.scan` for example. Do you think an approach like that might work for your usecase?","Is it not just a simple boolean comparison between p and some uniforms? I understand that there are technical reasons for what is happening. However, in theory, writing this in CUDA would result in 2GB of VRAM. That means that Jax Bernoulli has a 100x memory overhead for this operation.","> Is it not just a simple boolean comparison between p and some uniforms? Those uniforms take up 60GB on their own, so I'm not sure how you could fit that into 2GB in theory without doing some sort of sequentiallybatched operation. Even the output (4000000 x 64 x 64 x 1 byte booleans) takes up ~15 GB. Maybe there's some assumption you're making that I'm not understanding?","Why do those uniforms need to be stored in VRAM? Can’t they be generated, stored in one of the warp registers, then discarded immediately? They don’t need to be accumulated, right? I’m not familiar with TPU architecture. Also, isn’t a boolean 1 bit?","It's true that there is only one bit of information per boolean value, but JAX/XLA (like many other numerical computing systems) stores booleans as bytes for reasons related to computational efficiency.","I'm going to close this, because it seems to be working as expected."
442,"以下是一个github上的jax下的一个issue, 标题是(Clean up build_wheel.py and build_gpu_plugin_wheel.py.)， 内容是 (* Use pathlib.Path objectoriented paths. * Change copy_files() helper to copy many files in one call. * Make copy_files() also make the output directory, if needed. * Format file with pyink pyinkindentation=2)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Clean up build_wheel.py and build_gpu_plugin_wheel.py.,"* Use pathlib.Path objectoriented paths. * Change copy_files() helper to copy many files in one call. * Make copy_files() also make the output directory, if needed. * Format file with pyink pyinkindentation=2",2023-09-29T20:04:23Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17864
2802,"以下是一个github上的jax下的一个issue, 标题是(Support for ragged arrays, like torch.nested)， 内容是 (One of the quite compelling new additions to the torch ecosystem is NestedTensors, see https://pytorch.org/docs/stable/nested.html. Basically, this is a new primitive in the Torch ecosystem which allows for tensors which are ""ragged"" along one dimension (aka a tensor with shape `B x ? x 3`), as well as a bunch of ops which have been optimized to support this style of batching. Ragged batches are very common in a number of domains; variablelength sequences in NLP tasks, 3D vision tasks such as Point Cloud and Mesh analysis (which often have variable numbers of nodes+faces), graph processing, etc. When batching for parallel/vectorized computation, most researchers tend to use padding operations or do some clever (manual) operations on tensors of shape (`(sum N_i) x 3` where `N_i` is the number of elements in the ith element of a batch.  Examples of such clever vectorization: * https://vladfeinberg.com/2021/01/07/vectorizingraggedarrays.html * https://github.com/googledeepmind/jraph There are a few wellknown tricks for doing these kinds of operations (using existing operations in tensor libraries, as well as some custom accelerator code for specific ops)  aggregating them as primitives inside a Tensor library directly (and attempting to cover as many commonlyused ops as possible) would be extremely useful for the community. For instance, in torch.nested, you can do the following: ``` import torch import torch.nested a = torch.rand((10, 3)) b = torch.rand((20, 3)) R = torch.rand((3, 3)) nt = torch.nested.nested_tensor([a, b]) R_nt = torch.nested.nested_tensor([R, R]) res = nt @ R_nt ``` The API is currently a bit rough around the edges, but the eventual goal is to be able to have most/all ops in torch transparently support nested tensors, so that downstream libraries can focus on implementing algorithms instead of handling manual memory/batching layout optimizations (and often doing so inefficiently/incorrectly/with many layers of abstraction). What would it take for JAX to support something like this as a core primitive? I'm not very familiar with how vectorization/parallelization works under the hood (in any accelerator library), but my expectation is that there are probably many challenges in every part of the core (jit, vmap, pmap). One (potential) upshot of having some sort of support for ragged tensors would be the removal of this rough edge (https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmldynamicshapes), if enough properties can be inferred (like the tensor varying in shape only along a single dimension).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",few shot,"Support for ragged arrays, like torch.nested","One of the quite compelling new additions to the torch ecosystem is NestedTensors, see https://pytorch.org/docs/stable/nested.html. Basically, this is a new primitive in the Torch ecosystem which allows for tensors which are ""ragged"" along one dimension (aka a tensor with shape `B x ? x 3`), as well as a bunch of ops which have been optimized to support this style of batching. Ragged batches are very common in a number of domains; variablelength sequences in NLP tasks, 3D vision tasks such as Point Cloud and Mesh analysis (which often have variable numbers of nodes+faces), graph processing, etc. When batching for parallel/vectorized computation, most researchers tend to use padding operations or do some clever (manual) operations on tensors of shape (`(sum N_i) x 3` where `N_i` is the number of elements in the ith element of a batch.  Examples of such clever vectorization: * https://vladfeinberg.com/2021/01/07/vectorizingraggedarrays.html * https://github.com/googledeepmind/jraph There are a few wellknown tricks for doing these kinds of operations (using existing operations in tensor libraries, as well as some custom accelerator code for specific ops)  aggregating them as primitives inside a Tensor library directly (and attempting to cover as many commonlyused ops as possible) would be extremely useful for the community. For instance, in torch.nested, you can do the following: ``` import torch import torch.nested a = torch.rand((10, 3)) b = torch.rand((20, 3)) R = torch.rand((3, 3)) nt = torch.nested.nested_tensor([a, b]) R_nt = torch.nested.nested_tensor([R, R]) res = nt @ R_nt ``` The API is currently a bit rough around the edges, but the eventual goal is to be able to have most/all ops in torch transparently support nested tensors, so that downstream libraries can focus on implementing algorithms instead of handling manual memory/batching layout optimizations (and often doing so inefficiently/incorrectly/with many layers of abstraction). What would it take for JAX to support something like this as a core primitive? I'm not very familiar with how vectorization/parallelization works under the hood (in any accelerator library), but my expectation is that there are probably many challenges in every part of the core (jit, vmap, pmap). One (potential) upshot of having some sort of support for ragged tensors would be the removal of this rough edge (https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmldynamicshapes), if enough properties can be inferred (like the tensor varying in shape only along a single dimension).",2023-09-29T19:45:02Z,enhancement,open,3,2,https://github.com/jax-ml/jax/issues/17863,Any progress?,I didn't really test this thing so there's probably lurking bugs. But it's a high level sketch of a way to do something slightly more general than ragged arrays: https://gist.github.com/vyeevani/e10c4a92bb74edf51b03d8a05e652049. I don't necessarily think this is something that would have to be added directly into jax. Feels like it can be handled by libraries 
372,"以下是一个github上的jax下的一个issue, 标题是(Add tests to cover `PyTreeDef.flatten_up_to` error scenarios.)， 内容是 (Add tests to cover `PyTreeDef.flatten_up_to` error scenarios. Also improve coverage of `PyTreeDef.flatten_up_to` success scenarios.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add tests to cover `PyTreeDef.flatten_up_to` error scenarios.,Add tests to cover `PyTreeDef.flatten_up_to` error scenarios. Also improve coverage of `PyTreeDef.flatten_up_to` success scenarios.,2023-09-29T19:33:02Z,,closed,0,1,https://github.com/jax-ml/jax/issues/17862,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
697,"以下是一个github上的jax下的一个issue, 标题是(Divergence between `jnp.load` and `np.load`)， 内容是 ( Description ```python import jax.numpy as jnp import numpy as np jnp.save(""hi.npy"", 1e8) x = np.load(""hi.npy"").item() y = jnp.load(""hi.npy"").item() print(x)   1e08 print(y)   9.99999993922529e09 ``` I assume this is due to some internal dtype conversion that isn't supposed to be happening.  What jax/jaxlib version are you using? JAX: 173a27017; jaxlib: 0.4.16  Which accelerator(s) are you using? _No response_  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Divergence between `jnp.load` and `np.load`," Description ```python import jax.numpy as jnp import numpy as np jnp.save(""hi.npy"", 1e8) x = np.load(""hi.npy"").item() y = jnp.load(""hi.npy"").item() print(x)   1e08 print(y)   9.99999993922529e09 ``` I assume this is due to some internal dtype conversion that isn't supposed to be happening.  What jax/jaxlib version are you using? JAX: 173a27017; jaxlib: 0.4.16  Which accelerator(s) are you using? _No response_  Additional system info Linux  NVIDIA GPU info _No response_",2023-09-29T17:58:08Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/17858,"Unless you `ENABLE_X64`, `x` is float64 and `y` is float32, which would explain the discrepancy. Try this: ```python import jax.numpy as jnp import numpy as np import jax jax.config.update('jax_enable_x64', True) jnp.save(""hi.npy"", 1e8) x = np.load(""hi.npy"").item() y = jnp.load(""hi.npy"").item() print(x)   1e08 print(y)   1e08 ```","Right, but I don't always have 64bit enabled. I understand the reason for this behaviour, it's just hard to work around! Maybe simply to target my actual usecase: given an `np.generic` to deserialise, then this behaviour means that `np.load` must be used for `np.float64`, but `jnp.load` must be used for `jax.dtypes.bfloat16`. Is there a clean way to handle both of these cases, or must a manual whitelist be constructed? I'm hoping to avoid doing something like ```python if dtype.__module__ == ""ml_dtypes"":     jnp.load(...) else:     np.load(...) ```","I think if you want the output to be JAX arrays, use `jnp.load`. If you want the output to be numpy arrays, use `np.load`. Note that `jnp.load` is just a light wrapper around `np.load`: https://github.com/google/jax/blob/ef6fd2ebb6cb31174c63338a07bdec88886dcd8b/jax/_src/numpy/lax_numpy.pyL282L294 If the bakedin logic doesn't work for you, then you could write your own utility in a few lines.","Closing. This makes things a bit tricky for downstream libraries, but on reflection I appreciate there's not that much more JAX can do here."
6028,"以下是一个github上的jax下的一个issue, 标题是(scan + slice update leads to copy instead of in-place update)， 内容是 ( Description Hi, I am running a `scan` with a 1D `carry` array over several thousand steps, where in each step, for a given index `idx`, I need to return the value at `carry[idx]`, then update the `carry` by shifting a slice of the `carry` left oneplace, with `start_index = idx + 1` and a fixed `slice_size`. I call this operation a `pop`, and it is fairly simple to implement with a combination of `scan` + `dynamic_slice` + `dynamic_update_slice`. However, a naive implementation (called `slow_pop` below) seems to be very inefficient, and I suspect it leads to a copy of the `carry` at every step, instead of an inplace update. I came up with a workaround (called `fast_pop` below) that is a couple of orders of magnitude faster. Here is a minimal working example: ```python import timeit from functools import partial import jax import matplotlib.pyplot as plt import numpy as np from jax import lax from jax import numpy as jnp (jax.jit, static_argnames=""slice_size"", donate_argnames=""carry"") def slow_pop(carry, idx, slice_size):     value = carry[idx].copy()     update = lax.dynamic_slice(carry, (idx + 1,), (slice_size,))     carry = lax.dynamic_update_slice(carry, update, (idx,))     return carry, value (jax.jit, static_argnames=""slice_size"", donate_argnames=""carry"") def fast_pop(carry, idx, slice_size):     carry = carry.at[1].set(carry[idx])     update = lax.dynamic_slice(carry, (idx + 1,), (slice_size,))     carry = lax.dynamic_update_slice(carry, update, (idx,))     return carry, carry[1] def scan_fn(fn, init_state, indices):     state, out = lax.scan(fn, init_state.copy(), indices)     state.block_until_ready()     out.block_until_ready()     return state, out def eval_fns(carry_size, slice_size, fns):     assert slice_size <= carry_size     fns = [partial(fn, slice_size=slice_size) for fn in fns]     N_STEPS = 1_000     np.random.seed(42)     init_state = jnp.asarray(np.random.random((carry_size + slice_size + 1)).astype(jnp.float32))     indices = jnp.asarray(np.random.randint(0, carry_size, size=N_STEPS, dtype=np.int32))      Run functions once to JITcompile     expected_state, expected_out = None, None     for fn in fns:         state, out = scan_fn(fn, init_state, indices)         assert expected_out is None or (out == expected_out).all()         assert expected_state is None or (state[:1] == expected_state[:1]).all()         expected_state, expected_out = state, out      Benchmark functions     n_evals = 10     t = np.empty(len(fns))     print(f""{carry_size=}, {slice_size=}"")     for i, fn in enumerate(fns):         t[i] = timeit.timeit(partial(scan_fn, fn, init_state, indices), number=n_evals) / n_evals         print(f""    {fn.func.__name__}: {t[i]: .5f}"")     return t def main(carry_sizes, slice_sizes, fns):     time_carry_sizes = np.empty((len(fns), len(carry_sizes)))     for i, carry_size in enumerate(carry_sizes):         time_carry_sizes[:, i] = eval_fns(carry_size, slice_sizes[1], fns)     time_slice_sizes = np.empty((len(fns), len(slice_sizes)))     for i, slice_size in enumerate(slice_sizes):         time_slice_sizes[:, i] = eval_fns(carry_sizes[1], slice_size, fns)     return time_carry_sizes, time_slice_sizes carry_sizes = np.arange(100_000, 1_000_001, step=100_000, dtype=np.int32) slice_sizes = np.arange(10_000, 100_001, step=10_000, dtype=np.int32) fns = [slow_pop, fast_pop] time_carry_sizes, time_slice_sizes = main(carry_sizes, slice_sizes, fns) _, axes = plt.subplots(2, len(fns), figsize=(12, 8)) for i, (fn, ax) in enumerate(zip(fns, axes[0])):     ax.set_title(f""{fn.__name__} with slice_size = {slice_sizes[1]:_}"")     ax.set_xlabel(""carry_size (in thousands)"")     ax.set_ylabel(""time (in seconds)"")     ax.plot(carry_sizes / 1_000, time_carry_sizes[i]) for i, (fn, ax) in enumerate(zip(fns, axes[1])):     ax.set_title(f""{fn.__name__} with carry_size = {carry_sizes[1]:_}"")     ax.set_xlabel(""slice_size (in thousands)"")     ax.set_ylabel(""time (in seconds)"")     ax.plot(slice_sizes / 1_000, time_slice_sizes[i]) plt.tight_layout() ``` !image In the first row of the plots, we see that `fast_pop` has little dependence on `carry_size`, which is to be expected, while `slow_pop` scales linearly with increase in `carry_size`, which suggests it might be copying the `carry`. In the second row of plots, we see that `fast_pop` scales linearly with `slice_size`, which is again to be expected, while the dependence between `slow_pop` and `slice_size` is less clear, likely since it is bottlenecked elsewhere. In terms of implementation, `slow_pop` simply copies `carry[idx]` into a separate `value`, then updates the `carry` and returns it together with `value`. In contrast, `fast_pop` takes a more convoluted approach, where it first writes the value at `carry[idx]` into the last index of the `carry`, which is reserved for this operation, then does the shift left and finally returns the new `carry` together with the element at `carry[1]`. It seems like the key here is to only return elements of the last state of `carry` in a given step, instead of extracting a value from an earlier state of `carry`, which seems to disrupt the inplace optimizations. Is this expected behaviour? In my actual use case, I have a 2D `carry`, and each `pop` updates a single row. In that case, the performance scales even more badly because it seems we copy the entire `carry` instead of just a single row. Moreover, this example was actually motivated by this issue, where I observed slow performance in `grad` + `scan` + insert, and was able to improve performance of `grad` using a similar trick as `fast_pop` together with `custom_vjp` for the backward pass.  What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14  Which accelerator(s) are you using? CPU  Additional system info python v3.9.9, linux)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,scan + slice update leads to copy instead of in-place update," Description Hi, I am running a `scan` with a 1D `carry` array over several thousand steps, where in each step, for a given index `idx`, I need to return the value at `carry[idx]`, then update the `carry` by shifting a slice of the `carry` left oneplace, with `start_index = idx + 1` and a fixed `slice_size`. I call this operation a `pop`, and it is fairly simple to implement with a combination of `scan` + `dynamic_slice` + `dynamic_update_slice`. However, a naive implementation (called `slow_pop` below) seems to be very inefficient, and I suspect it leads to a copy of the `carry` at every step, instead of an inplace update. I came up with a workaround (called `fast_pop` below) that is a couple of orders of magnitude faster. Here is a minimal working example: ```python import timeit from functools import partial import jax import matplotlib.pyplot as plt import numpy as np from jax import lax from jax import numpy as jnp (jax.jit, static_argnames=""slice_size"", donate_argnames=""carry"") def slow_pop(carry, idx, slice_size):     value = carry[idx].copy()     update = lax.dynamic_slice(carry, (idx + 1,), (slice_size,))     carry = lax.dynamic_update_slice(carry, update, (idx,))     return carry, value (jax.jit, static_argnames=""slice_size"", donate_argnames=""carry"") def fast_pop(carry, idx, slice_size):     carry = carry.at[1].set(carry[idx])     update = lax.dynamic_slice(carry, (idx + 1,), (slice_size,))     carry = lax.dynamic_update_slice(carry, update, (idx,))     return carry, carry[1] def scan_fn(fn, init_state, indices):     state, out = lax.scan(fn, init_state.copy(), indices)     state.block_until_ready()     out.block_until_ready()     return state, out def eval_fns(carry_size, slice_size, fns):     assert slice_size <= carry_size     fns = [partial(fn, slice_size=slice_size) for fn in fns]     N_STEPS = 1_000     np.random.seed(42)     init_state = jnp.asarray(np.random.random((carry_size + slice_size + 1)).astype(jnp.float32))     indices = jnp.asarray(np.random.randint(0, carry_size, size=N_STEPS, dtype=np.int32))      Run functions once to JITcompile     expected_state, expected_out = None, None     for fn in fns:         state, out = scan_fn(fn, init_state, indices)         assert expected_out is None or (out == expected_out).all()         assert expected_state is None or (state[:1] == expected_state[:1]).all()         expected_state, expected_out = state, out      Benchmark functions     n_evals = 10     t = np.empty(len(fns))     print(f""{carry_size=}, {slice_size=}"")     for i, fn in enumerate(fns):         t[i] = timeit.timeit(partial(scan_fn, fn, init_state, indices), number=n_evals) / n_evals         print(f""    {fn.func.__name__}: {t[i]: .5f}"")     return t def main(carry_sizes, slice_sizes, fns):     time_carry_sizes = np.empty((len(fns), len(carry_sizes)))     for i, carry_size in enumerate(carry_sizes):         time_carry_sizes[:, i] = eval_fns(carry_size, slice_sizes[1], fns)     time_slice_sizes = np.empty((len(fns), len(slice_sizes)))     for i, slice_size in enumerate(slice_sizes):         time_slice_sizes[:, i] = eval_fns(carry_sizes[1], slice_size, fns)     return time_carry_sizes, time_slice_sizes carry_sizes = np.arange(100_000, 1_000_001, step=100_000, dtype=np.int32) slice_sizes = np.arange(10_000, 100_001, step=10_000, dtype=np.int32) fns = [slow_pop, fast_pop] time_carry_sizes, time_slice_sizes = main(carry_sizes, slice_sizes, fns) _, axes = plt.subplots(2, len(fns), figsize=(12, 8)) for i, (fn, ax) in enumerate(zip(fns, axes[0])):     ax.set_title(f""{fn.__name__} with slice_size = {slice_sizes[1]:_}"")     ax.set_xlabel(""carry_size (in thousands)"")     ax.set_ylabel(""time (in seconds)"")     ax.plot(carry_sizes / 1_000, time_carry_sizes[i]) for i, (fn, ax) in enumerate(zip(fns, axes[1])):     ax.set_title(f""{fn.__name__} with carry_size = {carry_sizes[1]:_}"")     ax.set_xlabel(""slice_size (in thousands)"")     ax.set_ylabel(""time (in seconds)"")     ax.plot(slice_sizes / 1_000, time_slice_sizes[i]) plt.tight_layout() ``` !image In the first row of the plots, we see that `fast_pop` has little dependence on `carry_size`, which is to be expected, while `slow_pop` scales linearly with increase in `carry_size`, which suggests it might be copying the `carry`. In the second row of plots, we see that `fast_pop` scales linearly with `slice_size`, which is again to be expected, while the dependence between `slow_pop` and `slice_size` is less clear, likely since it is bottlenecked elsewhere. In terms of implementation, `slow_pop` simply copies `carry[idx]` into a separate `value`, then updates the `carry` and returns it together with `value`. In contrast, `fast_pop` takes a more convoluted approach, where it first writes the value at `carry[idx]` into the last index of the `carry`, which is reserved for this operation, then does the shift left and finally returns the new `carry` together with the element at `carry[1]`. It seems like the key here is to only return elements of the last state of `carry` in a given step, instead of extracting a value from an earlier state of `carry`, which seems to disrupt the inplace optimizations. Is this expected behaviour? In my actual use case, I have a 2D `carry`, and each `pop` updates a single row. In that case, the performance scales even more badly because it seems we copy the entire `carry` instead of just a single row. Moreover, this example was actually motivated by this issue, where I observed slow performance in `grad` + `scan` + insert, and was able to improve performance of `grad` using a similar trick as `fast_pop` together with `custom_vjp` for the backward pass.  What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14  Which accelerator(s) are you using? CPU  Additional system info python v3.9.9, linux",2023-09-29T09:30:40Z,bug,open,2,0,https://github.com/jax-ml/jax/issues/17845
2233,"以下是一个github上的jax下的一个issue, 标题是(jit of scan gives XlaRuntimeError: UNIMPLEMENTED: Dot with multiple contracting dimensions not implemented.)， 内容是 ( Description Getting some weird behavior and trying to diagnose.  I have something of the form ```python thing = Thing() def compute_field(params):     def scanfun(out, params_i):         out += thing.compute(params_i)         return out, None     return jax.lax.scan(scanfun, jnp.zeros(n), params)[0] ``` Where `Thing` is a custom pytree class, `params` is a dict of arrays im scanning over, and `Thing.compute` is some big long function. As written above, things seem to run ok. However, if I jit `compute_field` I get the following: ```  XlaRuntimeError                           Traceback (most recent call last) Cell In[18], line 1 > 1 compute_field(params)     [... skipping hidden 14 frame] File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/dispatch.py:465, in backend_compile(backend, module, options, host_callbacks)     460   return backend.compile(built_c, compile_options=options,     461                          host_callbacks=host_callbacks)     462  Some backends don't have `host_callbacks` option yet     463  TODO(sharadmv): remove this fallback when all backends allow `compile`     464  to take in `host_callbacks` > 465 return backend.compile(built_c, compile_options=options) XlaRuntimeError: UNIMPLEMENTED: Dot with multiple contracting dimensions not implemented. ``` I'm unable to give a more complete MWE as I'm not even sure what operation is causing the issue, as far as I know there aren't any dot operations with multiple contracting dimensions, and the error message doesn't give any more detail about where it's failing. Also curious that although `scanfun` should be getting compiled by `jax.lax.scan` it seems like it recompiles on every call (from running `with jax.log_compiles()`) Any clues?  What jax/jaxlib version are you using? jax version=0.4.13, jaxlib version=0.4.13  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jit of scan gives XlaRuntimeError: UNIMPLEMENTED: Dot with multiple contracting dimensions not implemented.," Description Getting some weird behavior and trying to diagnose.  I have something of the form ```python thing = Thing() def compute_field(params):     def scanfun(out, params_i):         out += thing.compute(params_i)         return out, None     return jax.lax.scan(scanfun, jnp.zeros(n), params)[0] ``` Where `Thing` is a custom pytree class, `params` is a dict of arrays im scanning over, and `Thing.compute` is some big long function. As written above, things seem to run ok. However, if I jit `compute_field` I get the following: ```  XlaRuntimeError                           Traceback (most recent call last) Cell In[18], line 1 > 1 compute_field(params)     [... skipping hidden 14 frame] File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/dispatch.py:465, in backend_compile(backend, module, options, host_callbacks)     460   return backend.compile(built_c, compile_options=options,     461                          host_callbacks=host_callbacks)     462  Some backends don't have `host_callbacks` option yet     463  TODO(sharadmv): remove this fallback when all backends allow `compile`     464  to take in `host_callbacks` > 465 return backend.compile(built_c, compile_options=options) XlaRuntimeError: UNIMPLEMENTED: Dot with multiple contracting dimensions not implemented. ``` I'm unable to give a more complete MWE as I'm not even sure what operation is causing the issue, as far as I know there aren't any dot operations with multiple contracting dimensions, and the error message doesn't give any more detail about where it's failing. Also curious that although `scanfun` should be getting compiled by `jax.lax.scan` it seems like it recompiles on every call (from running `with jax.log_compiles()`) Any clues?  What jax/jaxlib version are you using? jax version=0.4.13, jaxlib version=0.4.13  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-09-28T19:29:35Z,bug needs info,closed,0,3,https://github.com/jax-ml/jax/issues/17834,"Can you please fill in the bug sections about accelerator and GPU info? What hardware platform are you using. Can you please try with the latest JAX and jaxlib releases (0.4.18)? The issue might already be fixed. If not, we'll need some sort of reproduction to debug this. One possibility would be for you to take an XLA HLO dump, which you can get by reproducing the bug with the environment variable `XLA_FLAGS=xla_dump_to=/somewhere`, and zip up `/somewhere` and attach it to this bug.",I searched for this message in the code and it appears to be in the CPU XLAtoLLVM IR translator here.,Closing; we need a reproduction or an HLO dump. We'd be happy to look into this if you can provide either.
2453,"以下是一个github上的jax下的一个issue, 标题是(Compilation error on jaxlib v0.4.16)， 内容是 ( Description Following this old tutorial for Jax on Jetson Nano I am unable to successfully build jax from source on a Jetson Orin Nano. Env setup & download JAX: ```py virtualenv p /usr/bin/python3.9 py39 source ./py39/bin/activate python3 m pip install numpy scipy six wheel git clone https://github.com/google/jax cd jax ``` Build taken from here: ```py python3 build/build.py enable_cuda cuda_compute_capabilities=sm_87 ``` Fails with the following output: ```py      _   _  __  __             ^~~~~~~~ cc1plus: warning: unrecognized command line option ‘Wnoarrayparameter’ cc1plus: warning: unrecognized command line option ‘Wnounknownwarningoption’ Target //jaxlib/tools:build_wheel failed to build [10,084 / 12,164] checking cached actions INFO: Elapsed time: 7864.725s, Critical Path: 360.25s INFO: 10084 processes: 3843 internal, 6241 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target Traceback (most recent call last):   File ""/home/jetson/jax/build/build.py"", line 598, in      main()   File ""/home/jetson/jax/build/build.py"", line 580, in main     shell(command)   File ""/home/jetson/jax/build/build.py"", line 44, in shell     output = subprocess.check_output(cmd)   File ""/usr/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/usr/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel6.1.2linuxarm64', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=/home/jetson/jax/dist', 'cpu=aarch64']' returned nonzero exit status 1. ``` I've been trying to get this working for the past weeks but I have not been able to find a proper solution. If anyone made it work and is willing to share what they did I would greatly appreciate it.  What jax/jaxlib version are you using? jaxlib 0.4.16  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 20.04(aarch64 NVIDIA Jetson Orin Nano), Python 3.9.18, gcc 9.4  NVIDIA GPU info No `nvidiasmi` but with jtop I can list the CUDA version and the cuDNN version. ```bash CUDA: 11.4.315 cuDNN: 8.6.0.166 CUDA Arch bin: 8.7 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Compilation error on jaxlib v0.4.16," Description Following this old tutorial for Jax on Jetson Nano I am unable to successfully build jax from source on a Jetson Orin Nano. Env setup & download JAX: ```py virtualenv p /usr/bin/python3.9 py39 source ./py39/bin/activate python3 m pip install numpy scipy six wheel git clone https://github.com/google/jax cd jax ``` Build taken from here: ```py python3 build/build.py enable_cuda cuda_compute_capabilities=sm_87 ``` Fails with the following output: ```py      _   _  __  __             ^~~~~~~~ cc1plus: warning: unrecognized command line option ‘Wnoarrayparameter’ cc1plus: warning: unrecognized command line option ‘Wnounknownwarningoption’ Target //jaxlib/tools:build_wheel failed to build [10,084 / 12,164] checking cached actions INFO: Elapsed time: 7864.725s, Critical Path: 360.25s INFO: 10084 processes: 3843 internal, 6241 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target Traceback (most recent call last):   File ""/home/jetson/jax/build/build.py"", line 598, in      main()   File ""/home/jetson/jax/build/build.py"", line 580, in main     shell(command)   File ""/home/jetson/jax/build/build.py"", line 44, in shell     output = subprocess.check_output(cmd)   File ""/usr/lib/python3.9/subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/usr/lib/python3.9/subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel6.1.2linuxarm64', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=/home/jetson/jax/dist', 'cpu=aarch64']' returned nonzero exit status 1. ``` I've been trying to get this working for the past weeks but I have not been able to find a proper solution. If anyone made it work and is willing to share what they did I would greatly appreciate it.  What jax/jaxlib version are you using? jaxlib 0.4.16  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 20.04(aarch64 NVIDIA Jetson Orin Nano), Python 3.9.18, gcc 9.4  NVIDIA GPU info No `nvidiasmi` but with jtop I can list the CUDA version and the cuDNN version. ```bash CUDA: 11.4.315 cuDNN: 8.6.0.166 CUDA Arch bin: 8.7 ```",2023-09-28T14:49:18Z,bug build NVIDIA GPU,closed,0,7,https://github.com/jax-ml/jax/issues/17829,"I'm not 100% sure, but my guess is this is related to your CUDA version (11.4), which is much older than anything JAX supports (11.8 or newer). Can you update?",I'll try to update today but if it fails is there any older version that works with the CUDA version that I have?,"I'm confident that's it. This code doesn't define that value for CUDA < 11.6. https://github.com/openxla/xla/blob/bdb788c2e164c661ed8390d7825b0d792d298ac1/xla/backends/profiler/gpu/cupti_tracer.ccL63 We could fix that, but I don't know there's a lot of point: we've dropped support for that CUDA version. You should just update. Closing since the fix is to update CUDA!","I have tried building some older versions of jax for CUDA 11.4, haven't had any success yet. The NX series is flashed with CUDA 11.4 and cuDNN 0.8.6. Any leads regarding this would be helpful, thanks!",If you want to use an older version of Jax for CUDA 11.4 and cuDNN 8.6.0 you can try using this wheel. Last year when I used it I remember that it worked as intended.,"  Thanks, I've checked your wheels, really helpful! Were you able to update cuDNN on Xavier NX to build the later versions?","I was able to use an updated version of cuDNN and CUDA on an Orin Nano. I am not sure about an Xavier NX but it should be possible. There are special versions for both that work on Jetson. cuDNN CUDA (these are the latest just an example) Whatever you are trying to do remember that if it doesn't work directly you could always try to use docker with Jax, CUDA, and cuDNN but it does require some tinkering."
333,"以下是一个github上的jax下的一个issue, 标题是(Fix incorrect backend allowlist in array_interoperability_test.)， 内容是 (We intended to only enable this test on CPU and GPU, but we were missing a critical ""not"".)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix incorrect backend allowlist in array_interoperability_test.,"We intended to only enable this test on CPU and GPU, but we were missing a critical ""not"".",2023-09-28T14:31:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17828
2813,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] jax.random.split fails on CUDA)， 内容是 ( Description I'm unable to use `jax.random.split` in code that gets converted with `tf2jax`. Example: ```python def f(k: jax.Array):     return jax.random.split(k, 2)[0] ftf = jax2tf.convert(f, polymorphic_shapes=[""(2)""]) print(""JAX"", f(jax.random.PRNGKey(42))) print(""TF"", ftf(tf.convert_to_tensor(jax.random.PRNGKey(42)))) ``` Expected result: ``` JAX [2465931498 3679230171] TF [2465931498 3679230171] ``` Actual result: ``` JAX [2465931498 3679230171] 20230927 15:05:00.097533: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_compile_on_demand_op.cc:292 : UNIMPLEMENTED: No registered implementation for custom call to ""cu_threefry2x32"" for platform ""CUDA"" Traceback (most recent call last):   File ""main.py"", line 109, in      main()   File ""main.py"", line 65, in main     print(""TF"", ftf(tf.convert_to_tensor(jax.random.PRNGKey(42))))   File ""/jax/experimental/jax2tf/jax2tf.py"", line 417, in converted_fun_tf     outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)   File ""/tensorflow/python/ops/custom_gradient.py"", line 343, in __call__     return self._d(self._f, a, k)   File ""/tensorflow/python/ops/custom_gradient.py"", line 297, in decorated     return _eager_mode_decorator(wrapped, args, kwargs)   File ""/tensorflow/python/ops/custom_gradient.py"", line 543, in _eager_mode_decorator     result, grad_fn = f(*args, **kwargs)   File ""/jax/experimental/jax2tf/jax2tf.py"", line 409, in converted_fun_flat_with_custom_gradient_tf     outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)   File ""/jax/experimental/jax2tf/jax2tf.py"", line 518, in run_fun_tf     results = _run_exported_as_tf(args_flat_tf, self.exported)   File ""/jax/experimental/jax2tf/jax2tf.py"", line 877, in _run_exported_as_tf     res = tfxla.call_module(args_flat_tf, **call_module_attrs)   File ""/tensorflow/compiler/tf2xla/python/xla.py"", line 645, in call_module     res = gen_xla_ops.xla_call_module(   File ""/tensorflow/compiler/tf2xla/ops/gen_xla_ops.py"", line 337, in xla_call_module     _ops.raise_from_not_ok_status(e, name)   File ""/tensorflow/python/framework/ops.py"", line 5887, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess ``` Seems like tf2jax is spitting out `cu_threefry2x32` calls that don't correspond to valid TF ops.  What jax/jaxlib version are you using? jax 0.4.16, jaxlib 0.4.16+cuda12.cudnn89, tfnightly 2.15.0.dev20230927  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22.04  NVIDIA GPU info ``` Wed Sep 27 15:08:54 2023        ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] jax.random.split fails on CUDA," Description I'm unable to use `jax.random.split` in code that gets converted with `tf2jax`. Example: ```python def f(k: jax.Array):     return jax.random.split(k, 2)[0] ftf = jax2tf.convert(f, polymorphic_shapes=[""(2)""]) print(""JAX"", f(jax.random.PRNGKey(42))) print(""TF"", ftf(tf.convert_to_tensor(jax.random.PRNGKey(42)))) ``` Expected result: ``` JAX [2465931498 3679230171] TF [2465931498 3679230171] ``` Actual result: ``` JAX [2465931498 3679230171] 20230927 15:05:00.097533: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_compile_on_demand_op.cc:292 : UNIMPLEMENTED: No registered implementation for custom call to ""cu_threefry2x32"" for platform ""CUDA"" Traceback (most recent call last):   File ""main.py"", line 109, in      main()   File ""main.py"", line 65, in main     print(""TF"", ftf(tf.convert_to_tensor(jax.random.PRNGKey(42))))   File ""/jax/experimental/jax2tf/jax2tf.py"", line 417, in converted_fun_tf     outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)   File ""/tensorflow/python/ops/custom_gradient.py"", line 343, in __call__     return self._d(self._f, a, k)   File ""/tensorflow/python/ops/custom_gradient.py"", line 297, in decorated     return _eager_mode_decorator(wrapped, args, kwargs)   File ""/tensorflow/python/ops/custom_gradient.py"", line 543, in _eager_mode_decorator     result, grad_fn = f(*args, **kwargs)   File ""/jax/experimental/jax2tf/jax2tf.py"", line 409, in converted_fun_flat_with_custom_gradient_tf     outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)   File ""/jax/experimental/jax2tf/jax2tf.py"", line 518, in run_fun_tf     results = _run_exported_as_tf(args_flat_tf, self.exported)   File ""/jax/experimental/jax2tf/jax2tf.py"", line 877, in _run_exported_as_tf     res = tfxla.call_module(args_flat_tf, **call_module_attrs)   File ""/tensorflow/compiler/tf2xla/python/xla.py"", line 645, in call_module     res = gen_xla_ops.xla_call_module(   File ""/tensorflow/compiler/tf2xla/ops/gen_xla_ops.py"", line 337, in xla_call_module     _ops.raise_from_not_ok_status(e, name)   File ""/tensorflow/python/framework/ops.py"", line 5887, in raise_from_not_ok_status     raise core._status_to_exception(e) from None   pylint: disable=protectedaccess ``` Seems like tf2jax is spitting out `cu_threefry2x32` calls that don't correspond to valid TF ops.  What jax/jaxlib version are you using? jax 0.4.16, jaxlib 0.4.16+cuda12.cudnn89, tfnightly 2.15.0.dev20230927  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22.04  NVIDIA GPU info ``` Wed Sep 27 15:08:54 2023        ++  ++++ ```",2023-09-27T22:11:12Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17816,"Hi , I tried to reproduce this issue with the latest JAX version 0.5.0 on colab notebook.It works without any error now. Please find the gist for reference. Thank you."
1126,"以下是一个github上的jax下的一个issue, 标题是(Destruct objects owned by `WeakRefLRUCache::CacheEntry` out of band using `GlobalPyRefManager()`)， 内容是 (Destruct objects owned by `WeakRefLRUCache::CacheEntry` out of band using `GlobalPyRefManager()` This assumes less about whether the thread that destructs `CacheEntry` has GIL or not, which is difficult to reason about due to the `xla::LRUCache`'s use of `std::shared_ptr`. The following changes have been made in JAX to accommodate the behavior differences from direct destruction to GC: * Since `PyLoadedExecutable`s cached in `WeakRefLRUCache` are now destructed out of band, `PyClient::LiveExecutables()` calls `GlobalPyRefManager()>CollectGarbage()` to make the returned information accurate and up to date. * `test_jit_reference_dropping` has been updated to call `gc.collect()` before verifying the live executable counts since the destruction of executables owned by weak ref maps is now done out of band as part of `GlobalPyRefManager`'s GC.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Destruct objects owned by `WeakRefLRUCache::CacheEntry` out of band using `GlobalPyRefManager()`,"Destruct objects owned by `WeakRefLRUCache::CacheEntry` out of band using `GlobalPyRefManager()` This assumes less about whether the thread that destructs `CacheEntry` has GIL or not, which is difficult to reason about due to the `xla::LRUCache`'s use of `std::shared_ptr`. The following changes have been made in JAX to accommodate the behavior differences from direct destruction to GC: * Since `PyLoadedExecutable`s cached in `WeakRefLRUCache` are now destructed out of band, `PyClient::LiveExecutables()` calls `GlobalPyRefManager()>CollectGarbage()` to make the returned information accurate and up to date. * `test_jit_reference_dropping` has been updated to call `gc.collect()` before verifying the live executable counts since the destruction of executables owned by weak ref maps is now done out of band as part of `GlobalPyRefManager`'s GC.",2023-09-27T18:12:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17810
525,"以下是一个github上的jax下的一个issue, 标题是([export] Set the default export serialization version to 8.)， 内容是 ([export] Set the default export serialization version to 8. This version has been supported by XlaCallModule since July 21, 2023 and we are now past the forwardcompatibility window. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[export] Set the default export serialization version to 8.,"[export] Set the default export serialization version to 8. This version has been supported by XlaCallModule since July 21, 2023 and we are now past the forwardcompatibility window. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions",2023-09-26T06:59:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17782
288,"以下是一个github上的jax下的一个issue, 标题是([random] wrap_key_data accepts impl=key.dtype.impl)， 内容是 (Fixes CC(jax.random.wrap_key_data(impl=key.impl) problems))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[random] wrap_key_data accepts impl=key.dtype.impl,Fixes CC(jax.random.wrap_key_data(impl=key.impl) problems),2023-09-25T16:04:16Z,pull ready,closed,0,8,https://github.com/jax-ml/jax/issues/17768,"Could we instead expose a function, `jax.random.key_impl`, that takes a key array and returns a string that specifies the implementation (acceptable by `key` and `wrap_key_data`)?","Note that `impl` is not a member of the key array class, but rather a member of the keyspecific `dtype` class.","Oh, thanks for correcting me there. Adjusting: what do you think of introducing `key_impl` that accepts a dtype (maybe as well as a key array)? And then we hide the `.impl` attribute of the `.dtype`returned object too, as well as any other attributes that diverge from Numpy's dtypes! The key dtype could then look more like other dtypes.","Are you proposing something like this for the new API? ```python def wrap_key_data(key_bits_array: Array, *, impl: Optional[str] = None, dtype: Optional[ExtendedDtype] = None):   if dtype is not None:     assert impl is None, ""only one of impl and dtype may be specified""     impl_obj = dtype.impl   eventually a private attribute   else:     impl_obj = resolve_prng_impl(impl)   return prng.random_wrap(key_bits_array, impl=impl_obj) ```","Not quite. What I'm suggesting we consider is keeping `wrap_key_data` as is, and then introducing a new function `key_impl`. The `key_impl` function takes a dtype (maybe also an array, for convenience) and returns a string specifying the implementation. The string, when passed back to `wrap_key_data`, closes the round trip: ```python k = jax.random.key(...) data = jax.random.key_data(k) impl = jax.random.key_impl(k)     or k.dtype k2 = jax.random.wrap_key_data(data, impl=impl) assert k.dtype == k2.dtype ``` Separately, I'm suggesting we hide `.impl` from the `k.dtype` object's public attributes. What do you think of that?","> ... returns a string specifying the implementation. The string, when passed back to wrap_key_data, closes the round trip: OK, in that case it sounds like we'll need `PRNGKeyImpl` to have some kind of registry system, whereby when it is created it will register its name with some dictionary that maps strings to impls that can then be used by `resolve_prng_impl`. We'll have to figure out what to do if someone tries to register a duplicate impl name: probably the best thing would be to error in that case, because it could cause unintended consequences. An annoying part about that though is if you're developing iteratively in a notebook, then reexecuting a cell that defines your `PRNGKeyImpl` would lead to a duplicate registration error. Another option is that rather than returning a unique string identifier, we instead return the actual `PRNGImpl` object. What do you think?","Another observation here: `jax.random.default_prng_impl()` returns a `PRNGImpl` object, not a string. I've found a case where a user is doing something like: ```python key = jax.random.key(seed, impl=jax.random.default_prng_impl()) ``` which seems like it should work, but it doesn't for the reasons discussed above. If we decide to go with the ""impls are only referred to by their string name"" route, we'll have to figure out what to do with `jax.random.default_prng_impl()`.",Replaced by CC(identify PRNG schemes on key arrays; recognize them in key constructors)
1011,"以下是一个github上的jax下的一个issue, 标题是(jax.random.key(0).itemsize crashes)， 内容是 ( Description I would expect the following to work, as it works for numpy arrays. ``` In [13]: jax.random.key(3).itemsize  NotImplementedError                       Traceback (most recent call last) Cell In[13], line 1 > 1 jax.random.key(3).itemsize File ~/Documents/pythonenvs/mpi4jax/python3.11.1/lib/python3.11/sitepackages/jax/_src/numpy/array_methods.py:779, in _make_abstract_method..method(*args, **kwargs)     776 .abstractmethod     777 (func)     778 def method(*args, **kwargs): > 779   raise NotImplementedError(f""Cannot call abstract method {name}"") NotImplementedError: Cannot call abstract method itemsize ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,jax.random.key(0).itemsize crashes," Description I would expect the following to work, as it works for numpy arrays. ``` In [13]: jax.random.key(3).itemsize  NotImplementedError                       Traceback (most recent call last) Cell In[13], line 1 > 1 jax.random.key(3).itemsize File ~/Documents/pythonenvs/mpi4jax/python3.11.1/lib/python3.11/sitepackages/jax/_src/numpy/array_methods.py:779, in _make_abstract_method..method(*args, **kwargs)     776 .abstractmethod     777 (func)     778 def method(*args, **kwargs): > 779   raise NotImplementedError(f""Cannot call abstract method {name}"") NotImplementedError: Cannot call abstract method itemsize ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-09-25T14:06:00Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/17761,Thanks for the report! CC([random] add itemsize property to custom PRNG) should fix it.
1160,"以下是一个github上的jax下的一个issue, 标题是(jax.random.wrap_key_data(impl=key.impl) problems)， 内容是 ( Description Problem: I would expect the following to work, in order to programmatically wrap/unwrap keys, but it does not ```python b = jax.random.key(1) jax.random.wrap_key_data(jax.random.key_data(b), impl=b.impl) ``` as it fails with error ```python File ~/Documents/pythonenvs/netket/python3.11.2/lib/python3.11/sitepackages/jax/_src/random.py:139, in resolve_prng_impl(impl_spec)     136   return PRNG_IMPLS[impl_spec]     138 keys_fmt = ', '.join(f'""{s}""' for s in PRNG_IMPLS.keys()) > 139 raise ValueError(f'unrecognized PRNG implementation ""{impl_spec}"". '     140                  f'Did you mean one of: {keys_fmt}?') ValueError: unrecognized PRNG implementation ""fry"". Did you mean one of: ""threefry2x32"", ""rbg"", ""unsafe_rbg""? ```  What jax/jaxlib version are you using? jax==master (today, 25 September) jaxlib==0.4.16  Which accelerator(s) are you using? CPU  Additional system info MacOs  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.random.wrap_key_data(impl=key.impl) problems," Description Problem: I would expect the following to work, in order to programmatically wrap/unwrap keys, but it does not ```python b = jax.random.key(1) jax.random.wrap_key_data(jax.random.key_data(b), impl=b.impl) ``` as it fails with error ```python File ~/Documents/pythonenvs/netket/python3.11.2/lib/python3.11/sitepackages/jax/_src/random.py:139, in resolve_prng_impl(impl_spec)     136   return PRNG_IMPLS[impl_spec]     138 keys_fmt = ', '.join(f'""{s}""' for s in PRNG_IMPLS.keys()) > 139 raise ValueError(f'unrecognized PRNG implementation ""{impl_spec}"". '     140                  f'Did you mean one of: {keys_fmt}?') ValueError: unrecognized PRNG implementation ""fry"". Did you mean one of: ""threefry2x32"", ""rbg"", ""unsafe_rbg""? ```  What jax/jaxlib version are you using? jax==master (today, 25 September) jaxlib==0.4.16  Which accelerator(s) are you using? CPU  Additional system info MacOs  NVIDIA GPU info _No response_",2023-09-25T12:08:10Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/17758,"Hi, thanks for the report! The `impl` argument to `wrap_random_key` must be a string, and `key.impl` is not a string. I suppose we could allow the `impl` argument to be a private `Impl` object, but this is an internal class that is not documented or easily userconstructable, so that might add some confusion. What do you think would be best?","My need is to be able to unwrap and rewrap arbitrary keys programmatically (to be able to communicate them using `mpi4jax`, which can only handle primitive/standard dtypes). I was thinking of doing something like ```python def mpi_call(value, ...)     if jax.dtypes.issubdtype(value.dtype, jax.dtypes.prng_key):           unwrap = partial(jax.random.wrap_key_data, impl=value.impl)           value = jax.random.key_data(value)     else:           unwrap = identity      res = _bare_mpi_call(value)      return unwrap(res) ``` For the simple fact that they have the same name (`impl`) I would expect this to work.  I imagine it would not be very hard for you to support, adding some logic to `resolve_prng_impl` such as: ```python def resolve_prng_impl(impl_spec: Optional[Union[PRNGImpl, str]]):   if impl_spec is None:     return default_prng_impl()   if impl_spec in PRNG_IMPLS:     return PRNG_IMPLS[impl_spec]   if isinstance(impl_spec, PRNGImpl):     return impl_spec   keys_fmt = ', '.join(f'""{s}""' for s in PRNG_IMPLS.keys())   raise ValueError(f'unrecognized PRNG implementation ""{impl_spec}"". '                    f'Did you mean one of: {keys_fmt}?') ``` If you do not think this is a clean solution (though, it seems to me it makes sense? why am I obliged to pass a string?) I think you should ensure that a PRNG implementation knows/contains its 'string' specification. For example, right now `impl.tag` returns something that cannot be roundtripped as an implementation specification `In [3]: a.impl.tag Out[3]: 'fry'`. Simply ensuring that `impl.tag` or another field has the correct key, and properly documenting this, would make it easier to work with. However, this latter approach is IMO more convoluted, as you're asking a value in a dictionary to know the key with which it is stored in a dictionary, and therefore it seems to me that the first approach, simply accepting a `PRNGImpl` as an implementation, is much cleaner. After all, the string to `PRNGImpl` is just some sort of _input sanitisation_ that you are running because you don't expect users to construct those objects themselves, but if they do... is it a problem?","Thanks – I agree that that's a clean solution at runtime, but people love their types. I'm not sure how to annotate that meaningfully without either (1) making the type `Any`, (2) exposing the currently private `PRNGImpl` type which we'd prefer not be in the public API, or (3) keeping it asis and requiring you to write ` type: ignore` when you write the code you suggest.  What do you think?","Well, I would like anything you define to be semipublic and semidocumented such that we don't have to change our code often.  If you do not want to expose the `PRNGImpl` type, there might be an alternative. I just noticed that `tag` is used to construct the `hash` of the `PRNGImpl` object, so it already must be 'correctly' defined. Can't you instead change the (semipublic, `jax.extend`) API to register a new `PRNGImplementation` to register it under the `tag` name? Then, the fact that the implementations are stored in a dictionary is just an implementation detail. In short, I'm suggesting adding a function  ```python PRNG_IMPLS = {} def register_prng_implementation(impl: PRNGImpl):     if impl.tag not in PRNG_IMPLS:                PRNG_IMPLS[impl.tag] = impl     else:         error... ``` instead of exposing `PRNG_IMPLS` in `jax.extend.random` you could expose this function (as well as use it internally whenever you register your implementations).","If that's not acceptable, of course a 'misannotation' of the type as you proposed in (3), if documented/tested, would also be okay.","Stepping back – we originally did not plan to expose `wrap_random_key`, because we expect usage of it to be relatively rare in the eventual steady state. Why do you need to unwrap and rewrap keys? Can you not change your package to work directly with the newstyle keys?","To get `mpi` working, the bare minimum we need is to get the pointer to the data and the 'data size' .  I could try to just treat an extended data type as a `uint` array of length `dtype.itemsize` as well...",Thanks for the feedback and discussion – I think CC([random] wrap_key_data accepts impl=key.dtype.impl) should address this.,"I commented on CC([random] wrap_key_data accepts impl=key.dtype.impl) about the possibility of exposing a function, `jax.random.key_impl` that supports the round trip. Separately, we could look into hiding the `impl` attribute, as well as any others that aren't shared with other dtypes (i.e. Numpy's). > If you do not want to expose the `PRNGImpl` type, there might be an alternative. We expose the `PRNGImpl` type via `jax.extend.random`, but not through the main jax API.",I believe CC(identify PRNG schemes on key arrays; recognize them in key constructors) (which replaced CC([random] wrap_key_data accepts impl=key.dtype.impl)) closes this. Please reopen if you disagree!
3727,"以下是一个github上的jax下的一个issue, 标题是(Class 'function' is not a valid JAX type with Custom VJP)， 内容是 ( Description Hi JAX team,  I am adapting this tutorial to define a custom vjp for the fixed point iteration below. I understand the changes you made in the past, and I believe I marked my `func` and `integrator` arguments (both functions) as nondiff as it should be; and I left `B0`, `z0`, and `rhs` as 'diff' arguments eventhough the only derivative I am interested into is the one wrt `rhs`.  ```python from jax.lax import while_loop from functools import partial import jax from jax import vjp, jit, vmap, grad import jax.numpy as jnp (jax.custom_vjp, nondiff_argnums=(0,4)) def fixed_point_finder(func, B0, z0, rhs, integrator):     def cond_fun(carry):         B_prev, B = carry         return jnp.linalg.norm(B_prev  B) > 1e2     def body_fun(carry):         _, B = carry         return B, func(B, z0, rhs, integrator)     _, B_star = jax.lax.while_loop(cond_fun, body_fun, (B0, func(B0, z0, rhs, integrator)))     return B_star def fixed_point_finder_fwd(func, B0, z0, rhs, integrator):     B_star = fixed_point_finder(func, B0, z0, rhs, integrator)     return B_star, (B_star, z0, rhs, integrator) def rev_iter(func, packed, w):     z0, rhs, integrator, B_star, v = packed     _, vjp_B = jax.vjp(lambda B: func(B, z0, rhs, integrator), B_star)     return v + vjp_B(w)[0] def fixed_point_finder_bwd(func, res, v):     B_star, z0, rhs, integrator = res     _, vjp_rhs = jax.vjp(lambda rhs: func(B_star, z0, rhs, integrator), rhs)     w = fixed_point_finder(partial(rev_iter, func),                             (z0, rhs, integrator, B_star, v),                              v)     theta_bar, = vjp_rhs(w)     return jnp.zeros_like(B_star), None, theta_bar fixed_point_finder.defvjp(fixed_point_finder_fwd, fixed_point_finder_bwd) def loss_function(func, B0, z0, rhs, integrator):     return jnp.linalg.norm(fixed_point_finder(func, B0, z0, rhs, integrator)) def myfunc(x,y,z,a):   return x def myint(x):   x grad_func = jax.grad(loss_function, argnums=3) grad_func(myfunc, jnp.zeros((4)), jnp.zeros((4)), jnp.ones((4,5)), myint) ``` I keep getting this error.  ```  TypeError                                 Traceback (most recent call last) [](https://tfblwthppxs496ff2e9c6d221160colab.googleusercontent.com/outputframe.html?vrz=colab_20230921060057_RC00_567231865) in ()      60       61 grad_func = jax.grad(loss_function, argnums=3) > 62 grad_func(myfunc, jnp.zeros((4)), jnp.zeros((4)), jnp.ones((4,5)), myint)     [... skipping hidden 10 frame] 1 frames     [... skipping hidden 11 frame] /usr/local/lib/python3.10/distpackages/jax/_src/core.py in concrete_aval(x)    1361   if hasattr(x, '__jax_array__'):    1362     return concrete_aval(x.__jax_array__()) > 1363   raise TypeError(f""Value {repr(x)} with type {type(x)} is not a valid JAX ""    1364                    ""type"")    1365  TypeError: Value  with type  is not a valid JAX type ``` I should note that the error was a bit different before I put together this MWE. Rather than `function` not being a valid JAX type, it used to say  ``` TypeError: Value  with type  is not a valid JAX type ``` I believe this issue is similar to CC(New `TypeError:  is not a valid JAX type`  exception in JAX 0.2.0), although that one mostly involved `jit`, rather than `vjp`. Please, how to overcome this issue ? Thanks.  What jax/jaxlib version are you using? jax 0.4.14; jaxlib 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Typical Colab runtime  NVIDIA GPU info ``` Sun Sep 24 08:13:11 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Class 'function' is not a valid JAX type with Custom VJP," Description Hi JAX team,  I am adapting this tutorial to define a custom vjp for the fixed point iteration below. I understand the changes you made in the past, and I believe I marked my `func` and `integrator` arguments (both functions) as nondiff as it should be; and I left `B0`, `z0`, and `rhs` as 'diff' arguments eventhough the only derivative I am interested into is the one wrt `rhs`.  ```python from jax.lax import while_loop from functools import partial import jax from jax import vjp, jit, vmap, grad import jax.numpy as jnp (jax.custom_vjp, nondiff_argnums=(0,4)) def fixed_point_finder(func, B0, z0, rhs, integrator):     def cond_fun(carry):         B_prev, B = carry         return jnp.linalg.norm(B_prev  B) > 1e2     def body_fun(carry):         _, B = carry         return B, func(B, z0, rhs, integrator)     _, B_star = jax.lax.while_loop(cond_fun, body_fun, (B0, func(B0, z0, rhs, integrator)))     return B_star def fixed_point_finder_fwd(func, B0, z0, rhs, integrator):     B_star = fixed_point_finder(func, B0, z0, rhs, integrator)     return B_star, (B_star, z0, rhs, integrator) def rev_iter(func, packed, w):     z0, rhs, integrator, B_star, v = packed     _, vjp_B = jax.vjp(lambda B: func(B, z0, rhs, integrator), B_star)     return v + vjp_B(w)[0] def fixed_point_finder_bwd(func, res, v):     B_star, z0, rhs, integrator = res     _, vjp_rhs = jax.vjp(lambda rhs: func(B_star, z0, rhs, integrator), rhs)     w = fixed_point_finder(partial(rev_iter, func),                             (z0, rhs, integrator, B_star, v),                              v)     theta_bar, = vjp_rhs(w)     return jnp.zeros_like(B_star), None, theta_bar fixed_point_finder.defvjp(fixed_point_finder_fwd, fixed_point_finder_bwd) def loss_function(func, B0, z0, rhs, integrator):     return jnp.linalg.norm(fixed_point_finder(func, B0, z0, rhs, integrator)) def myfunc(x,y,z,a):   return x def myint(x):   x grad_func = jax.grad(loss_function, argnums=3) grad_func(myfunc, jnp.zeros((4)), jnp.zeros((4)), jnp.ones((4,5)), myint) ``` I keep getting this error.  ```  TypeError                                 Traceback (most recent call last) [](https://tfblwthppxs496ff2e9c6d221160colab.googleusercontent.com/outputframe.html?vrz=colab_20230921060057_RC00_567231865) in ()      60       61 grad_func = jax.grad(loss_function, argnums=3) > 62 grad_func(myfunc, jnp.zeros((4)), jnp.zeros((4)), jnp.ones((4,5)), myint)     [... skipping hidden 10 frame] 1 frames     [... skipping hidden 11 frame] /usr/local/lib/python3.10/distpackages/jax/_src/core.py in concrete_aval(x)    1361   if hasattr(x, '__jax_array__'):    1362     return concrete_aval(x.__jax_array__()) > 1363   raise TypeError(f""Value {repr(x)} with type {type(x)} is not a valid JAX ""    1364                    ""type"")    1365  TypeError: Value  with type  is not a valid JAX type ``` I should note that the error was a bit different before I put together this MWE. Rather than `function` not being a valid JAX type, it used to say  ``` TypeError: Value  with type  is not a valid JAX type ``` I believe this issue is similar to CC(New `TypeError:  is not a valid JAX type`  exception in JAX 0.2.0), although that one mostly involved `jit`, rather than `vjp`. Please, how to overcome this issue ? Thanks.  What jax/jaxlib version are you using? jax 0.4.14; jaxlib 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Typical Colab runtime  NVIDIA GPU info ``` Sun Sep 24 08:13:11 2023        ++  ++ ```",2023-09-24T08:17:36Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/17753,"It looks like the problem is this line: ```python     w = fixed_point_finder(partial(rev_iter, func),                             (z0, rhs, integrator, B_star, v),                              v) ``` You are passing a tuple containing `(z0, rhs, integrator, B_star, v)` as the `B0` parameter of `fixed_point_finder`, which initializes the `carry` of the loop. Then in your function, you seem to assume here that `B0` is an array rather than a tuple of objects of mixed type: ```python         B_prev, B = carry         return jnp.linalg.norm(B_prev  B) > 1e2 ``` I think you need to take another look at the code and make sure you are calling the functions in the way you expected them to be called.","Hi , Thanks for your help. There was a problem with the code where you mentioned. The rewrite below should have fixed any such issues. But I still get the same error. I guess the bigger question is how to debug something like this. Is there any way to make the error message a bit more explicit, or maybe point to the line where the error actually happens ?  ```python from jax.lax import while_loop from functools import partial import jax from jax import vjp, jit, vmap, grad import jax.numpy as jnp (jax.custom_vjp, nondiff_argnums=(0,4)) def fixed_point_finder(func, B0, z0, rhs, integrator):     def cond_fun(carry):         B_prev, B = carry         return jnp.linalg.norm(B_prev  B) > 1e2     def body_fun(carry):         _, B = carry         return B, func(B, z0, rhs, integrator)     _, B_star = jax.lax.while_loop(cond_fun, body_fun, (B0, func(B0, z0, rhs, integrator)))     return B_star def fixed_point_finder_fwd(func, B0, z0, rhs, integrator):     B_star = fixed_point_finder(func, B0, z0, rhs, integrator)     return B_star, (B_star, z0, rhs, integrator) def inner_fixed_point(func, z0, rhs, integrator, B_star, v):     _, vjp_B = jax.vjp(lambda B: func(B, z0, rhs, integrator), B_star)     def cond_fun(carry):         w_prev, w = carry         return jnp.linalg.norm(w_prev  w) > 1e2     def body_fun(carry):         _, w = carry         return w, v + vjp_B(w)[0]     _, w_star = jax.lax.while_loop(cond_fun, body_fun, (v, v + vjp_B(v)[0]))     return w_star def fixed_point_finder_bwd(func, res, v):     B_star, z0, rhs, integrator = res     _, vjp_rhs = jax.vjp(lambda rhs: func(B_star, z0, rhs, integrator), rhs)     w = inner_fixed_point(func, z0, rhs, integrator, B_star, v)     theta_bar, = vjp_rhs(w)     return jnp.zeros_like(B_star), None, theta_bar fixed_point_finder.defvjp(fixed_point_finder_fwd, fixed_point_finder_bwd) def loss_function(func, B0, z0, rhs, integrator):     return jnp.linalg.norm(fixed_point_finder(func, B0, z0, rhs, integrator)) def myfunc(x,y,z,a):   return x def myint(x):   return x grad_func = jax.grad(loss_function, argnums=3) grad_func(myfunc, jnp.zeros((4)), jnp.zeros((4)), jnp.ones((4,5)), myint) ```","Thanks for adding the more complete code. The issue is that your `fwd` function should not be returning nondifferentiable arguments in its result. If you update your functions this way, it should work: ```python def fixed_point_finder_fwd(func, B0, z0, rhs, integrator):     B_star = fixed_point_finder(func, B0, z0, rhs, integrator)     return B_star, (B_star, z0, rhs)   DIFFERENT def fixed_point_finder_bwd(func, integrator, res, v):     B_star, z0, rhs = res   DIFFERENT     _, vjp_rhs = jax.vjp(lambda rhs: func(B_star, z0, rhs, integrator), rhs)     w = inner_fixed_point(func, z0, rhs, integrator, B_star, v)     theta_bar, = vjp_rhs(w)     return jnp.zeros_like(B_star), None, theta_bar ``` You can find more information on `custom_vjp` with `nondiff_argnums` here: https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.htmljaxcustomvjpwithnondiffargnums Regarding debugging... I'm not sure what to suggest, besides doublechecking that you're passing arguments correctly in `custom_vjp`. The error made me think that you were passing a python function where an array was expected, and that turned out to be the fix.","Wow, it worked! I completely missed that part of the tutorial. Apart from suggesting a more targeted error message for less sharp and experienced eyes like mine, I don't see what can be improved here. :) Thanks again for the debugging tips."
5813,"以下是一个github上的jax下的一个issue, 标题是(XlaRuntimeError: INTERNAL: custom_partitioner: NameError: name 'fft' is not defined)， 内容是 ( Description This issue is loosely related to CC(sharded fft unnecessarily loads entire array) and CC(Sharded FFT with JIT gives incorrect results on GPU since 0.4.9) in that I am trying to run a threedimensional FFT on a sharded threedimensional grid. However, the bug(?) can be demonstrated from the code in the documentation on `custom_partitioning`. The gist of the bug is that `fft` has to be explicitly imported via `from jax.numpy.fft import fft` when using the `custom_partitioning` example from the documentation. It is not enough to do `import jax.numpy as jnp` and then calling `jnp.fft.fft`. Code that produces the error: ```python import os import math device_shape = (8,) os.environ[     ""XLA_FLAGS"" ] = f""xla_force_host_platform_device_count={math.prod(device_shape)}"" import jax import jax.numpy as jnp from jax.sharding import NamedSharding from jax.experimental.custom_partitioning import custom_partitioning from jax.experimental.pjit import pjit from jax.sharding import PartitionSpec as P from jax.sharding import Mesh  Note: Uncommenting the explicit import of fft fixes the bug  from jax.numpy.fft import fft import re import numpy as np  For an ND input, keeps sharding along the first N1 dimensions  but replicate along the last dimension def supported_sharding(sharding, shape):     rank = len(shape.shape)     max_shared_dims = min(len(sharding.spec), rank  1)     names = tuple(sharding.spec[:max_shared_dims]) + tuple(         None for _ in range(rank  max_shared_dims)     )     return NamedSharding(sharding.mesh, P(*names)) def partition(mesh, arg_shapes, result_shape):     result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return (         mesh,         fft,         supported_sharding(arg_shardings[0], arg_shapes[0]),         (supported_sharding(arg_shardings[0], arg_shapes[0]),),     ) def infer_sharding_from_operands(mesh, arg_shapes, result_shape):     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return supported_sharding(arg_shardings[0], arg_shapes[0])  def my_fft(x):      Note: Using jnp.fft.fft instead of fft      return fft(x)     return jnp.fft.fft(x) my_fft.def_partition(     infer_sharding_from_operands=infer_sharding_from_operands, partition=partition ) with Mesh(np.array(jax.devices()), (""x"",)):     x = np.asarray(np.random.randn(32 * 1024, 1024), dtype=np.complex64)     y = pjit(lambda x: x, in_shardings=None, out_shardings=P(""x""))(x)     pjit_my_fft = pjit(my_fft, in_shardings=P(""x""), out_shardings=P(""x""))     print(jnp.allclose(np.fft.fft(x), pjit_my_fft(y), atol=1e4)) ``` Running this script gives the error: ``` WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1695387651.779556   29089 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created. No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Traceback (most recent call last):   File ""/repo/bugdemo.py"", line 63, in      print(jnp.allclose(np.fft.fft(x), pjit_my_fft(y), atol=1e4))                                       ^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: custom_partitioner: NameError: name 'fft' is not defined At:   /repo/bugdemo.py(37): partition   /repo/venv/lib/python3.11/sitepackages/jax/experimental/custom_partitioning.py(148): _custom_partitioning_partition   /repo/venv/lib/python3.11/sitepackages/jax/_src/compiler.py(251): backend_compile   /repo/venv/lib/python3.11/sitepackages/jax/_src/profiler.py(314): wrapper   /repo/venv/lib/python3.11/sitepackages/jax/_src/compiler.py(289): compile_or_get_cached   /repo/venv/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py(2518): _cached_compilation   /repo/venv/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py(2611): from_hlo   /repo/venv/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py(2263): compile   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(1134): _pjit_call_impl_python   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(1198): call_impl_cache_miss   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(1214): _pjit_call_impl   /repo/venv/lib/python3.11/sitepackages/jax/_src/core.py(821): process_primitive   /repo/venv/lib/python3.11/sitepackages/jax/_src/core.py(389): bind_with_trace   /repo/venv/lib/python3.11/sitepackages/jax/_src/core.py(2604): bind   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(166): _python_pjit_helper   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(255): cache_miss   /repo/venv/lib/python3.11/sitepackages/jax/_src/traceback_util.py(177): reraise_with_filtered_traceback   /repo/bugdemo.py(63):   For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ``` By uncommenting the line `from jax.numpy.fft import fft` the error disappears. The code above is slightly altered from the code in the documentation. The changes are:  No import of `regex`.  Adding `import jax.numpy as jnp`, and replacing `fft` > `jnp.fft.fft`.  Emulating 8 devices.  No output of the arrays nor checking of the HLO as done in the documentation. I thought maybe that the commit baab7b1 addressed this issue, but when trying out the same changes locally I still got the same error.  What jax/jaxlib version are you using? jax v0.4.16, jaxlib v0.4.16  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XlaRuntimeError: INTERNAL: custom_partitioner: NameError: name 'fft' is not defined," Description This issue is loosely related to CC(sharded fft unnecessarily loads entire array) and CC(Sharded FFT with JIT gives incorrect results on GPU since 0.4.9) in that I am trying to run a threedimensional FFT on a sharded threedimensional grid. However, the bug(?) can be demonstrated from the code in the documentation on `custom_partitioning`. The gist of the bug is that `fft` has to be explicitly imported via `from jax.numpy.fft import fft` when using the `custom_partitioning` example from the documentation. It is not enough to do `import jax.numpy as jnp` and then calling `jnp.fft.fft`. Code that produces the error: ```python import os import math device_shape = (8,) os.environ[     ""XLA_FLAGS"" ] = f""xla_force_host_platform_device_count={math.prod(device_shape)}"" import jax import jax.numpy as jnp from jax.sharding import NamedSharding from jax.experimental.custom_partitioning import custom_partitioning from jax.experimental.pjit import pjit from jax.sharding import PartitionSpec as P from jax.sharding import Mesh  Note: Uncommenting the explicit import of fft fixes the bug  from jax.numpy.fft import fft import re import numpy as np  For an ND input, keeps sharding along the first N1 dimensions  but replicate along the last dimension def supported_sharding(sharding, shape):     rank = len(shape.shape)     max_shared_dims = min(len(sharding.spec), rank  1)     names = tuple(sharding.spec[:max_shared_dims]) + tuple(         None for _ in range(rank  max_shared_dims)     )     return NamedSharding(sharding.mesh, P(*names)) def partition(mesh, arg_shapes, result_shape):     result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return (         mesh,         fft,         supported_sharding(arg_shardings[0], arg_shapes[0]),         (supported_sharding(arg_shardings[0], arg_shapes[0]),),     ) def infer_sharding_from_operands(mesh, arg_shapes, result_shape):     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return supported_sharding(arg_shardings[0], arg_shapes[0])  def my_fft(x):      Note: Using jnp.fft.fft instead of fft      return fft(x)     return jnp.fft.fft(x) my_fft.def_partition(     infer_sharding_from_operands=infer_sharding_from_operands, partition=partition ) with Mesh(np.array(jax.devices()), (""x"",)):     x = np.asarray(np.random.randn(32 * 1024, 1024), dtype=np.complex64)     y = pjit(lambda x: x, in_shardings=None, out_shardings=P(""x""))(x)     pjit_my_fft = pjit(my_fft, in_shardings=P(""x""), out_shardings=P(""x""))     print(jnp.allclose(np.fft.fft(x), pjit_my_fft(y), atol=1e4)) ``` Running this script gives the error: ``` WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1695387651.779556   29089 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created. No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Traceback (most recent call last):   File ""/repo/bugdemo.py"", line 63, in      print(jnp.allclose(np.fft.fft(x), pjit_my_fft(y), atol=1e4))                                       ^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: INTERNAL: custom_partitioner: NameError: name 'fft' is not defined At:   /repo/bugdemo.py(37): partition   /repo/venv/lib/python3.11/sitepackages/jax/experimental/custom_partitioning.py(148): _custom_partitioning_partition   /repo/venv/lib/python3.11/sitepackages/jax/_src/compiler.py(251): backend_compile   /repo/venv/lib/python3.11/sitepackages/jax/_src/profiler.py(314): wrapper   /repo/venv/lib/python3.11/sitepackages/jax/_src/compiler.py(289): compile_or_get_cached   /repo/venv/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py(2518): _cached_compilation   /repo/venv/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py(2611): from_hlo   /repo/venv/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py(2263): compile   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(1134): _pjit_call_impl_python   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(1198): call_impl_cache_miss   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(1214): _pjit_call_impl   /repo/venv/lib/python3.11/sitepackages/jax/_src/core.py(821): process_primitive   /repo/venv/lib/python3.11/sitepackages/jax/_src/core.py(389): bind_with_trace   /repo/venv/lib/python3.11/sitepackages/jax/_src/core.py(2604): bind   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(166): _python_pjit_helper   /repo/venv/lib/python3.11/sitepackages/jax/_src/pjit.py(255): cache_miss   /repo/venv/lib/python3.11/sitepackages/jax/_src/traceback_util.py(177): reraise_with_filtered_traceback   /repo/bugdemo.py(63):   For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. ``` By uncommenting the line `from jax.numpy.fft import fft` the error disappears. The code above is slightly altered from the code in the documentation. The changes are:  No import of `regex`.  Adding `import jax.numpy as jnp`, and replacing `fft` > `jnp.fft.fft`.  Emulating 8 devices.  No output of the arrays nor checking of the HLO as done in the documentation. I thought maybe that the commit baab7b1 addressed this issue, but when trying out the same changes locally I still got the same error.  What jax/jaxlib version are you using? jax v0.4.16, jaxlib v0.4.16  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, Linux  NVIDIA GPU info _No response_",2023-09-22T13:11:30Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17735,"Hi, I'm not sure I understand this issue and I never came across this in my FFT tests. Isn't the issue just that in this code block: ```python def partition(mesh, arg_shapes, result_shape):     result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return (         mesh,         fft,         supported_sharding(arg_shardings[0], arg_shapes[0]),         (supported_sharding(arg_shardings[0], arg_shapes[0]),),     ) ``` you are specifying a `fft` function/variable that is nowhere defined? If you would instead specify `mesh, jnp.fft.fft, ...` (so the same function you call inside `my_fft`) then I think the code should work as expected.","> I am trying to run a threedimensional FFT on a sharded threedimensional grid You might already know this, but keep in mind that the example code in jax.experimental.custom_partitioning runs a 1DFFT on a threedimensional grid. For a 3dFFT (`np.fft.fftn`) you would then afterwards need to run another FFT along the other two axis.","Oh, whoopie, you are absolutely right! I was so focused on the `my_fft`part that I overlooked the rest of the example. Thanks a lot! :)"
1014,"以下是一个github上的jax下的一个issue, 标题是(XLA Check failed: common_utilization <= producer_output_utilization)， 内容是 ( Description When trying to run a longer algorithm, the execution fails with an error message without a more precise indication of where in the code the issue occurred: ``` F external/xla/xla/service/gpu/gpu_performance_model.cc:119] Check failed: common_utilization <= producer_output_utilization (500.867 vs. 500.867) ``` This error only occurs in some use cases of the algorithm, and slightly changing parameters such as iterations and batch size sometimes permit it.   What jax/jaxlib version are you using? jax v0.4.9, jaxlib v0.4.9+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info python 3.8  NVIDIA GPU info ``` ++ +++ ``` I am running the code with disabled parallel compilation: ``` TF_USE_NVLINK_FOR_PARALLEL_COMPILATION=0 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XLA Check failed: common_utilization <= producer_output_utilization," Description When trying to run a longer algorithm, the execution fails with an error message without a more precise indication of where in the code the issue occurred: ``` F external/xla/xla/service/gpu/gpu_performance_model.cc:119] Check failed: common_utilization <= producer_output_utilization (500.867 vs. 500.867) ``` This error only occurs in some use cases of the algorithm, and slightly changing parameters such as iterations and batch size sometimes permit it.   What jax/jaxlib version are you using? jax v0.4.9, jaxlib v0.4.9+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info python 3.8  NVIDIA GPU info ``` ++ +++ ``` I am running the code with disabled parallel compilation: ``` TF_USE_NVLINK_FOR_PARALLEL_COMPILATION=0 ```",2023-09-22T06:22:33Z,bug XLA needs info GPU,open,0,9,https://github.com/jax-ml/jax/issues/17730,"Well, that sounds like an XLA bug. First, can you try with the latest jaxlib release (0.4.16)? The bug may already be fixed, so this is the first thing to try. You will need to update your Python version to 3.9 or newer to do this. If that doesn't work, can you please provide instructions to reproduce? If it's hard to do that, one way is to provide an HLO dump from XLA, which you can get by setting `XLA_FLAGS=xla_dump_to=/somewhere` and `JAX_COMPILER_DETAILED_LOGGING_MIN_OPS=0`, running your script, and zip up and attach the output of `/somewhere` to this issue.",Any updates? Can you share instructions to reproduce?,"I upgraded the jax and jaxlib versions, but the error persists. Unfortunately, I could not track the error to a specific part of the code.  However, I did the steps you described and attached the dump. xla_dump_part_1.tar.gz xla_dump_part_2.tar.gz","Hi Peter Hawkins, Do you have any updates on this issue? Best Sebastien","I've run into the same error  Description ``` (jax) bash4.2$ JAX_COMPILER_DETAILED_LOGGING_MIN_OPS=0 XLA_FLAGS=xla_dump_to=/tmp/xladump PYTHONPATH=""./"" python tests/tests.py .F1127 20:16:06.642083   26806 gpu_performance_model.cc:358] Check failed: common_utilization ()     @     0x7f901da2e8e7  xla::HloPassPipeline::Run()     @     0x7f9018e54c71  xla::HloPassInterface::Run()     @     0x7f9018e6a5dd  xla::gpu::GpuCompiler::OptimizeHloModule()     @     0x7f9018e6ee61  xla::gpu::GpuCompiler::RunHloPasses()     @     0x7f9018d8d3a9  xla::Service::BuildExecutable()     @     0x7f9018b472ad  xla::LocalService::CompileExecutables()     @     0x7f9018b41f82  xla::LocalClient::Compile()     @     0x7f9018b00d7c  xla::PjRtStreamExecutorClient::Compile()     @     0x7f9018adcc9f  xla::StreamExecutorGpuClient::Compile()     @     0x7f9018b1400a  xla::PjRtStreamExecutorClient::Compile()     @     0x7f9018a2e66f  xla::ifrt::PjRtLoadedExecutable::Create()     @     0x7f9018a24d14  xla::ifrt::PjRtCompiler::Compile()     @     0x7f9017fa9a52  xla::PyClient::Compile()     @     0x7f9017cdb0b3  pybind11::detail::argument_loader::call_impl()     @     0x7f9017cdb560  pybind11::cpp_function::initialize()::{lambda() CC(Undefined name: from ..core import JaxTuple)}::_FUN()     @     0x7f9017c90768  pybind11::cpp_function::dispatcher()     @           0x525d17  cfunction_call Aborted (core dumped) ``` The code in question is quite heavy in integer arithmetic, which may be part of the problem. You can find it here xladump.tar.gz  Jax version Jax/Jaxlib: `jax0.4.20 jaxlib0.4.20+cuda12.cudnn89`  System info Using GPU on an RTX 4090. My code runs successfully on CPU. Python 3.11.4, installed through anaconda ``` ++ +++ ```",Same problem here. Running on GPU. Any solutions ? Depends on the hyper parameters. ,EDIT: Originally thought I had the same problem but it looks like I'm failing a different XLA check so opened a separate issue. https://github.com/google/jax/issues/20024,Any updates on this?  Running into a similar error on A6000 GPU: F external/xla/xla/service/gpu/model/gpu_performance_model.cc:540] Check failed: common_utilization ()     @     0x7fd84a1e88e7  xla::HloPassPipeline::Run()     @     0x7fd84560ec71  xla::HloPassInterface::Run()     @     0x7fd8456245dd  xla::gpu::GpuCompiler::OptimizeHloModule()     @     0x7fd845628e61  xla::gpu::GpuCompiler::RunHloPasses()     @     0x7fd8455473a9  xla::Service::BuildExecutable()     @     0x7fd8453012ad  xla::LocalService::CompileExecutables()     @     0x7fd8452fbf82  xla::LocalClient::Compile()     @     0x7fd8452bad7c  xla::PjRtStreamExecutorClient::Compile()     @     0x7fd845296c9f  xla::StreamExecutorGpuClient::Compile()     @     0x7fd8452ce00a  xla::PjRtStreamExecutorClient::Compile()     @     0x7fd8451e866f  xla::ifrt::PjRtLoadedExecutable::Create()     @     0x7fd8451ded14  xla::ifrt::PjRtCompiler::Compile()     @     0x7fd844763a52  xla::PyClient::Compile()     @     0x7fd8444950b3  pybind11::detail::argument_loader::call_impl()     @     0x7fd844495560  pybind11::cpp_function::initialize()::{lambda() CC(Undefined name: from ..core import JaxTuple)}::_FUN()     @     0x7fd84444a768  pybind11::cpp_function::dispatcher()     @           0x525d17  cfunction_call,"Same error with `jax` and `jaxlib` version 0.4.28. I found that replacing one line in my code ``` return 0.5**(epoch / period) if period else 1 ``` with this ``` return jax.numpy.where(period, 0.5**(epoch / period), 1) ``` fixes the error. EDIT: Actually the error is intermittent, it sometimes appears even with the ""fix""."
501,"以下是一个github上的jax下的一个issue, 标题是(If an input to `jnp.asarray` is a numpy array, then convert it to a jax.Array via device_put to avoid a copy.)， 内容是 (If an input to `jnp.asarray` is a numpy array, then convert it to a jax.Array via device_put to avoid a copy. Do a similar thing for jax.Array too if dtypes match. Fixes https://github.com/google/jax/issues/17702)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"If an input to `jnp.asarray` is a numpy array, then convert it to a jax.Array via device_put to avoid a copy.","If an input to `jnp.asarray` is a numpy array, then convert it to a jax.Array via device_put to avoid a copy. Do a similar thing for jax.Array too if dtypes match. Fixes https://github.com/google/jax/issues/17702",2023-09-21T21:42:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17721
3077,"以下是一个github上的jax下的一个issue, 标题是(PyInstaller package jax error)， 内容是 ( Description My jax code like this, when I use PyInstaller package it, it will be ok.  ```python  import jax.numpy as jnp a = jnp.ones(10) b=a+2 print(b) ``` But when I run the packaged code ,it return some error, how can I solve it? ```  $ ./test Traceback (most recent call last):   File ""jax/_src/interpreters/mlir.py"", line 711, in lower_jaxpr_to_module jaxlib.mlir._mlir_libs._site_initialize..MLIRError: Verification failed: error: unknown: 'func.return' op created with unregistered dialect. If this is intended, please call allowUnregisteredDialects() on the MLIRContext, or use allowunregistereddialect with the MLIR opt tool used  note: unknown: see current operation: ""func.return""(%0) : (tensor) > () The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test.py"", line 3, in    File ""jax/_src/numpy/lax_numpy.py"", line 2161, in ones   File ""jax/_src/lax/lax.py"", line 1206, in full   File ""jax/_src/lax/lax.py"", line 768, in broadcast   File ""jax/_src/lax/lax.py"", line 797, in broadcast_in_dim   File ""jax/_src/core.py"", line 380, in bind   File ""jax/_src/core.py"", line 383, in bind_with_trace   File ""jax/_src/core.py"", line 815, in process_primitive   File ""jax/_src/dispatch.py"", line 132, in apply_primitive   File ""jax/_src/util.py"", line 284, in wrapper   File ""jax/_src/util.py"", line 277, in cached   File ""jax/_src/dispatch.py"", line 223, in xla_primitive_callable   File ""jax/_src/dispatch.py"", line 250, in _xla_callable_uncached   File ""jax/_src/dispatch.py"", line 242, in sharded_lowering   File ""jax/_src/profiler.py"", line 314, in wrapper   File ""jax/_src/interpreters/pxla.py"", line 2085, in lower_sharding_computation   File ""jax/_src/interpreters/pxla.py"", line 1928, in _cached_lowering_to_hlo   File ""jax/_src/interpreters/mlir.py"", line 717, in lower_jaxpr_to_module ValueError: Cannot lower jaxpr with verifier errors: loc = loc(unknown) ""builtin.module""()  ({   ""func.func""() ({   ^bb0(%arg0: tensor loc(unknown)):     %0 = ""stablehlo.broadcast_in_dim""(%arg0) {broadcast_dimensions = dense : tensor} : (tensor) > tensor loc(loc2)     ""func.return""(%0) : (tensor) > () loc(loc)   }) {arg_attrs = [{mhlo.sharding = ""{replicated}""}], function_type = (tensor) > tensor, res_attrs = [{}], sym_name = ""main"", sym_visibility = ""public""} : () > () loc(loc) }) {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} : () > () loc(loc) loc1 = loc(""jax/_src/source_info_util.py"":189:0) loc2 = loc(""jit(broadcast_in_dim)/jit(main)/broadcast_in_dim[shape=(10,) broadcast_dimensions=()]""(loc1)) [83224] Failed to execute script 'test' due to unhandled exception! ```  What jax/jaxlib version are you using? jax v0.4.13 jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python=3.10.12 Ubuntu 22.04  NVIDIA GPU info ``` Thu Sep 21 21:20:53 2023 ++  ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,PyInstaller package jax error," Description My jax code like this, when I use PyInstaller package it, it will be ok.  ```python  import jax.numpy as jnp a = jnp.ones(10) b=a+2 print(b) ``` But when I run the packaged code ,it return some error, how can I solve it? ```  $ ./test Traceback (most recent call last):   File ""jax/_src/interpreters/mlir.py"", line 711, in lower_jaxpr_to_module jaxlib.mlir._mlir_libs._site_initialize..MLIRError: Verification failed: error: unknown: 'func.return' op created with unregistered dialect. If this is intended, please call allowUnregisteredDialects() on the MLIRContext, or use allowunregistereddialect with the MLIR opt tool used  note: unknown: see current operation: ""func.return""(%0) : (tensor) > () The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test.py"", line 3, in    File ""jax/_src/numpy/lax_numpy.py"", line 2161, in ones   File ""jax/_src/lax/lax.py"", line 1206, in full   File ""jax/_src/lax/lax.py"", line 768, in broadcast   File ""jax/_src/lax/lax.py"", line 797, in broadcast_in_dim   File ""jax/_src/core.py"", line 380, in bind   File ""jax/_src/core.py"", line 383, in bind_with_trace   File ""jax/_src/core.py"", line 815, in process_primitive   File ""jax/_src/dispatch.py"", line 132, in apply_primitive   File ""jax/_src/util.py"", line 284, in wrapper   File ""jax/_src/util.py"", line 277, in cached   File ""jax/_src/dispatch.py"", line 223, in xla_primitive_callable   File ""jax/_src/dispatch.py"", line 250, in _xla_callable_uncached   File ""jax/_src/dispatch.py"", line 242, in sharded_lowering   File ""jax/_src/profiler.py"", line 314, in wrapper   File ""jax/_src/interpreters/pxla.py"", line 2085, in lower_sharding_computation   File ""jax/_src/interpreters/pxla.py"", line 1928, in _cached_lowering_to_hlo   File ""jax/_src/interpreters/mlir.py"", line 717, in lower_jaxpr_to_module ValueError: Cannot lower jaxpr with verifier errors: loc = loc(unknown) ""builtin.module""()  ({   ""func.func""() ({   ^bb0(%arg0: tensor loc(unknown)):     %0 = ""stablehlo.broadcast_in_dim""(%arg0) {broadcast_dimensions = dense : tensor} : (tensor) > tensor loc(loc2)     ""func.return""(%0) : (tensor) > () loc(loc)   }) {arg_attrs = [{mhlo.sharding = ""{replicated}""}], function_type = (tensor) > tensor, res_attrs = [{}], sym_name = ""main"", sym_visibility = ""public""} : () > () loc(loc) }) {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} : () > () loc(loc) loc1 = loc(""jax/_src/source_info_util.py"":189:0) loc2 = loc(""jit(broadcast_in_dim)/jit(main)/broadcast_in_dim[shape=(10,) broadcast_dimensions=()]""(loc1)) [83224] Failed to execute script 'test' due to unhandled exception! ```  What jax/jaxlib version are you using? jax v0.4.13 jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python=3.10.12 Ubuntu 22.04  NVIDIA GPU info ``` Thu Sep 21 21:20:53 2023 ++  ```",2023-09-21T13:21:12Z,bug contributions welcome P3 (no schedule),open,0,8,https://github.com/jax-ml/jax/issues/17705,"This sounds like a broken installation. For example, I can imagine this would happen if the MLIR initialization did not run JAX's site initialization (https://github.com/google/jax/blob/main/jaxlib/mlir/_mlir_libs/_site_initialize_0.cc), which might happen if for some reason that module could not be found. I don't think this is a bug in JAX: JAX when installed as intended works fine. This sounds like PyInstaller breaks the JAX installation when it packages up an app. PRs welcome, if there's something we can do on the JAX end to make PyInstaller work, but I don't think there's an action item for us here.","> This sounds like a broken installation. For example, I can imagine this would happen if the MLIR initialization did not run JAX's site initialization (https://github.com/google/jax/blob/main/jaxlib/mlir/_mlir_libs/_site_initialize_0.cc), which might happen if for some reason that module could not be found. >  > I don't think this is a bug in JAX: JAX when installed as intended works fine. This sounds like PyInstaller breaks the JAX installation when it packages up an app. PRs welcome, if there's something we can do on the JAX end to make PyInstaller work, but I don't think there's an action item for us here. Thank you for your reply. I installed jax through pip. I would also like to ask if there is any good solution to this problem. I look forward to your reply."," The problem is likely not your JAX installation itself, it's that pyinstaller doesn't successfully package up JAX into a package. I have no knowledge of pyinstaller and I'm not sure what to suggest. Contributions welcome!","Hey , did you find a solution?",  I was able to get around this by using `py2app`,">  I was able to get around this by using `py2app` I commented out the MLIR detection code in the error reporting part, and the program can still run normally.  Although it solves the problem, I don't think it's a good solution. https://github.com/google/jax/blob/2be6019f1c99b234b91bf736578cd3d6886a6f18/jax/_src/interpreters/mlir.pyL823C5L823C5   823839",lol fair enough :) with `py2app` I could get JAX to run but I am having issues getting `py2app` to find `jaxmetal`. Looks like neither solution is perfect so far. But thanks for sharing ! Hope you have a good week!,"Realised that `py2app` is only really viable for Mac users, I also tested pyinstaller on linux and found that commenting out that verification code in the mlir file to work as well lol (tested for CUDA backend)"
958,"以下是一个github上的jax下的一个issue, 标题是(jnp.asarray making copies in latest jax)， 内容是 ( Description ```python  import numpy as np import jax.numpy as jnp import os, psutil process = psutil.Process(os.getpid()) print(process.memory_info().rss / 1e9)   0.1 GB x = np.random.uniform(0, 1, (10000, 20000)).astype(""float32"") print(process.memory_info().rss / 1e9)   0.9 GB y = jnp.asarray(x) print(process.memory_info().rss / 1e9)   1.7 GB ``` In version 0.4.16, `jnp.asarray` now seems to be making copies when called on numpy arrays. In earlier versions, it would just reuse the underlying data and no (little) extra memory would be consumed. Is this intended?  What jax/jaxlib version are you using? jax, jaxlib 0.4.16  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jnp.asarray making copies in latest jax," Description ```python  import numpy as np import jax.numpy as jnp import os, psutil process = psutil.Process(os.getpid()) print(process.memory_info().rss / 1e9)   0.1 GB x = np.random.uniform(0, 1, (10000, 20000)).astype(""float32"") print(process.memory_info().rss / 1e9)   0.9 GB y = jnp.asarray(x) print(process.memory_info().rss / 1e9)   1.7 GB ``` In version 0.4.16, `jnp.asarray` now seems to be making copies when called on numpy arrays. In earlier versions, it would just reuse the underlying data and no (little) extra memory would be consumed. Is this intended?  What jax/jaxlib version are you using? jax, jaxlib 0.4.16  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-09-21T09:52:36Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/17702,"Yash, is it possible that this occurs because of the changes to the primitive dispatch path and the fact that we now copy for trivial computations?","Yeah, that sounds correct to me. We removed the trivial dispatch path from JAX because it was a preominstaging optimization which doesn't have much use postomnistaging. Also it helped speed up the dispatch path by 100x. Is the copy causing problems?",I think not copying (while it has only ever been best effort) is an important optimization.,https://github.com/google/jax/pull/17721 should fix,Thanks guys! before the next release is there another way of converting without copies?,"If you want to convert a numpy array to a jax array without copying in version 0.4.16, you can use `y = jax.device_put(x)` This will only be avoid copies if using a CPU device, and if your numpy array's byte alignment is compatible with XLA."
4039,"以下是一个github上的jax下的一个issue, 标题是(Copying a pmap replicated array gives an error.)， 内容是 ( Description ```python import os os.environ['XLA_FLAGS'] = (       f'{os.environ.get(""XLA_FLAGS"", """")} '       'xla_force_host_platform_device_count=4'   ) import jax import jax.numpy as jnp x = jax.pmap(lambda x: x, in_axes=0, out_axes=None)(jnp.arange(jax.device_count())) jnp.array(x) ``` gives an error ```  ValueError                                Traceback (most recent call last)  in ()       9       10 x = jax.pmap(lambda x: x, in_axes=0, out_axes=None)(jnp.arange(jax.device_count())) > 11 jnp.array(x) 8 frames /usr/local/lib/python3.10/distpackages/jax/_src/numpy/lax_numpy.py in array(object, dtype, copy, order, ndmin)    2052   elif isinstance(object, Array):    2053     assert object.aval is not None > 2054     out = _array_copy(object) if copy else object    2055   elif isinstance(object, (list, tuple)):    2056     if object: /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in _array_copy(arr)    4487     4488 def _array_copy(arr: ArrayLike) > Array: > 4489   return copy_p.bind(arr)    4490     4491  /usr/local/lib/python3.10/distpackages/jax/_src/core.py in bind(self, *args, **params)     384     assert (not config.jax_enable_checks or     385             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 386     return self.bind_with_trace(find_top_trace(args), args, params)     387      388   def bind_with_trace(self, trace, args, params): /usr/local/lib/python3.10/distpackages/jax/_src/core.py in bind_with_trace(self, trace, args, params)     387      388   def bind_with_trace(self, trace, args, params): > 389     out = trace.process_primitive(self, map(trace.full_raise, args), params)     390     return map(full_lower, out) if self.multiple_results else full_lower(out)     391  /usr/local/lib/python3.10/distpackages/jax/_src/core.py in process_primitive(self, primitive, tracers, params)     819      820   def process_primitive(self, primitive, tracers, params): > 821     return primitive.impl(*tracers, **params)     822      823   def process_call(self, primitive, f, tracers, params): /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in _copy_impl(prim, *args, **kwargs)    4525   if isinstance(a, jax.Array) and isinstance(a.sharding, PmapSharding):    4526     sharded_dim = _which_dim_sharded(a.sharding) > 4527     return _copy_impl_pmap_sharding(sharded_dim, *args, **kwargs)    4528   return dispatch.apply_primitive(prim, *args, **kwargs)    4529  /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in _copy_impl_pmap_sharding(sharded_dim, *args, **kwargs)    4505   axis_name, static_broadcasted_tuple, donate_tuple = api._shared_code_pmap(    4506     _identity_fn, None, (), (), sharded_dim, sharded_dim) > 4507   p = api._prepare_pmap(    4508       _identity_fn, sharded_dim, sharded_dim, static_broadcasted_tuple,    4509       donate_tuple, None, None, None, args, kwargs) /usr/local/lib/python3.10/distpackages/jax/_src/api.py in _prepare_pmap(fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple, in_devices, backend_name, axis_size, args, kwargs)    1694             ""the pmapped function."")    1695     raise ValueError(msg) from None > 1696   local_axis_size = _mapped_axis_size(fun, in_tree, args, in_axes_flat, ""pmap"")    1697     1698   f, res_paths = result_paths(f) /usr/local/lib/python3.10/distpackages/jax/_src/api.py in _mapped_axis_size(fn, tree, vals, dims, name)    1292   if not sizes:    1293     msg = f""{name} must have at least one nonNone value in in_axes"" > 1294     raise ValueError(msg)    1295     1296   def _get_argument_type(x): ValueError: pmap must have at least one nonNone value in in_axes ```  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Copying a pmap replicated array gives an error.," Description ```python import os os.environ['XLA_FLAGS'] = (       f'{os.environ.get(""XLA_FLAGS"", """")} '       'xla_force_host_platform_device_count=4'   ) import jax import jax.numpy as jnp x = jax.pmap(lambda x: x, in_axes=0, out_axes=None)(jnp.arange(jax.device_count())) jnp.array(x) ``` gives an error ```  ValueError                                Traceback (most recent call last)  in ()       9       10 x = jax.pmap(lambda x: x, in_axes=0, out_axes=None)(jnp.arange(jax.device_count())) > 11 jnp.array(x) 8 frames /usr/local/lib/python3.10/distpackages/jax/_src/numpy/lax_numpy.py in array(object, dtype, copy, order, ndmin)    2052   elif isinstance(object, Array):    2053     assert object.aval is not None > 2054     out = _array_copy(object) if copy else object    2055   elif isinstance(object, (list, tuple)):    2056     if object: /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in _array_copy(arr)    4487     4488 def _array_copy(arr: ArrayLike) > Array: > 4489   return copy_p.bind(arr)    4490     4491  /usr/local/lib/python3.10/distpackages/jax/_src/core.py in bind(self, *args, **params)     384     assert (not config.jax_enable_checks or     385             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 386     return self.bind_with_trace(find_top_trace(args), args, params)     387      388   def bind_with_trace(self, trace, args, params): /usr/local/lib/python3.10/distpackages/jax/_src/core.py in bind_with_trace(self, trace, args, params)     387      388   def bind_with_trace(self, trace, args, params): > 389     out = trace.process_primitive(self, map(trace.full_raise, args), params)     390     return map(full_lower, out) if self.multiple_results else full_lower(out)     391  /usr/local/lib/python3.10/distpackages/jax/_src/core.py in process_primitive(self, primitive, tracers, params)     819      820   def process_primitive(self, primitive, tracers, params): > 821     return primitive.impl(*tracers, **params)     822      823   def process_call(self, primitive, f, tracers, params): /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in _copy_impl(prim, *args, **kwargs)    4525   if isinstance(a, jax.Array) and isinstance(a.sharding, PmapSharding):    4526     sharded_dim = _which_dim_sharded(a.sharding) > 4527     return _copy_impl_pmap_sharding(sharded_dim, *args, **kwargs)    4528   return dispatch.apply_primitive(prim, *args, **kwargs)    4529  /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in _copy_impl_pmap_sharding(sharded_dim, *args, **kwargs)    4505   axis_name, static_broadcasted_tuple, donate_tuple = api._shared_code_pmap(    4506     _identity_fn, None, (), (), sharded_dim, sharded_dim) > 4507   p = api._prepare_pmap(    4508       _identity_fn, sharded_dim, sharded_dim, static_broadcasted_tuple,    4509       donate_tuple, None, None, None, args, kwargs) /usr/local/lib/python3.10/distpackages/jax/_src/api.py in _prepare_pmap(fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple, in_devices, backend_name, axis_size, args, kwargs)    1694             ""the pmapped function."")    1695     raise ValueError(msg) from None > 1696   local_axis_size = _mapped_axis_size(fun, in_tree, args, in_axes_flat, ""pmap"")    1697     1698   f, res_paths = result_paths(f) /usr/local/lib/python3.10/distpackages/jax/_src/api.py in _mapped_axis_size(fn, tree, vals, dims, name)    1292   if not sizes:    1293     msg = f""{name} must have at least one nonNone value in in_axes"" > 1294     raise ValueError(msg)    1295     1296   def _get_argument_type(x): ValueError: pmap must have at least one nonNone value in in_axes ```  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-09-20T22:35:57Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/17690
389,"以下是一个github上的jax下的一个issue, 标题是(Deprecate non-array inputs to several jax.numpy functions)， 内容是 (Some stragglers from CC(Regularize jax.numpy API) I've modified `check_arraylike` to emit warnings so we can more gracefully deprecate the old behavior.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Deprecate non-array inputs to several jax.numpy functions,Some stragglers from CC(Regularize jax.numpy API) I've modified `check_arraylike` to emit warnings so we can more gracefully deprecate the old behavior.,2023-09-20T21:02:42Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17687
490,"以下是一个github上的jax下的一个issue, 标题是(Avoid references to symbols removed in numpy 2.0)， 内容是 (NumPy 2.0 will remove `np.int_`, `np.float_`, and `np.complex_` (see https://github.com/numpy/numpy/issues/24743) Some of these have already been removed upstream, which is causing failures in our upstreamnightly tests ( CC(⚠️ Nightly upstreamdev CI failed ⚠️)).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Avoid references to symbols removed in numpy 2.0,"NumPy 2.0 will remove `np.int_`, `np.float_`, and `np.complex_` (see https://github.com/numpy/numpy/issues/24743) Some of these have already been removed upstream, which is causing failures in our upstreamnightly tests ( CC(⚠️ Nightly upstreamdev CI failed ⚠️)).",2023-09-19T18:35:02Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17668
8081,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Tests are failing)， 内容是 ( Description ```bash $ cd /tmp $ git clone https://github.com/google/jax.git b jaxv0.4.16 $ pip install matplotlib flax jaxlib tensorflow_datasets tensorflow_serving_api tensorflow e jax $ python jax/jax/experimental/jax2tf/examples/saved_model_main_test.py ``` Output: ``` Running tests under Python 3.10.12: /Users/dion/.virtualenvs/tempenv6bbe5525767/bin/python [ RUN      ] SavedModelMainTest.test_train_and_save_features_mnist_flax I0919 14:49:21.999132 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1695127762.000160       1 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created. I0919 14:49:22.000297 8342069376 xla_bridge.py:513] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' I0919 14:49:22.000332 8342069376 xla_bridge.py:513] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' I0919 14:49:22.002579 8342069376 xla_bridge.py:513] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/homebrew/lib/libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file), '/usr/local/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache) [  FAILED  ] SavedModelMainTest.test_train_and_save_features_mnist_flax [ RUN      ] SavedModelMainTest.test_train_and_save_features_mnist_pure_jax I0919 14:49:22.003413 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_features_mnist_pure_jax [ RUN      ] SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 I0919 14:49:22.003628 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 [ RUN      ] SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 I0919 14:49:22.003809 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 [ RUN      ] SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 I0919 14:49:22.003971 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 [ RUN      ] SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 I0919 14:49:22.004137 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 ====================================================================== ERROR: test_train_and_save_features_mnist_flax (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_features_mnist_flax test_train_and_save_features_mnist_flax(model='mnist_flax')  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_features_mnist_pure_jax (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_features_mnist_pure_jax test_train_and_save_features_mnist_pure_jax(model='mnist_pure_jax')  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_full_mnist_flax_batch=1 (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 test_train_and_save_full_mnist_flax_batch=1(model='mnist_flax', serving_batch_size=1)  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_full_mnist_flax_batch=1 (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 test_train_and_save_full_mnist_flax_batch=1(model='mnist_flax', serving_batch_size=1)  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_full_mnist_pure_jax_batch=1 (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 test_train_and_save_full_mnist_pure_jax_batch=1(model='mnist_pure_jax', serving_batch_size=1)  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_full_mnist_pure_jax_batch=1 (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 test_train_and_save_full_mnist_pure_jax_batch=1(model='mnist_pure_jax', serving_batch_size=1)  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version'  Ran 6 tests in 0.006s FAILED (errors=6) I0000 00:00:1695127762.198402       1 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed. ```  What jax/jaxlib version are you using? jax v0.4.16 jaxlib v0.4.16  Which accelerator(s) are you using? CPU  Additional system info OSX  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Tests are failing," Description ```bash $ cd /tmp $ git clone https://github.com/google/jax.git b jaxv0.4.16 $ pip install matplotlib flax jaxlib tensorflow_datasets tensorflow_serving_api tensorflow e jax $ python jax/jax/experimental/jax2tf/examples/saved_model_main_test.py ``` Output: ``` Running tests under Python 3.10.12: /Users/dion/.virtualenvs/tempenv6bbe5525767/bin/python [ RUN      ] SavedModelMainTest.test_train_and_save_features_mnist_flax I0919 14:49:21.999132 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1695127762.000160       1 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created. I0919 14:49:22.000297 8342069376 xla_bridge.py:513] Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' I0919 14:49:22.000332 8342069376 xla_bridge.py:513] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' I0919 14:49:22.002579 8342069376 xla_bridge.py:513] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/homebrew/lib/libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file), '/usr/local/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache) [  FAILED  ] SavedModelMainTest.test_train_and_save_features_mnist_flax [ RUN      ] SavedModelMainTest.test_train_and_save_features_mnist_pure_jax I0919 14:49:22.003413 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_features_mnist_pure_jax [ RUN      ] SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 I0919 14:49:22.003628 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 [ RUN      ] SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 I0919 14:49:22.003809 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 [ RUN      ] SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 I0919 14:49:22.003971 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 [ RUN      ] SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 I0919 14:49:22.004137 8342069376 tf_test_util.py:172] Running jax2tf converted code on LogicalDevice(name='/device:CPU:0', device_type='CPU'). [  FAILED  ] SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 ====================================================================== ERROR: test_train_and_save_features_mnist_flax (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_features_mnist_flax test_train_and_save_features_mnist_flax(model='mnist_flax')  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_features_mnist_pure_jax (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_features_mnist_pure_jax test_train_and_save_features_mnist_pure_jax(model='mnist_pure_jax')  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_full_mnist_flax_batch=1 (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 test_train_and_save_full_mnist_flax_batch=1(model='mnist_flax', serving_batch_size=1)  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_full_mnist_flax_batch=1 (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_full_mnist_flax_batch=1 test_train_and_save_full_mnist_flax_batch=1(model='mnist_flax', serving_batch_size=1)  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_full_mnist_pure_jax_batch=1 (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 test_train_and_save_full_mnist_pure_jax_batch=1(model='mnist_pure_jax', serving_batch_size=1)  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version' ====================================================================== ERROR: test_train_and_save_full_mnist_pure_jax_batch=1 (__main__.SavedModelMainTest) SavedModelMainTest.test_train_and_save_full_mnist_pure_jax_batch=1 test_train_and_save_full_mnist_pure_jax_batch=1(model='mnist_pure_jax', serving_batch_size=1)  Traceback (most recent call last):   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/examples/saved_model_main_test.py"", line 33, in setUp     super().setUp()   File ""/Users/dion/codes/jax/jax/experimental/jax2tf/tests/tf_test_util.py"", line 186, in setUp     tfxla.call_module_maximum_supported_version()) AttributeError: module 'tensorflow.compiler.tf2xla.python.xla' has no attribute 'call_module_maximum_supported_version'  Ran 6 tests in 0.006s FAILED (errors=6) I0000 00:00:1695127762.198402       1 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed. ```  What jax/jaxlib version are you using? jax v0.4.16 jaxlib v0.4.16  Which accelerator(s) are you using? CPU  Additional system info OSX  NVIDIA GPU info _No response_",2023-09-19T12:51:37Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/17660,"Having a similar issue when trying to follow the README here: https://github.com/google/jax/tree/main/jax/experimental/jax2tf/examples/serving Getting this error running the python command `TypeError: call_module() got an unexpected keyword argument 'function_list'`  Full output: ```   File ""/Users/ako/work/jax/jax/experimental/jax2tf/examples/saved_model_main.py"", line 210, in      app.run(lambda _: train_and_save())   File ""/Users/ako/venv/lib/python3.10/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/Users/ako/venv/lib/python3.10/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/Users/ako/work/jax/jax/experimental/jax2tf/examples/saved_model_main.py"", line 210, in      app.run(lambda _: train_and_save())   File ""/Users/ako/work/jax/jax/experimental/jax2tf/examples/saved_model_main.py"", line 125, in train_and_save     saved_model_lib.convert_and_save_model(   File ""/Users/ako/work/jax/jax/experimental/jax2tf/examples/saved_model_lib.py"", line 115, in convert_and_save_model     tf_graph.get_concrete_function(input_signatures[0])   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1189, in get_concrete_function     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1169, in _get_concrete_function_garbage_collected     self._initialize(args, kwargs, add_initializers_to=initializers)   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 694, in _initialize     self._variable_creation_fn     pylint: disable=protectedaccess   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 176, in _get_concrete_function_internal_garbage_collected     concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 171, in _maybe_define_concrete_function     return self._maybe_define_function(args, kwargs)   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 398, in _maybe_define_function     concrete_function = self._create_concrete_function(   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 305, in _create_concrete_function     func_graph_module.func_graph_from_py_func(   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/framework/func_graph.py"", line 1055, in func_graph_from_py_func     func_outputs = python_func(*func_args, **func_kwargs)   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 597, in wrapped_fn     out = weak_wrapped_fn().__wrapped__(*args, **kwds)   File ""/Users/ako/work/jax/jax/experimental/jax2tf/examples/saved_model_lib.py"", line 108, in      tf_graph = tf.function(lambda inputs: tf_fn(param_vars, inputs),   File ""/Users/ako/work/jax/jax/experimental/jax2tf/jax2tf.py"", line 417, in converted_fun_tf     outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/ops/custom_gradient.py"", line 343, in __call__     return self._d(self._f, a, k)   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/ops/custom_gradient.py"", line 299, in decorated     return _graph_mode_decorator(wrapped, args, kwargs)   File ""/Users/ako/venv/lib/python3.10/sitepackages/tensorflow/python/ops/custom_gradient.py"", line 425, in _graph_mode_decorator     result, grad_fn = f(*args)   File ""/Users/ako/work/jax/jax/experimental/jax2tf/jax2tf.py"", line 409, in converted_fun_flat_with_custom_gradient_tf     outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)   File ""/Users/ako/work/jax/jax/experimental/jax2tf/jax2tf.py"", line 518, in run_fun_tf     results = _run_exported_as_tf(args_flat_tf, self.exported)   File ""/Users/ako/work/jax/jax/experimental/jax2tf/jax2tf.py"", line 877, in _run_exported_as_tf     res = tfxla.call_module(args_flat_tf, **call_module_attrs) TypeError: call_module() got an unexpected keyword argument 'function_list' ``` Using same jax version, CPU, Macbook M2","Just downgraded jax to v0.4.12/13/14 and it is passing on all of them (I started at a way too old version oops), but issue starts from `jax[cpu]==v0.4.15.dev20230919` ","In general, `jax2tf` tests require the nightly tensorflow release (`pip install tfnightly`). They often get out of sync with the tensorflow release.","Thanks  tried that, same result. ```bash $ pip freeze | grep ""tfnightly"" tfnightly==2.15.0.dev20230919 tfnightlymacos==2.15.0.dev20230919 ```","`call_module` accepts a `function_list` argument as of four months ago: https://github.com/tensorflow/tensorflow/blame/a90eb068e805fbe39ccfd7f4bfc2e33dd8a592a0/tensorflow/compiler/tf2xla/python/xla.pyL637 Can you double check that the Python executable you're running is picking up the nightly tensorflow listed by `pip`? For example: ```python $ python c ""import tensorflow; print(tensorflow.__version__)"" 2.15.0dev20230919 ```","Thanks, that was it. The problem is that `tensorflow_serving_api` depends on `tensorflow`, so that gets installed along with `tfnightly` and is found first. A workaround is to do this instead: ```bash $ pip install matplotlib flax jaxlib tensorflow_datasets tensorflow_serving_api e jax $ pip install tfnightly ``` Maybe the jax2tf serving guide should recommend that instead?"
5090,"以下是一个github上的jax下的一个issue, 标题是(jax 0.4.6 does not provide the extra 'cuda12_pip' | No GPU/TPU found, falling back to CPU)， 内容是 ( Description I came to Jax's repository looking for a solution because I was facing an issue with another repository. Jax was one of the dependencies of that repository. No matter what I try or do, I get the following error. ```  C:\Users\rosha\Downloads\Compressed\appp>pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda12_pip]   Using cached jax0.4.14py3noneany.whl Collecting mldtypes>=0.2.0   Using cached ml_dtypes0.2.0cp310cp310win_amd64.whl (938 kB) Collecting opteinsum   Using cached opt_einsum3.3.0py3noneany.whl (65 kB) Collecting numpy>=1.22   Using cached numpy1.26.0cp310cp310win_amd64.whl (15.8 MB) Collecting scipy>=1.7   Using cached scipy1.11.2cp310cp310win_amd64.whl (44.0 MB) Collecting nvidiacufftcu12   Using cached nvidia_cufft_cu1211.0.8.103py3nonewin_amd64.whl (99.0 MB) Collecting nvidiacublascu12   Using cached nvidia_cublas_cu1212.2.5.6py3nonewin_amd64.whl (450.5 MB) Collecting nvidiacudacupticu12   Using cached nvidia_cuda_cupti_cu1212.2.142py3nonewin_amd64.whl (9.9 MB) Collecting nvidiacudaruntimecu12   Using cached nvidia_cuda_runtime_cu1212.2.140py3nonewin_amd64.whl (842 kB) Collecting nvidiacudnncu12>=8.9   Using cached nvidia_cudnn_cu128.9.4.25py3nonewin_amd64.whl (734.8 MB) Collecting nvidiacusolvercu12   Using cached nvidia_cusolver_cu1211.5.2.141py3nonewin_amd64.whl (122.2 MB) Collecting nvidiacudanvcccu12   Using cached nvidia_cuda_nvcc_cu1212.2.140py3nonewin_amd64.whl (15.9 MB) Collecting jax[cuda12_pip]   Using cached jax0.4.13.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.12.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.11.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.10.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.9.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.8.tar.gz (1.2 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.7.tar.gz (1.2 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.4.6.tar.gz (1.2 MB)   Preparing metadata (setup.py) ... done WARNING: jax 0.4.6 does not provide the extra 'cuda12_pip' Using legacy 'setup.py install' for jax, since package 'wheel' is not installed. Installing collected packages: numpy, scipy, opteinsum, jax   Running setup.py install for jax ... done Successfully installed jax0.4.6 numpy1.26.0 opteinsum3.3.0 scipy1.11.2 ``` And at the end, jax with cuda is not being installed. I have tried the workarounds provided in the issues section of the repository but no success.  Edit 1: I made a mistake. Without carefully reading the installation guide, I installed CUDA V12.2 but as the installation guide says: ``` JAX currently ships two CUDA wheel variants: CUDA 12.0 and CuDNN 8.9. CUDA 11.8 and CuDNN 8.6. ``` so I removed the CUDA V12.2 and installed V12.0. I also downloaded `cudnn` files and placed them in `lib`, `include` and `bin` folder but still the same output. Edit 2: I made a change to `pip` command `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and remove `_pip` after `cuda12` and the installed completed though not successfully. Instead, the v0.4.14 was installed in one go rather than cycling though the older versions and trying to install them. Edit 3: I used the following script to check whether GPU is present [cuda is installed]: ``` import jax devices = jax.devices() gpu_present = any(device.device_kind == 'gpu' for device in devices) if gpu_present:     print(""GPU is present."") else:     print(""No GPU detected."") ``` and the output is: ``` No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) No GPU detected. ``` Edit 4: Below is the output of `pip list` after installing `jax` and others packages (there was no conflict of packages), !Screenshot (2579)  What jax/jaxlib version are you using? jax v04.14, jaxlib v0.4.14  Which accelerator(s) are you using? Nvidia 3090 Ti  Additional system info Python: 3.10.7, OS(Windows 11)  NVIDIA GPU info !Screenshot (2577))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"jax 0.4.6 does not provide the extra 'cuda12_pip' | No GPU/TPU found, falling back to CPU"," Description I came to Jax's repository looking for a solution because I was facing an issue with another repository. Jax was one of the dependencies of that repository. No matter what I try or do, I get the following error. ```  C:\Users\rosha\Downloads\Compressed\appp>pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda12_pip]   Using cached jax0.4.14py3noneany.whl Collecting mldtypes>=0.2.0   Using cached ml_dtypes0.2.0cp310cp310win_amd64.whl (938 kB) Collecting opteinsum   Using cached opt_einsum3.3.0py3noneany.whl (65 kB) Collecting numpy>=1.22   Using cached numpy1.26.0cp310cp310win_amd64.whl (15.8 MB) Collecting scipy>=1.7   Using cached scipy1.11.2cp310cp310win_amd64.whl (44.0 MB) Collecting nvidiacufftcu12   Using cached nvidia_cufft_cu1211.0.8.103py3nonewin_amd64.whl (99.0 MB) Collecting nvidiacublascu12   Using cached nvidia_cublas_cu1212.2.5.6py3nonewin_amd64.whl (450.5 MB) Collecting nvidiacudacupticu12   Using cached nvidia_cuda_cupti_cu1212.2.142py3nonewin_amd64.whl (9.9 MB) Collecting nvidiacudaruntimecu12   Using cached nvidia_cuda_runtime_cu1212.2.140py3nonewin_amd64.whl (842 kB) Collecting nvidiacudnncu12>=8.9   Using cached nvidia_cudnn_cu128.9.4.25py3nonewin_amd64.whl (734.8 MB) Collecting nvidiacusolvercu12   Using cached nvidia_cusolver_cu1211.5.2.141py3nonewin_amd64.whl (122.2 MB) Collecting nvidiacudanvcccu12   Using cached nvidia_cuda_nvcc_cu1212.2.140py3nonewin_amd64.whl (15.9 MB) Collecting jax[cuda12_pip]   Using cached jax0.4.13.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.12.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.11.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.10.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.9.tar.gz (1.3 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.8.tar.gz (1.2 MB)   Installing build dependencies ... done   Getting requirements to build wheel ... done   Preparing metadata (pyproject.toml) ... done   Using cached jax0.4.7.tar.gz (1.2 MB)   Preparing metadata (setup.py) ... done   Using cached jax0.4.6.tar.gz (1.2 MB)   Preparing metadata (setup.py) ... done WARNING: jax 0.4.6 does not provide the extra 'cuda12_pip' Using legacy 'setup.py install' for jax, since package 'wheel' is not installed. Installing collected packages: numpy, scipy, opteinsum, jax   Running setup.py install for jax ... done Successfully installed jax0.4.6 numpy1.26.0 opteinsum3.3.0 scipy1.11.2 ``` And at the end, jax with cuda is not being installed. I have tried the workarounds provided in the issues section of the repository but no success.  Edit 1: I made a mistake. Without carefully reading the installation guide, I installed CUDA V12.2 but as the installation guide says: ``` JAX currently ships two CUDA wheel variants: CUDA 12.0 and CuDNN 8.9. CUDA 11.8 and CuDNN 8.6. ``` so I removed the CUDA V12.2 and installed V12.0. I also downloaded `cudnn` files and placed them in `lib`, `include` and `bin` folder but still the same output. Edit 2: I made a change to `pip` command `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and remove `_pip` after `cuda12` and the installed completed though not successfully. Instead, the v0.4.14 was installed in one go rather than cycling though the older versions and trying to install them. Edit 3: I used the following script to check whether GPU is present [cuda is installed]: ``` import jax devices = jax.devices() gpu_present = any(device.device_kind == 'gpu' for device in devices) if gpu_present:     print(""GPU is present."") else:     print(""No GPU detected."") ``` and the output is: ``` No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) No GPU detected. ``` Edit 4: Below is the output of `pip list` after installing `jax` and others packages (there was no conflict of packages), !Screenshot (2579)  What jax/jaxlib version are you using? jax v04.14, jaxlib v0.4.14  Which accelerator(s) are you using? Nvidia 3090 Ti  Additional system info Python: 3.10.7, OS(Windows 11)  NVIDIA GPU info !Screenshot (2577)",2023-09-17T10:12:45Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17634,"JAX does not provide CUDAenabled wheels on Windows (https://github.com/google/jaxinstallation). Closing, since this is working as intended: pip will not succeed because there aren't any wheels to install.",Thanks  for clarifying. I opened another issue due to the error I'm receiving while building jax (cuda enabled) for Windows. https://github.com/google/jax/issues/17635. So does it mean that there is no official support for the Cudaenabled builds for Windows?,"Correct, there is no support from us (the JAX core team) for CUDA enabled builds on Windows. We release CPUonly wheels on Windows, and even those are best effort. But we welcome PRs to fix things!"
9670,"以下是一个github上的jax下的一个issue, 标题是(`jaxlib` compilation error on 0.4.11)， 内容是 ( Description Following Apple's installation tutorial and trying to use `jaxlib` 0.4.11 on an M2 machine I am unable to build `jax` from source.  Env setup & download JAX ```bash python3 m venv ~/jaxmetal                                                                                                                                                                                                                                          10:20:47 am source ~/jaxmetal/bin/activate python m pip install U pip python m pip install numpy wheel git clone https://github.com/google/jax.git branch jaxlibv0.4.11 singlebranch depth=1 cd jax ``` Then build ```bash python build/build.py bazel_options=//xla/python:enable_tpu=true ``` fails with output ```      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Downloading bazel from: https://github.com/bazelbuild/bazel/releases/download/6.1.2/bazel6.1.2darwinarm64 bazel6.1.2darwinarm64 [] 100% Bazel binary path: ./bazel6.1.2darwinarm64 Bazel version: 6.1.2 Python binary path: /Users/tks32/jaxmetal/bin/python Python version: 3.11 NumPy version: 1.25.2 MKLDNN enabled: yes Target CPU: arm64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Plugin device enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel6.1.2darwinarm64 run verbose_failures=true :build_wheel  output_path=/Users/tks32/tmp/jax/dist cpu=arm64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/tks32/tmp/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/tks32/tmp/jax/.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false //xla/python:enable_tpu=false //xla/python:enable_plugin_device=false INFO: Reading rc options for 'run' from /Users/tks32/tmp/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/tks32/jaxmetal/bin/python action_env=PYENV_ROOT python_path=/Users/tks32/jaxmetal/bin/python distinct_host_configuration=false //xla/python:enable_tpu=true config=mkl_open_source_only INFO: Found applicable config definition build:short_logs in file /Users/tks32/tmp/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /Users/tks32/tmp/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/tks32/tmp/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/tks32/tmp/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 Loading: [... many lines of loading]  Loading: DEBUG: /private/var/tmp/_bazel_tks32/fc3829a9a8c5115880c7fe02f687024b/external/xla/third_party/repo.bzl:132:14: Warning: skipping import of repository 'tf_runtime' because it already exists. DEBUG: /private/var/tmp/_bazel_tks32/fc3829a9a8c5115880c7fe02f687024b/external/xla/third_party/repo.bzl:132:14: Warning: skipping import of repository 'llvmraw' because it already exists. Loading: [... many lines of loading]  Loading: WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/afb73f7a913ec8e7e8704afe18784571f320ebf6.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading: Loading: 1 packages loaded Analyzing: target //build:build_wheel (2 packages loaded, 0 targets configured) Analyzing: target //build:build_wheel (73 packages loaded, 655 targets configured) Analyzing: target //build:build_wheel (198 packages loaded, 13714 targets configured) Analyzing: target //build:build_wheel (199 packages loaded, 14055 targets configured) [... same duplicated many times ...] Analyzing: target //build:build_wheel (199 packages loaded, 14055 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/75c7095a97c6aaaee15dfab1fac529ce695e1d4a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found INFO: Analyzed target //build:build_wheel (200 packages loaded, 14798 targets configured). INFO: Found 1 target... [0 / 4] [Prepa] BazelWorkspaceStatusAction stablestatus.txt ... (3 actions, 0 running) ERROR: /private/var/tmp/_bazel_tks32/fc3829a9a8c5115880c7fe02f687024b/external/tsl/tsl/protobuf/BUILD:41:17: ProtoCompile external/tsl/tsl/protobuf/error_codes.pb.h failed: (Aborted): protoc failed: error executing command (from target //tsl/protobuf:error_codes_proto_impl_genproto)   (cd /private/var/tmp/_bazel_tks32/fc3829a9a8c5115880c7fe02f687024b/execroot/__main__ && \   exec env  \     PATH='/Users/tks32/jaxmetal/bin:/opt/homebrew/opt/utillinux/sbin:/opt/homebrew/opt/utillinux/bin:/opt/homebrew/opt/openjdk/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/tks32/Library/Application Support/JetBrains/Toolbox/scripts' \   bazelout/darwin_arm64optexec50AE0418/bin/external/com_google_protobuf/protoc 'cpp_out=bazelout/darwin_arm64opt/bin/external/tsl' Iexternal/tsl Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/any_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/source_context_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/type_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/api_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/descriptor_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/compiler_plugin_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/duration_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/empty_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/field_mask_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/struct_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/timestamp_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/wrappers_proto external/tsl/tsl/protobuf/error_codes.proto)  Configuration: ec3d6db951dd9aecce404ee3a1ce0b7bd210278e89843984239225bc46d1501a  Execution platform: //:platform dyld[39198]: symbol not found in flat namespace '__ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEED1Ev' Target //build:build_wheel failed to build INFO: Elapsed time: 511.749s, Critical Path: 0.03s INFO: 13 processes: 13 internal. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target b'' Traceback (most recent call last):   File ""/Users/tks32/tmp/jax/build/build.py"", line 564, in      main()   File ""/Users/tks32/tmp/jax/build/build.py"", line 559, in main     shell(command)   File ""/Users/tks32/tmp/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/Cellar/python.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py"", line 466, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/Cellar/python.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py"", line 571, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel6.1.2darwinarm64', 'run', 'verbose_failures=true', ':build_wheel', '', 'output_path=/Users/tks32/tmp/jax/dist', 'cpu=arm64']' returned nonzero exit status 1. ``` This seems to contain a warnings ``` WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/afb73f7a913ec8e7e8704afe18784571f320ebf6.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/75c7095a97c6aaaee15dfab1fac529ce695e1d4a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found ```  and a linking error: ``` dyld[39198]: symbol not found in flat namespace '__ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEED1Ev' ```  What jax/jaxlib version are you using? jaxlib 0.4.11  Which accelerator(s) are you using? Apple M2 Pro (MacMini),   Additional system info Python 3.11.5 (homebrew), MacOS 13.5.2, Apple M2 Pro chip (MacMini), gcc (Homebrew GCC 13.1.0) 13.1.0  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jaxlib` compilation error on 0.4.11," Description Following Apple's installation tutorial and trying to use `jaxlib` 0.4.11 on an M2 machine I am unable to build `jax` from source.  Env setup & download JAX ```bash python3 m venv ~/jaxmetal                                                                                                                                                                                                                                          10:20:47 am source ~/jaxmetal/bin/activate python m pip install U pip python m pip install numpy wheel git clone https://github.com/google/jax.git branch jaxlibv0.4.11 singlebranch depth=1 cd jax ``` Then build ```bash python build/build.py bazel_options=//xla/python:enable_tpu=true ``` fails with output ```      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Downloading bazel from: https://github.com/bazelbuild/bazel/releases/download/6.1.2/bazel6.1.2darwinarm64 bazel6.1.2darwinarm64 [] 100% Bazel binary path: ./bazel6.1.2darwinarm64 Bazel version: 6.1.2 Python binary path: /Users/tks32/jaxmetal/bin/python Python version: 3.11 NumPy version: 1.25.2 MKLDNN enabled: yes Target CPU: arm64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Plugin device enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel6.1.2darwinarm64 run verbose_failures=true :build_wheel  output_path=/Users/tks32/tmp/jax/dist cpu=arm64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/tks32/tmp/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/tks32/tmp/jax/.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false //xla/python:enable_tpu=false //xla/python:enable_plugin_device=false INFO: Reading rc options for 'run' from /Users/tks32/tmp/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/tks32/jaxmetal/bin/python action_env=PYENV_ROOT python_path=/Users/tks32/jaxmetal/bin/python distinct_host_configuration=false //xla/python:enable_tpu=true config=mkl_open_source_only INFO: Found applicable config definition build:short_logs in file /Users/tks32/tmp/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file /Users/tks32/tmp/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/tks32/tmp/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/tks32/tmp/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 Loading: [... many lines of loading]  Loading: DEBUG: /private/var/tmp/_bazel_tks32/fc3829a9a8c5115880c7fe02f687024b/external/xla/third_party/repo.bzl:132:14: Warning: skipping import of repository 'tf_runtime' because it already exists. DEBUG: /private/var/tmp/_bazel_tks32/fc3829a9a8c5115880c7fe02f687024b/external/xla/third_party/repo.bzl:132:14: Warning: skipping import of repository 'llvmraw' because it already exists. Loading: [... many lines of loading]  Loading: WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/afb73f7a913ec8e7e8704afe18784571f320ebf6.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading: Loading: 1 packages loaded Analyzing: target //build:build_wheel (2 packages loaded, 0 targets configured) Analyzing: target //build:build_wheel (73 packages loaded, 655 targets configured) Analyzing: target //build:build_wheel (198 packages loaded, 13714 targets configured) Analyzing: target //build:build_wheel (199 packages loaded, 14055 targets configured) [... same duplicated many times ...] Analyzing: target //build:build_wheel (199 packages loaded, 14055 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/75c7095a97c6aaaee15dfab1fac529ce695e1d4a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found INFO: Analyzed target //build:build_wheel (200 packages loaded, 14798 targets configured). INFO: Found 1 target... [0 / 4] [Prepa] BazelWorkspaceStatusAction stablestatus.txt ... (3 actions, 0 running) ERROR: /private/var/tmp/_bazel_tks32/fc3829a9a8c5115880c7fe02f687024b/external/tsl/tsl/protobuf/BUILD:41:17: ProtoCompile external/tsl/tsl/protobuf/error_codes.pb.h failed: (Aborted): protoc failed: error executing command (from target //tsl/protobuf:error_codes_proto_impl_genproto)   (cd /private/var/tmp/_bazel_tks32/fc3829a9a8c5115880c7fe02f687024b/execroot/__main__ && \   exec env  \     PATH='/Users/tks32/jaxmetal/bin:/opt/homebrew/opt/utillinux/sbin:/opt/homebrew/opt/utillinux/bin:/opt/homebrew/opt/openjdk/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/tks32/Library/Application Support/JetBrains/Toolbox/scripts' \   bazelout/darwin_arm64optexec50AE0418/bin/external/com_google_protobuf/protoc 'cpp_out=bazelout/darwin_arm64opt/bin/external/tsl' Iexternal/tsl Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/any_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/source_context_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/type_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/api_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/descriptor_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/compiler_plugin_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/duration_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/empty_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/field_mask_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/struct_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/timestamp_proto Ibazelout/darwin_arm64opt/bin/external/com_google_protobuf/_virtual_imports/wrappers_proto external/tsl/tsl/protobuf/error_codes.proto)  Configuration: ec3d6db951dd9aecce404ee3a1ce0b7bd210278e89843984239225bc46d1501a  Execution platform: //:platform dyld[39198]: symbol not found in flat namespace '__ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEED1Ev' Target //build:build_wheel failed to build INFO: Elapsed time: 511.749s, Critical Path: 0.03s INFO: 13 processes: 13 internal. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target b'' Traceback (most recent call last):   File ""/Users/tks32/tmp/jax/build/build.py"", line 564, in      main()   File ""/Users/tks32/tmp/jax/build/build.py"", line 559, in main     shell(command)   File ""/Users/tks32/tmp/jax/build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/Cellar/python.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py"", line 466, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/opt/homebrew/Cellar/python.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py"", line 571, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel6.1.2darwinarm64', 'run', 'verbose_failures=true', ':build_wheel', '', 'output_path=/Users/tks32/tmp/jax/dist', 'cpu=arm64']' returned nonzero exit status 1. ``` This seems to contain a warnings ``` WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/afb73f7a913ec8e7e8704afe18784571f320ebf6.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/75c7095a97c6aaaee15dfab1fac529ce695e1d4a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found ```  and a linking error: ``` dyld[39198]: symbol not found in flat namespace '__ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEED1Ev' ```  What jax/jaxlib version are you using? jaxlib 0.4.11  Which accelerator(s) are you using? Apple M2 Pro (MacMini),   Additional system info Python 3.11.5 (homebrew), MacOS 13.5.2, Apple M2 Pro chip (MacMini), gcc (Homebrew GCC 13.1.0) 13.1.0  NVIDIA GPU info _No response_",2023-09-16T09:42:07Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17630," I was struggling a bit with the same issues. However, jaxmetal 0.0.4 was just released (https://pypi.org/project/jaxmetal/0.0.4/). This seems to be installing a prebuilt version of jaxlibv0.4.11. ","I strongly suspect this is because of libraries in your homebrew setup. Note that this bug does not reproduce in our (nonhomebrew) CI. Can you try without homebrew? Closing, since I suspect you should now use a prebuilt jaxlib wheel, as  suggests, because `jaxmetal` is compatible with a released jaxlib (admittedly a slightly older one)."
2675,"以下是一个github上的jax下的一个issue, 标题是(Unexpected exception from jax.lax.fori_loop)， 内容是 ( Description There appears to be an issue with `jax.lax.fori_loop`. When I try to use this function, I get the following exception: ""the input carry component loop_carry[1][3].positions has type float32[0] but the corresponding output carry component has type float32[10,3], so the shapes do not match"" The code producing this error is the following: ```python (jax.jit, static_argnames=('targetForce', 'timesteps') def loss(model: controller, ball: BouncingBall, targetForce: float = 1.0, timesteps: int = 10):     positions = jp.array([[0]*3]*timesteps, dtype=jp.float32)     velocities = jp.array([[0]*6]*timesteps, dtype=jp.float32)     constraints = jp.array([[0]*6]*timesteps, dtype=jp.float32)     carry_i = (positions, velocities, constraints, ball, model)     def step(i: int, carry: tuple):         positions_s, velocities_s, constraints_s, ball_s, model_s = carry         positions_s = positions_s.at[i,:].add(ball_s.state.x.pos[0])         velocities_s = velocities_s.at[i,:].add(ball_s.state.qd)         constraints_s = constraints_s.at[i,:].add(ball_s.state.qf_constraint)         x = jp.array([ball_s.state.x.pos[0][2], ball_s.state.qd[2]])         force = model_s(x.transpose())         newstate = pipeline.step(ball_s.system, ball_s.state, force)         ball_s = ball_s.create(ball_s.system, newstate, positions_s, velocities_s, ball_s.contacts, constraints_s, model_s)         newStuff = (positions_s, velocities_s, constraints_s, ball_s, model_s)         return newStuff     positions, velocities, constraints, ball, model = jax.lax.fori_loop(0, timesteps, step, carry_i)     states = (positions, velocities, constraints)     loss_value = jp.linalg.norm(constraints[:,2]  jp.array([targetForce]*timesteps))     return loss_value, states ``` A similar exception is being thrown for velocities and constraints. In this function, `controller` extends `equinox.Module`, and `BouncingBall` is a `flax.struct.dataclass` that wraps a Brax `System` with some other arrays for state information at different timesteps. When I disable jit compiling using  ```python from jax.config import config config.update('jax_disable_jit', True) ``` the function runs without issues, but when it is JIT compiled it throws these exceptions.  What jax/jaxlib version are you using? jax v0.4.14, jaxlib 0.4.14  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.12, Ubuntu 22.04, Intel Xeon E31230 V2  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unexpected exception from jax.lax.fori_loop," Description There appears to be an issue with `jax.lax.fori_loop`. When I try to use this function, I get the following exception: ""the input carry component loop_carry[1][3].positions has type float32[0] but the corresponding output carry component has type float32[10,3], so the shapes do not match"" The code producing this error is the following: ```python (jax.jit, static_argnames=('targetForce', 'timesteps') def loss(model: controller, ball: BouncingBall, targetForce: float = 1.0, timesteps: int = 10):     positions = jp.array([[0]*3]*timesteps, dtype=jp.float32)     velocities = jp.array([[0]*6]*timesteps, dtype=jp.float32)     constraints = jp.array([[0]*6]*timesteps, dtype=jp.float32)     carry_i = (positions, velocities, constraints, ball, model)     def step(i: int, carry: tuple):         positions_s, velocities_s, constraints_s, ball_s, model_s = carry         positions_s = positions_s.at[i,:].add(ball_s.state.x.pos[0])         velocities_s = velocities_s.at[i,:].add(ball_s.state.qd)         constraints_s = constraints_s.at[i,:].add(ball_s.state.qf_constraint)         x = jp.array([ball_s.state.x.pos[0][2], ball_s.state.qd[2]])         force = model_s(x.transpose())         newstate = pipeline.step(ball_s.system, ball_s.state, force)         ball_s = ball_s.create(ball_s.system, newstate, positions_s, velocities_s, ball_s.contacts, constraints_s, model_s)         newStuff = (positions_s, velocities_s, constraints_s, ball_s, model_s)         return newStuff     positions, velocities, constraints, ball, model = jax.lax.fori_loop(0, timesteps, step, carry_i)     states = (positions, velocities, constraints)     loss_value = jp.linalg.norm(constraints[:,2]  jp.array([targetForce]*timesteps))     return loss_value, states ``` A similar exception is being thrown for velocities and constraints. In this function, `controller` extends `equinox.Module`, and `BouncingBall` is a `flax.struct.dataclass` that wraps a Brax `System` with some other arrays for state information at different timesteps. When I disable jit compiling using  ```python from jax.config import config config.update('jax_disable_jit', True) ``` the function runs without issues, but when it is JIT compiled it throws these exceptions.  What jax/jaxlib version are you using? jax v0.4.14, jaxlib 0.4.14  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.12, Ubuntu 22.04, Intel Xeon E31230 V2  NVIDIA GPU info _No response_",2023-09-15T20:16:25Z,bug question,closed,0,2,https://github.com/jax-ml/jax/issues/17629,"When running `fori_loop` under `jit`, the shapes of input arrays must match the shapes of output arrays. From the error message: ``` the input carry component loop_carry[1][3].positions has type float32[0] but the corresponding output carry component has type float32[10,3], so the shapes do not match ``` It looks like `loop_carry[1][3]` is the variable you call `ball`, and on input `ball.positions` has shape `(0,)` and on output `ball.positions` has shape `(10, 3)`. The way to fix this is to ensure that the input arrays have the same shape as the output arrays. I would look for where you're initializing `ball` in your code, and make sure it's initialized with the same shape arrays as you expect on output.",Thanks ! I hadn't thought to look at ball.positions. I changed the array in `BouncingBall` to have a preallocated size and now it works.
970,"以下是一个github上的jax下的一个issue, 标题是(No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.))， 内容是 ( Description I am trying to run this code on my college GPU with SLURM. Torch is running fine however JAX wont run. ```python import jax def check_gpu():      Get a list of devices available to JAX     devices = jax.devices()      Check if any of the devices are GPUs     for device in devices:         if ""gpu"" in device.device_kind.lower():             return True     return False if check_gpu():     print(""JAX is running on a GPU."") else:     print(""JAX is not running on a GPU."") ``` !Screenshot 20230915 at 6 43 22 PM  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Fri Sep 15 18:44:40 2023        ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)"," Description I am trying to run this code on my college GPU with SLURM. Torch is running fine however JAX wont run. ```python import jax def check_gpu():      Get a list of devices available to JAX     devices = jax.devices()      Check if any of the devices are GPUs     for device in devices:         if ""gpu"" in device.device_kind.lower():             return True     return False if check_gpu():     print(""JAX is running on a GPU."") else:     print(""JAX is not running on a GPU."") ``` !Screenshot 20230915 at 6 43 22 PM  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Fri Sep 15 18:44:40 2023        ++  ++",2023-09-15T16:44:59Z,bug,closed,0,20,https://github.com/jax-ml/jax/issues/17624,How did you install JAX?,"I installed JAX using pip install jax. As pip install jax[cuda] does not work and says version mismatch with python, hwoever my python is on latest version already. I cannot downgrade either.","As your `nvidiasmi` shows the CUDA version is 12.0, you should use ```sh pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` to install JAX, according to the installation guide.",What happens when you did what  suggested?,"Thanks for the reply. Its still at the same output. I did the installation using  suggestions, it installed noramlly. But ""JAX is not running on a GPU."" !Screenshot 20230923 at 12 51 30 PM", You should first uninstall the old `jax` and `jaxlib` using: ```sh pip uninstall jax jaxlib ```,Hi  did you find the solution?," I did that and didn't work  Not yet, and hence moved to torch. Lot others were also struggling  https://twitter.com/s4nyam/status/1702727177144295929", Please don't move to PyTorch! JAX is much better than PyTorch. I can fix this for you!,"Based on the screenshot provided, it seems that you are utilizing a conda environment. To proceed, I recommend the following steps: 1. Create a new conda environment with Python version 3.11: ``` conda create n  python=3.11 ``` Replace `` with your desired environment name. 2. Install the necessary package with pip: ``` pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Please ensure to activate the newly created conda environment before executing the pip installation command. 3. Test JAX GPU Accessibility: Execute the following Python code to check if JAX can access the GPU: ```python import jax print(jax.devices()) ``` This script prints a list of available devices. If the GPU is configured correctly, you should see `GpuDevice`."," I try your description. I give below error: ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflowcpu 2.12.0 requires keras=2.12.0, but you have keras 2.14.0 which is incompatible. tensorflowcpu 2.12.0 requires numpy=1.22, but you have numpy 1.26.1 which is incompatible. tensorflowcpu 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3, but you have protobuf 3.12.4 which is incompatible. tensorflowcpu 2.12.0 requires tensorboard=2.12, but you have tensorboard 2.8.0 which is incompatible. tensorflowcpu 2.12.0 requires tensorflowestimator=2.12.0, but you have tensorflowestimator 2.14.0 which is incompatible. torch 2.1.0 requires nvidiacublascu12==12.1.3.1; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacublascu12 12.3.2.9 which is incompatible. torch 2.1.0 requires nvidiacudacupticu12==12.1.105; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacudacupticu12 12.3.52 which is incompatible. torch 2.1.0 requires nvidiacudaruntimecu12==12.1.105; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacudaruntimecu12 12.3.52 which is incompatible. torch 2.1.0 requires nvidiacufftcu12==11.0.2.54; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacufftcu12 11.0.11.19 which is incompatible. torch 2.1.0 requires nvidiacusolvercu12==11.4.5.107; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacusolvercu12 11.5.3.52 which is incompatible. torch 2.1.0 requires nvidiacusparsecu12==12.1.0.106; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacusparsecu12 12.1.3.153 which is incompatible. torch 2.1.0 requires nvidiancclcu12==2.18.1; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiancclcu12 2.19.3 which is incompatible. Successfully installed jax0.4.19 jaxlib0.4.19+cuda12.cudnn89 nvidiacublascu1212.3.2.9 nvidiacudacupticu1212.3.52 nvidiacudanvcccu1212.3.52 nvidiacudaruntimecu1212.3.52 nvidiacufftcu1211.0.11.19 nvidiacusolvercu1211.5.3.52 nvidiacusparsecu1212.1.3.153 nvidiancclcu122.19.3 jax.device() resuilt is ""[cuda(id=0)]"" Have you got any idea? Thanks your help","I apologies for the oversight. It appears that `GpuDevice` is renamed to `cuda` in recent JAX releases. Therefore, the output `[cuda(id=0)]` means JAX is working correctly on your machine. For the pip's dependency resolver error, it seems that you are trying to install JAX into an environment with Tensorflow and PyTorch installed. It appears that there are version mismatches between the dependencies of these three packages. You should install JAX into a clean environment, preferably created with venv. Remember to activate the newly create environment before executing the pip installation command."," thnaks your answer. I remove tensorflowcpu and pytorch, next i can install the jax. How to install optax with jax and gpu compatibility?","  > I remove tensorflowcpu and pytorch, next i can install the jax. You should completely delete the environment and recreate a new one to avoid any potential conflicts between the dependencies. > How to install optax with jax and gpu compatibility? You can just use: ```sh pip install optax ``` Optax depends on JAX. If you install Optax in this way, it should work as expected, since you have already set up JAX successfully."," thanks your help. I have now other issue with jax. i created a new env, but i use cuda 12. 11/02/2023 18:42:19  WARNING  jax._src.xla_bridge    CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12020, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built.  Can I install jax 12010 version? I muss use the cuda 12010 version.","My solution in my project: Install cuda 11.8, because this cuda version compatiblity with pytorch, and jax. You can check the versions and install command: https://pytorch.org/getstarted/locally/ Create empty enviromet after install cuda 11.8. I use python 3.10 on Ubuntu `conda create n  python=3.10` Install torch: `pip3 install torch torchvision torchaudio indexurl https://download.pytorch.org/whl/cu118` Install jax with gpu. It's important, use jax with cuda 11, if you use 11.* conda version! `pip install U ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Install flax. This command install optax for me. `pip install flax` Good luck!","Hi, I also faced this problem: `No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` when directly installing it with `pip install flax`.  My cuda version is 12.2, python version is 3.8 (this is requested). I tried different commands to install gpubased jax, like: ``` pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` , and ``` pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` , and ``` pip install upgrade ""jax[cuda11_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```  and ``` pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` During installation, I would see a warning similar to `WARNING: jax 0.4.13 does not provide the extra 'cuda12local' `, but the installations would all succeed with a jax version of 0.4.13. However, when running: `python c ""import jax.numpy as jnp; jnp.ones((3,)); print('JAX installed successfully')"" `, I would always get something like: ``` 20240408 08:36:55.979761: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found. Traceback (most recent call last):   File """", line 1, in    File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2161, in ones     return lax.full(shape, 1, _jnp_dtype(dtype))   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/lax/lax.py"", line 1206, in full     return broadcast(fill_value, shape)   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/lax/lax.py"", line 768, in broadcast     return broadcast_in_dim(operand, tuple(sizes) + np.shape(operand), dims)   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/lax/lax.py"", line 797, in broadcast_in_dim     return broadcast_in_dim_p.bind(   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/core.py"", line 815, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 132, in apply_primitive     compiled_fun = xla_primitive_callable(   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/util.py"", line 284, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/util.py"", line 277, in cached     return f(*args, **kwargs)   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 223, in xla_primitive_callable     compiled = _xla_callable_uncached(   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 253, in _xla_callable_uncached     return computation.compile().unsafe_call   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2323, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2645, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2555, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 497, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/pengzhi/anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 465, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ``` May I have your suggestions on how to solve this problem? Thank you in advance for any help!! Look forward to your reply!"," In a clean environment (without any jax, flax, etc. installed), do: ``` pip install ""jax[cuda12_pip]==0.4.13"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html pip install ""flax==0.7.1"" ```","Thank you so much for your quick reply and help!!!   I'm really sorry that I made a very stupid mistake, that the cuda version is 10.1 based on `nvcc version`. In a clean env, I tried: ``` pip install ""jax[cuda10_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html pip install flax ``` and get it installed successfully with: ``` Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda10_pip]   Using cached jax0.4.13py3noneany.whl WARNING: jax 0.4.13 does not provide the extra 'cuda10pip' Collecting mldtypes>=0.1.0 (from jax[cuda10_pip])   Using cached ml_dtypes0.2.0cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB) Collecting numpy>=1.21 (from jax[cuda10_pip])   Using cached numpy1.24.4cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB) Collecting opteinsum (from jax[cuda10_pip])   Using cached opt_einsum3.3.0py3noneany.whl.metadata (6.5 kB) Collecting scipy>=1.7 (from jax[cuda10_pip])   Using cached scipy1.10.1cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB) Collecting importlibmetadata>=4.6 (from jax[cuda10_pip])   Using cached importlib_metadata7.1.0py3noneany.whl.metadata (4.7 kB) Collecting zipp>=0.5 (from importlibmetadata>=4.6>jax[cuda10_pip])   Using cached zipp3.18.1py3noneany.whl.metadata (3.5 kB) Using cached importlib_metadata7.1.0py3noneany.whl (24 kB) Using cached ml_dtypes0.2.0cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB) Using cached numpy1.24.4cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB) Using cached scipy1.10.1cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB) Using cached opt_einsum3.3.0py3noneany.whl (65 kB) Using cached zipp3.18.1py3noneany.whl (8.2 kB) Installing collected packages: zipp, numpy, scipy, opteinsum, mldtypes, importlibmetadata, jax Successfully installed importlibmetadata7.1.0 jax0.4.13 mldtypes0.2.0 numpy1.24.4 opteinsum3.3.0 scipy1.10.1 zipp3.18.1 Collecting flax   Using cached flax0.7.2py3noneany.whl.metadata (10.0 kB) Requirement already satisfied: numpy>=1.12 in ./anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages (from flax) (1.24.4) Requirement already satisfied: jax>=0.4.2 in ./anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages (from flax) (0.4.13) Collecting msgpack (from flax)   Using cached msgpack1.0.8cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB) Collecting optax (from flax)   Using cached optax0.1.8py3noneany.whl.metadata (14 kB) Collecting orbaxcheckpoint (from flax)   Using cached orbax_checkpoint0.2.3py3noneany.whl.metadata (1.8 kB) Collecting tensorstore (from flax)   Using cached tensorstore0.1.45cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB) Collecting rich>=11.1 (from flax)   Using cached rich13.7.1py3noneany.whl.metadata (18 kB) Collecting typingextensions>=4.1.1 (from flax)   Using cached typing_extensions4.11.0py3noneany.whl.metadata (3.0 kB) Collecting PyYAML>=5.4.1 (from flax)   Using cached PyYAML6.0.1cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB) Requirement already satisfied: mldtypes>=0.1.0 in ./anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages (from jax>=0.4.2>flax) (0.2.0) Requirement already satisfied: opteinsum in ./anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages (from jax>=0.4.2>flax) (3.3.0) Requirement already satisfied: scipy>=1.7 in ./anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages (from jax>=0.4.2>flax) (1.10.1) Requirement already satisfied: importlibmetadata>=4.6 in ./anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages (from jax>=0.4.2>flax) (7.1.0) Collecting markdownitpy>=2.2.0 (from rich>=11.1>flax)   Using cached markdown_it_py3.0.0py3noneany.whl.metadata (6.9 kB) Collecting pygments=2.13.0 (from rich>=11.1>flax)   Using cached pygments2.17.2py3noneany.whl.metadata (2.6 kB) Collecting abslpy>=0.7.1 (from optax>flax)   Using cached absl_py2.1.0py3noneany.whl.metadata (2.3 kB) Collecting chex>=0.1.7 (from optax>flax)   Using cached chex0.1.7py3noneany.whl.metadata (17 kB) Collecting jaxlib>=0.1.37 (from optax>flax)   Using cached jaxlib0.4.13cp38cp38manylinux2014_x86_64.whl.metadata (2.1 kB) Collecting cached_property (from orbaxcheckpoint>flax)   Using cached cached_property1.5.2py2.py3noneany.whl.metadata (11 kB) Collecting importlib_resources (from orbaxcheckpoint>flax)   Using cached importlib_resources6.4.0py3noneany.whl.metadata (3.9 kB) Collecting etils (from orbaxcheckpoint>flax)   Using cached etils1.3.0py3noneany.whl.metadata (5.5 kB) Collecting nest_asyncio (from orbaxcheckpoint>flax)   Using cached nest_asyncio1.6.0py3noneany.whl.metadata (2.8 kB) Collecting dmtree>=0.1.5 (from chex>=0.1.7>optax>flax)   Using cached dm_tree0.1.8cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB) Collecting toolz>=0.9.0 (from chex>=0.1.7>optax>flax)   Using cached toolz0.12.1py3noneany.whl.metadata (5.1 kB) Requirement already satisfied: zipp>=0.5 in ./anaconda3/envs/isaacgymtest/lib/python3.8/sitepackages (from importlibmetadata>=4.6>jax>=0.4.2>flax) (3.18.1) Collecting mdurl~=0.1 (from markdownitpy>=2.2.0>rich>=11.1>flax)   Using cached mdurl0.1.2py3noneany.whl.metadata (1.6 kB) Using cached flax0.7.2py3noneany.whl (226 kB) Using cached PyYAML6.0.1cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB) Using cached rich13.7.1py3noneany.whl (240 kB) Using cached typing_extensions4.11.0py3noneany.whl (34 kB) Using cached msgpack1.0.8cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB) Using cached optax0.1.8py3noneany.whl (199 kB) Using cached orbax_checkpoint0.2.3py3noneany.whl (81 kB) Using cached tensorstore0.1.45cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB) Using cached absl_py2.1.0py3noneany.whl (133 kB) Using cached chex0.1.7py3noneany.whl (89 kB) Using cached jaxlib0.4.13cp38cp38manylinux2014_x86_64.whl (71.6 MB) Using cached markdown_it_py3.0.0py3noneany.whl (87 kB) Using cached pygments2.17.2py3noneany.whl (1.2 MB) Using cached cached_property1.5.2py2.py3noneany.whl (7.6 kB) Using cached etils1.3.0py3noneany.whl (126 kB) Using cached importlib_resources6.4.0py3noneany.whl (38 kB) Using cached nest_asyncio1.6.0py3noneany.whl (5.2 kB) Using cached dm_tree0.1.8cp38cp38manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB) Using cached mdurl0.1.2py3noneany.whl (10.0 kB) Using cached toolz0.12.1py3noneany.whl (56 kB) Installing collected packages: dmtree, cached_property, typingextensions, toolz, tensorstore, PyYAML, pygments, nest_asyncio, msgpack, mdurl, importlib_resources, etils, abslpy, markdownitpy, jaxlib, rich, orbaxcheckpoint, chex, optax, flax Successfully installed PyYAML6.0.1 abslpy2.1.0 cached_property1.5.2 chex0.1.7 dmtree0.1.8 etils1.3.0 flax0.7.2 importlib_resources6.4.0 jaxlib0.4.13 markdownitpy3.0.0 mdurl0.1.2 msgpack1.0.8 nest_asyncio1.6.0 optax0.1.8 orbaxcheckpoint0.2.3 pygments2.17.2 rich13.7.1 tensorstore0.1.45 toolz0.12.1 typingextensions4.11.0 ``` But when trying to run some example codes, I got: ``` Python 3.8.19 (default, Mar 20 2024, 19:58:24)  [GCC 11.2.0] :: Anaconda, Inc. on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax.numpy as jnp >>> from jax import grad, jit, vmap >>> from jax import random >>> key = random.key(0) No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` Thus, Jax still cannot make use of GPU. May I again have your suggestions on this problem? Thank you so much for considering this matter and your help!","Regarding the `jax[cuda10_pip]` error, see the discussion in CC(未找到相关数据)"
320,"以下是一个github上的jax下的一个issue, 标题是([Mosaic] Add support for specifying estimated costs for Mosaic kernels)， 内容是 ([Mosaic] Add support for specifying estimated costs for Mosaic kernels)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Mosaic] Add support for specifying estimated costs for Mosaic kernels,[Mosaic] Add support for specifying estimated costs for Mosaic kernels,2023-09-13T10:58:02Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17588
1228,"以下是一个github上的jax下的一个issue, 标题是(fast dispatch for functions over typed PRNG key arrays)， 内容是 (fast dispatch for functions over typed PRNG key arrays Before this change, JAX could dispatch compiled functions over newstyle (typed) RNG key arrays, but it would always do so off of the fast (C++based) dispatch path. In other words, switching from oldstyle `uint32` RNG keys to newstyle keys would regress dispatch times. With this change, dispatch happens on the fast path again and performance regressions ought to be minimal. We currently maintain only one pytree registry, for all registered pytree node types. We want RNG key arrays to also be treated as pytree leaves everywhere *except* during dispatch. In other words: we want operations on (typed) RNG key arrays to appear in Jaxpr, but we want to unravel those arrays into their underlying `uint32` arrays only during dispatch. To do this, we add a new internal pytree registry that dispatch respects uniquely. This registry includes all items in the default registry, but also the RNG key array type. Coauthoredby: Matthew Johnson )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fast dispatch for functions over typed PRNG key arrays,"fast dispatch for functions over typed PRNG key arrays Before this change, JAX could dispatch compiled functions over newstyle (typed) RNG key arrays, but it would always do so off of the fast (C++based) dispatch path. In other words, switching from oldstyle `uint32` RNG keys to newstyle keys would regress dispatch times. With this change, dispatch happens on the fast path again and performance regressions ought to be minimal. We currently maintain only one pytree registry, for all registered pytree node types. We want RNG key arrays to also be treated as pytree leaves everywhere *except* during dispatch. In other words: we want operations on (typed) RNG key arrays to appear in Jaxpr, but we want to unravel those arrays into their underlying `uint32` arrays only during dispatch. To do this, we add a new internal pytree registry that dispatch respects uniquely. This registry includes all items in the default registry, but also the RNG key array type. Coauthoredby: Matthew Johnson ",2023-09-12T21:57:05Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17570
1623,"以下是一个github上的jax下的一个issue, 标题是(JAX does not load bundled CUDNN)， 内容是 ( Description Somehow JAX does not use the bundled cudnn. How to reproduce: ``` conda create n debug_jax_cudnn python=3.10 conda activate debug_jax_cudnn pip install upgrade ""jax[cuda11_pip]==0.4.13"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html export LD_LIBRARY_PATH="""" python c ""import jax; A = jax.numpy.zeros((10, 10));"" ``` Error: ``` 20230911 18:02:05.062414: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED   20230911 18:02:05.062576: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 7995392 bytes free, 11721572352 bytes total.   20230911 18:02:05.062650: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:453] Possibly insufficient driver version: 510.47.3   Traceback (most recent call last):     File """", line 1, in      File ""/nfs/kun2/users/oier/miniconda3/envs/debug_jax_cudnn/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2153, in zeros       return lax.full(shape, 0, _jnp_dtype(dtype)) ...   jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ``` The nvidia driver is  510.47.3, which should support cuda 11.  What jax/jaxlib version are you using? 0.4.13  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 20.04  NVIDIA GPU info NVIDIA GeForce GTX 1080 Ti)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX does not load bundled CUDNN," Description Somehow JAX does not use the bundled cudnn. How to reproduce: ``` conda create n debug_jax_cudnn python=3.10 conda activate debug_jax_cudnn pip install upgrade ""jax[cuda11_pip]==0.4.13"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html export LD_LIBRARY_PATH="""" python c ""import jax; A = jax.numpy.zeros((10, 10));"" ``` Error: ``` 20230911 18:02:05.062414: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED   20230911 18:02:05.062576: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 7995392 bytes free, 11721572352 bytes total.   20230911 18:02:05.062650: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:453] Possibly insufficient driver version: 510.47.3   Traceback (most recent call last):     File """", line 1, in      File ""/nfs/kun2/users/oier/miniconda3/envs/debug_jax_cudnn/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2153, in zeros       return lax.full(shape, 0, _jnp_dtype(dtype)) ...   jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ``` The nvidia driver is  510.47.3, which should support cuda 11.  What jax/jaxlib version are you using? 0.4.13  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 20.04  NVIDIA GPU info NVIDIA GeForce GTX 1080 Ti",2023-09-12T01:16:46Z,bug NVIDIA GPU,closed,0,6,https://github.com/jax-ml/jax/issues/17553,"I'm not 100% sure I interpret it correctly, but isn't 'Memory usage: 7995392 bytes free' implying that you only have ~8 MB of free memory on your GPU?","> I'm not 100% sure I interpret it correctly, but isn't 'Memory usage: 7995392 bytes free' implying that you only have ~8 MB of free memory on your GPU? When I try another GPU I get: `20230911 22:50:18.801369: E external/xla/xla/service/gpu/gpu_compiler.cc:1423] The CUDA linking API did not work. Please use XLA_FLAGS=xla_gpu_force_compilation_parallelism=1 to bypass it, but expect to get longer compilation time due to the lack of multithreading. Original error: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Input file '/tmp/tempfilegracee8fbd375348513060523048fe531.cubin' newer than toolkit (118 vs 116)`",This should be fixed with jax 0.4.20. JAX will prefer the pippackaged CUDA libraries if they are present.,"I just tried to reinstall 0.4.20 and I still have to set LD_LIBRARY_PATH to """" for it to work. Could there be a conflict with the changes in 0.4.17? > CUDA: JAX now verifies that the CUDA libraries it finds are at least as new as the CUDA libraries that JAX was built against. If older libraries are found, JAX raises an exception since that is preferable to mysterious failures and crashes. as without setting LD_LIBRARY_PATH I get ```CUDA backend failed to initialize: Found cuDNN version 8600, but JAX was built against version 8904, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)``` EDIT: apparently I also have to modify PATH to include the directory containing the 'ptxas"" executable installed by Jax as it's not picked up automatically.","> EDIT: apparently I also have to modify PATH to include the directory containing the 'ptxas"" executable installed by Jax as it's not picked up automatically This should actually be resolved by https://github.com/openxla/xla/pull/7884 for future releases!",We think this is fixed! Please comment if you're still having a problem.
4725,"以下是一个github上的jax下的一个issue, 标题是([Pallas] Subpar performance vs JAX (unexpected))， 内容是 ( Description In XLB library, I attempted to create an unrolled version of the equilibrium function using Pallas—a task that is typically not feasible in JAX. Normally, this approach results in a significant increase in computational speed (in CUDA/C++, due to reusing several computations and ignoring computatons that results in zero). However, I observed a 50% reduction in performance. Please see the repro below. While I acknowledge that my implementation may not fully leverage the capabilities of Pallas, I would be grateful if you could review it to determine whether the performance drop is due to an error on my part or if it is an expected outcome in this scenario. ``` import jax import numpy as jnp from functools import partial from time import time from jax.experimental import pallas as pl from jax import lax import jax.numpy as jnp import numpy as np c = [     [1, 0, 0],     [0, 1, 0],     [0, 0, 1],     [1, 1, 0],     [1, 1, 0],     [1, 0, 1],     [1, 0, 1],     [0, 1, 1],     [0, 1, 1],     [0, 0, 0],     [1, 0, 0],     [0, 1, 0],     [0, 0, 1],     [1, 1, 0],     [1, 1, 0],     [1, 0, 1],     [1, 0, 1],     [0, 1, 1],     [0, 1, 1], ] c = jnp.array(c) c_opp = [     10, 11, 12, 13, 14, 15, 16, 17, 18,     9, 0, 1, 2, 3, 4, 5, 6, 7, 8, ] c_opp = jnp.array(c_opp) w = [     1. / 18, 1. / 18, 1. / 18, 1. / 36, 1. / 36, 1. / 36, 1. / 36, 1. / 36, 1. / 36,     1. / 3, 1. / 18, 1. / 18, 1. / 18, 1. / 36, 1. / 36, 1. / 36, 1. / 36, 1. / 36, 1. / 36, ] w = jnp.array(w) .jit def equilibrium_jax(rho, u):   cu = 3.0 * jnp.dot(u, c.T)   usqr = 1.5 * jnp.sum(jnp.square(u), axis=1, keepdims=True)   feq = rho * w * (1.0 + cu * (1.0 + 0.5 * cu)  usqr)   return feq def equilibrium_kernel(rho_ref, u_ref, o_ref):   idx = pl.program_id(0)   idy = pl.program_id(1)   idz = pl.program_id(2)   rho = pl.load(rho_ref, (idx, idy, idz, 0))   u0 = pl.load(u_ref, (idx, idy, idz, 0))   u1 = pl.load(u_ref, (idx, idy, idz, 1))   u2 = pl.load(u_ref, (idx, idy, idz, 2))   usqr = 1.5 * (u0 * u0 + u1 * u1 + u2 * u2)   ck_u03 = u0 + u1   ck_u04 = u0  u1   ck_u05 = u0 + u2   ck_u06 = u0  u2   ck_u07 = u1 + u2   ck_u08 = u1  u2   eq = [0] * 19   eq[0] = rho * (1. / 18.) * (1.  3. * u0 + 4.5 * u0 * u0  usqr)   eq[1] = rho * (1. / 18.) * (1.  3. * u1 + 4.5 * u1 * u1  usqr)   eq[2] = rho * (1. / 18.) * (1.  3. * u2 + 4.5 * u2 * u2  usqr)   eq[3] = rho * (1. / 36.) * (1.  3. * ck_u03 + 4.5 * ck_u03 * ck_u03  usqr)   eq[4] = rho * (1. / 36.) * (1.  3. * ck_u04 + 4.5 * ck_u04 * ck_u04  usqr)   eq[5] = rho * (1. / 36.) * (1.  3. * ck_u05 + 4.5 * ck_u05 * ck_u05  usqr)   eq[6] = rho * (1. / 36.) * (1.  3. * ck_u06 + 4.5 * ck_u06 * ck_u06  usqr)   eq[7] = rho * (1. / 36.) * (1.  3. * ck_u07 + 4.5 * ck_u07 * ck_u07  usqr)   eq[8] = rho * (1. / 36.) * (1.  3. * ck_u08 + 4.5 * ck_u08 * ck_u08  usqr)   eq[9] = rho * (1. / 3.) * (1.  usqr)   eq[10] = eq[0] + rho * (1. / 18.) * 6. * u0   eq[11] = eq[1] + rho * (1. / 18.) * 6. * u1   eq[12] = eq[2] + rho * (1. / 18.) * 6. * u2   eq[13] = eq[3] + rho * (1. / 36.) * 6. * ck_u03   eq[14] = eq[4] + rho * (1. / 36.) * 6. * ck_u04   eq[15] = eq[5] + rho * (1. / 36.) * 6. * ck_u05   eq[16] = eq[6] + rho * (1. / 36.) * 6. * ck_u06   eq[17] = eq[7] + rho * (1. / 36.) * 6. * ck_u07   eq[18] = eq[8] + rho * (1. / 36.) * 6. * ck_u08   for i in range(19):     pl.store(o_ref, (idx, idy, idz, i), eq[i]) nx, ny, nz = 256, 256, 256 rho = jnp.ones((nx, ny, nz, 1), dtype=jnp.float32) u = jnp.zeros((nx, ny, nz, 3), dtype=jnp.float32) .jit def equilibrium_pallas(rho, u):   return pl.pallas_call(equilibrium_kernel,                         out_shape=jax.ShapeDtypeStruct((nx, ny, nz, 19), rho.dtype), grid=((nx, ny, nz))                         )(rho, u) equilibrium_pallas(rho, u).block_until_ready() equilibrium_jax(rho, u).block_until_ready()  Pallas speed test t0_pallas = time() equilibrium_pallas(rho, u).block_until_ready() t1_pallas = time() print('Pallas time: ', t1_pallas  t0_pallas)  Jax speed test t0_jax = time() equilibrium_jax(rho, u).block_until_ready() t1_jax = time() print('Jax time: ', t1_jax  t0_jax)  Pallas speedup vs Jax print('Pallas speedup: ', (t1_jax  t0_jax) / (t1_pallas  t0_pallas))  Check results are the same with allclose print('Allclose: ', jnp.allclose(equilibrium_pallas(rho, u), equilibrium_jax(rho, u))) ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info ``` Mon Sep 11 16:38:52 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[Pallas] Subpar performance vs JAX (unexpected)," Description In XLB library, I attempted to create an unrolled version of the equilibrium function using Pallas—a task that is typically not feasible in JAX. Normally, this approach results in a significant increase in computational speed (in CUDA/C++, due to reusing several computations and ignoring computatons that results in zero). However, I observed a 50% reduction in performance. Please see the repro below. While I acknowledge that my implementation may not fully leverage the capabilities of Pallas, I would be grateful if you could review it to determine whether the performance drop is due to an error on my part or if it is an expected outcome in this scenario. ``` import jax import numpy as jnp from functools import partial from time import time from jax.experimental import pallas as pl from jax import lax import jax.numpy as jnp import numpy as np c = [     [1, 0, 0],     [0, 1, 0],     [0, 0, 1],     [1, 1, 0],     [1, 1, 0],     [1, 0, 1],     [1, 0, 1],     [0, 1, 1],     [0, 1, 1],     [0, 0, 0],     [1, 0, 0],     [0, 1, 0],     [0, 0, 1],     [1, 1, 0],     [1, 1, 0],     [1, 0, 1],     [1, 0, 1],     [0, 1, 1],     [0, 1, 1], ] c = jnp.array(c) c_opp = [     10, 11, 12, 13, 14, 15, 16, 17, 18,     9, 0, 1, 2, 3, 4, 5, 6, 7, 8, ] c_opp = jnp.array(c_opp) w = [     1. / 18, 1. / 18, 1. / 18, 1. / 36, 1. / 36, 1. / 36, 1. / 36, 1. / 36, 1. / 36,     1. / 3, 1. / 18, 1. / 18, 1. / 18, 1. / 36, 1. / 36, 1. / 36, 1. / 36, 1. / 36, 1. / 36, ] w = jnp.array(w) .jit def equilibrium_jax(rho, u):   cu = 3.0 * jnp.dot(u, c.T)   usqr = 1.5 * jnp.sum(jnp.square(u), axis=1, keepdims=True)   feq = rho * w * (1.0 + cu * (1.0 + 0.5 * cu)  usqr)   return feq def equilibrium_kernel(rho_ref, u_ref, o_ref):   idx = pl.program_id(0)   idy = pl.program_id(1)   idz = pl.program_id(2)   rho = pl.load(rho_ref, (idx, idy, idz, 0))   u0 = pl.load(u_ref, (idx, idy, idz, 0))   u1 = pl.load(u_ref, (idx, idy, idz, 1))   u2 = pl.load(u_ref, (idx, idy, idz, 2))   usqr = 1.5 * (u0 * u0 + u1 * u1 + u2 * u2)   ck_u03 = u0 + u1   ck_u04 = u0  u1   ck_u05 = u0 + u2   ck_u06 = u0  u2   ck_u07 = u1 + u2   ck_u08 = u1  u2   eq = [0] * 19   eq[0] = rho * (1. / 18.) * (1.  3. * u0 + 4.5 * u0 * u0  usqr)   eq[1] = rho * (1. / 18.) * (1.  3. * u1 + 4.5 * u1 * u1  usqr)   eq[2] = rho * (1. / 18.) * (1.  3. * u2 + 4.5 * u2 * u2  usqr)   eq[3] = rho * (1. / 36.) * (1.  3. * ck_u03 + 4.5 * ck_u03 * ck_u03  usqr)   eq[4] = rho * (1. / 36.) * (1.  3. * ck_u04 + 4.5 * ck_u04 * ck_u04  usqr)   eq[5] = rho * (1. / 36.) * (1.  3. * ck_u05 + 4.5 * ck_u05 * ck_u05  usqr)   eq[6] = rho * (1. / 36.) * (1.  3. * ck_u06 + 4.5 * ck_u06 * ck_u06  usqr)   eq[7] = rho * (1. / 36.) * (1.  3. * ck_u07 + 4.5 * ck_u07 * ck_u07  usqr)   eq[8] = rho * (1. / 36.) * (1.  3. * ck_u08 + 4.5 * ck_u08 * ck_u08  usqr)   eq[9] = rho * (1. / 3.) * (1.  usqr)   eq[10] = eq[0] + rho * (1. / 18.) * 6. * u0   eq[11] = eq[1] + rho * (1. / 18.) * 6. * u1   eq[12] = eq[2] + rho * (1. / 18.) * 6. * u2   eq[13] = eq[3] + rho * (1. / 36.) * 6. * ck_u03   eq[14] = eq[4] + rho * (1. / 36.) * 6. * ck_u04   eq[15] = eq[5] + rho * (1. / 36.) * 6. * ck_u05   eq[16] = eq[6] + rho * (1. / 36.) * 6. * ck_u06   eq[17] = eq[7] + rho * (1. / 36.) * 6. * ck_u07   eq[18] = eq[8] + rho * (1. / 36.) * 6. * ck_u08   for i in range(19):     pl.store(o_ref, (idx, idy, idz, i), eq[i]) nx, ny, nz = 256, 256, 256 rho = jnp.ones((nx, ny, nz, 1), dtype=jnp.float32) u = jnp.zeros((nx, ny, nz, 3), dtype=jnp.float32) .jit def equilibrium_pallas(rho, u):   return pl.pallas_call(equilibrium_kernel,                         out_shape=jax.ShapeDtypeStruct((nx, ny, nz, 19), rho.dtype), grid=((nx, ny, nz))                         )(rho, u) equilibrium_pallas(rho, u).block_until_ready() equilibrium_jax(rho, u).block_until_ready()  Pallas speed test t0_pallas = time() equilibrium_pallas(rho, u).block_until_ready() t1_pallas = time() print('Pallas time: ', t1_pallas  t0_pallas)  Jax speed test t0_jax = time() equilibrium_jax(rho, u).block_until_ready() t1_jax = time() print('Jax time: ', t1_jax  t0_jax)  Pallas speedup vs Jax print('Pallas speedup: ', (t1_jax  t0_jax) / (t1_pallas  t0_pallas))  Check results are the same with allclose print('Allclose: ', jnp.allclose(equilibrium_pallas(rho, u), equilibrium_jax(rho, u))) ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info ``` Mon Sep 11 16:38:52 2023        ++  ++ ```",2023-09-11T20:40:17Z,bug performance NVIDIA GPU,closed,2,5,https://github.com/jax-ml/jax/issues/17545,"The problem comes from the grid_size, `(256, 256, 256)` is unfortunate. You should rather use smthg like `(4, 16, 4)`. It goes from `0.32x` to `80x` faster. CC:    Before ```python Pallas time:  0.0273892879486084 Jax time:  0.008744478225708008 Pallas speedup:  0.319266358516352 Allclose:  True ````  After ```python Pallas time: 0.1112697646021843 ms Jax time: 8.874844759702682 ms Pallas speedup:  79.75971542163633 Allclose:  True ```   Diff File ```diff 2,4c2  import time 6d3  pallas_grid_size = (4, 16, 4) 102c99                          out_shape=jax.ShapeDtypeStruct((nx, ny, nz, 19), rho.dtype), grid=(pallas_grid_size) 105,107d101  for _ in range(20): >     t0_pallas = time.perf_counter() >     equilibrium_pallas(rho, u).block_until_ready() >     t1_pallas = time.perf_counter() > print(f'Pallas time: {(t1_pallas  t0_pallas) * 1000} ms') 115,118c110,114  for _ in range(20): >     t0_jax = time.perf_counter() >     equilibrium_jax(rho, u).block_until_ready() >     t1_jax = time.perf_counter() > print(f'Jax time: {(t1_jax  t0_jax) * 1000} ms') ```",Great  !! Very interesting. Now if I can install Pallas easily I should be able to implement it in the library 😆  https://github.com/google/jax/issues/18603issuecomment1885016966 Why the JAX time went up though?," JAX time didn't go up. One is in seconds (old), new is in ms. It's easier to read ;) ",I see. This is great news! For this function that does a large part of compute in our code we will be replacing it with Pallas in our ongoing major refactoring. This could bring our JAX backend's speed to within twice that of the fastest C++ codes—a substantial improvement from the previous 68 times slower rate (I should verify this though) Thanks a lot for the analysis!,> Great  !! Very interesting. Now if I can install Pallas easily I should be able to implement it in the library 😆  CC(Make Pallas/GPU easier to install) (comment) We have a public container last month that should work: docker pull ghcr.io/nvidia/jax:nightlypallas20231216
2754,"以下是一个github上的jax下的一个issue, 标题是(""None of the algorithms provided by cuDNN heuristics worked"" for Ampere NV GPUs)， 内容是 ( Description I'm training learned optimizers using Jax and a custom version of https://github.com/google/learned_optimization. I get the following warnings when training on Ampere GPUs (tested for RTX 3090 and A6000), however, no warning message appears when using an RTX2080ti or an RTX8000 GPU.  I'm listing this as an issue since the **training is much slower (2x or more) on ampere GPUs** than their predecessors, which should not be the case.  ``` 20230907 03:07:57.541957: W external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:779] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.                                      20230907 03:07:57.541994: W external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:782] Conv: (f32[3,2048,3,3]{3,2,1,0}, u8[0]{0}) customcall(f32[3,8192,33,33]{3,2,1,0}, f32[2048,128,31,31]{3,2,1,0}, f32[2048]{0 }), window={size=31x31}, dim_labels=bf01_oi01>bf01, feature_group_count=64, custom_call_target=""__cudnn$convBiasActivationForward"", backend_config={""conv_result_scale"":0.5,""activation_mode"":""kNone"",""side_input_scale "":0} ``` Unfortunately, I did not manage to extract a minimal reproducing example within two hours, so I have gone ahead and posted the issue anyway. Here is the complete stack trace for reference: ``` 20230907 03:07:24.380550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT                                                                                               ERROR:absl:Oryx not found! This library will still work but no summarywill be logged.                        gpu                                                                    /btherien/github/learned_optimization/learned_optimization/outer_trainers/truncation_schedule.py:95: UserWarning: Explicitly requested dtype  requested in asarray is not available, and will be  truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jaxcurrentgotchas for more.   length = jnp.asarray(jnp.exp(log_length), dtype=jnp.int64)                                                 ERROR:wandb.sdk.lib.gitlib:git root error: Cmd('git') failed due to: exit code(128)                            cmdline: git revparse showtoplevel                 stderr: 'fatal: detected dubious ownership in repository at          Outer Loop:   0% 6/5000 [25:10  A6000   RTX 8000   RTX 2080 ti )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"""None of the algorithms provided by cuDNN heuristics worked"" for Ampere NV GPUs"," Description I'm training learned optimizers using Jax and a custom version of https://github.com/google/learned_optimization. I get the following warnings when training on Ampere GPUs (tested for RTX 3090 and A6000), however, no warning message appears when using an RTX2080ti or an RTX8000 GPU.  I'm listing this as an issue since the **training is much slower (2x or more) on ampere GPUs** than their predecessors, which should not be the case.  ``` 20230907 03:07:57.541957: W external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:779] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms.                                      20230907 03:07:57.541994: W external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:782] Conv: (f32[3,2048,3,3]{3,2,1,0}, u8[0]{0}) customcall(f32[3,8192,33,33]{3,2,1,0}, f32[2048,128,31,31]{3,2,1,0}, f32[2048]{0 }), window={size=31x31}, dim_labels=bf01_oi01>bf01, feature_group_count=64, custom_call_target=""__cudnn$convBiasActivationForward"", backend_config={""conv_result_scale"":0.5,""activation_mode"":""kNone"",""side_input_scale "":0} ``` Unfortunately, I did not manage to extract a minimal reproducing example within two hours, so I have gone ahead and posted the issue anyway. Here is the complete stack trace for reference: ``` 20230907 03:07:24.380550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT                                                                                               ERROR:absl:Oryx not found! This library will still work but no summarywill be logged.                        gpu                                                                    /btherien/github/learned_optimization/learned_optimization/outer_trainers/truncation_schedule.py:95: UserWarning: Explicitly requested dtype  requested in asarray is not available, and will be  truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jaxcurrentgotchas for more.   length = jnp.asarray(jnp.exp(log_length), dtype=jnp.int64)                                                 ERROR:wandb.sdk.lib.gitlib:git root error: Cmd('git') failed due to: exit code(128)                            cmdline: git revparse showtoplevel                 stderr: 'fatal: detected dubious ownership in repository at          Outer Loop:   0% 6/5000 [25:10  A6000   RTX 8000   RTX 2080 ti ",2023-09-08T19:45:23Z,bug XLA needs info NVIDIA GPU,open,0,7,https://github.com/jax-ml/jax/issues/17523,"Can you try with a `jaxlib` built from head? There was at least one recent XLA change (https://github.com/openxla/xla/commit/0ef9d092689e767a431c01b332d94b76d66866c9) that should be included in a head jaxlib that may help. (We'll probably make a new release this week, also.)","Just built jax & jaxlib from source and installed them, within the docker image mentioned above, using the following steps:  ``` git clone https://github.com/google/jax cd jax git checkout HEAD puts me on main python build/build.py enable_cuda cuda_path /usr/local/cuda cudnn_path /usr/lib/x86_64linuxgnu pip install e . pip install dist/*.whl  ``` After the above steps I get ``` pip list | grep jax jax                          0.4.16.dev20230912 /btherien/github/jax jaxlib                       0.4.16.dev20230912 ``` However, the same error appears again (with some small differences in the message due to batch size): ``` 20230912 17:57:33.259257: W external/xla/xla/service/gpu/conv_algorithm_picker.cc:809] None of the algorithms provided by cuDNN heuristics worked; trying fallback algorithms. 20230912 17:57:33.259283: W external/xla/xla/service/gpu/conv_algorithm_picker.cc:812] Conv: (f32[3,512,3,3]{3,2,1,0}, u8[0]{0}) customcall(f32[3,2048,33,33]{3,2,1,0}, f32[512,128,31,31]{3,2,1,0}, f32[512]{0}), window={size=31x31}, dim_labels =bf01_oi01>bf01, feature_group_count=16, custom_call_target=""__cudnn$convBiasActivationForward"", backend_config={""conv_result_scale"":0.5,""activation_mode"":""kNone"",""side_input_scale"":0,""leakyrelu_alpha"":0} ```  should I checkout to a particular dev branch before building or specify an XLA version (e.g., following https://jax.readthedocs.io/en/latest/developer.htmlbuildingjaxlibfromsourcewithamodifiedxlarepository)?","It looks like what you did was correct. In that case, we'd need a reproduction of the problem. One way you could give us that is to share an XLA HLO dump. Set the environment variable `XLA_FLAGS=xla_dump_to=/somewhere` and zip the contents of `/somewhere` and attach them. Note this in effect shares your model, which you may or may not want to do. Up to you.","Any chance you can grab that HLO dump, as requested above?", Here are the logs and a screenshot of the warning output when they were produced. This was run on an RTX 3090 GPU. Let me know if you need any more information. xla_logs.zip ,Hmm. I couldn't reproduce on A100. Does this still reproduce for you with `jax` and `jaxlib` 0.4.20?,I get an identical error with 0.4.20  
13159,"以下是一个github上的jax下的一个issue, 标题是(Singular value decomposition JVP not implemented for full matrices/ workaround?)， 内容是 ( Description It seems that the Singular Value Decomposition JVP is not implemented for full matrices. The output of the function jax.linalg.svd is a tuple of three matrices (U, Sigma, Vh). Is there a plan to implement this feature or a work around? Appreciate any help. Reference: ``` Detailed error  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/resnet18_extended_mcrr_Haiku.py"", line 483, in      train_on_mnist(   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/resnet18_extended_mcrr_Haiku.py"", line 428, in train_on_mnist     params, opt_state, loss = train_step(params, state, opt_state, batch)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 253, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(                                                  ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 161, in _python_pjit_helper     args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(                                                  ^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/api.py"", line 324, in infer_params     return pjit.common_infer_params(pjit_info_args, *args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 491, in common_infer_params     jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(                                                       ^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 969, in _pjit_jaxpr     jaxpr, final_consts, out_type = _create_pjit_jaxpr(                                     ^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/linear_util.py"", line 345, in memoized_fun     ans = call(fun, *args)           ^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 922, in _create_pjit_jaxpr     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(                                ^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/resnet18_extended_mcrr_Haiku.py"", line 381, in train_step     grads, (loss, state) = jax.grad(get_loss, has_aux=True)(params, state, batch)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/api.py"", line 671, in grad_f_aux     (_, aux), g = value_and_grad_f(*args, **kwargs)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/api.py"", line 743, in value_and_grad_f     ans, vjp_py, aux = _vjp(                        ^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/api.py"", line 2241, in _vjp     out_primal, out_vjp, aux = ad.vjp(                                ^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 142, in vjp     out_primals, pvals, jaxpr, consts, aux = linearize(traceable, *primals, has_aux=True)                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 129, in linearize     jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 777, in trace_to_jaxpr_nounits     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)                                       ^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/resnet18_extended_mcrr_Haiku.py"", line 366, in get_loss     simpleVMCRRLoss = simpleVMCRR(                       ^^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/mcrr_simplify_v3.py"", line 355, in __call__     self.A, self.G = self.latch.A, self.latch.G = self.latch(                                                   ^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/latching_v2.py"", line 34, in __call__     return self.latching(*args, **kwds)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/latching_v2.py"", line 155, in latching     U, S, _ = jnp.linalg.svd(self.Z_diag_Pi_j_Z_T)  don't need Vh               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 253, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(                                                  ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 166, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 2596, in bind     return self.bind_with_trace(top_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 389, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 316, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1483, in _pjit_jvp     jaxpr_jvp, is_nz_tangents_out = ad.jvp_jaxpr(                                     ^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 700, in jvp_jaxpr     return _jvp_jaxpr(jaxpr, tuple(nonzeros), tuple(instantiate))            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 710, in _jvp_jaxpr     jaxpr_out, avals_out, literals_out = pe.trace_to_jaxpr_dynamic(f_jvp, avals_in)                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(                                ^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 235, in jaxpr_as_fun     return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 454, in eval_jaxpr     ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 386, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 389, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 316, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py"", line 81, in inner     return func(*args, **kwds)            ^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/lax/linalg.py"", line 1736, in _svd_jvp_rule     raise NotImplementedError( jax._src.traceback_util.UnfilteredStackTrace: NotImplementedError: Singular value decomposition JVP not implemented for full matrices The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, MacOS 11.7.9, Jax 0.4.14, optax 0.1.7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Singular value decomposition JVP not implemented for full matrices/ workaround?," Description It seems that the Singular Value Decomposition JVP is not implemented for full matrices. The output of the function jax.linalg.svd is a tuple of three matrices (U, Sigma, Vh). Is there a plan to implement this feature or a work around? Appreciate any help. Reference: ``` Detailed error  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/resnet18_extended_mcrr_Haiku.py"", line 483, in      train_on_mnist(   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/resnet18_extended_mcrr_Haiku.py"", line 428, in train_on_mnist     params, opt_state, loss = train_step(params, state, opt_state, batch)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 253, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(                                                  ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 161, in _python_pjit_helper     args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(                                                  ^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/api.py"", line 324, in infer_params     return pjit.common_infer_params(pjit_info_args, *args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 491, in common_infer_params     jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(                                                       ^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 969, in _pjit_jaxpr     jaxpr, final_consts, out_type = _create_pjit_jaxpr(                                     ^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/linear_util.py"", line 345, in memoized_fun     ans = call(fun, *args)           ^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 922, in _create_pjit_jaxpr     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(                                ^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/resnet18_extended_mcrr_Haiku.py"", line 381, in train_step     grads, (loss, state) = jax.grad(get_loss, has_aux=True)(params, state, batch)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/api.py"", line 671, in grad_f_aux     (_, aux), g = value_and_grad_f(*args, **kwargs)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/api.py"", line 743, in value_and_grad_f     ans, vjp_py, aux = _vjp(                        ^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/api.py"", line 2241, in _vjp     out_primal, out_vjp, aux = ad.vjp(                                ^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 142, in vjp     out_primals, pvals, jaxpr, consts, aux = linearize(traceable, *primals, has_aux=True)                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 129, in linearize     jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 777, in trace_to_jaxpr_nounits     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)                                       ^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/resnet18_extended_mcrr_Haiku.py"", line 366, in get_loss     simpleVMCRRLoss = simpleVMCRR(                       ^^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/mcrr_simplify_v3.py"", line 355, in __call__     self.A, self.G = self.latch.A, self.latch.G = self.latch(                                                   ^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/latching_v2.py"", line 34, in __call__     return self.latching(*args, **kwds)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/pushkarmerwah/belugaangular/vision_transformer/sandbox/latching_v2.py"", line 155, in latching     U, S, _ = jnp.linalg.svd(self.Z_diag_Pi_j_Z_T)  don't need Vh               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 253, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(                                                  ^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 166, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 2596, in bind     return self.bind_with_trace(top_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 389, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 316, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1483, in _pjit_jvp     jaxpr_jvp, is_nz_tangents_out = ad.jvp_jaxpr(                                     ^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 700, in jvp_jaxpr     return _jvp_jaxpr(jaxpr, tuple(nonzeros), tuple(instantiate))            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 710, in _jvp_jaxpr     jaxpr_out, avals_out, literals_out = pe.trace_to_jaxpr_dynamic(f_jvp, avals_in)                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(                                ^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 235, in jaxpr_as_fun     return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 454, in eval_jaxpr     ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 386, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/core.py"", line 389, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 316, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py"", line 81, in inner     return func(*args, **kwds)            ^^^^^^^^^^^^^^^^^^^   File ""/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/sitepackages/jax/_src/lax/linalg.py"", line 1736, in _svd_jvp_rule     raise NotImplementedError( jax._src.traceback_util.UnfilteredStackTrace: NotImplementedError: Singular value decomposition JVP not implemented for full matrices The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, MacOS 11.7.9, Jax 0.4.14, optax 0.1.7  NVIDIA GPU info _No response_",2023-09-08T16:16:28Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/17518,"Hi  thanks for the report! This is a longstanding known issue; the problem is that for full matrices, the gradients are not welldefined. You can read more in https://github.com/google/jax/issues/508 I'm going to close this as a duplicate of CC(Implement JVP for SVD when full_matrices=True). Thanks!"
10018,"以下是一个github上的jax下的一个issue, 标题是(JAX uses the first CUDA libraries found in `LD_LIBRARY_PATH` instead of the bundled CUDA libraries installed by pip)， 内容是 ( Description  Problem  Description Most CUDAenabled systems tend to have CUDA libraries' directory paths added to `LD_LIBRARY_PATH` automatically by a `~/.bashrc` script, or similar. When installed through the pip installation option with CUDA installed via pip, JAX doesn't use the CUDA libraries installed by pip. Instead, JAX uses whatever CUDA libraries it finds first when searching through the directories specified in the `LD_LIBRARY_PATH` variable. This is confusing, as when I use the pip installation option that also installs CUDA, I expect JAX to use the bundled CUDA instead of any other, possibly older and incompatible, CUDA version I have on the system. Notably, PyTorch with CUDA installation handles this case as expected, see details below.  Expected result  If JAX was installed using the pip installation option that also installs CUDA, then JAX should use the CUDA libraries that were installed by pip during the JAX installation, disregarding the value of the `LD_LIBRARY_PATH` variable.  If JAX was installed using the pip installation option without CUDA, then JAX should use the first CUDA libraries it finds when searching through the directories in the `LD_LIBRARY_PATH` variable.  Reproduction As I understand, CUDA libraries can come from the driver installation or the toolkit installation:  `libcuda.so`: CUDA Driver API library. Installed by the NVIDIA driver installation. It is **not** included with the CUDA Toolkit distributed with JAX.  `libcudart.so`: CUDA Runtime API library. It is included with the CUDA Toolkit distributed with JAX. The problem is that JAX ignores `libcudart.so` that was installed by pip and uses the first `libcudart.so` it finds on `LD_LIBRARY_PATH`. Create a clean Python 3.10 conda environment and activate it: ```bash (base) olokshyn:~$ conda create n testjax python=3.10 y (base) olokshyn:~$ conda activate testjax ``` Install JAX with CUDA 12 following the pip installation: GPU (CUDA, installed via pip, easier) flow: ```bash pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```    Installation output ```bash (testjax) olokshyn:~$ pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda12_pip]   Using cached jax0.4.14py3noneany.whl Collecting mldtypes>=0.2.0 (from jax[cuda12_pip])   Obtaining dependency information for mldtypes>=0.2.0 from https://files.pythonhosted.org/packages/d1/1d/d5cf76e5e40f69dbd273036e3172ae4a614577cb141673427b80cac948df/ml_dtypes0.2.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata   Using cached ml_dtypes0.2.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB) Collecting numpy>=1.22 (from jax[cuda12_pip])   Obtaining dependency information for numpy>=1.22 from https://files.pythonhosted.org/packages/71/3c/3b1981c6a1986adc9ee7db760c0c34ea5b14ac3da9ecfcf1ea2a4ec6c398/numpy1.25.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata   Using cached numpy1.25.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB) Collecting opteinsum (from jax[cuda12_pip])   Using cached opt_einsum3.3.0py3noneany.whl (65 kB) Collecting scipy>=1.7 (from jax[cuda12_pip])   Obtaining dependency information for scipy>=1.7 from https://files.pythonhosted.org/packages/a8/cc/c36f3439f5d47c3b13833ce6687b43a040cc7638c502ac46b41e2d4f3d6f/scipy1.11.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata   Using cached scipy1.11.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB) Collecting jaxlib==0.4.14+cuda12.cudnn89 (from jax[cuda12_pip])   Using cached https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.14%2Bcuda12.cudnn89cp310cp310manylinux2014_x86_64.whl (191.3 MB) Collecting nvidiacublascu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacublascu12 from https://files.pythonhosted.org/packages/b6/6a/e8cca34f85b18a0280e3a19faca1923f6a04e7d587e9d8e33bc295a52b6d/nvidia_cublas_cu1212.2.5.6py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cublas_cu1212.2.5.6py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidiacudacupticu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacudacupticu12 from https://files.pythonhosted.org/packages/cf/27/7c2f33b4fbab658117fce3b029f44cce7886fc5a50f526a81b4b0436af02/nvidia_cuda_cupti_cu1212.2.142py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cuda_cupti_cu1212.2.142py3nonemanylinux1_x86_64.whl.metadata (1.6 kB) Collecting nvidiacudanvcccu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacudanvcccu12 from https://files.pythonhosted.org/packages/a7/a5/1b48eeda9bdc3ac5bf00d84eca6f31b568ab3da9008f754bf1fdd98ee97b/nvidia_cuda_nvcc_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cuda_nvcc_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidiacudaruntimecu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacudaruntimecu12 from https://files.pythonhosted.org/packages/95/46/6361d45c7a6fe3b3bb8d5fa35eb43c1dcd12d14799a0dc6faef3d76eaf41/nvidia_cuda_runtime_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cuda_runtime_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidiacudnncu12>=8.9 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacudnncu12>=8.9 from https://files.pythonhosted.org/packages/fa/d7/f46bd08337201ec875bda25e29fcb65d26bdc6d0be306dc33fc0b092faa6/nvidia_cudnn_cu128.9.4.25py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cudnn_cu128.9.4.25py3nonemanylinux1_x86_64.whl.metadata (1.6 kB) Collecting nvidiacufftcu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacufftcu12 from https://files.pythonhosted.org/packages/8f/86/311c32926aa321ebc169d6c910a1ecd30339ae198014e0d4a7cf448f6ce9/nvidia_cufft_cu1211.0.8.103py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cufft_cu1211.0.8.103py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidiacusolvercu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacusolvercu12 from https://files.pythonhosted.org/packages/1f/af/29126e0bae18bad59fe4018c2e32fd5d7ec0aa354a4b1e08c051515a25d1/nvidia_cusolver_cu1211.5.2.141py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cusolver_cu1211.5.2.141py3nonemanylinux1_x86_64.whl.metadata (1.6 kB) Collecting nvidiacusparsecu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacusparsecu12 from https://files.pythonhosted.org/packages/a9/b4/21af603eba3a803276bbcbbc41d6cf3a20919a2807217084cacef13ec779/nvidia_cusparse_cu1212.1.2.141py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cusparse_cu1212.1.2.141py3nonemanylinux1_x86_64.whl.metadata (1.6 kB) Collecting nvidiacudanvrtccu12 (from nvidiacudnncu12>=8.9>jax[cuda12_pip])   Obtaining dependency information for nvidiacudanvrtccu12 from https://files.pythonhosted.org/packages/cb/84/bb975ca63e3d4017824ad3d62e141db8492bc53cec6dc84705fb0650c22d/nvidia_cuda_nvrtc_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cuda_nvrtc_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidianvjitlinkcu12 (from nvidiacusolvercu12>jax[cuda12_pip])   Obtaining dependency information for nvidianvjitlinkcu12 from https://files.pythonhosted.org/packages/0a/f8/5193b57555cbeecfdb6ade643df0d4218cc6385485492b6e2f64ceae53bb/nvidia_nvjitlink_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_nvjitlink_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Using cached ml_dtypes0.2.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB) Using cached numpy1.25.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB) Using cached nvidia_cudnn_cu128.9.4.25py3nonemanylinux1_x86_64.whl (720.1 MB) Using cached scipy1.11.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB) Using cached nvidia_cublas_cu1212.2.5.6py3nonemanylinux1_x86_64.whl (417.8 MB) Using cached nvidia_cuda_cupti_cu1212.2.142py3nonemanylinux1_x86_64.whl (13.9 MB) Using cached nvidia_cuda_nvcc_cu1212.2.140py3nonemanylinux1_x86_64.whl (21.3 MB) Using cached nvidia_cuda_runtime_cu1212.2.140py3nonemanylinux1_x86_64.whl (845 kB) Using cached nvidia_cufft_cu1211.0.8.103py3nonemanylinux1_x86_64.whl (98.6 MB) Using cached nvidia_cusolver_cu1211.5.2.141py3nonemanylinux1_x86_64.whl (124.9 MB) Using cached nvidia_cusparse_cu1212.1.2.141py3nonemanylinux1_x86_64.whl (195.3 MB) Using cached nvidia_cuda_nvrtc_cu1212.2.140py3nonemanylinux1_x86_64.whl (23.4 MB) Using cached nvidia_nvjitlink_cu1212.2.140py3nonemanylinux1_x86_64.whl (20.2 MB) Installing collected packages: nvidianvjitlinkcu12, nvidiacufftcu12, nvidiacudaruntimecu12, nvidiacudanvrtccu12, nvidiacudanvcccu12, nvidiacudacupticu12, nvidiacublascu12, numpy, scipy, opteinsum, nvidiacusparsecu12, nvidiacudnncu12, mldtypes, nvidiacusolvercu12, jaxlib, jax Successfully installed jax0.4.14 jaxlib0.4.14+cuda12.cudnn89 mldtypes0.2.0 numpy1.25.2 nvidiacublascu1212.2.5.6 nvidiacudacupticu1212.2.142 nvidiacudanvcccu1212.2.140 nvidiacudanvrtccu1212.2.140 nvidiacudaruntimecu1212.2.140 nvidiacudnncu128.9.4.25 nvidiacufftcu1211.0.8.103 nvidiacusolvercu1211.5.2.141 nvidiacusparsecu1212.1.2.141 nvidianvjitlinkcu1212.2.140 opteinsum3.3.0 scipy1.11.2 ```  Check that JAX uses the bundled `libcudart.so` when `LD_LIBRARY_PATH` is empty: ```bash (testjax) olokshyn:~$ export LD_LIBRARY_PATH="""" (testjax) olokshyn:~$ strace f python c ""import jax; jax.numpy.array([1, 2, 3]);"" 2>&1  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX uses the first CUDA libraries found in `LD_LIBRARY_PATH` instead of the bundled CUDA libraries installed by pip," Description  Problem  Description Most CUDAenabled systems tend to have CUDA libraries' directory paths added to `LD_LIBRARY_PATH` automatically by a `~/.bashrc` script, or similar. When installed through the pip installation option with CUDA installed via pip, JAX doesn't use the CUDA libraries installed by pip. Instead, JAX uses whatever CUDA libraries it finds first when searching through the directories specified in the `LD_LIBRARY_PATH` variable. This is confusing, as when I use the pip installation option that also installs CUDA, I expect JAX to use the bundled CUDA instead of any other, possibly older and incompatible, CUDA version I have on the system. Notably, PyTorch with CUDA installation handles this case as expected, see details below.  Expected result  If JAX was installed using the pip installation option that also installs CUDA, then JAX should use the CUDA libraries that were installed by pip during the JAX installation, disregarding the value of the `LD_LIBRARY_PATH` variable.  If JAX was installed using the pip installation option without CUDA, then JAX should use the first CUDA libraries it finds when searching through the directories in the `LD_LIBRARY_PATH` variable.  Reproduction As I understand, CUDA libraries can come from the driver installation or the toolkit installation:  `libcuda.so`: CUDA Driver API library. Installed by the NVIDIA driver installation. It is **not** included with the CUDA Toolkit distributed with JAX.  `libcudart.so`: CUDA Runtime API library. It is included with the CUDA Toolkit distributed with JAX. The problem is that JAX ignores `libcudart.so` that was installed by pip and uses the first `libcudart.so` it finds on `LD_LIBRARY_PATH`. Create a clean Python 3.10 conda environment and activate it: ```bash (base) olokshyn:~$ conda create n testjax python=3.10 y (base) olokshyn:~$ conda activate testjax ``` Install JAX with CUDA 12 following the pip installation: GPU (CUDA, installed via pip, easier) flow: ```bash pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```    Installation output ```bash (testjax) olokshyn:~$ pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda12_pip]   Using cached jax0.4.14py3noneany.whl Collecting mldtypes>=0.2.0 (from jax[cuda12_pip])   Obtaining dependency information for mldtypes>=0.2.0 from https://files.pythonhosted.org/packages/d1/1d/d5cf76e5e40f69dbd273036e3172ae4a614577cb141673427b80cac948df/ml_dtypes0.2.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata   Using cached ml_dtypes0.2.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB) Collecting numpy>=1.22 (from jax[cuda12_pip])   Obtaining dependency information for numpy>=1.22 from https://files.pythonhosted.org/packages/71/3c/3b1981c6a1986adc9ee7db760c0c34ea5b14ac3da9ecfcf1ea2a4ec6c398/numpy1.25.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata   Using cached numpy1.25.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB) Collecting opteinsum (from jax[cuda12_pip])   Using cached opt_einsum3.3.0py3noneany.whl (65 kB) Collecting scipy>=1.7 (from jax[cuda12_pip])   Obtaining dependency information for scipy>=1.7 from https://files.pythonhosted.org/packages/a8/cc/c36f3439f5d47c3b13833ce6687b43a040cc7638c502ac46b41e2d4f3d6f/scipy1.11.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata   Using cached scipy1.11.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (59 kB) Collecting jaxlib==0.4.14+cuda12.cudnn89 (from jax[cuda12_pip])   Using cached https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.14%2Bcuda12.cudnn89cp310cp310manylinux2014_x86_64.whl (191.3 MB) Collecting nvidiacublascu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacublascu12 from https://files.pythonhosted.org/packages/b6/6a/e8cca34f85b18a0280e3a19faca1923f6a04e7d587e9d8e33bc295a52b6d/nvidia_cublas_cu1212.2.5.6py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cublas_cu1212.2.5.6py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidiacudacupticu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacudacupticu12 from https://files.pythonhosted.org/packages/cf/27/7c2f33b4fbab658117fce3b029f44cce7886fc5a50f526a81b4b0436af02/nvidia_cuda_cupti_cu1212.2.142py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cuda_cupti_cu1212.2.142py3nonemanylinux1_x86_64.whl.metadata (1.6 kB) Collecting nvidiacudanvcccu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacudanvcccu12 from https://files.pythonhosted.org/packages/a7/a5/1b48eeda9bdc3ac5bf00d84eca6f31b568ab3da9008f754bf1fdd98ee97b/nvidia_cuda_nvcc_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cuda_nvcc_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidiacudaruntimecu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacudaruntimecu12 from https://files.pythonhosted.org/packages/95/46/6361d45c7a6fe3b3bb8d5fa35eb43c1dcd12d14799a0dc6faef3d76eaf41/nvidia_cuda_runtime_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cuda_runtime_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidiacudnncu12>=8.9 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacudnncu12>=8.9 from https://files.pythonhosted.org/packages/fa/d7/f46bd08337201ec875bda25e29fcb65d26bdc6d0be306dc33fc0b092faa6/nvidia_cudnn_cu128.9.4.25py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cudnn_cu128.9.4.25py3nonemanylinux1_x86_64.whl.metadata (1.6 kB) Collecting nvidiacufftcu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacufftcu12 from https://files.pythonhosted.org/packages/8f/86/311c32926aa321ebc169d6c910a1ecd30339ae198014e0d4a7cf448f6ce9/nvidia_cufft_cu1211.0.8.103py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cufft_cu1211.0.8.103py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidiacusolvercu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacusolvercu12 from https://files.pythonhosted.org/packages/1f/af/29126e0bae18bad59fe4018c2e32fd5d7ec0aa354a4b1e08c051515a25d1/nvidia_cusolver_cu1211.5.2.141py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cusolver_cu1211.5.2.141py3nonemanylinux1_x86_64.whl.metadata (1.6 kB) Collecting nvidiacusparsecu12 (from jax[cuda12_pip])   Obtaining dependency information for nvidiacusparsecu12 from https://files.pythonhosted.org/packages/a9/b4/21af603eba3a803276bbcbbc41d6cf3a20919a2807217084cacef13ec779/nvidia_cusparse_cu1212.1.2.141py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cusparse_cu1212.1.2.141py3nonemanylinux1_x86_64.whl.metadata (1.6 kB) Collecting nvidiacudanvrtccu12 (from nvidiacudnncu12>=8.9>jax[cuda12_pip])   Obtaining dependency information for nvidiacudanvrtccu12 from https://files.pythonhosted.org/packages/cb/84/bb975ca63e3d4017824ad3d62e141db8492bc53cec6dc84705fb0650c22d/nvidia_cuda_nvrtc_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_cuda_nvrtc_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Collecting nvidianvjitlinkcu12 (from nvidiacusolvercu12>jax[cuda12_pip])   Obtaining dependency information for nvidianvjitlinkcu12 from https://files.pythonhosted.org/packages/0a/f8/5193b57555cbeecfdb6ade643df0d4218cc6385485492b6e2f64ceae53bb/nvidia_nvjitlink_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata   Using cached nvidia_nvjitlink_cu1212.2.140py3nonemanylinux1_x86_64.whl.metadata (1.5 kB) Using cached ml_dtypes0.2.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB) Using cached numpy1.25.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB) Using cached nvidia_cudnn_cu128.9.4.25py3nonemanylinux1_x86_64.whl (720.1 MB) Using cached scipy1.11.2cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB) Using cached nvidia_cublas_cu1212.2.5.6py3nonemanylinux1_x86_64.whl (417.8 MB) Using cached nvidia_cuda_cupti_cu1212.2.142py3nonemanylinux1_x86_64.whl (13.9 MB) Using cached nvidia_cuda_nvcc_cu1212.2.140py3nonemanylinux1_x86_64.whl (21.3 MB) Using cached nvidia_cuda_runtime_cu1212.2.140py3nonemanylinux1_x86_64.whl (845 kB) Using cached nvidia_cufft_cu1211.0.8.103py3nonemanylinux1_x86_64.whl (98.6 MB) Using cached nvidia_cusolver_cu1211.5.2.141py3nonemanylinux1_x86_64.whl (124.9 MB) Using cached nvidia_cusparse_cu1212.1.2.141py3nonemanylinux1_x86_64.whl (195.3 MB) Using cached nvidia_cuda_nvrtc_cu1212.2.140py3nonemanylinux1_x86_64.whl (23.4 MB) Using cached nvidia_nvjitlink_cu1212.2.140py3nonemanylinux1_x86_64.whl (20.2 MB) Installing collected packages: nvidianvjitlinkcu12, nvidiacufftcu12, nvidiacudaruntimecu12, nvidiacudanvrtccu12, nvidiacudanvcccu12, nvidiacudacupticu12, nvidiacublascu12, numpy, scipy, opteinsum, nvidiacusparsecu12, nvidiacudnncu12, mldtypes, nvidiacusolvercu12, jaxlib, jax Successfully installed jax0.4.14 jaxlib0.4.14+cuda12.cudnn89 mldtypes0.2.0 numpy1.25.2 nvidiacublascu1212.2.5.6 nvidiacudacupticu1212.2.142 nvidiacudanvcccu1212.2.140 nvidiacudanvrtccu1212.2.140 nvidiacudaruntimecu1212.2.140 nvidiacudnncu128.9.4.25 nvidiacufftcu1211.0.8.103 nvidiacusolvercu1211.5.2.141 nvidiacusparsecu1212.1.2.141 nvidianvjitlinkcu1212.2.140 opteinsum3.3.0 scipy1.11.2 ```  Check that JAX uses the bundled `libcudart.so` when `LD_LIBRARY_PATH` is empty: ```bash (testjax) olokshyn:~$ export LD_LIBRARY_PATH="""" (testjax) olokshyn:~$ strace f python c ""import jax; jax.numpy.array([1, 2, 3]);"" 2>&1  ++ ```",2023-09-07T16:37:40Z,bug,closed,1,6,https://github.com/jax-ml/jax/issues/17497,"I'm not sure I consider this a bug. If you're interested in why this happens: JAX sets a field called `RUNPATH` on its shared libraries, which specifies an additional search path for dependent shared libraries. Notably `RUNPATH` comes lower in the search priority order than the `LD_LIBRARY_PATH` environment variable. This is arguably a good thing: it allows users and system administrators to override the choice of libraries if they want. There is another field that can be set instead called `RPATH`, which also adds an additional search path, but does so at a *higher* priority than the `LD_LIBRARY_PATH` environment variable. I would guess (but have not personally confirmed) that is what PyTorch is setting. There is no mechanism to override an `RPATH`, even if you wanted to, bar tools like `patchelf`. JAX on CUDA can be installed in two modes: `cuda12_pip`, which installs the CUDA dependencies as `pip` packages, and `cuda12_local`, which does not and relies on the user having the right libraries installed. The `jaxlib` binaries we ship is the same in both cases. For `jaxlib` to be able to locate the `pip`installed NVIDIA libraries, `jaxlib` sets `RUNPATH` on its shared libraries to the relative location of the CUDA libraries when `pip`installed. If the `pip` packages aren't installed, this likely doesn't do much because the libraries won't be present: we'll fall back searching `LD_LIBRARY_PATH`. It would be possible to change `jax` to set `RPATH` instead of `RUNPATH`, but the catch would be that then if the CUDA `pip` packages were installed, then we would use them always and there would be no way to override that choice for development or by a system administrator. If you had the CUDA `pip` packages installed but needed to use another CUDA installation, you'd be forced to uninstall the CUDA pip packages. `RPATH` is also deprecated, according to some authorities: https://akkadia.org/drepper/dsohowto.pdf Is there a reason that it doesn't suffice simply not to set `LD_LIBRARY_PATH`? i.e., just don't override JAX's choice? That's the point of `LD_LIBRARY_PATH`  to override the app's choice!"," thank for the detailed explanation! This is insightful indeed. > Is there a reason that it doesn't suffice simply not to set LD_LIBRARY_PATH? In most cases, setting `LD_LIBRARY_PATH` to an empty string should be an acceptable solution. However, there is an important scenario where it will not work: when JAX is used together with another CUDA and `LD_LIBRARY_PATH` dependent library in the same process: 1. A single project requires both JAX and TensorFlow in a single process. 2. TensorFlow uses CUDA toolkit 11.8 that is specified in `LD_LIBRARY_PATH`. 3. JAX is supposed to use the bundled CUDA 12.2 installed by pip during the JAX's installation. 4. There is a script that uses both JAX and TensorFlow simultaneously. This is likely because 1) we may need `tensorflow_datasets` to load data for JAX models or 2) an older part of the codebase is implemented in Tensorflow and the newer in JAX, and we want to bridge them together. 5. Since TensorFlow and JAX need incompatible CUDA versions in the same process with the same `LD_LIBRARY_PATH` variable, there is no way the beforementioned script could work. JAX would be confused by `LD_LIBRARY_PATH` and would try to use an older 11.8 CUDA. There are two possible solutions: either split it into two separate processes, or find versions of TF and JAX that use the same CUDA version and point `LD_LIBRARY_PATH` to it. 6. However, had JAX always used the bundled CUDA version, this use case would've just worked. My main point is that the current dependency on `LD_LIBRARY_PATH` of the library that comes with bundled CUDA clearly causes confusion. Although, I am not sure that behavioral change is strictly necessary here. Apparently, the easiest way to resolve this confusion is by adding a warning to the pip installation: GPU (CUDA, installed via pip, easier) section that goes something like this: > **Warning**: Ensure your `LD_LIBRARY_PATH` environment variable does not include paths that contain CUDA libraries. Otherwise, JAX will use CUDA libraries from `LD_LIBRARY_PATH` instead of the CUDA libraries bundled with JAX and installed by pip. Do you think this is a feasible solution? In addition, it would be good to provide some guidance for people who want to get JAX and TensorFlow working in the same process, albeit with different CUDA versions. Especially for teams that are evaluating JAX alongside TF. Another point is convenience. When switching to a conda environment with JAX we need to remember to clear the `LD_LIBRARY_PATH` variable, which is an additional step that will frequently be forgotten. > It would be possible to change jax to set RPATH instead of RUNPATH, but the catch would be that then if the CUDA pip packages were installed, then we would use them always and there would be no way to override that choice for development or by a system administrator. If you had the CUDA pip packages installed but needed to use another CUDA installation, you'd be forced to uninstall the CUDA pip packages. The need to switch from the bundled CUDA to another CUDA from a single JAX installation seems unlikely to me. JAX is usually installed within a conda or a virtualenv environment. Thus, it's easier to create two conda environments with different setups of JAX and switch between them.","I agree that the lack of clarity is the main issue here. The fact the JAX behaves differently from PyTorch in such a common scenario is already a reason to think about what's going on. I also believe that since there are already two packages provided (i.e., pip and local) the user has already the ability to chose the behaviour of JAX with regard to LD_LIBRARY_PATH and so, maybe, the pip version shouldn't be affected by its value (if I'm a new user and I download the ""easytouse"" cuda1x_pip version I expect it to work out of the box, independently of how the system administrator configured my machine).","In my case I had to import jax first and do an initial computation before importing tensorflow, otherwise tensorflow would set the wrong cudnn version and I would get `jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed` afterwards","I'll note that *none* of these things will help if you try to import two libraries that want different CUDA/CuDNN versions into the same process. Irrespective of how we do things, whichever library you import first will ""win"". If you just want TF for a data pipieline, I recommend the `tensorflowcpu` package!","I suspect that https://github.com/google/jax/commit/9404518201c5ac8af6c85ecdf12a7cc34c102585 will help catch this case. We will now raise an exception if we detect that the versions of the CUDA libraries loaded are too old, which would happen if you override the `LD_LIBRARY_PATH` to point to an old version."
308,"以下是一个github上的jax下的一个issue, 标题是([Mosaic] apply_vector_layout C++ rewrite (13): scf.if, scf.yield)， 内容是 ([Mosaic] apply_vector_layout C++ rewrite (13): scf.if, scf.yield)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[Mosaic] apply_vector_layout C++ rewrite (13): scf.if, scf.yield","[Mosaic] apply_vector_layout C++ rewrite (13): scf.if, scf.yield",2023-09-07T07:08:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17488
332,"以下是一个github上的jax下的一个issue, 标题是(Improve the gpu lowering error message if users forget link the gpu library.)， 内容是 (Improve the gpu lowering error message if users forget link the gpu library.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve the gpu lowering error message if users forget link the gpu library.,Improve the gpu lowering error message if users forget link the gpu library.,2023-09-06T20:24:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17475
2414,"以下是一个github上的jax下的一个issue, 标题是(lax.conv_transpose takes FOREVER to compile)， 内容是 ( Description I initially submitted the issue here: https://github.com/deepmind/dmhaiku/issues/724 But then realized it was a jax issue. In short,  I've been trying to use Conv2DTranspose in my model, and even for very simple case... it takes forever to compile. ```python import jax from jax import lax, random import jax.numpy as jnp import time  Directly implement the Conv2DTranspose in JAX def toy_model_jax(x, params):     return lax.conv_transpose(x, params[""kernel""], strides=(16, 16), padding=""VALID"")  Initialize parameters for the toy model def initialize_params(key):     kernel_shape = (32, 32, 128, 32)   (height, width, in_channels, out_channels)     kernel = random.normal(key, kernel_shape)     return {""kernel"": kernel}  Generate random input and params start_time = time.time() key = random.PRNGKey(42) x = random.normal(key, (1, 8, 8, 128)) params = initialize_params(key) end_time = time.time() print(f""Initialization Run Time: {end_time  start_time:.6f} seconds"")  JITcompile and time the model run toy_model_jax_jitted = jax.jit(toy_model_jax)  Time the model compilation start_time = time.time()  Warmup call (this compiles the function) _ = toy_model_jax_jitted(x, params) end_time = time.time() print(f""JAX Compilation Time: {end_time  start_time:.6f} seconds"")  Time the model run start_time = time.time() o = toy_model_jax_jitted(x, params) print(""input_shape"", x.shape) print(""output_shape"", o.shape) end_time = time.time() print(f""JITted Run Time: {end_time  start_time:.6f} seconds"") ``` output ``` Initialization Run Time: 2.540971 seconds JAX Compilation Time: 251.976538 seconds input_shape (1, 8, 8, 128) output_shape (1, 144, 144, 32) JITted Run Time: 0.001842 seconds ``` For comparison, here is the pytorch: ``` Initialization Time: 0.033582 seconds input_shape torch.Size([1, 128, 8, 8]) output_shape torch.Size([1, 32, 144, 144]) Run Time: 0.047478 seconds ``` Google colab notebook: https://colab.research.google.com/drive/15YkOuK0EjqZdBNaXpF2wpYexGqtjZjLr  What jax/jaxlib version are you using? Google Colab  Which accelerator(s) are you using? GPU  Additional system info Google Colab  NVIDIA GPU info ``` Wed Sep  6 14:21:28 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,lax.conv_transpose takes FOREVER to compile," Description I initially submitted the issue here: https://github.com/deepmind/dmhaiku/issues/724 But then realized it was a jax issue. In short,  I've been trying to use Conv2DTranspose in my model, and even for very simple case... it takes forever to compile. ```python import jax from jax import lax, random import jax.numpy as jnp import time  Directly implement the Conv2DTranspose in JAX def toy_model_jax(x, params):     return lax.conv_transpose(x, params[""kernel""], strides=(16, 16), padding=""VALID"")  Initialize parameters for the toy model def initialize_params(key):     kernel_shape = (32, 32, 128, 32)   (height, width, in_channels, out_channels)     kernel = random.normal(key, kernel_shape)     return {""kernel"": kernel}  Generate random input and params start_time = time.time() key = random.PRNGKey(42) x = random.normal(key, (1, 8, 8, 128)) params = initialize_params(key) end_time = time.time() print(f""Initialization Run Time: {end_time  start_time:.6f} seconds"")  JITcompile and time the model run toy_model_jax_jitted = jax.jit(toy_model_jax)  Time the model compilation start_time = time.time()  Warmup call (this compiles the function) _ = toy_model_jax_jitted(x, params) end_time = time.time() print(f""JAX Compilation Time: {end_time  start_time:.6f} seconds"")  Time the model run start_time = time.time() o = toy_model_jax_jitted(x, params) print(""input_shape"", x.shape) print(""output_shape"", o.shape) end_time = time.time() print(f""JITted Run Time: {end_time  start_time:.6f} seconds"") ``` output ``` Initialization Run Time: 2.540971 seconds JAX Compilation Time: 251.976538 seconds input_shape (1, 8, 8, 128) output_shape (1, 144, 144, 32) JITted Run Time: 0.001842 seconds ``` For comparison, here is the pytorch: ``` Initialization Time: 0.033582 seconds input_shape torch.Size([1, 128, 8, 8]) output_shape torch.Size([1, 32, 144, 144]) Run Time: 0.047478 seconds ``` Google colab notebook: https://colab.research.google.com/drive/15YkOuK0EjqZdBNaXpF2wpYexGqtjZjLr  What jax/jaxlib version are you using? Google Colab  Which accelerator(s) are you using? GPU  Additional system info Google Colab  NVIDIA GPU info ``` Wed Sep  6 14:21:28 2023        ++  ++ ```",2023-09-06T14:21:47Z,bug XLA NVIDIA GPU,open,0,3,https://github.com/jax-ml/jax/issues/17464,I think this in turn is an XLA bug. Opened https://github.com/openxla/xla/issues/5541.,This is apparently due to convolution autotuning: some of the algorithms in cudnn are very slow and we try them all during autotuning. Once autotuning has run we will choose a fast algorithm.,"It seems in this case the same algorithms are returned by heuristics_mode_a and heuristics_mode_b. So when we deduplicate the algorithms to try during autotuning, we can half the compile time. That still means it is slow, but it is a step in the right direction. There is an idea how to potentially speed it up more by stopping an autotuning attempt if the best known runtime is already exceeded, but that will take a bit longer to implement."
5341,"以下是一个github上的jax下的一个issue, 标题是(build: write appropriate version strings to build artifacts)， 内容是 ( Description The goal of this PR is to address CC(`pip install jax jaxlib` broken with 0.4.15), the issue that led us to yank the 0.4.15 release. The desired endstate are for `jax.__version__` and `jaxlib.__version__` to satisfy the following (keeping PEP 440 in mind):  when importing or installing jax/jaxlib from the github source tree, the version looks like `0.4.16.dev20230906+g5227f1d14` where `20230906` is the date of the most recent commit, and `g5227f1d14` is the commit hash prefix.  when building the jax/jaxlib nightly release, the resulting source distribution and wheel files have the date of build (not date of commit) encoded in the version, like `0.4.16.dev20230906`  when building jax/jaxlib releases, the resulting source distribution and wheel files have clean release versions that look like `0.4.16`. I looked into the possibility of using `versioneer` or `miniver` for this, but they seem to not be well set up for repositories with multiple packages (`jax` and `jaxlib`), and also use mechanisms that are not compatible with our internal filesystem architecture. So instead I plan to roll our own solution. The general strategy here:  put a placeholder `_release_version = None` in `jax/version.py`  add utilities to determine a version string at build time based on environment variables:    if `JAX_RELEASE` or `JAXLIB_RELEASE` are set, set the string to the clean version (e.g. `'0.4.16'`)    if `JAX_NIGHTLY` or `JAXLIB_NIGHTLY` are set, set the string to a build date (e.g. `'0.4.16.dev20230906'`)    otherwise, set the string to a git commit date & hash (e.g. `0.4.16.dev20230906+g4e32a7683`), falling back to the nightly behavior if outside the git source tree.  when importing from source, if `_release_version is None` then dynamically generate the appropriate version at runtime from the current git hash.  add a setuptools `cmdclass` that updates the placeholder in source distributions and wheels with an appropriate hardcoded version string determined at build time.  Testing  `jax` source distributions I tested that JAX source distributions have the expected versions: ``` $ python setup.py sdist ... creating jax0.4.16.dev20230906+g4e32a7683 ... $ JAX_RELEASE=1 python setup.py sdist ... creating jax0.4.16 ... $ JAX_NIGHTLY=1 python setup.py sdist ... creating jax0.4.16.dev20230906 ... ``` I then confirmed that code installed from the source distribution has the matching version at runtime.  `jax` wheel builds Also confirmed that JAX wheels have the correct versions at runtime: ``` $ python m build ... Successfully built jax0.4.16.dev20230906+ge7f064ab4.tar.gz and jax0.4.16.dev20230906+ge7f064ab4py3noneany.whl $ cd dist $ pip install jax0.4.16.dev20230906+ge7f064ab4py3noneany.whl forcereinstall $ python c ""import jax; print(jax.__version__)"" 0.4.16.dev20230906+ge7f064ab4 $ cd .. $ JAX_RELEASE=1 python m build ... Successfully built jax0.4.16.tar.gz and jax0.4.16py3noneany.whl $ cd dist $ pip install jax0.4.16py3noneany.whl forcereinstall $ python c ""import jax; print(jax.__version__)"" 0.4.16 $ cd .. $ JAX_NIGHTLY=1 python m build ... Successfully built jax0.4.16.dev20230906.tar.gz and jax0.4.16.dev20230906py3noneany.whl $ cd dist $ pip install jax0.4.16.dev20230906py3noneany.whl forcereinstall $ python c ""import jax; print(jax.__version__)"" 0.4.16.dev20230906 ```  `jaxlib` wheel builds The jaxlib wheel build doesn't have any presubmission CI coverage, so I tested locally.  Nonrelease build Note that the build happens outside a git source tree, so it uses the fallback builddate version. ``` $ python build/build.py ... Output wheel: /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl To install the newlybuilt jaxlib wheel, run:   pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl forcereinstall $ pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl forcereinstall $ python c ""import jaxlib; print(jaxlib.__version__)"" 0.4.16.dev20230906 ```  Release Build ``` $ JAXLIB_RELEASE=1 python build/build.py ... Output wheel: /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16cp39cp39macosx_11_0_arm64.whl To install the newlybuilt jaxlib wheel, run:   pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16cp39cp39macosx_11_0_arm64.whl forcereinstall $ pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16cp39cp39macosx_11_0_arm64.whl forcereinstall $ python c ""import jaxlib; print(jaxlib.__version__)"" 0.4.16 ```  Nightly Build ``` $ JAXLIB_NIGHTLY=1 python build/build.py ... Output wheel: /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl To install the newlybuilt jaxlib wheel, run:   pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl forcereinstall $ pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl forcereinstall $ python c ""import jaxlib; print(jaxlib.__version__)"" 0.4.16.dev20230906 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,build: write appropriate version strings to build artifacts," Description The goal of this PR is to address CC(`pip install jax jaxlib` broken with 0.4.15), the issue that led us to yank the 0.4.15 release. The desired endstate are for `jax.__version__` and `jaxlib.__version__` to satisfy the following (keeping PEP 440 in mind):  when importing or installing jax/jaxlib from the github source tree, the version looks like `0.4.16.dev20230906+g5227f1d14` where `20230906` is the date of the most recent commit, and `g5227f1d14` is the commit hash prefix.  when building the jax/jaxlib nightly release, the resulting source distribution and wheel files have the date of build (not date of commit) encoded in the version, like `0.4.16.dev20230906`  when building jax/jaxlib releases, the resulting source distribution and wheel files have clean release versions that look like `0.4.16`. I looked into the possibility of using `versioneer` or `miniver` for this, but they seem to not be well set up for repositories with multiple packages (`jax` and `jaxlib`), and also use mechanisms that are not compatible with our internal filesystem architecture. So instead I plan to roll our own solution. The general strategy here:  put a placeholder `_release_version = None` in `jax/version.py`  add utilities to determine a version string at build time based on environment variables:    if `JAX_RELEASE` or `JAXLIB_RELEASE` are set, set the string to the clean version (e.g. `'0.4.16'`)    if `JAX_NIGHTLY` or `JAXLIB_NIGHTLY` are set, set the string to a build date (e.g. `'0.4.16.dev20230906'`)    otherwise, set the string to a git commit date & hash (e.g. `0.4.16.dev20230906+g4e32a7683`), falling back to the nightly behavior if outside the git source tree.  when importing from source, if `_release_version is None` then dynamically generate the appropriate version at runtime from the current git hash.  add a setuptools `cmdclass` that updates the placeholder in source distributions and wheels with an appropriate hardcoded version string determined at build time.  Testing  `jax` source distributions I tested that JAX source distributions have the expected versions: ``` $ python setup.py sdist ... creating jax0.4.16.dev20230906+g4e32a7683 ... $ JAX_RELEASE=1 python setup.py sdist ... creating jax0.4.16 ... $ JAX_NIGHTLY=1 python setup.py sdist ... creating jax0.4.16.dev20230906 ... ``` I then confirmed that code installed from the source distribution has the matching version at runtime.  `jax` wheel builds Also confirmed that JAX wheels have the correct versions at runtime: ``` $ python m build ... Successfully built jax0.4.16.dev20230906+ge7f064ab4.tar.gz and jax0.4.16.dev20230906+ge7f064ab4py3noneany.whl $ cd dist $ pip install jax0.4.16.dev20230906+ge7f064ab4py3noneany.whl forcereinstall $ python c ""import jax; print(jax.__version__)"" 0.4.16.dev20230906+ge7f064ab4 $ cd .. $ JAX_RELEASE=1 python m build ... Successfully built jax0.4.16.tar.gz and jax0.4.16py3noneany.whl $ cd dist $ pip install jax0.4.16py3noneany.whl forcereinstall $ python c ""import jax; print(jax.__version__)"" 0.4.16 $ cd .. $ JAX_NIGHTLY=1 python m build ... Successfully built jax0.4.16.dev20230906.tar.gz and jax0.4.16.dev20230906py3noneany.whl $ cd dist $ pip install jax0.4.16.dev20230906py3noneany.whl forcereinstall $ python c ""import jax; print(jax.__version__)"" 0.4.16.dev20230906 ```  `jaxlib` wheel builds The jaxlib wheel build doesn't have any presubmission CI coverage, so I tested locally.  Nonrelease build Note that the build happens outside a git source tree, so it uses the fallback builddate version. ``` $ python build/build.py ... Output wheel: /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl To install the newlybuilt jaxlib wheel, run:   pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl forcereinstall $ pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl forcereinstall $ python c ""import jaxlib; print(jaxlib.__version__)"" 0.4.16.dev20230906 ```  Release Build ``` $ JAXLIB_RELEASE=1 python build/build.py ... Output wheel: /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16cp39cp39macosx_11_0_arm64.whl To install the newlybuilt jaxlib wheel, run:   pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16cp39cp39macosx_11_0_arm64.whl forcereinstall $ pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16cp39cp39macosx_11_0_arm64.whl forcereinstall $ python c ""import jaxlib; print(jaxlib.__version__)"" 0.4.16 ```  Nightly Build ``` $ JAXLIB_NIGHTLY=1 python build/build.py ... Output wheel: /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl To install the newlybuilt jaxlib wheel, run:   pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl forcereinstall $ pip install /Users/vanderplas/github/google/jax/dist/jaxlib0.4.16.dev20230906cp39cp39macosx_11_0_arm64.whl forcereinstall $ python c ""import jaxlib; print(jaxlib.__version__)"" 0.4.16.dev20230906 ```",2023-09-05T22:01:42Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17453
1137,"以下是一个github上的jax下的一个issue, 标题是(Jax leaks memory with random keys)， 内容是 ( Description I'm able to leak memory on CPU by running the following script ```python from jax import random import psutil key = random.PRNGKey(0) key, netkey = random.split(key) iters = 500_000 key, *data_keys = random.split(key, iters + 1) for i in range(iters):     inputs = random.normal(data_keys[i], (10,))     if i % 1000 == 0:         print(f""{psutil.Process().memory_info().rss / 1024 ** 2:.1f} MB"") ``` When run, it prints: ```bash 910.5 MB 910.7 MB 910.8 MB 911.0 MB 911.3 MB 911.5 MB 911.7 MB 911.9 MB 912.1 MB 912.3 MB 912.5 MB 912.7 MB 912.9 MB 913.2 MB ... ``` This also uses way more memory than I was expecting. Why does the first run require nearly 1GB? 500,000 random keys at 8 bytes each should take up less than 4MB, right?  What jax/jaxlib version are you using? jax0.4.14, jaxlib0.4.14  Which accelerator(s) are you using? CPU  Additional system info python3.11.4, MacOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax leaks memory with random keys," Description I'm able to leak memory on CPU by running the following script ```python from jax import random import psutil key = random.PRNGKey(0) key, netkey = random.split(key) iters = 500_000 key, *data_keys = random.split(key, iters + 1) for i in range(iters):     inputs = random.normal(data_keys[i], (10,))     if i % 1000 == 0:         print(f""{psutil.Process().memory_info().rss / 1024 ** 2:.1f} MB"") ``` When run, it prints: ```bash 910.5 MB 910.7 MB 910.8 MB 911.0 MB 911.3 MB 911.5 MB 911.7 MB 911.9 MB 912.1 MB 912.3 MB 912.5 MB 912.7 MB 912.9 MB 913.2 MB ... ``` This also uses way more memory than I was expecting. Why does the first run require nearly 1GB? 500,000 random keys at 8 bytes each should take up less than 4MB, right?  What jax/jaxlib version are you using? jax0.4.14, jaxlib0.4.14  Which accelerator(s) are you using? CPU  Additional system info python3.11.4, MacOS  NVIDIA GPU info _No response_",2023-09-05T10:09:12Z,bug,open,0,14,https://github.com/jax-ml/jax/issues/17432,"This looks like a garbage collection issue. In each iteration, once `inputs` goes out of scope, it doesn't immediately get deleted. Rather, its CPython reference count goes to zero, and sometime later the garbage collector will delete the variable. Once the garbage collector deletes the Python object, it triggers a call to XLA to clear the allocated buffer from the device. This delete call is also asynchronous, and will happen some time later in the program. If you're concerned about arrays/buffers from previous loops being deleted more quickly during your program execution, one thing you can do is to explicitly delete or garbage collect, though this will slow the execution of your code. For example: ```python for i in range(iters):     inputs = random.normal(data_keys[i], (10,))     if i % 1000 == 0:         print(f""{psutil.Process().memory_info().rss / 1024 ** 2:.1f} MB"")         gc.collect()   Python garbage collection: this is slow, so avoid doing it every iteration.     inputs.delete()  JAX/XLA buffer deletion. This is fast                      (it's an asynchronous call) so we can do it every iteration.     del inputs ``` You should see the memory use greatly reduced.","Interestingly, this doesn't grow: ```python from jax import random import psutil key = random.PRNGKey(0) key, netkey = random.split(key) iters = 500_000 data_keys = random.split(key, iters)   notice no tuple being formed here for i in range(iters):     inputs = random.normal(data_keys[i], (10,))     if i % 1000 == 0:         print(f""{psutil.Process().memory_info().rss / 1024 ** 2:.1f} MB"") ``` But this does (one line added): ```python from jax import random import psutil key = random.PRNGKey(0) key, netkey = random.split(key) iters = 500_000 data_keys = random.split(key, iters) data_keys = tuple(data_keys)   NOTE NOTE NOTE for i in range(iters):     inputs = random.normal(data_keys[i], (10,))     if i % 1000 == 0:         print(f""{psutil.Process().memory_info().rss / 1024 ** 2:.1f} MB"") ```","Ah I think  figured it out. This also doesn't leak: ```python from jax import random import psutil key = random.PRNGKey(0) key, netkey = random.split(key) iters = 500_000 data_keys = random.split(key, iters) data_keys = list(data_keys)   NOTE list not tuple for i in range(iters):     inputs = random.normal(data_keys.pop(), (10,))   NOTE pop     if i % 1000 == 0:         print(f""{psutil.Process().memory_info().rss / 1024 ** 2:.1f} MB"") ``` The issues are: 1. the program uses more memory than you might expect because `key, *data_keys = random.split(key, iters + 1)` is unpacking a single array into 50,001 separate arrays (e.g. the 50,000 components of the `data_keys` tuple); 2. the memory usage grows because each jax.Array has cached properties on it, and so each of the 50,000 arrays has that cached metadata populated when it's touched (i.e. when it's read by the `ranodm.normal(data_keys[i], (10,))` expression) I'm not sure if there's anything to fix here, at least in JAX itself, though I could be wrong. If you have a real program with this kind of memory issue, consider not unpacking 50,000 separate arrays and instead writing something more like ```python from jax import random import psutil key = random.PRNGKey(0) key, netkey = random.split(key) iters = 500_000 keys = random.split(key, iters + 1) key = keys[0] data_keys = keys[1:]   note: don't unpack data_keys into a tuple for i in range(iters):     inputs = random.normal(data_keys[i], (10,))     if i % 1000 == 0:         print(f""{psutil.Process().memory_info().rss / 1024 ** 2:.1f} MB"") ```",Actually  and  have ideas for how we can reduce the cached stuff here... looking into it!," thanks for the tip, I was unaware of the array caching mechanics (hidden side effect!). I figured precomputing all the keys in one batched operation would be more efficient than doing it one at a time in the loop. The array slice seems like a good alternative.",https://github.com/google/jax/pull/17452 should fix the leak problem.,The PR has been submitted. Can you try again?,"Sorry I'm not super familiar with your PR process. https://github.com/google/jax/pull/17452 does not have any commits attached to it. I just cloned and tested on master and I'm still seeing the memory leak, even with the call to `gc.collect()` (albeit it leaks much more slowly).","The script I ran, for posterity: ```python from jax import random import psutil import gc key = random.PRNGKey(0) iters = 500_000 key, *data_keys = random.split(key, iters + 1) for i in range(iters):     inputs = random.normal(data_keys[i], (10,))     if i % 1000 == 0:         print(f""{psutil.Process().memory_info().rss / 1024 ** 2:.1f} MB"")         gc.collect() ``` ``` 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.8 MB 909.9 MB 909.9 MB 909.9 MB 909.9 MB 909.9 MB 909.9 MB 909.9 MB 910.0 MB ```","I can't seem to repro it. Are you sure you installed jax at HEAD? Note you would need to clone jax and then `cd jax; pip install U .`. Maybe also uninstall jax before you do this? Also note that the memory usage will go up and down which is expected. ```    ...: iters = 500_000    ...: key, *data_keys = random.split(key, iters + 1)    ...: for i in range(iters):    ...:     inputs = random.normal(data_keys[i], (10,))    ...:     if i % 1000 == 0:    ...:         print(f""{psutil.Process().memory_info().rss / 1024 ** 2:.1f} MB"")    ...:         gc.collect()    ...: No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) 1011.9 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB 1012.5 MB ```","Weird, maybe it's something to do with my python/macos version.  ```bash pip uninstall jax jaxlib pip install git+https://github.com/google/jax jaxlib  Successfully installed jax0.4.16.dev20230906 jaxlib0.4.14 python debug.py  The listed script ``` ```bash 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.4 MB 907.5 MB 907.5 MB 907.5 MB 907.6 MB 907.6 MB 907.6 MB 907.6 MB 907.6 MB 907.6 MB 907.6 MB 907.6 MB 907.6 MB 907.6 MB 907.6 MB 907.6 MB 907.7 MB 907.7 MB 907.7 MB 907.7 MB 907.7 MB 907.8 MB 907.8 MB 907.8 MB 907.8 MB 907.8 MB 907.8 MB 907.8 MB 907.8 MB 907.8 MB 907.8 MB 907.8 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 907.9 MB 908.0 MB 908.0 MB 908.0 MB 908.1 MB 908.1 MB 908.1 MB 908.2 MB 908.2 MB 908.2 MB ```",Any updates to this issue  ? I'm on jax==0.4.23 and jaxlib==0.4.23+cuda11.cudnn86 (which should include changes from CC(Make `is_fully_addressable` an abstract method and implement it on each concrete Sharding.) ) and see the same memory leak issue.,"I think this is working as expected: when you run these lines: ```python iters = 500_000 key, *data_keys = random.split(key, iters + 1) ``` you're creating a list `data_keys` with 500,000 elements, each of which is a JAX array (and each of which will have the memory overhead assocated with a concrete JAX array instance). I would not expect the memory footprint to be reduced until those 500,000 arrays are deallocated, for example by deleting `data_keys` and then calling `gc.collect()`. Iterating through `data_keys` does not change or deallocate the contents of `data_keys`. Perhaps you were aiming for something like this: ``` keys = random.split(key, iters + 1) key, data_keys = keys[0], keys[1:] ``` in which `data_keys` will be a single array rather than a list of arrays.",(I just realized my comment is the same solution  offered above... sorry if there's something I'm missing)
3717,"以下是一个github上的jax下的一个issue, 标题是(XLARunTime Error: Cannot remove instruction %all-reduce while sharding with convolutions)， 内容是 ( Description Hi team, I have been using jax with equinox for some time, and was excited by the new AutoParallelism feature using sharding. Unfortunately, there's a bug I have encountered while using sharding using convolutions. XLA with sharding and convolutions just breaks.  With all other operations with FC layers and other, sharding works, but fails with convolution. See the below error:  !image MWE for the replication of code is below: ```python import equinox as eqx import jax import jax.experimental.mesh_utils as mesh_utils import jax.numpy as jnp import jax.random as jr import jax.sharding as sharding import numpy as np import optax   https://github.com/deepmind/optax  Hyperparameters dataset_size = 64 channel_size = 4 hidden_size = 32 depth = 1 learning_rate = 3e4 num_steps = 10 batch_size = 16   must be a multiple of our number of devices.  Generate some synthetic data xs = np.random.normal(size=(dataset_size, channel_size)) ys = np.sin(xs) num_samples = 100 image_height = 64 image_width = 64 num_channels = 3  Generate random image data with values between 0 and 255 images = np.random.randint(0, 256, size=(num_samples, num_channels, image_height, image_width ), dtype=np.uint8)  Generate corresponding random labels from 0 to 5 labels = np.random.randint(0, 6, size=num_samples) class SimpleConv(eqx.Module):     conv_layer: eqx.nn.Conv2d     linear : eqx.nn.Linear     def __init__(self, key):         key1, key2 = jr.split(key, 2)         self.conv_layer = eqx.nn.Conv2d(3, 5, 5, 1, padding=2, key=key1)         self.linear = eqx.nn.Linear(20480, 1, key=key2)     def __call__(self, x):         x = self.conv_layer(x)         return self.linear(x.flatten()) model = SimpleConv(key=jr.PRNGKey(6789)) optim = optax.adam(learning_rate) opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array)) def compute_loss(model, x, y):     pred_y = jax.vmap(model)(x)     return jnp.mean((y  pred_y) ** 2) .filter_jit def make_step(model, opt_state, x, y):     grads = eqx.filter_grad(compute_loss)(model, x, y)     updates, opt_state = optim.update(grads, opt_state)     model = eqx.apply_updates(model, updates)     return model, opt_state def dataloader(arrays, batch_size):     dataset_size = arrays[0].shape[0]     assert all(array.shape[0] == dataset_size for array in arrays)     indices = np.arange(dataset_size)     while True:         perm = np.random.permutation(indices)         start = 0         end = batch_size         while end != dataset_size:             batch_perm = perm[start:end]             yield tuple(array[batch_perm] for array in arrays)             start = end             end = start + batch_size num_devices = len(jax.devices()) devices_x = mesh_utils.create_device_mesh((num_devices, 1, 1, 1)) devices_y = mesh_utils.create_device_mesh((num_devices,)) shard_x = sharding.PositionalSharding(devices_x) shard_y = sharding.PositionalSharding(devices_y) print(f""Devices being used {num_devices}"")  for step, (x, y) in zip(range(num_steps), dataloader((images, labels), batch_size)):     x = jnp.asarray(x, dtype=""float32"")     x, y = jax.device_put(x, shard_x), jax.device_put(y, shard_y)     model, opt_state = make_step(model, opt_state, x, y) ```  What jax/jaxlib version are you using? jax==0.4.13, jaxlib==0.4.13+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info NVIDIASMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XLARunTime Error: Cannot remove instruction %all-reduce while sharding with convolutions," Description Hi team, I have been using jax with equinox for some time, and was excited by the new AutoParallelism feature using sharding. Unfortunately, there's a bug I have encountered while using sharding using convolutions. XLA with sharding and convolutions just breaks.  With all other operations with FC layers and other, sharding works, but fails with convolution. See the below error:  !image MWE for the replication of code is below: ```python import equinox as eqx import jax import jax.experimental.mesh_utils as mesh_utils import jax.numpy as jnp import jax.random as jr import jax.sharding as sharding import numpy as np import optax   https://github.com/deepmind/optax  Hyperparameters dataset_size = 64 channel_size = 4 hidden_size = 32 depth = 1 learning_rate = 3e4 num_steps = 10 batch_size = 16   must be a multiple of our number of devices.  Generate some synthetic data xs = np.random.normal(size=(dataset_size, channel_size)) ys = np.sin(xs) num_samples = 100 image_height = 64 image_width = 64 num_channels = 3  Generate random image data with values between 0 and 255 images = np.random.randint(0, 256, size=(num_samples, num_channels, image_height, image_width ), dtype=np.uint8)  Generate corresponding random labels from 0 to 5 labels = np.random.randint(0, 6, size=num_samples) class SimpleConv(eqx.Module):     conv_layer: eqx.nn.Conv2d     linear : eqx.nn.Linear     def __init__(self, key):         key1, key2 = jr.split(key, 2)         self.conv_layer = eqx.nn.Conv2d(3, 5, 5, 1, padding=2, key=key1)         self.linear = eqx.nn.Linear(20480, 1, key=key2)     def __call__(self, x):         x = self.conv_layer(x)         return self.linear(x.flatten()) model = SimpleConv(key=jr.PRNGKey(6789)) optim = optax.adam(learning_rate) opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array)) def compute_loss(model, x, y):     pred_y = jax.vmap(model)(x)     return jnp.mean((y  pred_y) ** 2) .filter_jit def make_step(model, opt_state, x, y):     grads = eqx.filter_grad(compute_loss)(model, x, y)     updates, opt_state = optim.update(grads, opt_state)     model = eqx.apply_updates(model, updates)     return model, opt_state def dataloader(arrays, batch_size):     dataset_size = arrays[0].shape[0]     assert all(array.shape[0] == dataset_size for array in arrays)     indices = np.arange(dataset_size)     while True:         perm = np.random.permutation(indices)         start = 0         end = batch_size         while end != dataset_size:             batch_perm = perm[start:end]             yield tuple(array[batch_perm] for array in arrays)             start = end             end = start + batch_size num_devices = len(jax.devices()) devices_x = mesh_utils.create_device_mesh((num_devices, 1, 1, 1)) devices_y = mesh_utils.create_device_mesh((num_devices,)) shard_x = sharding.PositionalSharding(devices_x) shard_y = sharding.PositionalSharding(devices_y) print(f""Devices being used {num_devices}"")  for step, (x, y) in zip(range(num_steps), dataloader((images, labels), batch_size)):     x = jnp.asarray(x, dtype=""float32"")     x, y = jax.device_put(x, shard_x), jax.device_put(y, shard_y)     model, opt_state = make_step(model, opt_state, x, y) ```  What jax/jaxlib version are you using? jax==0.4.13, jaxlib==0.4.13+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info NVIDIASMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1",2023-09-05T09:19:57Z,bug XLA needs info NVIDIA GPU,closed,0,2,https://github.com/jax-ml/jax/issues/17431,"I can't reproduce this at head on 2 GPUs. Can you say more about your GPU setup? How many GPUs are you using and what kinds? If you're so inclined, if you build `jaxlib` from source at head does the problem reproduce? Another thing that might help is if you include an XLA HLO dump. You can collect one by setting the environment variable `XLA_FLAGS=xla_dump_to=/some/directory`, running your computation, and zipping up the output directory and attaching it to this bug.",Closing because no further details were provided. I'd be happy to look into this if you can still reproduce this with the latest jax and jaxlib!
8029,"以下是一个github上的jax下的一个issue, 标题是(XLA Runtime Error at jax.ppermute when setting ""XLA_PYTHON_CLIENT_MEM_FRACTION"")， 内容是 ( Description We are performing scaling runs of our JAX code (https://github.com/tumaer/JAXFLUIDS) on the JUWELS Booster (https://apps.fzjuelich.de/jsc/hps/juwels/boosteroverview.html). We experience errors using jax.lax.ppermute depending on the setting of certain environmental variables. Software in use: Python 3.10.4, jax 0.4.14, jaxlib, 0.4.14+cuda12.cudnn89, CUDA 12 installed using pip wheels provided by the jax pip install. Please find attached requirements.txt for details about the used package versions. We run the following python script on a single compute node (4xNVIDIA A100) using a single process.  ```python import jax, numpy as np print(jax.devices()) x = np.arange(jax.device_count()) print(x) def func(arr):     arr = jax.lax.ppermute(arr, axis_name=""i"", perm=((0,1),(1,2),(2,3),(3,0)))     return arr x = jax.pmap(func, axis_name=""i"")(x) print(x) ``` And get the expected output: ``` srun: job 8375613 queued and waiting for resources srun: job 8375613 has been allocated resources [gpu(id=0), gpu(id=1), gpu(id=2), gpu(id=3)] [0 1 2 3] [3 0 1 2] ``` However, when we set the environmental variable ```python os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""]=""100"" ``` and run the exact same script, we get the following error: ``` srun: job 8375610 queued and waiting for resources srun: job 8375610 has been allocated resources 20230904 15:19:15.108898: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error 20230904 15:19:15.109009: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 1 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0. 20230904 15:19:15.109096: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error 20230904 15:19:15.109113: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0. 20230904 15:19:15.109152: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error 20230904 15:19:15.109167: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 3 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0. 20230904 15:19:15.109261: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error 20230904 15:19:15.109277: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 2 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0. [gpu(id=0), gpu(id=1), gpu(id=2), gpu(id=3)] [0 1 2 3] Traceback (most recent call last):   File ""/p/project/jaxfluids/jaxfluids/tests/parallelization/multihost/gpu/runfile.py"", line 9, in      x = jax.pmap(func, axis_name=""i"")(x)   File ""/p/project/jaxfluids/venv_jax_cuda_12/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/p/project/jaxfluids/venv_jax_cuda_12/lib/python3.10/sitepackages/jax/_src/api.py"", line 1803, in cache_miss     out = map_bind_continuation(execute(*tracers))   File ""/p/project/jaxfluids/venv_jax_cuda_12/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/p/project/jaxfluids/venv_jax_cuda_12/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 1229, in __call__     results = self.xla_executable.execute_sharded(input_bufs) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well). The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/p/project/jaxfluids/jaxfluids/tests/parallelization/multihost/gpu/runfile.py"", line 9, in      x = jax.pmap(func, axis_name=""i"")(x) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well). srun: error: jwb0897: task 0: Exited with exit code 1 ``` The program fails at compiling the pmapped function that performs a simple jax.lax.ppermute (jax.lax.psum works just fine). We noticed however, that if we deactivate P2P using ```python os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""]=""100"" os.environ[""NCCL_P2P_DISABLE""] = ""1"" ``` the script works again. Specifying the preallocated memory fraction enables us to run larger jobs for the scaling runs without getting OOM, so we would like to use it, but still also use P2P, as the program runs faster with it. Do you have an idea what the problem is? Some additional info:  We also tried using CUDA 11 from pip wheels, which produces the same error behavior. Best JAXFLUIDS  What jax/jaxlib version are you using? jax 0.4.14, jaxlib, 0.4.14+cuda12.cudnn89  Which accelerator(s) are you using? GPU A100 40GB   Additional system info https://apps.fzjuelich.de/jsc/hps/juwels/configuration.htmlsoftwareoverview https://apps.fzjuelich.de/jsc/hps/juwels/configuration.htmlhardwareconfigurationofthesystemnameboostermodule https://apps.fzjuelich.de/jsc/hps/juwels/boosteroverview.htmlnodeconfiguration requirements.txt  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"XLA Runtime Error at jax.ppermute when setting ""XLA_PYTHON_CLIENT_MEM_FRACTION"""," Description We are performing scaling runs of our JAX code (https://github.com/tumaer/JAXFLUIDS) on the JUWELS Booster (https://apps.fzjuelich.de/jsc/hps/juwels/boosteroverview.html). We experience errors using jax.lax.ppermute depending on the setting of certain environmental variables. Software in use: Python 3.10.4, jax 0.4.14, jaxlib, 0.4.14+cuda12.cudnn89, CUDA 12 installed using pip wheels provided by the jax pip install. Please find attached requirements.txt for details about the used package versions. We run the following python script on a single compute node (4xNVIDIA A100) using a single process.  ```python import jax, numpy as np print(jax.devices()) x = np.arange(jax.device_count()) print(x) def func(arr):     arr = jax.lax.ppermute(arr, axis_name=""i"", perm=((0,1),(1,2),(2,3),(3,0)))     return arr x = jax.pmap(func, axis_name=""i"")(x) print(x) ``` And get the expected output: ``` srun: job 8375613 queued and waiting for resources srun: job 8375613 has been allocated resources [gpu(id=0), gpu(id=1), gpu(id=2), gpu(id=3)] [0 1 2 3] [3 0 1 2] ``` However, when we set the environmental variable ```python os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""]=""100"" ``` and run the exact same script, we get the following error: ``` srun: job 8375610 queued and waiting for resources srun: job 8375610 has been allocated resources 20230904 15:19:15.108898: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error 20230904 15:19:15.109009: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 1 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0. 20230904 15:19:15.109096: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error 20230904 15:19:15.109113: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0. 20230904 15:19:15.109152: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error 20230904 15:19:15.109167: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 3 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0. 20230904 15:19:15.109261: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error 20230904 15:19:15.109277: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 2 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0. [gpu(id=0), gpu(id=1), gpu(id=2), gpu(id=3)] [0 1 2 3] Traceback (most recent call last):   File ""/p/project/jaxfluids/jaxfluids/tests/parallelization/multihost/gpu/runfile.py"", line 9, in      x = jax.pmap(func, axis_name=""i"")(x)   File ""/p/project/jaxfluids/venv_jax_cuda_12/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/p/project/jaxfluids/venv_jax_cuda_12/lib/python3.10/sitepackages/jax/_src/api.py"", line 1803, in cache_miss     out = map_bind_continuation(execute(*tracers))   File ""/p/project/jaxfluids/venv_jax_cuda_12/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/p/project/jaxfluids/venv_jax_cuda_12/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 1229, in __call__     results = self.xla_executable.execute_sharded(input_bufs) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well). The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/p/project/jaxfluids/jaxfluids/tests/parallelization/multihost/gpu/runfile.py"", line 9, in      x = jax.pmap(func, axis_name=""i"")(x) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.collective_permute' failed: external/xla/xla/service/gpu/nccl_collective_permute_thunk.cc:250: NCCL operation ncclGroupEnd() failed: internal error; current tracing scope: collectivepermutestart; current profiling annotation: XlaModule:hlo_module=pmap_func,program_id=0.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well). srun: error: jwb0897: task 0: Exited with exit code 1 ``` The program fails at compiling the pmapped function that performs a simple jax.lax.ppermute (jax.lax.psum works just fine). We noticed however, that if we deactivate P2P using ```python os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""]=""100"" os.environ[""NCCL_P2P_DISABLE""] = ""1"" ``` the script works again. Specifying the preallocated memory fraction enables us to run larger jobs for the scaling runs without getting OOM, so we would like to use it, but still also use P2P, as the program runs faster with it. Do you have an idea what the problem is? Some additional info:  We also tried using CUDA 11 from pip wheels, which produces the same error behavior. Best JAXFLUIDS  What jax/jaxlib version are you using? jax 0.4.14, jaxlib, 0.4.14+cuda12.cudnn89  Which accelerator(s) are you using? GPU A100 40GB   Additional system info https://apps.fzjuelich.de/jsc/hps/juwels/configuration.htmlsoftwareoverview https://apps.fzjuelich.de/jsc/hps/juwels/configuration.htmlhardwareconfigurationofthesystemnameboostermodule https://apps.fzjuelich.de/jsc/hps/juwels/boosteroverview.htmlnodeconfiguration requirements.txt  NVIDIA GPU info _No response_",2023-09-04T13:23:06Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/17426,"`XLA_PYTHON_CLIENT_MEM_FRACTION` is normally expressed as a ratio. 0.75 is typical. Using ""100"" seems like a mistake.  Also you cannot use the entire GPU for XLA's heap: NVIDIA's libraries need a certain amount of GPU memory also. Hope that helps!"
1702,"以下是一个github上的jax下的一个issue, 标题是(Flexible Type Annotation for jax.lax.scan Function - Fixes #17405)， 内容是 (What does this PR do? fix : CC(Type annotation for jax.lax.scan appears incorrect) This pull request addresses issue CC(Type annotation for jax.lax.scan appears incorrect), which concerns the type annotation for the jax.lax.scan function in the JAX library. The issue pointed out that the existing type annotation was not accurate when the function f returned different types, especially when it returned Python scalar types. To resolve this issue, this pull request introduces a more flexible type annotation for the jax.lax.scan function. Instead of specifying a fixed type for the return value, we use Any for the type of Y. This change acknowledges the inherent variability in the return type of f and ensures that the type annotation is more inclusive and representative of realworld usage. By using Any for Y, we avoid overly restrictive type annotations that might lead to type errors in cases where f returns Python scalar types, while still maintaining type safety for common cases where Y represents an array with an unspecified number of dimensions. This change aims to make the type annotation for jax.lax.scan more accurate and compatible with various use cases, ultimately improving the developer experience when working with JAX. Changes Made: Updated the type annotation for the jax.lax.scan function by changing the type of Y from a specific type to Any, allowing for more flexibility in handling different return types from the function f.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Flexible Type Annotation for jax.lax.scan Function - Fixes #17405,"What does this PR do? fix : CC(Type annotation for jax.lax.scan appears incorrect) This pull request addresses issue CC(Type annotation for jax.lax.scan appears incorrect), which concerns the type annotation for the jax.lax.scan function in the JAX library. The issue pointed out that the existing type annotation was not accurate when the function f returned different types, especially when it returned Python scalar types. To resolve this issue, this pull request introduces a more flexible type annotation for the jax.lax.scan function. Instead of specifying a fixed type for the return value, we use Any for the type of Y. This change acknowledges the inherent variability in the return type of f and ensures that the type annotation is more inclusive and representative of realworld usage. By using Any for Y, we avoid overly restrictive type annotations that might lead to type errors in cases where f returns Python scalar types, while still maintaining type safety for common cases where Y represents an array with an unspecified number of dimensions. This change aims to make the type annotation for jax.lax.scan more accurate and compatible with various use cases, ultimately improving the developer experience when working with JAX. Changes Made: Updated the type annotation for the jax.lax.scan function by changing the type of Y from a specific type to Any, allowing for more flexibility in handling different return types from the function f.",2023-09-02T12:45:25Z,,closed,0,3,https://github.com/jax-ml/jax/issues/17412,"Jakevdp, Thank you for taking the time to review this pull request and provide valuable feedback. I appreciate your insights and concerns regarding the proposed changes. It's clear that there are some challenges in finding an optimal solution for annotating the `jax.lax.scan` function, especially given the variability in the return type of `f`. I acknowledge your point about shadowing the builtin `Any` with a `TypeVar` of the same name, which could be confusing. I'll address this concern in the next iteration of the pull request. Regarding the use of `tuple` versus `Tuple`, I understand the preference for the builtin lowercase `tuple`, and I'll make sure to follow that convention in the updated code. I'll also take into consideration your comment about the limitations of Python's type system and continue to explore potential alternatives or improvements to the type annotation. Your feedback is greatly appreciated, and I'll work on addressing these concerns and refining the pull request accordingly. Thank you.",Hello  are you still interested in working on this? Thanks!,> Hello  are you still interested in working on this? Thanks! No
1703,"以下是一个github上的jax下的一个issue, 标题是(Flexible Type Annotation for jax.lax.scan Function - Fixes #17405)， 内容是 (What does this PR do? fix : CC(Type annotation for jax.lax.scan appears incorrect)  This pull request addresses issue CC(Type annotation for jax.lax.scan appears incorrect), which concerns the type annotation for the jax.lax.scan function in the JAX library. The issue pointed out that the existing type annotation was not accurate when the function f returned different types, especially when it returned Python scalar types. To resolve this issue, this pull request introduces a more flexible type annotation for the jax.lax.scan function. Instead of specifying a fixed type for the return value, we use Any for the type of Y. This change acknowledges the inherent variability in the return type of f and ensures that the type annotation is more inclusive and representative of realworld usage. By using Any for Y, we avoid overly restrictive type annotations that might lead to type errors in cases where f returns Python scalar types, while still maintaining type safety for common cases where Y represents an array with an unspecified number of dimensions. This change aims to make the type annotation for jax.lax.scan more accurate and compatible with various use cases, ultimately improving the developer experience when working with JAX. Changes Made: Updated the type annotation for the jax.lax.scan function by changing the type of Y from a specific type to Any, allowing for more flexibility in handling different return types from the function f.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Flexible Type Annotation for jax.lax.scan Function - Fixes #17405,"What does this PR do? fix : CC(Type annotation for jax.lax.scan appears incorrect)  This pull request addresses issue CC(Type annotation for jax.lax.scan appears incorrect), which concerns the type annotation for the jax.lax.scan function in the JAX library. The issue pointed out that the existing type annotation was not accurate when the function f returned different types, especially when it returned Python scalar types. To resolve this issue, this pull request introduces a more flexible type annotation for the jax.lax.scan function. Instead of specifying a fixed type for the return value, we use Any for the type of Y. This change acknowledges the inherent variability in the return type of f and ensures that the type annotation is more inclusive and representative of realworld usage. By using Any for Y, we avoid overly restrictive type annotations that might lead to type errors in cases where f returns Python scalar types, while still maintaining type safety for common cases where Y represents an array with an unspecified number of dimensions. This change aims to make the type annotation for jax.lax.scan more accurate and compatible with various use cases, ultimately improving the developer experience when working with JAX. Changes Made: Updated the type annotation for the jax.lax.scan function by changing the type of Y from a specific type to Any, allowing for more flexibility in handling different return types from the function f.",2023-09-02T12:41:06Z,,closed,0,1,https://github.com/jax-ml/jax/issues/17411,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
932,"以下是一个github上的jax下的一个issue, 标题是(jax.debug.breakpoint() give numpy array and with bfloat16, it comptue)， 内容是 ( Description jax.debug.breakpoint() give strange results. Some of the inputs are tensors of 1s with shape of 2048 (bfloat16). But if we do `x.sum()` is return 256. But if I do that in the python script, I get the good results (2048): ```  python test.py 2048   Entering jdb: (jdb) x.sum() 256 (jdb) c 2048.0   Entering jdb: (jdb) x.sum() 2048.0 (jdb) c 2048.0   Entering jdb: (jdb) x.sum() 2048.0 ``` To prevent all that category of issues, breakpoint should give ArrayImpl object and not numpy object.    What jax/jaxlib version are you using? upstream  Which accelerator(s) are you using? Independent of backend.  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"jax.debug.breakpoint() give numpy array and with bfloat16, it comptue"," Description jax.debug.breakpoint() give strange results. Some of the inputs are tensors of 1s with shape of 2048 (bfloat16). But if we do `x.sum()` is return 256. But if I do that in the python script, I get the good results (2048): ```  python test.py 2048   Entering jdb: (jdb) x.sum() 256 (jdb) c 2048.0   Entering jdb: (jdb) x.sum() 2048.0 (jdb) c 2048.0   Entering jdb: (jdb) x.sum() 2048.0 ``` To prevent all that category of issues, breakpoint should give ArrayImpl object and not numpy object.    What jax/jaxlib version are you using? upstream  Which accelerator(s) are you using? Independent of backend.  Additional system info _No response_  NVIDIA GPU info _No response_",2023-09-01T15:47:07Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17402,We should probably return a CPU `Array` in all cases.
591,"以下是一个github上的jax下的一个issue, 标题是(Differentiation failing on real function)， 内容是 ( Description First off, thank you all for your work on this incredible project! We are trying to compute the derivative of the potential function below (found in gravitational wave parameter estimation). We have a complex valued inner product $ : \mathbb{C}^n \times \mathbb{C}^n \to \mathbb{C}$, and a probability distribution defined by $$ p(x) = e^{\frac{1}{2}  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Differentiation failing on real function," Description First off, thank you all for your work on this incredible project! We are trying to compute the derivative of the potential function below (found in gravitational wave parameter estimation). We have a complex valued inner product $ : \mathbb{C}^n \times \mathbb{C}^n \to \mathbb{C}$, and a probability distribution defined by $$ p(x) = e^{\frac{1}{2}  ++ ```",2023-09-01T15:40:24Z,bug needs info,closed,0,7,https://github.com/jax-ml/jax/issues/17401,"Thanks for the report! It's definitely possible that something in JAX's autodiff machinery is producing incorrect gradients, but my first guess seeing the code would be that your analytical expression is incorrect – it's a relatively complicated expression, and after staring at it for two minutes it's not obvious to me that the gradient is correct (not obvious that it's wrong either). Still, it wouldn't be hard to make a mistake there. If you think JAX is producing the wrong answer, I'd suggest trying to come up with a reproduction in which it is more straightforward to see and confirm that JAX is producing the wrong value.","Thank you for such a quick reply!  So the method `gradient_strain` which implements the derivative analytically is confirmed to yield the same result as `jax.jacfwd(model.strain)`. So this portion of the code doesn't have any bugs in it! Its only when we try to differentiate through the inner product that we get bugs. However, this part is very clean. Since the potential is given by $$V(x) = \frac{1}{2} Re \lt h(x)  d, h(x)  d \gt$$ and since $d$ is a constant, the gradient has the following form $$\nabla V(x) = Re \lt \nabla h(x) , h(x)  d \gt$$ And for reference, the inner product is defined as $$\lt a,b \gt = \sum_i a_i^*b_i$$ If you have any hypotheses on what may be causing this bug I can reply with a more minimal example, but I'm stumped. Update 1: I got rid of a bunch of fluff in the original code I posted, and the bug remains Update 2: Just an observation, but the last component of the gradient agrees, but all others disagree","I haven't looked into the details, but I just wanted to point out that numerical checking of gradients (to convince yourself that autodiff is doing the right thing) is pretty easy: just compare to finite differences, like $f'(x) \approx \frac{f(x + \epsilon)  f(x)}{\epsilon}$ (along a random direction). That's what the internal tool `jax._src.test_util.check_grads` does automatically. (Also I recommend using `\langle` and `\rangle` to typeset inner products in TeX!)",One thing that immediately sticks out to me is that you have a 10^22 range in the magnitude of your inputs – it wouldn't be surprising to me if floating point roundoff errors are coming into play with any computation involving numbers at such different scales.,"I believe you have the right idea Jake. It appears that changing the scales for the amplitude variable fixes the issue. Specifically, changing ``` self.PSD = 1e40 * jnp.ones_like(self.frequency) injection = jnp.array([0, 0, 30.0, 0.24, 2e22]) ``` to ``` self.PSD = jnp.ones_like(self.frequency) injection = jnp.array([0, 0, 30.0, 0.24, 2]) ``` resolves the issue. Could you explain why this would be an issue? Also, would it be possible to have JAX throw a warning in a situation like this?","> Could you explain why this would be an issue? For example: ``` >>> 1.0 + 1E22 == 1.0 True ``` Floating point math is only an approximation of real math. 64bit floating point only can represent about 16 decimal places in a single value; 32bit can only represent about 8. So if you're doing arithmetic operations involving numbers of very different scales, floating point expressions are likely to lose precision. > Also, would it be possible to have JAX throw a warning in a situation like this? No, this is just something you have to work with and be aware of when doing floating point operations on all modern systems. There's a good reference on this topic here: https://stackoverflow.com/q/588004",Makes sense! I appreciate the help.
1290,"以下是一个github上的jax下的一个issue, 标题是(rolling forward shard_map transpose fixes)， 内容是 (rolling forward shard_map transpose fixes The new efficienttranspose path, enabled by setting check_rep=True in the shard_map call, had kept working. But the change inadvertently broke the check_rep=False path. And because most tests set check_rep=True, we didn't notice it in the tests! The issue was that with check_rep=False, we need the shard_map transpose rule to insert psums corresponding to in_specs with fanout, and correspondingly insert division for out_specs with faninconsensus. (With the new check_rep=True path that this change adds, those extra operations aren't necessary as the body itself transposes correctly.) But the PR accidentally removed those! The fix was simple: just track whether we've applied the efficienttransposebodyrewrite (i.e. whether we're in the new bodyistransposable path or old needextraoperations path) by adding a boolean parameter `rewrite` to the shard_map primitive, and if the rewrite hasn't been applied then include the explicit psum/div operations in the transpose rule. Reverts 8a04dfd830ff89f46e1fe3e866ee4fb2da9c90aa)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,rolling forward shard_map transpose fixes,"rolling forward shard_map transpose fixes The new efficienttranspose path, enabled by setting check_rep=True in the shard_map call, had kept working. But the change inadvertently broke the check_rep=False path. And because most tests set check_rep=True, we didn't notice it in the tests! The issue was that with check_rep=False, we need the shard_map transpose rule to insert psums corresponding to in_specs with fanout, and correspondingly insert division for out_specs with faninconsensus. (With the new check_rep=True path that this change adds, those extra operations aren't necessary as the body itself transposes correctly.) But the PR accidentally removed those! The fix was simple: just track whether we've applied the efficienttransposebodyrewrite (i.e. whether we're in the new bodyistransposable path or old needextraoperations path) by adding a boolean parameter `rewrite` to the shard_map primitive, and if the rewrite hasn't been applied then include the explicit psum/div operations in the transpose rule. Reverts 8a04dfd830ff89f46e1fe3e866ee4fb2da9c90aa",2023-08-31T23:47:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17395
484,"以下是一个github上的jax下的一个issue, 标题是(Change jax's api_test.py for IFRT compatibility.)， 内容是 (Change jax's api_test.py for IFRT compatibility. The test relies relying on implicit crossbackend resharding, which is  not supported on arbitrary IFRT clients, see: (https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Change jax's api_test.py for IFRT compatibility.,"Change jax's api_test.py for IFRT compatibility. The test relies relying on implicit crossbackend resharding, which is  not supported on arbitrary IFRT clients, see: (https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b)",2023-08-31T21:37:51Z,,closed,0,1,https://github.com/jax-ml/jax/issues/17392,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
2842,"以下是一个github上的jax下的一个issue, 标题是(Setting `typing.TYPE_CHECKING=True` breaks Jax's API)， 内容是 ( Description I am not sure if this is intended, but I recently encountered an issue when documenting my jaxrelated library, where array would by of type `ArrayImpl`, which would break my code. This is caused by an update in `Sphinx`'s autodoc, where they import the packages with `TYPE_CHECKING=True` as of `v7.2`, see the discussion in https://github.com/sphinxdoc/sphinx/issues/11652. However, they feel it is an issue from Jax that it fails to have the expected behavior when type checking is enabled. I could not produce the same error, but we can see that importing `jax` after type checking was set results in an error: ```python import typing typing.TYPE_CHECKING = True import jax ``` which outputs: ``` Traceback (most recent call last):   File ""/export/home/eertmans/repositories/jeertmans.github.io/test.py"", line 5, in      import jax   File ""/home/eertmans/.local/lib/python3.10/sitepackages/jax/__init__.py"", line 169, in      from jax import scipy as scipy   File ""/home/eertmans/.local/lib/python3.10/sitepackages/jax/scipy/__init__.py"", line 21, in      from jax.scipy import interpolate as interpolate   File ""/home/eertmans/.local/lib/python3.10/sitepackages/jax/scipy/interpolate/__init__.py"", line 17, in      from jax._src.third_party.scipy.interpolate import (   File ""/home/eertmans/.local/lib/python3.10/sitepackages/jax/_src/third_party/scipy/interpolate.py"", line 2, in      import scipy.interpolate as osp_interpolate   File ""/home/eertmans/.local/lib/python3.10/sitepackages/scipy/interpolate/__init__.py"", line 166, in      from ._interpolate import *   File ""/home/eertmans/.local/lib/python3.10/sitepackages/scipy/interpolate/_interpolate.py"", line 21, in      from .interpnd import _ndim_coords_from_arrays   File ""interpnd.pyx"", line 1, in init scipy.interpolate.interpnd   File ""/home/eertmans/.local/lib/python3.10/sitepackages/scipy/spatial/__init__.py"", line 108, in      from ._geometric_slerp import geometric_slerp   File ""/home/eertmans/.local/lib/python3.10/sitepackages/scipy/spatial/_geometric_slerp.py"", line 12, in      import numpy.typing as npt   File ""/home/eertmans/.local/lib/python3.10/sitepackages/numpy/typing/__init__.py"", line 158, in      from numpy._typing import (   File ""/home/eertmans/.local/lib/python3.10/sitepackages/numpy/_typing/__init__.py"", line 209, in      from ._ufunc import ( ModuleNotFoundError: No module named 'numpy._typing._ufunc' ```  What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14+cuda12.cudnn89  Which accelerator(s) are you using? CPU/GPU  Additional system info 3.10, Linux  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Setting `typing.TYPE_CHECKING=True` breaks Jax's API," Description I am not sure if this is intended, but I recently encountered an issue when documenting my jaxrelated library, where array would by of type `ArrayImpl`, which would break my code. This is caused by an update in `Sphinx`'s autodoc, where they import the packages with `TYPE_CHECKING=True` as of `v7.2`, see the discussion in https://github.com/sphinxdoc/sphinx/issues/11652. However, they feel it is an issue from Jax that it fails to have the expected behavior when type checking is enabled. I could not produce the same error, but we can see that importing `jax` after type checking was set results in an error: ```python import typing typing.TYPE_CHECKING = True import jax ``` which outputs: ``` Traceback (most recent call last):   File ""/export/home/eertmans/repositories/jeertmans.github.io/test.py"", line 5, in      import jax   File ""/home/eertmans/.local/lib/python3.10/sitepackages/jax/__init__.py"", line 169, in      from jax import scipy as scipy   File ""/home/eertmans/.local/lib/python3.10/sitepackages/jax/scipy/__init__.py"", line 21, in      from jax.scipy import interpolate as interpolate   File ""/home/eertmans/.local/lib/python3.10/sitepackages/jax/scipy/interpolate/__init__.py"", line 17, in      from jax._src.third_party.scipy.interpolate import (   File ""/home/eertmans/.local/lib/python3.10/sitepackages/jax/_src/third_party/scipy/interpolate.py"", line 2, in      import scipy.interpolate as osp_interpolate   File ""/home/eertmans/.local/lib/python3.10/sitepackages/scipy/interpolate/__init__.py"", line 166, in      from ._interpolate import *   File ""/home/eertmans/.local/lib/python3.10/sitepackages/scipy/interpolate/_interpolate.py"", line 21, in      from .interpnd import _ndim_coords_from_arrays   File ""interpnd.pyx"", line 1, in init scipy.interpolate.interpnd   File ""/home/eertmans/.local/lib/python3.10/sitepackages/scipy/spatial/__init__.py"", line 108, in      from ._geometric_slerp import geometric_slerp   File ""/home/eertmans/.local/lib/python3.10/sitepackages/scipy/spatial/_geometric_slerp.py"", line 12, in      import numpy.typing as npt   File ""/home/eertmans/.local/lib/python3.10/sitepackages/numpy/typing/__init__.py"", line 158, in      from numpy._typing import (   File ""/home/eertmans/.local/lib/python3.10/sitepackages/numpy/_typing/__init__.py"", line 209, in      from ._ufunc import ( ModuleNotFoundError: No module named 'numpy._typing._ufunc' ```  What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14+cuda12.cudnn89  Which accelerator(s) are you using? CPU/GPU  Additional system info 3.10, Linux  NVIDIA GPU info ``` ++  ++ ```",2023-08-31T12:48:26Z,bug,closed,0,12,https://github.com/jax-ml/jax/issues/17385,"It looks like the error is coming from `import numpy.typing`, which comes via `import scipy`  if this is a bug, it’s a bug in numpy and scipy, not in JAX itself I think.","Looking closer at this – I don't think this would be expected to work. `typing.TYPE_CHECKING=True` implies that the code is not being executed by a Python runtime, but rather only typechecked. So for example, `numpy/typing/_ufunc.py` does not exist: you cannot import it. But `numpy/typing/_ufunc.pyi` does exist, so when a static type checker parses this code, it will be able to find the type declarations at that path. The sphinx bug looks like its real, but the code snippet above seems unrelated to the original issue.",I think this issue comes from what different Python modules assume `TYPE_CHECKING=True` means...,"Looking at the error from the sphinx issue, I also don't understand where that is coming from. `ArrayImpl` is in fact subscriptable in the current version of JAX: ```python  contents of main.py import jax.numpy as jnp from jax._src.array import ArrayImpl x: ArrayImpl = jnp.arange(5) print(x[0]) ``` ``` $ mypy main.py Success: no issues found in 1 source file ``` What version of JAX are you using when you're seeing this error?",Versions 0.4.14 for both `jax` and `jaxlib`.,"Strange  ArrayImpl is definitiely indexable in jax v0.4.14 (see https://github.com/google/jax/blob/jaxv0.4.14/jax/_src/array.pyL297). If you can come up with a selfcontained reproducer, let me know.","I suspect the bug you're hitting has to do with this TODO, which has to do with working around a static type issue in pytype: https://github.com/google/jax/blob/88a60b808c1f91260cc9e75b9aa2508aae5bc9f9/jax/_src/array.pyL544L548 But I'd still like to understand in which situations this would come up.",Did you check the MWE I put in the Sphinx issue I me mentioned above?,"Yes I saw that, but it's not very minimal... If it's a JAX bug, we should be able to reproduce it without setting up a sphinx project directory. If we can't reproduce it without sphinx, then I'd assume it's a sphinx bug. The fact that sphinx sets `TYPE_CHECKING = True` before doing a runtime import is a bit suspicious.",I’ll try to have a better example when I can,Hi   Looks like the PR CC(Collapsed a few unnecessary ``if TYPE_CHECKING`` blocks) successfully addressed an issue that prevented JAX from importing when `typing.TYPE_CHECKING=True`. I tested the issue with the latest nightly version of JAX. ```python import typing typing.TYPE_CHECKING = True import jax jax.__version__ ``` Output: ``` '0.4.32.dev20240820' ``` Attaching the gist for reference. Thank you.,Thanks for notifying me ! I think this issue can be closed then :)
3015,"以下是一个github上的jax下的一个issue, 标题是(Constant segfaults using latest JAX on NVIDIA Turing GPU, even on small arrays and with sufficient memory)， 内容是 ( Description With three different installations of JAX on the same hardware, I have consistently encountered segfaults on all but the most trivial operations. For example, ``` import jax.numpy as jnp a = jnp.ones((10, 10)) b = jnp.ones((10,)) jnp.tensordot(a, b, 1)   segfault happens here ``` Install scenarios follow.  1. Installed JAX from pip (with `pip install jax[cuda11_local]`). After digging deeper into the problem, found CuDNN was missing and installed it (version 8) and upgraded CUDA itself (to version 12.2).  2. Uninstalling and reinstalling from pip again (`pip install jax[cuda12_local]`) had exactly the same results regarding segfaults.  3. Built from source (following the instructions in the docs) and eventually got a successful build (`import jax` works without error) but no change in behavior. Using my build of JAX, I ran the unittests with Bazel and see that the vast majority of them (149 of 170) fail. I am including a snippet of one arbitrarilyselected log file below. ``` Running tests under Python 3.11.0: /path/redacted/bin/python [ RUN      ] BCOOTest.test_bcoo_dot_general2 (props=BatchedDotGeneralProperties(lhs_shape=(3, 2, 4, 4), rhs_shape=(4, 2, 4, 3), n_batch=2, n_dense=0, dimension_numbers=(([2], [0]), ([], []))), dtype=) 20230829 17:56:16.088272: W external/xla/xla/service/platform_util.cc:198] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16877027328 I0829 17:56:16.088498 140276183101440 xla_bridge.py:514] Unable to initialize backend 'cuda': INTERNAL: no supported devices found for platform CUDA I0829 17:56:16.088726 140276183101440 xla_bridge.py:514] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: ""rocm"". Available platform names are: CUDA Interpreter I0829 17:56:16.089131 140276183101440 xla_bridge.py:514] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory Fatal Python error: Segmentation fault ``` JAX was built against the correct CUDA libraries, which I have tested separately using the sample `mnistCUDNN` program from NVIDIA. The outofmemory message has appeared a few times, but there is clearly plenty of memory available to JAX; it preallocates a large chunk (about 12GB) without issue, yet still segfaults trying to do operations like `tensordot` on even tiny arrays.  What jax/jaxlib version are you using? jax 0.4.15, jaxlib 0.4.15  Which accelerator(s) are you using? GPU  Additional system info Ubuntu Linux 22.04  NVIDIA GPU info ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Constant segfaults using latest JAX on NVIDIA Turing GPU, even on small arrays and with sufficient memory"," Description With three different installations of JAX on the same hardware, I have consistently encountered segfaults on all but the most trivial operations. For example, ``` import jax.numpy as jnp a = jnp.ones((10, 10)) b = jnp.ones((10,)) jnp.tensordot(a, b, 1)   segfault happens here ``` Install scenarios follow.  1. Installed JAX from pip (with `pip install jax[cuda11_local]`). After digging deeper into the problem, found CuDNN was missing and installed it (version 8) and upgraded CUDA itself (to version 12.2).  2. Uninstalling and reinstalling from pip again (`pip install jax[cuda12_local]`) had exactly the same results regarding segfaults.  3. Built from source (following the instructions in the docs) and eventually got a successful build (`import jax` works without error) but no change in behavior. Using my build of JAX, I ran the unittests with Bazel and see that the vast majority of them (149 of 170) fail. I am including a snippet of one arbitrarilyselected log file below. ``` Running tests under Python 3.11.0: /path/redacted/bin/python [ RUN      ] BCOOTest.test_bcoo_dot_general2 (props=BatchedDotGeneralProperties(lhs_shape=(3, 2, 4, 4), rhs_shape=(4, 2, 4, 3), n_batch=2, n_dense=0, dimension_numbers=(([2], [0]), ([], []))), dtype=) 20230829 17:56:16.088272: W external/xla/xla/service/platform_util.cc:198] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 16877027328 I0829 17:56:16.088498 140276183101440 xla_bridge.py:514] Unable to initialize backend 'cuda': INTERNAL: no supported devices found for platform CUDA I0829 17:56:16.088726 140276183101440 xla_bridge.py:514] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: ""rocm"". Available platform names are: CUDA Interpreter I0829 17:56:16.089131 140276183101440 xla_bridge.py:514] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory Fatal Python error: Segmentation fault ``` JAX was built against the correct CUDA libraries, which I have tested separately using the sample `mnistCUDNN` program from NVIDIA. The outofmemory message has appeared a few times, but there is clearly plenty of memory available to JAX; it preallocates a large chunk (about 12GB) without issue, yet still segfaults trying to do operations like `tensordot` on even tiny arrays.  What jax/jaxlib version are you using? jax 0.4.15, jaxlib 0.4.15  Which accelerator(s) are you using? GPU  Additional system info Ubuntu Linux 22.04  NVIDIA GPU info ``` ++  ++++ ```",2023-08-29T19:29:17Z,bug,closed,0,12,https://github.com/jax-ml/jax/issues/17349,"I'd like to update this bug with some additional information I've been able to gather. Using `faulthandler` and JAX's logging environment variables, I now know about where the segfault happens, and can share additional details of the unusual behavior I'm seeing.  Testing script ```py import os import faulthandler faulthandler.enable()  avoid the oom errors os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.10'  XXX: if the line below is uncommented, jax stops detecting any   gpu at all; but as long as this isn't set, jax sees the gpu just fine os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'gpu' os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'  to ensure jax finds a device:  import jax  print(jax.device_count())  to ensure jax is detecting gpu backend:  from jax.lib import xla_bridge  print(xla_bridge.get_backend().platform) import jax.numpy as jnp a = jnp.zeros((10,10)) b = jnp.zeros((10,10)) for _ in range(10):     z = jnp.tensordot(a, b, 2) ```  Backtrace and logs ``` 20230914 12:13:57.945337: I external/xla/xla/pjrt/tfrt_cpu_pjrt_client.cc:462] TfrtCpuClient created. 20230914 12:13:58.041508: I external/xla/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230914 12:13:58.041708: I external/xla/xla/service/service.cc:168] XLA service 0x20d05f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 20230914 12:13:58.041722: I external/xla/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 5000, Compute Capability 7.5 20230914 12:13:58.041948: I external/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc:633] Using BFC allocator. 20230914 12:13:58.041983: I external/xla/xla/pjrt/gpu/gpu_helpers.cc:105] XLA backend allocating 1687702732 bytes on device 0 for BFCAllocator. 20230914 12:13:58.109273: I external/xla/xla/stream_executor/cuda/cuda_dnn.cc:440] Loaded cuDNN version 8905 20230914 12:13:58.160870: I external/xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:52] Using nvlink for parallel linking Fatal Python error: Segmentation fault Current thread 0x00007f8a7f3c6000 (most recent call first):   File ""/path/redacted/jax/_src/array.py"", line 761 in _array_shard_arg   File ""/path/redacted/jax/_src/interpreters/pxla.py"", line 118 in shard_arg   File ""/path/redacted/jax/_src/interpreters/pxla.py"", line 142 in    File ""/path/redacted/jax/_src/interpreters/pxla.py"", line 142 in shard_args   File ""/path/redacted/jax/_src/profiler.py"", line 314 in wrapper   File ""/path/redacted/jax/_src/interpreters/pxla.py"", line 1061 in __call__   File ""/path/redacted/jax/_src/interpreters/pxla.py"", line 1217 in __call__   File ""/path/redacted/jax/_src/profiler.py"", line 314 in wrapper   File ""/path/redacted/jax/_src/dispatch.py"", line 226 in    File ""/path/redacted/jax/_src/dispatch.py"", line 143 in apply_primitive   File ""/path/redacted/jax/_src/core.py"", line 821 in process_primitive   File ""/path/redacted/jax/_src/core.py"", line 389 in bind_with_trace   File ""/path/redacted/jax/_src/core.py"", line 386 in bind   File ""/path/redacted/jax/_src/lax/lax.py"", line 752 in dot_general   File ""/path/redacted/jax/_src/numpy/lax_numpy.py"", line 3182 in tensordot   File ""test.py"", line 22 in  Extension modules: jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator (total: 14) Segmentation fault (core dumped) ```","Hmm.. Well, we can't reproduce this, which makes it hard to debug. If you wanted to help a bit more, can you grab a C++ backtrace with `gdb`? A Python stack trace isn't really enough to speculate on the cause. You can probably do it something like this: ``` gdb /path/to/python core  at the gdb prompt: bt ``` and attach the output. Another way would be to attach to a running process, e.g. run an `ipython` session in one window, find its process ID with `ps`, and then do: ``` gdb  at the gdb prompt attach  cont  trigger the crash in the other window, e.g. %run myscript.py bt ``` and then share the trace.","I'm more than happy to help track this down; I've used JAX pretty extensively before and never run into a problem like this. When I examine the core dump, there are 140 threads (!) and most of them have the exact same backtrace: ``` CC(未找到相关数据)  __futex_abstimed_wait_common64 (private=0, cancel=true, abstime=0x0, op=393, expected=0,      futex_word=0x7f72ebf4c0e0 ) at ./nptl/futexinternal.c:57 57      in ./nptl/futexinternal.c (gdb) bt CC(未找到相关数据)  __futex_abstimed_wait_common64 (private=0, cancel=true, abstime=0x0, op=393, expected=0,      futex_word=0x7f72ebf4c0e0 ) at ./nptl/futexinternal.c:57 CC(Python 3 compatibility issues)  __futex_abstimed_wait_common (cancel=true, private=0, abstime=0x0, clockid=0, expected=0,      futex_word=0x7f72ebf4c0e0 ) at ./nptl/futexinternal.c:87 CC(Explicit tuples are not valid function parameters in Python 3)  __GI___futex_abstimed_wait_cancelable64 (     futex_word=futex_word=0x7f72ebf4c0e0 , expected=expected=0,      clockid=clockid=0, abstime=abstime=0x0, private=private=0)     at ./nptl/futexinternal.c:139 CC(Undefined name: from ..core import JaxTuple)  0x00007f72f6031ac1 in __pthread_cond_wait_common (abstime=0x0, clockid=0,      mutex=0x7f72ebf4c090 , cond=0x7f72ebf4c0b8 )     at ./nptl/pthread_cond_wait.c:503 CC(Undefined name: from six.moves import xrange)  ___pthread_cond_wait (cond=0x7f72ebf4c0b8 ,      mutex=0x7f72ebf4c090 ) at ./nptl/pthread_cond_wait.c:627 CC(Building on OSX with CUDA)  0x00007f72ea2d16fb in blas_thread_server ()    from /path/redacted/numpy/core/../../numpy.libs/libopenblas64_pr05007b62f.3.23.dev.so CC(Made a shim to handle configuration without having absl parse flags)  0x00007f72f6032b43 in start_thread (arg=) at ./nptl/pthread_create.c:442 CC(Quickish check)  0x00007f72f60c4a00 in clone3 () at ../sysdeps/unix/sysv/linux/x86_64/clone3.S:81 ``` The other threads' backtraces appear to contain Python API calls and nothing else (`PyEval_*`, `PyObject_*`, etc.). This is a lot to try to sift through, and I don't even see any JAXrelated code. I don't import NumPy in the script at all (just `jax.numpy`), so I'm not sure why it's appearing in the backtrace. Any thoughts?","Oops, I managed to find a thread that at least includes jaxlib in the backtrace. ``` CC(未找到相关数据)  __futex_abstimed_wait_common64 (private=0, cancel=true, abstime=0x0, op=393, expected=0,      futex_word=0x3c17558) at ./nptl/futexinternal.c:57 57      ./nptl/futexinternal.c: No such file or directory. (gdb) bt CC(未找到相关数据)  __futex_abstimed_wait_common64 (private=0, cancel=true, abstime=0x0, op=393, expected=0,      futex_word=0x3c17558) at ./nptl/futexinternal.c:57 CC(Python 3 compatibility issues)  __futex_abstimed_wait_common (cancel=true, private=0, abstime=0x0, clockid=0, expected=0,      futex_word=0x3c17558) at ./nptl/futexinternal.c:87 CC(Explicit tuples are not valid function parameters in Python 3)  __GI___futex_abstimed_wait_cancelable64 (futex_word=futex_word=0x3c17558,      expected=expected=0, clockid=clockid=0, abstime=abstime=0x0,      private=private=0) at ./nptl/futexinternal.c:139 CC(Undefined name: from ..core import JaxTuple)  0x00007faccc688ac1 in __pthread_cond_wait_common (abstime=0x0, clockid=0, mutex=0x3c17508,      cond=0x3c17530) at ./nptl/pthread_cond_wait.c:503 CC(Undefined name: from six.moves import xrange)  ___pthread_cond_wait (cond=0x3c17530, mutex=0x3c17508) at ./nptl/pthread_cond_wait.c:627 CC(Building on OSX with CUDA)  0x00007facc8a9d747 in std::condition_variable::wait(std::unique_lock&) ()    from /lib/x86_64linuxgnu/libstdc++.so.6 CC(Made a shim to handle configuration without having absl parse flags)  0x00007facb245ed93 in Eigen::ThreadPoolTempl::WaitForWork(Eigen::EventCount::Waiter*, tsl::thread::EigenEnvironment::Task*) ()    from /path/redacted/jaxlib/xla_extension.so CC(Quickish check)  0x00007facb245f605 in Eigen::ThreadPoolTempl::WorkerLoop(int) () from /path/redacted/jaxlib/xla_extension.so CC(Quickish check)  0x00007facb245d3d3 in void absl::lts_20230125::internal_any_invocable::RemoteInvoker)::{lambda() CC(Python 3 compatibility issues)}&>(absl::lts_20230125::internal_any_invocable::TypeErasedState*) ()    from /path/redacted/jaxlib/xla_extension.so CC(Adding quickstart notebook, and corresponding gitignore rules)  0x00007facb244980f in tsl::(anonymous namespace)::PThread::ThreadFn(void*) ()    from /path/redacted/jaxlib/xla_extension.so CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) 0x00007faccc689b43 in start_thread (arg=) at ./nptl/pthread_create.c:442 CC(Split out `jax` and `jaxlib` packages) 0x00007faccc71ba00 in clone3 () at ../sysdeps/unix/sysv/linux/x86_64/clone3.S:81 ``` No idea if it's helpful, but it may be more relevant than the above.","I'd really want to see the backtrace from the faulting thread. If you run by attaching to the process, then `gdb` will give you that thread by default. Also you could set `NPROC=1` to reduce the size of several of JAX's thread pools while debugging. (JAX starts one or two threads per CPU core, in the form of a thread pool. Threads that are sitting at a futex wait inside a ThreadPool are likely idle and waiting for work). And the other thread you have there comes from a similar threadpool from OpenBLAS, which probably comes from numpy or scipy.","Sure, here you go. This should be from the thread with the segfault, since I didn't switch threads this time. ``` CC(未找到相关数据)  0x00000000005266a0 in ?? () CC(Python 3 compatibility issues)  0x00000000005074c9 in _PyEval_EvalFrameDefault () CC(Explicit tuples are not valid function parameters in Python 3)  0x0000000000531823 in _PyFunction_Vectorcall () CC(Undefined name: from ..core import JaxTuple)  0x00000000004fefe1 in _PyEval_EvalFrameDefault () CC(Undefined name: from six.moves import xrange)  0x0000000000531823 in _PyFunction_Vectorcall () CC(Building on OSX with CUDA)  0x00000000004e145a in ?? () CC(Made a shim to handle configuration without having absl parse flags)  0x00000000005bc2f0 in ?? () CC(Quickish check)  0x000000000050d745 in PyObject_Vectorcall () CC(Quickish check)  0x00000000004fb152 in _PyEval_EvalFrameDefault () CC(Adding quickstart notebook, and corresponding gitignore rules)  0x00000000004eefb1 in _PyObject_FastCallDictTstate () CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) 0x000000000053c709 in _PyObject_Call_Prepend () CC(Split out `jax` and `jaxlib` packages) 0x00000000005cbea7 in ?? () CC(Update the quickstart notebook.) 0x00000000004e75dc in _PyObject_MakeTpCall () CC(Fixing logo size so resize is not required) 0x00000000004fb152 in _PyEval_EvalFrameDefault () CC(Add copyright notice to quickstart notebook.) 0x0000000000531823 in _PyFunction_Vectorcall () CC(rename in_bdims, out_bdims > in_axes, out_axes) 0x00000000004fefe1 in _PyEval_EvalFrameDefault () CC(Add wheelbuilding scripts) 0x00000000004eefb1 in _PyObject_FastCallDictTstate () CC(Implement np.repeat for scalar repeats.) 0x000000000053c709 in _PyObject_Call_Prepend () CC(Populate readme) 0x00000000005cbea7 in ?? () CC(Notebook showing how to write gufuncs with vmap) 0x00000000005412a5 in PyObject_Call () CC(Fix link in gufuncs notebook) 0x00000000004fefe1 in _PyEval_EvalFrameDefault () CC(Typo) 0x0000000000531823 in _PyFunction_Vectorcall () CC(differention > differentiation) 0x00000000004fefe1 in _PyEval_EvalFrameDefault () CC(Typo, Python parens) 0x0000000000531823 in _PyFunction_Vectorcall () CC(attempt to centerjustify the jax logo in readme) 0x00000000004e145a in ?? () CC(Barebones neural network and data loading example notebook) 0x00000000005bc26f in ?? () CC(fix symbolic zero handling in concat transpose) 0x0000000000541195 in PyObject_Call () CC(Cloud TPU Support) 0x00000000004fefe1 in _PyEval_EvalFrameDefault () CC(examples/datasets.py doesn’t work in python3) 0x000000000062e1b4 in ?? () CC(Add support for `np.trace` ) 0x00000000004f3a67 in PyEval_EvalCode () CC(Error on NaN?) 0x0000000000569033 in ?? () CC(Bug in examples?) 0x000000000050d977 in ?? () CC(Fix the bug in classifier example, batching_test and README) 0x000000000050d745 in PyObject_Vectorcall () CC(Broadcasting of size0 dimensions not implemented) 0x00000000004fb152 in _PyEval_EvalFrameDefault () CC(minor spelling tweaks) 0x000000000055e097 in ?? () CC(CUDA90 and py3 ) 0x000000000055c852 in ?? () CC(add dot_general batching rule) 0x00000000004fefe1 in _PyEval_EvalFrameDefault () Type  for more, q to quit, c to continue without pagingc CC(np.einsum support) 0x000000000062e1b4 in ?? () CC(Require protobuf 3.6.0 or later) 0x00000000004f3a67 in PyEval_EvalCode () CC(Hard crash when no compatible cuda devices found) 0x0000000000569033 in ?? () CC(Invalid proto descriptor for file ""tensorflow/compiler/xla/xla_data.proto"") 0x000000000050d977 in ?? () CC(Fix support for arrays with size0 dimensions.) 0x000000000050d745 in PyObject_Vectorcall () CC(Set distinct_host_configuration=false in the bazel options.) 0x00000000004fb152 in _PyEval_EvalFrameDefault () CC(Open Source Contributions) 0x00000000005c21b7 in PyIter_Send () CC(np.linalg.inv support) 0x00000000004ff4ed in _PyEval_EvalFrameDefault () CC(Feature request: export TF ops) 0x00000000005c21b7 in PyIter_Send () CC(Update XLA and reenable numpy tests that failed on Mac) 0x00000000004ff4ed in _PyEval_EvalFrameDefault () CC(jacrev and jacfwd usage example) 0x0000000000640cb0 in ?? () CC(Unimplemented: binary integer op 'power') 0x000000000064243c in ?? () CC(Update neural_network_and_data_loading.ipynb) 0x000000000053e384 in ?? () CC(Update README.md) 0x000000000050d745 in PyObject_Vectorcall () CC(add docstrings for major public functions) 0x00000000004fb152 in _PyEval_EvalFrameDefault () CC(Scenarios to prefer over cupy) 0x000000000055e097 in ?? () CC(More informative error on trying to concatenate zerodimensional arrays) 0x000000000055c8b4 in ?? () CC(Batching rules for pad and concatenate primitives not implemented) 0x0000000000541195 in PyObject_Call () CC(np.rot90 support) 0x00000000004fefe1 in _PyEval_EvalFrameDefault () CC(Improving jax.scipy.stats) 0x000000000062e1b4 in ?? () CC(v0.2 tasks) 0x00000000004f3a67 in PyEval_EvalCode () CC(Frequenty asked questions doc) 0x0000000000647c37 in ?? () CC(Autodiff cookbook) 0x0000000000645350 in ?? () CC(Vmap cookbook) 0x0000000000650d15 in ?? () CC(Docstrings in api.py) 0x0000000000650a64 in _PyRun_SimpleFileObject () CC(Upload existing JAX talks to docs/ or talks/ folder) 0x0000000000650833 in _PyRun_AnyFileObject () CC(Keras NumPy backend demo) 0x000000000064f787 in Py_RunMain () CC(JAX philosophy, or ""culture and values"" doc) 0x000000000061ee0d in Py_BytesMain () CC(Rename jaxlib to xlapy) 0x00007fbe20c70d90 in __libc_start_call_main (main=main=0x61ed60, argc=argc=2, argv=argv=0x7ffc62908d98) at ../sysdeps/nptl/libc_start_call_main.h:58 CC(Conda installations) 0x00007fbe20c70e40 in __libc_start_main_impl (main=0x61ed60, argc=2, argv=0x7ffc62908d98, init=, fini=, rtld_fini=, stack_end=0x7ffc62908d88) at ../csu/libcstart.c:392 CC(Open source tests) 0x000000000061ec95 in _start () ```","That's really strange! It doesn't even seem to be calling jax code in that thread. I wonder if there's a way for me to reproduce this? e.g.,: * it sounds like you're using Ubuntu 22.04? * how did you install Python? * how exactly did you install CUDA? The best idea I have is to try reproducing this in a cloud VM.","Yeah, I'm pretty stumped, too. I'm using Kubuntu 22.04 (the latest LTS). The Python I'm running is installed via APT. As for CUDA, it's from NVIDIA's repo, instructions here.","Continuing the theme of bizarre behavior, the test script works without segfaulting if I run under Python 3.10 instead of 3.11. It was something I tried on a whim after reinstalling all my CUDA software, which made no difference. In a clean Python 3.11 virtual environment, the segfault returns. I'm curious to hear if anyone was able to reproduce this.","Sorry about bumping this thread again, but I have a little more information now. The version of Python 3.11 that APT provides (and so the one I've been using in a virtual environment) is 3.11.0rc1, which is not the latest 3.11 available. So I built Python 3.11.5 from sources downloaded straight from the Python site. The segfault is gone under this version, using the same test script as before. I suppose it's possible this was a bug in Python itself that had already been fixed, exacerbated by some quirk of my particular system? As  noted, the segfault doesn't even happen in JAX code, but in Python itself. Given the new info, my problem is essentially resolved for now, so I don't know if it's worth pursuing any further.","Without looking further, I think we can assume it was a bug in a prerelease Python. If it happens again with a released version be sure to let us know! Thanks for digging into this, I'm not sure I would have ever figured this out.","> Without looking further, I think we can assume it was a bug in a prerelease Python.  In a completely different setup, I also happened to be seeing a similar error. I also was using a release candidate (which was installed on Ubuntu 22.04).  Upgrading Python fixed it in my case (for Ubuntu 22.04 this meant just adding the deadsnakes ppa described here: https://vegastack.com/tutorials/howtoinstallpython311onubuntu2204/). Anyway, just wanted to report some evidence that points to your hunch being correct."
3086,"以下是一个github上的jax下的一个issue, 标题是(Sharp edge in (probably ill-advised) `custom_vmap` usecase)， 内容是 ( Description Hey  hope all is well! I'm trying to use `custom_vmap` to do something that probably isn't advised, but if it could work it would be awesome. I have a function that I want to be defined without a batch dimension and then to add the batch dimension via `vmap`. However, I have one component of the function that I would like to act on a flattened representation of the data, even in the batched setting. I can do this with `custom_vmap` using something like this ``` import jax.numpy as jnp from jax.custom_batching import custom_vmap from jax import grad, vmap  def inner_flatten(x, idx):   (deltas,) = jnp.nonzero(idx[1:]  idx[:1], size=len(idx)  1)   deltas = jnp.concatenate((deltas, jnp.array([len(idx)])))   return x, idx, deltas .def_vmap def _(batch_size, is_batched, x, idx):   return inner_flatten(x.reshape((1,)), idx.reshape((1,))), (False, False, False)  def inner_unflatten(x, x_flat):   return x_flat .def_vmap def _(batch_size, is_batched, x, x_flat):   if is_batched[0]:     return x_flat.reshape(x.shape), True   return x_flat.reshape(x.shape), False def thing(x):   idx = jnp.arange(15)   x_flat, idx, deltas = inner_flatten(x, idx)    Do stuff....   return inner_unflatten(x, x_flat) def thing2(x):   return jnp.sum(vmap(thing)(x)) x = jnp.ones((3, 5)) grad(thing2)(x) ``` which works great! However, this breaks with AD (either forward or backward) and ``` grad(outer_thing)(x) ``` gives the error: ``` JaxStackTraceBeforeTransformation: TypeError: lax.concatenate requires arguments to have the same dtypes, got [('float0', 'V')], int32. (Tip: jnp.concatenate is a similar function that does automatic type promotion on inputs). The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last)     [... skipping hidden 10 frame]     [... skipping hidden 3 frame]     [... skipping hidden 9 frame]     [... skipping hidden 25 frame] /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in check_same_dtypes(name, *avals)    4779       equiv = _JNP_FUNCTION_EQUIVALENTS[name]    4780       msg += f"" (Tip: jnp.{equiv} is a similar function that does automatic type promotion on inputs)."" > 4781     raise TypeError(msg.format(name, "", "".join(str(a.dtype) for a in avals)))    4782     4783  TypeError: lax.concatenate requires arguments to have the same dtypes, got [('float0', 'V')], int32. (Tip: jnp.concatenate is a similar function that does automatic type promotion on inputs). ``` Is this out of scope or is this a sane thing to try to do?  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Sharp edge in (probably ill-advised) `custom_vmap` usecase," Description Hey  hope all is well! I'm trying to use `custom_vmap` to do something that probably isn't advised, but if it could work it would be awesome. I have a function that I want to be defined without a batch dimension and then to add the batch dimension via `vmap`. However, I have one component of the function that I would like to act on a flattened representation of the data, even in the batched setting. I can do this with `custom_vmap` using something like this ``` import jax.numpy as jnp from jax.custom_batching import custom_vmap from jax import grad, vmap  def inner_flatten(x, idx):   (deltas,) = jnp.nonzero(idx[1:]  idx[:1], size=len(idx)  1)   deltas = jnp.concatenate((deltas, jnp.array([len(idx)])))   return x, idx, deltas .def_vmap def _(batch_size, is_batched, x, idx):   return inner_flatten(x.reshape((1,)), idx.reshape((1,))), (False, False, False)  def inner_unflatten(x, x_flat):   return x_flat .def_vmap def _(batch_size, is_batched, x, x_flat):   if is_batched[0]:     return x_flat.reshape(x.shape), True   return x_flat.reshape(x.shape), False def thing(x):   idx = jnp.arange(15)   x_flat, idx, deltas = inner_flatten(x, idx)    Do stuff....   return inner_unflatten(x, x_flat) def thing2(x):   return jnp.sum(vmap(thing)(x)) x = jnp.ones((3, 5)) grad(thing2)(x) ``` which works great! However, this breaks with AD (either forward or backward) and ``` grad(outer_thing)(x) ``` gives the error: ``` JaxStackTraceBeforeTransformation: TypeError: lax.concatenate requires arguments to have the same dtypes, got [('float0', 'V')], int32. (Tip: jnp.concatenate is a similar function that does automatic type promotion on inputs). The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last)     [... skipping hidden 10 frame]     [... skipping hidden 3 frame]     [... skipping hidden 9 frame]     [... skipping hidden 25 frame] /usr/local/lib/python3.10/distpackages/jax/_src/lax/lax.py in check_same_dtypes(name, *avals)    4779       equiv = _JNP_FUNCTION_EQUIVALENTS[name]    4780       msg += f"" (Tip: jnp.{equiv} is a similar function that does automatic type promotion on inputs)."" > 4781     raise TypeError(msg.format(name, "", "".join(str(a.dtype) for a in avals)))    4782     4783  TypeError: lax.concatenate requires arguments to have the same dtypes, got [('float0', 'V')], int32. (Tip: jnp.concatenate is a similar function that does automatic type promotion on inputs). ``` Is this out of scope or is this a sane thing to try to do?  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-08-29T07:32:40Z,bug,open,2,0,https://github.com/jax-ml/jax/issues/17342
1375,"以下是一个github上的jax下的一个issue, 标题是(Make mlir.custom_call() more general and expose it as jax.interpreters.mlir.custom_call().)， 内容是 (Make mlir.custom_call() more general and expose it as jax.interpreters.mlir.custom_call(). This change is in preparation for deprecating the XlaBuilder APIs for building nonMLIR HLO. In general JAX would be best served by adding a more userfriendly ""custom kernel"" API that doesn't require the user to build IR directly, but for the moment the best we can do is migrate users to use MLIR/StableHLO utilities instead of classic HLO utilities. Since most users of custom kernels probably want to build a customcall we can get most of the benefit by providing an ergonomic helper function for building the IR for custom calls that can be called by external primitive lowering rules. This function has two benefits over just building the stablehlo directly: a) it is a JAX API, and we can be more confident the API won't change because of upstream MLIR changes b) the Python API to build stablehlo.custom_call generated by the bindings isn't that easy to use (e.g. it doesn't have sensible defaults). Next step will be to deprecate XlaBuilder and encourage users to switch to lowering rules using this helper.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Make mlir.custom_call() more general and expose it as jax.interpreters.mlir.custom_call().,"Make mlir.custom_call() more general and expose it as jax.interpreters.mlir.custom_call(). This change is in preparation for deprecating the XlaBuilder APIs for building nonMLIR HLO. In general JAX would be best served by adding a more userfriendly ""custom kernel"" API that doesn't require the user to build IR directly, but for the moment the best we can do is migrate users to use MLIR/StableHLO utilities instead of classic HLO utilities. Since most users of custom kernels probably want to build a customcall we can get most of the benefit by providing an ergonomic helper function for building the IR for custom calls that can be called by external primitive lowering rules. This function has two benefits over just building the stablehlo directly: a) it is a JAX API, and we can be more confident the API won't change because of upstream MLIR changes b) the Python API to build stablehlo.custom_call generated by the bindings isn't that easy to use (e.g. it doesn't have sensible defaults). Next step will be to deprecate XlaBuilder and encourage users to switch to lowering rules using this helper.",2023-08-28T22:34:25Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17338
286,"以下是一个github上的jax下的一个issue, 标题是(Remove tests for jax.numpy.in1d, which is deprecated.)， 内容是 (Remove tests for jax.numpy.in1d, which is deprecated.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Remove tests for jax.numpy.in1d, which is deprecated.","Remove tests for jax.numpy.in1d, which is deprecated.",2023-08-28T20:36:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17336
2707,"以下是一个github上的jax下的一个issue, 标题是(Improve compilation speed for generated codes with automatically marginalized MCMC)， 内容是 (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Hi JAX team, this is Jinlin Lai, an author of the ICML paper Automatically Marginalized MCMC in Probabilistic Programming. Thank you for inviting me to open this issue at AABI!  When doing this work, we have observed excessive compilation time for the gradient function of the log density function of certain models, especially when a node with hundreds of children is marginalized. As you could imagine very complicated codes can be generated by Algorithm 1 and Algorithm 2 in our paper. A small model provided in Appendix G is $$ x\sim \mathcal{N}(0,1), \log\sigma\sim\mathcal{N}(0,1), y_i\sim\mathcal{N}(x,\sigma^2), $$ where $y_1,...y_N$ are observed. After marginalizing $x$, codes of $\mathcal{O}(N)$ length are generated, yet the compilation time grows superlinear with $N$. This can also be reproduced with manual marginalization. The following codes are the marginalized model by manually executing Algorithm 1 and Algorithm 2. ```python import jax.numpy as jnp from jax import grad, jit import numpyro.distributions as dist def model(x, log_s, y):     N = len(y)     ans = dist.Normal().log_prob(x) + dist.Normal().log_prob(log_s)     s = jnp.exp(log_s)     x_m = 0.     x_s = 1.     for i in range(N):         k = x_s * x_s / (x_s * x_s + s * s)         y_m = x_m         y_s = jnp.sqrt(x_s * x_s + s * s)         ans += dist.Normal(y_m, y_s).log_prob(y[i])         x_m = x_m + k * (y[i]  y_m)         x_s = jnp.sqrt(1  k) * x_s     return ans f_jit = jit(model) g = grad(model, argnums=(0, 1, 2)) g_jit = jit(g) print(f_jit(0.,0.,jnp.zeros(500))) print(g_jit(0.,0.,jnp.zeros(500))) ``` When running the codes, `f_jit` takes seconds, but `g_jit` is super slow. With additional experiments it can be found that the lines of Jaxprs in `g` do not grow significantly over `f` (and still stay linear with respect to $N$), so it could be more related to the structure of the computation graph. I am wondering what can be done in the compiler's side to improve the `jit` performance of functions like this.  Note that the codes are written to simulate generated codes. In the generated codes the for loop is completely unrolled. With manual inspection it is possible to optimize the above codes with scan, but it is beyond my knowledge to automatically generate scan given a graphical model representation. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve compilation speed for generated codes with automatically marginalized MCMC,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Hi JAX team, this is Jinlin Lai, an author of the ICML paper Automatically Marginalized MCMC in Probabilistic Programming. Thank you for inviting me to open this issue at AABI!  When doing this work, we have observed excessive compilation time for the gradient function of the log density function of certain models, especially when a node with hundreds of children is marginalized. As you could imagine very complicated codes can be generated by Algorithm 1 and Algorithm 2 in our paper. A small model provided in Appendix G is $$ x\sim \mathcal{N}(0,1), \log\sigma\sim\mathcal{N}(0,1), y_i\sim\mathcal{N}(x,\sigma^2), $$ where $y_1,...y_N$ are observed. After marginalizing $x$, codes of $\mathcal{O}(N)$ length are generated, yet the compilation time grows superlinear with $N$. This can also be reproduced with manual marginalization. The following codes are the marginalized model by manually executing Algorithm 1 and Algorithm 2. ```python import jax.numpy as jnp from jax import grad, jit import numpyro.distributions as dist def model(x, log_s, y):     N = len(y)     ans = dist.Normal().log_prob(x) + dist.Normal().log_prob(log_s)     s = jnp.exp(log_s)     x_m = 0.     x_s = 1.     for i in range(N):         k = x_s * x_s / (x_s * x_s + s * s)         y_m = x_m         y_s = jnp.sqrt(x_s * x_s + s * s)         ans += dist.Normal(y_m, y_s).log_prob(y[i])         x_m = x_m + k * (y[i]  y_m)         x_s = jnp.sqrt(1  k) * x_s     return ans f_jit = jit(model) g = grad(model, argnums=(0, 1, 2)) g_jit = jit(g) print(f_jit(0.,0.,jnp.zeros(500))) print(g_jit(0.,0.,jnp.zeros(500))) ``` When running the codes, `f_jit` takes seconds, but `g_jit` is super slow. With additional experiments it can be found that the lines of Jaxprs in `g` do not grow significantly over `f` (and still stay linear with respect to $N$), so it could be more related to the structure of the computation graph. I am wondering what can be done in the compiler's side to improve the `jit` performance of functions like this.  Note that the codes are written to simulate generated codes. In the generated codes the for loop is completely unrolled. With manual inspection it is possible to optimize the above codes with scan, but it is beyond my knowledge to automatically generate scan given a graphical model representation. ",2023-08-28T20:09:58Z,enhancement,open,2,0,https://github.com/jax-ml/jax/issues/17335
2075,"以下是一个github上的jax下的一个issue, 标题是(Broken compilation in embarrassing parallelization with `concurrent.futures.ProcessPoolExecutor` and `jax.experimental.ode`)， 内容是 ( Description I noticed that parallelization via `concurrent.futures.ProcessPoolExecutor` does trying to jitcompile a function that is using an already jitcompiled function inside a worker. I could not reduce it down to a MWE _without_ `jax.experimental.ode`, so my suspicion is that it is specific to that. **MWE** When trying to execute the following, it simply gets stuck and checking the table of processes on my machine indicates that nothing is happening. A single execution works as expected. ```python import concurrent.futures import jax import jax.numpy as jnp from jax.experimental.ode import odeint .jit def loss(theta):     M = jnp.eye(10)     y0 = theta     def func(y, t):         return 1j * M @ y     ys = odeint(func, y0, jnp.linspace(0., 10., 1000))     y = ys[1]     return y.conj().T @ M @ y print(loss(jnp.ones(10, dtype=complex))) max_workers = 4 hyper_params = [seed for seed in range(max_workers)] def _wrap_run_job(hyp):     seed = hyp     key = jax.random.PRNGKey(seed)     theta = 1.*jax.random.normal(key, shape=(10,), dtype=complex)     .jit     def loss2(theta):         return loss(theta)**2     print(loss2(theta)) with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:    exec_map = executor.map(_wrap_run_job, hyper_params)    tuple(circuit for circuit in exec_map) ``` I noticed is that if I dont compile `loss` before the execution (i.e. comment out `print(loss(theta))`, there is no problem and it executes as expected. So my suspicion is that the problem is stemming from multiple workers trying to simultaneously access the compiled cache.  What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? CPU  Additional system info Python 3.11, ubuntu WSL  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Broken compilation in embarrassing parallelization with `concurrent.futures.ProcessPoolExecutor` and `jax.experimental.ode`," Description I noticed that parallelization via `concurrent.futures.ProcessPoolExecutor` does trying to jitcompile a function that is using an already jitcompiled function inside a worker. I could not reduce it down to a MWE _without_ `jax.experimental.ode`, so my suspicion is that it is specific to that. **MWE** When trying to execute the following, it simply gets stuck and checking the table of processes on my machine indicates that nothing is happening. A single execution works as expected. ```python import concurrent.futures import jax import jax.numpy as jnp from jax.experimental.ode import odeint .jit def loss(theta):     M = jnp.eye(10)     y0 = theta     def func(y, t):         return 1j * M @ y     ys = odeint(func, y0, jnp.linspace(0., 10., 1000))     y = ys[1]     return y.conj().T @ M @ y print(loss(jnp.ones(10, dtype=complex))) max_workers = 4 hyper_params = [seed for seed in range(max_workers)] def _wrap_run_job(hyp):     seed = hyp     key = jax.random.PRNGKey(seed)     theta = 1.*jax.random.normal(key, shape=(10,), dtype=complex)     .jit     def loss2(theta):         return loss(theta)**2     print(loss2(theta)) with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:    exec_map = executor.map(_wrap_run_job, hyper_params)    tuple(circuit for circuit in exec_map) ``` I noticed is that if I dont compile `loss` before the execution (i.e. comment out `print(loss(theta))`, there is no problem and it executes as expected. So my suspicion is that the problem is stemming from multiple workers trying to simultaneously access the compiled cache.  What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? CPU  Additional system info Python 3.11, ubuntu WSL  NVIDIA GPU info _No response_",2023-08-28T15:05:33Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/17329
402,"以下是一个github上的jax下的一个issue, 标题是(Update __init__.py to include dlpack module)， 内容是 (import for dlpack module was missing in `__init__.py` file. just added that. will also closes the issue CC(AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__'))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update __init__.py to include dlpack module,import for dlpack module was missing in `__init__.py` file. just added that. will also closes the issue CC(AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__'),2023-08-28T05:02:15Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/17322,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
7735,"以下是一个github上的jax下的一个issue, 标题是(jaxlib.xla_extension.XlaRuntimeError due to failed: Shared memory requested exceeds device resources)， 内容是 ( Description **Hi, there. While applying FusedAttention with jaxtriton, the following XLA error happens on NvidiaA100:** 20230828 03:06:51.319566: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.319790: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. 20230828 03:06:51.319901: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.320187: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.320240: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. 20230828 03:06:51.320386: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. 20230828 03:06:51.320465: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.320846: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code     exec(code, run_globals)   File ""/workspace/paxml/paxml/main.py"", line 510, in      app.run(main, flags_parser=absl_flags.flags_parser)   File ""/usr/local/lib/python3.10/distpackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/usr/local/lib/python3.10/distpackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/workspace/paxml/paxml/main.py"", line 445, in main     _main(argv)   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/main.py"", line 487, in _main     run(experiment_config=experiment_config,   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/main.py"", line 420, in run     run_experiment(   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/main.py"", line 285, in run_experiment     train.train_and_evaluate(   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/train.py"", line 274, in train_and_evaluate     executor.start()   File ""/workspace/paxml/paxml/executors.py"", line 269, in start     _train_and_evaluate_common(   File ""/workspace/paxml/paxml/executors.py"", line 406, in _train_and_evaluate_common     program_output = train_program.run(partitioned_train_state, step_i)   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/programs.py"", line 332, in run     new_step, new_state, train_outputs = self.train_step(   File ""/workspace/paxml/paxml/programs.py"", line 620, in train_step     return step + 1, *train_step(state, prng_key, inputs, static_args)   File ""/workspace/paxml/paxml/trainer_lib.py"", line 1634, in call     return pjitted_fn(*args) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well). **Steps for reproducing:** Add model variants to **/root/.local/lib/python3.10/sitepackages/paxml/tasks/lm/params/nvidia.py** ```  a/paxml/tasks/lm/params/nvidia.py +++ b/paxml/tasks/lm/params/nvidia.py @@ 350,6 +350,20 @@ class NVIDIA70BProxy(NVIDIA5B):    MODEL_DIMS = 8192    HIDDEN_DIMS = 4 * 8192 +.register +class test7B(NVIDIA70BProxy): +  PERCORE_BATCH_SIZE = 16 +  MICROBATCH_SIZE = 1 +  USE_FLASH_ATTENTION = False +  USE_TRITON_LAYER_NORM = False +  NUM_LAYERS = 8 +  NUM_STAGES = 4 +  ICI_MESH_SHAPE = [4, 1, 1, 1] + +.register +class test7BFA(test7B): +  USE_FLASH_ATTENTION = True +  USE_TRITON_LAYER_NORM = True  .register  class NVIDIA116BProxy(NVIDIA5B): ``` **Run w FusedAttention:** python3 u m paxml.main noenable_checkpoint_saving job_log_dir=./jax_tmp exp=paxml.tasks.lm.params.nvidia.test7BFA **Versions:** ``` python3 m pip install git+https://github.com/google/paxml orbax==0.1.6 user python3 m pip install git+https://github.com/google/praxis user python3 m pip install git+https://github.com/google/flax user python3 m pip uninstall orbax orbaxcheckpoint y python3 m pip install git+https://github.com/google/orbax/subdirectory=checkpoint user python3 m pip uninstall triton y python3 m pip install git+https://github.com/openai/tritonsubdirectory=python python3 m pip uninstall jaxtriton y python3 m pip install git+https://github.com/jaxml/jaxtriton nodeps python3 m pip uninstall jax jaxlib y git clone https://github.com/google/jax pushd jax git checkout 8d80e25 build JAXLIB apt update y;apt install g++ y python3 m pip install numpy wheel build python3 build/build.py enable_cuda install JAX python3 setup.py develop user install JAXLIB python3 m pip install dist/*.whl popd  Change the used source of pallas.ops in praxis sed i 's/jax.experimental.pallas.ops/jax_triton.pallas.ops/g' /root/.local/lib/python3.10/sitepackages/praxis/layers/gpu_fast_attention.py ```  What jax/jaxlib version are you using? commit: 8d80e25  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info 4 A100SXM80GB GPUs)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jaxlib.xla_extension.XlaRuntimeError due to failed: Shared memory requested exceeds device resources," Description **Hi, there. While applying FusedAttention with jaxtriton, the following XLA error happens on NvidiaA100:** 20230828 03:06:51.319566: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.319790: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. 20230828 03:06:51.319901: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.320187: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.320240: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. 20230828 03:06:51.320386: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. 20230828 03:06:51.320465: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.320846: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these. The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code     exec(code, run_globals)   File ""/workspace/paxml/paxml/main.py"", line 510, in      app.run(main, flags_parser=absl_flags.flags_parser)   File ""/usr/local/lib/python3.10/distpackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/usr/local/lib/python3.10/distpackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/workspace/paxml/paxml/main.py"", line 445, in main     _main(argv)   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/main.py"", line 487, in _main     run(experiment_config=experiment_config,   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/main.py"", line 420, in run     run_experiment(   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/main.py"", line 285, in run_experiment     train.train_and_evaluate(   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/train.py"", line 274, in train_and_evaluate     executor.start()   File ""/workspace/paxml/paxml/executors.py"", line 269, in start     _train_and_evaluate_common(   File ""/workspace/paxml/paxml/executors.py"", line 406, in _train_and_evaluate_common     program_output = train_program.run(partitioned_train_state, step_i)   File ""/root/.local/lib/python3.10/sitepackages/praxis/py_utils.py"", line 1023, in wrapper     result = func(*args, **kwargs)   File ""/workspace/paxml/paxml/programs.py"", line 332, in run     new_step, new_state, train_outputs = self.train_step(   File ""/workspace/paxml/paxml/programs.py"", line 620, in train_step     return step + 1, *train_step(state, prng_key, inputs, static_args)   File ""/workspace/paxml/paxml/trainer_lib.py"", line 1634, in call     return pjitted_fn(*args) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well). **Steps for reproducing:** Add model variants to **/root/.local/lib/python3.10/sitepackages/paxml/tasks/lm/params/nvidia.py** ```  a/paxml/tasks/lm/params/nvidia.py +++ b/paxml/tasks/lm/params/nvidia.py @@ 350,6 +350,20 @@ class NVIDIA70BProxy(NVIDIA5B):    MODEL_DIMS = 8192    HIDDEN_DIMS = 4 * 8192 +.register +class test7B(NVIDIA70BProxy): +  PERCORE_BATCH_SIZE = 16 +  MICROBATCH_SIZE = 1 +  USE_FLASH_ATTENTION = False +  USE_TRITON_LAYER_NORM = False +  NUM_LAYERS = 8 +  NUM_STAGES = 4 +  ICI_MESH_SHAPE = [4, 1, 1, 1] + +.register +class test7BFA(test7B): +  USE_FLASH_ATTENTION = True +  USE_TRITON_LAYER_NORM = True  .register  class NVIDIA116BProxy(NVIDIA5B): ``` **Run w FusedAttention:** python3 u m paxml.main noenable_checkpoint_saving job_log_dir=./jax_tmp exp=paxml.tasks.lm.params.nvidia.test7BFA **Versions:** ``` python3 m pip install git+https://github.com/google/paxml orbax==0.1.6 user python3 m pip install git+https://github.com/google/praxis user python3 m pip install git+https://github.com/google/flax user python3 m pip uninstall orbax orbaxcheckpoint y python3 m pip install git+https://github.com/google/orbax/subdirectory=checkpoint user python3 m pip uninstall triton y python3 m pip install git+https://github.com/openai/tritonsubdirectory=python python3 m pip uninstall jaxtriton y python3 m pip install git+https://github.com/jaxml/jaxtriton nodeps python3 m pip uninstall jax jaxlib y git clone https://github.com/google/jax pushd jax git checkout 8d80e25 build JAXLIB apt update y;apt install g++ y python3 m pip install numpy wheel build python3 build/build.py enable_cuda install JAX python3 setup.py develop user install JAXLIB python3 m pip install dist/*.whl popd  Change the used source of pallas.ops in praxis sed i 's/jax.experimental.pallas.ops/jax_triton.pallas.ops/g' /root/.local/lib/python3.10/sitepackages/praxis/layers/gpu_fast_attention.py ```  What jax/jaxlib version are you using? commit: 8d80e25  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info 4 A100SXM80GB GPUs",2023-08-28T03:18:02Z,bug NVIDIA GPU,open,0,2,https://github.com/jax-ml/jax/issues/17320,"My guess is that this isn't a JAX bug, in the sense that this means that a custom Pallas/Triton kernel in PAX/Praxis is using an infeasible amount of shared memory. I think the fix would need to be to that kernel, not to JAX.","I believe that praxis is using the pallas kernel in Jax, so this is our issue."
2083,"以下是一个github上的jax下的一个issue, 标题是(bug(pallas): Unimplemented primitive in Pallas GPU lowering: sign)， 内容是 ( Description ```python start_q = pl.program_id(0) upper_bound = pl.cdiv(block_q * (start_q + 1), block_k) ``` or ```python start_q = pl.program_id(0) upper_bound = (block_q * (start_q + 1) + block_k   1) // block_k ``` in a `pallas_call` yields: ``` NotImplementedError: Unimplemented primitive in Pallas GPU lowering: sign. Please file an issue on https://github.com/google/jax/issues. ``` Seems like the jaxpr lowering just totally fails for (`a // b`) where `a` is an expression containing a `program_id` Traceback:  ``` >   out_flat = pallas_call_p.bind(         *consts, *flat_args, jaxpr=jaxpr, name=name, which_linear=which_linear,         in_shapes=tuple(jax.ShapeDtypeStruct(a.shape, a.dtype)                         for a in flat_args),         out_shapes=tuple(flat_out_shapes), debug=debug,         interpret=interpret,         grid_mapping=grid_mapping,         input_output_aliases=tuple(input_output_aliases.items()),         **compiler_params) E   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn: E     a:i32[] = pjit[ E     jaxpr={ lambda ; b:i32[] c:i32[]. let E         d:i32[] = convert_element_type[new_dtype=int32 weak_type=False] c E         e:i32[] = div b d E         f:i32[] = sign b E         g:i32[] = sign d E         h:bool[] = ne f g E         i:i32[] = rem b d E         j:bool[] = ne i 0 E         k:bool[] = and h j E         l:i32[] = sub e 1 E         m:i32[] = pjit[ E           jaxpr={ lambda ; n:bool[] o:i32[] p:i32[]. let E               q:i32[] = select_n n p o E             in (q,) } E           name=_where E         ] k l e E       in (m,) } E     name=floor_divide E   ] r 64 E   With context: ```   What jax/jaxlib version are you using? master  Which accelerator(s) are you using? GPU CC:  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,bug(pallas): Unimplemented primitive in Pallas GPU lowering: sign," Description ```python start_q = pl.program_id(0) upper_bound = pl.cdiv(block_q * (start_q + 1), block_k) ``` or ```python start_q = pl.program_id(0) upper_bound = (block_q * (start_q + 1) + block_k   1) // block_k ``` in a `pallas_call` yields: ``` NotImplementedError: Unimplemented primitive in Pallas GPU lowering: sign. Please file an issue on https://github.com/google/jax/issues. ``` Seems like the jaxpr lowering just totally fails for (`a // b`) where `a` is an expression containing a `program_id` Traceback:  ``` >   out_flat = pallas_call_p.bind(         *consts, *flat_args, jaxpr=jaxpr, name=name, which_linear=which_linear,         in_shapes=tuple(jax.ShapeDtypeStruct(a.shape, a.dtype)                         for a in flat_args),         out_shapes=tuple(flat_out_shapes), debug=debug,         interpret=interpret,         grid_mapping=grid_mapping,         input_output_aliases=tuple(input_output_aliases.items()),         **compiler_params) E   jax._src.source_info_util.JaxStackTraceBeforeTransformation: jax._src.pallas.triton.lowering.TritonLoweringException: Exception while lowering eqn: E     a:i32[] = pjit[ E     jaxpr={ lambda ; b:i32[] c:i32[]. let E         d:i32[] = convert_element_type[new_dtype=int32 weak_type=False] c E         e:i32[] = div b d E         f:i32[] = sign b E         g:i32[] = sign d E         h:bool[] = ne f g E         i:i32[] = rem b d E         j:bool[] = ne i 0 E         k:bool[] = and h j E         l:i32[] = sub e 1 E         m:i32[] = pjit[ E           jaxpr={ lambda ; n:bool[] o:i32[] p:i32[]. let E               q:i32[] = select_n n p o E             in (q,) } E           name=_where E         ] k l e E       in (m,) } E     name=floor_divide E   ] r 64 E   With context: ```   What jax/jaxlib version are you using? master  Which accelerator(s) are you using? GPU CC:  ",2023-08-27T15:34:49Z,bug NVIDIA GPU pallas,closed,0,1,https://github.com/jax-ml/jax/issues/17317,You can get around this for now by using lax.div. The // operator expands into more than just division.
2092,"以下是一个github上的jax下的一个issue, 标题是([export] Implement the calling convention for exporting with multi-platform lowering)， 内容是 (This is a first step towards supporting multiplatform exported JAX modules. Such modules are usable on more than one platform, and take an additional first argument that encodes the actual compilation platform as an index into the sequence of platforms for which the module was lowered. More details about the calling convention are in the docstring for jax_export.Exported in this PR. The value of the platform index is set by `jax_export.call_exported` when calling from JAX, and in the tf.XlaCallModule prior to compilation, when called from TensorFlow. This is already implemented in tf.XlaCallModule. This PR has some incomplete pieces:   * Currently we actually lower only for the first platform specified, and the platform argument is not used. There are a couple of implementation strategies for actual multiplatform lowering, both using the same calling convention. We could lower separately for each platform and put the results together with one toplevel conditional. Alternatively, we can take advantage of the fact that few primitives have perplatform lowering; we could lower those using a conditional.   * we implement multiplatform lowering only for jax_export, not for regular JAX jit or AOT lowering. This ensure that this change is narrowly scoped and safe for most JAX usage.   * we abuse the `_experimental_lowering_platform` kwarg to `lower()` to pass a tuple of platforms when we want multiplatform lowering. We ought to rename it to `_experimental_lowering_platforms`, but that requires more plumbing.   * we take advantage of the fact that the lowering for the platform index is identical to that for dimension variables: add a new argument to inner functions and pass the values to callees. We implement platform index as a dimension variable.   * we do not yet have the connection with jax2tf.convert.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[export] Implement the calling convention for exporting with multi-platform lowering,"This is a first step towards supporting multiplatform exported JAX modules. Such modules are usable on more than one platform, and take an additional first argument that encodes the actual compilation platform as an index into the sequence of platforms for which the module was lowered. More details about the calling convention are in the docstring for jax_export.Exported in this PR. The value of the platform index is set by `jax_export.call_exported` when calling from JAX, and in the tf.XlaCallModule prior to compilation, when called from TensorFlow. This is already implemented in tf.XlaCallModule. This PR has some incomplete pieces:   * Currently we actually lower only for the first platform specified, and the platform argument is not used. There are a couple of implementation strategies for actual multiplatform lowering, both using the same calling convention. We could lower separately for each platform and put the results together with one toplevel conditional. Alternatively, we can take advantage of the fact that few primitives have perplatform lowering; we could lower those using a conditional.   * we implement multiplatform lowering only for jax_export, not for regular JAX jit or AOT lowering. This ensure that this change is narrowly scoped and safe for most JAX usage.   * we abuse the `_experimental_lowering_platform` kwarg to `lower()` to pass a tuple of platforms when we want multiplatform lowering. We ought to rename it to `_experimental_lowering_platforms`, but that requires more plumbing.   * we take advantage of the fact that the lowering for the platform index is identical to that for dimension variables: add a new argument to inner functions and pass the values to callees. We implement platform index as a dimension variable.   * we do not yet have the connection with jax2tf.convert.",2023-08-27T14:26:54Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/17316,  PTAL,"> One highlevel question (not a blocker): what are the advantages of exporting one StableHLO module with multiplatform functions like in this PR, compared to simply exporting one StableHLO module per platform? >  > I'm wondering if we will ever be in a situation where having just one StableHLO module for all platforms could be limiting, e.g., different modulelevel params depending on platforms, having to do constant folding to get rid of irrelevant branches (especially if some custom calls are not known by other platforms), etc. If so, dispatching different StableHLO modules at the `XlaCallModule` level might be simpler/modular. The main benefit I can see is that JAX has few primitives with plaformspecific lowerings and I expect that a multiplatform lowering using conditionals for those primitives will be much smaller than multiple lowerings. The lowering time should similarly be smaller. Philosophically, such a lowering seems to fit better with JAX, that is mostly platformindependent and picks different lowerings only exceptionally to workaround differences in the XLA compilers. It is likely that there differences will get smaller. However, I am not 100% certain that this scheme will work in all cases, it is somewhat of an experiment. In terms of the costs of this scheme, there is a bit of extra complexity, but not much. I have already tested that custom calls are not an issue, because the HLO lowering already eliminates the redundant branches, and the resulting HLO that gets to the compiler is the same as if we lowering just for one platform. The alternative, which I think you have in mind, is to add multiple serialized modules to the XlaCallModule op?   ","Thanks for the clarification. > The main benefit I can see is that JAX has few primitives with plaformspecific lowerings and I expect that a multiplatform lowering using conditionals for those primitives will be much smaller than multiple lowerings. The lowering time should similarly be smaller. Philosophically, such a lowering seems to fit better with JAX, that is mostly platformindependent and picks different lowerings only exceptionally to workaround differences in the XLA compilers. It is likely that there differences will get smaller. This makes sense and it does sound useful. I'm curious how this will play out with our another goal of not transforming StableHLO as much as possible for debuggability (e.g., how can we constantfold only the platform index switch and nothing else), but we can continue experimenting with this idea. > The alternative, which I think you have in mind, is to add multiple serialized modules to the XlaCallModule op?    Yeah, that's what I had in mind. But it seems reasonable to start from this idea and see if it works well, especially given that it looks like we can switch between two approaches without having to change the userfacing interface much (if any).","Good discussion.  There is another user case maybe worth to mention here. In PAX/SAX, I saw some jax function has backendconditional logic.  For example ```python if jax.default_backend() == 'tpu':    do something else:   do something else ``` I wonder how to do multiplatform lowering here now.   .","> Good discussion. There is another user case maybe worth to mention here. In PAX/SAX, I saw some jax function has backendconditional logic. For example >  > ```python > if jax.default_backend() == 'tpu': >    do something > else: >   do something else > ``` >  > I wonder how to do multiplatform lowering here now. . That code cannot be handled by multiplatform lowering, or crossplatform lowering. It is in fact very suspicious because it uses global device state to decide what code to use, but JAX will make platform decisions based on what arguments are passed to the function being traced. We should look into why they are doing this and what are cleaner ways of achieving that. But this should be a separate issue.",>  I paste one example here  third_party/py/praxis/py_utils.py;l=457466
1084,"以下是一个github上的jax下的一个issue, 标题是(fix(pallas): flash attention tiled iteration upper bound too small when `block_q > block_k`)， 内容是 (https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL80  For reference: https://github.com/openai/triton/blob/5f448b2f085f554aaf291db07d18f7b16942d21f/python/triton/ops/flash_attention.pyL74  When `block_q` > `block_k`, the bound is too small. Consider `start_q = 1, block_q = 3, block_k = 1`. As implemented, this would yield 4 iterations. However, what is actually required is `cdiv((block_q * (start_q + 1)), block_k) == 6`  Originally identified: https://github.com/google/jax/issues/17267  Added regression test that fails on master: ` tests/pallas/pallas_test.py::FusedAttentionTest::test_fused_attention_fwd_batch_size=1_seq_len=384_num_heads=8_head_dim=64_causal=True_use_fwd=True_kwargs={'block_q': 128, 'block_k': 64} FAILED [ 55%] ` CC:  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fix(pallas): flash attention tiled iteration upper bound too small when `block_q > block_k`,"https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL80  For reference: https://github.com/openai/triton/blob/5f448b2f085f554aaf291db07d18f7b16942d21f/python/triton/ops/flash_attention.pyL74  When `block_q` > `block_k`, the bound is too small. Consider `start_q = 1, block_q = 3, block_k = 1`. As implemented, this would yield 4 iterations. However, what is actually required is `cdiv((block_q * (start_q + 1)), block_k) == 6`  Originally identified: https://github.com/google/jax/issues/17267  Added regression test that fails on master: ` tests/pallas/pallas_test.py::FusedAttentionTest::test_fused_attention_fwd_batch_size=1_seq_len=384_num_heads=8_head_dim=64_causal=True_use_fwd=True_kwargs={'block_q': 128, 'block_k': 64} FAILED [ 55%] ` CC:  ",2023-08-27T10:43:50Z,pull ready,closed,0,4,https://github.com/jax-ml/jax/issues/17314,"FYI, we just updated the kernel to handle segment_ids, you will probably need to rebase on top of those changes. Could you also squash your commits?",Thanks for the updates! One last thing: could you update your commit message to reflect the change? It currently looks like this: ``` fix Add segment_ids support to pallas flash attention on GPU. PiperOriginRevId: 561130379 ```,Hello  could this be merged now?,Yes I'll try to get it merged today
824,"以下是一个github上的jax下的一个issue, 标题是(Subsampling sparse matrices during (or after) multiplication)， 内容是 (Hi, I'm wondering if it is possible to efficiently implement a function analogous to `bcoo_dot_general_sampled` with sparse operands. I noticed that the function is actually expecting dense arrays, and also slicing a sparse array appears to return a dense array? My use case is simply trying to do online accumulation of gradients for a sparse recurrent network, with the sparsity constraint enforced on the gradients. Casting to and from a dense array in between would be way too slow and memoryintensive. Are there any workarounds I missed? How difficult would this be to implement?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Subsampling sparse matrices during (or after) multiplication,"Hi, I'm wondering if it is possible to efficiently implement a function analogous to `bcoo_dot_general_sampled` with sparse operands. I noticed that the function is actually expecting dense arrays, and also slicing a sparse array appears to return a dense array? My use case is simply trying to do online accumulation of gradients for a sparse recurrent network, with the sparsity constraint enforced on the gradients. Casting to and from a dense array in between would be way too slow and memoryintensive. Are there any workarounds I missed? How difficult would this be to implement?",2023-08-26T19:54:13Z,enhancement,open,0,3,https://github.com/jax-ml/jax/issues/17312,"No, I don't know of any implementation similar to `bcoo_dot_general_sampled` with sparse operands. >  I noticed that the function is actually expecting dense arrays Yes, this is the intent of `bcoo_dot_general_sampled`. It essentially implements the generalized SDDMM operation, which is important because it appears in the transpose of a sparsedense dot product. > also slicing a sparse array appears to return a dense array? I don't think this is true in general; for example: ```python In [1]: from jax.experimental import sparse In [2]: import jax.numpy as jnp In [3]: sparse.BCOO.fromdense(jnp.arange(10))[:5] Out[3]: BCOO(int32[5], nse=5) ``` > Are there any workarounds I missed? Not that I know of – with some thought, you could probably implement this logic from scratch for the particular matrix layouts you're interested in.","Thanks for your fast response as usual! > I don't think this is true in general; You are right. Taking that into account, I think the following general idea works: ``` >>> matrix = sparse.BCOO.fromdense(jnp.eye(10)) >>> matrix BCOO(float32[10, 10], nse=10) >>> sampled_product = sparse.BCOO(((matrix)[matrix.indices[:, 0], matrix.indices[:, 1]].data[:, 0], matrix.indices), shape=matrix.shape) >>> sampled_product BCOO(float32[10, 10], nse=10) >>>  ``` As long as the operands have the same indices and shape, this should work. Although it runs a few unnecessary multiplies in the general case, I think after JIT this will be pretty efficient?","I don't totally understand what you have in mind, but it looks like you're trying to remove the batch dimension in the indexing results. If that's the case, you could use `result.update_layout(n_batch=0)` rather than manually creating the new BCOO matrix from the data and index buffers."
1112,"以下是一个github上的jax下的一个issue, 标题是(Error importing jax)， 内容是 ( Description I have created a fresh environemnt `conda create name myenv`. Then I activate the environment and the very first package I am installing is `jax` with the command: `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` That looks to install everything in this folder under my home directory: `$HOME/.local/lib/python3.10/sitepackages` However when I install the other packages that I need to work with, these go into the typical location for conda enviroments: `$HOME/miniconda3/envs/myenv/lib/python3.8/sitepackages` Hence, when I try to import jax from within my new environment, I got the `ModuleNotFoundError: No module named 'jax'` error. What am I doing wrong please?  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Error importing jax," Description I have created a fresh environemnt `conda create name myenv`. Then I activate the environment and the very first package I am installing is `jax` with the command: `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` That looks to install everything in this folder under my home directory: `$HOME/.local/lib/python3.10/sitepackages` However when I install the other packages that I need to work with, these go into the typical location for conda enviroments: `$HOME/miniconda3/envs/myenv/lib/python3.8/sitepackages` Hence, when I try to import jax from within my new environment, I got the `ModuleNotFoundError: No module named 'jax'` error. What am I doing wrong please?  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-08-26T11:42:20Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/17309,"When you install a package with `pip install`, it will be installed in the `python` environment associated with the `pip` executable you are using. From your description, it sounds like the `pip` executable you are using is not the one associated with your conda environment (you can check this by running `type pip`). Rather than digging into system paths to try to figure out how the `pip` command is being resolved, it's often easier to fix situations like this by specifying exactly which Python environment you want to run `pip` in; you can do this with ``` python m pip install ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` This will ensure that JAX is installed in the packages directory associated with the Python executable you are using.",Oh ok! yes you are right! I installed pip with `sudo apt install python3pip` and that messed thinks up with the conda pip. Do you know by the way how to unistall jax (and all its dependencies like nvidia cuda binaries) that sit under my  `$HOME/.local/lib/python3.10/sitepackages` I guess the whole python folder in this location`$HOME/.local/lib/python3.10/` needs removal to be honest.  Many thanks!,"This line should should get those JAX, JAXLIB, and the immediate NVIDIA dependencies. (link to list of packages) pip uninstall y jax jaxlib nvidiacublascu12 nvidiacudacupticu12 nvidiacudanvcccu12 nvidiacudaruntimecu12 nvidiacudnncu12 nvidiacufftcu12 nvidiacusolvercu12 nvidiacusparsecu12 These are other required packages, but may have already been installed: ml_dtypes numpy opt_einsum scipy  If your python is <3.10, then it should have also installed ""importlib_metadata"" I imagine some of these packages had their own dependencies, which may have fetched more from there... if you want it 100% clean, it may be easier to just delete your Python packages directory and reinstall your other packages.","Thanks so much! I am now left with these.  ``` scikit_learn.libs                                     sklearn joblib                                      snowballstemmer joblib1.3.2.distinfo                      snowballstemmer2.2.0.distinfo nvidia                                      third_party nvidia_cuda_nvrtc_cu1212.2.128.distinfo   threadpoolctl3.2.0.distinfo nvidia_nvjitlink_cu1212.2.128.distinfo    threadpoolctl.py __pycache__                                 wcwidth scikit_learn1.3.0.distinfo               wcwidth0.2.6.distinfo ``` As you said these must be dependencies fetched from the packages that have just been removed. The timestamp of all these dependencies that I list above matches the time when I was trying to install jax. They are the only contents under `~/.local/lib/python3.10/sitepackages` Just want to double check, would you guys think it will be ok to just manually delete inside `~/.local/lib/python3.10/sitepackages`. My miniconda is in a totally different location anyway.","I think of those packages only those ending with `cu12` might have been installed as `jax` dependencies. I'm not sure if it's safe for you to delete that directory: there's no way for us to tell without knowing your setup. Sounds like this issue is resolved, so closing..."
2428,"以下是一个github上的jax下的一个issue, 标题是(TinyMap: An XLA map with linear-time operations)， 内容是 (Hey JAX folks, I needed a map in XLA so I created this TinyMap class: ```python from flax import struct from jax import numpy as jnp  A map from any scalar type to any scalar type.  This map can be compiled via jax.jit to XLA, allowing dynamic access and  updates, unlike a Python dict.  This is compatible with jax.vmap.   Note, this map does lineartime compares to find keys, so it is only  appropriate for small maps. class TinyMap(struct.PyTreeNode):      Keys are shape (capacity,) of any type.      Keys must be nonnegative; we use 1 to indicate an empty slot.     keys: jnp.ndarray      Values are shape (capacity,) of any type.      TODO: Allow values to have shape (capacity, ...).     values: jnp.ndarray      We require the user to specify a fixed capacity to avoid ragged tensors      if we batch multiple TinyMaps together.          def create(         cls, capacity: int, keys: jnp.ndarray, values: jnp.ndarray     ) > ""TinyMap"":         assert keys.shape == values.shape, (keys.shape, values.shape)         assert keys.ndim == 1, keys.ndim         assert keys.shape[0]  jnp.ndarray:         assert keys.ndim == 1, keys.ndim         mask = self.keys[:, None] == keys[None, :]         inds = jnp.argmax(mask, axis=0)         return self.values[inds]      We assume keys are unique and already exist in the map.     def update_existing(         self, keys: jnp.ndarray, values: jnp.ndarray     ) > ""TinyMap"":         assert keys.shape == values.shape, (keys.shape, values.shape)         assert keys.ndim == 1, keys.ndim         mask = self.keys[:, None] == keys[None, :]         inds = jnp.argmax(mask, axis=0)         return TinyMap(self.keys, self.values.at[inds].set(values)) ``` You can use it like this: ```python capacity = 2 keys = jnp.array([1, 2]) values = jnp.array([3, 4]) keys_query = jnp.array([1, 2, 1]) map_ = TinyMap.create(capacity, keys, values) map_ = map_.update_existing(keys, values + 10)  Prints [13 14 13]. print(map_.get(keys_query)) ``` My questions: 1. Does this already exist somewhere? I tried to find something like this before I implemented it, but perhaps I used the wrong search terms. 1. If not, should I contribute it somewhere, and if so, where? Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,TinyMap: An XLA map with linear-time operations,"Hey JAX folks, I needed a map in XLA so I created this TinyMap class: ```python from flax import struct from jax import numpy as jnp  A map from any scalar type to any scalar type.  This map can be compiled via jax.jit to XLA, allowing dynamic access and  updates, unlike a Python dict.  This is compatible with jax.vmap.   Note, this map does lineartime compares to find keys, so it is only  appropriate for small maps. class TinyMap(struct.PyTreeNode):      Keys are shape (capacity,) of any type.      Keys must be nonnegative; we use 1 to indicate an empty slot.     keys: jnp.ndarray      Values are shape (capacity,) of any type.      TODO: Allow values to have shape (capacity, ...).     values: jnp.ndarray      We require the user to specify a fixed capacity to avoid ragged tensors      if we batch multiple TinyMaps together.          def create(         cls, capacity: int, keys: jnp.ndarray, values: jnp.ndarray     ) > ""TinyMap"":         assert keys.shape == values.shape, (keys.shape, values.shape)         assert keys.ndim == 1, keys.ndim         assert keys.shape[0]  jnp.ndarray:         assert keys.ndim == 1, keys.ndim         mask = self.keys[:, None] == keys[None, :]         inds = jnp.argmax(mask, axis=0)         return self.values[inds]      We assume keys are unique and already exist in the map.     def update_existing(         self, keys: jnp.ndarray, values: jnp.ndarray     ) > ""TinyMap"":         assert keys.shape == values.shape, (keys.shape, values.shape)         assert keys.ndim == 1, keys.ndim         mask = self.keys[:, None] == keys[None, :]         inds = jnp.argmax(mask, axis=0)         return TinyMap(self.keys, self.values.at[inds].set(values)) ``` You can use it like this: ```python capacity = 2 keys = jnp.array([1, 2]) values = jnp.array([3, 4]) keys_query = jnp.array([1, 2, 1]) map_ = TinyMap.create(capacity, keys, values) map_ = map_.update_existing(keys, values + 10)  Prints [13 14 13]. print(map_.get(keys_query)) ``` My questions: 1. Does this already exist somewhere? I tried to find something like this before I implemented it, but perhaps I used the wrong search terms. 1. If not, should I contribute it somewhere, and if so, where? Thanks!",2023-08-25T18:21:53Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/17298
1692,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__')， 内容是 ( Description 1  jax only consumes arrays with __dlpack__ and __dlpack_device__ denders (torch and numpy arrays). It can't consumes PyCapsule which makes it not compatible with frameworks which only produce PyCapsule (tensorflow and paddle). 2  Jax has __dlpack__ and __dlpack_device__ denders but still torch can't consume jax arrays? ```python import jax.numpy as jnp from jax._src.dlpack import to_dlpack jax_exp = jnp.array([1,2,3,4]) jax_cap = to_dlpack(jax_exp) consuming jax array import torch torch.from_dlpack(jax_exp) ``` Error: ```  AttributeError                            Traceback (most recent call last) Cell In[58], line 2       1 import torch > 2 torch.from_dlpack(jax_exp) File /opt/fw/torch/torch/utils/dlpack.py:100, in from_dlpack(ext_tensor)      49 """"""from_dlpack(ext_tensor) > Tensor      50       51 Converts a tensor from an external library into a ``torch.Tensor``.    (...)      97       98 """"""      99 if hasattr(ext_tensor, '__dlpack__'): > 100     device = ext_tensor.__dlpack_device__()     101      device is either CUDA or ROCm, we need to pass the current     102      stream     103     if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM): AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__'  What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.0, Ubuntu  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__'," Description 1  jax only consumes arrays with __dlpack__ and __dlpack_device__ denders (torch and numpy arrays). It can't consumes PyCapsule which makes it not compatible with frameworks which only produce PyCapsule (tensorflow and paddle). 2  Jax has __dlpack__ and __dlpack_device__ denders but still torch can't consume jax arrays? ```python import jax.numpy as jnp from jax._src.dlpack import to_dlpack jax_exp = jnp.array([1,2,3,4]) jax_cap = to_dlpack(jax_exp) consuming jax array import torch torch.from_dlpack(jax_exp) ``` Error: ```  AttributeError                            Traceback (most recent call last) Cell In[58], line 2       1 import torch > 2 torch.from_dlpack(jax_exp) File /opt/fw/torch/torch/utils/dlpack.py:100, in from_dlpack(ext_tensor)      49 """"""from_dlpack(ext_tensor) > Tensor      50       51 Converts a tensor from an external library into a ``torch.Tensor``.    (...)      97       98 """"""      99 if hasattr(ext_tensor, '__dlpack__'): > 100     device = ext_tensor.__dlpack_device__()     101      device is either CUDA or ROCm, we need to pass the current     102      stream     103     if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM): AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__'  What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.0, Ubuntu  NVIDIA GPU info _No response_",2023-08-25T04:08:34Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17285,"For 1., that's not true: you can pass capsules to `jax.dlpack.from_dlpack`. (Note: please do *not* import things from `jax._src` since that's private and we can and will break your code.) 2. Actually, support for this just landed at head. `__dlpack_device__` was only just implemented very recently. If you build JAX and jaxlib at head, it should work. Please try?","Hi , `__dlpack_device__` is working very fine at head. Thanks for assistance :) But for 1. dlpack module exists in jax but I think you forgot to import in `__init__.py`, that's why it's not available publicly and I've to use private `to_dlpack`. If that's the case, please let me know I'm very happy to make my first contribution to jax. Thanks again!","For 1. yes it appears not to be imported by default. But irrespective of that, you can import it: just `import jax.dlpack`. No need for private APIs. I'll comment on the PR also."
349,"以下是一个github上的jax下的一个issue, 标题是(Deprecate jax.numpy.trapz.)， 内容是 (Expose the current implementation of jax.numpy.trapz as jax.scipy.integrate.trapezoid instead. Fixes https://github.com/google/jax/issues/17244)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Deprecate jax.numpy.trapz.,Expose the current implementation of jax.numpy.trapz as jax.scipy.integrate.trapezoid instead. Fixes https://github.com/google/jax/issues/17244,2023-08-24T20:04:51Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17277
3677,"以下是一个github上的jax下的一个issue, 标题是(feat(pallas): support MQA/GQA and FlashAttention-2)， 内容是 (Please:  [X] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Relevant files:  https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/attention.py Pallas provides a way of writing jaxpr/JAX DSL that get converted to tiled kernels. The original flash attention implementation exists in the above linked file. But it is only for MQA and does not have the optimizations in flash attention 2. (https://princetonnlp.github.io/flashatttention2/).  In particular, flash attention 2: 1. uses a work partitioning that was found to perform better (in A100/H100)     actually seems to have already been implemented        https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL92       https://github.com/google/jax/blob/841baabd3ffa63b27fa1c4199adc65fbf073c7c0/jax/experimental/pallas/ops/attention.pyL46     ~~or perhaps Triton does not expose how work is partitioned across warps...? Is making this decision part of Triton's job as a compiler? (It seems that it might be, see for instance: scheduling for GPUs, https://github.com/openai/triton/blob/main/docs/programmingguide/chapter2/relatedwork.rst)~~       What Triton does is more about allocating and managing lifetime of SRAM (e.g. cacheing, also other SRAM related like avoiding bank conflicts), orchestrating transfers, grid search over tile sizes, memory coalescing (for DRAM access). It also has awareness for things like 2d tiles, broadcast and reduce, and can specialize to devicespecific intrinsics (e.g. tensor core scheduling, warp reduction, TMA) 2. It has a new online softmax intended to reduce nonmatmul arithmetic intensity Aditionally (I may not implement these): 1. ~~Parallelize over sequence length (helpful for long sequences)~~     Parallelization of Q seq_len dimension is already done. In addition, in the causal attention case, we truncate the perSM tiled iterations which are completely masked out.     https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL80       Btw this is erroneous.          For reference: https://github.com/openai/triton/blob/5f448b2f085f554aaf291db07d18f7b16942d21f/python/triton/ops/flash_attention.pyL74          When `block_q` > `block_k`, the bound is too small. Consider `start_q = 1, block_q = 3, block_k = 1`. This would yield 4 iterations. However, what is actually required is `cdiv((block_q * start_q + 1), block_k) == 6`  MQA/GQA: 1. Reference flashattn implementation here: https://github.com/DaoAILab/flashattention/commit/a157cc8c9bf4460da70243bc100a363b5daad9ba I would like to work on this as an interesting entry point into JAX, Triton, and flash attention. In addition to the implementation, I will attempt to include some benchmarks. Here is a (slightly outdated) example of benchmark for original flash attention in jaxtriton (now pallas)  the idea of how we will benchmark should be similar. It can simply live in the PR or perhaps as a small aside in the docs. I will only benchmark on a Laptop RTX 4070 GPU (Ada Lovelace). I believe this will be an interesting and valuable exploration into one of the means to improve the extensibility of JAX. Further Resources: 1. SHARK v.s. Triton on Flash Attention 2 2. xformers FHMA impl 4. Tridao's implementation 5. Triton's implementation of work partitioning improvement??)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,feat(pallas): support MQA/GQA and FlashAttention-2,"Please:  [X] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Relevant files:  https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/attention.py Pallas provides a way of writing jaxpr/JAX DSL that get converted to tiled kernels. The original flash attention implementation exists in the above linked file. But it is only for MQA and does not have the optimizations in flash attention 2. (https://princetonnlp.github.io/flashatttention2/).  In particular, flash attention 2: 1. uses a work partitioning that was found to perform better (in A100/H100)     actually seems to have already been implemented        https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL92       https://github.com/google/jax/blob/841baabd3ffa63b27fa1c4199adc65fbf073c7c0/jax/experimental/pallas/ops/attention.pyL46     ~~or perhaps Triton does not expose how work is partitioned across warps...? Is making this decision part of Triton's job as a compiler? (It seems that it might be, see for instance: scheduling for GPUs, https://github.com/openai/triton/blob/main/docs/programmingguide/chapter2/relatedwork.rst)~~       What Triton does is more about allocating and managing lifetime of SRAM (e.g. cacheing, also other SRAM related like avoiding bank conflicts), orchestrating transfers, grid search over tile sizes, memory coalescing (for DRAM access). It also has awareness for things like 2d tiles, broadcast and reduce, and can specialize to devicespecific intrinsics (e.g. tensor core scheduling, warp reduction, TMA) 2. It has a new online softmax intended to reduce nonmatmul arithmetic intensity Aditionally (I may not implement these): 1. ~~Parallelize over sequence length (helpful for long sequences)~~     Parallelization of Q seq_len dimension is already done. In addition, in the causal attention case, we truncate the perSM tiled iterations which are completely masked out.     https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL80       Btw this is erroneous.          For reference: https://github.com/openai/triton/blob/5f448b2f085f554aaf291db07d18f7b16942d21f/python/triton/ops/flash_attention.pyL74          When `block_q` > `block_k`, the bound is too small. Consider `start_q = 1, block_q = 3, block_k = 1`. This would yield 4 iterations. However, what is actually required is `cdiv((block_q * start_q + 1), block_k) == 6`  MQA/GQA: 1. Reference flashattn implementation here: https://github.com/DaoAILab/flashattention/commit/a157cc8c9bf4460da70243bc100a363b5daad9ba I would like to work on this as an interesting entry point into JAX, Triton, and flash attention. In addition to the implementation, I will attempt to include some benchmarks. Here is a (slightly outdated) example of benchmark for original flash attention in jaxtriton (now pallas)  the idea of how we will benchmark should be similar. It can simply live in the PR or perhaps as a small aside in the docs. I will only benchmark on a Laptop RTX 4070 GPU (Ada Lovelace). I believe this will be an interesting and valuable exploration into one of the means to improve the extensibility of JAX. Further Resources: 1. SHARK v.s. Triton on Flash Attention 2 2. xformers FHMA impl 4. Tridao's implementation 5. Triton's implementation of work partitioning improvement??",2023-08-24T06:34:39Z,enhancement,open,1,0,https://github.com/jax-ml/jax/issues/17267
1222,"以下是一个github上的jax下的一个issue, 标题是(JAX_DISABLE_JIT=1 doesn't disable (xmap's) jitting)， 内容是 ( Description I'm trying to run a multihost dataparallel job using JAX. Due to working with a nested mess of xmapscangradpmapscangrad (multihost outer optimization loop, multidevice inner optimization loop), I'm trying to disable jit altogether in an attempt to avoid some suspected performance gotchas. However, means of disabling jitting don't seem to affect xmap. In fact, I suspect it is disabling jit that (ironically) yields obscene compilation times of minutes to hours in my case (depending on configuration), because the subxmap logic gets unrolled into nonjitted versions, and xmap jits the entirety of that, maybe? This issue was already mentioned in a comment on a closed issue, but seemed not to have much visibility. A minimal repro can also be found there. Opening a dedicated issue for better visibility.  What jax/jaxlib version are you using? jax==0.4.14 jaxlib==0.4.14  Which accelerator(s) are you using? CPU, TPU  Additional system info 3.11  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JAX_DISABLE_JIT=1 doesn't disable (xmap's) jitting," Description I'm trying to run a multihost dataparallel job using JAX. Due to working with a nested mess of xmapscangradpmapscangrad (multihost outer optimization loop, multidevice inner optimization loop), I'm trying to disable jit altogether in an attempt to avoid some suspected performance gotchas. However, means of disabling jitting don't seem to affect xmap. In fact, I suspect it is disabling jit that (ironically) yields obscene compilation times of minutes to hours in my case (depending on configuration), because the subxmap logic gets unrolled into nonjitted versions, and xmap jits the entirety of that, maybe? This issue was already mentioned in a comment on a closed issue, but seemed not to have much visibility. A minimal repro can also be found there. Opening a dedicated issue for better visibility.  What jax/jaxlib version are you using? jax==0.4.14 jaxlib==0.4.14  Which accelerator(s) are you using? CPU, TPU  Additional system info 3.11  NVIDIA GPU info _No response_",2023-08-24T06:28:38Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/17266,Can I suggest using shard_map instead?  https://github.com/google/jax/blob/0ddbe76cba371c5290bfb7a317f83c89c2c2641d/docs/jep/14273shardmap.md,"Thanks for the suggestion, , will definitely take a look!",Is there a way to disable jit for xmapped models (say for existing codebases)?,"xmap was removed from JAX, so this is obsolete"
913,"以下是一个github上的jax下的一个issue, 标题是(Indexing error when looping over zero length arrays)， 内容是 ( Description MWE:  ```python modes_old = jnp.empty((0,3)) modes_new = jnp.empty((4,3)) c_old = jnp.empty(0) c_new = jnp.empty(4) def body(i, c_new):     mask = (modes_old[i,:] == modes_new).all(axis=1)     c_new = jnp.where(mask, c_old[i], c_new)     return c_new fori_loop(0, modes_old.shape, body, c_new) ``` Gives an index error: ``` IndexError: index is out of bounds for axis 0 with size 0 ``` Given that `modes_old.shape[0] == 0`, this shouldn't even be entering the body, but it seems like it is?   What jax/jaxlib version are you using? jax==0.4.11, jaxlib==0.4.11  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Indexing error when looping over zero length arrays," Description MWE:  ```python modes_old = jnp.empty((0,3)) modes_new = jnp.empty((4,3)) c_old = jnp.empty(0) c_new = jnp.empty(4) def body(i, c_new):     mask = (modes_old[i,:] == modes_new).all(axis=1)     c_new = jnp.where(mask, c_old[i], c_new)     return c_new fori_loop(0, modes_old.shape, body, c_new) ``` Gives an index error: ``` IndexError: index is out of bounds for axis 0 with size 0 ``` Given that `modes_old.shape[0] == 0`, this shouldn't even be entering the body, but it seems like it is?   What jax/jaxlib version are you using? jax==0.4.11, jaxlib==0.4.11  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-08-24T02:56:14Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17264,"Thanks for the report! I believe this is expected behavior: JAX will always trace the body of the `fori_loop`, even in cases where the loop is executed zero times. See the similar discussion for `while_loop` in CC(lax.while_loop fails with zero iterations). The suggested fix would be to not call `fori_loop` in this kind of problematic case – there are examples in JAX's codebase where behavior similar to what you're seeing is anticipated and accounted for; for example: https://github.com/google/jax/blob/b4f7928a8154435c92f6ce9d7f904c9f99c7132b/jax/_src/lax/linalg.pyL1133L1137","Ahh I see, thanks! "
253,"以下是一个github上的jax下的一个issue, 标题是(Deprecate jax.numpy.in1d.)， 内容是 (Issue https://github.com/google/jax/issues/17244)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Deprecate jax.numpy.in1d.,Issue https://github.com/google/jax/issues/17244,2023-08-23T23:30:38Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17261
643,"以下是一个github上的jax下的一个issue, 标题是(Add `TransferToMemoryKind` as a private API to allow device_put to transfer to different memories without specifying the sharding and allowing the SPMD partitioner to choose the sharding for the intermediate.)， 内容是 (Add `TransferToMemoryKind` as a private API to allow device_put to transfer to different memories without specifying the sharding and allowing the SPMD partitioner to choose the sharding for the intermediate. Exposing it as a public API can be done later.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add `TransferToMemoryKind` as a private API to allow device_put to transfer to different memories without specifying the sharding and allowing the SPMD partitioner to choose the sharding for the intermediate.,Add `TransferToMemoryKind` as a private API to allow device_put to transfer to different memories without specifying the sharding and allowing the SPMD partitioner to choose the sharding for the intermediate. Exposing it as a public API can be done later.,2023-08-23T04:11:13Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17241
1094,"以下是一个github上的jax下的一个issue, 标题是(memory leak when calling odeint)， 内容是 ( Description I managed to make the following MWE ```python import jax.numpy as jnp from jax.experimental.ode import odeint while True:     vector_field = lambda y, t: y     sol = odeint(vector_field, jnp.asarray(1.0), jnp.asarray([0., 1., 2., 3.])) ``` and you can see the memory usage keep growing, while scipy.odeint has no such issue ```python from scipy.integrate import odeint while True:     vector_field = lambda y, t: y     sol = odeint(vector_field, 1, [0, 1, 2, 3]) ``` thus I suspect there are memory leak somewhere. This happens to diffrax as well, and it seems to be an issue with jax itself. See also kidger's comments in https://github.com/patrickkidger/diffrax/issues/142   What jax/jaxlib version are you using? jax/jaxlib 0.4.14  Which accelerator(s) are you using? CUDA  Additional system info _No response_  NVIDIA GPU info this behaviour can be reproduced on CPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,memory leak when calling odeint," Description I managed to make the following MWE ```python import jax.numpy as jnp from jax.experimental.ode import odeint while True:     vector_field = lambda y, t: y     sol = odeint(vector_field, jnp.asarray(1.0), jnp.asarray([0., 1., 2., 3.])) ``` and you can see the memory usage keep growing, while scipy.odeint has no such issue ```python from scipy.integrate import odeint while True:     vector_field = lambda y, t: y     sol = odeint(vector_field, 1, [0, 1, 2, 3]) ``` thus I suspect there are memory leak somewhere. This happens to diffrax as well, and it seems to be an issue with jax itself. See also kidger's comments in https://github.com/patrickkidger/diffrax/issues/142   What jax/jaxlib version are you using? jax/jaxlib 0.4.14  Which accelerator(s) are you using? CUDA  Additional system info _No response_  NVIDIA GPU info this behaviour can be reproduced on CPU",2023-08-22T20:03:00Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17232,"I believe this is due to CC(`jax.jit(..., static_argnums=...)` keeps strong (not weak) reference in cache, causing memory leak). I think everyone agrees something should probably be done about it. ? FWIW this issue doesn't usually turn up very often in reallife programs. It's fairly rare to dynamically define functions in a loop outside of JIT. E.g. I don't think I've ever needed this. (Side note  Roger, it's great to see your interest in JAX!)","> FWIW this issue doesn't usually turn up very often in reallife programs. It's fairly rare to dynamically define functions in a loop outside of JIT. E.g. I don't think I've ever needed this. Indeed, I realize in my case I should just use the parameters instead of creating a new equation object. I think I'm gonna close this issue then given this is reported already. > (Side note  Roger, it's great to see your interest in JAX!) I've been writing JAX since Jan actually ;)"
1116,"以下是一个github上的jax下的一个issue, 标题是(""cudaErrorMemoryAllocation : out of memory"" when more than one program involving JAX and cuDNN)， 内容是 ( Description Hi, The thing is, when i first create a code involving JAX and cuDNN, everything works just as fine. But if i start running a completely same code, the second will instantly crash and report the following information: ```E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:413] There was an error before creating cudnn handle (2): cudaErrorMemoryAllocation : out of memory``` Note that the code involve JAX.JIT and i did put these two code into seperate GPUs.  May i know what happened? thanks~  What jax/jaxlib version are you using? I install via the following command: pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? GPU  Additional system info Linux Ubuntu 22.04  NVIDIA GPU info ``` Tue Aug 22 15:45:38 2023        ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"""cudaErrorMemoryAllocation : out of memory"" when more than one program involving JAX and cuDNN"," Description Hi, The thing is, when i first create a code involving JAX and cuDNN, everything works just as fine. But if i start running a completely same code, the second will instantly crash and report the following information: ```E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:413] There was an error before creating cudnn handle (2): cudaErrorMemoryAllocation : out of memory``` Note that the code involve JAX.JIT and i did put these two code into seperate GPUs.  May i know what happened? thanks~  What jax/jaxlib version are you using? I install via the following command: pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? GPU  Additional system info Linux Ubuntu 22.04  NVIDIA GPU info ``` Tue Aug 22 15:45:38 2023        ++  ++++ ```",2023-08-22T07:47:21Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17217,"are you using any special array sharding methods like pjit ,pmap or anything like that? The error message ""cudaErrorMemoryAllocation : out of memory""  can happen for a few reasons: 1. The code is trying to allocate too much memory. 2. The GPU is already running low on memory due to other processes. 3. There is a memory leak in the code. To troubleshoot this issue, you can try the following: 1. Check the code to make sure that it is not trying to allocate too much memory. 2. Close any other processes that are using the GPU. 3. Use the gdb debugger to find memory leaks in the code. You can also try setting the environment variable XLA_PYTHON_CLIENT_PREALLOCATE=false. This will disable the preallocation behavior, which can sometimes help to reduce memory usage. ```python import os os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false' os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'gpu' ``` and if you want to have a better look at the detail of the processing code and know exactly about the size of the buffer or executable you can use Jaxsmi or my code ```shell pip install fjutils ``` ```python from fjutils.tracker import initialise_tracking, threaded_log, get_mem if __name__ == ""__main__"":     initialise_tracking()     threaded_log().start()      or     print(get_mem()) ```","Closing as a duplicate of https://github.com/google/jax/issues/12461 The gist of it is as mentioned above: JAX preallocates memory, and this can lead to memory starvation which renders CUDA initialization impossible. See this comment from the linked issue, as well as this section of the FAQ."
1147,"以下是一个github上的jax下的一个issue, 标题是(bf16 * int8 matmul results in incorrect value)， 内容是 ( Description ```  Let us define a bf16 array and an int8 array: X=jnp.array([[1.6171875,0.5703125]],dtype=jax.numpy.bfloat16) W=jnp.array([[127],[4]],dtype=jax.numpy.int8)  perform matrix multiplication: jax.numpy.matmul(X,W,precision=jax.lax.Precision.HIGHEST) DeviceArray([[208]], dtype=bfloat16)  However, if we manually do the multiplication: X[0,0]*W[0,0] DeviceArray(205, dtype=bfloat16) X[0,1]*W[1,0] DeviceArray(2.28125, dtype=bfloat16) X[0,0]*W[0,0]+X[0,1]*W[1,0] DeviceArray(207, dtype=bfloat16)  That is 207 which is different to 208 from the matmul function.  ``` I have been trying to find a DL framework that does bf16 and int8 matrix multiplication, so far only Jax supports it, but it seems to have this rounding issue at the moment.  What jax/jaxlib version are you using? 0.3.20+cuda11.cudnn82  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info A100)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,bf16 * int8 matmul results in incorrect value," Description ```  Let us define a bf16 array and an int8 array: X=jnp.array([[1.6171875,0.5703125]],dtype=jax.numpy.bfloat16) W=jnp.array([[127],[4]],dtype=jax.numpy.int8)  perform matrix multiplication: jax.numpy.matmul(X,W,precision=jax.lax.Precision.HIGHEST) DeviceArray([[208]], dtype=bfloat16)  However, if we manually do the multiplication: X[0,0]*W[0,0] DeviceArray(205, dtype=bfloat16) X[0,1]*W[1,0] DeviceArray(2.28125, dtype=bfloat16) X[0,0]*W[0,0]+X[0,1]*W[1,0] DeviceArray(207, dtype=bfloat16)  That is 207 which is different to 208 from the matmul function.  ``` I have been trying to find a DL framework that does bf16 and int8 matrix multiplication, so far only Jax supports it, but it seems to have this rounding issue at the moment.  What jax/jaxlib version are you using? 0.3.20+cuda11.cudnn82  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info A100",2023-08-22T03:27:26Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/17214,"Thanks for the question! I believe this is working as expected: you're doing math at `bfloat16` precision, and `bfloat16` only has 7 bits of mantissa, meaning that you should generally expect numerical results to be good to within roughly one part in $2^7$. Doing this computation in `float32` reveals the ""true"" result: ```python X.astype('float32') @ W.astype('float32')  Array([[207.66406]], dtype=float32) ``` In `bfloat16`, you got `208`, which is actually the closest bfloat16representable value to the true answer. You can see this by using the `jnp.nextafter` function to see what the next representable value is: ```python print(jnp.nextafter(jnp.bfloat16(208), jnp.bfloat16(0)))  207 ``` The next bfloat16representable value greater than `208` is `207`, so it's clear that `208` is the best possible bfloat16 representation of the answer to your computation. The reason your manual matmul returns this incorrect value is because by splitting the ops you incur bfloat16 rounding errors twice instead of once. Hope that helps!","Great, thank you for the help!","I guess this implies that matmul internally converts the bf16/int8 arrays to fp32 for both multiplication and accumulation? ```  i.e. y=x1.float32()*W1.float32()+x2.float32()*W2.float32()+... print(X[0,0].astype(jnp.float32)*W[0,0].astype(jnp.float32)+X[0,1].astype(jnp.float32)*W[1,0].astype(jnp.float32)) 207.66406  in this case the closest bf16 number is 208 ``` but this means we cast everything to fp32 such that the acceleration from lowbit computation is lost. Thus, what I would have expected is: ```  i.e. y=(x1*W1).float32()+(x2*W2).float32()+... print((X[0,0]*W[0,0]).astype(jnp.float32)+(X[0,1]*W[1,0]).astype(jnp.float32)) 207.28125  in this case the closest bf16 number is 207 ``` I am unfamiliar with A100's internal instruction, but I would have thought the bf16/int8 matrix multiplication is performed in lowbit for mul and highbit for add, in order to reduce accumulation error whilst maintaining a performance edge.","The implementation of bfloat16 matmul is hardwarespecific, and I’m not sure of the details on A100."
405,"以下是一个github上的jax下的一个issue, 标题是(Improve type annotations for jax.numpy.)， 内容是 (Improve type annotations for jax.numpy. * Allow sequences of axes to jnp.flip, rather than mandating tuples. Users sometimes pass lists here. * Allow arraylike pad_width values to pad().)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve type annotations for jax.numpy.,"Improve type annotations for jax.numpy. * Allow sequences of axes to jnp.flip, rather than mandating tuples. Users sometimes pass lists here. * Allow arraylike pad_width values to pad().",2023-08-21T21:05:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17207
47353,"以下是一个github上的jax下的一个issue, 标题是(xmap + expm bug)， 内容是 ( Description Not sure if I'm misusing `xmap`, but the following code: ``` from jax.scipy.linalg import expm import jax.numpy as jnp from jax.experimental.maps import xmap xmap(expm, in_axes={0: 'a'}, out_axes={0: 'a'})(jnp.array([[[0., 1.], [1., 0.]]])) ``` produces the error: ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) File :198, in _run_module_as_main() File :88, in _run_code() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel_launcher.py:17      15 from ipykernel import kernelapp as app > 17 app.launch_new_instance() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/traitlets/config/application.py:1043, in launch_instance()    1042 app.initialize(argv) > 1043 app.start() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelapp.py:725, in start()     724 try: > 725     self.io_loop.start()     726 except KeyboardInterrupt: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/tornado/platform/asyncio.py:195, in start()     194 def start(self) > None: > 195     self.asyncio_loop.run_forever() File /opt/anaconda3/envs/devEnv311/lib/python3.11/asyncio/base_events.py:607, in run_forever()     606 while True: > 607     self._run_once()     608     if self._stopping: File /opt/anaconda3/envs/devEnv311/lib/python3.11/asyncio/base_events.py:1922, in _run_once()    1921     else: > 1922         handle._run()    1923 handle = None File /opt/anaconda3/envs/devEnv311/lib/python3.11/asyncio/events.py:80, in _run()      79 try: > 80     self._context.run(self._callback, *self._args)      81 except (SystemExit, KeyboardInterrupt): File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelbase.py:513, in dispatch_queue()     512 try: > 513     await self.process_one()     514 except Exception: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelbase.py:502, in process_one()     501         return None > 502 await dispatch(*args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelbase.py:409, in dispatch_shell()     408     if inspect.isawaitable(result): > 409         await result     410 except Exception: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelbase.py:729, in execute_request()     728 if inspect.isawaitable(reply_content): > 729     reply_content = await reply_content     731  Flush output before sending the reply. File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/ipkernel.py:422, in do_execute()     421 if with_cell_id: > 422     res = shell.run_cell(     423         code,     424         store_history=store_history,     425         silent=silent,     426         cell_id=cell_id,     427     )     428 else: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/zmqshell.py:540, in run_cell()     539 self._last_traceback = None > 540 return super().run_cell(*args, **kwargs) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3009, in run_cell()    3008 try: > 3009     result = self._run_cell(    3010         raw_cell, store_history, silent, shell_futures, cell_id    3011     )    3012 finally: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3064, in _run_cell()    3063 try: > 3064     result = runner(coro)    3065 except BaseException as e: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/async_helpers.py:129, in _pseudo_sync_runner()     128 try: > 129     coro.send(None)     130 except StopIteration as exc: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3269, in run_cell_async()    3266 interactivity = ""none"" if silent else self.ast_node_interactivity > 3269 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3270        interactivity=interactivity, compiler=compiler, result=result)    3272 self.last_execution_succeeded = not has_raised File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3448, in run_ast_nodes()    3447     asy = compare(code) > 3448 if await self.run_code(code, result, async_=asy):    3449     return True File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3508, in run_code()    3507     else: > 3508         exec(code_obj, self.user_global_ns, self.user_ns)    3509 finally:    3510      Reset our crash handler in place Cell In[2], line 1 > 1 xmap(expm, in_axes={0: 'a'}, out_axes={0: 'a'})(jnp.array([[[0., 1.], [1., 0.]]])) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/scipy/linalg.py:457, in expm()     455   return R > 457 R = lax.cond(n_squarings > max_squarings, _nan, _compute, (A, P, Q))     458 return R File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/scipy/linalg.py:453, in _compute()     452 A, P, Q = args > 453 R = _solve_P_Q(P, Q, upper_triangular)     454 R = _squaring(R, n_squarings, max_squarings) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/scipy/linalg.py:496, in _solve_P_Q()     495 else: > 496   return jnp.linalg.solve(Q, P) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/numpy/linalg.py:608, in solve()     607 a, b = promote_dtypes_inexact(jnp.asarray(a), jnp.asarray(b)) > 608 return lax_linalg._solve(a, b) JaxStackTraceBeforeTransformation: NameError: unbound axis name: a. The following axis names (e.g. defined by pmap) are available to collective operations: [, , , , ] The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: NameError                                 Traceback (most recent call last) Cell In[2], line 1 > 1 xmap(expm, in_axes={0: 'a'}, out_axes={0: 'a'})(jnp.array([[[0., 1.], [1., 0.]]])) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:603, in xmap..fun_mapped(*args)     601 tree_map(dispatch.check_arg, args)     602 fun_flat, args_flat, params, _, out_tree = infer_params(*args) > 603 out_flat = xmap_p.bind(fun_flat, *args_flat, **params)     604 return verify_outputs(out_flat, out_tree, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:825, in XMapPrimitive.bind(self, fun, in_axes, *args, **params)     823 def bind(self, fun, *args, in_axes, **params):     824   assert len(in_axes) == len(args), (in_axes, args) > 825   return core.map_bind(self, fun, *args, in_axes=in_axes, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2312, in map_bind(primitive, fun, *args, **params)    2308 def map_bind(primitive: MapPrimitive, fun, *args, **params):    2309   map_bind_continuation, top_trace, fun, tracers, params = (    2310       map_bind_with_continuation(primitive, fun, *args, **params))    2311   return map_bind_continuation( > 2312       primitive.process(top_trace, fun, tracers, params)) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:828, in XMapPrimitive.process(self, trace, fun, tracers, params)     827 def process(self, trace, fun, tracers, params): > 828   return trace.process_xmap(self, fun, tracers, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:824, in EvalTrace.process_call(self, primitive, f, tracers, params)     823 def process_call(self, primitive, f, tracers, params): > 824   return primitive.impl(f, *tracers, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:632, in xmap_impl(fun, name, in_axes, out_axes_thunk, donated_invars, global_axis_sizes, axis_resources, resource_env, backend, spmd_in_axes, spmd_out_axes_thunk, *args)     628 def xmap_impl(fun: lu.WrappedFun, *args, name, in_axes, out_axes_thunk, donated_invars,     629               global_axis_sizes, axis_resources, resource_env, backend,     630               spmd_in_axes, spmd_out_axes_thunk):     631   in_avals = [core.raise_to_shaped(core.get_aval(arg)) for arg in args] > 632   xmap_callable = make_xmap_callable(     633       fun, name, in_axes, out_axes_thunk, donated_invars, global_axis_sizes,     634       axis_resources, resource_env, backend,     635       spmd_in_axes, spmd_out_axes_thunk,     636       None, *in_avals).compile().unsafe_call     637   distributed_debug_log((""Running xmapped function"", name),     638                         (""python function"", fun.f),     639                         (""mesh"", resource_env.physical_mesh),     640                         (""abstract args"", in_avals))     641   return xmap_callable(*args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:345, in cache..memoized_fun(fun, *args)     343   fun.populate_stores(stores)     344 else: > 345   ans = call(fun, *args)     346   cache[key] = (ans, fun.stores)     348 return ans File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:705, in make_xmap_callable(fun, name, in_axes, out_axes_thunk, donated_invars, global_axis_sizes, axis_resources, resource_env, backend, spmd_in_axes, spmd_out_axes_thunk, lowering_platform, *in_avals)     698   return pxla.lower_mesh_computation(     699       f, 'xmap', name, mesh,     700       in_shardings, out_shardings, donated_invars,     701       use_spmd_lowering, in_avals,     702       tiling_method=tiling_method,     703       lowering_platform=lowering_platform)     704 else: > 705   return dispatch.sharded_lowering(     706       f, name, donated_invars, True, False, in_avals, (None,) * len(in_avals),     707       lowering_platform=lowering_platform) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/dispatch.py:241, in sharded_lowering(fun, name, donated_invars, keep_unused, inline, in_avals, in_shardings, lowering_platform)     236 in_shardings = [UNSPECIFIED if i is None else i for i in in_shardings]   type: ignore     238  Pass in a singleton `UNSPECIFIED` for out_shardings because we don't know     239  the number of output avals at this stage. lower_sharding_computation will     240  apply it to all out_avals. > 241 return pxla.lower_sharding_computation(     242     fun, 'jit', name, in_shardings, UNSPECIFIED, donated_invars,     243     tuple(in_avals), keep_unused=keep_unused, inline=inline, always_lower=False,     244     devices_from_context=None, lowering_platform=lowering_platform) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py:1899, in lower_sharding_computation(fun_or_jaxpr, api_name, fun_name, in_shardings, out_shardings, donated_invars, global_in_avals, keep_unused, inline, always_lower, devices_from_context, lowering_platform, override_lowering_rules)    1893  1. Trace to jaxpr and preprocess/verify it    1894 auto_spmd_lowering = (    1895     check_if_any_auto(in_shardings) if is_unspecified(out_shardings) else    1896     check_if_any_auto(it.chain.from_iterable([in_shardings, out_shardings])))   type: ignore    1898 (closed_jaxpr, global_in_avals, global_out_avals, donated_invars, > 1899  kept_var_idx, name_stack) = _trace_to_jaxpr_and_dce(    1900     fun_or_jaxpr, global_in_avals, api_name, fun_name, keep_unused,    1901     donated_invars, auto_spmd_lowering)    1902 jaxpr = closed_jaxpr.jaxpr    1903 in_shardings = tuple(s for i, s in enumerate(in_shardings) if i in kept_var_idx) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py:1692, in cache_wrap..wrapped(f, *args, **kwargs)    1690 def wrapped(f, *args, **kwargs):    1691   if isinstance(f, lu.WrappedFun): > 1692     return _wrapped_with_lu_cache(f, *args, **kwargs)    1693   else:    1694     return _wrapped_with_weakref_lru_cache(f, *args, **kwargs) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:345, in cache..memoized_fun(fun, *args)     343   fun.populate_stores(stores)     344 else: > 345   ans = call(fun, *args)     346   cache[key] = (ans, fun.stores)     348 return ans File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py:1707, in _trace_to_jaxpr_and_dce(fun_or_jaxpr, global_in_avals, api_name, fun_name, keep_unused, donated_invars, auto_spmd_lowering)    1703 if isinstance(fun_or_jaxpr, lu.WrappedFun):    1704   with dispatch.log_elapsed_time(    1705       ""Finished tracing + transforming {fun_name} in {elapsed_time} sec"",    1706       fun_name=str(name_stack), event=dispatch.JAXPR_TRACE_EVENT): > 1707     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_final(    1708         fun_or_jaxpr, global_in_avals)    1709 else:    1710   assert isinstance(fun_or_jaxpr, core.ClosedJaxpr) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2233, in trace_to_jaxpr_final(fun, in_avals, debug_info, keep_inputs)    2231   main.jaxpr_stack = ()   type: ignore    2232   with core.new_sublevel(): > 2233     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2234       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2235   del fun, main    2236 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2596, in AxisPrimitive.bind(self, *args, **params)    2592 axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                 default=None, key=lambda t: getattr(t, 'level', 1))    2594 top_trace = (top_trace if not axis_main or axis_main.level  2596 return self.bind_with_trace(top_trace, args, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:389, in Primitive.bind_with_trace(self, trace, args, params)     388 def bind_with_trace(self, trace, args, params): > 389   out = trace.process_primitive(self, map(trace.full_raise, args), params)     390   return map(full_lower, out) if self.multiple_results else full_lower(out) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:425, in BatchTrace.process_primitive(self, primitive, tracers, params)     423   frame = self.get_frame(vals_in, dims_in)     424   batcher_primitive = self.get_axis_primitive_batcher(primitive, frame) > 425   val_out, dim_out = batcher_primitive(vals_in, dims_in, **params)     426 elif all(bdim is not_mapped for bdim in dims_in):     427   return primitive.bind(*vals_in, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py:1409, in _pjit_batcher(insert_axis, spmd_axis_name, axis_size, axis_name, main_type, vals_in, dims_in, jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline)    1403 def _pjit_batcher(insert_axis, spmd_axis_name,    1404                   axis_size, axis_name, main_type,    1405                   vals_in, dims_in,    1406                   jaxpr, in_shardings, out_shardings,    1407                   resource_env, donated_invars, name, keep_unused, inline):    1408   segment_lens, dims_in = batching.indirectify_ragged_axes(dims_in) > 1409   new_jaxpr, axes_out = batching.batch_jaxpr2(    1410       jaxpr, axis_size, dims_in, axis_name=axis_name,    1411       spmd_axis_name=spmd_axis_name, main_type=main_type)    1413    `insert_axis` is set to True only for some `xmap` uses.    1414   new_parts = (axis_name,) if insert_axis else (    1415       () if spmd_axis_name is None else spmd_axis_name) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:767, in batch_jaxpr2(closed_jaxpr, axis_size, in_axes, axis_name, spmd_axis_name, main_type)     754 def batch_jaxpr2(     755     closed_jaxpr: core.ClosedJaxpr,     756     axis_size: core.AxisSize,    (...)     765    consistency constraints, such as typeagreement across arms of a     766    `lax.cond`, or inputoutput agreement for the body of a `lax.scan`. > 767   return _batch_jaxpr2(closed_jaxpr, axis_size, tuple(in_axes), axis_name,     768                        spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:790, in _batch_jaxpr2(closed_jaxpr, axis_size, in_axes, axis_name, spmd_axis_name, main_type)     783 in_axes2, avals_in = unzip2([     784     handle_ragged(closed_jaxpr.in_avals, dim, aval)     785     if isinstance(dim, RaggedAxis) else (dim, aval)     786     for dim, aval in zip(in_axes, closed_jaxpr.in_avals)])     787 avals_in2 = [core.unmapped_aval(axis_size, axis_name, b, aval)     788              if b is not not_mapped else aval     789              for aval, b in unsafe_zip(avals_in, in_axes2)] > 790 jaxpr_out, _, consts = pe.trace_to_jaxpr_dynamic(f, avals_in2)     791 return core.ClosedJaxpr(jaxpr_out, consts), out_axes() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2155, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2153 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2154   main.jaxpr_stack = ()   type: ignore > 2155   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2156     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2157   del main, fun    2158 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/lax/control_flow/conditionals.py:813, in cond_bind(branches, linear, *args)     811   for jaxpr in branches:     812     core.check_jaxpr(jaxpr.jaxpr) > 813 return core.AxisPrimitive.bind(cond_p, *args, branches=branches, linear=linear) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2596, in AxisPrimitive.bind(self, *args, **params)    2592 axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                 default=None, key=lambda t: getattr(t, 'level', 1))    2594 top_trace = (top_trace if not axis_main or axis_main.level  2596 return self.bind_with_trace(top_trace, args, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:389, in Primitive.bind_with_trace(self, trace, args, params)     388 def bind_with_trace(self, trace, args, params): > 389   out = trace.process_primitive(self, map(trace.full_raise, args), params)     390   return map(full_lower, out) if self.multiple_results else full_lower(out) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:425, in BatchTrace.process_primitive(self, primitive, tracers, params)     423   frame = self.get_frame(vals_in, dims_in)     424   batcher_primitive = self.get_axis_primitive_batcher(primitive, frame) > 425   val_out, dim_out = batcher_primitive(vals_in, dims_in, **params)     426 elif all(bdim is not_mapped for bdim in dims_in):     427   return primitive.bind(*vals_in, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/lax/control_flow/conditionals.py:374, in _cond_batching_rule(spmd_axis_name, axis_size, axis_name, main_type, args, dims, branches, linear)     371 in_batched  = [True] * len(branches[0].in_avals)     372 out_batched = [True] * len(branches[0].out_avals) > 374 branches_batched = [     375     batching.batch_jaxpr(     376         jaxpr, axis_size, in_batched, out_batched, axis_name, spmd_axis_name,     377         main_type)[0]     378     for jaxpr in branches]     380 branch_outs = []     381 for i, jaxpr in enumerate(branches_batched):     382    Perform a select on the inputs for safety of reversemode autodiff; see     383    https://github.com/google/jax/issues/1052 File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/lax/control_flow/conditionals.py:375, in (.0)     371 in_batched  = [True] * len(branches[0].in_avals)     372 out_batched = [True] * len(branches[0].out_avals)     374 branches_batched = [ > 375     batching.batch_jaxpr(     376         jaxpr, axis_size, in_batched, out_batched, axis_name, spmd_axis_name,     377         main_type)[0]     378     for jaxpr in branches]     380 branch_outs = []     381 for i, jaxpr in enumerate(branches_batched):     382    Perform a select on the inputs for safety of reversemode autodiff; see     383    https://github.com/google/jax/issues/1052 File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:804, in batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name, spmd_axis_name, main_type)     801 def batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name,     802                 spmd_axis_name, main_type):     803   inst = tuple(instantiate) if isinstance(instantiate, list) else instantiate > 804   return _batch_jaxpr(closed_jaxpr, axis_size, tuple(in_batched), inst,     805                       axis_name, spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:816, in _batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name, spmd_axis_name, main_type)     814 in_axes = [0 if b else not_mapped for b in in_batched]     815 out_axes_dest = [0 if inst else zero_if_mapped for inst in instantiate] > 816 return batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest,     817                         axis_name, spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:821, in batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name, spmd_axis_name, main_type)     819 def batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name,     820                      spmd_axis_name, main_type): > 821   return _batch_jaxpr_axes(closed_jaxpr, axis_size, tuple(in_axes),     822                            tuple(out_axes_dest), axis_name, spmd_axis_name,     823                            main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:835, in _batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name, spmd_axis_name, main_type)     831 f = _batch_jaxpr_outer(f, axis_name, spmd_axis_name, axis_size, in_axes,     832                        main_type)     833 avals_in = [core.unmapped_aval(axis_size, axis_name, b, aval) if b is not not_mapped     834             else aval for aval, b in unsafe_zip(closed_jaxpr.in_avals, in_axes)] > 835 jaxpr_out, _, consts = pe.trace_to_jaxpr_dynamic(f, avals_in)     836 return core.ClosedJaxpr(jaxpr_out, consts), out_batched() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2155, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2153 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2154   main.jaxpr_stack = ()   type: ignore > 2155   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2156     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2157   del main, fun    2158 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2596, in AxisPrimitive.bind(self, *args, **params)    2592 axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                 default=None, key=lambda t: getattr(t, 'level', 1))    2594 top_trace = (top_trace if not axis_main or axis_main.level  2596 return self.bind_with_trace(top_trace, args, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:389, in Primitive.bind_with_trace(self, trace, args, params)     388 def bind_with_trace(self, trace, args, params): > 389   out = trace.process_primitive(self, map(trace.full_raise, args), params)     390   return map(full_lower, out) if self.multiple_results else full_lower(out) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:425, in BatchTrace.process_primitive(self, primitive, tracers, params)     423   frame = self.get_frame(vals_in, dims_in)     424   batcher_primitive = self.get_axis_primitive_batcher(primitive, frame) > 425   val_out, dim_out = batcher_primitive(vals_in, dims_in, **params)     426 elif all(bdim is not_mapped for bdim in dims_in):     427   return primitive.bind(*vals_in, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py:1409, in _pjit_batcher(insert_axis, spmd_axis_name, axis_size, axis_name, main_type, vals_in, dims_in, jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline)    1403 def _pjit_batcher(insert_axis, spmd_axis_name,    1404                   axis_size, axis_name, main_type,    1405                   vals_in, dims_in,    1406                   jaxpr, in_shardings, out_shardings,    1407                   resource_env, donated_invars, name, keep_unused, inline):    1408   segment_lens, dims_in = batching.indirectify_ragged_axes(dims_in) > 1409   new_jaxpr, axes_out = batching.batch_jaxpr2(    1410       jaxpr, axis_size, dims_in, axis_name=axis_name,    1411       spmd_axis_name=spmd_axis_name, main_type=main_type)    1413    `insert_axis` is set to True only for some `xmap` uses.    1414   new_parts = (axis_name,) if insert_axis else (    1415       () if spmd_axis_name is None else spmd_axis_name) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:767, in batch_jaxpr2(closed_jaxpr, axis_size, in_axes, axis_name, spmd_axis_name, main_type)     754 def batch_jaxpr2(     755     closed_jaxpr: core.ClosedJaxpr,     756     axis_size: core.AxisSize,    (...)     765    consistency constraints, such as typeagreement across arms of a     766    `lax.cond`, or inputoutput agreement for the body of a `lax.scan`. > 767   return _batch_jaxpr2(closed_jaxpr, axis_size, tuple(in_axes), axis_name,     768                        spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:790, in _batch_jaxpr2(closed_jaxpr, axis_size, in_axes, axis_name, spmd_axis_name, main_type)     783 in_axes2, avals_in = unzip2([     784     handle_ragged(closed_jaxpr.in_avals, dim, aval)     785     if isinstance(dim, RaggedAxis) else (dim, aval)     786     for dim, aval in zip(in_axes, closed_jaxpr.in_avals)])     787 avals_in2 = [core.unmapped_aval(axis_size, axis_name, b, aval)     788              if b is not not_mapped else aval     789              for aval, b in unsafe_zip(avals_in, in_axes2)] > 790 jaxpr_out, _, consts = pe.trace_to_jaxpr_dynamic(f, avals_in2)     791 return core.ClosedJaxpr(jaxpr_out, consts), out_axes() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2155, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2153 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2154   main.jaxpr_stack = ()   type: ignore > 2155   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2156     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2157   del main, fun    2158 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2596, in AxisPrimitive.bind(self, *args, **params)    2592 axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                 default=None, key=lambda t: getattr(t, 'level', 1))    2594 top_trace = (top_trace if not axis_main or axis_main.level  2596 return self.bind_with_trace(top_trace, args, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:389, in Primitive.bind_with_trace(self, trace, args, params)     388 def bind_with_trace(self, trace, args, params): > 389   out = trace.process_primitive(self, map(trace.full_raise, args), params)     390   return map(full_lower, out) if self.multiple_results else full_lower(out) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:431, in BatchTrace.process_primitive(self, primitive, tracers, params)     429   frame = self.get_frame(vals_in, dims_in)     430   batched_primitive = self.get_primitive_batcher(primitive, frame) > 431   val_out, dim_out = batched_primitive(vals_in, dims_in, **params)     432 src = source_info_util.current()     433 if primitive.multiple_results: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/lax/control_flow/solves.py:399, in _linear_solve_batching_rule(spmd_axis_name, axis_size, axis_name, main_type, args, dims, const_lengths, jaxprs)     396 x_bat = [False] * len(solve.out_avals)     397 for i in range(1 + len(orig_b_bat) + len(solve.out_avals)):     398    Apply vecmat and solve > new batched parts of x > 399   solve_jaxpr_batched, solve_x_bat = batching.batch_jaxpr(     400       solve, axis_size, solve_bat + b_bat, instantiate=x_bat,     401       axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)     402   if vecmat is None:     403     vecmat_jaxpr_batched = None File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:804, in batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name, spmd_axis_name, main_type)     801 def batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name,     802                 spmd_axis_name, main_type):     803   inst = tuple(instantiate) if isinstance(instantiate, list) else instantiate > 804   return _batch_jaxpr(closed_jaxpr, axis_size, tuple(in_batched), inst,     805                       axis_name, spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:816, in _batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name, spmd_axis_name, main_type)     814 in_axes = [0 if b else not_mapped for b in in_batched]     815 out_axes_dest = [0 if inst else zero_if_mapped for inst in instantiate] > 816 return batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest,     817                         axis_name, spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:821, in batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name, spmd_axis_name, main_type)     819 def batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name,     820                      spmd_axis_name, main_type): > 821   return _batch_jaxpr_axes(closed_jaxpr, axis_size, tuple(in_axes),     822                            tuple(out_axes_dest), axis_name, spmd_axis_name,     823                            main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:835, in _batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name, spmd_axis_name, main_type)     831 f = _batch_jaxpr_outer(f, axis_name, spmd_axis_name, axis_size, in_axes,     832                        main_type)     833 avals_in = [core.unmapped_aval(axis_size, axis_name, b, aval) if b is not not_mapped     834             else aval for aval, b in unsafe_zip(closed_jaxpr.in_avals, in_axes)] > 835 jaxpr_out, _, consts = pe.trace_to_jaxpr_dynamic(f, avals_in)     836 return core.ClosedJaxpr(jaxpr_out, consts), out_batched() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2155, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2153 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2154   main.jaxpr_stack = ()   type: ignore > 2155   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2156     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2157   del main, fun    2158 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2592, in AxisPrimitive.bind(self, *args, **params)    2590 def bind(self, *args, **params):    2591   top_trace = find_top_trace(args) > 2592   axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                   default=None, key=lambda t: getattr(t, 'level', 1))    2594   top_trace = (top_trace if not axis_main or axis_main.level  set[AxisName]:    2495   subst = NameGatheringSubst() > 2496   subst_axis_names(primitive, params, subst)    2497   return subst.axis_names File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2515, in subst_axis_names(primitive, params, subst, traverse)    2513 new_params = dict(params)    2514 for name, jaxpr in jaxpr_params: > 2515   new_params[name] = subst_axis_names_jaxpr(jaxpr, shadowed_subst)    2516 return new_params File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2571, in subst_axis_names_jaxpr(jaxpr, subst)    2569 def subst_axis_names_jaxpr(jaxpr: Jaxpr  ClosedJaxpr):    2565   subst = NameGatheringSubst() > 2566   do_subst_axis_names_jaxpr(jaxpr, subst)    2567   return frozenset(subst.axis_names) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2554, in do_subst_axis_names_jaxpr(jaxpr, subst)    2552   jaxpr = jaxpr.jaxpr    2553 var_map: dict[Var, Var] = {} > 2554 invars = [subst_axis_names_var(v, subst, var_map) for v in jaxpr.invars]   type: ignore[unionattr]    2555 constvars = [subst_axis_names_var(v, subst, var_map) for v in jaxpr.constvars]   type: ignore[unionattr]    2556 eqns = [subst_axis_names_eqn(eqn, subst, var_map) for eqn in jaxpr.eqns]   type: ignore[unionattr] File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2554, in (.0)    2552   jaxpr = jaxpr.jaxpr    2553 var_map: dict[Var, Var] = {} > 2554 invars = [subst_axis_names_var(v, subst, var_map) for v in jaxpr.invars]   type: ignore[unionattr]    2555 constvars = [subst_axis_names_var(v, subst, var_map) for v in jaxpr.constvars]   type: ignore[unionattr]    2556 eqns = [subst_axis_names_eqn(eqn, subst, var_map) for eqn in jaxpr.eqns]   type: ignore[unionattr] File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2531, in subst_axis_names_var(v, subst, var_map)    2529   return v    2530 names = tuple(it.chain.from_iterable(subst(name) for name in v.aval.named_shape)) > 2531 named_shape = {name: axis_frame(name).size for name in names}    2532 if len(named_shape) != len(names):    2533   raise DuplicateAxisNameError(v) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2531, in (.0)    2529   return v    2530 names = tuple(it.chain.from_iterable(subst(name) for name in v.aval.named_shape)) > 2531 named_shape = {name: axis_frame(name).size for name in names}    2532 if len(named_shape) != len(names):    2533   raise DuplicateAxisNameError(v) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2479, in axis_frame(axis_name, main_trace)    2476     return frame    2477 named_axes = [frame.name for frame in reversed(frames)    2478               if not isinstance(frame.name, _TempAxisName)] > 2479 raise NameError(    2480     f'unbound axis name: {axis_name}. The following axis names (e.g. defined '    2481     f'by pmap) are available to collective operations: {named_axes}') NameError: unbound axis name: a. The following axis names (e.g. defined by pmap) are available to collective operations: [, , , , ] ``` My understanding is that the above usage of `xmap` should produce the same output as the default behaviour of `vmap` (i.e. mapping over the 0th axis).  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info python 3.11, macOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,xmap + expm bug," Description Not sure if I'm misusing `xmap`, but the following code: ``` from jax.scipy.linalg import expm import jax.numpy as jnp from jax.experimental.maps import xmap xmap(expm, in_axes={0: 'a'}, out_axes={0: 'a'})(jnp.array([[[0., 1.], [1., 0.]]])) ``` produces the error: ```  JaxStackTraceBeforeTransformation         Traceback (most recent call last) File :198, in _run_module_as_main() File :88, in _run_code() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel_launcher.py:17      15 from ipykernel import kernelapp as app > 17 app.launch_new_instance() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/traitlets/config/application.py:1043, in launch_instance()    1042 app.initialize(argv) > 1043 app.start() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelapp.py:725, in start()     724 try: > 725     self.io_loop.start()     726 except KeyboardInterrupt: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/tornado/platform/asyncio.py:195, in start()     194 def start(self) > None: > 195     self.asyncio_loop.run_forever() File /opt/anaconda3/envs/devEnv311/lib/python3.11/asyncio/base_events.py:607, in run_forever()     606 while True: > 607     self._run_once()     608     if self._stopping: File /opt/anaconda3/envs/devEnv311/lib/python3.11/asyncio/base_events.py:1922, in _run_once()    1921     else: > 1922         handle._run()    1923 handle = None File /opt/anaconda3/envs/devEnv311/lib/python3.11/asyncio/events.py:80, in _run()      79 try: > 80     self._context.run(self._callback, *self._args)      81 except (SystemExit, KeyboardInterrupt): File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelbase.py:513, in dispatch_queue()     512 try: > 513     await self.process_one()     514 except Exception: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelbase.py:502, in process_one()     501         return None > 502 await dispatch(*args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelbase.py:409, in dispatch_shell()     408     if inspect.isawaitable(result): > 409         await result     410 except Exception: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/kernelbase.py:729, in execute_request()     728 if inspect.isawaitable(reply_content): > 729     reply_content = await reply_content     731  Flush output before sending the reply. File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/ipkernel.py:422, in do_execute()     421 if with_cell_id: > 422     res = shell.run_cell(     423         code,     424         store_history=store_history,     425         silent=silent,     426         cell_id=cell_id,     427     )     428 else: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/ipykernel/zmqshell.py:540, in run_cell()     539 self._last_traceback = None > 540 return super().run_cell(*args, **kwargs) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3009, in run_cell()    3008 try: > 3009     result = self._run_cell(    3010         raw_cell, store_history, silent, shell_futures, cell_id    3011     )    3012 finally: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3064, in _run_cell()    3063 try: > 3064     result = runner(coro)    3065 except BaseException as e: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/async_helpers.py:129, in _pseudo_sync_runner()     128 try: > 129     coro.send(None)     130 except StopIteration as exc: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3269, in run_cell_async()    3266 interactivity = ""none"" if silent else self.ast_node_interactivity > 3269 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,    3270        interactivity=interactivity, compiler=compiler, result=result)    3272 self.last_execution_succeeded = not has_raised File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3448, in run_ast_nodes()    3447     asy = compare(code) > 3448 if await self.run_code(code, result, async_=asy):    3449     return True File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/IPython/core/interactiveshell.py:3508, in run_code()    3507     else: > 3508         exec(code_obj, self.user_global_ns, self.user_ns)    3509 finally:    3510      Reset our crash handler in place Cell In[2], line 1 > 1 xmap(expm, in_axes={0: 'a'}, out_axes={0: 'a'})(jnp.array([[[0., 1.], [1., 0.]]])) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/scipy/linalg.py:457, in expm()     455   return R > 457 R = lax.cond(n_squarings > max_squarings, _nan, _compute, (A, P, Q))     458 return R File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/scipy/linalg.py:453, in _compute()     452 A, P, Q = args > 453 R = _solve_P_Q(P, Q, upper_triangular)     454 R = _squaring(R, n_squarings, max_squarings) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/scipy/linalg.py:496, in _solve_P_Q()     495 else: > 496   return jnp.linalg.solve(Q, P) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/numpy/linalg.py:608, in solve()     607 a, b = promote_dtypes_inexact(jnp.asarray(a), jnp.asarray(b)) > 608 return lax_linalg._solve(a, b) JaxStackTraceBeforeTransformation: NameError: unbound axis name: a. The following axis names (e.g. defined by pmap) are available to collective operations: [, , , , ] The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: NameError                                 Traceback (most recent call last) Cell In[2], line 1 > 1 xmap(expm, in_axes={0: 'a'}, out_axes={0: 'a'})(jnp.array([[[0., 1.], [1., 0.]]])) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:603, in xmap..fun_mapped(*args)     601 tree_map(dispatch.check_arg, args)     602 fun_flat, args_flat, params, _, out_tree = infer_params(*args) > 603 out_flat = xmap_p.bind(fun_flat, *args_flat, **params)     604 return verify_outputs(out_flat, out_tree, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:825, in XMapPrimitive.bind(self, fun, in_axes, *args, **params)     823 def bind(self, fun, *args, in_axes, **params):     824   assert len(in_axes) == len(args), (in_axes, args) > 825   return core.map_bind(self, fun, *args, in_axes=in_axes, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2312, in map_bind(primitive, fun, *args, **params)    2308 def map_bind(primitive: MapPrimitive, fun, *args, **params):    2309   map_bind_continuation, top_trace, fun, tracers, params = (    2310       map_bind_with_continuation(primitive, fun, *args, **params))    2311   return map_bind_continuation( > 2312       primitive.process(top_trace, fun, tracers, params)) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:828, in XMapPrimitive.process(self, trace, fun, tracers, params)     827 def process(self, trace, fun, tracers, params): > 828   return trace.process_xmap(self, fun, tracers, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:824, in EvalTrace.process_call(self, primitive, f, tracers, params)     823 def process_call(self, primitive, f, tracers, params): > 824   return primitive.impl(f, *tracers, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:632, in xmap_impl(fun, name, in_axes, out_axes_thunk, donated_invars, global_axis_sizes, axis_resources, resource_env, backend, spmd_in_axes, spmd_out_axes_thunk, *args)     628 def xmap_impl(fun: lu.WrappedFun, *args, name, in_axes, out_axes_thunk, donated_invars,     629               global_axis_sizes, axis_resources, resource_env, backend,     630               spmd_in_axes, spmd_out_axes_thunk):     631   in_avals = [core.raise_to_shaped(core.get_aval(arg)) for arg in args] > 632   xmap_callable = make_xmap_callable(     633       fun, name, in_axes, out_axes_thunk, donated_invars, global_axis_sizes,     634       axis_resources, resource_env, backend,     635       spmd_in_axes, spmd_out_axes_thunk,     636       None, *in_avals).compile().unsafe_call     637   distributed_debug_log((""Running xmapped function"", name),     638                         (""python function"", fun.f),     639                         (""mesh"", resource_env.physical_mesh),     640                         (""abstract args"", in_avals))     641   return xmap_callable(*args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:345, in cache..memoized_fun(fun, *args)     343   fun.populate_stores(stores)     344 else: > 345   ans = call(fun, *args)     346   cache[key] = (ans, fun.stores)     348 return ans File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/maps.py:705, in make_xmap_callable(fun, name, in_axes, out_axes_thunk, donated_invars, global_axis_sizes, axis_resources, resource_env, backend, spmd_in_axes, spmd_out_axes_thunk, lowering_platform, *in_avals)     698   return pxla.lower_mesh_computation(     699       f, 'xmap', name, mesh,     700       in_shardings, out_shardings, donated_invars,     701       use_spmd_lowering, in_avals,     702       tiling_method=tiling_method,     703       lowering_platform=lowering_platform)     704 else: > 705   return dispatch.sharded_lowering(     706       f, name, donated_invars, True, False, in_avals, (None,) * len(in_avals),     707       lowering_platform=lowering_platform) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/dispatch.py:241, in sharded_lowering(fun, name, donated_invars, keep_unused, inline, in_avals, in_shardings, lowering_platform)     236 in_shardings = [UNSPECIFIED if i is None else i for i in in_shardings]   type: ignore     238  Pass in a singleton `UNSPECIFIED` for out_shardings because we don't know     239  the number of output avals at this stage. lower_sharding_computation will     240  apply it to all out_avals. > 241 return pxla.lower_sharding_computation(     242     fun, 'jit', name, in_shardings, UNSPECIFIED, donated_invars,     243     tuple(in_avals), keep_unused=keep_unused, inline=inline, always_lower=False,     244     devices_from_context=None, lowering_platform=lowering_platform) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py:1899, in lower_sharding_computation(fun_or_jaxpr, api_name, fun_name, in_shardings, out_shardings, donated_invars, global_in_avals, keep_unused, inline, always_lower, devices_from_context, lowering_platform, override_lowering_rules)    1893  1. Trace to jaxpr and preprocess/verify it    1894 auto_spmd_lowering = (    1895     check_if_any_auto(in_shardings) if is_unspecified(out_shardings) else    1896     check_if_any_auto(it.chain.from_iterable([in_shardings, out_shardings])))   type: ignore    1898 (closed_jaxpr, global_in_avals, global_out_avals, donated_invars, > 1899  kept_var_idx, name_stack) = _trace_to_jaxpr_and_dce(    1900     fun_or_jaxpr, global_in_avals, api_name, fun_name, keep_unused,    1901     donated_invars, auto_spmd_lowering)    1902 jaxpr = closed_jaxpr.jaxpr    1903 in_shardings = tuple(s for i, s in enumerate(in_shardings) if i in kept_var_idx) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py:1692, in cache_wrap..wrapped(f, *args, **kwargs)    1690 def wrapped(f, *args, **kwargs):    1691   if isinstance(f, lu.WrappedFun): > 1692     return _wrapped_with_lu_cache(f, *args, **kwargs)    1693   else:    1694     return _wrapped_with_weakref_lru_cache(f, *args, **kwargs) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:345, in cache..memoized_fun(fun, *args)     343   fun.populate_stores(stores)     344 else: > 345   ans = call(fun, *args)     346   cache[key] = (ans, fun.stores)     348 return ans File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py:1707, in _trace_to_jaxpr_and_dce(fun_or_jaxpr, global_in_avals, api_name, fun_name, keep_unused, donated_invars, auto_spmd_lowering)    1703 if isinstance(fun_or_jaxpr, lu.WrappedFun):    1704   with dispatch.log_elapsed_time(    1705       ""Finished tracing + transforming {fun_name} in {elapsed_time} sec"",    1706       fun_name=str(name_stack), event=dispatch.JAXPR_TRACE_EVENT): > 1707     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_final(    1708         fun_or_jaxpr, global_in_avals)    1709 else:    1710   assert isinstance(fun_or_jaxpr, core.ClosedJaxpr) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2233, in trace_to_jaxpr_final(fun, in_avals, debug_info, keep_inputs)    2231   main.jaxpr_stack = ()   type: ignore    2232   with core.new_sublevel(): > 2233     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2234       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2235   del fun, main    2236 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2596, in AxisPrimitive.bind(self, *args, **params)    2592 axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                 default=None, key=lambda t: getattr(t, 'level', 1))    2594 top_trace = (top_trace if not axis_main or axis_main.level  2596 return self.bind_with_trace(top_trace, args, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:389, in Primitive.bind_with_trace(self, trace, args, params)     388 def bind_with_trace(self, trace, args, params): > 389   out = trace.process_primitive(self, map(trace.full_raise, args), params)     390   return map(full_lower, out) if self.multiple_results else full_lower(out) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:425, in BatchTrace.process_primitive(self, primitive, tracers, params)     423   frame = self.get_frame(vals_in, dims_in)     424   batcher_primitive = self.get_axis_primitive_batcher(primitive, frame) > 425   val_out, dim_out = batcher_primitive(vals_in, dims_in, **params)     426 elif all(bdim is not_mapped for bdim in dims_in):     427   return primitive.bind(*vals_in, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py:1409, in _pjit_batcher(insert_axis, spmd_axis_name, axis_size, axis_name, main_type, vals_in, dims_in, jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline)    1403 def _pjit_batcher(insert_axis, spmd_axis_name,    1404                   axis_size, axis_name, main_type,    1405                   vals_in, dims_in,    1406                   jaxpr, in_shardings, out_shardings,    1407                   resource_env, donated_invars, name, keep_unused, inline):    1408   segment_lens, dims_in = batching.indirectify_ragged_axes(dims_in) > 1409   new_jaxpr, axes_out = batching.batch_jaxpr2(    1410       jaxpr, axis_size, dims_in, axis_name=axis_name,    1411       spmd_axis_name=spmd_axis_name, main_type=main_type)    1413    `insert_axis` is set to True only for some `xmap` uses.    1414   new_parts = (axis_name,) if insert_axis else (    1415       () if spmd_axis_name is None else spmd_axis_name) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:767, in batch_jaxpr2(closed_jaxpr, axis_size, in_axes, axis_name, spmd_axis_name, main_type)     754 def batch_jaxpr2(     755     closed_jaxpr: core.ClosedJaxpr,     756     axis_size: core.AxisSize,    (...)     765    consistency constraints, such as typeagreement across arms of a     766    `lax.cond`, or inputoutput agreement for the body of a `lax.scan`. > 767   return _batch_jaxpr2(closed_jaxpr, axis_size, tuple(in_axes), axis_name,     768                        spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:790, in _batch_jaxpr2(closed_jaxpr, axis_size, in_axes, axis_name, spmd_axis_name, main_type)     783 in_axes2, avals_in = unzip2([     784     handle_ragged(closed_jaxpr.in_avals, dim, aval)     785     if isinstance(dim, RaggedAxis) else (dim, aval)     786     for dim, aval in zip(in_axes, closed_jaxpr.in_avals)])     787 avals_in2 = [core.unmapped_aval(axis_size, axis_name, b, aval)     788              if b is not not_mapped else aval     789              for aval, b in unsafe_zip(avals_in, in_axes2)] > 790 jaxpr_out, _, consts = pe.trace_to_jaxpr_dynamic(f, avals_in2)     791 return core.ClosedJaxpr(jaxpr_out, consts), out_axes() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2155, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2153 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2154   main.jaxpr_stack = ()   type: ignore > 2155   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2156     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2157   del main, fun    2158 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/lax/control_flow/conditionals.py:813, in cond_bind(branches, linear, *args)     811   for jaxpr in branches:     812     core.check_jaxpr(jaxpr.jaxpr) > 813 return core.AxisPrimitive.bind(cond_p, *args, branches=branches, linear=linear) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2596, in AxisPrimitive.bind(self, *args, **params)    2592 axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                 default=None, key=lambda t: getattr(t, 'level', 1))    2594 top_trace = (top_trace if not axis_main or axis_main.level  2596 return self.bind_with_trace(top_trace, args, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:389, in Primitive.bind_with_trace(self, trace, args, params)     388 def bind_with_trace(self, trace, args, params): > 389   out = trace.process_primitive(self, map(trace.full_raise, args), params)     390   return map(full_lower, out) if self.multiple_results else full_lower(out) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:425, in BatchTrace.process_primitive(self, primitive, tracers, params)     423   frame = self.get_frame(vals_in, dims_in)     424   batcher_primitive = self.get_axis_primitive_batcher(primitive, frame) > 425   val_out, dim_out = batcher_primitive(vals_in, dims_in, **params)     426 elif all(bdim is not_mapped for bdim in dims_in):     427   return primitive.bind(*vals_in, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/lax/control_flow/conditionals.py:374, in _cond_batching_rule(spmd_axis_name, axis_size, axis_name, main_type, args, dims, branches, linear)     371 in_batched  = [True] * len(branches[0].in_avals)     372 out_batched = [True] * len(branches[0].out_avals) > 374 branches_batched = [     375     batching.batch_jaxpr(     376         jaxpr, axis_size, in_batched, out_batched, axis_name, spmd_axis_name,     377         main_type)[0]     378     for jaxpr in branches]     380 branch_outs = []     381 for i, jaxpr in enumerate(branches_batched):     382    Perform a select on the inputs for safety of reversemode autodiff; see     383    https://github.com/google/jax/issues/1052 File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/lax/control_flow/conditionals.py:375, in (.0)     371 in_batched  = [True] * len(branches[0].in_avals)     372 out_batched = [True] * len(branches[0].out_avals)     374 branches_batched = [ > 375     batching.batch_jaxpr(     376         jaxpr, axis_size, in_batched, out_batched, axis_name, spmd_axis_name,     377         main_type)[0]     378     for jaxpr in branches]     380 branch_outs = []     381 for i, jaxpr in enumerate(branches_batched):     382    Perform a select on the inputs for safety of reversemode autodiff; see     383    https://github.com/google/jax/issues/1052 File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:804, in batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name, spmd_axis_name, main_type)     801 def batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name,     802                 spmd_axis_name, main_type):     803   inst = tuple(instantiate) if isinstance(instantiate, list) else instantiate > 804   return _batch_jaxpr(closed_jaxpr, axis_size, tuple(in_batched), inst,     805                       axis_name, spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:816, in _batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name, spmd_axis_name, main_type)     814 in_axes = [0 if b else not_mapped for b in in_batched]     815 out_axes_dest = [0 if inst else zero_if_mapped for inst in instantiate] > 816 return batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest,     817                         axis_name, spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:821, in batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name, spmd_axis_name, main_type)     819 def batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name,     820                      spmd_axis_name, main_type): > 821   return _batch_jaxpr_axes(closed_jaxpr, axis_size, tuple(in_axes),     822                            tuple(out_axes_dest), axis_name, spmd_axis_name,     823                            main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:835, in _batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name, spmd_axis_name, main_type)     831 f = _batch_jaxpr_outer(f, axis_name, spmd_axis_name, axis_size, in_axes,     832                        main_type)     833 avals_in = [core.unmapped_aval(axis_size, axis_name, b, aval) if b is not not_mapped     834             else aval for aval, b in unsafe_zip(closed_jaxpr.in_avals, in_axes)] > 835 jaxpr_out, _, consts = pe.trace_to_jaxpr_dynamic(f, avals_in)     836 return core.ClosedJaxpr(jaxpr_out, consts), out_batched() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2155, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2153 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2154   main.jaxpr_stack = ()   type: ignore > 2155   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2156     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2157   del main, fun    2158 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2596, in AxisPrimitive.bind(self, *args, **params)    2592 axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                 default=None, key=lambda t: getattr(t, 'level', 1))    2594 top_trace = (top_trace if not axis_main or axis_main.level  2596 return self.bind_with_trace(top_trace, args, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:389, in Primitive.bind_with_trace(self, trace, args, params)     388 def bind_with_trace(self, trace, args, params): > 389   out = trace.process_primitive(self, map(trace.full_raise, args), params)     390   return map(full_lower, out) if self.multiple_results else full_lower(out) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:425, in BatchTrace.process_primitive(self, primitive, tracers, params)     423   frame = self.get_frame(vals_in, dims_in)     424   batcher_primitive = self.get_axis_primitive_batcher(primitive, frame) > 425   val_out, dim_out = batcher_primitive(vals_in, dims_in, **params)     426 elif all(bdim is not_mapped for bdim in dims_in):     427   return primitive.bind(*vals_in, **params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/pjit.py:1409, in _pjit_batcher(insert_axis, spmd_axis_name, axis_size, axis_name, main_type, vals_in, dims_in, jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline)    1403 def _pjit_batcher(insert_axis, spmd_axis_name,    1404                   axis_size, axis_name, main_type,    1405                   vals_in, dims_in,    1406                   jaxpr, in_shardings, out_shardings,    1407                   resource_env, donated_invars, name, keep_unused, inline):    1408   segment_lens, dims_in = batching.indirectify_ragged_axes(dims_in) > 1409   new_jaxpr, axes_out = batching.batch_jaxpr2(    1410       jaxpr, axis_size, dims_in, axis_name=axis_name,    1411       spmd_axis_name=spmd_axis_name, main_type=main_type)    1413    `insert_axis` is set to True only for some `xmap` uses.    1414   new_parts = (axis_name,) if insert_axis else (    1415       () if spmd_axis_name is None else spmd_axis_name) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:767, in batch_jaxpr2(closed_jaxpr, axis_size, in_axes, axis_name, spmd_axis_name, main_type)     754 def batch_jaxpr2(     755     closed_jaxpr: core.ClosedJaxpr,     756     axis_size: core.AxisSize,    (...)     765    consistency constraints, such as typeagreement across arms of a     766    `lax.cond`, or inputoutput agreement for the body of a `lax.scan`. > 767   return _batch_jaxpr2(closed_jaxpr, axis_size, tuple(in_axes), axis_name,     768                        spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:790, in _batch_jaxpr2(closed_jaxpr, axis_size, in_axes, axis_name, spmd_axis_name, main_type)     783 in_axes2, avals_in = unzip2([     784     handle_ragged(closed_jaxpr.in_avals, dim, aval)     785     if isinstance(dim, RaggedAxis) else (dim, aval)     786     for dim, aval in zip(in_axes, closed_jaxpr.in_avals)])     787 avals_in2 = [core.unmapped_aval(axis_size, axis_name, b, aval)     788              if b is not not_mapped else aval     789              for aval, b in unsafe_zip(avals_in, in_axes2)] > 790 jaxpr_out, _, consts = pe.trace_to_jaxpr_dynamic(f, avals_in2)     791 return core.ClosedJaxpr(jaxpr_out, consts), out_axes() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2155, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2153 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2154   main.jaxpr_stack = ()   type: ignore > 2155   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2156     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2157   del main, fun    2158 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2596, in AxisPrimitive.bind(self, *args, **params)    2592 axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                 default=None, key=lambda t: getattr(t, 'level', 1))    2594 top_trace = (top_trace if not axis_main or axis_main.level  2596 return self.bind_with_trace(top_trace, args, params) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:389, in Primitive.bind_with_trace(self, trace, args, params)     388 def bind_with_trace(self, trace, args, params): > 389   out = trace.process_primitive(self, map(trace.full_raise, args), params)     390   return map(full_lower, out) if self.multiple_results else full_lower(out) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:431, in BatchTrace.process_primitive(self, primitive, tracers, params)     429   frame = self.get_frame(vals_in, dims_in)     430   batched_primitive = self.get_primitive_batcher(primitive, frame) > 431   val_out, dim_out = batched_primitive(vals_in, dims_in, **params)     432 src = source_info_util.current()     433 if primitive.multiple_results: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/lax/control_flow/solves.py:399, in _linear_solve_batching_rule(spmd_axis_name, axis_size, axis_name, main_type, args, dims, const_lengths, jaxprs)     396 x_bat = [False] * len(solve.out_avals)     397 for i in range(1 + len(orig_b_bat) + len(solve.out_avals)):     398    Apply vecmat and solve > new batched parts of x > 399   solve_jaxpr_batched, solve_x_bat = batching.batch_jaxpr(     400       solve, axis_size, solve_bat + b_bat, instantiate=x_bat,     401       axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)     402   if vecmat is None:     403     vecmat_jaxpr_batched = None File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:804, in batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name, spmd_axis_name, main_type)     801 def batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name,     802                 spmd_axis_name, main_type):     803   inst = tuple(instantiate) if isinstance(instantiate, list) else instantiate > 804   return _batch_jaxpr(closed_jaxpr, axis_size, tuple(in_batched), inst,     805                       axis_name, spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:816, in _batch_jaxpr(closed_jaxpr, axis_size, in_batched, instantiate, axis_name, spmd_axis_name, main_type)     814 in_axes = [0 if b else not_mapped for b in in_batched]     815 out_axes_dest = [0 if inst else zero_if_mapped for inst in instantiate] > 816 return batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest,     817                         axis_name, spmd_axis_name, main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:821, in batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name, spmd_axis_name, main_type)     819 def batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name,     820                      spmd_axis_name, main_type): > 821   return _batch_jaxpr_axes(closed_jaxpr, axis_size, tuple(in_axes),     822                            tuple(out_axes_dest), axis_name, spmd_axis_name,     823                            main_type) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/batching.py:835, in _batch_jaxpr_axes(closed_jaxpr, axis_size, in_axes, out_axes_dest, axis_name, spmd_axis_name, main_type)     831 f = _batch_jaxpr_outer(f, axis_name, spmd_axis_name, axis_size, in_axes,     832                        main_type)     833 avals_in = [core.unmapped_aval(axis_size, axis_name, b, aval) if b is not not_mapped     834             else aval for aval, b in unsafe_zip(closed_jaxpr.in_avals, in_axes)] > 835 jaxpr_out, _, consts = pe.trace_to_jaxpr_dynamic(f, avals_in)     836 return core.ClosedJaxpr(jaxpr_out, consts), out_batched() File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2155, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2153 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2154   main.jaxpr_stack = ()   type: ignore > 2155   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2156     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2157   del main, fun    2158 return jaxpr, out_avals, consts File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/interpreters/partial_eval.py:2177, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2175 in_tracers = _input_type_to_tracers(trace.new_arg, in_avals)    2176 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep] > 2177 ans = fun.call_wrapped(*in_tracers_)    2178 out_tracers = map(trace.full_raise, ans)    2179 jaxpr, consts = frame.to_jaxpr(out_tracers) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/linear_util.py:188, in WrappedFun.call_wrapped(self, *args, **kwargs)     185 gen = gen_static_args = out_store = None     187 try: > 188   ans = self.f(*args, **dict(self.params, **kwargs))     189 except:     190    Some transformations yield from inside context managers, so we have to     191    interrupt them before reraising the exception. Otherwise they will only     192    get garbagecollected at some later time, running their cleanup tasks     193    only after this exception is handled, which can corrupt the global     194    state.     195   while stack: File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:235, in jaxpr_as_fun(closed_jaxpr, *args)     233      234 def jaxpr_as_fun(closed_jaxpr: ClosedJaxpr, *args): > 235   return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:454, in eval_jaxpr(jaxpr, consts, propagate_source_info, *args)     452 traceback = eqn.source_info.traceback if propagate_source_info else None     453 with source_info_util.user_context(traceback, name_stack=name_stack): > 454   ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)     455 if eqn.primitive.multiple_results:     456   map(write, eqn.outvars, ans) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2592, in AxisPrimitive.bind(self, *args, **params)    2590 def bind(self, *args, **params):    2591   top_trace = find_top_trace(args) > 2592   axis_main = max((axis_frame(a).main_trace for a in used_axis_names(self, params)),    2593                   default=None, key=lambda t: getattr(t, 'level', 1))    2594   top_trace = (top_trace if not axis_main or axis_main.level  set[AxisName]:    2495   subst = NameGatheringSubst() > 2496   subst_axis_names(primitive, params, subst)    2497   return subst.axis_names File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2515, in subst_axis_names(primitive, params, subst, traverse)    2513 new_params = dict(params)    2514 for name, jaxpr in jaxpr_params: > 2515   new_params[name] = subst_axis_names_jaxpr(jaxpr, shadowed_subst)    2516 return new_params File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2571, in subst_axis_names_jaxpr(jaxpr, subst)    2569 def subst_axis_names_jaxpr(jaxpr: Jaxpr  ClosedJaxpr):    2565   subst = NameGatheringSubst() > 2566   do_subst_axis_names_jaxpr(jaxpr, subst)    2567   return frozenset(subst.axis_names) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2554, in do_subst_axis_names_jaxpr(jaxpr, subst)    2552   jaxpr = jaxpr.jaxpr    2553 var_map: dict[Var, Var] = {} > 2554 invars = [subst_axis_names_var(v, subst, var_map) for v in jaxpr.invars]   type: ignore[unionattr]    2555 constvars = [subst_axis_names_var(v, subst, var_map) for v in jaxpr.constvars]   type: ignore[unionattr]    2556 eqns = [subst_axis_names_eqn(eqn, subst, var_map) for eqn in jaxpr.eqns]   type: ignore[unionattr] File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2554, in (.0)    2552   jaxpr = jaxpr.jaxpr    2553 var_map: dict[Var, Var] = {} > 2554 invars = [subst_axis_names_var(v, subst, var_map) for v in jaxpr.invars]   type: ignore[unionattr]    2555 constvars = [subst_axis_names_var(v, subst, var_map) for v in jaxpr.constvars]   type: ignore[unionattr]    2556 eqns = [subst_axis_names_eqn(eqn, subst, var_map) for eqn in jaxpr.eqns]   type: ignore[unionattr] File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2531, in subst_axis_names_var(v, subst, var_map)    2529   return v    2530 names = tuple(it.chain.from_iterable(subst(name) for name in v.aval.named_shape)) > 2531 named_shape = {name: axis_frame(name).size for name in names}    2532 if len(named_shape) != len(names):    2533   raise DuplicateAxisNameError(v) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2531, in (.0)    2529   return v    2530 names = tuple(it.chain.from_iterable(subst(name) for name in v.aval.named_shape)) > 2531 named_shape = {name: axis_frame(name).size for name in names}    2532 if len(named_shape) != len(names):    2533   raise DuplicateAxisNameError(v) File /opt/anaconda3/envs/devEnv311/lib/python3.11/sitepackages/jax/_src/core.py:2479, in axis_frame(axis_name, main_trace)    2476     return frame    2477 named_axes = [frame.name for frame in reversed(frames)    2478               if not isinstance(frame.name, _TempAxisName)] > 2479 raise NameError(    2480     f'unbound axis name: {axis_name}. The following axis names (e.g. defined '    2481     f'by pmap) are available to collective operations: {named_axes}') NameError: unbound axis name: a. The following axis names (e.g. defined by pmap) are available to collective operations: [, , , , ] ``` My understanding is that the above usage of `xmap` should produce the same output as the default behaviour of `vmap` (i.e. mapping over the 0th axis).  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info python 3.11, macOS  NVIDIA GPU info _No response_",2023-08-21T15:34:13Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17201,"Hi   Looks like this issue appears to be resolved. I tested this with JAX version 0.4.26 on Colab CPU. It gives the same result as that of `vmap` `xmap`: ```python from jax.scipy.linalg import expm import jax.numpy as jnp from jax.experimental.maps import xmap xmap(expm, in_axes={0: 'a'}, out_axes={0: 'a'})(jnp.array([[[0., 1.], [1., 0.]]])) ``` Output: ``` :3: DeprecationWarning: jax.experimental.maps and jax.experimental.maps.xmap are deprecated and will be removed in a future release. Use jax.experimental.shard_map or jax.vmap with the spmd_axis_name argument for expressing SPMD deviceparallel computations. Please file an issue on https://github.com/google/jax/issues if neither jax.experimental.shard_map nor jax.vmap are suitable for your use case.   from jax.experimental.maps import xmap Array([[[1.5430806, 1.1752012],         [1.1752012, 1.5430806]]], dtype=float32) ``` `vmap`: ```python from jax import vmap vmap(expm)(jnp.array([[[0., 1.], [1., 0.]]])) ``` Output: ``` Array([[[1.5430806, 1.1752012],         [1.1752012, 1.5430806]]], dtype=float32) ``` Please find the gist for reference. I have also tested this on Macbook Pro with M1 Pro chip and it works there also. please find the screenshot for reference.  However, it is recommended transitioning away from `xmap` as it is deprecated and will be removed in future versions. Consider using `jax.vmap` with the `spmd_axis_name` argument or `jax.experimental.shard_map.shard_map` instead as suggested by warning message. You can find the documentation on `jax.experimental.shard_map.shard_map` here:https://jax.readthedocs.io/en/latest/notebooks/shard_map.html  Thank you.","Ah thanks  , I wasn't aware that `xmap` had been deprecated.",Hi   Please feel free to close the issue if resolved. Thank you.
4700,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] call_module() got an unexpected keyword argument 'function_list')， 内容是 ( Description When trying to run minimal example for jax2tf.convert using native_serialization, I'm getting the following error: tensorflow==2.12.0 jax==0.4.14 **Code** ``` import jax import jax.numpy as jp import tensorflow as tf def f_jax(x):     return jp.sin(jp.cos(x)) f_tf = jax2tf.convert(f_jax, native_serialization=True) res = f_tf(tf.random.uniform((20, 3))) ``` **Error** ```  TypeError                                 Traceback (most recent call last) Cell In[26], line 11 > 11 res = f_tf(tf.random.uniform((20, 3))) File /opt/conda/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:416, in convert..converted_fun_tf(*args_tf, **kwargs_tf)     408     outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)     409     return (tuple(outs_tf),     410             _make_custom_gradient_fn_tf(     411                 impl=impl,     412                 args_tf=args_flat_tf,     413                 outs_avals=outs_avals,     414                 outs_tf=outs_tf)) > 416   outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)     417 else:     418   outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf) File /opt/conda/lib/python3.10/sitepackages/tensorflow/python/ops/custom_gradient.py:343, in Bind.__call__(self, *a, **k)     342 def __call__(self, *a, **k): > 343   return self._d(self._f, a, k) File /opt/conda/lib/python3.10/sitepackages/tensorflow/python/ops/custom_gradient.py:297, in custom_gradient..decorated(wrapped, args, kwargs)     295 """"""Decorated function with custom gradient.""""""     296 if context.executing_eagerly(): > 297   return _eager_mode_decorator(wrapped, args, kwargs)     298 else:     299   return _graph_mode_decorator(wrapped, args, kwargs) File /opt/conda/lib/python3.10/sitepackages/tensorflow/python/ops/custom_gradient.py:543, in _eager_mode_decorator(f, args, kwargs)     541 """"""Implement custom gradient decorator for eager mode.""""""     542 with tape_lib.VariableWatcher() as variable_watcher: > 543   result, grad_fn = f(*args, **kwargs)     544 flat_args = composite_tensor_gradient.get_flat_tensors_for_gradients(     545     nest.flatten(args))     546 flat_kwargs = composite_tensor_gradient.get_flat_tensors_for_gradients(     547     nest.flatten(kwargs)) File /opt/conda/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:408, in convert..converted_fun_tf..converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)     405 .custom_gradient     406 def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) > TfVal:     407   nonlocal outs_tree > 408   outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)     409   return (tuple(outs_tf),     410           _make_custom_gradient_fn_tf(     411               impl=impl,     412               args_tf=args_flat_tf,     413               outs_avals=outs_avals,     414               outs_tf=outs_tf)) File /opt/conda/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:517, in NativeSerializationImpl.run_fun_tf(self, args_flat_tf)     514 def run_fun_tf(self,     515                args_flat_tf: Sequence[TfVal]     516                ) > tuple[Sequence[TfVal], Sequence[core.ShapedArray], tree_util.PyTreeDef]: > 517   results = _run_exported_as_tf(args_flat_tf, self.exported)     518   return results, tuple(self.exported.out_avals), self.exported.out_tree File /opt/conda/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:876, in _run_exported_as_tf(args_flat_tf, exported)     872 if exported.in_shardings is not None:     873   args_flat_tf = tuple(     874     map(partial(_shard_value, skip_replicated_sharding=tf.executing_eagerly()),     875         kept_args_flat_tf, kept_args_avals, exported.in_shardings)) > 876 res = tfxla.call_module(args_flat_tf, **call_module_attrs)     877  TODO(b/278940799): Replace the TF v1 API with public TF2 API.     878  Add the custom call tf.function into the default graph, so those functions     879  will be available during tf.SavedModel.save.     880 if _thread_local_state.call_tf_concrete_function_list is not None: TypeError: call_module() got an unexpected keyword argument 'function_list' ```  What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14, tensorflow v2.12.0,   Which accelerator(s) are you using? CPU and/or GPU  Additional system info python==3.10, linux or macos silicon  NVIDIA GPU info  NVIDIASMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6      ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] call_module() got an unexpected keyword argument 'function_list'," Description When trying to run minimal example for jax2tf.convert using native_serialization, I'm getting the following error: tensorflow==2.12.0 jax==0.4.14 **Code** ``` import jax import jax.numpy as jp import tensorflow as tf def f_jax(x):     return jp.sin(jp.cos(x)) f_tf = jax2tf.convert(f_jax, native_serialization=True) res = f_tf(tf.random.uniform((20, 3))) ``` **Error** ```  TypeError                                 Traceback (most recent call last) Cell In[26], line 11 > 11 res = f_tf(tf.random.uniform((20, 3))) File /opt/conda/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:416, in convert..converted_fun_tf(*args_tf, **kwargs_tf)     408     outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)     409     return (tuple(outs_tf),     410             _make_custom_gradient_fn_tf(     411                 impl=impl,     412                 args_tf=args_flat_tf,     413                 outs_avals=outs_avals,     414                 outs_tf=outs_tf)) > 416   outs_flat_tf = converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)     417 else:     418   outs_tf, _, outs_tree = impl.run_fun_tf(args_flat_tf) File /opt/conda/lib/python3.10/sitepackages/tensorflow/python/ops/custom_gradient.py:343, in Bind.__call__(self, *a, **k)     342 def __call__(self, *a, **k): > 343   return self._d(self._f, a, k) File /opt/conda/lib/python3.10/sitepackages/tensorflow/python/ops/custom_gradient.py:297, in custom_gradient..decorated(wrapped, args, kwargs)     295 """"""Decorated function with custom gradient.""""""     296 if context.executing_eagerly(): > 297   return _eager_mode_decorator(wrapped, args, kwargs)     298 else:     299   return _graph_mode_decorator(wrapped, args, kwargs) File /opt/conda/lib/python3.10/sitepackages/tensorflow/python/ops/custom_gradient.py:543, in _eager_mode_decorator(f, args, kwargs)     541 """"""Implement custom gradient decorator for eager mode.""""""     542 with tape_lib.VariableWatcher() as variable_watcher: > 543   result, grad_fn = f(*args, **kwargs)     544 flat_args = composite_tensor_gradient.get_flat_tensors_for_gradients(     545     nest.flatten(args))     546 flat_kwargs = composite_tensor_gradient.get_flat_tensors_for_gradients(     547     nest.flatten(kwargs)) File /opt/conda/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:408, in convert..converted_fun_tf..converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)     405 .custom_gradient     406 def converted_fun_flat_with_custom_gradient_tf(*args_flat_tf: TfVal) > TfVal:     407   nonlocal outs_tree > 408   outs_tf, outs_avals, outs_tree = impl.run_fun_tf(args_flat_tf)     409   return (tuple(outs_tf),     410           _make_custom_gradient_fn_tf(     411               impl=impl,     412               args_tf=args_flat_tf,     413               outs_avals=outs_avals,     414               outs_tf=outs_tf)) File /opt/conda/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:517, in NativeSerializationImpl.run_fun_tf(self, args_flat_tf)     514 def run_fun_tf(self,     515                args_flat_tf: Sequence[TfVal]     516                ) > tuple[Sequence[TfVal], Sequence[core.ShapedArray], tree_util.PyTreeDef]: > 517   results = _run_exported_as_tf(args_flat_tf, self.exported)     518   return results, tuple(self.exported.out_avals), self.exported.out_tree File /opt/conda/lib/python3.10/sitepackages/jax/experimental/jax2tf/jax2tf.py:876, in _run_exported_as_tf(args_flat_tf, exported)     872 if exported.in_shardings is not None:     873   args_flat_tf = tuple(     874     map(partial(_shard_value, skip_replicated_sharding=tf.executing_eagerly()),     875         kept_args_flat_tf, kept_args_avals, exported.in_shardings)) > 876 res = tfxla.call_module(args_flat_tf, **call_module_attrs)     877  TODO(b/278940799): Replace the TF v1 API with public TF2 API.     878  Add the custom call tf.function into the default graph, so those functions     879  will be available during tf.SavedModel.save.     880 if _thread_local_state.call_tf_concrete_function_list is not None: TypeError: call_module() got an unexpected keyword argument 'function_list' ```  What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14, tensorflow v2.12.0,   Which accelerator(s) are you using? CPU and/or GPU  Additional system info python==3.10, linux or macos silicon  NVIDIA GPU info  NVIDIASMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6      ++",2023-08-20T15:28:37Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17194,"Hi  thanks for the report. It looks like this part of jax2tf is incompatible with the current tensorflow release: I can reproduce this error with v2.13.0, but the error goes away with the 2.14.0 release candidate: ``` $ python m pip install U pre tensorflow $ python m pip list | grep tensorflow tensorflow                       2.14.0rc0 $ python 17194.py    ``` I think the best fix here will be to use the newer tensorflow version.","Indeed, you need to use a more recent version of tensorflow."
2201,"以下是一个github上的jax下的一个issue, 标题是(XLA_FLAGS is only parsed once under JAX, which lead to side effects in unit test)， 内容是 ( Description I was trying to setup my unit test to run with 8 virtual CPUs for distribution testing, and by following https://github.com/google/jax/blob/main/tests/pmap_test.pyL90L110, I was able to config 8 CPUs. However, the teardown logic doesn't actually reset the number of CPU device back. The root cause seems to be that the XLA_FLAGS is only parsed once by XLA in https://github.com/tensorflow/tensorflow/blob/e9391d9005ee38e0803026eeee34ecbec5995750/tensorflow/compiler/xla/debug_options_flags.ccL1230. Consider the following snippet, once the backend is initialized, the number of devices can't be modified again. ```  Run all tests with 8 CPU devices. def setUpModule():   devices = jax.devices()    This will trigger backend init   assert len(devices) == 1, f""Expected {devices} to have 1 devices before setup""   global prev_xla_flags   prev_xla_flags = os.getenv(""XLA_FLAGS"")   flags_str = prev_xla_flags or """"    Don't override userspecified device count, or other XLA flags.   if ""xla_force_host_platform_device_count"" not in flags_str:     os.environ[""XLA_FLAGS""] = (flags_str +                                "" xla_force_host_platform_device_count=8"")    Clear any cached backends so new CPU backend will pick up the env var.   xla_bridge.get_backend.cache_clear()   devices = jax.devices()    This will fail if the backend is already initialized, and only 1 device all be available.   assert len(devices) == 8, f""Expected {devices} to have 8 devices""  Reset to previous configuration in case other test modules will be run. def tearDownModule():   if prev_xla_flags is None:     del os.environ[""XLA_FLAGS""]   else:     os.environ[""XLA_FLAGS""] = prev_xla_flags   devices = jax.devices()   assert len(devices) == 1, f""Expected {devices} to have 1 devices after cleanup"" ```  What jax/jaxlib version are you using? head  Which accelerator(s) are you using? CPU  Additional system info python 3.8  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"XLA_FLAGS is only parsed once under JAX, which lead to side effects in unit test"," Description I was trying to setup my unit test to run with 8 virtual CPUs for distribution testing, and by following https://github.com/google/jax/blob/main/tests/pmap_test.pyL90L110, I was able to config 8 CPUs. However, the teardown logic doesn't actually reset the number of CPU device back. The root cause seems to be that the XLA_FLAGS is only parsed once by XLA in https://github.com/tensorflow/tensorflow/blob/e9391d9005ee38e0803026eeee34ecbec5995750/tensorflow/compiler/xla/debug_options_flags.ccL1230. Consider the following snippet, once the backend is initialized, the number of devices can't be modified again. ```  Run all tests with 8 CPU devices. def setUpModule():   devices = jax.devices()    This will trigger backend init   assert len(devices) == 1, f""Expected {devices} to have 1 devices before setup""   global prev_xla_flags   prev_xla_flags = os.getenv(""XLA_FLAGS"")   flags_str = prev_xla_flags or """"    Don't override userspecified device count, or other XLA flags.   if ""xla_force_host_platform_device_count"" not in flags_str:     os.environ[""XLA_FLAGS""] = (flags_str +                                "" xla_force_host_platform_device_count=8"")    Clear any cached backends so new CPU backend will pick up the env var.   xla_bridge.get_backend.cache_clear()   devices = jax.devices()    This will fail if the backend is already initialized, and only 1 device all be available.   assert len(devices) == 8, f""Expected {devices} to have 8 devices""  Reset to previous configuration in case other test modules will be run. def tearDownModule():   if prev_xla_flags is None:     del os.environ[""XLA_FLAGS""]   else:     os.environ[""XLA_FLAGS""] = prev_xla_flags   devices = jax.devices()   assert len(devices) == 1, f""Expected {devices} to have 1 devices after cleanup"" ```  What jax/jaxlib version are you using? head  Which accelerator(s) are you using? CPU  Additional system info python 3.8  NVIDIA GPU info _No response_",2023-08-18T23:27:08Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17188,"The issue you are facing is due to the way that XLA parses the XLA_FLAGS environment variable. As you mentioned, XLA only parses the XLA_FLAGS environment variable once, and it does not reparse the environment variable if it is changed after the backend has been initialized. I think you can fix this by reinitializing the backend like  ```python if __name__ == '__main__':     import jax     jax.config.jax_xla_backend = None     import jax.lib.xla_bridge     jax.lib.xla_bridge.get_backend() ```"
10832,"以下是一个github上的jax下的一个issue, 标题是(Jaxlib fails to build from source on Windows)， 内容是 ( Description Here's the complete output (dljax) C:\Users\shash\jax>python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2"" cuda_version=""12.2"" cudnn_version=""8.9.4""      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: C:\bazel\bazel.EXE Bazel version: 6.3.2 Python binary path: C:/Users/shash/.conda/envs/dljax/python.exe Python version: 3.9 NumPy version: 1.25.2 MKLDNN enabled: yes Target CPU: AMD64 Target CPU features: release CUDA enabled: yes CUDA toolkit path: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 CUDNN library path: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 CUDA version: 12.2 CUDNN version: 8.9.4 NCCL enabled: yes TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... C:\bazel\bazel.EXE run verbose_failures=true //jaxlib/tools:build_wheel  output_path=C:\Users\shash\jax\dist cpu=AMD64 Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=80 INFO: Reading rc options for 'run' from c:\users\shash\jax\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   Inherited 'build' options: python_path=C:/Users/shash/.conda/envs/dljax/python.exe INFO: Reading rc options for 'run' from c:\users\shash\jax\.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false //xla/python:enable_tpu=false INFO: Reading rc options for 'run' from c:\users\shash\jax\.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=C:/Users/shash/.conda/envs/dljax/python.exe action_env=PYENV_ROOT python_path=C:/Users/shash/.conda/envs/dljax/python.exe action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 action_env CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 action_env TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 action_env TF_CUDA_VERSION=12.2 action_env TF_CUDNN_VERSION=8.9.4 config=mkl_open_source_only config=cuda INFO: Found applicable config definition build:short_logs in file c:\users\shash\jax\.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file c:\users\shash\jax\.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file c:\users\shash\jax\.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,sm_80,compute_90 crosstool_top=//crosstool:toolchain //:enable_cuda //xla/python:enable_gpu=true //xla/python:jax_cuda_pip_rpaths=true define=xla_python_enable_gpu=true INFO: Found applicable config definition build:windows in file c:\users\shash\jax\.bazelrc: copt=/D_USE_MATH_DEFINES host_copt=/D_USE_MATH_DEFINES copt=DWIN32_LEAN_AND_MEAN host_copt=DWIN32_LEAN_AND_MEAN copt=DNOGDI host_copt=DNOGDI copt=/Zc:preprocessor cxxopt=/std:c++17 host_cxxopt=/std:c++17 linkopt=/DEBUG host_linkopt=/DEBUG linkopt=/OPT:REF host_linkopt=/OPT:REF linkopt=/OPT:ICF host_linkopt=/OPT:ICF incompatible_strict_action_env=trueDEBUG: C:/users/shash/_bazel_shash/b3x3rdg2/external/xla/third_party/repo.bzl:132:14: Warning: skipping import of repository 'tf_runtime' because it already exists. DEBUG: C:/users/shash/_bazel_shash/b3x3rdg2/external/xla/third_party/repo.bzl:132:14: Warning: skipping import of repository 'llvmraw' because it already exists. WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseilcpp/archive/b971ac5250ea8de900eae9f95e06548d14cd95fe.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/re2/archive/03da4fc0857c285e3a26782f6bc8931c4c950df4.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/triton/archive/cl547802044.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/5aa1adeb2a412d1aca8972385cd9b26b1a524052.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVIDIA/cudnnfrontend/archive/refs/tags/v0.9.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found INFO: Analyzed target //jaxlib/tools:build_wheel (220 packages loaded, 18304 targets configured). INFO: Found 1 target... ERROR: C:/users/shash/_bazel_shash/b3x3rdg2/external/flatbuffers/BUILD.bazel:84:10: Linking external/flatbuffers/flatc.exe [for tool] failed: (Exit 1120): link.exe failed: error executing command (from target //:flatc)   cd /d C:/users/shash/_bazel_shash/b3x3rdg2/execroot/__main__   SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt     SET LIB=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\ATLMFC\lib\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.22621.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\\lib\10.0.22621.0\\um\x64     SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\vcpkg     SET PWD=/proc/self/cwd     SET RUNFILES_MANIFEST_ONLY=1     SET TEMP=C:\Users\shash\AppData\Local\Temp     SET TMP=C:\Users\shash\AppData\Local\Temp   C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\bin\HostX64\x64\link.exe out/x64_windowsoptexec50AE0418/bin/external/flatbuffers/flatc.exe2.params  Configuration: d960a019c29abfe37a18e15ad5a5dad5a3057c532cbeb0efbd45b2fd0b0808a3  Execution platform: //:platform Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 222.900s, Critical Path: 26.63s INFO: 782 processes: 430 internal, 352 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target b'LINK : warning LNK4001: no object files specified; libraries used\r\nLINK : error LNK2001: unresolved external symbol mainCRTStartup\r\nbazelout\\x64_windowsoptexec50AE0418\\bin\\external\\flatbuffers\\flatc.exe : fatal error LNK1120: 1 unresolved externals\r\n' Traceback (most recent call last):   File ""C:\Users\shash\jax\build\build.py"", line 559, in      main()   File ""C:\Users\shash\jax\build\build.py"", line 554, in main     shell(command)   File ""C:\Users\shash\jax\build\build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""C:\Users\shash\.conda\envs\dljax\lib\subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""C:\Users\shash\.conda\envs\dljax\lib\subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['C:\\bazel\\bazel.EXE', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=C:\\Users\\shash\\jax\\dist', 'cpu=AMD64']' returned nonzero exit status 1.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Jaxlib fails to build from source on Windows," Description Here's the complete output (dljax) C:\Users\shash\jax>python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2"" cuda_version=""12.2"" cudnn_version=""8.9.4""      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: C:\bazel\bazel.EXE Bazel version: 6.3.2 Python binary path: C:/Users/shash/.conda/envs/dljax/python.exe Python version: 3.9 NumPy version: 1.25.2 MKLDNN enabled: yes Target CPU: AMD64 Target CPU features: release CUDA enabled: yes CUDA toolkit path: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 CUDNN library path: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 CUDA version: 12.2 CUDNN version: 8.9.4 NCCL enabled: yes TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... C:\bazel\bazel.EXE run verbose_failures=true //jaxlib/tools:build_wheel  output_path=C:\Users\shash\jax\dist cpu=AMD64 Extracting Bazel installation... Starting local Bazel server and connecting to it... INFO: Options provided by the client:   Inherited 'common' options: isatty=1 terminal_columns=80 INFO: Reading rc options for 'run' from c:\users\shash\jax\.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Options provided by the client:   Inherited 'build' options: python_path=C:/Users/shash/.conda/envs/dljax/python.exe INFO: Reading rc options for 'run' from c:\users\shash\jax\.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false //xla/python:enable_tpu=false INFO: Reading rc options for 'run' from c:\users\shash\jax\.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=C:/Users/shash/.conda/envs/dljax/python.exe action_env=PYENV_ROOT python_path=C:/Users/shash/.conda/envs/dljax/python.exe action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 action_env CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 action_env TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 action_env TF_CUDA_VERSION=12.2 action_env TF_CUDNN_VERSION=8.9.4 config=mkl_open_source_only config=cuda INFO: Found applicable config definition build:short_logs in file c:\users\shash\jax\.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:mkl_open_source_only in file c:\users\shash\jax\.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file c:\users\shash\jax\.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,sm_80,compute_90 crosstool_top=//crosstool:toolchain //:enable_cuda //xla/python:enable_gpu=true //xla/python:jax_cuda_pip_rpaths=true define=xla_python_enable_gpu=true INFO: Found applicable config definition build:windows in file c:\users\shash\jax\.bazelrc: copt=/D_USE_MATH_DEFINES host_copt=/D_USE_MATH_DEFINES copt=DWIN32_LEAN_AND_MEAN host_copt=DWIN32_LEAN_AND_MEAN copt=DNOGDI host_copt=DNOGDI copt=/Zc:preprocessor cxxopt=/std:c++17 host_cxxopt=/std:c++17 linkopt=/DEBUG host_linkopt=/DEBUG linkopt=/OPT:REF host_linkopt=/OPT:REF linkopt=/OPT:ICF host_linkopt=/OPT:ICF incompatible_strict_action_env=trueDEBUG: C:/users/shash/_bazel_shash/b3x3rdg2/external/xla/third_party/repo.bzl:132:14: Warning: skipping import of repository 'tf_runtime' because it already exists. DEBUG: C:/users/shash/_bazel_shash/b3x3rdg2/external/xla/third_party/repo.bzl:132:14: Warning: skipping import of repository 'llvmraw' because it already exists. WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseilcpp/archive/b971ac5250ea8de900eae9f95e06548d14cd95fe.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/re2/archive/03da4fc0857c285e3a26782f6bc8931c4c950df4.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/triton/archive/cl547802044.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/5aa1adeb2a412d1aca8972385cd9b26b1a524052.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVIDIA/cudnnfrontend/archive/refs/tags/v0.9.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found INFO: Analyzed target //jaxlib/tools:build_wheel (220 packages loaded, 18304 targets configured). INFO: Found 1 target... ERROR: C:/users/shash/_bazel_shash/b3x3rdg2/external/flatbuffers/BUILD.bazel:84:10: Linking external/flatbuffers/flatc.exe [for tool] failed: (Exit 1120): link.exe failed: error executing command (from target //:flatc)   cd /d C:/users/shash/_bazel_shash/b3x3rdg2/execroot/__main__   SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt     SET LIB=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\ATLMFC\lib\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.22621.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\\lib\10.0.22621.0\\um\x64     SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\vcpkg     SET PWD=/proc/self/cwd     SET RUNFILES_MANIFEST_ONLY=1     SET TEMP=C:\Users\shash\AppData\Local\Temp     SET TMP=C:\Users\shash\AppData\Local\Temp   C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.37.32822\bin\HostX64\x64\link.exe out/x64_windowsoptexec50AE0418/bin/external/flatbuffers/flatc.exe2.params  Configuration: d960a019c29abfe37a18e15ad5a5dad5a3057c532cbeb0efbd45b2fd0b0808a3  Execution platform: //:platform Target //jaxlib/tools:build_wheel failed to build INFO: Elapsed time: 222.900s, Critical Path: 26.63s INFO: 782 processes: 430 internal, 352 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target b'LINK : warning LNK4001: no object files specified; libraries used\r\nLINK : error LNK2001: unresolved external symbol mainCRTStartup\r\nbazelout\\x64_windowsoptexec50AE0418\\bin\\external\\flatbuffers\\flatc.exe : fatal error LNK1120: 1 unresolved externals\r\n' Traceback (most recent call last):   File ""C:\Users\shash\jax\build\build.py"", line 559, in      main()   File ""C:\Users\shash\jax\build\build.py"", line 554, in main     shell(command)   File ""C:\Users\shash\jax\build\build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""C:\Users\shash\.conda\envs\dljax\lib\subprocess.py"", line 424, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""C:\Users\shash\.conda\envs\dljax\lib\subprocess.py"", line 528, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['C:\\bazel\\bazel.EXE', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=C:\\Users\\shash\\jax\\dist', 'cpu=AMD64']' returned nonzero exit status 1.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-08-18T16:28:52Z,bug build contributions welcome Windows,closed,0,6,https://github.com/jax-ml/jax/issues/17174,Windows builds are community supported. I'll note that jaxlib built fine in the most recent CI run from today: https://github.com/google/jax/actions/workflows/windows_ci.yml Can you try building from a checkout at github head?,"Hey there. Thank you very much for replying. Can you please tell me what is this flatc.exe error and how to correct it. On Fri, 18 Aug 2023, 23:04 Peter Hawkins, ***@***.***> wrote: > Windows builds are community supported. > > I'll note that jaxlib built fine in the most recent CI run from today: > https://github.com/google/jax/actions/workflows/windows_ci.yml > > Can you try building from a checkout at github head? > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","`flatc` is a binary built as part of the jaxlib build process. Did you try from github head, as I already suggested?","I also strongly recommend installing a binary release of JAX, rather than building it yourself. I'm not sure that CUDA support even builds on Windows right now, for what its worth. So it's CPUonly or nothing. Another option is to use the Windows Subsystem for Linux and install the Linux GPU build.","I'm sorry but I don't understand what is meant by checking out by github head.  Can you please explain On Fri, 18 Aug 2023, 23:16 Peter Hawkins, ***@***.***> wrote: > flatc is a binary built as part of the jaxlib build process. > > Did you try from github head, as I already suggested? > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Please use a binary release of jaxlib (`pip install jax[cpu]`). Hope that helps!
2001,"以下是一个github上的jax下的一个issue, 标题是(fail to download  got 404 code when to build JAX from source in Mac M1)， 内容是 ( Description TRy to build Jax from source in M1 with doc:  https://developer.apple.com/metal/jax/ I got warnings : ``` WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/e13d1b5227a77bb7becfd4c49a60720ecc33f870.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseilcpp/archive/b971ac5250ea8de900eae9f95e06548d14cd95fe.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/09c82d2dbefeae322010548ee2542c97364f3d32.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found Analyzing: target //build:build_wheel (201 packages loaded, 13579 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found ```  What jax/jaxlib version are you using? 0.4.10  Which accelerator(s) are you using? CPU  Additional system info Mac(m1)  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,fail to download  got 404 code when to build JAX from source in Mac M1," Description TRy to build Jax from source in M1 with doc:  https://developer.apple.com/metal/jax/ I got warnings : ``` WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/e13d1b5227a77bb7becfd4c49a60720ecc33f870.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseilcpp/archive/b971ac5250ea8de900eae9f95e06548d14cd95fe.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/09c82d2dbefeae322010548ee2542c97364f3d32.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found Analyzing: target //build:build_wheel (201 packages loaded, 13579 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found ```  What jax/jaxlib version are you using? 0.4.10  Which accelerator(s) are you using? CPU  Additional system info Mac(m1)  NVIDIA GPU info _No response_",2023-08-18T06:52:01Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/17167,"This is a warning, not an error: it simply means we didn't add some of those files to our mirror. However the mirror is only there for redundancy: if github is up, you don't need it. But sometimes github goes down :) Closing because it's fine to ignore the warnings for now. Hope that helps!"
379,"以下是一个github上的jax下的一个issue, 标题是(deprecate jax.numpy.issubsctype)， 内容是 (Part of CC(ENH: Reflect changes from numpy namespace refactor Part 3). This anticipates removal of this function in numpy 2.0: https://github.com/numpy/numpy/pull/24376)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,deprecate jax.numpy.issubsctype,Part of CC(ENH: Reflect changes from numpy namespace refactor Part 3). This anticipates removal of this function in numpy 2.0: https://github.com/numpy/numpy/pull/24376,2023-08-17T16:44:00Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17160
2769,"以下是一个github上的jax下的一个issue, 标题是(BFGS gets lost in Poisson regression example)， 内容是 ( Description I'm working on a Poisson regression example. I define the objective in JAX and then optimize it in two different ways: 1. With jax.scipy.optimize 2. With scipy.optimize I tried to keep the two setups as similar as possible. The JAX implementation seems to get stuck with a line search error (maybe related https://github.com/google/jax/issues/4594) and quite a large gradient. Scipy terminates successfully. ```python import numpy as np import scipy.optimize import jax import jax.numpy as jnp import jax.scipy.optimize jax.config.update(""jax_enable_x64"", True)  data generation rng = np.random.default_rng(27) T = 200 ts = np.linspace(0, 1, T+1)[:1] alpha = 0.1 beta = 1 rates = np.exp(alpha + beta * ts) counts = rng.poisson(lam=rates)  problem set up X = jnp.asarray(np.c_[np.ones_like(ts), ts]) y = jnp.asarray(counts) initial_params = jnp.zeros(X.shape[1]) def objective(params):     log_rate = X @ params     return jnp.sum(jnp.exp(log_rate)  y * log_rate)  optimize using jax print('jax.scipy.optimize results') print(jax.scipy.optimize.minimize(objective, initial_params, method='bfgs', options={'line_search_maxiter': 1000, 'maxiter': 1000}))  optimize using scipy def wrapped_objective(params):     params_jax = jnp.asarray(params)     return np.float64(objective(params_jax)) def wrapped_gradient(params):     params_jax = jnp.asarray(params)     return np.asarray(jax.grad(objective)(params_jax)) print('scipy.optimize results') print(scipy.optimize.minimize(wrapped_objective, np.asarray(initial_params), jac=wrapped_gradient, method='bfgs')) ``` Output ``` jax.scipy.optimize results OptimizeResults(x=Array([0.50331112, 0.32459955], dtype=float64), success=Array(False, dtype=bool), status=Array(3, dtype=int64, weak_type=True), fun=Array(108.62462671, dtype=float64), jac=Array([18.4706947 , 29.45663606], dtype=float64), hess_inv=Array([[ 0.23711728, 0.42412468],        [0.42412468,  0.77024769]], dtype=float64), nfev=Array(3, dtype=int64, weak_type=True), njev=Array(3, dtype=int64, weak_type=True), nit=Array(1, dtype=int64, weak_type=True)) scipy.optimize results   message: Optimization terminated successfully.   success: True    status: 0       fun: 102.37793361220376         x: [ 2.246e01  9.166e01]       nit: 8       jac: [2.120e06 1.385e06]  hess_inv: [[ 1.249e02 1.750e02]             [1.750e02  3.063e02]]      nfev: 12      njev: 12 ```  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info 3.10.12, MacOS 13.5  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,BFGS gets lost in Poisson regression example," Description I'm working on a Poisson regression example. I define the objective in JAX and then optimize it in two different ways: 1. With jax.scipy.optimize 2. With scipy.optimize I tried to keep the two setups as similar as possible. The JAX implementation seems to get stuck with a line search error (maybe related https://github.com/google/jax/issues/4594) and quite a large gradient. Scipy terminates successfully. ```python import numpy as np import scipy.optimize import jax import jax.numpy as jnp import jax.scipy.optimize jax.config.update(""jax_enable_x64"", True)  data generation rng = np.random.default_rng(27) T = 200 ts = np.linspace(0, 1, T+1)[:1] alpha = 0.1 beta = 1 rates = np.exp(alpha + beta * ts) counts = rng.poisson(lam=rates)  problem set up X = jnp.asarray(np.c_[np.ones_like(ts), ts]) y = jnp.asarray(counts) initial_params = jnp.zeros(X.shape[1]) def objective(params):     log_rate = X @ params     return jnp.sum(jnp.exp(log_rate)  y * log_rate)  optimize using jax print('jax.scipy.optimize results') print(jax.scipy.optimize.minimize(objective, initial_params, method='bfgs', options={'line_search_maxiter': 1000, 'maxiter': 1000}))  optimize using scipy def wrapped_objective(params):     params_jax = jnp.asarray(params)     return np.float64(objective(params_jax)) def wrapped_gradient(params):     params_jax = jnp.asarray(params)     return np.asarray(jax.grad(objective)(params_jax)) print('scipy.optimize results') print(scipy.optimize.minimize(wrapped_objective, np.asarray(initial_params), jac=wrapped_gradient, method='bfgs')) ``` Output ``` jax.scipy.optimize results OptimizeResults(x=Array([0.50331112, 0.32459955], dtype=float64), success=Array(False, dtype=bool), status=Array(3, dtype=int64, weak_type=True), fun=Array(108.62462671, dtype=float64), jac=Array([18.4706947 , 29.45663606], dtype=float64), hess_inv=Array([[ 0.23711728, 0.42412468],        [0.42412468,  0.77024769]], dtype=float64), nfev=Array(3, dtype=int64, weak_type=True), njev=Array(3, dtype=int64, weak_type=True), nit=Array(1, dtype=int64, weak_type=True)) scipy.optimize results   message: Optimization terminated successfully.   success: True    status: 0       fun: 102.37793361220376         x: [ 2.246e01  9.166e01]       nit: 8       jac: [2.120e06 1.385e06]  hess_inv: [[ 1.249e02 1.750e02]             [1.750e02  3.063e02]]      nfev: 12      njev: 12 ```  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info 3.10.12, MacOS 13.5  NVIDIA GPU info _No response_",2023-08-17T15:20:18Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17159,"Thanks for the report. We're not doing much maintenance on `jax.scipy.optimize`, and longterm we're planning to remove it. For optimization in JAX, I'd suggest checking out the jaxopt  package.","Ah thanks, I hadn't heard of that package before.","jaxopt is a dead project now too, and optax doesn't have this capability... Anyway if jax.scipy.optimize.minimize is known to be broken, the fix in https://github.com/google/jax/issues/16236 will not be merged, and it will someday be removed  maybe the documentation could be updated to say so?"
1525,"以下是一个github上的jax下的一个issue, 标题是(type <class 'jaxlib.xla_extension.ArrayImpl'>is non-hashable when I use a class member function, while there is no problem when I use a general function.)， 内容是 ( Description Hi, I am confused about using . I would not like the variable neg was traced, so I added the decorator  as the docs.  There was no problem when I ran the following code: ``` from functools import partial import jax import jax.numpy as jnp (jax.jit, static_argnums=(1,)) def f(x, neg):   return x if neg else x arr = jnp.array([1,2]) result = f(arr, True) ``` However, when I added the function f to a class, **ValueError** appeared. ``` from functools import partial import jax import jax.numpy as jnp class Test:   (jax.jit, static_argnums=(1,))   def f(self, x, neg):     return x if neg else x t = Test() arr = jnp.array([1,2]) result = t.f(arr, True) ``` > ValueError: Nonhashable static arguments are not supported, as this can lead to unexpected cachemisses. Static argument (index 1) of type  for function f is nonhashable. I am new to JAX. I want to know why the problem appears and how can I solve it? Are there any methods that gracefully convert **ArrayImpl**  to other hashable types?  What jax/jaxlib version are you using? jax 0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python: 3.8, OS: WSL Ubuntu 22.04.2  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"type <class 'jaxlib.xla_extension.ArrayImpl'>is non-hashable when I use a class member function, while there is no problem when I use a general function."," Description Hi, I am confused about using . I would not like the variable neg was traced, so I added the decorator  as the docs.  There was no problem when I ran the following code: ``` from functools import partial import jax import jax.numpy as jnp (jax.jit, static_argnums=(1,)) def f(x, neg):   return x if neg else x arr = jnp.array([1,2]) result = f(arr, True) ``` However, when I added the function f to a class, **ValueError** appeared. ``` from functools import partial import jax import jax.numpy as jnp class Test:   (jax.jit, static_argnums=(1,))   def f(self, x, neg):     return x if neg else x t = Test() arr = jnp.array([1,2]) result = t.f(arr, True) ``` > ValueError: Nonhashable static arguments are not supported, as this can lead to unexpected cachemisses. Static argument (index 1) of type  for function f is nonhashable. I am new to JAX. I want to know why the problem appears and how can I solve it? Are there any methods that gracefully convert **ArrayImpl**  to other hashable types?  What jax/jaxlib version are you using? jax 0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python: 3.8, OS: WSL Ubuntu 22.04.2  NVIDIA GPU info _No response_",2023-08-17T15:17:32Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/17158,"The reason it differs is because `self` counts as an argument – so you'd have to use `static_argnums=(2,)` for the class method if you want `neg` to be static. However, that doesn't tell the whole story, because you also need to tell JAX how to appropriately handle `self` in this case. That's a frequently asked question with a notsosimple answer that we cover here: https://jax.readthedocs.io/en/latest/faq.htmlhowtousejitwithmethods","> The reason it differs is because `self` counts as an argument – so you'd have to use `static_argnums=(2,)` for the class method if you want `neg` to be static. >  > However, that doesn't tell the whole story, because you also need to tell JAX how to appropriately handle `self` in this case. That's a frequently asked question with a notsosimple answer that we cover here: https://jax.readthedocs.io/en/latest/faq.htmlhowtousejitwithmethods Thanks  !  I carefully read how to use jax in class and it helps me a lot to understand jax. But I still want to solve the problem caused by static arguments not supporting nonhashable type.  I have a program where the intermediate result computed by some arrays is **type ArrayImpl**.  I would like to use the intermediate result as the condition (if else). So I must use   to ensure the intermediate result is not traced. This creates a contradiction. After I go through other issues, I still can't come up with a appropriate solution. Do you have any ideas? ","Within JIT, you cannot use a JAX array as a static condition. I'd suggest taking a look at JAX Sharp Bits: Control Flow, and in particular the subsection Python Control Flow + JIT. If you want code to be conditioned on a dynamic value, you should use `lax.cond` rather than `if`/`else`.","> Within JIT, you cannot use a JAX array as a static condition. I'd suggest taking a look at JAX Sharp Bits: Control Flow, and in particular the subsection Python Control Flow + JIT. >  > If you want code to be conditioned on a dynamic value, you should use `lax.cond` rather than `if`/`else`. Thanks for your suggestions  .I tried another way to achieve my algorithm and it ran correctly.  It's very kind of you. I really appreciate it. "
419,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Relax the limit on the number of error inputs for shape assertions)， 内容是 (Lift the limit from 4 to 32 to follow the change in tf.XlaCallModule of this limit. Currently, the error message formatter needs at most 6 error message inputs.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Relax the limit on the number of error inputs for shape assertions,"Lift the limit from 4 to 32 to follow the change in tf.XlaCallModule of this limit. Currently, the error message formatter needs at most 6 error message inputs.",2023-08-17T09:21:03Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17154
448,"以下是一个github上的jax下的一个issue, 标题是(Create lax.polygamma with native HLO lowering)， 内容是 (~Trying this out... not sure whether `polygamma` is implemented on all backends.~ Tests pass on CPU, GPU & TPU... let's ship it! Inspired by CC(Please consider implementing differentiation for the Hessian of gamma variates))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Create lax.polygamma with native HLO lowering,"~Trying this out... not sure whether `polygamma` is implemented on all backends.~ Tests pass on CPU, GPU & TPU... let's ship it! Inspired by CC(Please consider implementing differentiation for the Hessian of gamma variates)",2023-08-16T17:51:51Z,pull ready,closed,1,5,https://github.com/jax-ml/jax/issues/17142,"Wow, thanks for the quick solution! Will this solve CC(Please consider implementing differentiation for the Hessian of gamma variates)?  I'm trying it like this: ```sh ❯ which pipigp pipigp () { 	noglob pip install ""git+https://github.com/$1.git/pull/$2/head"" } ❯ pipigp google/jax 17142 Collecting git+https://github.com/google/jax.git/pull/17142/head ... ❯ export JAX_PLATFORM_NAME=cpu ❯ cat a.py from jax import hessian from jax.random import PRNGKey, gamma key = PRNGKey(1) hessian_sample = hessian(gamma, argnums=(1,))  NotImplementedError: Differentiation rule for 'random_gamma_grad' not implemented hessian_sample(key, 1.3) ❯ py a.py ... jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Differentiation rule for 'random_gamma_grad' not implemented ❯ export JAX_PLATFORM_NAME=gpu ❯ py a.py ... jax._src.source_info_util.JaxStackTraceBeforeTransformation: NotImplementedError: Differentiation rule for 'random_gamma_grad' not implemented ```",I guess the same thing would need to be added for `gamma_grad` primitive?,"No, this won't solve CC(Please consider implementing differentiation for the Hessian of gamma variates). The issue there is that there is no autodiff rule for `gamma_grad`.","Okay, but it's possible to write one?  Should I invest some time in doing what you did for the `gamma_grad` primitive? ",See https://github.com/google/jax/issues/16076issuecomment1681273130 – let's keep discussions of CC(Please consider implementing differentiation for the Hessian of gamma variates) in CC(Please consider implementing differentiation for the Hessian of gamma variates)
1375,"以下是一个github上的jax下的一个issue, 标题是(Ensure that CompileOptions serializes deterministically.)， 内容是 (Ensure that CompileOptions serializes deterministically. CompileOptions has two serialization mechanisms: Py pickle and SerializeAsString. Neither mechanism serializes deterministically. Deterministic serialization (also called idempotent serialization or inorder serialization) ensures that a given structure serializes to the same string repeatedly. Both these mechanisms serialize by first generating the proto and then serializing it. There are three points to note: . Deterministic serialization will yield the same result   even if proto map fields are in a different order. Thus   map({""1"": 1, ""2"": 2}) and map({""2"": 2, ""1"": 1}) will   serialize the same. . Deterministic serialization does not yield the same   result for repeated fields that are out of order. Thus,   for message Foo { repeated string s = 1; },   Foo{s: ""1"", s: ""2""} will not result in the same   serialization as Foo{s: ""2"", s: ""1""}. . Deterministic serialization applies only in the context   of a given binary. It does not apply across releases. Testing: the original serialization code with the new unit test fails as expected while the revised code does not.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Ensure that CompileOptions serializes deterministically.,"Ensure that CompileOptions serializes deterministically. CompileOptions has two serialization mechanisms: Py pickle and SerializeAsString. Neither mechanism serializes deterministically. Deterministic serialization (also called idempotent serialization or inorder serialization) ensures that a given structure serializes to the same string repeatedly. Both these mechanisms serialize by first generating the proto and then serializing it. There are three points to note: . Deterministic serialization will yield the same result   even if proto map fields are in a different order. Thus   map({""1"": 1, ""2"": 2}) and map({""2"": 2, ""1"": 1}) will   serialize the same. . Deterministic serialization does not yield the same   result for repeated fields that are out of order. Thus,   for message Foo { repeated string s = 1; },   Foo{s: ""1"", s: ""2""} will not result in the same   serialization as Foo{s: ""2"", s: ""1""}. . Deterministic serialization applies only in the context   of a given binary. It does not apply across releases. Testing: the original serialization code with the new unit test fails as expected while the revised code does not.",2023-08-16T00:34:28Z,,closed,0,1,https://github.com/jax-ml/jax/issues/17133,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
301,"以下是一个github上的jax下的一个issue, 标题是(Re-enable mixed-precision testDot on TPU)， 内容是 (Reenable mixedprecision testDot on TPU The underlying issue has been fixed in XLA)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Re-enable mixed-precision testDot on TPU,Reenable mixedprecision testDot on TPU The underlying issue has been fixed in XLA,2023-08-15T22:56:05Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17131
562,"以下是一个github上的jax下的一个issue, 标题是(Remove the `canonicalize_dtypes` argument from mlir.ir_constant(s).)， 内容是 (Remove the `canonicalize_dtypes` argument from mlir.ir_constant(s). Instead, force the caller to explicitly canonicalize the argument if that's what they want. The current behavior (canonicalize by default) is not the behavior we want to encourage: we want to canonicalize exactly where we need to and nowhere else.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Remove the `canonicalize_dtypes` argument from mlir.ir_constant(s).,"Remove the `canonicalize_dtypes` argument from mlir.ir_constant(s). Instead, force the caller to explicitly canonicalize the argument if that's what they want. The current behavior (canonicalize by default) is not the behavior we want to encourage: we want to canonicalize exactly where we need to and nowhere else.",2023-08-15T21:44:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17130
1839,"以下是一个github上的jax下的一个issue, 标题是(Same functions but not same result)， 内容是 ( Description  Different results in Jax and torch and Numpy on CPU/TPU and GPU this is the function that I'm using to find are two arrays different or no ```python def is_same(t1, t2):     for t1_, t2_ in zip(t1.reshape(1), t2.reshape(1)):         if not t1_ == t2_:             return False     return True     ``` and this is the function that I'm trying to recreate from Torch (RoPE) and the part that my code is getting error or different results in calculating sin and cos and freq if you do the same as what iv done in jax for original numpy you wont get any error or False results  ```python base_torch = 1. / (10000. ** (torch.arange(0, 64, 2, dtype=torch.float32) / 64)) base_jax = 1. / (10000. ** (jnp.arange(0, 64, 2, dtype=jnp.float32) / 64)) t_t = torch.arange(64) j_t = jnp.arange(64) einsum_torch = torch.einsum('i,j>ij', t_t, base_torch) einsum_jax = jnp.einsum('i,j>ij', j_t, base_jax)  is_same(einsum_jax, einsum_torch.detach().numpy()) > True freq_torch = torch.cat([einsum_torch, einsum_torch], dim=1) freq_jax = jnp.concatenate([einsum_jax, einsum_jax], axis=1)  is_same(freq_jax, freq_torch.detach().numpy()) > True sin_jax = jnp.sin(freq_jax) cos_jax = jnp.cos(freq_jax) sin_torch = torch.sin(freq_torch) cos_torch = torch.cos(freq_torch)  is_same(sin_jax, sin_torch.detach().numpy()) > False  is_same(cos_jax, cos_torch.detach().numpy()) > False ```  What jax/jaxlib version are you using? jax/jaxlib v0.4.13  Which accelerator(s) are you using? CPU/GPU/TPU  Additional system info Linux Ubuntu 22.04 LTS  NVIDIA GPU info GPU = RTX 3080 Cuda 11.8 driver 530 CPU = Intel® Core™ i79700K TPU = Kaggle TPUs)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Same functions but not same result," Description  Different results in Jax and torch and Numpy on CPU/TPU and GPU this is the function that I'm using to find are two arrays different or no ```python def is_same(t1, t2):     for t1_, t2_ in zip(t1.reshape(1), t2.reshape(1)):         if not t1_ == t2_:             return False     return True     ``` and this is the function that I'm trying to recreate from Torch (RoPE) and the part that my code is getting error or different results in calculating sin and cos and freq if you do the same as what iv done in jax for original numpy you wont get any error or False results  ```python base_torch = 1. / (10000. ** (torch.arange(0, 64, 2, dtype=torch.float32) / 64)) base_jax = 1. / (10000. ** (jnp.arange(0, 64, 2, dtype=jnp.float32) / 64)) t_t = torch.arange(64) j_t = jnp.arange(64) einsum_torch = torch.einsum('i,j>ij', t_t, base_torch) einsum_jax = jnp.einsum('i,j>ij', j_t, base_jax)  is_same(einsum_jax, einsum_torch.detach().numpy()) > True freq_torch = torch.cat([einsum_torch, einsum_torch], dim=1) freq_jax = jnp.concatenate([einsum_jax, einsum_jax], axis=1)  is_same(freq_jax, freq_torch.detach().numpy()) > True sin_jax = jnp.sin(freq_jax) cos_jax = jnp.cos(freq_jax) sin_torch = torch.sin(freq_torch) cos_torch = torch.cos(freq_torch)  is_same(sin_jax, sin_torch.detach().numpy()) > False  is_same(cos_jax, cos_torch.detach().numpy()) > False ```  What jax/jaxlib version are you using? jax/jaxlib v0.4.13  Which accelerator(s) are you using? CPU/GPU/TPU  Additional system info Linux Ubuntu 22.04 LTS  NVIDIA GPU info GPU = RTX 3080 Cuda 11.8 driver 530 CPU = Intel® Core™ i79700K TPU = Kaggle TPUs",2023-08-15T11:17:39Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17121,"Thanks for the question! I think this is working as expected. In general, using exact equality checks for floating point computations is an antipattern: floating point operations are inexact representations of real numbers, and so will always have some degree of rounding error. This is not a JAXspecific thing: this is true of any library that uses floating point (which includes jax, torch, numpy, and almost all other modern computing frameworks) To compare equivalent floating point outputs, you need to account for this computation error. One function you could use for this is `numpy.allclose`, and indeed if you replace `is_same` with `numpy.allclose`, you'll see that the results in your code match. For a good practical read on rounding errors in floating point math, I'd check out this StackOverflow answer: Is Floating Point Math Broken?.",thanks<3
2495,"以下是一个github上的jax下的一个issue, 标题是(`where` related gradient bug in jax.nn.softmax and jax.nn.log_softmax (unprotected use of jnp.exp?))， 内容是 ( Description I see `nan`s in the gradient of `jax.nn.softmax` and `jax.nn.log_softmax`, specifically when I use `where`, and the logits that are filtered out by the `where` condition are large, but not large elsewhere in the logits tensor: ```python def fn(x, w):     s = jax.nn.softmax(x, axis=0, where=w, initial=0.0)     return jnp.sum(s, where=w, initial=0.0) def lfn(x, w):     s = jax.nn.log_softmax(x, axis=0, where=w, initial=0.0)     return jnp.sum(s, where=w, initial=0.0) x = jnp.array([1.0, 1.0, 100.0])  w = jnp.array([True, True, False]) jax.grad(fn)(x, w) Array([nan, nan, nan], dtype=float32) jax.grad(lfn)(x, w) Array([ 0.,  0., nan], dtype=float32) ``` I believe there is an easy fix, and the bug is due to the fact that `jnp.exp` is being called over the values `x  x_max`, which may overflow since `x_max` is only taken over the values passing the `where` condition.  This is already warned against in the documentation for `jnp.where`: https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere ```python def _softmax(     x,     axis: Optional[Union[int, tuple[int, ...]]] = 1,     where: Optional[Array] = None,     initial: Optional[Array] = None) > Array:  if where is used, x_max may not reflect the global maximum across axis   x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)  then, here, exp may yield inf   unnormalized = jnp.exp(x  x_max)   result = unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)   if where is not None:     result = jnp.where(where, result, 0)   return result ``` Instead, I tried this which provides proper gradients: ```python def softmax(x, axis, where=None, initial=None):     x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)     shifted = x  x_max     if where is not None:         shifted = jnp.where(where, shifted, 1000.0)     unnormalized = jnp.exp(shifted)     result = unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)     if where is not None:         result = jnp.where(where, result, 0.0) ```  What jax/jaxlib version are you using? jax 0.4.2  Which accelerator(s) are you using? GPU  Additional system info Ubuntu  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`where` related gradient bug in jax.nn.softmax and jax.nn.log_softmax (unprotected use of jnp.exp?)," Description I see `nan`s in the gradient of `jax.nn.softmax` and `jax.nn.log_softmax`, specifically when I use `where`, and the logits that are filtered out by the `where` condition are large, but not large elsewhere in the logits tensor: ```python def fn(x, w):     s = jax.nn.softmax(x, axis=0, where=w, initial=0.0)     return jnp.sum(s, where=w, initial=0.0) def lfn(x, w):     s = jax.nn.log_softmax(x, axis=0, where=w, initial=0.0)     return jnp.sum(s, where=w, initial=0.0) x = jnp.array([1.0, 1.0, 100.0])  w = jnp.array([True, True, False]) jax.grad(fn)(x, w) Array([nan, nan, nan], dtype=float32) jax.grad(lfn)(x, w) Array([ 0.,  0., nan], dtype=float32) ``` I believe there is an easy fix, and the bug is due to the fact that `jnp.exp` is being called over the values `x  x_max`, which may overflow since `x_max` is only taken over the values passing the `where` condition.  This is already warned against in the documentation for `jnp.where`: https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere ```python def _softmax(     x,     axis: Optional[Union[int, tuple[int, ...]]] = 1,     where: Optional[Array] = None,     initial: Optional[Array] = None) > Array:  if where is used, x_max may not reflect the global maximum across axis   x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)  then, here, exp may yield inf   unnormalized = jnp.exp(x  x_max)   result = unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)   if where is not None:     result = jnp.where(where, result, 0)   return result ``` Instead, I tried this which provides proper gradients: ```python def softmax(x, axis, where=None, initial=None):     x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)     shifted = x  x_max     if where is not None:         shifted = jnp.where(where, shifted, 1000.0)     unnormalized = jnp.exp(shifted)     result = unnormalized / jnp.sum(unnormalized, axis, where=where, keepdims=True)     if where is not None:         result = jnp.where(where, result, 0.0) ```  What jax/jaxlib version are you using? jax 0.4.2  Which accelerator(s) are you using? GPU  Additional system info Ubuntu  NVIDIA GPU info _No response_",2023-08-14T23:40:28Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17117,"Hi   Looks like this issue has been resolved by the PR CC(nn.softmax: use doublewhere when where is specified). I tried to execute the mentioned code using JAX 0.4.26 and it now produces the result as expected: ```python import jax from jax import numpy as jnp def fn(x, w):     s = jax.nn.softmax(x, axis=0, where=w, initial=0.0)     return jnp.sum(s, where=w, initial=0.0) def lfn(x, w):     s = jax.nn.log_softmax(x, axis=0, where=w, initial=0.0)     return jnp.sum(s, where=w, initial=0.0) x = jnp.array([1.0, 1.0, 100.0])  w = jnp.array([True, True, False]) jax.grad(fn)(x, w) Array([0., 0., 0.], dtype=float32) jax.grad(lfn)(x, w) Array([0., 0., 0.], dtype=float32) ``` Please find the gist for reference. Thank you.",Thanks for following up!
494,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Disable some graph serialization tests on GPU)， 内容是 ([jax2tf] Disable some graph serialization tests on GPU We recently increased the test coverage of testing for dot_general with different dtype for lhs and rhs. Some of the new combinations of dtypes are not supported by XLA:GPU, and we disable those tests now.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Disable some graph serialization tests on GPU,"[jax2tf] Disable some graph serialization tests on GPU We recently increased the test coverage of testing for dot_general with different dtype for lhs and rhs. Some of the new combinations of dtypes are not supported by XLA:GPU, and we disable those tests now.",2023-08-10T05:56:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17056
14642,"以下是一个github上的jax下的一个issue, 标题是(Compilation Error trying to Build jaxlib)， 内容是 ( Description I am trying to build jaxlib for metal, following these instructions. https://developer.apple.com/metal/jax/ I'm on MacOS 13.5 with an AMD Radeon Pro 5700 XT GPU ``` % llvmconfig version 14.0.6 (jaxmetal) (base) davidlaxer2 jax % whereis llvmconfig llvmconfig: /opt/local/bin/llvmconfig (jaxmetal) (base) davidlaxer2 jax % /usr/local/bin/llvmconfig version 15.0.0git (jaxmetal) (base) davidlaxer2 jax % python build/build.py bazel_options=//xla/python:enable_tpu=true      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: ./bazel5.1.1darwinx86_64 Bazel version: 5.1.1 Python binary path: /Users/davidlaxer/jaxmetal/bin/python Python version: 3.8 NumPy version: 1.24.4 MKLDNN enabled: yes Target CPU: x86_64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Plugin device enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel5.1.1darwinx86_64 run verbose_failures=true :build_wheel  output_path=/Users/davidlaxer/PySR/jax/dist cpu=x86_64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/davidlaxer/PySR/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/davidlaxer/PySR/jax/.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false //xla/python:enable_tpu=false //xla/python:enable_plugin_device=false INFO: Reading rc options for 'run' from /Users/davidlaxer/PySR/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/davidlaxer/jaxmetal/bin/python action_env=PYENV_ROOT python_path=/Users/davidlaxer/jaxmetal/bin/python distinct_host_configuration=false //xla/python:enable_tpu=true config=avx_posix config=mkl_open_source_only INFO: Found applicable config definition build:short_logs in file /Users/davidlaxer/PySR/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /Users/davidlaxer/PySR/jax/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /Users/davidlaxer/PySR/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/davidlaxer/PySR/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/davidlaxer/PySR/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 DEBUG: /private/var/tmp/_bazel_davidlaxer/d468a538d443dde81b2bc827e6b05dcd/external/xla/third_party/repo.bzl:132:14:  Warning: skipping import of repository 'tf_runtime' because it already exists. DEBUG: /private/var/tmp/_bazel_davidlaxer/d468a538d443dde81b2bc827e6b05dcd/external/xla/third_party/repo.bzl:132:14:  Warning: skipping import of repository 'llvmraw' because it already exists. WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/e13d1b5227a77bb7becfd4c49a60720ecc33f870.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading:  Loading: 0 packages loaded Analyzing: target //build:build_wheel (0 packages loaded, 0 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseilcpp/archive/b971ac5250ea8de900eae9f95e06548d14cd95fe.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/09c82d2dbefeae322010548ee2542c97364f3d32.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found INFO: Analyzed target //build:build_wheel (0 packages loaded, 0 targets configured). INFO: Found 1 target... [1 / 4] [Prepa] BazelWorkspaceStatusAction stablestatus.txt [12 / 1,782] Compiling llvm/utils/TableGen/CodeGenIntrinsics.cpp; 1s local ... (16 actions, 15 running) ERROR: /private/var/tmp/_bazel_davidlaxer/d468a538d443dde81b2bc827e6b05dcd/external/llvmproject/llvm/BUILD.bazel:583:10: Compiling llvm/utils/TableGen/Attributes.cpp failed: (Exit 1): cc_wrapper.sh failed: error executing command    (cd /private/var/tmp/_bazel_davidlaxer/d468a538d443dde81b2bc827e6b05dcd/execroot/__main__ && \   exec env  \     PATH='/Users/davidlaxer/.opam/_coqplatform_.2021.02.1/bin:/Users/davidlaxer/jaxmetal/bin:/Users/davidlaxer/.juliaup/bin:/Users/davidlaxer/.cabal/bin:/Users/davidlaxer/.ghcup/bin:/Users/davidlaxer/anaconda3/bin:/Users/davidlaxer/anaconda3/condabin:/opt/local/bin:/opt/local/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet:~/.dotnet/tools:/Library/Apple/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/davidlaxer/.cargo/bin:/Users/jetbrains/.local/bin' \     PWD=/proc/self/cwd \   external/local_config_cc/cc_wrapper.sh U_FORTIFY_SOURCE fstackprotector Wall Wthreadsafety Wselfassign Wunusedbutsetparameter Wnofreenonheapobject fcolordiagnostics fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections fdatasections 'std=c++0x' MD MF bazelout/darwinopt/bin/external/llvmproject/llvm/_objs/llvmmintblgen/Attributes.d 'frandomseed=bazelout/darwinopt/bin/external/llvmproject/llvm/_objs/llvmmintblgen/Attributes.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' 'DHAVE_MACH_MACH_H=1' 'DHAVE_MALLOC_MALLOC_H=1' 'DHAVE_MALLOC_ZONE_STATISTICS=1' 'DHAVE_PROC_PID_RUSAGE=1' 'DHAVE_UNW_ADD_DYNAMIC_FDE=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknowndarwin""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknowndarwin""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS 'DBLAKE3_USE_NEON=0' DBLAKE3_NO_AVX2 DBLAKE3_NO_AVX512 DBLAKE3_NO_SSE2 DBLAKE3_NO_SSE41 iquote external/llvmproject iquote bazelout/darwinopt/bin/external/llvmproject iquote external/llvm_terminfo iquote bazelout/darwinopt/bin/external/llvm_terminfo iquote external/llvm_zlib iquote bazelout/darwinopt/bin/external/llvm_zlib iquote external/bazel_tools iquote bazelout/darwinopt/bin/external/bazel_tools isystem external/llvmproject/llvm/include isystem bazelout/darwinopt/bin/external/llvmproject/llvm/include 'fvisibility=hidden' Wnosigncompare 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' nocanonicalprefixes Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' 'fmodulename=project//llvm:llvmmintblgen' 'fmodulemapfile=bazelout/darwinopt/bin/external/llvmproject/llvm/llvmmintblgen.cppmap' fmodulesstrictdecluse Wprivateheader 'fmodulemapfile=external/local_config_cc/module.modulemap' 'fmodulemapfile=bazelout/darwinopt/bin/external/llvmproject/llvm/Support.cppmap' 'fmodulemapfile=bazelout/darwinopt/bin/external/llvmproject/llvm/TableGen.cppmap' 'fmodulemapfile=bazelout/darwinopt/bin/external/llvmproject/llvm/config.cppmap' 'fmodulemapfile=bazelout/darwinopt/bin/external/bazel_tools/tools/cpp/malloc.cppmap' c external/llvmproject/llvm/utils/TableGen/Attributes.cpp o bazelout/darwinopt/bin/external/llvmproject/llvm/_objs/llvmmintblgen/Attributes.o)  Configuration: 453baf4879e102ee5df052ec6087622b871704c7bccc3f61df28e0cf20545b9f  Execution platform: //:platform In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:21:9: warning: 'LLVM_DEFAULT_TARGET_TRIPLE' macro redefined [Wmacroredefined] define LLVM_DEFAULT_TARGET_TRIPLE ""x86_64appledarwin22.3.0""         ^ :34:9: note: previous definition is here define LLVM_DEFAULT_TARGET_TRIPLE ""x86_64unknowndarwin""         ^ In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:30:9: warning: 'LLVM_HOST_TRIPLE' macro redefined [Wmacroredefined] define LLVM_HOST_TRIPLE ""x86_64appledarwin22.3.0""         ^ :33:9: note: previous definition is here define LLVM_HOST_TRIPLE ""x86_64unknowndarwin""         ^ In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:33:9: warning: 'LLVM_NATIVE_ARCH' macro redefined [Wmacroredefined] define LLVM_NATIVE_ARCH X86         ^ :25:9: note: previous definition is here define LLVM_NATIVE_ARCH ""X86""         ^ In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:69:9: warning: 'LLVM_VERSION_MAJOR' macro redefined [Wmacroredefined] define LLVM_VERSION_MAJOR 15         ^ :35:9: note: previous definition is here define LLVM_VERSION_MAJOR 17         ^ In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:78:9: warning: 'LLVM_VERSION_STRING' macro redefined [Wmacroredefined] define LLVM_VERSION_STRING ""15.0.0git""         ^ :38:9: note: previous definition is here define LLVM_VERSION_STRING ""17.0.0git""         ^ external/llvmproject/llvm/utils/TableGen/Attributes.cpp:134:8: error: use of undeclared identifier 'TableGen' static TableGen::Emitter::OptClass X(""genattrs"",        ^ 5 warnings and 1 error generated. Target //build:build_wheel failed to build INFO: Elapsed time: 2.317s, Critical Path: 1.64s INFO: 34 processes: 23 internal, 11 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully b'' Traceback (most recent call last):   File ""build/build.py"", line 564, in      main()   File ""build/build.py"", line 559, in main     shell(command)   File ""build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/Users/davidlaxer/anaconda3/lib/python3.8/subprocess.py"", line 415, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/Users/davidlaxer/anaconda3/lib/python3.8/subprocess.py"", line 516, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.1darwinx86_64', 'run', 'verbose_failures=true', ':build_wheel', '', 'output_path=/Users/davidlaxer/PySR/jax/dist', 'cpu=x86_64']' returned nonzero exit status 1. ```  What jax/jaxlib version are you using? jaxlibv0.4.10  Which accelerator(s) are you using? MPS AMD GPU  Additional system info Python 3.8, MacOS 13.5  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Compilation Error trying to Build jaxlib," Description I am trying to build jaxlib for metal, following these instructions. https://developer.apple.com/metal/jax/ I'm on MacOS 13.5 with an AMD Radeon Pro 5700 XT GPU ``` % llvmconfig version 14.0.6 (jaxmetal) (base) davidlaxer2 jax % whereis llvmconfig llvmconfig: /opt/local/bin/llvmconfig (jaxmetal) (base) davidlaxer2 jax % /usr/local/bin/llvmconfig version 15.0.0git (jaxmetal) (base) davidlaxer2 jax % python build/build.py bazel_options=//xla/python:enable_tpu=true      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: ./bazel5.1.1darwinx86_64 Bazel version: 5.1.1 Python binary path: /Users/davidlaxer/jaxmetal/bin/python Python version: 3.8 NumPy version: 1.24.4 MKLDNN enabled: yes Target CPU: x86_64 Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Plugin device enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel5.1.1darwinx86_64 run verbose_failures=true :build_wheel  output_path=/Users/davidlaxer/PySR/jax/dist cpu=x86_64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/davidlaxer/PySR/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/davidlaxer/PySR/jax/.bazelrc:   Inherited 'build' options: nocheck_visibility apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //xla/python:enable_gpu=false //xla/python:enable_tpu=false //xla/python:enable_plugin_device=false INFO: Reading rc options for 'run' from /Users/davidlaxer/PySR/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/Users/davidlaxer/jaxmetal/bin/python action_env=PYENV_ROOT python_path=/Users/davidlaxer/jaxmetal/bin/python distinct_host_configuration=false //xla/python:enable_tpu=true config=avx_posix config=mkl_open_source_only INFO: Found applicable config definition build:short_logs in file /Users/davidlaxer/PySR/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /Users/davidlaxer/PySR/jax/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /Users/davidlaxer/PySR/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/davidlaxer/PySR/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/davidlaxer/PySR/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 DEBUG: /private/var/tmp/_bazel_davidlaxer/d468a538d443dde81b2bc827e6b05dcd/external/xla/third_party/repo.bzl:132:14:  Warning: skipping import of repository 'tf_runtime' because it already exists. DEBUG: /private/var/tmp/_bazel_davidlaxer/d468a538d443dde81b2bc827e6b05dcd/external/xla/third_party/repo.bzl:132:14:  Warning: skipping import of repository 'llvmraw' because it already exists. WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/e13d1b5227a77bb7becfd4c49a60720ecc33f870.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found Loading:  Loading: 0 packages loaded Analyzing: target //build:build_wheel (0 packages loaded, 0 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseilcpp/archive/b971ac5250ea8de900eae9f95e06548d14cd95fe.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/09c82d2dbefeae322010548ee2542c97364f3d32.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found INFO: Analyzed target //build:build_wheel (0 packages loaded, 0 targets configured). INFO: Found 1 target... [1 / 4] [Prepa] BazelWorkspaceStatusAction stablestatus.txt [12 / 1,782] Compiling llvm/utils/TableGen/CodeGenIntrinsics.cpp; 1s local ... (16 actions, 15 running) ERROR: /private/var/tmp/_bazel_davidlaxer/d468a538d443dde81b2bc827e6b05dcd/external/llvmproject/llvm/BUILD.bazel:583:10: Compiling llvm/utils/TableGen/Attributes.cpp failed: (Exit 1): cc_wrapper.sh failed: error executing command    (cd /private/var/tmp/_bazel_davidlaxer/d468a538d443dde81b2bc827e6b05dcd/execroot/__main__ && \   exec env  \     PATH='/Users/davidlaxer/.opam/_coqplatform_.2021.02.1/bin:/Users/davidlaxer/jaxmetal/bin:/Users/davidlaxer/.juliaup/bin:/Users/davidlaxer/.cabal/bin:/Users/davidlaxer/.ghcup/bin:/Users/davidlaxer/anaconda3/bin:/Users/davidlaxer/anaconda3/condabin:/opt/local/bin:/opt/local/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet:~/.dotnet/tools:/Library/Apple/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/davidlaxer/.cargo/bin:/Users/jetbrains/.local/bin' \     PWD=/proc/self/cwd \   external/local_config_cc/cc_wrapper.sh U_FORTIFY_SOURCE fstackprotector Wall Wthreadsafety Wselfassign Wunusedbutsetparameter Wnofreenonheapobject fcolordiagnostics fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections fdatasections 'std=c++0x' MD MF bazelout/darwinopt/bin/external/llvmproject/llvm/_objs/llvmmintblgen/Attributes.d 'frandomseed=bazelout/darwinopt/bin/external/llvmproject/llvm/_objs/llvmmintblgen/Attributes.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' 'DHAVE_MACH_MACH_H=1' 'DHAVE_MALLOC_MALLOC_H=1' 'DHAVE_MALLOC_ZONE_STATISTICS=1' 'DHAVE_PROC_PID_RUSAGE=1' 'DHAVE_UNW_ADD_DYNAMIC_FDE=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknowndarwin""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknowndarwin""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS 'DBLAKE3_USE_NEON=0' DBLAKE3_NO_AVX2 DBLAKE3_NO_AVX512 DBLAKE3_NO_SSE2 DBLAKE3_NO_SSE41 iquote external/llvmproject iquote bazelout/darwinopt/bin/external/llvmproject iquote external/llvm_terminfo iquote bazelout/darwinopt/bin/external/llvm_terminfo iquote external/llvm_zlib iquote bazelout/darwinopt/bin/external/llvm_zlib iquote external/bazel_tools iquote bazelout/darwinopt/bin/external/bazel_tools isystem external/llvmproject/llvm/include isystem bazelout/darwinopt/bin/external/llvmproject/llvm/include 'fvisibility=hidden' Wnosigncompare 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' nocanonicalprefixes Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' 'fmodulename=project//llvm:llvmmintblgen' 'fmodulemapfile=bazelout/darwinopt/bin/external/llvmproject/llvm/llvmmintblgen.cppmap' fmodulesstrictdecluse Wprivateheader 'fmodulemapfile=external/local_config_cc/module.modulemap' 'fmodulemapfile=bazelout/darwinopt/bin/external/llvmproject/llvm/Support.cppmap' 'fmodulemapfile=bazelout/darwinopt/bin/external/llvmproject/llvm/TableGen.cppmap' 'fmodulemapfile=bazelout/darwinopt/bin/external/llvmproject/llvm/config.cppmap' 'fmodulemapfile=bazelout/darwinopt/bin/external/bazel_tools/tools/cpp/malloc.cppmap' c external/llvmproject/llvm/utils/TableGen/Attributes.cpp o bazelout/darwinopt/bin/external/llvmproject/llvm/_objs/llvmmintblgen/Attributes.o)  Configuration: 453baf4879e102ee5df052ec6087622b871704c7bccc3f61df28e0cf20545b9f  Execution platform: //:platform In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:21:9: warning: 'LLVM_DEFAULT_TARGET_TRIPLE' macro redefined [Wmacroredefined] define LLVM_DEFAULT_TARGET_TRIPLE ""x86_64appledarwin22.3.0""         ^ :34:9: note: previous definition is here define LLVM_DEFAULT_TARGET_TRIPLE ""x86_64unknowndarwin""         ^ In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:30:9: warning: 'LLVM_HOST_TRIPLE' macro redefined [Wmacroredefined] define LLVM_HOST_TRIPLE ""x86_64appledarwin22.3.0""         ^ :33:9: note: previous definition is here define LLVM_HOST_TRIPLE ""x86_64unknowndarwin""         ^ In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:33:9: warning: 'LLVM_NATIVE_ARCH' macro redefined [Wmacroredefined] define LLVM_NATIVE_ARCH X86         ^ :25:9: note: previous definition is here define LLVM_NATIVE_ARCH ""X86""         ^ In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:69:9: warning: 'LLVM_VERSION_MAJOR' macro redefined [Wmacroredefined] define LLVM_VERSION_MAJOR 15         ^ :35:9: note: previous definition is here define LLVM_VERSION_MAJOR 17         ^ In file included from external/llvmproject/llvm/utils/TableGen/Attributes.cpp:9: In file included from /usr/local/include/llvm/TableGen/Record.h:17: In file included from /usr/local/include/llvm/ADT/ArrayRef.h:12: In file included from /usr/local/include/llvm/ADT/Hashing.h:48: In file included from /usr/local/include/llvm/Support/ErrorHandling.h:17: In file included from /usr/local/include/llvm/Support/Compiler.h:18: /usr/local/include/llvm/Config/llvmconfig.h:78:9: warning: 'LLVM_VERSION_STRING' macro redefined [Wmacroredefined] define LLVM_VERSION_STRING ""15.0.0git""         ^ :38:9: note: previous definition is here define LLVM_VERSION_STRING ""17.0.0git""         ^ external/llvmproject/llvm/utils/TableGen/Attributes.cpp:134:8: error: use of undeclared identifier 'TableGen' static TableGen::Emitter::OptClass X(""genattrs"",        ^ 5 warnings and 1 error generated. Target //build:build_wheel failed to build INFO: Elapsed time: 2.317s, Critical Path: 1.64s INFO: 34 processes: 23 internal, 11 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully b'' Traceback (most recent call last):   File ""build/build.py"", line 564, in      main()   File ""build/build.py"", line 559, in main     shell(command)   File ""build/build.py"", line 53, in shell     output = subprocess.check_output(cmd)   File ""/Users/davidlaxer/anaconda3/lib/python3.8/subprocess.py"", line 415, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/Users/davidlaxer/anaconda3/lib/python3.8/subprocess.py"", line 516, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel5.1.1darwinx86_64', 'run', 'verbose_failures=true', ':build_wheel', '', 'output_path=/Users/davidlaxer/PySR/jax/dist', 'cpu=x86_64']' returned nonzero exit status 1. ```  What jax/jaxlib version are you using? jaxlibv0.4.10  Which accelerator(s) are you using? MPS AMD GPU  Additional system info Python 3.8, MacOS 13.5  NVIDIA GPU info _No response_",2023-08-09T16:22:30Z,bug Apple GPU (Metal) plugin,closed,0,6,https://github.com/jax-ml/jax/issues/17045,"I'm not sure what's up here, given we routinely build jaxlib on Mac. The paths like ` /usr/local/include/llvm/Support/Compiler.h` make me think that for some reason a system LLVM is involved: it should not be.","There's a llvmproject built from github which installed with files in /usr/local/include and /usr/local/bin.  There's also one from MacPorts ``` % port list llvm15 llvm15                        .0.7         lang/llvm15 ``` Then there's the Apple version. What's the correct version to build jaxlib on the Mac?  How do I specify which version to use (e.g. environment variable, build flag?","You shouldn't be using any of these. The `jaxlib` build downloads its own LLVM version. I'm not sure how or why these are getting picked up, but one thing you could try is removing the MacPorts LLVM release.","The trace shows /usr/local ... but MacPorts would be in /opt/local.  So, it's picking up the github version of llvmproject that was built and installed.  Somehow,it seems that it's using my search path to find the include files and not one's that are local to the jaxlib build.","Also, issue finding xla: ``` % python build/build.py bazel_options=//xla/python:enable_tpu=true ERROR: //xla/python:enable_tpu :: Error loading option //xla/python:enable_tpu: no such package '//xla/python': The repository '' could not be resolved ``` If I remove ' bazel_options=//xla/python:enable_tpu=true', then, ``` (ai) (jaxmetal) davidlaxer2 jax % export CC=/usr/local/bin/clang export CXX=/usr/local/bin/clang++  % /usr/local/bin/clang version clang version 15.0.0 (https://github.com/llvm/llvmproject.git 89818f2dc0b6edeaf9f74ada513ebe489de8ba40) Target: x86_64appledarwin22.6.0 Thread model: posix InstalledDir: /usr/local/bin % python build/build.py               _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ WARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE. Bazel binary path: ./bazel3.7.2darwinx86_64 Python binary path: /Users/davidlaxer/jaxmetal/bin/python Python version: 3.8 NumPy version: 1.24.4 SciPy version: 1.10.1 MKLDNN enabled: yes Target CPU features: release CUDA enabled: no TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... ./bazel3.7.2darwinx86_64 run verbose_failures=true config=short_logs config=avx_posix config=mkl_open_source_only :build_wheel  output_path=/Users/davidlaxer/jax/dist WARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE. INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /Users/davidlaxer/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /Users/davidlaxer/jax/.bazelrc:   Inherited 'build' options: repo_env PYTHON_BIN_PATH=/Users/davidlaxer/jaxmetal/bin/python action_env=PYENV_ROOT python_path=/Users/davidlaxer/jaxmetal/bin/python repo_env TF_NEED_CUDA=0 action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0 repo_env TF_NEED_ROCM=0 action_env TF_ROCM_AMDGPU_TARGETS=gfx803,gfx900,gfx906,gfx1010 distinct_host_configuration=false c opt apple_platform_type=macos macos_minimum_os=10.9 announce_rc define open_source_build=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true spawn_strategy=standalone strategy=Genrule=standalone enable_platform_specific_config INFO: Found applicable config definition build:short_logs in file /Users/davidlaxer/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /Users/davidlaxer/jax/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /Users/davidlaxer/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:macos in file /Users/davidlaxer/jax/.bazelrc: config=posix INFO: Found applicable config definition build:posix in file /Users/davidlaxer/jax/.bazelrc: copt=Wnosigncompare define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true cxxopt=std=c++14 host_cxxopt=std=c++14 Loading:  Loading: 0 packages loaded DEBUG: /private/var/tmp/_bazel_davidlaxer/8ad43b8709647bf5741554c65757d353/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software. Analyzing: target //build:build_wheel (0 packages loaded, 0 targets configured) Analyzing: target //build:build_wheel (0 packages loaded, 0 targets configured) Analyzing: target //build:build_wheel (0 packages loaded, 0 targets configured) DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 0400"" DEBUG: Repository io_bazel_rules_docker instantiated at:   /Users/davidlaxer/jax/WORKSPACE:34:10: in    /private/var/tmp/_bazel_davidlaxer/8ad43b8709647bf5741554c65757d353/external/org_tensorflow/tensorflow/workspace0.bzl:108:34: in workspace   /private/var/tmp/_bazel_davidlaxer/8ad43b8709647bf5741554c65757d353/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories Repository rule git_repository defined at:   /private/var/tmp/_bazel_davidlaxer/8ad43b8709647bf5741554c65757d353/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in  WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/4c4f1ae93ea7477ccb4772007fc78313f5a0644f.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found INFO: Analyzed target //build:build_wheel (1 packages loaded, 2450 targets configured). INFO: Found 1 target... [0 / 5] [Prepa] BazelWorkspaceStatusAction stablestatus.txt ERROR: /private/var/tmp/_bazel_davidlaxer/8ad43b8709647bf5741554c65757d353/external/flatbuffers/grpc/src/compiler/BUILD:75:11: C++ compilation of rule '//grpc/src/compiler:python_generator_private' failed (Exit 1): cc_wrapper.sh failed: error executing command    (cd /private/var/tmp/_bazel_davidlaxer/8ad43b8709647bf5741554c65757d353/execroot/__main__ && \   exec env  \     PATH='/Users/davidlaxer/.opam/_coqplatform_.2021.02.1/bin:/usr/local/bin:/Users/davidlaxer/jaxmetal/bin:/Users/davidlaxer/.juliaup/bin:/Users/davidlaxer/.cabal/bin:/Users/davidlaxer/.ghcup/bin:/Users/davidlaxer/anaconda3/envs/ai/bin:/Users/davidlaxer/anaconda3/condabin:/opt/local/bin:/opt/local/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/share/dotnet:~/.dotnet/tools:/Library/Apple/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/davidlaxer/.cargo/bin:/Users/jetbrains/.local/bin' \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0 \     TF_ROCM_AMDGPU_TARGETS=gfx803,gfx900,gfx906,gfx1010 \   external/local_config_cc/cc_wrapper.sh U_FORTIFY_SOURCE fstackprotector Wall Wthreadsafety Wselfassign Wunusedbutsetparameter Wnofreenonheapobject fcolordiagnostics fnoomitframepointer g0 O2 'D_FORTIFY_SOURCE=1' DNDEBUG ffunctionsections fdatasections 'std=c++11' MD MF bazelout/darwinopt/bin/external/flatbuffers/grpc/src/compiler/_objs/python_generator_private/python_generator.d 'frandomseed=bazelout/darwinopt/bin/external/flatbuffers/grpc/src/compiler/_objs/python_generator_private/python_generator.o' iquote external/flatbuffers iquote bazelout/darwinopt/bin/external/flatbuffers Ibazelout/darwinopt/bin/external/flatbuffers/grpc/src/compiler/_virtual_includes/python_generator_private Ibazelout/darwinopt/bin/external/flatbuffers/_virtual_includes/flatbuffers Ibazelout/darwinopt/bin/external/flatbuffers/src/_virtual_includes/flatbuffers Wnosigncompare mavx 'std=c++14' nocanonicalprefixes Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' c external/flatbuffers/grpc/src/compiler/python_generator./darwinopt/bin/external/flatbuffers/grpc/src/compiler/_objs/python_generator_private/python_generator.o) Execution platform: //:platform external/flatbuffers/grpc/src/compiler/python_generator.cc:19:10: fatal error: 'algorithm' file not found include           ^~~~~~~~~~~ 1 error generated. Target //build:build_wheel failed to build INFO: Elapsed time: 7.957s, Critical Path: 0.65s INFO: 33 processes: 28 internal, 5 local. FAILED: Build did NOT complete successfully ERROR: Build failed. Not running target FAILED: Build did NOT complete successfully b'' Traceback (most recent call last):   File ""build/build.py"", line 564, in      main()   File ""build/build.py"", line 559, in main     shell(command)   File ""build/build.py"", line 52, in shell     output = subprocess.check_output(cmd)   File ""/Users/davidlaxer/anaconda3/lib/python3.8/subprocess.py"", line 415, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,   File ""/Users/davidlaxer/anaconda3/lib/python3.8/subprocess.py"", line 516, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['./bazel3.7.2darwinx86_64', 'run', 'verbose_failures=true', 'config=short_logs', 'config=avx_posix', 'config=mkl_open_source_only', ':build_wheel', '', 'output_path=/Users/davidlaxer/jax/dist']' returned nonzero exit status 1. ```",Successfully built 'jaxlib' with Python 3.9.
2095,"以下是一个github上的jax下的一个issue, 标题是(Nesting `vmap` within `pmap` using `jax.pure_callback` segfaults)， 内容是 ( Description Some background, I'm trying to parallelize a CPUintensive computation using a callback to some `scipy.optimize` routines using `jax.pure_callback` across the available CPUs on my machine. Using `vmap` and `pmap` separately on `jax.pure_callback` works when `pmap`ing over available CPUs, but not nesting the two. Here's a toy example below: ``` import os, multiprocessing os.environ[""XLA_FLAGS""] = ""xla_force_host_platform_device_count={}"".format(     multiprocessing.cpu_count()) from jax import config config.update(""jax_enable_x64"", True) import jax from jax import vmap, pmap import jax.numpy as jnp import numpy as np devs = jax.devices('cpu') n_devs = len(devs) def _np_sq(x,b): return np.square(xb) def np_sq(x,b):     shape_dtype = jax.ShapeDtypeStruct(shape=x.shape,                                      dtype=x.dtype)     res = jax.pure_callback(_np_sq, shape_dtype, x, b,                             vectorized=False)     return res ``` * vmap works: ``` x = np.random.randn(2, 3) b = np.random.randn(2, 3) vmap(np_sq)(x,b) ``` * pmap works: ``` y = np.random.randn(n_devs, 3) c = np.random.randn(n_devs, 3) pmap(np_sq, devices=devs)(y,c) ``` * but not nesting the two: ``` z = np.random.randn(n_devs, 2, 3) d = np.random.randn(n_devs, 2, 3) pmap(vmap(np_sq), devices=devs)(y,c)  Out[15]: Segmentation fault ``` Looks like the reverse composition `vmap` \circ `pmap` works. though I'm not sure if this is advisable. Something similar also happens using `shmap`, although I haven't tried that with the toy example above. Note: for just a single CPU the code runs fine on my machine.  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPUs, on a machine with a GPU. The GPU is not used.   Additional system info Ubuntu 22.04.2 LTS (GNU/Linux 5.15.072generic x86_64)  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Nesting `vmap` within `pmap` using `jax.pure_callback` segfaults," Description Some background, I'm trying to parallelize a CPUintensive computation using a callback to some `scipy.optimize` routines using `jax.pure_callback` across the available CPUs on my machine. Using `vmap` and `pmap` separately on `jax.pure_callback` works when `pmap`ing over available CPUs, but not nesting the two. Here's a toy example below: ``` import os, multiprocessing os.environ[""XLA_FLAGS""] = ""xla_force_host_platform_device_count={}"".format(     multiprocessing.cpu_count()) from jax import config config.update(""jax_enable_x64"", True) import jax from jax import vmap, pmap import jax.numpy as jnp import numpy as np devs = jax.devices('cpu') n_devs = len(devs) def _np_sq(x,b): return np.square(xb) def np_sq(x,b):     shape_dtype = jax.ShapeDtypeStruct(shape=x.shape,                                      dtype=x.dtype)     res = jax.pure_callback(_np_sq, shape_dtype, x, b,                             vectorized=False)     return res ``` * vmap works: ``` x = np.random.randn(2, 3) b = np.random.randn(2, 3) vmap(np_sq)(x,b) ``` * pmap works: ``` y = np.random.randn(n_devs, 3) c = np.random.randn(n_devs, 3) pmap(np_sq, devices=devs)(y,c) ``` * but not nesting the two: ``` z = np.random.randn(n_devs, 2, 3) d = np.random.randn(n_devs, 2, 3) pmap(vmap(np_sq), devices=devs)(y,c)  Out[15]: Segmentation fault ``` Looks like the reverse composition `vmap` \circ `pmap` works. though I'm not sure if this is advisable. Something similar also happens using `shmap`, although I haven't tried that with the toy example above. Note: for just a single CPU the code runs fine on my machine.  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPUs, on a machine with a GPU. The GPU is not used.   Additional system info Ubuntu 22.04.2 LTS (GNU/Linux 5.15.072generic x86_64)  NVIDIA GPU info _No response_",2023-08-07T16:20:16Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17001,"Hi Tan  Looks like this issue has been resolved in later versions of JAX. I executed the provided repro code with JAX version 0.4.23 on Google Colab using GPU run time. It executes without any error. ```python import os, multiprocessing os.environ[""XLA_FLAGS""] = ""xla_force_host_platform_device_count={}"".format(     multiprocessing.cpu_count()) from jax import config config.update(""jax_enable_x64"", True) import jax from jax import vmap, pmap import jax.numpy as jnp import numpy as np print(f""Jax version: {jax.__version__}"") devs = jax.devices('cpu') print(f""Devices: {devs}"") n_devs = len(devs) print(f""Number of devices: {n_devs}"") def _np_sq(x,b): return np.square(xb) def np_sq(x,b):     shape_dtype = jax.ShapeDtypeStruct(shape=x.shape,                                     dtype=x.dtype)     res = jax.pure_callback(_np_sq, shape_dtype, x, b,                             vectorized=False)     return res ``` Output: ``` Jax version: 0.4.23 Devices: [CpuDevice(id=0), CpuDevice(id=1)] Number of devices: 2 ``` `vmap`: ```python x = np.random.randn(2, 3) b = np.random.randn(2, 3) print(f""vmap:{vmap(np_sq)(x,b)}"") ``` Output: ``` vmap:[[0.11448556 0.5447552  3.27006149]  [0.16259046 0.12088213 0.45500056]] ``` `pmap`:  ```python y = np.random.randn(n_devs, 3) c = np.random.randn(n_devs, 3) print(f""pmap: {pmap(np_sq, devices=devs)(y,c)}"") ``` Output: ``` pmap: [[7.33969863 0.19640176 0.60541126]  [5.45969277 2.96010452 0.0125925 ]] ``` Nesting `vmap` with `pmap`: ```python z = np.random.randn(n_devs, 2, 3) d = np.random.randn(n_devs, 2, 3) print(f""Nesting vmap within pmap: {pmap(vmap(np_sq), devices=devs)(y,c)}"") ``` Output: ``` Nesting vmap within pmap: [[7.33969863 0.19640176 0.60541126]  [5.45969277 2.96010452 0.0125925 ]] ``` I have also verified the results in cloud VM with 4 CPUs and 4GPUs using the latest JAX version 0.4.24. It produces the following result. ``` Jax version: 0.4.24 Devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3)] Number of devices: 4 vmap:[[0.08676551 0.28720522 0.03171861]  [0.03151458 1.35523808 0.01098814]] pmap: [[1.10615892 1.54875127 0.09205769]  [1.87091675 3.67551264 1.17709292]  [1.39692925 0.00487045 0.88255363]  [0.9227732  0.07558887 0.21490667]] Nesting vmap within pmap: [[1.10615892 1.54875127 0.09205769]  [1.87091675 3.67551264 1.17709292]  [1.39692925 0.00487045 0.88255363]  [0.9227732  0.07558887 0.21490667]] ``` Please find the gist on colab for reference. Thank you"
6407,"以下是一个github上的jax下的一个issue, 标题是(Possible bug of `jnp.tile()`)， 内容是 ( Description I ran into a possible bug when trying to make an Keras IO example to be KerasCore based and backendagnostic. https://github.com/kerasteam/kerascore/pull/623 with the following code snippets,  ```python import jax import jax.numpy as jnp import numpy as np def foo(a, b):     print(f""a.shape {a.shape}"")     print(f""b.shape {b.shape}"")     return jnp.tile(a, b) .jit def foo_jit(a, b):     print(f""a.shape {a.shape}"")     print(f""b.shape {b.shape}"")     return jnp.tile(a, b) a = np.ndarray([1, 20, 20]) b = np.array([64, 1, 1]) c = foo(a, b) print(c.shape) c_jit = foo_jit(a, b) print(c_jit.shape) ``` for `foo(a, b)`, I got ``` a.shape ((1, 20, 20), ) b.shape ((3,), ) and get expected `(64, 20, 20)` ``` for `foo_jit(a, b)`, I got ``` a.shape ((1, 20, 20), Tracedwith) b.shape ((3,), Tracedwith) ``` and error messages: ``` Traceback (most recent call last):   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/numpy/core/fromnumeric.py"", line 3154, in ndim     return a.ndim AttributeError: 'list' object has no attribute 'ndim' During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/tmp/f_jax.py"", line 21, in      c_jit = foo_jit(a, b)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 253, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 161, in _python_pjit_helper     args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/api.py"", line 324, in infer_params     return pjit.common_infer_params(pjit_info_args, *args, **kwargs)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 491, in common_infer_params     jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 969, in _pjit_jaxpr     jaxpr, final_consts, out_type = _create_pjit_jaxpr(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 345, in memoized_fun     ans = call(fun, *args)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 922, in _create_pjit_jaxpr     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/tmp/f_jax.py"", line 14, in foo_jit     return jnp.tile(a, b)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1813, in tile     result = broadcast_to(reshape(A, [j for i in A_shape for j in [1, i]]),   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1191, in broadcast_to     return util._broadcast_to(array, shape)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/util.py"", line 388, in _broadcast_to     if not isinstance(shape, tuple) and np.ndim(shape) == 0:   File """", line 180, in ndim   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/numpy/core/fromnumeric.py"", line 3156, in ndim     return asarray(a).ndim   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/core.py"", line 611, in __array__     raise TracerArrayConversionError(self) jax._src.traceback_util.UnfilteredStackTrace: jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape int32[]. The error occurred while tracing the function foo_jit at /tmp/f_jax.py:10 for jit. This concrete value was not available in Python because it depends on the value of the argument b. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerArrayConversionError The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/tmp/f_jax.py"", line 21, in      c_jit = foo_jit(a, b)   File ""/tmp/f_jax.py"", line 14, in foo_jit     return jnp.tile(a, b)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1813, in tile     result = broadcast_to(reshape(A, [j for i in A_shape for j in [1, i]]),   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1191, in broadcast_to     return util._broadcast_to(array, shape)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/util.py"", line 388, in _broadcast_to     if not isinstance(shape, tuple) and np.ndim(shape) == 0:   File """", line 180, in ndim   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/numpy/core/fromnumeric.py"", line 3156, in ndim     return asarray(a).ndim jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape int32[]. The error occurred while tracing the function foo_jit at /tmp/f_jax.py:10 for jit. This concrete value was not available in Python because it depends on the value of the argument b. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerArrayConversionError ```  What jax/jaxlib version are you using? jax/jaxlib v0.4.14  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.9/3.10, macOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Possible bug of `jnp.tile()`," Description I ran into a possible bug when trying to make an Keras IO example to be KerasCore based and backendagnostic. https://github.com/kerasteam/kerascore/pull/623 with the following code snippets,  ```python import jax import jax.numpy as jnp import numpy as np def foo(a, b):     print(f""a.shape {a.shape}"")     print(f""b.shape {b.shape}"")     return jnp.tile(a, b) .jit def foo_jit(a, b):     print(f""a.shape {a.shape}"")     print(f""b.shape {b.shape}"")     return jnp.tile(a, b) a = np.ndarray([1, 20, 20]) b = np.array([64, 1, 1]) c = foo(a, b) print(c.shape) c_jit = foo_jit(a, b) print(c_jit.shape) ``` for `foo(a, b)`, I got ``` a.shape ((1, 20, 20), ) b.shape ((3,), ) and get expected `(64, 20, 20)` ``` for `foo_jit(a, b)`, I got ``` a.shape ((1, 20, 20), Tracedwith) b.shape ((3,), Tracedwith) ``` and error messages: ``` Traceback (most recent call last):   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/numpy/core/fromnumeric.py"", line 3154, in ndim     return a.ndim AttributeError: 'list' object has no attribute 'ndim' During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/tmp/f_jax.py"", line 21, in      c_jit = foo_jit(a, b)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 253, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 161, in _python_pjit_helper     args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/api.py"", line 324, in infer_params     return pjit.common_infer_params(pjit_info_args, *args, **kwargs)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 491, in common_infer_params     jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 969, in _pjit_jaxpr     jaxpr, final_consts, out_type = _create_pjit_jaxpr(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 345, in memoized_fun     ans = call(fun, *args)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 922, in _create_pjit_jaxpr     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/tmp/f_jax.py"", line 14, in foo_jit     return jnp.tile(a, b)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1813, in tile     result = broadcast_to(reshape(A, [j for i in A_shape for j in [1, i]]),   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1191, in broadcast_to     return util._broadcast_to(array, shape)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/util.py"", line 388, in _broadcast_to     if not isinstance(shape, tuple) and np.ndim(shape) == 0:   File """", line 180, in ndim   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/numpy/core/fromnumeric.py"", line 3156, in ndim     return asarray(a).ndim   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/core.py"", line 611, in __array__     raise TracerArrayConversionError(self) jax._src.traceback_util.UnfilteredStackTrace: jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape int32[]. The error occurred while tracing the function foo_jit at /tmp/f_jax.py:10 for jit. This concrete value was not available in Python because it depends on the value of the argument b. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerArrayConversionError The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/tmp/f_jax.py"", line 21, in      c_jit = foo_jit(a, b)   File ""/tmp/f_jax.py"", line 14, in foo_jit     return jnp.tile(a, b)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1813, in tile     result = broadcast_to(reshape(A, [j for i in A_shape for j in [1, i]]),   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1191, in broadcast_to     return util._broadcast_to(array, shape)   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/jax/_src/numpy/util.py"", line 388, in _broadcast_to     if not isinstance(shape, tuple) and np.ndim(shape) == 0:   File """", line 180, in ndim   File ""/Users/freedom/tfmaster/lib/python3.9/sitepackages/numpy/core/fromnumeric.py"", line 3156, in ndim     return asarray(a).ndim jax.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on traced array with shape int32[]. The error occurred while tracing the function foo_jit at /tmp/f_jax.py:10 for jit. This concrete value was not available in Python because it depends on the value of the argument b. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerArrayConversionError ```  What jax/jaxlib version are you using? jax/jaxlib v0.4.14  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.9/3.10, macOS  NVIDIA GPU info _No response_",2023-08-07T01:44:26Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/16994,"For `jax.jit`, the output shapes must only depend on the input shapes, not the input values.  Your `foo_jit` violates that: the output shape depends on values in `b`.  The usual way to mitigate this is to declare such arguments as ""static"", which causes recompilation for different values and requires the argument to be hashable. Repro: ```python import jax import jax.numpy as jnp import numpy as np from functools import partial def foo(a, b):     return jnp.tile(a, b) (jax.jit, static_argnames=""b"")    declare `b` as static def foo_jit(a, b):     return jnp.tile(a, b) a = np.ndarray([1, 20, 20]) b = (64, 1, 1)    a tuple is hashable c = foo(a, b) print(c.shape)  (64, 20, 20) c_jit = foo_jit(a, b) print(c_jit.shape)  (64, 20, 20) ```","FYI, printing from a jit'ed function does not work as usual. See the docs about `jax.debug.print`.","Thanks for answering the question! 's answer touches all the relevant pieces: this is working as expected. > FYI, printing from a jit'ed function does not work as usual. See the docs about `jax.debug.print`. Keep in mind `jax.debug.print` is only necessary for printing traced runtime values like array contents. Static values like array shapes and dtypes can be printed at tracetime with a standard numpy `print`. I'm going to close this, because it's working as expected. Feel free to comment here or open another issue if you have additional questions!"
1781,"以下是一个github上的jax下的一个issue, 标题是(jaxlib.xla_extension.XlaRuntimeError: UNIMPLEMENTED: batch in most minor dimension)， 内容是 ( Description I recently upgraded my JAX versions in a project from `jax` v0.4.10 and `jaxlib` v0.4.10+cuda11.cudnn86 to `jax` v0.4.14 and `jaxlib` v0.4.14+cuda11.cudnn86 and started seeing the following error on a piece of jitted code: ``` jaxlib.xla_extension.XlaRuntimeError: UNIMPLEMENTED: batch in most minor dimension ``` Unfortunately, I haven't been able to put together a minimal reproducible example and there are a lot of moving parts to the code, but I have noted the following behavior: 1. The code runs without error on CPU  it is only a problem on GPU. 2. When commenting out the return value of the function in question, the code runs without error: ```  def f(args):      many lines of code      return value   when this is commented out, the code JIT compiles and runs without error ``` After bisecting on JAX versions, I've found that the error starts afflicting all versions >=0.4.11. What are some possible causes or further directions to look that might help yield a minimal reproducible example or fix?  What jax/jaxlib version are you using? Working on 0.4.10/0.4.10+cuda11.cudnn86, not working on all versions >=0.4.11/>=0.4.11+cuda11.cudnn86  Which accelerator(s) are you using? CPU and GPU  Additional system info Python 3.10.12, Ubuntu 22.04  NVIDIA GPU info Tested on two different machines. The output of `nvidiasmi` on each is ``` ++  ++++ ``` I'm running the code in a conda environment with `cudatoolkit` v11.8 as some of the dependencies of this project are not compatible with Cuda 12.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jaxlib.xla_extension.XlaRuntimeError: UNIMPLEMENTED: batch in most minor dimension," Description I recently upgraded my JAX versions in a project from `jax` v0.4.10 and `jaxlib` v0.4.10+cuda11.cudnn86 to `jax` v0.4.14 and `jaxlib` v0.4.14+cuda11.cudnn86 and started seeing the following error on a piece of jitted code: ``` jaxlib.xla_extension.XlaRuntimeError: UNIMPLEMENTED: batch in most minor dimension ``` Unfortunately, I haven't been able to put together a minimal reproducible example and there are a lot of moving parts to the code, but I have noted the following behavior: 1. The code runs without error on CPU  it is only a problem on GPU. 2. When commenting out the return value of the function in question, the code runs without error: ```  def f(args):      many lines of code      return value   when this is commented out, the code JIT compiles and runs without error ``` After bisecting on JAX versions, I've found that the error starts afflicting all versions >=0.4.11. What are some possible causes or further directions to look that might help yield a minimal reproducible example or fix?  What jax/jaxlib version are you using? Working on 0.4.10/0.4.10+cuda11.cudnn86, not working on all versions >=0.4.11/>=0.4.11+cuda11.cudnn86  Which accelerator(s) are you using? CPU and GPU  Additional system info Python 3.10.12, Ubuntu 22.04  NVIDIA GPU info Tested on two different machines. The output of `nvidiasmi` on each is ``` ++  ++++ ``` I'm running the code in a conda environment with `cudatoolkit` v11.8 as some of the dependencies of this project are not compatible with Cuda 12.",2023-08-06T06:31:55Z,bug XLA needs info NVIDIA GPU,closed,1,7,https://github.com/jax-ml/jax/issues/16991,Can you provide a minimal repro please?,"I'm getting the same issue. I've managed to reduce it down to this: ``` import jax import jax.numpy as jnp def f(x):     C = jnp.zeros((10,10))     C = C.at[jnp.triu_indices(10)].set(x)     A = jnp.dot(C,C.T)     return A .jit def g(x):     return jax.vmap(jax.vmap(f))(x) x = jnp.ones((10,3,55)) g(x) ``` Throws  ``` XlaRuntimeError: UNIMPLEMENTED: batch in most minor dimension ``` Confirmed it occurs on my local machine and on Colab. The code works with CPU, but breaks on GPU. Also works if the jit is removed.","Thanks for putting together the minimal repro! I hadn't found the time to get around to it. I can confirm that this snippet throws the error on both machines tested above under the same conditions on version 0.4.14 (works without JIT on GPU, works on CPU with JIT, breaks with JIT on GPU).","No worries, it's been on my list for a while too.  Note I'm able to workaround this by flattening the batch dimensions. I.e. something like: ``` .jit def g(x):     y = x.reshape(1,x.shape[1])     z = jax.vmap(f)(y)     return z.reshape(x.shape[:1]) ```",I filed an XLA bug (https://github.com/openxla/xla/issues/4833).,"I still have this issue, even though the XLA bug seems to be resolved. Downgrading to `jaxlib==0.4.10` seems to fix.","That's correct. The issue is fixed at jax head, but you'll need to either build `jaxlib` from source or wait until we make a new release. I suspect we'll make a new release this week, as it happens. Hope that helps!"
1183,"以下是一个github上的jax下的一个issue, 标题是([JAX] Introduce `DeviceList` backed by C++ `xla::ifrt::DeviceList`)， 内容是 ([JAX] Introduce `DeviceList` backed by C++ `xla::ifrt::DeviceList` This change adds `xla_client.DeviceList` that is implemented in C++ `jax::PyDeviceList`. `jax::PyDeviceList` implements the features of `pxla._DeviceAssignment` as a functional dropin replacement. `jax::PyDeviceList` internally has `xla::ifrt::DeviceList`, which will be used when using IFRT APIs without having to construct a new copy of a potentially large device list. `pxla._DeviceAssignment`'s interface is changed slightly to encourage avoiding conversion to tuple. Note that for the backward compatibility (and fast `xla_client.Device` conversion), `jax::PyDeviceList` still uses a Python tuple whose element can be any Python object matches `xla_client.Device` interface with duck typing. This duck typing support will be removed when such use case is deprecated. Eventually, we can try to avoid any type conversion to remove a shadow copy of device list in JAX.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[JAX] Introduce `DeviceList` backed by C++ `xla::ifrt::DeviceList`,"[JAX] Introduce `DeviceList` backed by C++ `xla::ifrt::DeviceList` This change adds `xla_client.DeviceList` that is implemented in C++ `jax::PyDeviceList`. `jax::PyDeviceList` implements the features of `pxla._DeviceAssignment` as a functional dropin replacement. `jax::PyDeviceList` internally has `xla::ifrt::DeviceList`, which will be used when using IFRT APIs without having to construct a new copy of a potentially large device list. `pxla._DeviceAssignment`'s interface is changed slightly to encourage avoiding conversion to tuple. Note that for the backward compatibility (and fast `xla_client.Device` conversion), `jax::PyDeviceList` still uses a Python tuple whose element can be any Python object matches `xla_client.Device` interface with duck typing. This duck typing support will be removed when such use case is deprecated. Eventually, we can try to avoid any type conversion to remove a shadow copy of device list in JAX.",2023-08-04T21:39:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16982
7395,"以下是一个github上的jax下的一个issue, 标题是(Triton softmax cause a crash)， 内容是 ( Description There is a crash related to triton softmax. Disabling it with the `XLA_FLAGS=xla_gpu_enable_triton_softmax_fusion=false` remove the error. The error is: ``` ======================= test session starts ======================= platform linux  Python 3.10.6, pytest7.4.0, pluggy1.2.0 rootdir: /opt/praxis plugins: jaxtyping0.2.20, anyio3.7.1, typeguard4.1.0 collected 52 items / 51 deselected / 1 selected embedding_softmax_test.py Fatal Python error: Aborted Thread 0x00007fab7d354000 (most recent call first):   File ""/opt/jaxsource/jax/_src/interpreters/pxla.py"", line 1228 in __call__   File ""/opt/jaxsource/jax/_src/profiler.py"", line 314 in wrapper   File ""/opt/jaxsource/jax/_src/pjit.py"", line 1143 in _pjit_call_impl_python   File ""/opt/jaxsource/jax/_src/pjit.py"", line 1187 in call_impl_cache_miss   File ""/opt/jaxsource/jax/_src/pjit.py"", line 1203 in _pjit_call_impl   File ""/opt/jaxsource/jax/_src/core.py"", line 821 in process_primitive   File ""/opt/jaxsource/jax/_src/core.py"", line 389 in bind_with_trace   File ""/opt/jaxsource/jax/_src/core.py"", line 2596 in bind   File ""/opt/jaxsource/jax/_src/pjit.py"", line 165 in _python_pjit_helper   File ""/opt/jaxsource/jax/_src/pjit.py"", line 252 in cache_miss   File ""/opt/jaxsource/jax/_src/traceback_util.py"", line 166 in reraise_with_filtered_traceback   File ""/opt/jaxsource/jax/_src/numpy/array_methods.py"", line 256 in deferring_binary_op   File ""/opt/praxis/praxis/layers/embedding_softmax.py"", line 318 in __call__   File ""/opt/flax/flax/linen/module.py"", line 967 in _call_wrapped_method   File ""/opt/flax/flax/linen/module.py"", line 467 in wrapped_module_method     File ""/opt/flax/flax/linen/module.py"", line 2307 in scope_fn   File ""/opt/flax/flax/core/scope.py"", line 998 in wrapper   File ""/opt/flax/flax/linen/module.py"", line 1682 in apply   File ""/opt/jaxsource/jax/_src/traceback_util.py"", line 166 in reraise_with_filtered_traceback   File ""/opt/praxis/praxis/base_layer.py"", line 1799 in apply   File ""/opt/flax/flax/linen/module.py"", line 967 in _call_wrapped_method   File ""/opt/flax/flax/linen/module.py"", line 467 in wrapped_module_method   File ""/opt/praxis/praxis/layers/embedding_softmax_test.py"", line 374 in test_simple_softmax_layer_class_probs   File ""/usr/local/lib/python3.10/distpackages/absl/testing/parameterized.py"", line 320 in bound_param_test   File ""/usr/lib/python3.10/unittest/case.py"", line 549 in _callTestMethod   File ""/usr/lib/python3.10/unittest/case.py"", line 591 in run   File ""/usr/lib/python3.10/unittest/case.py"", line 650 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/unittest.py"", line 333 in runtest   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 169 in pytest_runtest_call   File ""/usr/local/lib/python3.10/distpackages/pluggy/_callers.py"", line 80 in _multicall   File ""/usr/local/lib/python3.10/distpackages/pluggy/_manager.py"", line 112 in _hookexec   File ""/usr/local/lib/python3.10/distpackages/pluggy/_hooks.py"", line 433 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 262 in    File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 341 in from_call   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 261 in call_runtest_hook   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 222 in call_and_report   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 133 in runtestprotocol   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 114 in pytest_runtest_protocol   File ""/usr/local/lib/python3.10/distpackages/pluggy/_callers.py"", line 80 in _multicall   File ""/usr/local/lib/python3.10/distpackages/pluggy/_manager.py"", line 112 in _hookexec   File ""/usr/local/lib/python3.10/distpackages/pluggy/_hooks.py"", line 433 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/main.py"", line 349 in pytest_runtestloop     File ""/usr/local/lib/python3.10/distpackages/pluggy/_callers.py"", line 80 in _multicall   File ""/usr/local/lib/python3.10/distpackages/pluggy/_manager.py"", line 112 in _hookexec   File ""/usr/local/lib/python3.10/distpackages/pluggy/_hooks.py"", line 433 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/main.py"", line 324 in _main   File ""/usr/local/lib/python3.10/distpackages/_pytest/main.py"", line 270 in wrap_session   File ""/usr/local/lib/python3.10/distpackages/_pytest/main.py"", line 317 in pytest_cmdline_main   File ""/usr/local/lib/python3.10/distpackages/pluggy/_callers.py"", line 80 in _multicall   File ""/usr/local/lib/python3.10/distpackages/pluggy/_manager.py"", line 112 in _hookexec   File ""/usr/local/lib/python3.10/distpackages/pluggy/_hooks.py"", line 433 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/config/__init__.py"", line 166 in main   File ""/usr/local/lib/python3.10/distpackages/_pytest/config/__init__.py"", line 189 in console_main   File ""/usr/local/bin/pytest"", line 8 in    Extension modules: jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, msgpack._cmsgpack, yaml._yaml, scipy._lib._ccallback_c, numpy.linalg.lapack_lite, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.linalg._flinalg, scipy.sparse.linalg._dsolve._superlu, scipy. ... s, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.j son, sentencepiece._sentencepiece, matplotlib._c_internal_utils, matplotlib._path, kiwisolver._cext, matplotlib._image, scipy.cluster._ vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering (total: 196) Aborted (core dumped) ``` Repro instruction on a 80G GPU: ``` docker run it rm gpus=all ghcr.io/nvidia/pax:nightly20230803 cd /opt/praxis/praxis/layers sed i 's/(32, 30000)/(65536, 50304)/g' embedding_softmax_test.py  GPT 126M size, b=32, s=2048 sed i 's/num_classes])/num_classes]).astype(jnp.bfloat16)/g' embedding_softmax_test.py  change to bfloat16 pip install pytest pytest embedding_softmax_test.py k test_simple_softmax_layer_class_probs2  Aborted (core dumped) XLA_FLAGS=""xla_gpu_enable_triton_softmax_fusion=false"" pytest embedding_softmax_test.py k test_simple_softmax_layer_class_probs2  Runnable ``` My first guess would be that the grid/block passed isn't the right one.  What jax/jaxlib version are you using? upstream  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Reproduced on H100 80G. Need 80G to reproduce. Probably reproduce on A100 80G)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Triton softmax cause a crash," Description There is a crash related to triton softmax. Disabling it with the `XLA_FLAGS=xla_gpu_enable_triton_softmax_fusion=false` remove the error. The error is: ``` ======================= test session starts ======================= platform linux  Python 3.10.6, pytest7.4.0, pluggy1.2.0 rootdir: /opt/praxis plugins: jaxtyping0.2.20, anyio3.7.1, typeguard4.1.0 collected 52 items / 51 deselected / 1 selected embedding_softmax_test.py Fatal Python error: Aborted Thread 0x00007fab7d354000 (most recent call first):   File ""/opt/jaxsource/jax/_src/interpreters/pxla.py"", line 1228 in __call__   File ""/opt/jaxsource/jax/_src/profiler.py"", line 314 in wrapper   File ""/opt/jaxsource/jax/_src/pjit.py"", line 1143 in _pjit_call_impl_python   File ""/opt/jaxsource/jax/_src/pjit.py"", line 1187 in call_impl_cache_miss   File ""/opt/jaxsource/jax/_src/pjit.py"", line 1203 in _pjit_call_impl   File ""/opt/jaxsource/jax/_src/core.py"", line 821 in process_primitive   File ""/opt/jaxsource/jax/_src/core.py"", line 389 in bind_with_trace   File ""/opt/jaxsource/jax/_src/core.py"", line 2596 in bind   File ""/opt/jaxsource/jax/_src/pjit.py"", line 165 in _python_pjit_helper   File ""/opt/jaxsource/jax/_src/pjit.py"", line 252 in cache_miss   File ""/opt/jaxsource/jax/_src/traceback_util.py"", line 166 in reraise_with_filtered_traceback   File ""/opt/jaxsource/jax/_src/numpy/array_methods.py"", line 256 in deferring_binary_op   File ""/opt/praxis/praxis/layers/embedding_softmax.py"", line 318 in __call__   File ""/opt/flax/flax/linen/module.py"", line 967 in _call_wrapped_method   File ""/opt/flax/flax/linen/module.py"", line 467 in wrapped_module_method     File ""/opt/flax/flax/linen/module.py"", line 2307 in scope_fn   File ""/opt/flax/flax/core/scope.py"", line 998 in wrapper   File ""/opt/flax/flax/linen/module.py"", line 1682 in apply   File ""/opt/jaxsource/jax/_src/traceback_util.py"", line 166 in reraise_with_filtered_traceback   File ""/opt/praxis/praxis/base_layer.py"", line 1799 in apply   File ""/opt/flax/flax/linen/module.py"", line 967 in _call_wrapped_method   File ""/opt/flax/flax/linen/module.py"", line 467 in wrapped_module_method   File ""/opt/praxis/praxis/layers/embedding_softmax_test.py"", line 374 in test_simple_softmax_layer_class_probs   File ""/usr/local/lib/python3.10/distpackages/absl/testing/parameterized.py"", line 320 in bound_param_test   File ""/usr/lib/python3.10/unittest/case.py"", line 549 in _callTestMethod   File ""/usr/lib/python3.10/unittest/case.py"", line 591 in run   File ""/usr/lib/python3.10/unittest/case.py"", line 650 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/unittest.py"", line 333 in runtest   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 169 in pytest_runtest_call   File ""/usr/local/lib/python3.10/distpackages/pluggy/_callers.py"", line 80 in _multicall   File ""/usr/local/lib/python3.10/distpackages/pluggy/_manager.py"", line 112 in _hookexec   File ""/usr/local/lib/python3.10/distpackages/pluggy/_hooks.py"", line 433 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 262 in    File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 341 in from_call   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 261 in call_runtest_hook   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 222 in call_and_report   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 133 in runtestprotocol   File ""/usr/local/lib/python3.10/distpackages/_pytest/runner.py"", line 114 in pytest_runtest_protocol   File ""/usr/local/lib/python3.10/distpackages/pluggy/_callers.py"", line 80 in _multicall   File ""/usr/local/lib/python3.10/distpackages/pluggy/_manager.py"", line 112 in _hookexec   File ""/usr/local/lib/python3.10/distpackages/pluggy/_hooks.py"", line 433 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/main.py"", line 349 in pytest_runtestloop     File ""/usr/local/lib/python3.10/distpackages/pluggy/_callers.py"", line 80 in _multicall   File ""/usr/local/lib/python3.10/distpackages/pluggy/_manager.py"", line 112 in _hookexec   File ""/usr/local/lib/python3.10/distpackages/pluggy/_hooks.py"", line 433 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/main.py"", line 324 in _main   File ""/usr/local/lib/python3.10/distpackages/_pytest/main.py"", line 270 in wrap_session   File ""/usr/local/lib/python3.10/distpackages/_pytest/main.py"", line 317 in pytest_cmdline_main   File ""/usr/local/lib/python3.10/distpackages/pluggy/_callers.py"", line 80 in _multicall   File ""/usr/local/lib/python3.10/distpackages/pluggy/_manager.py"", line 112 in _hookexec   File ""/usr/local/lib/python3.10/distpackages/pluggy/_hooks.py"", line 433 in __call__   File ""/usr/local/lib/python3.10/distpackages/_pytest/config/__init__.py"", line 166 in main   File ""/usr/local/lib/python3.10/distpackages/_pytest/config/__init__.py"", line 189 in console_main   File ""/usr/local/bin/pytest"", line 8 in    Extension modules: jaxlib.cpu_feature_guard, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, zstandard.backend_c, msgpack._cmsgpack, yaml._yaml, scipy._lib._ccallback_c, numpy.linalg.lapack_lite, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.linalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.linalg._flinalg, scipy.sparse.linalg._dsolve._superlu, scipy. ... s, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.testing, pandas._libs.parsers, pandas._libs.j son, sentencepiece._sentencepiece, matplotlib._c_internal_utils, matplotlib._path, kiwisolver._cext, matplotlib._image, scipy.cluster._ vq, scipy.cluster._hierarchy, scipy.cluster._optimal_leaf_ordering (total: 196) Aborted (core dumped) ``` Repro instruction on a 80G GPU: ``` docker run it rm gpus=all ghcr.io/nvidia/pax:nightly20230803 cd /opt/praxis/praxis/layers sed i 's/(32, 30000)/(65536, 50304)/g' embedding_softmax_test.py  GPT 126M size, b=32, s=2048 sed i 's/num_classes])/num_classes]).astype(jnp.bfloat16)/g' embedding_softmax_test.py  change to bfloat16 pip install pytest pytest embedding_softmax_test.py k test_simple_softmax_layer_class_probs2  Aborted (core dumped) XLA_FLAGS=""xla_gpu_enable_triton_softmax_fusion=false"" pytest embedding_softmax_test.py k test_simple_softmax_layer_class_probs2  Runnable ``` My first guess would be that the grid/block passed isn't the right one.  What jax/jaxlib version are you using? upstream  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Reproduced on H100 80G. Need 80G to reproduce. Probably reproduce on A100 80G",2023-08-04T14:55:57Z,bug GPU,closed,1,6,https://github.com/jax-ml/jax/issues/16973,"Assigning  to follow up, but it's probably  who needs to look.","Managed to reproduce this also on A10040G by allowing input/output buffers to be aliased. After investigating and trying to figure out the dimension for which it starts failing, this seems to start failing around shape `(42692, 50304)`. The product of the shape is close to 2**31, so my money is on an integer overflow in Triton itself. I will follow up there, but our version of Triton in XLA is still behind the big initialH100support commit, so it might take a little bit of time to get the fix in XLA even if once is fixed upstream, as we need to make sure nothing breaks while integrating H100 support. In the meantime, I can probably provide a temporary fix that disallows Softmax fusion in such cases.","So as it turns out, I gave myself too much creditthe mistake was mine! This should fix it and is landing as we speak :)","With the container `ghcr.io/nvidia/pax:nightly20230803`, I get another error now: ``` ============================================================== FAILURES =============================================================== _____________________________________ EmbeddingSoftmaxTest.test_simple_softmax_layer_class_probs2 _____________________________________ self =  batch_size = 65536, num_classes = 50304     .parameters((8, 1001), (16, 1024), (65536, 50304))     def test_simple_softmax_layer_class_probs(self, batch_size, num_classes):     ...       np_get_logits = to_np(logits)       tf_np_get_logits = to_np(tf_logits) >     self.assertAllClose(np_get_logits, tf_np_get_logits) embedding_softmax_test.py:409: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ../test_utils.py:87: in assertAllClose     np.testing.assert_allclose(x, y, rtol=rtol, atol=atol, **kwargs) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ args = (.compare at 0x7f1783bdf910>, array([[ 0.00582462,  0.0245036 ,  0.07030115, ..., 0...],        [ 0.03172682, 0.02000502,  0.03393763, ..., 0.06318258,          0.14392377,  0.08553023]], dtype=float32)) kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e05, atol=1e05', 'verbose': True}     (func)     def inner(*args, **kwds):         with self._recreate_cm(): >           return func(*args, **kwds) E           AssertionError: E           Not equal to tolerance rtol=1e05, atol=1e05 E E           Mismatched elements: 2345742940 / 3296722944 (71.2%) E           Max absolute difference: 0.00022475 E           Max relative difference: 179561.94 E            x: array([[ 0.005825,  0.024504,  0.070301, ..., 0.021309,  0.100876, E                   0.00767 ], E                  [0.083098,  0.137958,  0.125972, ...,  0.10841 ,  0.077706,... E            y: array([[ 0.005845,  0.024529,  0.070316, ..., 0.02129 ,  0.100827, E                   0.007618], E                  [0.083073,  0.137941,  0.125988, ...,  0.108402,  0.077672,... /usr/lib/python3.10/contextlib.py:79: AssertionError ```","The numbers are not too far off, and this reproduces also with smaller shape (e.g. `(500, 50304)`), with or without Triton Softmax fusion. This seems like an issue of numerical stability because you changed the test to use `bfloat16` as the initial data typeand thus not a bug.",Good catch. Thanks. Closing.
506,"以下是一个github上的jax下的一个issue, 标题是(Add functions to unregister event duration listeners.)， 内容是 (Add functions to unregister event duration listeners. Add private functions _unregister_event_duration_listener_by_callback and _unregister_event_duration_listener_by_index to remove registered event duration listeners. The functions are supposed to be called in test only.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add functions to unregister event duration listeners.,Add functions to unregister event duration listeners. Add private functions _unregister_event_duration_listener_by_callback and _unregister_event_duration_listener_by_index to remove registered event duration listeners. The functions are supposed to be called in test only.,2023-08-04T01:12:28Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16958,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
368,"以下是一个github上的jax下的一个issue, 标题是(Create the failure test when tf.SavedModel miss the XLACallModule function_list after loading.)， 内容是 (Create the failure test when tf.SavedModel miss the XLACallModule function_list after loading.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Create the failure test when tf.SavedModel miss the XLACallModule function_list after loading.,Create the failure test when tf.SavedModel miss the XLACallModule function_list after loading.,2023-08-03T05:22:09Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16942
466,"以下是一个github上的jax下的一个issue, 标题是(Enable test for indexing with u8 indices.)， 内容是 (Enable test for indexing with u8 indices. https://github.com/openxla/xla/commit/4e4eff35bf9a5f8ed54fd290391bd0612f49533e fixed the underlying XLA problem. Fixes https://github.com/google/jax/issues/6122 https://github.com/google/jax/issues/16836)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Enable test for indexing with u8 indices.,Enable test for indexing with u8 indices. https://github.com/openxla/xla/commit/4e4eff35bf9a5f8ed54fd290391bd0612f49533e fixed the underlying XLA problem. Fixes https://github.com/google/jax/issues/6122 https://github.com/google/jax/issues/16836,2023-08-01T14:27:33Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16917
1455,"以下是一个github上的jax下的一个issue, 标题是(Simple optimization problem running faster in Tensor Flow compared to JAX with Optax)， 内容是 ( Description Hi, I've been trying to port Tensor Flow code in Google Colab (free version) to JAX but the execution time is 5 times slower: The original TF code takes 4 seconds to run: https://colab.research.google.com/drive/1HYYBr4M3hstzmPnTmYO8DEbodTuFBJb4?usp=drive_link The equivalent JAX code takes 20 seconds to run: https://colab.research.google.com/drive/1E9L_iyf7ZG0WfUgxmtbwAhTSuWdWzMei?usp=drive_link I changed the optimizer to the JAX library Adam optimizer (jax.example_libraries) and it runs in 5 seconds: https://colab.research.google.com/drive/1dM0QeRWeOoMhejnyymtCqh_EjVmCi8z?usp=sharing The target image is here: https://drive.google.com/file/d/1jsxSEv0c0SfXaDtKhYd6hz08yX_pm4g/view?usp=sharing I got the equivalent JAX code to run 5 times faster by taking the iteration step calculations, putting them in a function and jitting it but I haven't been able to accelerate the code further. The loss and field calculation functions on their own are 4 times faster in JAX than in TF. Regards, Alex  What jax/jaxlib version are you using? Latest in Colab  Which accelerator(s) are you using? GPU (in free Colab)  Additional system info Google Colab  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Simple optimization problem running faster in Tensor Flow compared to JAX with Optax," Description Hi, I've been trying to port Tensor Flow code in Google Colab (free version) to JAX but the execution time is 5 times slower: The original TF code takes 4 seconds to run: https://colab.research.google.com/drive/1HYYBr4M3hstzmPnTmYO8DEbodTuFBJb4?usp=drive_link The equivalent JAX code takes 20 seconds to run: https://colab.research.google.com/drive/1E9L_iyf7ZG0WfUgxmtbwAhTSuWdWzMei?usp=drive_link I changed the optimizer to the JAX library Adam optimizer (jax.example_libraries) and it runs in 5 seconds: https://colab.research.google.com/drive/1dM0QeRWeOoMhejnyymtCqh_EjVmCi8z?usp=sharing The target image is here: https://drive.google.com/file/d/1jsxSEv0c0SfXaDtKhYd6hz08yX_pm4g/view?usp=sharing I got the equivalent JAX code to run 5 times faster by taking the iteration step calculations, putting them in a function and jitting it but I haven't been able to accelerate the code further. The loss and field calculation functions on their own are 4 times faster in JAX than in TF. Regards, Alex  What jax/jaxlib version are you using? Latest in Colab  Which accelerator(s) are you using? GPU (in free Colab)  Additional system info Google Colab  NVIDIA GPU info _No response_",2023-08-01T13:21:48Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/16916,"Hi  thanks for sharing the results of your exploration! So if I understand correctly, it sounds like something in the optax framework is leading to a slowdown. It might be worth opening a bug at https://github.com/deepmind/optax to see if one of the developers of that library has ideas on how to improve the runtime. You might also check out jaxopt, which is a jaxbased optimization library developed by a different research group within Google."
10323,"以下是一个github上的jax下的一个issue, 标题是(Sharded FFT with JIT gives incorrect results on GPU since 0.4.9)， 内容是 ( Description (this is not the same issue as CC(sharded fft unnecessarily loads entire array), but it gives helpful information on this topic) I am trying to calculate the 3DFFT of a 1024^3 matrix using 2 GPUs. This should work using code similar to https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html (but adapted to calculate a 3DFFT instead of a 1D) or the JAXonly code from https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuFFTMp/JAX_FFT (as shared in CC(sharded fft unnecessarily loads entire array)). My code seems to give the correct result when not using JIT, when only using GPU devices or when not sharding the memory. But in the combination I need (JIT with GPU on sharded memory), the results differ from the other results (and what scipy/numpy calculates). main code as a minimal example: ```python import os from pathlib import Path import jax import numpy as np import scipy from jax import jit from jax.experimental import mesh_utils from jax.sharding import Mesh, PartitionSpec as P, NamedSharding import sharded_fft_minimal as sharded_fft os.environ['XLA_FLAGS'] = f'xla_force_host_platform_device_count=2' jax.config.update(""jax_enable_x64"", True) def print_subset(x):     print(x[0, :4, :4]) size = 4  xs = jax.random.normal(jax.random.PRNGKey(0), (size, size, size), dtype=jax.numpy.complex64) xs = jax.numpy.ones((size, size, size), dtype=jax.numpy.complex64) x_np = np.asarray(xs) out_ref = scipy.fft.fftn(x_np, workers=128) devices = mesh_utils.create_device_mesh((2,)) mesh = Mesh(devices, axis_names=('gpus',)) with mesh:     xshard = jax.device_put(xs, NamedSharding(mesh, P(""gpus"", None)))     def two_fft(x):         out = sharded_fft.fft(x)         out = sharded_fft.fft_X(out)         return out     two_fft_jit = jit(         two_fft,         in_shardings=(NamedSharding(mesh, P(""gpus"", None))),         out_shardings=(NamedSharding(mesh, P(None, ""gpus"")))     )     out = two_fft(xshard).block_until_ready()     out_jit = two_fft_jit(xshard).block_until_ready()     print(""JAX output without JIT:"")     print_subset(out)     print(""JAX output with JIT:"")     print_subset(out_jit)     print(""Reference output:"")     print_subset(out_ref)     is_equal = np.allclose(out_ref, out_jit, rtol=1.e2, atol=1.e4)     print(f""Are the jax (JIT) and scipy results the same? {is_equal}"")     a = Path(""compiled.txt"")     a.write_text(two_fft_jit.lower(xshard).compile().as_text()) ``` with `shared_fft_minimal.py`: ```python import jax from jax.experimental.custom_partitioning import custom_partitioning from jax.sharding import PartitionSpec as P, NamedSharding def _fft(x):     return jax.numpy.fft.fftn(x, axes=list(range(1, len(x.shape)))) def _fftX(x):     return jax.numpy.fft.fft(x, axis=0) def supported_sharding(sharding, shape):     return NamedSharding(sharding.mesh, P(""gpus"",None)) def partition(arg_shapes, result_shape):     result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return _fft, supported_sharding(arg_shardings[0], arg_shapes[0]), (         supported_sharding(arg_shardings[0], arg_shapes[0]),) def infer_sharding_from_operands(arg_shapes, result_shape):     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return supported_sharding(arg_shardings[0], arg_shapes[0]) def supported_sharding_X(sharding, shape):     return NamedSharding(sharding.mesh, P(None, ""gpus"")) def partition_X(arg_shapes, result_shape):     result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return _fftX, supported_sharding_X(arg_shardings[0], arg_shapes[0]), (         supported_sharding_X(arg_shardings[0], arg_shapes[0]),) def infer_sharding_from_operands_X(arg_shapes, result_shape):     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return supported_sharding_X(arg_shardings[0], arg_shapes[0]) def fft(x):          def _fft_(x):         return _fft(x)     _fft_.def_partition(         infer_sharding_from_operands=infer_sharding_from_operands,         partition=partition)     return _fft_(x) def fft_X(x):          def _fft_(x):         return _fftX(x)     _fft_.def_partition(         infer_sharding_from_operands=infer_sharding_from_operands_X,         partition=partition_X)     return _fft_(x) ``` When I run this on a host with 2 GPUs (with CPUs all three outputs are the same), I get: ```text 20230731 18:28:38.496970: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 11.6 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. JAX output without JIT: [[64.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]] JAX output with JIT: [[32.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]] Reference output: [[64.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]] Are the jax (JIT) and scipy results the same? False ``` HLO  ``` HloModule jit_two_fft, is_scheduled=true, entry_computation_layout={(c64[2,4,4]{2,1,0})>c64[4,2,4]{2,1,0}} %fused_computation (param_0.1: c64[2,2,2,4]) > c64[2,4,4] {   %param_0.1 = c64[2,2,2,4]{3,2,0,1} parameter(0)   %bitcast.30 = c64[4,2,4]{2,1,0} bitcast(c64[2,2,2,4]{3,2,0,1} %param_0.1)   ROOT %transpose.5 = c64[2,4,4]{2,1,0} transpose(c64[4,2,4]{2,1,0} %bitcast.30), dimensions={1,2,0}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65} } %fused_computation.1 (param_0.4: c64[2,4,4]) > c64[2,2,2,4] {   %param_0.4 = c64[2,4,4]{2,1,0} parameter(0)   %bitcast.32 = c64[2,2,2,4]{3,2,1,0} bitcast(c64[2,4,4]{2,1,0} %param_0.4)   %transpose.6 = c64[2,2,2,4]{3,2,1,0} transpose(c64[2,2,2,4]{3,2,1,0} %bitcast.32), dimensions={1,0,2,3}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}   ROOT %bitcast.31 = c64[2,2,2,4]{3,2,0,1} bitcast(c64[2,2,2,4]{3,2,1,0} %transpose.6), metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65} } ENTRY %main.6_spmd (param: c64[2,4,4]) > c64[4,2,4] {   %param = c64[2,4,4]{2,1,0} parameter(0), sharding={devices=[2,1,1]0,1}   %fft.0 = c64[2,4,4]{2,1,0} fft(c64[2,4,4]{2,1,0} %param), fft_type=FFT, fft_length={4,4}, metadata={op_name=""jit(main)/jit(fft)/fft[fft_type=FftType.FFT fft_lengths=(4, 4)]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=7}   %fusion.1 = c64[2,2,2,4]{3,2,0,1} fusion(c64[2,4,4]{2,1,0} %fft.0), kind=kLoop, calls=%fused_computation.1, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}   %alltoallstart = ((c64[2,2,2,4]{3,2,0,1}), c64[2,2,2,4]{3,2,0,1}) alltoallstart(c64[2,2,2,4]{3,2,0,1} %fusion.1), channel_id=1, replica_groups={{0,1}}, dimensions={1}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}, backend_config={""is_sync"":true}   %alltoalldone = c64[2,2,2,4]{3,2,0,1} alltoalldone(((c64[2,2,2,4]{3,2,0,1}), c64[2,2,2,4]{3,2,0,1}) %alltoallstart), channel_id=1, replica_groups={{0,1}}, dimensions={1}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}   %fusion = c64[2,4,4]{2,1,0} fusion(c64[2,2,2,4]{3,2,0,1} %alltoalldone), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}   %fft.1 = c64[2,4,4]{2,1,0} fft(c64[2,4,4]{2,1,0} %fusion), fft_type=FFT, fft_length={4}, metadata={op_name=""jit(main)/jit(fft)/fft[fft_type=FftType.FFT fft_lengths=(4,)]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=11}   ROOT %transpose.4 = c64[4,2,4]{2,1,0} transpose(c64[2,4,4]{2,1,0} %fft.1), dimensions={2,0,1}, frontend_attributes={fingerprint_before_lhs=""902ad6c9909648982a9ff9203157ab0d""}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65} } ```     What jax/jaxlib version are you using? jax==0.4.14, jaxlib==0.4.14+cuda11.cudnn86  Which accelerator(s) are you using? GPU (2x Nvidia A40)  Additional system info Python 3.9.15, AlmaLinux 8.5  NVIDIA GPU info ```text ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Sharded FFT with JIT gives incorrect results on GPU since 0.4.9," Description (this is not the same issue as CC(sharded fft unnecessarily loads entire array), but it gives helpful information on this topic) I am trying to calculate the 3DFFT of a 1024^3 matrix using 2 GPUs. This should work using code similar to https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html (but adapted to calculate a 3DFFT instead of a 1D) or the JAXonly code from https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuFFTMp/JAX_FFT (as shared in CC(sharded fft unnecessarily loads entire array)). My code seems to give the correct result when not using JIT, when only using GPU devices or when not sharding the memory. But in the combination I need (JIT with GPU on sharded memory), the results differ from the other results (and what scipy/numpy calculates). main code as a minimal example: ```python import os from pathlib import Path import jax import numpy as np import scipy from jax import jit from jax.experimental import mesh_utils from jax.sharding import Mesh, PartitionSpec as P, NamedSharding import sharded_fft_minimal as sharded_fft os.environ['XLA_FLAGS'] = f'xla_force_host_platform_device_count=2' jax.config.update(""jax_enable_x64"", True) def print_subset(x):     print(x[0, :4, :4]) size = 4  xs = jax.random.normal(jax.random.PRNGKey(0), (size, size, size), dtype=jax.numpy.complex64) xs = jax.numpy.ones((size, size, size), dtype=jax.numpy.complex64) x_np = np.asarray(xs) out_ref = scipy.fft.fftn(x_np, workers=128) devices = mesh_utils.create_device_mesh((2,)) mesh = Mesh(devices, axis_names=('gpus',)) with mesh:     xshard = jax.device_put(xs, NamedSharding(mesh, P(""gpus"", None)))     def two_fft(x):         out = sharded_fft.fft(x)         out = sharded_fft.fft_X(out)         return out     two_fft_jit = jit(         two_fft,         in_shardings=(NamedSharding(mesh, P(""gpus"", None))),         out_shardings=(NamedSharding(mesh, P(None, ""gpus"")))     )     out = two_fft(xshard).block_until_ready()     out_jit = two_fft_jit(xshard).block_until_ready()     print(""JAX output without JIT:"")     print_subset(out)     print(""JAX output with JIT:"")     print_subset(out_jit)     print(""Reference output:"")     print_subset(out_ref)     is_equal = np.allclose(out_ref, out_jit, rtol=1.e2, atol=1.e4)     print(f""Are the jax (JIT) and scipy results the same? {is_equal}"")     a = Path(""compiled.txt"")     a.write_text(two_fft_jit.lower(xshard).compile().as_text()) ``` with `shared_fft_minimal.py`: ```python import jax from jax.experimental.custom_partitioning import custom_partitioning from jax.sharding import PartitionSpec as P, NamedSharding def _fft(x):     return jax.numpy.fft.fftn(x, axes=list(range(1, len(x.shape)))) def _fftX(x):     return jax.numpy.fft.fft(x, axis=0) def supported_sharding(sharding, shape):     return NamedSharding(sharding.mesh, P(""gpus"",None)) def partition(arg_shapes, result_shape):     result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return _fft, supported_sharding(arg_shardings[0], arg_shapes[0]), (         supported_sharding(arg_shardings[0], arg_shapes[0]),) def infer_sharding_from_operands(arg_shapes, result_shape):     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return supported_sharding(arg_shardings[0], arg_shapes[0]) def supported_sharding_X(sharding, shape):     return NamedSharding(sharding.mesh, P(None, ""gpus"")) def partition_X(arg_shapes, result_shape):     result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return _fftX, supported_sharding_X(arg_shardings[0], arg_shapes[0]), (         supported_sharding_X(arg_shardings[0], arg_shapes[0]),) def infer_sharding_from_operands_X(arg_shapes, result_shape):     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return supported_sharding_X(arg_shardings[0], arg_shapes[0]) def fft(x):          def _fft_(x):         return _fft(x)     _fft_.def_partition(         infer_sharding_from_operands=infer_sharding_from_operands,         partition=partition)     return _fft_(x) def fft_X(x):          def _fft_(x):         return _fftX(x)     _fft_.def_partition(         infer_sharding_from_operands=infer_sharding_from_operands_X,         partition=partition_X)     return _fft_(x) ``` When I run this on a host with 2 GPUs (with CPUs all three outputs are the same), I get: ```text 20230731 18:28:38.496970: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 11.6 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. JAX output without JIT: [[64.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]] JAX output with JIT: [[32.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]] Reference output: [[64.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]  [ 0.+0.j  0.+0.j  0.+0.j  0.+0.j]] Are the jax (JIT) and scipy results the same? False ``` HLO  ``` HloModule jit_two_fft, is_scheduled=true, entry_computation_layout={(c64[2,4,4]{2,1,0})>c64[4,2,4]{2,1,0}} %fused_computation (param_0.1: c64[2,2,2,4]) > c64[2,4,4] {   %param_0.1 = c64[2,2,2,4]{3,2,0,1} parameter(0)   %bitcast.30 = c64[4,2,4]{2,1,0} bitcast(c64[2,2,2,4]{3,2,0,1} %param_0.1)   ROOT %transpose.5 = c64[2,4,4]{2,1,0} transpose(c64[4,2,4]{2,1,0} %bitcast.30), dimensions={1,2,0}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65} } %fused_computation.1 (param_0.4: c64[2,4,4]) > c64[2,2,2,4] {   %param_0.4 = c64[2,4,4]{2,1,0} parameter(0)   %bitcast.32 = c64[2,2,2,4]{3,2,1,0} bitcast(c64[2,4,4]{2,1,0} %param_0.4)   %transpose.6 = c64[2,2,2,4]{3,2,1,0} transpose(c64[2,2,2,4]{3,2,1,0} %bitcast.32), dimensions={1,0,2,3}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}   ROOT %bitcast.31 = c64[2,2,2,4]{3,2,0,1} bitcast(c64[2,2,2,4]{3,2,1,0} %transpose.6), metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65} } ENTRY %main.6_spmd (param: c64[2,4,4]) > c64[4,2,4] {   %param = c64[2,4,4]{2,1,0} parameter(0), sharding={devices=[2,1,1]0,1}   %fft.0 = c64[2,4,4]{2,1,0} fft(c64[2,4,4]{2,1,0} %param), fft_type=FFT, fft_length={4,4}, metadata={op_name=""jit(main)/jit(fft)/fft[fft_type=FftType.FFT fft_lengths=(4, 4)]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=7}   %fusion.1 = c64[2,2,2,4]{3,2,0,1} fusion(c64[2,4,4]{2,1,0} %fft.0), kind=kLoop, calls=%fused_computation.1, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}   %alltoallstart = ((c64[2,2,2,4]{3,2,0,1}), c64[2,2,2,4]{3,2,0,1}) alltoallstart(c64[2,2,2,4]{3,2,0,1} %fusion.1), channel_id=1, replica_groups={{0,1}}, dimensions={1}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}, backend_config={""is_sync"":true}   %alltoalldone = c64[2,2,2,4]{3,2,0,1} alltoalldone(((c64[2,2,2,4]{3,2,0,1}), c64[2,2,2,4]{3,2,0,1}) %alltoallstart), channel_id=1, replica_groups={{0,1}}, dimensions={1}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}   %fusion = c64[2,4,4]{2,1,0} fusion(c64[2,2,2,4]{3,2,0,1} %alltoalldone), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65}   %fft.1 = c64[2,4,4]{2,1,0} fft(c64[2,4,4]{2,1,0} %fusion), fft_type=FFT, fft_length={4}, metadata={op_name=""jit(main)/jit(fft)/fft[fft_type=FftType.FFT fft_lengths=(4,)]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=11}   ROOT %transpose.4 = c64[4,2,4]{2,1,0} transpose(c64[2,4,4]{2,1,0} %fft.1), dimensions={2,0,1}, frontend_attributes={fingerprint_before_lhs=""902ad6c9909648982a9ff9203157ab0d""}, metadata={op_name=""jit(two_fft)/jit(main)/custom_partitioning[partition= propagate_user_sharding=None infer_sharding_from_operands= decode_shardings=True in_tree=PyTreeDef((*,)) out_tree=PyTreeDef(*) static_args=[]]"" source_file=""/jaxtesting/sharded_fft_minimal.py"" source_line=65} } ```     What jax/jaxlib version are you using? jax==0.4.14, jaxlib==0.4.14+cuda11.cudnn86  Which accelerator(s) are you using? GPU (2x Nvidia A40)  Additional system info Python 3.9.15, AlmaLinux 8.5  NVIDIA GPU info ```text ++  ++ ```",2023-07-31T16:35:32Z,bug XLA NVIDIA GPU,closed,0,11,https://github.com/jax-ml/jax/issues/16909,"I also tried running the benchmark from cuFFTMp directly, but needed to change a few minor things so that imports work with my jax version and multiple GPUs on one host.  When modifying this code block https://github.com/NVIDIA/CUDALibrarySamples/blob/f299986ecc990c49dce295e345f9e3ad95ded188/cuFFTMp/JAX_FFT/tests/fft_test.pyL117L122 to  ```python start = time.time() print(""input:"") print(dinput) x = dinput for _ in range(cycles):     x = fwd_bwd_bench(x)     print(""x:"")     print(x)     print(""numpy:"")     print(numpy.fft.ifftn(numpy.fft.fftn(input))) doutput = x.block_until_ready() stop = time.time() ``` I also get different results between numpy and jax FFT: ```text input: [[[1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]]  [[1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]]] x: [[[1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]]  [[0.+0.j 0.+0.j 0.+0.j 0.+0.j]   [0.+0.j 0.+0.j 0.+0.j 0.+0.j]   [0.+0.j 0.+0.j 0.+0.j 0.+0.j]   [0.+0.j 0.+0.j 0.+0.j 0.+0.j]]] numpy: [[[1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]]  [[1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]   [1.+0.j 1.+0.j 1.+0.j 1.+0.j]]] ``` and the same results when replacing `fwd_bwd_pjit` with `fwd_bwd`","Another test shows that the incorrect result differs strongly by datatype: With the above code updated to  ```python  jax.config.update(""jax_enable_x64"", True) xs = jax.random.normal(     jax.random.PRNGKey(0),     (size, size, size),     dtype=jax.numpy.complex64 ) ``` I get  ```text JAX output without JIT: [[ 4.6250744  +1.655502j   0.8575981  2.2615793j   8.510313   +7.1343594j   7.286973   5.468896j  ]  [ 5.747694   +2.854593j    8.017227  12.640258j    2.4195366  +9.599846j    2.3991098  6.2334623j ]  [ 0.05215216 +7.5721874j  8.085979   3.6076527j   0.29224873 +0.36597013j 7.8748264  5.073735j  ]  [ 6.603516  +12.5646515j  0.58227634 +5.071442j   9.074454   +0.47400856j  4.6850696  +5.99322j   ]] JAX output with JIT: [[2.0321913e+00+1.5058144e+01j  4.2970805e+009.6251125e+00j   7.3626089e+00+1.5319314e+01j  1.4256954e+016.5427341e+00j]  [ 1.2095287e+01+1.6869164e+01j 6.7879229e+00+9.4263554e03j   2.0586638e+00+9.6544361e+00j 7.8202343e+009.4283175e01j]  [6.4622705e+336.4622705e+33j 6.4622705e+336.4622705e+33j   6.4622705e+336.4622705e+33j 6.4622705e+336.4622705e+33j]  [6.4622705e+336.4622705e+33j 6.4622705e+336.4622705e+33j   6.4622705e+336.4622705e+33j 6.4622705e+336.4622705e+33j]] Reference output: [[ 4.6250744  +1.6555014j  0.8575978  2.261579j   8.510313   +7.1343594j   7.2869735  5.4688964j ]  [ 5.747694   +2.854593j    8.017227  12.640258j    2.4195373  +9.599846j    2.3991096  6.2334623j ]  [ 0.05215228 +7.5721874j  8.085979   3.6076522j   0.2922498  +0.3659699j  7.8748283  5.0737348j ]  [ 6.6035166 +12.5646515j  0.5822754  +5.0714417j   9.074454   +0.47400856j  4.6850705  +5.9932194j ]] ``` And with 64bit: ```python jax.config.update(""jax_enable_x64"", True) xs = jax.random.normal(     jax.random.PRNGKey(0),     (size, size, size),     dtype=jax.numpy.complex128 ) ``` it becomes: ```text JAX output without JIT: [[  4.53110121 5.42857443j   2.823367   9.09603099j     7.2847458211.47654029j   1.99185486 +1.74253872j]  [  6.93847013 2.95786878j 16.05594063 3.47812555j    5.68283733 1.55930829j  2.23199019 +6.66552633j]  [  2.74730892 2.34144991j 12.500137   3.62033921j     7.45987686 +4.74912884j  10.95039256 4.19533421j]  [ 4.09811938 +3.48808763j  8.19007414 +1.73404671j    0.80590044 4.69896172j   2.15027043 9.17528198j]] JAX output with JIT: [[ 8.40507180e+0002.43439964e+000j  1.76275197e+0011.98252892e+000j    3.77844995e+0009.01459259e+000j 5.32531468e+000+2.21369021e+000j]  [ 4.85005727e+0008.36303426e+000j 1.43999323e+001+6.71514992e+000j    1.23173687e+0015.39414742e+000j  8.30568192e+000+5.77396391e+000j]  [1.08187595e+301+1.84421548e+022j  2.34007863e+2663.90630795e+173j    1.00342272e+2655.57232292e+000j 7.10074304e+0002.21372768e+165j]  [1.31710865e+2021.51260176e+229j 7.92660879e+225+5.34813502e+222j   1.70851517e+132+6.65772753e+138j 6.18259311e+144+4.89858632e+000j]] Reference output: [[  4.53110121 5.42857443j   2.823367   9.09603099j     7.2847458211.47654029j   1.99185486 +1.74253872j]  [  6.93847013 2.95786878j 16.05594063 3.47812555j    5.68283733 1.55930829j  2.23199019 +6.66552633j]  [  2.74730892 2.34144991j 12.500137   3.62033921j     7.45987686 +4.74912884j  10.95039256 4.19533421j]  [ 4.09811938 +3.48808763j  8.19007414 +1.73404671j    0.80590044 4.69896172j   2.15027043 9.17528198j]] ``` Also checking the full 4x4x4 array in the first case shows quite a few entries that are zero or repeating: Full Array  ```text [[[2.0321913e+00+1.5058144e+01j  4.2970805e+009.6251125e+00j    7.3626089e+00+1.5319314e+01j  1.4256954e+016.5427341e+00j]   [ 1.2095287e+01+1.6869164e+01j 6.7879229e+00+9.4263554e03j    2.0586638e+00+9.6544361e+00j 7.8202343e+009.4283175e01j]   [6.4622705e+336.4622705e+33j 6.4622705e+336.4622705e+33j    6.4622705e+336.4622705e+33j 6.4622705e+336.4622705e+33j]   [6.4622705e+336.4622705e+33j 6.4622705e+336.4622705e+33j    6.4622705e+336.4622705e+33j 6.4622705e+336.4622705e+33j]]  [[3.7904797e+001.9218649e+01j  4.0303049e+00+1.2647402e01j     2.0930943e+00+4.8396072e+00j  9.0125875e+003.9036689e+00j]   [6.8532133e01+1.6853342e+00j 5.8682117e+002.1767204e+01j     1.4586600e+01+1.3182940e+01j  1.1533028e+011.8864393e+01j]   [6.4622705e+33+0.0000000e+00j 6.4622705e+33+0.0000000e+00j    6.4622705e+33+0.0000000e+00j 6.4622705e+33+0.0000000e+00j]   [6.4622705e+33+0.0000000e+00j 6.4622705e+33+0.0000000e+00j    6.4622705e+33+0.0000000e+00j 6.4622705e+33+0.0000000e+00j]]  [[ 1.9365990e+01+1.1662802e+01j  7.6619020e+00+8.4834232e+00j    4.8324118e+009.3418112e+00j 4.5125694e+001.8177962e01j]   [4.2745185e+00+4.4168015e+00j  2.2013352e+00+7.6215229e+00j    3.9565558e+00+1.8024149e+01j 9.3683624e+00+8.8611822e+00j]   [ 0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j     0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j]   [ 0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j     0.0000000e+00+0.0000000e+00j  0.0000000e+00+0.0000000e+00j]]  [[5.2897873e+004.3190117e+00j 1.6217491e+01+3.6025798e01j     1.2814125e+011.4356110e+01j 1.5521908e011.3899309e+01j]   [ 1.0835196e+016.8513775e01j  1.5637071e+00+1.4616768e+01j     1.1689168e+015.9961653e01j  2.1295629e+004.7527847e+00j]   [ 0.0000000e+006.4622705e+33j  0.0000000e+006.4622705e+33j     0.0000000e+006.4622705e+33j  0.0000000e+006.4622705e+33j]   [ 0.0000000e+006.4622705e+33j  0.0000000e+006.4622705e+33j     0.0000000e+006.4622705e+33j  0.0000000e+006.4622705e+33j]]] ```   ","Okay, I just tried out version 0.4.8 (as it is the oldest one I could easily install with `jax[cuda11_pip]`) and interestingly enough, it works correctly. I just needed to slightly change the above script to adapt to the older function arguments: ```diff diff git a/sharded_fft_minimal.py b/sharded_fft_minimal_old.py index 3c870a4..dd907f5 100644  a/sharded_fft_minimal.py +++ b/sharded_fft_minimal_old.py @@ 15,15 +15,12 @@ def supported_sharding(sharding, shape):      return NamedSharding(sharding.mesh, P(""gpus"",None)) def partition(arg_shapes, result_shape):     result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes) +def partition(arg_shapes, arg_shardings, result_shape, result_sharding):      return _fft, supported_sharding(arg_shardings[0], arg_shapes[0]), (          supported_sharding(arg_shardings[0], arg_shapes[0]),) def infer_sharding_from_operands(arg_shapes, result_shape):     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes) +def infer_sharding_from_operands(arg_shapes, arg_shardings, shape):      return supported_sharding(arg_shardings[0], arg_shapes[0]) @@ 31,15 +28,12 @@ def supported_sharding_X(sharding, shape):      return NamedSharding(sharding.mesh, P(None, ""gpus"")) def partition_X(arg_shapes, result_shape):     result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes) +def partition_X(arg_shapes, arg_shardings, result_shape, result_sharding):      return _fftX, supported_sharding_X(arg_shardings[0], arg_shapes[0]), (          supported_sharding_X(arg_shardings[0], arg_shapes[0]),) def infer_sharding_from_operands_X(arg_shapes, result_shape):     arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes) +def infer_sharding_from_operands_X(arg_shapes, arg_shardings, shape):      return supported_sharding_X(arg_shardings[0], arg_shapes[0]) ``` Testing all newer versions seems to indicate that already 0.4.9 is broken in the same way. So I guess something in https://github.com/google/jax/compare/jaxv0.4.8...jaxv0.4.9 or a different XLA version (maybe https://github.com/openxla/xla/compare/dad64948516e3672b3e2518945831a70b5e90b81...79ca8d03c296ede04dc9a86ce9dde79ed909dda8) causes this issue.","I finally got jax to compile from source on my system and was able to bisect the exact xlacommit that broke this code: It turns out that using jax 0.4.9 and jaxlib 0.4.7, everything works. But with jaxlib 0.4.9 the above issue occurs. And it turns out the issue is in XLA as compiling jaxlib 0.4.9 with https://github.com/openxla/xla/commit/dad64948516e3672b3e2518945831a70b5e90b81 is broken (as expected), but the same jax/jaxlib with https://github.com/openxla/xla/commit/79ca8d03c296ede04dc9a86ce9dde79ed909dda8 works. From there I could bisect all commits in https://github.com/openxla/xla/compare/79ca8d03c296ede04dc9a86ce9dde79ed909dda8...dad64948516e3672b3e2518945831a70b5e90b81 and it turns out the first broken commit is https://github.com/openxla/xla/commit/8680d1bad06d45396b171793d936ea71065c7107 or https://github.com/openxla/xla/pull/2778 (with https://github.com/openxla/xla/commit/9d244bfa91793c4c58ddf82ffaebb5c505b2da07 being the last working commit). Test Details  ```text $ pip install tmp/jaxlib9d244bfa0.4.9cp39cp39manylinux2014_x86_64.whl force [...] $ python test_minimal_old.py jax version 0.4.9 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1694451732.594824 1486184 tfrt_cpu_pjrt_client.cc:445] TfrtCpuClient created. 4.7570 / 4.7570: start 6.1209 / 1.3638: fft_jit run 6.6493 / 0.5284: fft finished JAX output with JIT: [[ 8.3883450e+06   +0.j        3.3468820e+02 +386.0061j    6.7245923e+022043.2562j   1.9714886e+021012.20386j ]  [6.1117786e+02 +224.31882j   4.6303351e+02 974.9061j    3.8815759e+02 +610.24756j   1.1820348e+03  40.36635j ]  [ 3.5503680e+02 305.97424j  1.9115245e+021535.6515j    4.1164917e+01  +86.547005j 1.4985818e+02 +250.45987j ]  [8.6542932e+02 +775.30206j  2.0335649e+02+1296.3466j   1.0643232e+03 +225.98059j  1.3763538e+03 738.6568j  ]] Reference output: [[ 8.38834515e+06   0.j          3.34689661e+02 +386.00554251j    6.72462030e+022043.25471474j 1.97148671e+021012.20381339j]  [6.11177902e+02 +224.31882354j  4.63033443e+02 974.90613044j    3.88157682e+02 +610.24784839j  1.18203458e+03  40.36616595j]  [ 3.55036784e+02 305.97435746j 1.91152509e+021535.65147145j    4.11648390e+01  +86.54715244j 1.49858053e+02 +250.45974427j]  [8.65429421e+02 +775.30192023j 2.03356435e+02+1296.34653685j   1.06432337e+03 +225.98059439j 1.37635384e+03 738.65675133j]] Are the jax (JIT) and scipy results the same? True I0000 00:00:1694451739.560787 1486184 tfrt_cpu_pjrt_client.cc:448] TfrtCpuClient destroyed. $ pip install tmp/jaxlib8680d1b0.4.9cp39cp39manylinux2014_x86_64.whl force [...] $ python test_minimal_old.py jax version 0.4.9 WARNING: All log messages before absl::InitializeLog() is called are written to STDERR I0000 00:00:1694452019.944300 1487907 tfrt_cpu_pjrt_client.cc:445] TfrtCpuClient created. 4.7255 / 4.7255: start 7.4376 / 2.7121: fft_jit run 7.9355 / 0.4979: fft finished JAX output with JIT: [[ 6.2947845e+06   +0.j       2.1749468e+03 +319.78992j    1.2456562e+03+2438.8867j   9.2939832e+02 +329.0697j ]  [8.6273816e+02 811.44666j  1.4054868e+02 +585.1288j   6.2169360e+021246.3406j  2.4191275e+02+1264.7472j ]  [7.0989075e+021084.596j    9.6910162e+02 591.3728j   7.3031091e+02 601.01984j  9.0614209e+02  +80.99515j]  [ 6.9135394e+02 +300.6054j   7.7462994e+021490.218j   1.3380891e+03 425.75577j  8.3488062e+02 +615.5832j ]] Reference output: [[ 8.38968949e+06   0.j          3.53973220e+02 588.75971071j    6.06135765e+02 +857.65845664j  3.16912375e+02 +726.83423305j]  [1.28798081e+02 +133.87261708j 1.05350562e+03 +581.77241821j    2.56638558e+02 946.75897686j  3.17061442e+02 +238.45195598j]  [1.09668213e+031078.57225684j  3.88102428e+02 356.94145411j   1.12851382e+03 843.19431105j  7.73129891e+021721.19615227j]  [ 6.82739483e+02 +835.9835899j   8.84505658e+021299.3917609j   2.11410959e+03 +411.73759055j  4.84705677e+02 459.79164364j]] Are the jax (JIT) and scipy results the same? False I0000 00:00:1694452028.261661 1487907 tfrt_cpu_pjrt_client.cc:448] TfrtCpuClient destroyed. ```    (sidenote: I had to cherrypick https://github.com/openxla/xla/commit/be1cee3e4aeee55ea5487d15398d92d9e21e19b2 on every commit to make it compile on my setup) Unfortunately I don't understand XLA enough to know how removing that optimization pass broke jax here. Maybe  has a good idea here, I would be really thankful. But to doublecheck this is correct, I went back to https://github.com/openxla/xla/commit/dad64948516e3672b3e2518945831a70b5e90b81 and reverted https://github.com/openxla/xla/commit/8680d1bad06d45396b171793d936ea71065c7107 there and the resulting jaxlib (that should be identical to the released 0.4.9 apart from that change) indeed works correctly. Applying it to the latest jax/xla version doesn't seem that easy as the related code changed a bit since. And the CPU code still contains this optimization pass which might explain why this issue only occurs on GPUs: https://github.com/openxla/xla/blob/dad64948516e3672b3e2518945831a70b5e90b81/xla/service/cpu/cpu_compiler.ccL625","Thanks. AllToAll in XLA has a mode where it first splits all inputs into N chunks (N =  of devices) and then concatenates all Chunk[0] across all device on device 0, Chunk[1] across all devices on device 1 etc. XLA/GPU at some point did not support this in the backend, so the pass in question was used to decompose such AllToAlls into other collectives. However, XLA/GPU later added support for such AllToAlls in the backend, but the pass remained. The changes in question were to remove these expansion that was not needed.  I suspect this code is exposing a potential correctness issue in our AllToAll implementation with spit dims."," thanks for sharing your excellent work. I'm facing the same problem, but your post has saved me a lot of time.  thanks for the update. Do you know if anyone is working on the AllToAll issue you described?","I looked into this issue again as it is frustrating having to use an outdated jax version to get correct results. Starting with jax 0.4.16 the reproduction script has to be slightly adapted to accommodate to https://github.com/google/jax/pull/17072. ```diff diff git a/sharded_fft_minimal.py b/sharded_fft_minimal_newest.py index 6a44d14..17eb685 100644  a/sharded_fft_minimal.py +++ b/sharded_fft_minimal_newest.py @@ 15,14 +15,14 @@ def supported_sharding(sharding, shape):      return NamedSharding(sharding.mesh, P(""gpus"", None)) def partition(arg_shapes, result_shape): +def partition(mesh, arg_shapes, result_shape):      result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)      arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return _fft, supported_sharding(arg_shardings[0], arg_shapes[0]), ( +    return mesh, _fft, supported_sharding(arg_shardings[0], arg_shapes[0]), (          supported_sharding(arg_shardings[0], arg_shapes[0]),) def infer_sharding_from_operands(arg_shapes, result_shape): +def infer_sharding_from_operands(mesh, arg_shapes, result_shape):      arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)      return supported_sharding(arg_shardings[0], arg_shapes[0]) @@ 31,14 +31,14 @@ def supported_sharding_X(sharding, shape):      return NamedSharding(sharding.mesh, P(None, ""gpus"")) def partition_X(arg_shapes, result_shape): +def partition_X(mesh, arg_shapes, result_shape):      result_shardings = jax.tree_map(lambda x: x.sharding, result_shape)      arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)     return _fftX, supported_sharding_X(arg_shardings[0], arg_shapes[0]), ( +    return mesh, _fftX, supported_sharding_X(arg_shardings[0], arg_shapes[0]), (          supported_sharding_X(arg_shardings[0], arg_shapes[0]),) def infer_sharding_from_operands_X(arg_shapes, result_shape): +def infer_sharding_from_operands_X(mesh, arg_shapes, result_shape):      arg_shardings = jax.tree_map(lambda x: x.sharding, arg_shapes)      return supported_sharding_X(arg_shardings[0], arg_shapes[0]) ``` I checked all jax version since I created this issue and it turns out that while jax+jaxlib 0.4.18 still shows the exact same issue, 0.4.19 seems to give correct results. I will need to do more testing, but as nothing in the changelog seems related, I guess some change in XLA fixed this. kogsys You might also want to try this out again with jax+jaxlib 0.4.19","I strongly suspect the fix for https://github.com/google/jax/issues/18122, which is in 0.4.19, also fixed this issue.","Let me know if you can still reproduce, but I'm going to assume this is fixed!","After a bit of testing it seems like the bug is indeed solved. For everyone interested in 3d FFTs stumbling over this post, I have now also updated my reference code for the latest jax version: https://gist.github.com/Findus23/eb5ecb9f65ccf13152cda7c7e521cbdd","All working for me. Thank you both for all your help, this is a real enabler for anything requiring large FFTs.   , one small comment, the code in the link above still appears to be targeted at 0.4.9, as the partitioning arguments are the old style. For example: `def partition(arg_shapes, arg_shardings, result_shape, result_sharding):     ...` instead of: `def partition(mesh, arg_shapes, result_shape):     ...` I got it working with the mods you mentioned on October 23rd though. Thanks again!"
3353,"以下是一个github上的jax下的一个issue, 标题是(Fast path for device_put)， 内容是 ( Some background I've been using JAX for several years to accelerate computation of various scale. For large transformer models, for my use case, the improvement is around 1.5x to 2x compared to PyTorch (thus not quite obvious). For smaller but complex scientific models, this often leads to orders of magnitude improvement. For example, a recent workload runs on CPU and contains a lot of small computations. The jitted JAX function avoids the huge dispatch cost that PyTorch incurs. The workload also requires auto differentiation, thus Numba or Cython is not an option. In short, I was able to speed up the workload by 1000 times, from 20ms to 30us.  Problem Then I discovered that while the computation is fast, there was nontrivial overhead at JIT boundary. I used to prefer explicit conversion between `np.ndarray` and `jax.Array` at JIT boundary with `jax.device_put/jax.device_get` or `jnp.asarray/np.asarray`. But for my use case, the time cost of explicit conversion is even larger than the actual execution time! Some quick benchmark below (`batch_context` is a PyTree with 8 arrays) ```python  Method I used to prefer %timeit jax.device_put(batch_context)  134 µs ± 657 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)  An alternative %timeit jax.tree_util.tree_map(lambda x: jnp.asarray(x), batch_context)  345 µs ± 1.41 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) ``` A comparison with PyTorch ```python  Much faster even copied on both Numpy and PyTorch side %timeit jax.tree_util.tree_map(lambda x: torch.from_numpy(x.copy()).clone(), batch_context)  16.4 µs ± 21.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) ``` A potential fast path using some internals is much faster and comparable to PyTorch performance ```python from jax._src.lib import xla_client as xc from jax.core import ShapedArray from jax.sharding import SingleDeviceSharding device = jax.devices()[0]   CpuDevice(id=0) sharding = SingleDeviceSharding(device) def fast_numpy_to_device_array(x):     aval = ShapedArray(x.shape, x.dtype)     return xc.batched_device_put(aval, sharding, [x], [device], True) %timeit jax.tree_util.tree_map(fast_numpy_to_device_array, batch_context)  25.3 µs ± 35.8 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) ``` In addition, I found that directly passing PyTree with raw Numpy array leaves to the jitted function also works as expected, and only leads to negligible slowdown of serveral microseconds, which becomes the fastest way to invoke the jitted function.  Goal JAX gives me the impression that  while the composition of functional transform is elegant and the jitted execution is efficient, something like JIT dispatch is less optimally handled. The previously landing CPP dispatch path has improved the dispatch efficiency a lot, but for array conversion, there may still be room for improvement. Or at least, it could be confirmed and documented somewhere that for simple unsharded usecase, passing the Numpy array directly is faster. Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Fast path for device_put," Some background I've been using JAX for several years to accelerate computation of various scale. For large transformer models, for my use case, the improvement is around 1.5x to 2x compared to PyTorch (thus not quite obvious). For smaller but complex scientific models, this often leads to orders of magnitude improvement. For example, a recent workload runs on CPU and contains a lot of small computations. The jitted JAX function avoids the huge dispatch cost that PyTorch incurs. The workload also requires auto differentiation, thus Numba or Cython is not an option. In short, I was able to speed up the workload by 1000 times, from 20ms to 30us.  Problem Then I discovered that while the computation is fast, there was nontrivial overhead at JIT boundary. I used to prefer explicit conversion between `np.ndarray` and `jax.Array` at JIT boundary with `jax.device_put/jax.device_get` or `jnp.asarray/np.asarray`. But for my use case, the time cost of explicit conversion is even larger than the actual execution time! Some quick benchmark below (`batch_context` is a PyTree with 8 arrays) ```python  Method I used to prefer %timeit jax.device_put(batch_context)  134 µs ± 657 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)  An alternative %timeit jax.tree_util.tree_map(lambda x: jnp.asarray(x), batch_context)  345 µs ± 1.41 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) ``` A comparison with PyTorch ```python  Much faster even copied on both Numpy and PyTorch side %timeit jax.tree_util.tree_map(lambda x: torch.from_numpy(x.copy()).clone(), batch_context)  16.4 µs ± 21.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) ``` A potential fast path using some internals is much faster and comparable to PyTorch performance ```python from jax._src.lib import xla_client as xc from jax.core import ShapedArray from jax.sharding import SingleDeviceSharding device = jax.devices()[0]   CpuDevice(id=0) sharding = SingleDeviceSharding(device) def fast_numpy_to_device_array(x):     aval = ShapedArray(x.shape, x.dtype)     return xc.batched_device_put(aval, sharding, [x], [device], True) %timeit jax.tree_util.tree_map(fast_numpy_to_device_array, batch_context)  25.3 µs ± 35.8 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each) ``` In addition, I found that directly passing PyTree with raw Numpy array leaves to the jitted function also works as expected, and only leads to negligible slowdown of serveral microseconds, which becomes the fastest way to invoke the jitted function.  Goal JAX gives me the impression that  while the composition of functional transform is elegant and the jitted execution is efficient, something like JIT dispatch is less optimally handled. The previously landing CPP dispatch path has improved the dispatch efficiency a lot, but for array conversion, there may still be room for improvement. Or at least, it could be confirmed and documented somewhere that for simple unsharded usecase, passing the Numpy array directly is faster. Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2023-07-31T12:58:01Z,enhancement,open,2,4,https://github.com/jax-ml/jax/issues/16905,"Yes, we're aware `device_put` is slower than it should be and looking at speeding it up soon. One possible workaround for now: try `jit(lambda x: x)` or `jit(lambda x: x+0)`. The latter at least will force us to use the (probably faster) `jit` dispatch path.","Yes, I was thinking of ""abusing"" jit for this purpose. But I only tried `jit(lambda x: x)`, and it turned out to be even slower! Thus I did not include that solution above. I did not realize that some ""clever"" optimization occurred (maybe by comparing Tracer identity?), and that somehow slowed down the execution. The extra `+ 0` or copy did the trick, making it the fastest explicit conversion solution (somewhat amusing). The extra benchmark: ```python f1 = jax.jit(lambda x: x); f1(batch_context) f2 = jax.jit(lambda x: jax.tree_util.tree_map(jnp.copy, x)); f2(batch_context) %timeit f1(batch_context)  234 µs ± 2.55 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) %timeit f2(batch_context)  12.3 µs ± 239 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each) ```","Yes. If the `jaxpr` of the `jit` is trivial (has no equations), then we hit a ""fast"" path. But as you note, the ""fast"" path is no longer as fast as the path that actually dispatches a computation. We put in much more work optimizing the `jit` dispatch path to date and it shows!","You""'re aware device_put is slower than it should be and looking at speeding it up soon"" do you have a work in progress design or any progress on this? Thanks."
1407,"以下是一个github上的jax下的一个issue, 标题是(Possible BUG: Different behavior of 64-bit integers when used inside Sharding ops)， 内容是 ( Description Hey, I'm trying to vmap a function over a sharded integer array. The arrays are distributed over 8 TPU cores and have divisible dimensions. In the code below, when I run the function, ``jax.vmap(simple_op)`` with an int8 array, I get the following output. ```python import jax import jax.numpy as jnp from jax.experimental import mesh_utils from jax.sharding import PositionalSharding sharding8 = PositionalSharding(mesh_utils.create_device_mesh((8,))) viz = jax.debug.visualize_array_sharding a = jnp.arange(64,dtype=jnp.uint8) a = jax.device_put(a,sharding8) def simple_op(a):     return a+1 out = jax.vmap(simple_op)(a) ```  However, when I use ``jnp.uint64`` instead. I get a different result  The corresponding code  ```python jax.config.update(""jax_enable_x64"", True) a = jnp.arange(64,dtype=jnp.int64) a = jax.device_put(a,sharding8) out = jax.vmap(simple_op)(a) ``` This is a very strange bug. What could be the possible cause?   What jax/jaxlib version are you using? jax==0.4.6, jaxlib==0.4.6  Which accelerator(s) are you using? TPU  Additional system info Python 3.8.16 (Kaggle Environment)  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Possible BUG: Different behavior of 64-bit integers when used inside Sharding ops," Description Hey, I'm trying to vmap a function over a sharded integer array. The arrays are distributed over 8 TPU cores and have divisible dimensions. In the code below, when I run the function, ``jax.vmap(simple_op)`` with an int8 array, I get the following output. ```python import jax import jax.numpy as jnp from jax.experimental import mesh_utils from jax.sharding import PositionalSharding sharding8 = PositionalSharding(mesh_utils.create_device_mesh((8,))) viz = jax.debug.visualize_array_sharding a = jnp.arange(64,dtype=jnp.uint8) a = jax.device_put(a,sharding8) def simple_op(a):     return a+1 out = jax.vmap(simple_op)(a) ```  However, when I use ``jnp.uint64`` instead. I get a different result  The corresponding code  ```python jax.config.update(""jax_enable_x64"", True) a = jnp.arange(64,dtype=jnp.int64) a = jax.device_put(a,sharding8) out = jax.vmap(simple_op)(a) ``` This is a very strange bug. What could be the possible cause?   What jax/jaxlib version are you using? jax==0.4.6, jaxlib==0.4.6  Which accelerator(s) are you using? TPU  Additional system info Python 3.8.16 (Kaggle Environment)  NVIDIA GPU info _No response_",2023-07-31T08:29:00Z,bug needs info,closed,0,2,https://github.com/jax-ml/jax/issues/16903,Can you upgrade to the latest jax and jaxlib versions and see if you can still repro it?,I cannot repro with jax 0.4.14
4935,"以下是一个github上的jax下的一个issue, 标题是(XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed.)， 内容是 ( Description The OUTPUT: [1]  20230731 01:53:45.016563: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:427] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. [2] XlaRuntimeError                           Traceback (most recent call last) Cell In[4], line 29      26 model = trainer.make_model(nmask)      28 lr_fn, opt = trainer.make_optimizer(steps_per_epoch=len(train_dl)) > 29 state = trainer.create_train_state(jax.random.PRNGKey(0), model, opt)      30 state = checkpoints.restore_checkpoint(ckpt.parent, state) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/random.py:137, in PRNGKey(seed)     134 if np.ndim(seed):     135   raise TypeError(""PRNGKey accepts a scalar seed, but was given an array of""     136                   f""shape {np.shape(seed)} != (). Use jax.vmap for batching"") > 137 key = prng.seed_with_impl(impl, seed)     138 return _return_prng_keys(True, key) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:320, in seed_with_impl(impl, seed)     319 def seed_with_impl(impl: PRNGImpl, seed: Union[int, Array]) > PRNGKeyArrayImpl: > 320   return random_seed(seed, impl=impl) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:734, in random_seed(seeds, impl)     732 else:     733   seeds_arr = jnp.asarray(seeds) > 734 return random_seed_p.bind(seeds_arr, impl=impl) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:380, in Primitive.bind(self, *args, **params)     377 def bind(self, *args, **params):     378   assert (not config.jax_enable_checks or     379           all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 380   return self.bind_with_trace(find_top_trace(args), args, params) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:383, in Primitive.bind_with_trace(self, trace, args, params)     382 def bind_with_trace(self, trace, args, params): > 383   out = trace.process_primitive(self, map(trace.full_raise, args), params)     384   return map(full_lower, out) if self.multiple_results else full_lower(out) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:790, in EvalTrace.process_primitive(self, primitive, tracers, params)     789 def process_primitive(self, primitive, tracers, params): > 790   return primitive.impl(*tracers, **params) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:746, in random_seed_impl(seeds, impl)     744 .def_impl     745 def random_seed_impl(seeds, *, impl): > 746   base_arr = random_seed_impl_base(seeds, impl=impl)     747   return PRNGKeyArrayImpl(impl, base_arr) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:751, in random_seed_impl_base(seeds, impl)     749 def random_seed_impl_base(seeds, *, impl):     750   seed = iterated_vmap_unary(seeds.ndim, impl.seed) > 751   return seed(seeds) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:980, in threefry_seed(seed)     968 def threefry_seed(seed: typing.Array) > typing.Array:     969   """"""Create a single raw threefry PRNG key from an integer seed.     970      971   Args:    (...)     978     first padding out with zeros).     979   """""" > 980   return _threefry_seed(seed)     [... skipping hidden 12 frame] File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/dispatch.py:463, in backend_compile(backend, module, options, host_callbacks)     458   return backend.compile(built_c, compile_options=options,     459                          host_callbacks=host_callbacks)     460  Some backends don't have `host_callbacks` option yet     461  TODO(sharadmv): remove this fallback when all backends allow `compile`     462  to take in `host_callbacks` > 463 return backend.compile(built_c, compile_options=options) XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. `  What jax/jaxlib version are you using? jax0.4.10, jaxlib0.4.10+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info python3.11.4, Ubuntu22.04, cuda11.7,cudnn86  NVIDIA GPU info `++  ++ `)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed.," Description The OUTPUT: [1]  20230731 01:53:45.016563: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:427] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. [2] XlaRuntimeError                           Traceback (most recent call last) Cell In[4], line 29      26 model = trainer.make_model(nmask)      28 lr_fn, opt = trainer.make_optimizer(steps_per_epoch=len(train_dl)) > 29 state = trainer.create_train_state(jax.random.PRNGKey(0), model, opt)      30 state = checkpoints.restore_checkpoint(ckpt.parent, state) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/random.py:137, in PRNGKey(seed)     134 if np.ndim(seed):     135   raise TypeError(""PRNGKey accepts a scalar seed, but was given an array of""     136                   f""shape {np.shape(seed)} != (). Use jax.vmap for batching"") > 137 key = prng.seed_with_impl(impl, seed)     138 return _return_prng_keys(True, key) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:320, in seed_with_impl(impl, seed)     319 def seed_with_impl(impl: PRNGImpl, seed: Union[int, Array]) > PRNGKeyArrayImpl: > 320   return random_seed(seed, impl=impl) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:734, in random_seed(seeds, impl)     732 else:     733   seeds_arr = jnp.asarray(seeds) > 734 return random_seed_p.bind(seeds_arr, impl=impl) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:380, in Primitive.bind(self, *args, **params)     377 def bind(self, *args, **params):     378   assert (not config.jax_enable_checks or     379           all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 380   return self.bind_with_trace(find_top_trace(args), args, params) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:383, in Primitive.bind_with_trace(self, trace, args, params)     382 def bind_with_trace(self, trace, args, params): > 383   out = trace.process_primitive(self, map(trace.full_raise, args), params)     384   return map(full_lower, out) if self.multiple_results else full_lower(out) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:790, in EvalTrace.process_primitive(self, primitive, tracers, params)     789 def process_primitive(self, primitive, tracers, params): > 790   return primitive.impl(*tracers, **params) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:746, in random_seed_impl(seeds, impl)     744 .def_impl     745 def random_seed_impl(seeds, *, impl): > 746   base_arr = random_seed_impl_base(seeds, impl=impl)     747   return PRNGKeyArrayImpl(impl, base_arr) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:751, in random_seed_impl_base(seeds, impl)     749 def random_seed_impl_base(seeds, *, impl):     750   seed = iterated_vmap_unary(seeds.ndim, impl.seed) > 751   return seed(seeds) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:980, in threefry_seed(seed)     968 def threefry_seed(seed: typing.Array) > typing.Array:     969   """"""Create a single raw threefry PRNG key from an integer seed.     970      971   Args:    (...)     978     first padding out with zeros).     979   """""" > 980   return _threefry_seed(seed)     [... skipping hidden 12 frame] File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/dispatch.py:463, in backend_compile(backend, module, options, host_callbacks)     458   return backend.compile(built_c, compile_options=options,     459                          host_callbacks=host_callbacks)     460  Some backends don't have `host_callbacks` option yet     461  TODO(sharadmv): remove this fallback when all backends allow `compile`     462  to take in `host_callbacks` > 463 return backend.compile(built_c, compile_options=options) XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. `  What jax/jaxlib version are you using? jax0.4.10, jaxlib0.4.10+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info python3.11.4, Ubuntu22.04, cuda11.7,cudnn86  NVIDIA GPU info `++  ++ `",2023-07-31T08:03:53Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16901,"The error message says what's wrong: ```  Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0. CuDNN ``` You installed a version of `jax` that needs CuDNN 8.6, but CuDNN 8.5 was found. I suggest reinstalling using the `cuda11_pip` or `cuda12_pip` packages, in a fresh virtual environment. Hope that helps!", Thank you so much！
785,"以下是一个github上的jax下的一个issue, 标题是([WIP] Implement `scipy.spatial.distance`)， 内容是 (Partially addresses CC(Implement scipy.spatial). I saw that there's a similar PR CC(Spatial Distances), but it looks like it's inactive. Todo:  [x] Metric functions  [ ] `scipy.spatial.distance.cdist`  [ ] `scipy.spatial.distance.pdist`  [ ] `scipy.spatial.distance.squareform`  [ ] `scipy.spatial.distance.directed_hausdorff`  [ ] Utility functions (`is_valid_dm`, `is_valid_y`, `num_obs_dm`, `num_obs_y`)  [ ] Clean up metric functions  [ ] `rogerstanimono` and `sokalmichener` are the same? https://github.com/scipy/scipy/issues/2011  [ ] Add tests  [ ] Add docs)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[WIP] Implement `scipy.spatial.distance`,"Partially addresses CC(Implement scipy.spatial). I saw that there's a similar PR CC(Spatial Distances), but it looks like it's inactive. Todo:  [x] Metric functions  [ ] `scipy.spatial.distance.cdist`  [ ] `scipy.spatial.distance.pdist`  [ ] `scipy.spatial.distance.squareform`  [ ] `scipy.spatial.distance.directed_hausdorff`  [ ] Utility functions (`is_valid_dm`, `is_valid_y`, `num_obs_dm`, `num_obs_y`)  [ ] Clean up metric functions  [ ] `rogerstanimono` and `sokalmichener` are the same? https://github.com/scipy/scipy/issues/2011  [ ] Add tests  [ ] Add docs",2023-07-28T11:55:50Z,,closed,1,3,https://github.com/jax-ml/jax/issues/16879,"Hi  thanks for working on this! Just a note: it looks like the implementations are copypasted from the scipy source code. This is problematic from a code licensing standpoint – for this sort of work I'd recommend not referencing the scipy source, but rather implementing the metrics from scratch, using unit tests to ensure that the implementations produce similar results."," Hi, thanks for the feedback! Makes sense, I'll take a second pass at this and implement the metrics from scratch.","Hi again: we've done some thinking about the scope of `jax.scipy` longterm, and the consensus was that `scipy.spatial` is out of scope. See https://jax.readthedocs.io/en/latest/jep/18137numpyscipyscope.html for details. Thanks so much for the contribution, and I'm sorry we weren't able to merge your code!"
1209,"以下是一个github上的jax下的一个issue, 标题是(Introduce version 8 of XlaCallModule.)， 内容是 (Introduce version 8 of XlaCallModule. Previously, XlaCallModule was running the shape refinement pass for all compilations, even if the module did not use shape polymorphism. Currently shape refinement changes the structure of the module, through inlining and constant folding all integer operations. This complicates debugging because the HLO dump is very different than the one from JAX native executions. Starting with version 8, we run shape refinement only if the module contains a boolean module attribute jax.uses_shape_polymorphism=true. I think it makes sense to put this flag as a module attribute, rather than as a TF op attribute, because the same processing will be needed when the module is executed from JAX. This attribute is not yet populated by the JAX exporter. As part of this change we moved the error check for the number of invocation arguments from RefineDynamicShapes to LoadAndPreprocessModule. This required adding a couple more arguments to the loader constructor.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Introduce version 8 of XlaCallModule.,"Introduce version 8 of XlaCallModule. Previously, XlaCallModule was running the shape refinement pass for all compilations, even if the module did not use shape polymorphism. Currently shape refinement changes the structure of the module, through inlining and constant folding all integer operations. This complicates debugging because the HLO dump is very different than the one from JAX native executions. Starting with version 8, we run shape refinement only if the module contains a boolean module attribute jax.uses_shape_polymorphism=true. I think it makes sense to put this flag as a module attribute, rather than as a TF op attribute, because the same processing will be needed when the module is executed from JAX. This attribute is not yet populated by the JAX exporter. As part of this change we moved the error check for the number of invocation arguments from RefineDynamicShapes to LoadAndPreprocessModule. This required adding a couple more arguments to the loader constructor.",2023-07-21T15:34:04Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16814
535,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Document the JAX serialization version numbers.)， 内容是 (Previously this was documented in code comments in TF source code: https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+path%3Axla_call_module+%22int+VERSION_MAXIMUM_SUPPORTED%22&type=code We want to establish a new source of truth for this. We will then update the code comments in TF source.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Document the JAX serialization version numbers.,Previously this was documented in code comments in TF source code: https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+path%3Axla_call_module+%22int+VERSION_MAXIMUM_SUPPORTED%22&type=code We want to establish a new source of truth for this. We will then update the code comments in TF source.,2023-07-20T08:59:29Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16802,  PTAL
430,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Bump the default JAX serialization version to 7.)， 内容是 ([jax2tf] Bump the default JAX serialization version to 7. This enables shape assertion checking, the support for which landed in XlaCallModule on July 12th, 2023. See the CHANGELOG for details.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Bump the default JAX serialization version to 7.,"[jax2tf] Bump the default JAX serialization version to 7. This enables shape assertion checking, the support for which landed in XlaCallModule on July 12th, 2023. See the CHANGELOG for details.",2023-07-20T05:23:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16800
598,"以下是一个github上的jax下的一个issue, 标题是(Refactor opaque dtype implementation.)， 内容是 (This makes it closer to numpy, with `dtypes.OpaqueDtype` analogous to `np.dtype`, and `dtypes.opaque` analogous to `np.numeric`. This will let us replace the `dtypes.is_opaque_dtype(dtype)` function with `jnp.issubdtype(dtype, dtypes.opaque)`. This underlying change will make the goal in CC(custom prng: introduce mechanism to identify key arrays by dtype) much easier to realize.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Refactor opaque dtype implementation.,"This makes it closer to numpy, with `dtypes.OpaqueDtype` analogous to `np.dtype`, and `dtypes.opaque` analogous to `np.numeric`. This will let us replace the `dtypes.is_opaque_dtype(dtype)` function with `jnp.issubdtype(dtype, dtypes.opaque)`. This underlying change will make the goal in CC(custom prng: introduce mechanism to identify key arrays by dtype) much easier to realize.",2023-07-19T23:00:05Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16795
3273,"以下是一个github上的jax下的一个issue, 标题是(unsafe_rbg + vmap --> 10x slow down)， 内容是 ( Description unsafe_rbg is advertised as the solution for performance issues for rng, but it has surprising pessimization in the presence of vmap, more than 10x in this script, but more like 4x in real use. The key is `vmap(loss)`, where loss calls the RNG. This is with `LIBTPU_INIT_ARGS=xla_tpu_spmd_rng_bit_generator_unsafe=true` Times: * vmap, no dropout: 0.057 * vmap, dropout, threefry: 0.092 * vmap, dropout, unsafe: 0.736  (!!!!!) * no vmap, no dropout: 0.057 * no vmap, dropout, threefy: 0.088 * no vmap, dropout, unsafe: 0.075 ```  import functools import time import jax import jax.numpy as jnp import numpy as onp from jax.sharding import Mesh, NamedSharding, PartitionSpec batch_size = 256 seq_len = 2048 embed_size = 256 vocab_size = 2000 num_layers = 20 pdrop = 0.1 USE_VMAP = True USE_UNSAFE_RBG = True mesh = Mesh(onp.array(jax.devices()), (""dp"",)) if USE_UNSAFE_RBG:     jax.config.update(""jax_default_prng_impl"", ""unsafe_rbg"") else:     jax.config.update(""jax_threefry_partitionable"", True) with mesh:     key = jax.random.PRNGKey(0)     def model(tokens, key):         embed = jnp.take(jnp.ones((vocab_size, embed_size)), tokens, axis=0)          dumb fake gpt2 attn         for i in range(0, num_layers):             attn = jnp.einsum(""...ld,...kd>...lk"", embed, embed)             if pdrop > 0.0:                 key, subkey = jax.random.split(key)                 dout = jax.random.bernoulli(subkey, pdrop, shape=attn.shape)                 attn = jnp.where(dout, jnp.zeros_like(attn), attn)             attn = jax.nn.softmax(attn, axis=1)             embed = jnp.einsum(""...ld,...lk>...kd"", attn, embed)         out = jnp.einsum(""...ld,...kd>...lk"", embed, jnp.ones((vocab_size, embed_size)))         return out     def compute_loss(example, key):         pred_y = model(example, key=key)         return jnp.mean(pred_y)     def compute_loss_vmap(examples, key):         key = jax.random.split(key, batch_size)         per_ex_loss = jax.vmap(compute_loss)(examples, key)         return jnp.mean(per_ex_loss)     if USE_VMAP:         compute_loss_pjit = jax.jit(compute_loss_vmap)     else:         compute_loss_pjit = jax.jit(compute_loss)      i still honestly find the way to turn a ""replicated"" array like batch into a sharded array to be a bit confusing     batch = jnp.ones((batch_size, seq_len), dtype=jnp.int32)     batch = jax.make_array_from_callback(         (batch_size, seq_len), NamedSharding(mesh, PartitionSpec(""dp"", None)), lambda idx: batch[idx]     )     total_loss = 0.0     total_time = 0.0     for n in range(20):         this_key, key = jax.random.split(key)         time_in = time.time()         loss = compute_loss_pjit(batch, this_key)         total_loss += loss.item()         time_out = time.time()         if n > 0:             total_time += time_out  time_in     print(f""eval loss: {total_loss / n:.3f}"")     print(f""eval time: {total_time / (n1):.3f}"") ```  What jax/jaxlib version are you using? 0.4.13  Which accelerator(s) are you using? TPU  Additional system info v332  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,unsafe_rbg + vmap --> 10x slow down," Description unsafe_rbg is advertised as the solution for performance issues for rng, but it has surprising pessimization in the presence of vmap, more than 10x in this script, but more like 4x in real use. The key is `vmap(loss)`, where loss calls the RNG. This is with `LIBTPU_INIT_ARGS=xla_tpu_spmd_rng_bit_generator_unsafe=true` Times: * vmap, no dropout: 0.057 * vmap, dropout, threefry: 0.092 * vmap, dropout, unsafe: 0.736  (!!!!!) * no vmap, no dropout: 0.057 * no vmap, dropout, threefy: 0.088 * no vmap, dropout, unsafe: 0.075 ```  import functools import time import jax import jax.numpy as jnp import numpy as onp from jax.sharding import Mesh, NamedSharding, PartitionSpec batch_size = 256 seq_len = 2048 embed_size = 256 vocab_size = 2000 num_layers = 20 pdrop = 0.1 USE_VMAP = True USE_UNSAFE_RBG = True mesh = Mesh(onp.array(jax.devices()), (""dp"",)) if USE_UNSAFE_RBG:     jax.config.update(""jax_default_prng_impl"", ""unsafe_rbg"") else:     jax.config.update(""jax_threefry_partitionable"", True) with mesh:     key = jax.random.PRNGKey(0)     def model(tokens, key):         embed = jnp.take(jnp.ones((vocab_size, embed_size)), tokens, axis=0)          dumb fake gpt2 attn         for i in range(0, num_layers):             attn = jnp.einsum(""...ld,...kd>...lk"", embed, embed)             if pdrop > 0.0:                 key, subkey = jax.random.split(key)                 dout = jax.random.bernoulli(subkey, pdrop, shape=attn.shape)                 attn = jnp.where(dout, jnp.zeros_like(attn), attn)             attn = jax.nn.softmax(attn, axis=1)             embed = jnp.einsum(""...ld,...lk>...kd"", attn, embed)         out = jnp.einsum(""...ld,...kd>...lk"", embed, jnp.ones((vocab_size, embed_size)))         return out     def compute_loss(example, key):         pred_y = model(example, key=key)         return jnp.mean(pred_y)     def compute_loss_vmap(examples, key):         key = jax.random.split(key, batch_size)         per_ex_loss = jax.vmap(compute_loss)(examples, key)         return jnp.mean(per_ex_loss)     if USE_VMAP:         compute_loss_pjit = jax.jit(compute_loss_vmap)     else:         compute_loss_pjit = jax.jit(compute_loss)      i still honestly find the way to turn a ""replicated"" array like batch into a sharded array to be a bit confusing     batch = jnp.ones((batch_size, seq_len), dtype=jnp.int32)     batch = jax.make_array_from_callback(         (batch_size, seq_len), NamedSharding(mesh, PartitionSpec(""dp"", None)), lambda idx: batch[idx]     )     total_loss = 0.0     total_time = 0.0     for n in range(20):         this_key, key = jax.random.split(key)         time_in = time.time()         loss = compute_loss_pjit(batch, this_key)         total_loss += loss.item()         time_out = time.time()         if n > 0:             total_time += time_out  time_in     print(f""eval loss: {total_loss / n:.3f}"")     print(f""eval time: {total_time / (n1):.3f}"") ```  What jax/jaxlib version are you using? 0.4.13  Which accelerator(s) are you using? TPU  Additional system info v332  NVIDIA GPU info _No response_",2023-07-19T21:58:03Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16792,Our batching rule is more of a looping rule: https://github.com/google/jax/blob/cd951f491718841a806bb509c9950a9630c5a8b6/jax/_src/lax/control_flow/loops.pyL1956L1968
708,"以下是一个github上的jax下的一个issue, 标题是(tensorflow-datasets documentation)， 内容是 (I was trying to decide on a dataloader to use but after reading the documentation neither option (tensorflowdatasets or pytorch) seemed attractive because they both required installing 1+GB packages. After reading more about tensorflowdatasets it seems like they eliminated the tensorflow requirement recently. But JAX documentation still says it is needed: ``` import tensorflow as tf  Ensure TF does not see GPU and grab all GPU memory. tf.config.set_visible_devices([], device_type='GPU') ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,tensorflow-datasets documentation,"I was trying to decide on a dataloader to use but after reading the documentation neither option (tensorflowdatasets or pytorch) seemed attractive because they both required installing 1+GB packages. After reading more about tensorflowdatasets it seems like they eliminated the tensorflow requirement recently. But JAX documentation still says it is needed: ``` import tensorflow as tf  Ensure TF does not see GPU and grab all GPU memory. tf.config.set_visible_devices([], device_type='GPU') ```",2023-07-19T02:50:50Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/16782,Hmm... well tried running the example and looks like it still does require a full `tensorflow` installation unfortunately...,The `tf.config` stuff isn't necessary if `tensorflow` isn't installed.
512,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Remove unnecessary test)， 内容是 ([jax2tf] Remove unnecessary test The `jax2tf_test.XlaCallModuleTest` was added in the early days of native serialization. Now we have much better testing through `xla_call_module_test.py` (near the `XlaCallModule` definition in TF) and through all the native serialization tests in jax2tf test suite.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Remove unnecessary test,[jax2tf] Remove unnecessary test The `jax2tf_test.XlaCallModuleTest` was added in the early days of native serialization. Now we have much better testing through `xla_call_module_test.py` (near the `XlaCallModule` definition in TF) and through all the native serialization tests in jax2tf test suite.,2023-07-17T14:29:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16758
2294,"以下是一个github上的jax下的一个issue, 标题是(Different promotion behavior between numpy.right_shift and jax.numpy.right_shift for scalars)， 内容是 ( Description Hi, Thank you for your all efforts developing such a great library! I'm trying to use jax.numpy.left_shift and jax.numpy.right_shift method for bitwise calculation. And I found that the jax.numpy.right_shift operation does nothing if I do shift operation with jnp.array. Here is what I've tried, ```python  Working fine bit_idx = np.random.randint(0, 31, size=(2, )) mask = ~jnp.zeros((2, )) mask = (mask   [11111111_11111111_11111111_00000000, 11111111_11111111_11111111_00000000] ``` ```python  Working fine bit_idx = np.random.randint(0, 31, size=(2, )) mask = ~jnp.zeros((2, )) mask = jnp.left_shift(mask, bit_idx).astype(jnp.uint32)  if bit_idx = [8, 8]  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] >  [11111111_11111111_11111111_00000000, 11111111_11111111_11111111_00000000] ``` ```python  Not working bit_idx = np.random.randint(0, 31, size=(2, )) mask = ~jnp.zeros((2, )) mask = (mask >> bit_idx).astype(jnp.uint32)  if bit_idx = [8, 8]  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] >  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] ``` ```python  Not working bit_idx = np.random.randint(0, 31, size=(2, )) mask = ~jnp.zeros((2, )) mask = jnp.right_shift(mask, bit_idx).astype(jnp.uint32)  if bit_idx = [8, 8]  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] >  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] ``` And here is the code for printing bit string. ```python def show_bits(number):     bit_string = bin(number).replace('0b', '')     return bit_string.zfill(32) vshow_bits = np.vectorize(show_bits) ``` Is this a bug or did I miss something to use right_shift operator, or does my show_bits function matter? Regards,  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.6 / Ubuntu 22.04 with Docker  NVIDIA GPU info All RTX 3090 ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Different promotion behavior between numpy.right_shift and jax.numpy.right_shift for scalars," Description Hi, Thank you for your all efforts developing such a great library! I'm trying to use jax.numpy.left_shift and jax.numpy.right_shift method for bitwise calculation. And I found that the jax.numpy.right_shift operation does nothing if I do shift operation with jnp.array. Here is what I've tried, ```python  Working fine bit_idx = np.random.randint(0, 31, size=(2, )) mask = ~jnp.zeros((2, )) mask = (mask   [11111111_11111111_11111111_00000000, 11111111_11111111_11111111_00000000] ``` ```python  Working fine bit_idx = np.random.randint(0, 31, size=(2, )) mask = ~jnp.zeros((2, )) mask = jnp.left_shift(mask, bit_idx).astype(jnp.uint32)  if bit_idx = [8, 8]  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] >  [11111111_11111111_11111111_00000000, 11111111_11111111_11111111_00000000] ``` ```python  Not working bit_idx = np.random.randint(0, 31, size=(2, )) mask = ~jnp.zeros((2, )) mask = (mask >> bit_idx).astype(jnp.uint32)  if bit_idx = [8, 8]  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] >  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] ``` ```python  Not working bit_idx = np.random.randint(0, 31, size=(2, )) mask = ~jnp.zeros((2, )) mask = jnp.right_shift(mask, bit_idx).astype(jnp.uint32)  if bit_idx = [8, 8]  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] >  [11111111_11111111_11111111_11111111, 11111111_11111111_11111111_11111111] ``` And here is the code for printing bit string. ```python def show_bits(number):     bit_string = bin(number).replace('0b', '')     return bit_string.zfill(32) vshow_bits = np.vectorize(show_bits) ``` Is this a bug or did I miss something to use right_shift operator, or does my show_bits function matter? Regards,  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.6 / Ubuntu 22.04 with Docker  NVIDIA GPU info All RTX 3090 ``` ++  ++ ```",2023-07-16T01:41:59Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/16748,"You're right! Simpler, more complete repro: ``` In [1]: import jax.numpy as jnp, numpy as np In [2]: x = np.array(0xFFFFFFFF, np.uint32) In [3]: y = np.array(1, np.int32) In [4]: x >> y Out[4]: 2147483647 In [5]: x = jnp.array(0xFFFFFFFF, np.uint32) In [6]: y = jnp.array(1, np.int32) In [7]: x >> y Out[7]: Array(1, dtype=int32) ``` The issue is that, given `(uint32 << int32)`, JAX promotes both arguments to `int64` (or `int32`) first. The right shift is then an arithmetic shift because that's how right shifts on signed values are defined in NumPy (i.e., the leading sign bit is copied into lower bits when shifting). This is a bug in JAX, since `jax.numpy` should match `numpy`. However, as a workaround, just make sure that the value on the right hand side of the shift operator is of unsigned type also.","Actually, I'm not so sure any more. NumPy's behavior seems to differ between scalars and arrays: ``` In [1]: import jax.numpy as jnp, numpy as np In [2]: np.arange(5, dtype=np.uint16) >> np.arange(5, dtype=np.int32) Out[2]: array([0, 0, 0, 0, 0], dtype=int32) In [3]: np.arange(5, dtype=np.uint16) >> np.arange(1, dtype=np.int32) Out[3]: array([0, 1, 2, 3, 4], dtype=int32) In [4]: np.arange(5, dtype=np.uint16) >> np.array(1, dtype=np.int32) Out[4]: array([0, 0, 1, 1, 2], dtype=uint16) ``` JAX doesn't distinguish between scalars and arrays (this is an intentional difference from NumPy).  What do you think?","This is a corner case that is poorly affected by the design of JAX's X64 flag. The type promotion result for `int32, uint32` in both JAX and NumPy is `int64`, and you can see with `int64` enabled that JAX produces the expected output: ```python import jax import jax.numpy as jnp jax.config.update('jax_enable_x64', True) x = jnp.array(0xFFFFFFFF, jnp.uint32) y = jnp.array(1, jnp.int32) out = x >> y print(out, out.dtype)  2147483647 int64 ``` Unfortunately, with `X64` disabled, this gets squashed to `int32` that causes problematic type casting of inputs, ultimately leading to an unexpected result. I'm not sure what to suggest, beyond perhaps using explicit casting of inputs to avoid these problematic corner cases. Beyond that, we should probably fix this by changing how the X64 flag operates, but that's a piece of technical debt that's so tightly entwined with the core uses of JAX that it's proven incredibly difficult to change, as even subtle tweaks can have unexpectedly farreaching consequences for users.","If you're finding that implicit dtype promotion is causing problems, you might consider running your code with strict type promotion semantics to detect these potentially problematic cases: ```python import jax import jax.numpy as jnp x = jnp.array(0xFFFFFFFF, jnp.uint32) y = jnp.array(1, jnp.int32) with jax.numpy_dtype_promotion('strict'):   out = x >> y    TypePromotionError: Input dtypes ('uint32', 'int32') have no available implicit dtype promotion path when jax_numpy_dtype_promotion=strict. with jax.numpy_dtype_promotion('strict'):   out = x >> y.astype('uint32')   print(out)    2147483647 ```","Note this isn't just a problem about type squashing. It's certainly relevant to scalar handling, also. e.g., even with  `JAX_ENABLE_X64`. ``` In [1]: import jax.numpy as jnp, numpy as np In [2]: np.arange(5, dtype=np.uint64) >> np.array(1, np.int64) Out[2]: array([0, 0, 1, 1, 2], dtype=uint64) In [3]: jnp.arange(5, dtype=np.uint64) >> jnp.array(1, np.int64)  TypeError                                 Traceback (most recent call last) Cell In[3], line 1 > 1 jnp.arange(5, dtype=np.uint64) >> jnp.array(1, np.int64) File ~/p/jax/jax/_src/numpy/array_methods.py:256, in _defer_to_unrecognized_arg..deferring_binary_op(self, other)     254 args = (other, self) if swap else (self, other)     255 if isinstance(other, _accepted_binop_types): > 256   return binary_op(*args)     257 if isinstance(other, _rejected_binop_types):     258   raise TypeError(f""unsupported operand type(s) for {opchar}: ""     259                   f""{type(args[0]).__name__!r} and {type(args[1]).__name__!r}"")     [... skipping hidden 12 frame] File ~/p/jax/jax/_src/numpy/ufuncs.py:207, in right_shift(x1, x2)     204 x1, x2 = promote_args_numeric(np.right_shift.__name__, x1, x2)     205 lax_fn = lax.shift_right_logical if \     206   np.issubdtype(x1.dtype, np.unsignedinteger) else lax.shift_right_arithmetic > 207 return lax_fn(x1, x2)     [... skipping hidden 7 frame] File ~/p/jax/jax/_src/lax/lax.py:1557, in naryop_dtype_rule(***failed resolving arguments***)    1555       typename = dtype_to_string(aval.dtype)    1556       typenames = ', '.join(t.__name__ for t in types) > 1557       raise TypeError(msg.format(name, typename, i, i, typenames))    1558 check_same_dtypes(name, *avals)    1559 return result_dtype(*avals) TypeError: shift_right_arithmetic does not accept dtype float64 at position 0. Accepted dtypes at position 0 are subtypes of integer. ``` NumPy handles a scalar RHS differently here.","Numpy scalars are akin to JAX weak types, and JAX and NumPy act similarly in these cases: ```python >>> jnp.arange(5, dtype=np.uint64) >> 1 Array([0, 0, 1, 1, 2], dtype=uint64) ``` If you have a stronglytyped (nonscalar) value in NumPy, you see the same error as with a stronglytyped righthandside value in JAX: ```python >>> np.arange(5, dtype=np.uint64) >> np.ones(5, np.int64)  TypeError                                 Traceback (most recent call last) [](https://localhost:8080/) in () > 1 np.arange(5, dtype=np.uint64) >> np.ones(5, np.int64) TypeError: ufunc 'right_shift' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'' ```"
484,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Added a flag and environment variable to control the serialization version)， 内容是 (This allows us to control the serialization version to be compatible with the deployed version of tf.XlaCallModule. In particular, we can run most tests with the maximum available version, while keeping the default lower.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Added a flag and environment variable to control the serialization version,"This allows us to control the serialization version to be compatible with the deployed version of tf.XlaCallModule. In particular, we can run most tests with the maximum available version, while keeping the default lower.",2023-07-15T09:17:39Z,pull ready,closed,0,4,https://github.com/jax-ml/jax/issues/16746,  PTAL,"> The changes look good to me. >  > Somewhat related question: do you plan to maintain min/max supported version in jax2tf (if we ever need one when we clean up some old jax2tf logic) separately from TF2XLA, or in sync? I would guess that the former makes more sense, but not sure if it will make the testing more complex. I don't quite see how to keep the versioning perfectly in sync. My plan is to support in jax2tf the versions that have been active in google3 over the last month. This means 1 or 2 versions, the most recent. The tests in google3 will verify that we can use the most recent version and the one that is marked as default. There is no test per se that we can use the version that has been active a month ago; we rely on people not updating the default serialization version too early. Also in OSS people will have different versions of JAX and TF. I do assume that people will be willing to use tfnightly if they get an error; this has been true for jax2tf so far because it relies always on some very recent details for the TF ops.","> > The changes look good to me. >  > >  >  > > Somewhat related question: do you plan to maintain min/max supported version in jax2tf (if we ever need one when we clean up some old jax2tf logic) separately from TF2XLA, or in sync? I would guess that the former makes more sense, but not sure if it will make the testing more complex. >  >  >  > I don't quite see how to keep the versioning perfectly in sync. My plan is to support in jax2tf the versions that have been active in google3 over the last month. This means 1 or 2 versions, the most recent. The tests in google3 will verify that we can use the most recent version and the one that is marked as default. There is no test per se that we can use the version that has been active a month ago; we rely on people not updating the default serialization version too early. >  >  >  > Also in OSS people will have different versions of JAX and TF. I do assume that people will be willing to use tfnightly if they get an error; this has been true for jax2tf so far because it relies always on some very recent details for the TF ops. Thanks. Sounds good to me.","This was already merged, but copybara did not close it. See 603eeb19017d50526a85e9c6c49f76330254bd47"
1410,"以下是一个github上的jax下的一个issue, 标题是(Can't find libdevice)， 内容是 ( Description Hi, I'm running Debian 12 with an nvidia GPU.  I didn't install a system CUDA.  I installed Jaxlib via `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and now when I try to run my code I get: ```20230713 23:53:10.976636: W external/xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:523] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice. Searched for CUDA in the following directories:   /usr/local/cuda11.8   /usr/local/cuda   . You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=xla_gpu_cuda_data_dir=/path/to/cuda will work. ``` Based on this comment https://github.com/google/jax/issues/989issuecomment839772971 the error I'm seeing shouldn't be possible with the pipinstalled CUDA that I'm trying to use?  What jax/jaxlib version are you using? jax0.4.13, jaxlib 0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? nvidia tesla t4  Additional system info Debian 12  NVIDIA GPU info ```++  ++```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Can't find libdevice," Description Hi, I'm running Debian 12 with an nvidia GPU.  I didn't install a system CUDA.  I installed Jaxlib via `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and now when I try to run my code I get: ```20230713 23:53:10.976636: W external/xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:523] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice. Searched for CUDA in the following directories:   /usr/local/cuda11.8   /usr/local/cuda   . You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=xla_gpu_cuda_data_dir=/path/to/cuda will work. ``` Based on this comment https://github.com/google/jax/issues/989issuecomment839772971 the error I'm seeing shouldn't be possible with the pipinstalled CUDA that I'm trying to use?  What jax/jaxlib version are you using? jax0.4.13, jaxlib 0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? nvidia tesla t4  Additional system info Debian 12  NVIDIA GPU info ```++  ++```",2023-07-13T23:58:18Z,bug XLA NVIDIA GPU,closed,0,10,https://github.com/jax-ml/jax/issues/16726,"For what its worth, `env/lib/python3.11/sitepackages/jaxlib/cuda/nvvm/libdevice/libdevice.10.bc` does in fact exist",The logic for finding `libdevice` is here: https://github.com/google/jax/blob/6c699815bc307e09ee553245ea5cdf8b6298b040/jax/_src/lib/__init__.pyL121 Is it possible you have `jaxlib` and NVIDIA's pip packages installed in different sitepackage directories or something like that?,"That function finds & returns `env/lib/python3.11/sitepackages/nvidia/cuda_nvcc`, which does in fact contain ptxas and libdevice.10.bc I don't think anything is installed anywhere other than `env/lib/python3.11/sitepackages`","(by anything I mean literally anything, this is a clean image and the only thing i've done besides installing `nvidiadriver` from debian nonfree is run `python m venv env; source env/bin/activate; pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`)","I'm not able to reproduce this. I did the following: * I made a fresh GCP instance with Debian 12 bookworm (amd64) and a T4 GPU. * `sudo apt install python3 python3pip python3.11venv`. * I installed the NVIDIA driver following https://wiki.debian.org/NvidiaGraphicsDriversDebian_12_.22Bookworm.22, which I did verbatim except I had to include the GCP kernel's headers. ``` python3 m venv env source env/bin/activate pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html pip install ipython python c ""import jax; print(jax.devices())"" ``` I also then checked out JAX and ran a bunch of the test suite, which seemed happy. Can you say more about how your environment/test is different?","Hm that should be exactly the same I think? I didn't install ipython, but I don't see how that could matter... `python c ""import jax; print(jax.devices())""` works fine for me; the error doesn't happen until I actually try to run some code.  I'll see if I can get a minimal example, and also make a new image just to be extra certain, and update the ticket","Ok, here's my probably not quite minimal example: ``` import jax.numpy as jnp from jax import vmap def mul(X, Y):   return jnp.dot(X, Y) batched_mul = vmap(mul, in_axes=(None, 0)) def main():    note the 0.0; X will have dtype float32   X = jnp.array([[0.0,1,0],[1,0,0],[0,0,1]])   print(X.dtype)    Ys will have dtype int32   Ys = jnp.array([[1,2,3], [4,5,6]])   print(Ys.dtype)   print(batched_mul(X, Ys)) main() ``` The bug only happens when I'm trying to multiply an int32 array by a float32 array so I guess that's the actual problem, however just in case it's useful I went ahead and created a new instance and carefully logged every command I ran: ``` sudo apt install linuxheaderscloudamd64 softwarepropertiescommon pciutils gcc make dkms sudo apt install emacs  add ""deb http://deb.debian.org/debian/ bookworm main contrib nonfree nonfreefirmware""  to /etc/apt/sources.list sudo apt update sudo apt install nvidiadriver firmwaremiscnonfree sudo systemctl reboot nvidiasmi  GPU is detected, hooray mkdir bug cd bug sudo apt install python3 python3pip python3.11venv pythonispython3 python m venv env source env/bin/activate pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  copy the minimal example into minimal.py python minimal.py  Error! ```","Thanks, I can reproduce. It seems to be related to when XLA uses Triton to emit a mixedprecision GEMM kernel. You can work around for the moment by setting `XLA_FLAGS=xla_gpu_enable_triton_gemm=false`.","Good news! I think this was actually already fixed last week by https://github.com/openxla/xla/commit/6374373c40a918215813bb8a6dd5dadd695c5a9a That fix is not in the latest jaxlib release yet. Until then, use the workaround. Hope that helps!",Thanks!
491,"以下是一个github上的jax下的一个issue, 标题是(Include compile time along with executable in cache entry.)， 内容是 (Include compile time along with executable in cache entry. In order to measure cache savings, we add compilation time to the cache entry along with the serialized executable. The compile time can then be retrieved on a cache hit. Testing: updated tests.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Include compile time along with executable in cache entry.,"Include compile time along with executable in cache entry. In order to measure cache savings, we add compilation time to the cache entry along with the serialized executable. The compile time can then be retrieved on a cache hit. Testing: updated tests.",2023-07-13T21:35:38Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16722,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
271,"以下是一个github上的jax下的一个issue, 标题是(Ragged batching polish)， 内容是 (Add more test cases; also rename to jumbles to avoid misassociations.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Ragged batching polish,Add more test cases; also rename to jumbles to avoid misassociations.,2023-07-13T19:49:07Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16719,nit: you should squash the commits so that the github history is a bit clean! This is the advice I got from other people after which I stopped doing PRs lol 
1292,"以下是一个github上的jax下的一个issue, 标题是(hide typed key array class in favor of distinguishing key arrays by dtype)， 内容是 (From the outside perspective, a typed key array should be known as a `jax.Array` with a special element type. Currently we support—and even encourage—distinguishing it based on a special array type (`PRNGKeyArray`) instead, but let's move away from that. To achieve this, we'll want to hide the array type and expose the dtype in some way. At least two notable ways in which this will affect code that has already started to use typed key arrays: * `isinstance(keys, PRNGKeyArray)` will need an alternative based on `keys.dtype` * Type annotations with `PRNGKeyArray` will need to be switched to `jax.Array`. This is more permissive in principle, but actually the current `jax.random` module sets `PRNGKeyArray` to `Any` during typechecking. It might make sense to introduce a public convenience function to `jax.random` that determines that an array is a key array, analogous to `jnp.iscomplex` et. al. basically being dtype check conveniences as well. Maybe we call it `jax.random.isprngkey`? (RNGs: key types and custom implementations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,hide typed key array class in favor of distinguishing key arrays by dtype,"From the outside perspective, a typed key array should be known as a `jax.Array` with a special element type. Currently we support—and even encourage—distinguishing it based on a special array type (`PRNGKeyArray`) instead, but let's move away from that. To achieve this, we'll want to hide the array type and expose the dtype in some way. At least two notable ways in which this will affect code that has already started to use typed key arrays: * `isinstance(keys, PRNGKeyArray)` will need an alternative based on `keys.dtype` * Type annotations with `PRNGKeyArray` will need to be switched to `jax.Array`. This is more permissive in principle, but actually the current `jax.random` module sets `PRNGKeyArray` to `Any` during typechecking. It might make sense to introduce a public convenience function to `jax.random` that determines that an array is a key array, analogous to `jnp.iscomplex` et. al. basically being dtype check conveniences as well. Maybe we call it `jax.random.isprngkey`? (RNGs: key types and custom implementations)",2023-07-13T15:55:03Z,enhancement,closed,3,10,https://github.com/jax-ml/jax/issues/16716,"Sounds great to me! I've never used `numpy.iscomplex` so the convenience wrapper doesn't sound crucial (though it sounds fine). I usually write things more like `np.issubdtype(x.dtype, np.integer)` or whatever. (EDIT: I wrote ""isinstance"" but I meant ""issubdtype"", thanks Jake.)","Maybe `jnp.issubdtype(x.dtype, random.PRNGDtype)` or something like that?","We might want to be able to answer two questions: * Is this an array of keys of a particular implementation (e.g. `'threefry2x32'`)? * Is this an array of keys, of any RNG implementation? There may be some analogy to Numpy's `issubdtype` versus dtype equality, although we don't have to follow suit even if so.","I think I'd propose treating this like any other dtype. E.g. adding dtypes `jax.dtypes.key` and `jax.dtypes.threefry`, such that `jnp.issubdtype(threefry, key)` and `jnp.issubdtype(key, jnp.generic)`.",Initial proposal in CC(custom prng: introduce mechanism to identify key arrays by dtype),"I think I ran into some changes related to this. I like kidger's suggestion for the runtime behavior. For the static type checking, it may be worth considering making the dtype generic like NumPy does.  I proposed this earlier for common dtypes.  For RNG dtypes, it would be really nice because you could then provide a much more constrained type annotation: ```python Array = jax.GenericArray[Any] KeyArray = jax.GenericArray[jnp.generic] ``` This would then work as usual and give errors (at least with PyRight) if someone tries to pass another type of Jax array.  (MyPy seems to be fine with it too.)","Thanks for the comment – it may be that typed generics would be useful for annotating dtypes, but we haven't looked more deeply into that. I personally tend to be pretty pessimistic about the usefulness of overlysophisticated Python type annotations: in my experience they lead to far more noise and false positives than actual bugs caught. You end up having to write your code in a stilted way to appease the type checker, or alternatively sprinkle ` type: ignore` throughout, which entirely defeats the purpose. This is one reason why I kept `jax.Array` simple: it hits the sweet spot of enabling reasonable annotations and `isinstance` checks without introducing the diminishing returns of more complicated types. That said, I think reasonable people can disagree, and if someone wanted to explore using more complicated types, I wouldn't get in their way. Regarding the bug here: going forward, I don't think you should rely on `KeyArray` as a meaningful annotation. If anything, it should probably be aliased to `KeyArray = Array`, because key arrays are just jax arrays with a different dtype. Again, I think that would hit the sweet spot of being useful without being overly complex.","> I personally tend to be pretty pessimistic about the usefulness of overlysophisticated Python type annotations: I agree with you in general. Although in Jax, I bet 99% of arrays are either: ```python RealArray = GenericArray[jnp.floating[Any]]   or KeyArray = GenericArray[jnp.generic] ``` Just distinguishing these two would be beneficial for not accidentally passing value arrays as keys and vice versa. I understand if the Jax team has better things to do though.  Just on my wish list. > Regarding the bug here: going forward, I don't think you should rely on `KeyArray` as a meaningful annotation. I'm just using the definition provided by Jax: ```python from jax.random import KeyArray ``` Right now it seems to be `Any` in static analysis.  I guess that'll eventually change to `Array` in some future version of Jax?","> Right now it seems to be `Any` in static analysis. I guess that'll eventually change to `Array` in some future version of Jax? In some future version of JAX, `KeyArray` will no longer be a public symbol, because it's an implementation detail that we don't want to expose to the user (the main thrust of this issue is that `KeyArray` should no longer be load bearing, in favor of using `dtype` to distinguish newstyle keys).",Finished in https://github.com/google/jax/pull/16781
1108,"以下是一个github上的jax下的一个issue, 标题是(nvlink fatal : Value 'sm_80' is not defined for option 'arch')， 内容是 ( Description I have a problem running jax with CUDA on a workstation with A100 GPUS. I have created a conda environment with python 3.9 and installed jax as described: ``` pip install upgrade pip pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` However, when using any jax function, for example: ``` import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""2"" import jax jax.random.PRNGKey(42) ``` I get the following error message: `jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Value 'sm_80' is not defined for option 'arch'`  What jax/jaxlib version are you using? 0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? A100 GPU  Additional system info Linux  NVIDIA GPU info ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,nvlink fatal : Value 'sm_80' is not defined for option 'arch'," Description I have a problem running jax with CUDA on a workstation with A100 GPUS. I have created a conda environment with python 3.9 and installed jax as described: ``` pip install upgrade pip pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` However, when using any jax function, for example: ``` import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""2"" import jax jax.random.PRNGKey(42) ``` I get the following error message: `jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Value 'sm_80' is not defined for option 'arch'`  What jax/jaxlib version are you using? 0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? A100 GPU  Additional system info Linux  NVIDIA GPU info ``` ++  ++++ ```",2023-07-12T22:03:29Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16699,I think this is the same issue as https://github.com/google/jax/issues/16586. Can you try the workarounds in https://github.com/google/jax/issues/16586issuecomment1630817089 ?,"> I think this is the same issue as CC(jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256). Can you try the workarounds in  CC(jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256) (comment) ? Thanks, solved the issue!"
203,"以下是一个github上的jax下的一个issue, 标题是(JIT through ragged axes)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JIT through ragged axes,,2023-07-11T19:09:27Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16685
2210,"以下是一个github上的jax下的一个issue, 标题是(pjit uses too much memory)， 内容是 ( Description Suppose I have a large state stored on CPU, and a tree of shardings matching the state structure. Let's assume the partition is fairly uniform, so that about the same amount of memory should supposedly be allocated per device after sharding. On my multiGPU setup, the following code goes out of memory: ``` from jax.experimental.pjit import pjit state = pjit(lambda: state, in_shardings=(), out_shardings=shardings)() ``` while the following does not: ``` from jax import device_put state = device_put(state, shardings) ``` Why? The result should be the same. In both cases, I should obtain a state sharded across my GPU devices. **Some context** The example above is a proof of concept. More specifically, I am trying to instantiate on GPU the state of GPTJ, an LLM with 6b parameters. The state is mainly comprised of the model parameters, as well as two more replicas corresponding to the mu and nu parameters of an optax.adam optimizer. So, 18b parameters in total. In halfprecision (float16, i.e. 2 bytes), this gives me 36b bytes, that is 36 GB of memory. My Amazon EC2 instance has 8 GPUs, each with 16 GiB of GPU memory. I have created a sharding tree that partitions the state fairly uniformly across the 8 devices. Since 36 / 8 GB = 4.5 GB, the sharded state should comfortably fit in GPU memory. And yet, when initializing the state with pjit, I go out of memory. I then stripped down my example to the bone, and discovered, like in the example above, that device_put works instead, as it should. Do you have an explanation for why this is happening? Is there anything I can do to make pjit work? Eventually, I really want to work with pjit rather than device_put, because I want my state to be directly sharded on GPU, rather than having to store it on CPU first and then putting it on the devices.  What jax/jaxlib version are you using? jax 0.4.13, jaxlib 0.4.13+cuda11.cudnn86   Which accelerator(s) are you using? GPU  Additional system info OS  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,pjit uses too much memory," Description Suppose I have a large state stored on CPU, and a tree of shardings matching the state structure. Let's assume the partition is fairly uniform, so that about the same amount of memory should supposedly be allocated per device after sharding. On my multiGPU setup, the following code goes out of memory: ``` from jax.experimental.pjit import pjit state = pjit(lambda: state, in_shardings=(), out_shardings=shardings)() ``` while the following does not: ``` from jax import device_put state = device_put(state, shardings) ``` Why? The result should be the same. In both cases, I should obtain a state sharded across my GPU devices. **Some context** The example above is a proof of concept. More specifically, I am trying to instantiate on GPU the state of GPTJ, an LLM with 6b parameters. The state is mainly comprised of the model parameters, as well as two more replicas corresponding to the mu and nu parameters of an optax.adam optimizer. So, 18b parameters in total. In halfprecision (float16, i.e. 2 bytes), this gives me 36b bytes, that is 36 GB of memory. My Amazon EC2 instance has 8 GPUs, each with 16 GiB of GPU memory. I have created a sharding tree that partitions the state fairly uniformly across the 8 devices. Since 36 / 8 GB = 4.5 GB, the sharded state should comfortably fit in GPU memory. And yet, when initializing the state with pjit, I go out of memory. I then stripped down my example to the bone, and discovered, like in the example above, that device_put works instead, as it should. Do you have an explanation for why this is happening? Is there anything I can do to make pjit work? Eventually, I really want to work with pjit rather than device_put, because I want my state to be directly sharded on GPU, rather than having to store it on CPU first and then putting it on the devices.  What jax/jaxlib version are you using? jax 0.4.13, jaxlib 0.4.13+cuda11.cudnn86   Which accelerator(s) are you using? GPU  Additional system info OS  NVIDIA GPU info ``` ++  ++ ```",2023-07-11T07:33:00Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/16679,"I think you will have to provide more information regarding the `pjit` case. Can you dump the HLO please? Also if there are no arguments, then there is no need to specify `in_shardings`. It's an optional argument."," Here is something perhaps better. I managed to create an example reproducing the issue. ``` from transformers import FlaxAutoModelForCausalLM from jax.sharding import Mesh, PartitionSpec, NamedSharding from jax.experimental.mesh_utils import create_device_mesh from jax.experimental.pjit import pjit from jax import eval_shape, device_put, local_device_count from copy import deepcopy import numpy as np import re from jax.tree_util import DictKey, FlattenedIndexKey, GetAttrKey, SequenceKey, tree_map_with_path, tree_map def path_to_string(path, separator: str = None):     keys = []     for key in path:         if isinstance(key, SequenceKey):             keys.append(str(key.idx))         elif isinstance(key, DictKey):             keys.append(str(key.key))         elif isinstance(key, GetAttrKey):             keys.append(str(key.name))         elif isinstance(key, FlattenedIndexKey):             keys.append(str(key.key))         else:             keys.append(str(key))     if separator is None:         return tuple(keys)     return separator.join(keys) def named_tree_map(f, tree, *rest, is_leaf=None, separator=None):     return tree_map_with_path(         lambda string_path, x, *r: f(path_to_string(string_path, separator=separator), x, *r),         tree,         *rest,         is_leaf=is_leaf,     ) def match_partition_specs(specs, tree):     def get_partition_spec(path, shape_leaf):         if len(shape_leaf.shape) == 0 or np.prod(shape_leaf.shape) == 1:             return PartitionSpec()         for rule, ps in specs.items():             if re.search(rule, path) is not None:                 return ps         return PartitionSpec()     return named_tree_map(get_partition_spec, tree, separator=""/"") if __name__ == ""__main__"":     mesh = Mesh(create_device_mesh((1, 1, local_device_count())), (""dp"", ""fsdp"", ""mp""))     partition_specs = {         'transformer/wte/embedding': PartitionSpec('mp', 'fsdp'),         'attn/(k_projv_proj)/kernel': PartitionSpec('fsdp', 'mp'),         'attn/out_proj/kernel': PartitionSpec('mp', 'fsdp'),         'mlp/fc_in/kernel': PartitionSpec('fsdp', 'mp'),         'mlp/fc_in/bias': PartitionSpec('mp'),         'mlp/fc_out/kernel': PartitionSpec('mp', 'fsdp'),         'lm_head/kernel': PartitionSpec('fsdp', 'mp'),         'lm_head/bias': PartitionSpec('mp'),     }     model, params = FlaxAutoModelForCausalLM.from_pretrained(""EleutherAI/gptj6B"", _do_init=False)     params = tree_map(lambda v: v.astype(""float16"") if (v.dtype == ""float32"" or v.dtype == ""float64"") else v, params)     state = {k: deepcopy(params) for k in [1, 2, 3, 4]}     def init_state_fn():         return state     shapes_dtypes = eval_shape(init_state_fn)     partitions = match_partition_specs(partition_specs, shapes_dtypes)     shardings = tree_map(lambda p: NamedSharding(mesh=mesh, spec=p), partitions)     sharded_state = pjit(init_state_fn, out_shardings=shardings)()      sharded_state = device_put(state, shardings)     print(sharded_state.keys()) ``` As described above, I'm running this across 8 GPU devices. When using `pjit`, I get the following warning,  ``` W external/xla/xla/service/hlo_rematerialization.cc:2218] Can't reduce memory use below 11.83GiB (12701564928 bytes) by rematerialization; only reduced to 16.91GiB ``` and then an error message like `Execution of replica 0 failed: INTERNAL: Failed to allocate XXX bytes for new constant`. I don't understand why, since in theory the sharded state should fit in GPU memory. On the other end, when I comment the line with `pjit` and uncomment the line with `device_put`, it works as expected. Any ideas? This is blocking me quite a bit. Thanks!","The error message tells me that it is the `state` that you are closing over which is causing a problem. Try creating the `state` inside the pjitted function instead of materializing it outside? This way the state will be materialized as sharded directly. Also, if device_put works, then why are you using `pjit` to do this? `device_put` seems like a better solution to me instead of running a XLA computation.","The initial `state` is on CPU, not on GPU. I expect `pjit` to materialize the state as sharded directly on GPU. The code above is just a simplification, but eventually I would need to instantiate the state using `pjit`. The state will be created using `flax.train_state.create`. This will internally instantiate the parameters of an `optax` optimizer. While it might be possible to instantiate the whole state on CPU first (currently, I wouldn't know how to do it, as optax directly initializes its parameters on GPU, if possible), it seems like a hassle compared to just using `pjit`.  Independently of the best practice for this though, `pjit` seems to take far too much memory. I would love to understand why, and how to circumvent this. If `pjit` takes so much memory to materialize the state, it might also take a lot of memory when distributing a training step, making it very hard to train large models.","Hi, bringing this up again as I haven't yet managed to solve the issue. Any chance you might have time to look into this?","Here is a common solution: ```python shard_model_params_to_multihost = lambda params: ...   write your own sharding function here cpu_device = jax.devices('cpu')[0] with jax.default_device(cpu_device):     params = load_params(...) params = shard_model_params_to_multihost(params) ``` Besides, from my understanding, if the `params` are sharded, the `train_state` created from the params would also be sharded.","Hi, we are observing the same issue. We have a model that runs fine with pmap but fails at initialization with pjit. We tested this with pjit batch parallelism and ZerO sharding and both fail.  We are doing exactly the same, loading pretrained numpy params and closing over them with pjit. Our function only takes the `rng_key` as input and inside the network checks whether params are present in outer pretrained params and does `jnp.asarray` otherwise it creates a new param to train from scratch. With pmap this works fine as it's somehow able to free up the memory, but with pjit simple batch parallelism fails. It compiles fine but then fails with: > XlaRuntimeError: RESOURCE_EXHAUSTED: Error loading program: Attempting to allocate 11.49G. That was not possible. There are 3.59G free.; (1x1x0_HBM0):"," thanks for the answer. Loading parameters on CPU is not really the bottleneck here  in my example script above, `params` is loaded on CPU. The problem arises later when sharding, basically the last line of your little script.  if you happen to find a solution, would you mind sharing it here? Thanks!",A workaround that we ended up going for is to create a new function for initializing parameters that takes pretrained parameters as input instead of taking them from the global scope. This way there are no values to close over so pjit should not OOM.  The fact that pjit does not efficiently handle memory of values closed over still worries me though. ,Do you have an update on this?
1327,"以下是一个github上的jax下的一个issue, 标题是(Wider support for FP8 datatypes)， 内容是 ( I am trying to use float8 datatypes as part of training quantized spiking neural networks but cannot initialise weights for a network because jax.random.\_uniform does not support jnp.float8_* dtypes. My intent is to train on an H100 GPU which supports fp8 with double the throughput of fp16. The error message for jax.random.\_uniform seems to be a little out of date as well as it only discusses fp32/64 while also accepting fp16.  I'm not sure how much work it would take to permit fp8 types but would greatly appreciate it! It seems that the uniform sampling method would need updated but I'm not sure how many other changes would be required to support fp8. Beyond language model applications, neuroevolution would largely benefit from fp8 because of the fact that you could double the population size for the same compute/memory capacity and SNN research would also benefit as one could represent both the input spikes and model weights in fp8 and eliminate having to recast int8 inputs to fp16 values for use. _Originally posted by  in https://github.com/google/jax/discussions/16342discussioncomment6396762_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",large language model,Wider support for FP8 datatypes," I am trying to use float8 datatypes as part of training quantized spiking neural networks but cannot initialise weights for a network because jax.random.\_uniform does not support jnp.float8_* dtypes. My intent is to train on an H100 GPU which supports fp8 with double the throughput of fp16. The error message for jax.random.\_uniform seems to be a little out of date as well as it only discusses fp32/64 while also accepting fp16.  I'm not sure how much work it would take to permit fp8 types but would greatly appreciate it! It seems that the uniform sampling method would need updated but I'm not sure how many other changes would be required to support fp8. Beyond language model applications, neuroevolution would largely benefit from fp8 because of the fact that you could double the population size for the same compute/memory capacity and SNN research would also benefit as one could represent both the input spikes and model weights in fp8 and eliminate having to recast int8 inputs to fp16 values for use. _Originally posted by  in https://github.com/google/jax/discussions/16342discussioncomment6396762_",2023-07-10T19:27:55Z,enhancement,closed,0,15,https://github.com/jax-ml/jax/issues/16673,"Hi  thanks for the request! I think this is a fundamentally difficult question, for the same reasons as those discussed in https://github.com/google/jax/discussions/13798discussioncomment4499272. The issue is that typical algorithms for generating uniform deviates assume that the set of floating point numbers has similar behavior to the set of real numbers, but as you reduce the bit width this becomes less and less true. For example, if we were generating uniform deviates in `float8`, we will quickly run into resolution limits when trying to select values between `0.0` and `1.0`. Here's some quick code to visualize which values are actually possible to represent: ```python import ml_dtypes import numpy as np import matplotlib.pyplot as plt x = np.arange(256, dtype='uint8').view(ml_dtypes.float8_e5m2).astype(float) x_in_range = x[(0 <= x) & (x < 1)] plt.hist(x_in_range, bins=100); ``` !download1 Again, this shows all the values that are *possible* to represent in `float8_e5m2`, and this makes clear that it wouldn't be entirely straightforward to generate uniformlydistributed values in this range, since it's really fundamentally a discrete distribution. What do you think? Do you know of any existing work in this area?"," , That's a good point  I'll have to do some research as I don't know a good method for uniformly sampling floats for 8 bits. I've tried looking at the Brevitas and bitsandbytes packages but haven't found anything there. Based on the other issue you linked perhaps uniformly sampling in a higher precision and casting down to FP8 might be a decent hack? My other thought would be some kind of inverse frequency weighting or using a lookup table since the 8 bit space is relatively small? In the meantime I think I might explore binary initialisation for the networks since this is a tough question.","> My other thought would be some kind of inverse frequency weighting or using a lookup table since the 8 bit space is relatively small? Inverse frequency weighting requires floatingpoint math, so you'd probably have to do it in higher precision, at which point you've given up on the goal of generating float8 directly. I think generating at high precision and then casting is a useful solution, if that's suitable for the usecase. But that's easy enough to do already with something like `random.uniform(key, size).astype('float8')`. It's not clear to me that we should default to that approach if you pass `float8` to `uniform`.","> I think generating at high precision and then casting is a useful solution, if that's suitable for the usecase. But that's easy enough to do already with something like random.uniform(key, size).astype('float8'). It's not clear to me that we should default to that approach if you pass float8 to uniform. Yeah that makes sense from a design perspective. The downside of having to rely on astype/casting at the moment is that I don't think that it dovetails/works with jmp/the jax mixed precision library which allows for setting the dtype for neural net modules in an organised fashion. I agree that it would be nice to have a way to directly generate in fp8 as while it may not be an issue for a neural network that gets initialised only once at the beginning it would definitely negate any gains for applications such as neuroevolution if it's still sampling in fp16 under the hood and then casting.","I haven't done a deep dive on the dtype internals but is there a reason why the jnp.float8 types work when casting with astype but not when specifying them as the dtype of an array from the start? Thank you so much for giving this attention! `>>> import jax.numpy as jnp >>> import jax >>> jax.random.rademacher(jax.random.PRNGKey(0), [3,3]).astype(jnp.float8_e4m3fn) Array([[1, 1, 1],        [1, 1, 1],        [1, 1, 1]], dtype=float8_e4m3fn) >>> jax.random.rademacher(jax.random.PRNGKey(0), [3,3], dtype=jnp.float8_e4m3fn) Traceback (most recent call last):   File """", line 1, in    File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/random.py"", line 1780, in rademacher     return _rademacher(key, shape, dtype)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 208, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 155, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/core.py"", line 2633, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1085, in _pjit_call_impl     compiled = _pjit_lower(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1177, in _pjit_lower     return _pjit_lower_cached(jaxpr, in_shardings, out_shardings, *args, **kwargs)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1237, in _pjit_lower_cached     return pxla.lower_sharding_computation(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 2072, in lower_sharding_computation     nreps, tuple_args) = _cached_lowering_to_hlo(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 1919, in _cached_lowering_to_hlo     lowering_result = mlir.lower_jaxpr_to_module(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 617, in lower_jaxpr_to_module     lower_jaxpr_to_fun(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 932, in lower_jaxpr_to_fun     out_vals, tokens_out = jaxpr_subcomp(ctx.replace(name_stack=callee_name_stack),   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1044, in jaxpr_subcomp     in_nodes = map(read, eqn.invars)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1019, in read     return ir_constants(v.val, canonicalize_types=True)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 217, in ir_constants     out = handler(val, canonicalize_types)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 295, in _ndarray_constant_handler     return _numpy_array_constant(val, canonicalize_types)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 249, in _numpy_array_constant     attr = ir.DenseElementsAttr.get(x, type=element_type, shape=shape) jax._src.traceback_util.UnfilteredStackTrace: ValueError: cannot include dtype '4' in a buffer The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/random.py"", line 1780, in rademacher     return _rademacher(key, shape, dtype) ValueError: cannot include dtype '4' in a buffer `","It's because `jax.random` doesn't know about float8 dtypes, but `jax.numpy` does. They're still relatively new and experimental, and not fully supported across the JAX package.",Gotchya  while direct initialization of float8s seems like a tough problem would at least be possible to support float8 types beyond just jax.numpy so that neural networks could at least perform inference in float8? It seems like this would require enabling them in the MLIR interpreter... I know these types are new but they're supported on the H100 so they'll be around for years and it would be nice to not have to deal with NVIDIA's transformer engine package to bolton float8 support. Thank you so much for your time!,"Sure, it’s possible to support float8 types in other places, and they’re already being used in neural network training. It seems like my recommendation of generating random numbers in float32 and then casting to float8 is what you want, and that’s currently a supported operation. Does that not address your issue?"," , Initialising/maintaining the parameters in higher precision works for the time being, the issue remains that when trying to use float8s for compute/outputs there is still a hang up that prevents things from running. I know this involves the Haiku/JMP but I believe the blockage remains within JAX here. policy I'm trying to use: ``` policy = jmp.Policy(compute_dtype=jnp.float8_e4m3fn,                        param_dtype=jnp.float16,                        output_dtype=jnp.float8_e4m3fn) ``` last part of error trace when attempting to initialise the neural net: ``` ~/.local/lib/python3.10/sitepackages/spyx/nn.py in __call__(self, x, V)     175             beta = hk.get_parameter(""b"", self.hidden_shape,      176                                 init=hk.initializers.TruncatedNormal(0.25, 0.5)) > 177             beta = jax.nn.hard_sigmoid(beta)     178      179         spikes = self.act(V  self.threshold)     [... skipping hidden 18 frame] ~/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py in _numpy_array_constant(x, canonicalize_types)     247     x = x.view(np.uint16)     248   x = np.ascontiguousarray(x) > 249   attr = ir.DenseElementsAttr.get(x, type=element_type, shape=shape)     250   return (hlo.ConstantOp(attr).result,)     251  ValueError: cannot include dtype '4' in a buffer ``` I can try to conjure up a minimal example if that would be helpful. ","I see, thanks. This is an example of what I mentioned – FP8 is still experimental and not fully supported throughout JAX (mainly because there are not supported operations on all hardware). Here's a minimal example of what you ran into: ```python import jax import jax.numpy as jnp x = jnp.array(0, dtype='float8_e4m3fn') print(x + 1)  1.0 print(jax.jit(lambda x: x + 1)(x))  ...  ValueError: cannot include dtype '4' in a buffer ```",So just to be clear the intent is that float8 will remain in its current experimental state and be limited to jax.numpy for the time being until there is broader hardware support for it? If that's the case then there's nothing more to address at this time. Thank you for taking the time to respond and follow up on this!,"I'm actually not sure what the intent is, but it's probably safe to assume that experimental narrowwidth dtypes will have limited support at least for the near future."," CC(Make jit work with custom float inputs) fixes the float8+jit issue you saw above. There are still operations that are not supported for float8, but this gets us a step closer.","`kernel_init = lambda key,shape,dtype : jax.nn.initializers.normal(stddev=self.config.initializer_range）(key,shape,jnp.float32).astype(self.dtype)` It could fix kernel_init problem. But I found my H100 running FP8 and FP16 with same speed!",I'm going to close this – I think it's expected that for the foreseeable future float8 will only be supported for certain operations.
1292,"以下是一个github上的jax下的一个issue, 标题是(Different result while using ``at[].apply`` inside jax.vmap)， 内容是 ( Description Jax's Array index helper `at` potentially has a bug when used inside `jax.vmap`. Using ``apply(lambda...)``  inside ``jax.vmap`` gives me a different result as to when run sequentially and inside jit.  Here's the following code that I'm trying to run ```python import jax import jax.numpy as np a = np.array([1,2,3,4,5,6]) def adder(a,val):     a = a.at[val].apply(lambda a:a1)     return a ind = np.array([3]) print(adder(a,ind)) ``` I get the expected result; ``[1 2 3 4 5 6]`` However, when the same function is used inside jax.vmap I get a different result ```python v_adder = jax.vmap(adder,in_axes=(0,None)) a = np.array([[1,2,3,4,5,6]]) ind = np.array([3]) print(v_adder(a,ind)) ``` ``[[1 2 3 0 5 6]]`` Note that I had to add another dimension in order to use it inside vmap which I don't think is the root cause. Took me a day of debugging to hunt down this bug 😭.   What jax/jaxlib version are you using? jax==0.4.13 jaxlib==0.4.13  Which accelerator(s) are you using? CPU  Additional system info WSL  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Different result while using ``at[].apply`` inside jax.vmap," Description Jax's Array index helper `at` potentially has a bug when used inside `jax.vmap`. Using ``apply(lambda...)``  inside ``jax.vmap`` gives me a different result as to when run sequentially and inside jit.  Here's the following code that I'm trying to run ```python import jax import jax.numpy as np a = np.array([1,2,3,4,5,6]) def adder(a,val):     a = a.at[val].apply(lambda a:a1)     return a ind = np.array([3]) print(adder(a,ind)) ``` I get the expected result; ``[1 2 3 4 5 6]`` However, when the same function is used inside jax.vmap I get a different result ```python v_adder = jax.vmap(adder,in_axes=(0,None)) a = np.array([[1,2,3,4,5,6]]) ind = np.array([3]) print(v_adder(a,ind)) ``` ``[[1 2 3 0 5 6]]`` Note that I had to add another dimension in order to use it inside vmap which I don't think is the root cause. Took me a day of debugging to hunt down this bug 😭.   What jax/jaxlib version are you using? jax==0.4.13 jaxlib==0.4.13  Which accelerator(s) are you using? CPU  Additional system info WSL  NVIDIA GPU info _No response_",2023-07-08T00:32:54Z,bug,closed,1,4,https://github.com/jax-ml/jax/issues/16655,"Thanks for the report, and sorry for the difficulty in tracking down the bug. We really appreciate it though, especially with the minimal repro. > I get the expected result; > [1 2 3 4 5 6] Did you mean `[1 2 3 3 5 6]`? This certainly looks like a bug to me! It reproduces on TPU too. Here are the jaxprs: ```python print(jax.make_jaxpr(adder)(a,ind)) ``` ``` { lambda ; a:i32[6] b:i32[1]. let     c:bool[1] = lt b 0     d:i32[1] = add b 6     e:i32[1] = select_n c b d     f:i32[1,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(1, 1)] e     _:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     g:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     h:i32[6] = scatter[       dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,))       indices_are_sorted=False       mode=GatherScatterMode.FILL_OR_DROP       unique_indices=False       update_consts=()       update_jaxpr={ lambda ; i:i32[] j:i32[]. let k:i32[] = sub i 1 in (k,) }     ] a f g   in (h,) } ``` ```python print(jax.make_jaxpr(v_adder)(a,ind)) ``` ``` { lambda ; a:i32[1,6] b:i32[1]. let     c:bool[1] = lt b 0     d:i32[1] = add b 6     e:i32[1] = select_n c b d     f:i32[1,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(1, 1)] e     _:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     g:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     h:i32[1,1] = broadcast_in_dim[broadcast_dimensions=(1,) shape=(1, 1)] g     i:i32[1,6] = scatter[       dimension_numbers=ScatterDimensionNumbers(update_window_dims=(0,), inserted_window_dims=(1,), scatter_dims_to_operand_dims=(1,))       indices_are_sorted=False       mode=GatherScatterMode.FILL_OR_DROP       unique_indices=False       update_consts=()       update_jaxpr={ lambda ; j:i32[] k:i32[]. let  in (k,) }     ] a f h   in (i,) } ``` What's up with that second `update_jaxpr`!?","Yeah sorry, scatterapply just isn't batchable; there's a rule defined but it doesn't work. In particular, it ignores the `update_jaxpr` (and hence the userdefined fucntion) it's given.",Thanks for the report! The fix should make it into the next JAX release.,Note CC(Bug: scatter_apply autodiff returns incorrect result) – it looks like `scatter_apply` still doesn't work correctly with autodiff. I'm going to try to tackle this soon.
1209,"以下是一个github上的jax下的一个issue, 标题是(`random.split`: generalize to optional shape parameter)， 内容是 (Pair with ! Take two on CC(random.split: add optional shape parameter). Several PRNG implementations (notably partitionable threefry) support splitting to arbitrary shapes, rather than only to a 1D vector of keys. This change: * Upgrades `jax.random.split` to accept a general shape as an argument. * Updates the internal PRNG interface, and our various PRNG implementations, to accept and handle such a shape argument. This change keeps the argument name `num`. We can still think on whether and how we'd like to upgrade to `shape`. Note that we could have supported arbitrary shapes by reduction to the previous API (with a flat split count), using reshapes. We'd like to avoid that, so as not to hide this structure from the underlying implementation. For instance, partitionable threefry hashes a *shaped* iota in order to split keys, and we don't want to flatten and reshape around that for no reason. Fixes CC(random.split() should allow for passing in a desired shape))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`random.split`: generalize to optional shape parameter,"Pair with ! Take two on CC(random.split: add optional shape parameter). Several PRNG implementations (notably partitionable threefry) support splitting to arbitrary shapes, rather than only to a 1D vector of keys. This change: * Upgrades `jax.random.split` to accept a general shape as an argument. * Updates the internal PRNG interface, and our various PRNG implementations, to accept and handle such a shape argument. This change keeps the argument name `num`. We can still think on whether and how we'd like to upgrade to `shape`. Note that we could have supported arbitrary shapes by reduction to the previous API (with a flat split count), using reshapes. We'd like to avoid that, so as not to hide this structure from the underlying implementation. For instance, partitionable threefry hashes a *shaped* iota in order to split keys, and we don't want to flatten and reshape around that for no reason. Fixes CC(random.split() should allow for passing in a desired shape)",2023-07-06T21:10:01Z,kokoro:force-run pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/16644
3513,"以下是一个github上的jax下的一个issue, 标题是(Jaxpr of a function without input argument is wrong)， 内容是 ( Description I am writing a function without any input argument and want to translate it into Jaxpr. Here is the example, ```py def func():   frag_coord = jnp.zeros(([4]))   real = (frag_coord[0] / 1080.0  0.5) * 5.0   imag = (frag_coord[1] / 1080.0  0.5) * 5.0   r_a = real   r_b = imag   max_iteration = 500   def body_func(carry):     i, a, b = carry     t_a = a     a = a * a  b * b + r_a     b = 2 * t_a * b + r_b     return i + 1, a, b   def cond_func(carry):     i, a, b = carry     return ((a * a + b * b) <= 4) & (i < max_iteration)   i = lax.while_loop(cond_func, body_func, (0, real, imag))[0]   res = jnp.where(       i == max_iteration,       jnp.array([0, 0, 0, 1], jnp.float32),       jnp.array([0, i / max_iteration, 0, 1], jnp.float32),   )   return res jaxpr = jax.make_jaxpr(func)().jaxpr print(jaxpr) ``` The output Jaxpr: ```py { lambda a:f32[4]; . let     b:f32[4] = broadcast_in_dim[broadcast_dimensions=() shape=(4,)] 0.0     c:f32[1] = dynamic_slice[slice_sizes=(1,)] b 0     d:f32[] = squeeze[dimensions=(0,)] c     e:f32[] = div d 1080.0     f:f32[] = sub e 0.5     g:f32[] = mul f 5.0     h:f32[1] = dynamic_slice[slice_sizes=(1,)] b 1     i:f32[] = squeeze[dimensions=(0,)] h     j:f32[] = div i 1080.0     k:f32[] = sub j 0.5     l:f32[] = mul k 5.0     m:i32[] _:f32[] _:f32[] = while[       body_jaxpr={ lambda ; n:f32[] o:f32[] p:i32[] q:f32[] r:f32[]. let           s:f32[] = mul q q           t:f32[] = mul r r           u:f32[] = sub s t           v:f32[] = add u n           w:f32[] = mul 2.0 q           x:f32[] = mul w r           y:f32[] = add x o           z:i32[] = add p 1         in (z, v, y) }       body_nconsts=2       cond_jaxpr={ lambda ; ba:i32[] bb:f32[] bc:f32[]. let           bd:f32[] = mul bb bb           be:f32[] = mul bc bc           bf:f32[] = add bd be           bg:bool[] = le bf 4.0           bh:bool[] = lt ba 500           bi:bool[] = convert_element_type[new_dtype=bool weak_type=False] bh           bj:bool[] = and bg bi         in (bj,) }       cond_nconsts=0     ] g l 0 g l     bk:bool[] = eq m 500     bl:f32[] = convert_element_type[new_dtype=float32 weak_type=True] m     bm:f32[] = div bl 500.0     bn:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bm     bo:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0.0     bp:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] bn     bq:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0.0     br:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 1.0     bs:f32[4] = concatenate[dimension=0] bo bp bq br     bt:f32[4] = pjit[       jaxpr={ lambda ; bu:bool[] bv:f32[4] bw:f32[4]. let           bx:bool[4] = broadcast_in_dim[broadcast_dimensions=() shape=(4,)] bu           by:f32[4] = select_n bx bw bv         in (by,) }       name=_where     ] bk a bs   in (bt,) } ``` The Jaxpr treats the `jnp.array([0, 0, 0, 1])` as an input argument, which is a wrong behavior. But I found that the `invars` for the Jaxpr is empty. Is this a bug or feature? If it is a feature, how can I get the Jaxpr for a function without argument correctly?  What jax/jaxlib version are you using? Internal version  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Jaxpr of a function without input argument is wrong," Description I am writing a function without any input argument and want to translate it into Jaxpr. Here is the example, ```py def func():   frag_coord = jnp.zeros(([4]))   real = (frag_coord[0] / 1080.0  0.5) * 5.0   imag = (frag_coord[1] / 1080.0  0.5) * 5.0   r_a = real   r_b = imag   max_iteration = 500   def body_func(carry):     i, a, b = carry     t_a = a     a = a * a  b * b + r_a     b = 2 * t_a * b + r_b     return i + 1, a, b   def cond_func(carry):     i, a, b = carry     return ((a * a + b * b) <= 4) & (i < max_iteration)   i = lax.while_loop(cond_func, body_func, (0, real, imag))[0]   res = jnp.where(       i == max_iteration,       jnp.array([0, 0, 0, 1], jnp.float32),       jnp.array([0, i / max_iteration, 0, 1], jnp.float32),   )   return res jaxpr = jax.make_jaxpr(func)().jaxpr print(jaxpr) ``` The output Jaxpr: ```py { lambda a:f32[4]; . let     b:f32[4] = broadcast_in_dim[broadcast_dimensions=() shape=(4,)] 0.0     c:f32[1] = dynamic_slice[slice_sizes=(1,)] b 0     d:f32[] = squeeze[dimensions=(0,)] c     e:f32[] = div d 1080.0     f:f32[] = sub e 0.5     g:f32[] = mul f 5.0     h:f32[1] = dynamic_slice[slice_sizes=(1,)] b 1     i:f32[] = squeeze[dimensions=(0,)] h     j:f32[] = div i 1080.0     k:f32[] = sub j 0.5     l:f32[] = mul k 5.0     m:i32[] _:f32[] _:f32[] = while[       body_jaxpr={ lambda ; n:f32[] o:f32[] p:i32[] q:f32[] r:f32[]. let           s:f32[] = mul q q           t:f32[] = mul r r           u:f32[] = sub s t           v:f32[] = add u n           w:f32[] = mul 2.0 q           x:f32[] = mul w r           y:f32[] = add x o           z:i32[] = add p 1         in (z, v, y) }       body_nconsts=2       cond_jaxpr={ lambda ; ba:i32[] bb:f32[] bc:f32[]. let           bd:f32[] = mul bb bb           be:f32[] = mul bc bc           bf:f32[] = add bd be           bg:bool[] = le bf 4.0           bh:bool[] = lt ba 500           bi:bool[] = convert_element_type[new_dtype=bool weak_type=False] bh           bj:bool[] = and bg bi         in (bj,) }       cond_nconsts=0     ] g l 0 g l     bk:bool[] = eq m 500     bl:f32[] = convert_element_type[new_dtype=float32 weak_type=True] m     bm:f32[] = div bl 500.0     bn:f32[] = convert_element_type[new_dtype=float32 weak_type=False] bm     bo:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0.0     bp:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] bn     bq:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0.0     br:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 1.0     bs:f32[4] = concatenate[dimension=0] bo bp bq br     bt:f32[4] = pjit[       jaxpr={ lambda ; bu:bool[] bv:f32[4] bw:f32[4]. let           bx:bool[4] = broadcast_in_dim[broadcast_dimensions=() shape=(4,)] bu           by:f32[4] = select_n bx bw bv         in (by,) }       name=_where     ] bk a bs   in (bt,) } ``` The Jaxpr treats the `jnp.array([0, 0, 0, 1])` as an input argument, which is a wrong behavior. But I found that the `invars` for the Jaxpr is empty. Is this a bug or feature? If it is a feature, how can I get the Jaxpr for a function without argument correctly?  What jax/jaxlib version are you using? Internal version  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-07-06T18:15:50Z,question,closed,0,3,https://github.com/jax-ml/jax/issues/16643,"It seems that it is not related to whether there is any argument or not. After I add one argument to the function, it stills treat the `jnp.array([0, 0, 0, 1], jnp.float32)` in the `jnp.where` as one *addtional* input argument.","Hi  thanks for the report! This is expected behavior. Essentially the only way to get array data into jaxprs is to either (1) create the array with a primitive like `iota` (i.e. `arange`) or `full`, or (2) pass the data as an argument to the jaxpr. In this case, you created an array within your function, but there's no XLA primitive for `jnp.asarray` with arbitrary Python arguments. So in the process of tracing this, JAX constructs that array and adds it as an implicit argument to the jaxpr. Does that make sense?",Thanks for your explanation! I will create the array in another way.
1573,"以下是一个github上的jax下的一个issue, 标题是(`jax.lax.while_loop` with no attribute error `GSPMDSharding`)， 内容是 ( Description Hi,  I'm trying to implement a finding root algorithm `bisect` in the following function.  ```python def bisect(fn: callable, target, lower, upper, precision=1e6, max_iter=100_000):     if lower >= upper:         raise ValueError(""`lower` should be smaller than `upper`"")     if fn(lower) > fn(upper):         fn = lambda x: fn(x)         return bisect(fn, target, lower, upper, precision, max_iter)     def cond_fn(val):         lower, upper, n_iter = val         return jnp.logical_and(upper lower > precision, n_iter = target, lower, m)         upper = jnp.where(output  41 from jax._src import sharding_impls      42 from jax._src import source_info_util      43 from jax._src import traceback_util [~/anaconda3/lib/python3.8/sitepackages/jax/_src/sharding_impls.py) in      654      655  > 656 (xc.GSPMDSharding)     657 class GSPMDSharding(XLACompatibleSharding):     658   _devices: Tuple[Device, ...] AttributeError: module 'jaxlib.xla_client' has no attribute 'GSPMDSharding' ``` A similar error occurs when I've tried to use `jaxopt.Bisection`.    What jax/jaxlib version are you using? jax v0.4.13 jaxlib v0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.5, Ubuntu  NVIDIA GPU info ``` NVIDIASMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.lax.while_loop` with no attribute error `GSPMDSharding`," Description Hi,  I'm trying to implement a finding root algorithm `bisect` in the following function.  ```python def bisect(fn: callable, target, lower, upper, precision=1e6, max_iter=100_000):     if lower >= upper:         raise ValueError(""`lower` should be smaller than `upper`"")     if fn(lower) > fn(upper):         fn = lambda x: fn(x)         return bisect(fn, target, lower, upper, precision, max_iter)     def cond_fn(val):         lower, upper, n_iter = val         return jnp.logical_and(upper lower > precision, n_iter = target, lower, m)         upper = jnp.where(output  41 from jax._src import sharding_impls      42 from jax._src import source_info_util      43 from jax._src import traceback_util [~/anaconda3/lib/python3.8/sitepackages/jax/_src/sharding_impls.py) in      654      655  > 656 (xc.GSPMDSharding)     657 class GSPMDSharding(XLACompatibleSharding):     658   _devices: Tuple[Device, ...] AttributeError: module 'jaxlib.xla_client' has no attribute 'GSPMDSharding' ``` A similar error occurs when I've tried to use `jaxopt.Bisection`.    What jax/jaxlib version are you using? jax v0.4.13 jaxlib v0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.5, Ubuntu  NVIDIA GPU info ``` NVIDIASMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6 ```",2023-07-06T06:54:35Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16636,"I'm sorry for mistaking this with a bug. It seems there is problem with my Jupyter notebook. Upon restart the notebook, the error is solved. I don't know why it is so though."
6099,"以下是一个github上的jax下的一个issue, 标题是(TypeError: Shapes must be 1D sequences of concrete values of integer type- dynamic_slice)， 内容是 ( Description I'm using jax to implement a pca function, but I get an error. Here's the code ``` import numpy as np  import jax.numpy as jnp from jax import jit, vmap import jax from jax import lax from functools import partial class PCA:     def __init__(self, n_components=None, svd_solver='full'):         self.n_components = n_components         self.svd_solver = svd_solver         self.components = None         self.mean = None         self.explained_variance = None     def QR_GivenRot(self,mat):         rows, cols = mat.shape         R = jnp.copy(mat)         Q = jnp.eye(cols)         def loop_body_fn(col, RQ):             R, Q = RQ             def update_RQ(RQ):                 R, Q = RQ                 if jnp.abs(R[row, col]) .loop_body_fn(row, H_eigv)      61 def loop_body_fn(row, H_eigv):      62     H, eigv = H_eigv > 63     array = H[row:, row1]      65      Householder      66     a = jnp.linalg.norm(array) File /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py:3908, in _rewriting_take(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)    3905       return lax.dynamic_index_in_dim(arr, idx, keepdims=False)    3907 treedef, static_idx, dynamic_idx = _split_index_for_jit(idx, arr.shape) > 3908 return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,    3909                unique_indices, mode, fill_value) File /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py:3917, in _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)    3914 def _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,    3915             unique_indices, mode, fill_value):    3916   idx = _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx) > 3917   indexer = _index_to_gather(shape(arr), idx)   shared with _scatter_update    3918   y = arr    3920   if fill_value is not None: File /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py:4169, in _index_to_gather(x_shape, idx, normalize_indices)    4160 if not _all(_is_slice_element_none_or_constant(elt)    4161             for elt in (start, stop, step)):    4162   msg = (""Array slice indices must have static start/stop/step to be used ""    4163          ""with NumPy indexing syntax. ""    4164          f""Found slice({start}, {stop}, {step}). ""    (...)    4167          ""dynamic_update_slice (JAX does not support dynamically sized ""    4168          ""arrays within JIT compiled functions)."") > 4169   raise IndexError(msg)    4170 if not core.is_constant_dim(x_shape[x_axis]):    4171   msg = (""Cannot use NumPy slice indexing on an array dimension whose ""    4172          f""size is not statically known ({x_shape[x_axis]}). ""    4173          ""Try using lax.dynamic_slice/dynamic_update_slice"") IndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Tracedwith, None, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions). ``` When I modify it with lax.dynamic_slice `array = lax.dynamic_slice(H, (row, row  1), (rows  row,))` An error occurs again ``` TypeError                                 Traceback (most recent call last) Cell In[72], line 156     154 X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])     155 pca = PCA(n_components=2) > 156 pca.fit(X) Cell In[72], line 138, in PCA.fit(self, X)     136 X_centered = X  self.mean     137 if self.svd_solver == 'full': > 138     U, S, V = self.SVD(X_centered, 0)     139 V = V[:self.n_components]     140  V = lax.dynamic_slice(V, (0, 0), (self.n_components,V.shape[0])) Cell In[72], line 131, in PCA.SVD(self, mat, Method)     128 def SVD(self, mat, Method):     129      Method: 0: QR_Basic, 1: QR_Heisenberg, 2: Jacobi_Basic     130     if Method == 0: > 131         U, S, V = self.SVD_Solver(mat, self.QR_Heisenberg)     132     return U, S, V Cell In[72], line 113, in PCA.SVD_Solver(self, mat, Func)     110 aTa = jnp.dot(mat.T, mat)     111 aaT = jnp.dot(mat.T, mat) > 113 eig, eigv = Func(aTa)     114 eig, eigv = self.SortEig(eig, eigv)     116 rows, cols = mat.shape Cell In[72], line 84, in PCA.QR_Heisenberg(self, mat)      80     eigv = jnp.dot(eigv, T.T)      82     return H, eigv > 84 _, eigv = lax.fori_loop(1, rows  1, loop_body_fn, (Hsbg, eigv))      85 eigv = eigv.T      87  Given     [... skipping hidden 12 frame] Cell In[72], line 63, in PCA.QR_Heisenberg..loop_body_fn(row, H_eigv)      61 def loop_body_fn(row, H_eigv):      62     H, eigv = H_eigv > 63     array = lax.dynamic_slice(H, (row, row  1), (rows  row,))      65      Householder      66     a = jnp.linalg.norm(array)     [... skipping hidden 1 frame] File /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/core.py:2039, in canonicalize_shape(shape, context)    2037 except TypeError:    2038   pass > 2039 raise _invalid_shape_error(shape, context) TypeError: Shapes must be 1D sequences of concrete values of integer type, got (Tracedwith,). If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. The error occurred while tracing the function scanned_fun at /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/lax/control_flow/loops.py:1668 for scan. This concrete value was not available in Python because it depends on the value of the argument loop_carry[0]. ``` How can I fix it? Thanks a lot!  What jax/jaxlib version are you using? jax  0.4.8, jaxlib  0.4.7  Which accelerator(s) are you using? GPU  Additional system info python 3.8 , linux/mac  NVIDIA GPU info ++               )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,TypeError: Shapes must be 1D sequences of concrete values of integer type- dynamic_slice," Description I'm using jax to implement a pca function, but I get an error. Here's the code ``` import numpy as np  import jax.numpy as jnp from jax import jit, vmap import jax from jax import lax from functools import partial class PCA:     def __init__(self, n_components=None, svd_solver='full'):         self.n_components = n_components         self.svd_solver = svd_solver         self.components = None         self.mean = None         self.explained_variance = None     def QR_GivenRot(self,mat):         rows, cols = mat.shape         R = jnp.copy(mat)         Q = jnp.eye(cols)         def loop_body_fn(col, RQ):             R, Q = RQ             def update_RQ(RQ):                 R, Q = RQ                 if jnp.abs(R[row, col]) .loop_body_fn(row, H_eigv)      61 def loop_body_fn(row, H_eigv):      62     H, eigv = H_eigv > 63     array = H[row:, row1]      65      Householder      66     a = jnp.linalg.norm(array) File /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py:3908, in _rewriting_take(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)    3905       return lax.dynamic_index_in_dim(arr, idx, keepdims=False)    3907 treedef, static_idx, dynamic_idx = _split_index_for_jit(idx, arr.shape) > 3908 return _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,    3909                unique_indices, mode, fill_value) File /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py:3917, in _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)    3914 def _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,    3915             unique_indices, mode, fill_value):    3916   idx = _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx) > 3917   indexer = _index_to_gather(shape(arr), idx)   shared with _scatter_update    3918   y = arr    3920   if fill_value is not None: File /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/numpy/lax_numpy.py:4169, in _index_to_gather(x_shape, idx, normalize_indices)    4160 if not _all(_is_slice_element_none_or_constant(elt)    4161             for elt in (start, stop, step)):    4162   msg = (""Array slice indices must have static start/stop/step to be used ""    4163          ""with NumPy indexing syntax. ""    4164          f""Found slice({start}, {stop}, {step}). ""    (...)    4167          ""dynamic_update_slice (JAX does not support dynamically sized ""    4168          ""arrays within JIT compiled functions)."") > 4169   raise IndexError(msg)    4170 if not core.is_constant_dim(x_shape[x_axis]):    4171   msg = (""Cannot use NumPy slice indexing on an array dimension whose ""    4172          f""size is not statically known ({x_shape[x_axis]}). ""    4173          ""Try using lax.dynamic_slice/dynamic_update_slice"") IndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Tracedwith, None, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions). ``` When I modify it with lax.dynamic_slice `array = lax.dynamic_slice(H, (row, row  1), (rows  row,))` An error occurs again ``` TypeError                                 Traceback (most recent call last) Cell In[72], line 156     154 X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])     155 pca = PCA(n_components=2) > 156 pca.fit(X) Cell In[72], line 138, in PCA.fit(self, X)     136 X_centered = X  self.mean     137 if self.svd_solver == 'full': > 138     U, S, V = self.SVD(X_centered, 0)     139 V = V[:self.n_components]     140  V = lax.dynamic_slice(V, (0, 0), (self.n_components,V.shape[0])) Cell In[72], line 131, in PCA.SVD(self, mat, Method)     128 def SVD(self, mat, Method):     129      Method: 0: QR_Basic, 1: QR_Heisenberg, 2: Jacobi_Basic     130     if Method == 0: > 131         U, S, V = self.SVD_Solver(mat, self.QR_Heisenberg)     132     return U, S, V Cell In[72], line 113, in PCA.SVD_Solver(self, mat, Func)     110 aTa = jnp.dot(mat.T, mat)     111 aaT = jnp.dot(mat.T, mat) > 113 eig, eigv = Func(aTa)     114 eig, eigv = self.SortEig(eig, eigv)     116 rows, cols = mat.shape Cell In[72], line 84, in PCA.QR_Heisenberg(self, mat)      80     eigv = jnp.dot(eigv, T.T)      82     return H, eigv > 84 _, eigv = lax.fori_loop(1, rows  1, loop_body_fn, (Hsbg, eigv))      85 eigv = eigv.T      87  Given     [... skipping hidden 12 frame] Cell In[72], line 63, in PCA.QR_Heisenberg..loop_body_fn(row, H_eigv)      61 def loop_body_fn(row, H_eigv):      62     H, eigv = H_eigv > 63     array = lax.dynamic_slice(H, (row, row  1), (rows  row,))      65      Householder      66     a = jnp.linalg.norm(array)     [... skipping hidden 1 frame] File /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/core.py:2039, in canonicalize_shape(shape, context)    2037 except TypeError:    2038   pass > 2039 raise _invalid_shape_error(shape, context) TypeError: Shapes must be 1D sequences of concrete values of integer type, got (Tracedwith,). If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. The error occurred while tracing the function scanned_fun at /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/lax/control_flow/loops.py:1668 for scan. This concrete value was not available in Python because it depends on the value of the argument loop_carry[0]. ``` How can I fix it? Thanks a lot!  What jax/jaxlib version are you using? jax  0.4.8, jaxlib  0.4.7  Which accelerator(s) are you using? GPU  Additional system info python 3.8 , linux/mac  NVIDIA GPU info ++               ",2023-07-04T05:59:25Z,question,closed,0,5,https://github.com/jax-ml/jax/issues/16622,"The issue is that `row` is a traced value, so `array = H[row:, row1]` has a dynamic shape. JAX transormations like `jit` and `vmap`, as well as higherorder functions like `scan` and `fori_loop` require all array shapes to be static. This means that the shapes of arrays cannot depend on dynamic values like `row` in your program. If you want to use JAX, you'll have to reexpress your program in terms of staticallyshaped arrays. You can read more at JAX Sharp Bits: Dynamic Shapes.","> The issue is that `row` is a traced value, so `array = H[row:, row1]` has a dynamic shape. JAX transormations like `jit` and `vmap`, as well as higherorder functions like `scan` and `fori_loop` require all array shapes to be static. This means that the shapes of arrays cannot depend on dynamic values like `row` in your program. If you want to use JAX, you'll have to reexpress your program in terms of staticallyshaped arrays. >  > You can read more at JAX Sharp Bits: Dynamic Shapes. Thanks ! How can I reexpress my program in terms of staticallyshaped arrays? I didn't find a suitable example in the tutorial yet.😭","I don’t see any easy way to express this program in terms of static shapes, but I probably don’t understand the algorithm as well as you do.","  I'm sorry if my algorithm has confused you. I rewrote a simple version of the function ``` from functools import partial from typing import NamedTuple import jax import jax.numpy as jnp import numpy as np import unittest import json import jax.numpy as jnp import spu.utils.simulation as spsim import spu.spu_pb2 as spu_pb2    class PCAState(NamedTuple):     components: jax.Array     means: jax.Array     explained_variance: jax.Array def transform(state, x):     x = x  state.means     return jnp.dot(x, jnp.transpose(state.components)) def recover(state, x):     return jnp.dot(x, state.components) + state.means def fit(x, n_components, solver=""full"", rng=None):     if solver == ""full"":         return _fit_full(x, n_components)     elif solver == ""randomized"":         if rng is None:             rng = jax.random.PRNGKey(n_components)         return _fit_randomized(x, n_components, rng)     else:         raise ValueError(""solver parameter is not correct"")  (jax.jit, static_argnums=(1,)) def fit_and_transform(x, n_components=2):     state = fit(x, n_components)     return transform(state, x)  (jax.jit, static_argnames=[""n_components""]) def _fit_full(x, n_components):     n_samples, n_features = x.shape      Subtract the mean of the input data     means = x.mean(axis=0, keepdims=True)     x = x  means      Factorize the data matrix with singular value decomposition.     U, S, Vt = jax.scipy.linalg.svd(x, full_matrices=False)      Compute the explained variance     explained_variance = (S[:n_components] ** 2) / (n_samples  1)      Return the transformation matrix     A = Vt[:n_components]     return PCAState(components=A, means=means, explained_variance=explained_variance) def _fit_randomized(x, n_components, rng, n_iter=5):     """"""Randomized PCA based on Halko et al [https://doi.org/10.48550/arXiv.1007.5510].""""""     n_samples, n_features = x.shape     means = jnp.mean(x, axis=0, keepdims=True)     x = x  means      Generate n_features normal vectors of the given size     size = jnp.minimum(2 * n_components, n_features)     Q = jax.random.normal(rng, shape=(n_features, size))     def step_fn(q, _):         q, _ = jax.scipy.linalg.lu(x @ q, permute_l=True)         q, _ = jax.scipy.linalg.lu(x.T @ q, permute_l=True)         return q, None     Q, _ = jax.lax.scan(step_fn, init=Q, xs=None, length=n_iter)     Q, _ = jax.scipy.linalg.qr(x @ Q, mode=""economic"")     B = Q.T @ x     _, S, Vt = jax.scipy.linalg.svd(B, full_matrices=False)     explained_variance = (S[:n_components] ** 2) / (n_samples  1)     A = Vt[:n_components]     return PCAState(components=A, means=means, explained_variance=explained_variance) X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) x_centered = X  X.mean(axis=0, keepdims=True) state = fit(x_centered,n_components=2) X_pca = transform(state, x_centered) X_recovered = recover(state, X_pca)  def fit_and_transform(x,n_components=2):      state = fit(x,n_components)      return transform(state,x) result = jax.jit(_fit_full)(x_centered, 2)  sim  = spsim.Simulator.simple(      3, spu_pb2.ProtocolKind.ABY3, spu_pb2.FieldType.FM64  )  result = spsim.sim_jax(sim, _fit_full)(x_centered, 2)  result = spsim.sim_jax(sim, fit_and_transform)(x_centered, 2) ``` The main problem I encountered was how to split the data from the first n_components. The error of the above code is: ``` Cell In[21], line 58, in _fit_full(x, n_components)      55 U, S, Vt = jax.scipy.linalg.svd(x, full_matrices=False)      57  Compute the explained variance > 58 explained_variance = (S[:n_components] ** 2) / (n_samples  1)      60  Return the transformation matrix      61 A = Vt[:n_components]     [... skipping hidden 1 frame] IndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(None, Tracedwith, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions). ``` When I use lax.dynamic_slice, I still get an error: ``` Cell In[23], line 59, in _fit_full(x, n_components)      55 U, S, Vt = jax.scipy.linalg.svd(x, full_matrices=False)      57  Compute the explained variance > 59 explained_variance = (jax.lax.dynamic_slice(S, (0,), (n_components,)) ** 2) / (n_samples  1)      61  Return the transformation matrix      62 A = Vt[:n_components]     [... skipping hidden 1 frame] File /opt/homebrew/Caskroom/miniforge/base/envs/sf/lib/python3.8/sitepackages/jax/_src/core.py:2039, in canonicalize_shape(shape, context)    2037 except TypeError:    2038   pass > 2039 raise _invalid_shape_error(shape, context) TypeError: Shapes must be 1D sequences of concrete values of integer type, got (Tracedwith,). If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. The error occurred while tracing the function _fit_full at /var/folders/1b/fb42jjtx1b707mz9njgqkd1c0000gp/T/ipykernel_93715/3940440843.py:47 for jit. This concrete value was not available in Python because it depends on the value of the argument n_components. ```","Understood  it’s not possible to split out the first `n` components if `n` is dynamic. Note that `dynamic_slice` will not help you here because it is for slicing a static *number* of elements at a dynamic *location*, not slicing a dynamic number of elements. You’ll have to figure out how to reexpress your algorithm in terms of staticallysized slices if you want to use this algorithm with JAX. It’s not clear to me that there’s a straightforward approach to this given your code, but you may be able to figure one out."
1722,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Improve shape constraint checking using shape assertions)， 内容是 (This is a major improvement in the shape constraint checking. Even before this change JAX assumes certain constraints on the shapes of actual arguments in presence of shape polymorphism. For example,  given the `polymorphic_shapes=""(b, b, 2*d)""` specification for an argument `arg`, JAX assumes the following:   * `arg.shape[0] >= 1`   * `arg.shape[1] == arg.shape[0]`   * `arg.shape[2] % 2 == 0` and `arg.shape[0] // 2 >= 1` Previously, these constraints were checked only in graph serialization mode and only for eager execution, when we had access to the actual shapes of `arg` during tracing. With this change we check them for native serialization also (relying on newly added support for shape assertions to tf.XlaCallModule version 7). For graph serialization, we use `tf.debugger.assert`, which works in eager execution mode **and** graph execution mode (but not `jit_compile=True` mode, due to TF limitations) and produces the same errors as for native serialization. Note that even after submitting this change it does not become active until the default serialization version is bumped to version 7, in about one month. We also add a mechanism to disable these checks using the `disabled_checks` parameter to `jax2tf.convert` or the `TF_XLA_FLAGS` environment variable. Note that there is no mechanism to disable these checks for graph serialization. We expect this to be fine, since more users will be switching to native serialization before this change lands.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Improve shape constraint checking using shape assertions,"This is a major improvement in the shape constraint checking. Even before this change JAX assumes certain constraints on the shapes of actual arguments in presence of shape polymorphism. For example,  given the `polymorphic_shapes=""(b, b, 2*d)""` specification for an argument `arg`, JAX assumes the following:   * `arg.shape[0] >= 1`   * `arg.shape[1] == arg.shape[0]`   * `arg.shape[2] % 2 == 0` and `arg.shape[0] // 2 >= 1` Previously, these constraints were checked only in graph serialization mode and only for eager execution, when we had access to the actual shapes of `arg` during tracing. With this change we check them for native serialization also (relying on newly added support for shape assertions to tf.XlaCallModule version 7). For graph serialization, we use `tf.debugger.assert`, which works in eager execution mode **and** graph execution mode (but not `jit_compile=True` mode, due to TF limitations) and produces the same errors as for native serialization. Note that even after submitting this change it does not become active until the default serialization version is bumped to version 7, in about one month. We also add a mechanism to disable these checks using the `disabled_checks` parameter to `jax2tf.convert` or the `TF_XLA_FLAGS` environment variable. Note that there is no mechanism to disable these checks for graph serialization. We expect this to be fine, since more users will be switching to native serialization before this change lands.",2023-07-03T14:35:35Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/16615,"  PTAL. I would like to submit this, even though we cannot activate it for a while as explained in the descritiption, because I have more changes coming stacked on top of this. You can also see this as cl/547484806 if it is more convenient.",This got merged as part of merging https://github.com/google/jax/pull/16710.
809,"以下是一个github上的jax下的一个issue, 标题是(increase random test coverage over RNG key constructors and representations)， 内容是 (This is an incremental change to our random tests that primarily: * Increases test coverage of both key constructors (`random.key` and `random.PRNGKey`), often by parameterizing tests over both. * Increases test coverage of both key representations (typed key arrays and `uint32` arrays). * Removes a handful of guards on `config.jax_enable_custom_prng`, either replacing them with `isinstance` checks for typed keys or removing them altogether if possible. * Makes a handful of other individual test improvements and fixes, and leaves comments for more.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,increase random test coverage over RNG key constructors and representations,"This is an incremental change to our random tests that primarily: * Increases test coverage of both key constructors (`random.key` and `random.PRNGKey`), often by parameterizing tests over both. * Increases test coverage of both key representations (typed key arrays and `uint32` arrays). * Removes a handful of guards on `config.jax_enable_custom_prng`, either replacing them with `isinstance` checks for typed keys or removing them altogether if possible. * Makes a handful of other individual test improvements and fixes, and leaves comments for more.",2023-07-01T03:31:25Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16605
1051,"以下是一个github上的jax下的一个issue, 标题是(Jax' transfer guard and XLA-CPU)， 内容是 ( Description Jax' transfer guard  potentially has two problems related to the CPUplaced arrays. 1. Transfering numpy (or pure python) arrays/scalars to XLACPU device is considered to be host2device transfer: ``` jax.jit(jax.random.PRNGKey, backend='cpu')(np.array(0)) ``` Transfer guard says: `hosttodevice transfer: aval=ShapedArray(int32[]), dst_sharding=GSPMDSharding({replicated})` Ideally I want to be able to ignore `numpy > XLACPU` transfers and only trigger the transfer guard for transfers to the accelerators. 2. Interestingly, XLACPU > numpy is not triggering transfer guard: ``` np.array(jax.device_put(0, device=jax.devices('cpu')[0])) ``` Transfer guard says: 🤐.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax' transfer guard and XLA-CPU," Description Jax' transfer guard  potentially has two problems related to the CPUplaced arrays. 1. Transfering numpy (or pure python) arrays/scalars to XLACPU device is considered to be host2device transfer: ``` jax.jit(jax.random.PRNGKey, backend='cpu')(np.array(0)) ``` Transfer guard says: `hosttodevice transfer: aval=ShapedArray(int32[]), dst_sharding=GSPMDSharding({replicated})` Ideally I want to be able to ignore `numpy > XLACPU` transfers and only trigger the transfer guard for transfers to the accelerators. 2. Interestingly, XLACPU > numpy is not triggering transfer guard: ``` np.array(jax.device_put(0, device=jax.devices('cpu')[0])) ``` Transfer guard says: 🤐.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-30T17:34:51Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/16602,"Thanks for the report! I suspect the reason for this is that while XLA:CPU → NumPy transfers can always be done in a zerocopy fashion, NumPy → XLA:CPU transfers cannot always be done in this manner. The reason for this asymmetry is that XLA arrays have stricter requirements regarding byte layout than do NumPy arrays.  may be able to provide more context on the behavior you observe here","Yes, as  pointed out, the asymmetry came from the zerocopy vs. copy difference in two transfer directions.","I see, thanks for the clarification regarding the asymmetry. It was actually not my main question, sorry for obscure phrasing. The main problem is that I can not configure transfer guard to disallow implicit accelerator transfers, while allowing XLA:CPU transfers. Normally, I do not care much about host/cpu memory, but care about acceleratorrelated transfers and memory. Do you think you can add something like `jax.transfer_guard(""disallow_accelerator)` or  `jax.transfer_guard(""disallow_tpu)`?","There seem to be a few potential highlevel design options: 1. Focusing on logical operations that involve converting NumPy array and JAX array, or transferring JAX arrays to different devices: This is semantically clean/succinct though it may not necessarily be the not useful form of the API. In this direction, we will want to make XLA:CPU to NumPy to be guarded and remove the asymmetry. 2. Focusing on involved backend or platform types (e.g., ""CPU"", ""GPU"", ""TPU""): It takes into account the backend/platform types that are involved in the transfer, expecting the user to apply expert knowledge on which backend/platform types are considered costly when used for transfers. 3. Focusing on involved communication channels (e.g., ""memcpy"", ""pcie"", ""host network"", ""accelerator interconnect"": This instead makes JAX knowledgeable on which underlying communication channels of transfers can be expensive. JAX needs to determine which level it can draw a line, or it may attempt to let the user choose a certain transfer type. Since lowlevel runtimes typically do not explicitly reveal what communication channels are used, JAX often has to guess communication channel types being used by the lowlevel runtimes. The current transfer guard is close to Option 3 with a single threshold that looks at memcpy/relayouting. As for this feature request to extend the transfer guard API, potential candidates are (a) Option 2 with multiple backend/paltform type support (proposal in https://github.com/google/jax/issues/16602issuecomment1625413621). (b) Option 2 with XLA:CPU being ignored for all users as a special case. (c) Option 3 with multiple channel type support. (d) Option 3 with the single threshold that is raised to ignore local memcpy/relayouting for all users. Is this list complete? I think we should also hear from JAX core runtime devs.","Just stumbled upon this too and agree with Alex. The guard implementation (and most of Jax actually) already makes very clear difference between the concept of ""host"" and ""device"": https://github.com/google/jax/blob/7e1278c0400eed18a57b6402f9308c023493f942/jax/_src/config.pyL1218L1221 With that in mind, I think specialcasing ""host to host transfer"" (ie memcpy/XLA:CPU) as in your (b) suggestion seems to be the best tradeoff between practical usefulness, work required, and simplicity. Edit: I guess another way out is to allow us to cook up our own transfer guard, which would be possible if `transfer_guard_device_to_device` came with a flag or callback to choose which devices it covers. Edit on the edit: or maybe not, as I'd like to configure such transfer guard globally using `jax.config.update`."
5427,"以下是一个github上的jax下的一个issue, 标题是(vmap raises ambiguous exception when inconsistent sizes for array axes to be mapped occurs with user defined objects as static parameters)， 内容是 ( Description When trying to use vmap with functions, that accept other functions, an ambiguous error is raised if there is are inconsistent sizes for array axes to be mapped. A minimal example is shown below: ``` import jax.numpy as jnp import jax def functionThatTakesAFunction(array1,array2,function):     """"""     Takes in two arrays, applies function 1 to one of them and returns     """"""     array3 = function(array1)     return jnp.vstack((array3,array2)),jnp.hstack((array3,array2)) def multiplyBy2(array):     return 2 * array batchMultiplyAndStack = jax.vmap(functionThatTakesAFunction,in_axes=[0,0,None]) test = jnp.array([[1,2],                   [1,2]]) test2 = jnp.array([[1,2],                    [1,2],                    [1,2]]) testStacked = jnp.array((test,test,test)) test2Stacked = jnp.array((test2,test2,test2)) print(batchMultiplyAndStack(test,test2,multiplyBy2)) ``` Running the above code produces the following stack trace: ``` Traceback (most recent call last):   File ""/Users/user/Desktop/jax/jax/_src/api_util.py"", line 562, in shaped_abstractify     return _shaped_abstractify_handlerstype(x) KeyError:  During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/Users/user/Desktop/test.py"", line 41, in      print(batchMultiplyAndStack(test,test2,multiplyBy2))   File ""/Users/user/Desktop/jax/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1239, in vmap_f     _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, ""vmap""))   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1298, in _mapped_axis_size     key_paths = [f'argument {name}{keystr(p)} '   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1299, in      f'of type {_get_argument_type(x)}'   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1280, in _get_argument_type     return shaped_abstractify(x).str_short()   File ""/Users/user/Desktop/jax/jax/_src/api_util.py"", line 564, in shaped_abstractify     return _shaped_abstractify_slow(x)   File ""/Users/user/Desktop/jax/jax/_src/api_util.py"", line 553, in _shaped_abstractify_slow     raise TypeError( jax._src.traceback_util.UnfilteredStackTrace: TypeError: Cannot interpret value of type  as an abstract array; it does not have a dtype attribute The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/user/Desktop/test.py"", line 41, in      print(batchMultiplyAndStack(test,test2,multiplyBy2)) TypeError: Cannot interpret value of type  as an abstract array; it does not have a dtype attribute ``` I've tracked down the issue to how `jax._src.api._mapped_axis_size` creates the exception to inform users about inconsistent sizes for array axes to be mapped. When trying to get the data type of a function object, a TypeError is raised and not caught, resulting in an ambiguous exception as it is unclear why the exception is raised (since vmap normally works just fine with functions passed as static arguments). I've made a pull request that fixes the issue ( CC(Fixed _mapped_axis_size raising an uncaught TypeError)), but I'm not sure if my fix (making a wrapper around a problematic call to  `shaped_abstractify(x).str_short()` is compliant with jax's coding standards. With this fix we get the expected stack trace for this error: ``` Traceback (most recent call last):   File ""/Users/user/Desktop/test.py"", line 41, in      print(batchMultiplyAndStack(test,test2,multiplyBy2))   File ""/Users/user/Desktop/jax/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1239, in vmap_f     _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, ""vmap""))   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1322, in _mapped_axis_size     raise ValueError(''.join(msg)[:2])   remove last semicolon and newline jax._src.traceback_util.UnfilteredStackTrace: ValueError: vmap got inconsistent sizes for array axes to be mapped:   * one axis had size 2: axis 0 of argument array1 of type int32[2,2];   * one axis had size 3: axis 0 of argument array2 of type int32[3,2] The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/user/Desktop/test.py"", line 41, in      print(batchMultiplyAndStack(test,test2,multiplyBy2)) ValueError: vmap got inconsistent sizes for array axes to be mapped:   * one axis had size 2: axis 0 of argument array1 of type int32[2,2];   * one axis had size 3: axis 0 of argument array2 of type int32[3,2] ```  What jax/jaxlib version are you using? jax v0.4.14 jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.10, MacOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,vmap raises ambiguous exception when inconsistent sizes for array axes to be mapped occurs with user defined objects as static parameters," Description When trying to use vmap with functions, that accept other functions, an ambiguous error is raised if there is are inconsistent sizes for array axes to be mapped. A minimal example is shown below: ``` import jax.numpy as jnp import jax def functionThatTakesAFunction(array1,array2,function):     """"""     Takes in two arrays, applies function 1 to one of them and returns     """"""     array3 = function(array1)     return jnp.vstack((array3,array2)),jnp.hstack((array3,array2)) def multiplyBy2(array):     return 2 * array batchMultiplyAndStack = jax.vmap(functionThatTakesAFunction,in_axes=[0,0,None]) test = jnp.array([[1,2],                   [1,2]]) test2 = jnp.array([[1,2],                    [1,2],                    [1,2]]) testStacked = jnp.array((test,test,test)) test2Stacked = jnp.array((test2,test2,test2)) print(batchMultiplyAndStack(test,test2,multiplyBy2)) ``` Running the above code produces the following stack trace: ``` Traceback (most recent call last):   File ""/Users/user/Desktop/jax/jax/_src/api_util.py"", line 562, in shaped_abstractify     return _shaped_abstractify_handlerstype(x) KeyError:  During handling of the above exception, another exception occurred: Traceback (most recent call last):   File ""/Users/user/Desktop/test.py"", line 41, in      print(batchMultiplyAndStack(test,test2,multiplyBy2))   File ""/Users/user/Desktop/jax/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1239, in vmap_f     _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, ""vmap""))   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1298, in _mapped_axis_size     key_paths = [f'argument {name}{keystr(p)} '   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1299, in      f'of type {_get_argument_type(x)}'   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1280, in _get_argument_type     return shaped_abstractify(x).str_short()   File ""/Users/user/Desktop/jax/jax/_src/api_util.py"", line 564, in shaped_abstractify     return _shaped_abstractify_slow(x)   File ""/Users/user/Desktop/jax/jax/_src/api_util.py"", line 553, in _shaped_abstractify_slow     raise TypeError( jax._src.traceback_util.UnfilteredStackTrace: TypeError: Cannot interpret value of type  as an abstract array; it does not have a dtype attribute The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/user/Desktop/test.py"", line 41, in      print(batchMultiplyAndStack(test,test2,multiplyBy2)) TypeError: Cannot interpret value of type  as an abstract array; it does not have a dtype attribute ``` I've tracked down the issue to how `jax._src.api._mapped_axis_size` creates the exception to inform users about inconsistent sizes for array axes to be mapped. When trying to get the data type of a function object, a TypeError is raised and not caught, resulting in an ambiguous exception as it is unclear why the exception is raised (since vmap normally works just fine with functions passed as static arguments). I've made a pull request that fixes the issue ( CC(Fixed _mapped_axis_size raising an uncaught TypeError)), but I'm not sure if my fix (making a wrapper around a problematic call to  `shaped_abstractify(x).str_short()` is compliant with jax's coding standards. With this fix we get the expected stack trace for this error: ``` Traceback (most recent call last):   File ""/Users/user/Desktop/test.py"", line 41, in      print(batchMultiplyAndStack(test,test2,multiplyBy2))   File ""/Users/user/Desktop/jax/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1239, in vmap_f     _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, ""vmap""))   File ""/Users/user/Desktop/jax/jax/_src/api.py"", line 1322, in _mapped_axis_size     raise ValueError(''.join(msg)[:2])   remove last semicolon and newline jax._src.traceback_util.UnfilteredStackTrace: ValueError: vmap got inconsistent sizes for array axes to be mapped:   * one axis had size 2: axis 0 of argument array1 of type int32[2,2];   * one axis had size 3: axis 0 of argument array2 of type int32[3,2] The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/user/Desktop/test.py"", line 41, in      print(batchMultiplyAndStack(test,test2,multiplyBy2)) ValueError: vmap got inconsistent sizes for array axes to be mapped:   * one axis had size 2: axis 0 of argument array1 of type int32[2,2];   * one axis had size 3: axis 0 of argument array2 of type int32[3,2] ```  What jax/jaxlib version are you using? jax v0.4.14 jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.10, MacOS  NVIDIA GPU info _No response_",2023-06-29T23:05:11Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/16594,"Thanks for the report! Here's a shorter repro: ```python import jax import jax.numpy as jnp def f(x1, x2, g):   return g(x1, x2) jax.vmap(f, (0, 0, None))(jnp.ones(2), jnp.ones(3), jnp.add) ``` It seems the main issue is that when you pass invalid arguments to `vmap`, it can fail in constructing the error message if there are nonarray inputs.","That's a much more succinct way to put it! That's exactly the issue. The root cause seems to be an uncaught exception when trying to determine the data type of the nonarray inputs. The pull request I made tries to fix it. I just cleaned it up, but in squashing commits to one commit, there seems to be a noreply.com author () who's being blocked by the CLA check. Let me know if I need to do anything to fix that or if you'd rather fix the issue some other way.",Made a clean version of the pull request at CC(Fixed _mapped_axis_size raising an uncaught TypeError). 
1583,"以下是一个github上的jax下的一个issue, 标题是(Stricter validation of inputs to `dynamic_update_slice_in_dim`)， 内容是 (I was extremely surprised to find that `start_indices` is implicitly set to 0 for dimensions other than `axis`, rather than requiring that all other dimensions be equal or broadcastable to each other: ```python >>> z = jnp.zeros((3, 5)) >>> u = jnp.ones((2, 2)) >>> jax.lax.dynamic_update_slice_in_dim(z, u, 1, axis=0) Array([[0., 0., 0., 0., 0.],        [1., 1., 0., 0., 0.],        [1., 1., 0., 0., 0.]], dtype=float32) ``` I think it's nonobvious that this is how the `dynamic_update_*_in_dim` methods would work, so noting it in the documentation would be nice. This led to me really struggling to track down a bug yesterday, which turned out to be in the following code: ```python updated = jax.lax.dynamic_update_slice_in_dim(operand, update, start_index, axis=2) ``` Unfortunately, I called this code both with arrays of shape `(batch, head, time, hidden)` _and_ `(batch2, batch, head, time, hidden)`. When I was looking for the bug, I thought this couldn't be the problem since I was assuming it would error if it received ""incompatible"" shapes. Later when I did find it, I assumed it was just broadcasting since the `time` dimension was size 1. (Thanks for the tip on Twitter !) If this _is_ an unintended use of `dynamic_update_slice_in_dim`, it might be nice to do a dimension check, but maybe people are relying on this behavior.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Stricter validation of inputs to `dynamic_update_slice_in_dim`,"I was extremely surprised to find that `start_indices` is implicitly set to 0 for dimensions other than `axis`, rather than requiring that all other dimensions be equal or broadcastable to each other: ```python >>> z = jnp.zeros((3, 5)) >>> u = jnp.ones((2, 2)) >>> jax.lax.dynamic_update_slice_in_dim(z, u, 1, axis=0) Array([[0., 0., 0., 0., 0.],        [1., 1., 0., 0., 0.],        [1., 1., 0., 0., 0.]], dtype=float32) ``` I think it's nonobvious that this is how the `dynamic_update_*_in_dim` methods would work, so noting it in the documentation would be nice. This led to me really struggling to track down a bug yesterday, which turned out to be in the following code: ```python updated = jax.lax.dynamic_update_slice_in_dim(operand, update, start_index, axis=2) ``` Unfortunately, I called this code both with arrays of shape `(batch, head, time, hidden)` _and_ `(batch2, batch, head, time, hidden)`. When I was looking for the bug, I thought this couldn't be the problem since I was assuming it would error if it received ""incompatible"" shapes. Later when I did find it, I assumed it was just broadcasting since the `time` dimension was size 1. (Thanks for the tip on Twitter !) If this _is_ an unintended use of `dynamic_update_slice_in_dim`, it might be nice to do a dimension check, but maybe people are relying on this behavior.",2023-06-29T21:39:19Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/16592,"Hi  thanks for the report, and sorry for the unclear semantics here. Can you say more about what your expected result would be? Were you expecting an error because `u.shape[1] != z.shape[1]`?","Yeah I think I was expecting the semantics to be something like numpy slice updates, so for axis=2: ```python operand[:, :, start_index:start_index + update.shape[2]] = update ``` I think another way this might trip someone up is if they were expecting broadcasting of size1 dimensions, but instead got the 0th index updated in those dimensions.","Makes sense, thanks. I believe this was the intended usecase for this function, I might add some shape validation and see if it breaks any code.",Reopening and renaming to track the possibility of adding input validation to these functions.
5955,"以下是一个github上的jax下的一个issue, 标题是(`jax.numpy.ndarray.at[].set()` much slower than numpy on GPU?)， 内容是 (I am sorry  I have been opening new issues quite frequently on indexing in the last couple of days and that might me annoying. I am just finding some limitations in the indexing operations, and, for each I wonder if there's is a solution (or if it is a fake problem). This might be a microbenchmark, but if this holds also for a longer program, when a set of functions rely heavily on this mechanism, updates becomes an issue. I see this is also the case for TF https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update, so perhaps not a limitation of jax per sé. ```python import jax import jax.numpy as jnp import numpy as np from timeit import repeat import matplotlib.pyplot as plt def set_jax(a, idx):     return a.at[idx].set(idx) def set_numpy(a, idx):     a[idx] = idx     return a results = [] for i in range(1, 8):     shape = (10 ** i,)     idx = i      jax cpu     cpu = jax.devices(""cpu"")[0]     jax_array_cpu = jax.random.uniform(jax.random.PRNGKey(0), shape=shape)     jax_array_cpu = jax.device_put(jax_array_cpu, cpu)     idx_jax_cpu = jax.device_put(jnp.asarray(idx), cpu)     set_jax_cpu = jax.jit(set_jax, backend=""cpu"").lower(jax_array_cpu, idx_jax_cpu).compile()     jax_cpu_res = repeat(lambda: set_jax_cpu(jax_array_cpu, idx_jax_cpu).block_until_ready(), number=10, repeat=100)      jax gpu     gpu = jax.devices(""gpu"")[0]     jax_array_gpu =  jax.random.uniform(jax.random.PRNGKey(0), shape=shape)     jax_array_gpu = jax.device_put(jax_array_gpu, gpu)     idx_jax_gpu = jax.device_put(jnp.asarray(idx), gpu)     set_jax_gpu = jax.jit(set_jax, backend=""gpu"").lower(jax_array_gpu, idx_jax_gpu).compile()     jax_gpu_res = repeat(lambda: set_jax_gpu(jax_array_gpu, idx_jax_gpu).block_until_ready(), number=10, repeat=100)      np     numpy_array = np.random.uniform(size=shape)     numpy_res = repeat(lambda: set_numpy(numpy_array, idx), number=10, repeat=100)      results     jax_cpu_res = (sum(jax_cpu_res) / len(jax_cpu_res))     jax_gpu_res = (sum(jax_gpu_res) / len(jax_gpu_res))     numpy_res = (sum(numpy_res) / len(numpy_res))     results.append((jax_cpu_res, jax_gpu_res, numpy_res)) print(jnp.asarray(results)) jax_cpu_res, jax_gpu_res, numpy_res = list(zip(*results)) plt.plot(jax_cpu_res) plt.plot(jax_gpu_res) plt.plot(numpy_res) plt.yscale(""log"") plt.legend([""jax (cpu)"", ""jax (gpu)"", ""numpy""]) plt.show() ``` !image The x axis is shifted by `1`, so `0` should read`1`.  Also, I am sure I am misinterpreting because I don't know XLA, but it seems like the indexed array is copied at every update (`  %copy.1 = f32[100]{0} copy(f32[100]{0} %Arg_0.1)`), which is the opposite of what the docs%20compiled%20function%2C%20expressions%20like%20x%20%3D%20x.at%5Bidx%5D.set(y)%20are%20guaranteed%20to%20be%20applied%20in%2Dplace.) say? ```python print(jax.jit(set_jax, backend=""gpu"").lower(jax_array_gpu, idx_jax_gpu).compile().as_text()) ``` ``` HloModule jit_set_jax, is_scheduled=true, entry_computation_layout={(f32[100]{0}, s32[])>f32[100]{0}} %fused_computation (param_0: f32[100], param_1.2: s32[]) > f32[100] {   %param_0 = f32[100]{0} parameter(0)   %constant_40 = s32[1]{0} constant({0})   %param_1.2 = s32[] parameter(1)   %constant_39 = s32[] constant(0)   %compare.12 = pred[] compare(s32[] %param_1.2, s32[] %constant_39), direction=LT, metadata={op_name=""jit(set_jax)/jit(main)/lt"" source_file="""" source_line=9}   %constant_38 = s32[] constant(100)   %add.4 = s32[] add(s32[] %param_1.2, s32[] %constant_38), metadata={op_name=""jit(set_jax)/jit(main)/add"" source_file="""" source_line=9}   %select.6 = s32[] select(pred[] %compare.12, s32[] %add.4, s32[] %param_1.2), metadata={op_name=""jit(set_jax)/jit(main)/select_n"" source_file="""" source_line=9}   %bitcast.62 = s32[1]{0} bitcast(s32[] %select.6)   %compare.11 = pred[1]{0} compare(s32[1]{0} %constant_40, s32[1]{0} %bitcast.62), direction=LE   %constant_37 = s32[1]{0} constant({99})   %compare.10 = pred[1]{0} compare(s32[1]{0} %constant_37, s32[1]{0} %bitcast.62), direction=GE   %and.4 = pred[1]{0} and(pred[1]{0} %compare.11, pred[1]{0} %compare.10)   %convert.1 = f32[] convert(s32[] %param_1.2), metadata={op_name=""jit(set_jax)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]"" source_file="""" source_line=9}   %bitcast.61 = f32[1]{0} bitcast(f32[] %convert.1)   %dynamicslice.5 = f32[1]{0} dynamicslice(f32[100]{0} %param_0, s32[] %select.6), dynamic_slice_sizes={1}   %select.5 = f32[1]{0} select(pred[1]{0} %and.4, f32[1]{0} %bitcast.61, f32[1]{0} %dynamicslice.5)   ROOT %dynamicupdateslice.3 = f32[100]{0} dynamicupdateslice(f32[100]{0} %param_0, f32[1]{0} %select.5, s32[] %select.6), sharding={replicated}, metadata={op_name=""jit(set_jax)/jit(main)/scatter[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.FILL_OR_DROP]"" source_file="""" source_line=9} } ENTRY %main.16 (Arg_0.1: f32[100], Arg_1.2: s32[]) > f32[100] {   %Arg_1.2 = s32[] parameter(1), sharding={replicated}   %Arg_0.1 = f32[100]{0} parameter(0), sharding={replicated}   %copy.1 = f32[100]{0} copy(f32[100]{0} %Arg_0.1)   ROOT %fusion = f32[100]{0} fusion(f32[100]{0} %copy.1, s32[] %Arg_1.2), kind=kLoop, calls=%fused_computation, sharding={replicated}, frontend_attributes={fingerprint_before_lhs=""08956b1478b109d4ebe2d6dc5d47afc7""}, metadata={op_name=""jit(set_jax)/jit(main)/scatter[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.FILL_OR_DROP]"" source_file="""" source_line=9} } ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.numpy.ndarray.at[].set()` much slower than numpy on GPU?,"I am sorry  I have been opening new issues quite frequently on indexing in the last couple of days and that might me annoying. I am just finding some limitations in the indexing operations, and, for each I wonder if there's is a solution (or if it is a fake problem). This might be a microbenchmark, but if this holds also for a longer program, when a set of functions rely heavily on this mechanism, updates becomes an issue. I see this is also the case for TF https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update, so perhaps not a limitation of jax per sé. ```python import jax import jax.numpy as jnp import numpy as np from timeit import repeat import matplotlib.pyplot as plt def set_jax(a, idx):     return a.at[idx].set(idx) def set_numpy(a, idx):     a[idx] = idx     return a results = [] for i in range(1, 8):     shape = (10 ** i,)     idx = i      jax cpu     cpu = jax.devices(""cpu"")[0]     jax_array_cpu = jax.random.uniform(jax.random.PRNGKey(0), shape=shape)     jax_array_cpu = jax.device_put(jax_array_cpu, cpu)     idx_jax_cpu = jax.device_put(jnp.asarray(idx), cpu)     set_jax_cpu = jax.jit(set_jax, backend=""cpu"").lower(jax_array_cpu, idx_jax_cpu).compile()     jax_cpu_res = repeat(lambda: set_jax_cpu(jax_array_cpu, idx_jax_cpu).block_until_ready(), number=10, repeat=100)      jax gpu     gpu = jax.devices(""gpu"")[0]     jax_array_gpu =  jax.random.uniform(jax.random.PRNGKey(0), shape=shape)     jax_array_gpu = jax.device_put(jax_array_gpu, gpu)     idx_jax_gpu = jax.device_put(jnp.asarray(idx), gpu)     set_jax_gpu = jax.jit(set_jax, backend=""gpu"").lower(jax_array_gpu, idx_jax_gpu).compile()     jax_gpu_res = repeat(lambda: set_jax_gpu(jax_array_gpu, idx_jax_gpu).block_until_ready(), number=10, repeat=100)      np     numpy_array = np.random.uniform(size=shape)     numpy_res = repeat(lambda: set_numpy(numpy_array, idx), number=10, repeat=100)      results     jax_cpu_res = (sum(jax_cpu_res) / len(jax_cpu_res))     jax_gpu_res = (sum(jax_gpu_res) / len(jax_gpu_res))     numpy_res = (sum(numpy_res) / len(numpy_res))     results.append((jax_cpu_res, jax_gpu_res, numpy_res)) print(jnp.asarray(results)) jax_cpu_res, jax_gpu_res, numpy_res = list(zip(*results)) plt.plot(jax_cpu_res) plt.plot(jax_gpu_res) plt.plot(numpy_res) plt.yscale(""log"") plt.legend([""jax (cpu)"", ""jax (gpu)"", ""numpy""]) plt.show() ``` !image The x axis is shifted by `1`, so `0` should read`1`.  Also, I am sure I am misinterpreting because I don't know XLA, but it seems like the indexed array is copied at every update (`  %copy.1 = f32[100]{0} copy(f32[100]{0} %Arg_0.1)`), which is the opposite of what the docs%20compiled%20function%2C%20expressions%20like%20x%20%3D%20x.at%5Bidx%5D.set(y)%20are%20guaranteed%20to%20be%20applied%20in%2Dplace.) say? ```python print(jax.jit(set_jax, backend=""gpu"").lower(jax_array_gpu, idx_jax_gpu).compile().as_text()) ``` ``` HloModule jit_set_jax, is_scheduled=true, entry_computation_layout={(f32[100]{0}, s32[])>f32[100]{0}} %fused_computation (param_0: f32[100], param_1.2: s32[]) > f32[100] {   %param_0 = f32[100]{0} parameter(0)   %constant_40 = s32[1]{0} constant({0})   %param_1.2 = s32[] parameter(1)   %constant_39 = s32[] constant(0)   %compare.12 = pred[] compare(s32[] %param_1.2, s32[] %constant_39), direction=LT, metadata={op_name=""jit(set_jax)/jit(main)/lt"" source_file="""" source_line=9}   %constant_38 = s32[] constant(100)   %add.4 = s32[] add(s32[] %param_1.2, s32[] %constant_38), metadata={op_name=""jit(set_jax)/jit(main)/add"" source_file="""" source_line=9}   %select.6 = s32[] select(pred[] %compare.12, s32[] %add.4, s32[] %param_1.2), metadata={op_name=""jit(set_jax)/jit(main)/select_n"" source_file="""" source_line=9}   %bitcast.62 = s32[1]{0} bitcast(s32[] %select.6)   %compare.11 = pred[1]{0} compare(s32[1]{0} %constant_40, s32[1]{0} %bitcast.62), direction=LE   %constant_37 = s32[1]{0} constant({99})   %compare.10 = pred[1]{0} compare(s32[1]{0} %constant_37, s32[1]{0} %bitcast.62), direction=GE   %and.4 = pred[1]{0} and(pred[1]{0} %compare.11, pred[1]{0} %compare.10)   %convert.1 = f32[] convert(s32[] %param_1.2), metadata={op_name=""jit(set_jax)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]"" source_file="""" source_line=9}   %bitcast.61 = f32[1]{0} bitcast(f32[] %convert.1)   %dynamicslice.5 = f32[1]{0} dynamicslice(f32[100]{0} %param_0, s32[] %select.6), dynamic_slice_sizes={1}   %select.5 = f32[1]{0} select(pred[1]{0} %and.4, f32[1]{0} %bitcast.61, f32[1]{0} %dynamicslice.5)   ROOT %dynamicupdateslice.3 = f32[100]{0} dynamicupdateslice(f32[100]{0} %param_0, f32[1]{0} %select.5, s32[] %select.6), sharding={replicated}, metadata={op_name=""jit(set_jax)/jit(main)/scatter[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.FILL_OR_DROP]"" source_file="""" source_line=9} } ENTRY %main.16 (Arg_0.1: f32[100], Arg_1.2: s32[]) > f32[100] {   %Arg_1.2 = s32[] parameter(1), sharding={replicated}   %Arg_0.1 = f32[100]{0} parameter(0), sharding={replicated}   %copy.1 = f32[100]{0} copy(f32[100]{0} %Arg_0.1)   ROOT %fusion = f32[100]{0} fusion(f32[100]{0} %copy.1, s32[] %Arg_1.2), kind=kLoop, calls=%fused_computation, sharding={replicated}, frontend_attributes={fingerprint_before_lhs=""08956b1478b109d4ebe2d6dc5d47afc7""}, metadata={op_name=""jit(set_jax)/jit(main)/scatter[update_consts=() dimension_numbers=ScatterDimensionNumbers(update_window_dims=(), inserted_window_dims=(0,), scatter_dims_to_operand_dims=(0,)) indices_are_sorted=True unique_indices=True mode=GatherScatterMode.FILL_OR_DROP]"" source_file="""" source_line=9} } ```",2023-06-29T13:40:03Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/16587,"Hi  this is expected, because in the JAX CPU and GPU  versions, you're not only benchmarking array updates, but array copies. The reason for copies is that, in the Python side of things, no two JAX arrays can share the same memory, and so a JAX function that returns an input array must return a copy. When you do a microbenchmark of a numpy function that returns a view vs. a JAX function that returns a copy, the relevant difference will be the cost of the copy. But how is that consistent with the docs you link to, which claim that updates do not lead to copies? Well, in the course of a real program, the update will generally not cause the array to be copied, because you generally will not be returning the result of the array update in a Python function, and so no copy needs to be made. In short, it's not the update that's causing your array to be copied, it's your `return` statement. For more on what to expect when comparing microbenchmarks of JAX and NumPy code, see FAQ: Is JAX Faster than NumPy?.","If you want a fairer comparison of JAX and NumPy, you could do something like this: ```python def set_jax(a, idx):     return a.at[idx].set(idx).sum() def set_numpy(a, idx):     a[idx] = idx     return a.sum() ``` Returning `a.sum()` makes the JAX and numpy code moreorless semantically equivalent, and you can see that jaxoncpu is similar to numpy, but jaxongpu beats numpy once the arrays are large enough to overcome JAX's larger Python dispatch overhead. The main computation cost here is the `sum`, because the updates happen inplace and virtually free in all three cases: !download","> In short, it's not the update that's causing your array to be copied, it's your return statement. Golden info, thank you very much for the thorough answer, Jake.",Great! Feel free to let us know if you run into other issues
4909,"以下是一个github上的jax下的一个issue, 标题是(jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with non-zero error code 256)， 内容是 ( Description I'm unable to get JAX on nvidia GPUs working  Start up script  ``` python3 m venv venv source venv/bin/activate pip install upgrade pip pip install upgrade ""jax[cuda12_pip]==0.4.11"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Demonstration of error inside of virtual env ``` (venv) willb260145201:~/cleanba$ nvidiasmi Thu Jun 29 01:54:34 2023        ++  ++ (venv) willb260145201:~/cleanba$ python Python 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.__version__ '0.4.11' >>> jax.numpy.sqrt(1) 20230629 01:55:19.052856: E external/xla/xla/service/gpu/gpu_compiler.cc:1312] The CUDA linking API did not work. Please use XLA_FLAGS=xla_gpu_force_compilation_parallelism=1 to bypass it, but expect to get longer compilation time due to the lack of multithreading. Original error: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Input file '/tmp/tempfileip260145201c55862092866965ff3afe0db76c.cubin' newer than toolkit (122 vs 120) Traceback (most recent call last):   File """", line 1, in    File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 249, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 160, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/core.py"", line 2647, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1193, in _pjit_call_impl     return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1177, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1110, in _pjit_call_impl_python     compiled = _pjit_lower(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2319, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2638, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 497, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 465, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Input file '/tmp/tempfileip260145201c55862092866965ff3afe0db76c.cubin' newer than toolkit (122 vs 120) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in  jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Input file '/tmp/tempfileip260145201c55862092866965ff3afe0db76c.cubin' newer than toolkit (122 vs 120) ```  What jax/jaxlib version are you using? jax==0.4.11, jaxlib==0.4.11  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with non-zero error code 256," Description I'm unable to get JAX on nvidia GPUs working  Start up script  ``` python3 m venv venv source venv/bin/activate pip install upgrade pip pip install upgrade ""jax[cuda12_pip]==0.4.11"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Demonstration of error inside of virtual env ``` (venv) willb260145201:~/cleanba$ nvidiasmi Thu Jun 29 01:54:34 2023        ++  ++ (venv) willb260145201:~/cleanba$ python Python 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.__version__ '0.4.11' >>> jax.numpy.sqrt(1) 20230629 01:55:19.052856: E external/xla/xla/service/gpu/gpu_compiler.cc:1312] The CUDA linking API did not work. Please use XLA_FLAGS=xla_gpu_force_compilation_parallelism=1 to bypass it, but expect to get longer compilation time due to the lack of multithreading. Original error: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Input file '/tmp/tempfileip260145201c55862092866965ff3afe0db76c.cubin' newer than toolkit (122 vs 120) Traceback (most recent call last):   File """", line 1, in    File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 249, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 160, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/core.py"", line 2647, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1193, in _pjit_call_impl     return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1177, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/pjit.py"", line 1110, in _pjit_call_impl_python     compiled = _pjit_lower(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2319, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2638, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 497, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/admin/homewillb/cleanba/venv/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 465, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Input file '/tmp/tempfileip260145201c55862092866965ff3afe0db76c.cubin' newer than toolkit (122 vs 120) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in  jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Input file '/tmp/tempfileip260145201c55862092866965ff3afe0db76c.cubin' newer than toolkit (122 vs 120) ```  What jax/jaxlib version are you using? jax==0.4.11, jaxlib==0.4.11  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_",2023-06-29T02:00:25Z,bug XLA NVIDIA GPU,closed,0,8,https://github.com/jax-ml/jax/issues/16586,I had the same problem with CUDA 12.1., is this a known issue with 12.1 compatibility to the JAX team?, have you been able to find a work around?,"I upgraded CUDA to 12.2 when possible; otherwise, set `XLA_FLAGS=xla_gpu_force_compilation_parallelism=1` to suppress the issue.",I am getting the same error with CUDA12.0. Could anyone figure it out?,"This issue will occur if you have an older CUDA toolkit installed (containing `nvlink`), when using the CUDA `pip` install of JAX. You can either upgrade your local CUDA installation (so `nvlink` in the PATH is newer), remove `nvlink` from your PATH, or use the workaround suggested in https://github.com/google/jax/issues/16586issuecomment1626852639. We can probably make this fail more gracefully in a future JAX release.",https://github.com/openxla/xla/pull/4262 should fix this and it should be part of the next jaxlib release.,The fix is merged in XLA. Another and slightly better workaround until we make a new jaxlib release: set `TF_USE_NVLINK_FOR_PARALLEL_COMPILATION=0` Hope that helps!
1387,"以下是一个github上的jax下的一个issue, 标题是(""Minimal output batching"" option for vmap)， 内容是 ( pointed out to me that `out_axes` is superflous in a lot of cases, since the output batching structure could in theory be inferred without user input. For example: consider running a transformer with partially batched parameters, i.e. the first 5 layers of a transformer are unbatched, but there are N copies of each parameter tensor for the last 5 layers. The `out_axes` structure needed for the keyvalue cache is the something like `[None] * 5 + [0] * 5`. It's not that hard to specify, but it's a bit annoying that it's required to specify something in two different ways (both through the input and output axes). The feature I'm suggesting is an option to vmap which only batches outputs when necessary, rather than by default. This shouldn't be the default behavior, since something like `vmap(lambda x: 0.)(jnp.ones(3))`  is expected to return an array of size 3. To allow toggling the optional behavior, ~~`out_axes=None` could be defined to have this behavior~~, or it could be invoked by `out_axes=SomeModeConstant`. I'm not sure, but I think this only requires skipping the `broadcast()` call if `dst` is `not_mapped` when processing the batching outputs.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"""Minimal output batching"" option for vmap"," pointed out to me that `out_axes` is superflous in a lot of cases, since the output batching structure could in theory be inferred without user input. For example: consider running a transformer with partially batched parameters, i.e. the first 5 layers of a transformer are unbatched, but there are N copies of each parameter tensor for the last 5 layers. The `out_axes` structure needed for the keyvalue cache is the something like `[None] * 5 + [0] * 5`. It's not that hard to specify, but it's a bit annoying that it's required to specify something in two different ways (both through the input and output axes). The feature I'm suggesting is an option to vmap which only batches outputs when necessary, rather than by default. This shouldn't be the default behavior, since something like `vmap(lambda x: 0.)(jnp.ones(3))`  is expected to return an array of size 3. To allow toggling the optional behavior, ~~`out_axes=None` could be defined to have this behavior~~, or it could be invoked by `out_axes=SomeModeConstant`. I'm not sure, but I think this only requires skipping the `broadcast()` call if `dst` is `not_mapped` when processing the batching outputs.",2023-06-29T00:46:02Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/16585,"Equinox has this. `equinox.filter_vmap(..., out_axes=equinox.internal.if_mapped(0))`. This is like `out_axes=0` if it is batched, and `out_axes=None` if it is not. Right now it's in the undocumanted `eqx.internal` namespace, just because this requires touching some JAX internals (specifically I do `isinstance(output, BatchTracer) and output._trace.main is main`)."
2496,"以下是一个github上的jax下的一个issue, 标题是(`sparse.grad` only returns the gradient with respect to the first element of a PyTree)， 内容是 ( Description When applying `sparse.grad` from `jax.experimental.sparse` to a function which take in a Pytree as the first argument, only the gradient with respect to the first item in the Pytree is returned. This is both unexpected and inconsistent with the behaviour of `jax.grad`, which returns a gradient which has the same tree structure as the input.  Here is a small working example demonstrating this behaviour: ```python import jax import jax.numpy as jnp from jax.experimental import sparse def foo1(wb, x, y):  first item is a tuple with W first and B second     w, b = wb     return ((w @ x + b)  y).sum() def foo2(bw, x, y):  first item is a tuple with B first and W second     b, w = bw     return ((w @ x + b)  y).sum() rng = jax.random.PRNGKey(0) keys = jax.random.split(rng, 4) w = jax.random.normal(keys[0], (3, 3)) b = jax.random.normal(keys[1], (3,)) x = jax.random.normal(keys[2], (3,)) y = jax.random.normal(keys[3], (3,)) """""" ``` Here are the outputs for the different tests: ```python  normal jax.grad, W first and B second jax.grad(foo1)((w, b), x, y) (Array([[0.47994015,  0.42577833,  0.765658  ],         [0.47994015,  0.42577833,  0.765658  ],         [0.47994015,  0.42577833,  0.765658  ]], dtype=float32),  Array([1., 1., 1.], dtype=float32))  ============================  normal jax.grad, B first and W second jax.grad(foo2)((b, w), x, y) (Array([1., 1., 1.], dtype=float32),  Array([[0.47994015,  0.42577833,  0.765658  ],         [0.47994015,  0.42577833,  0.765658  ],         [0.47994015,  0.42577833,  0.765658  ]], dtype=float32))  ============================  sparse.grad, W first and B second. only the gradient with respect to W is returned! sparse.grad(foo1)((w, b), x, y) Array([[0.47994015,  0.42577833,  0.765658  ],        [0.47994015,  0.42577833,  0.765658  ],        [0.47994015,  0.42577833,  0.765658  ]], dtype=float32)  ============================  sparse.grad, B first and W second. only the gradient with respect to B is returned! sparse.grad(foo2)((b, w), x, y) Array([1., 1., 1.], dtype=float32) ```  What jax/jaxlib version are you using? jax v0.4.13  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`sparse.grad` only returns the gradient with respect to the first element of a PyTree," Description When applying `sparse.grad` from `jax.experimental.sparse` to a function which take in a Pytree as the first argument, only the gradient with respect to the first item in the Pytree is returned. This is both unexpected and inconsistent with the behaviour of `jax.grad`, which returns a gradient which has the same tree structure as the input.  Here is a small working example demonstrating this behaviour: ```python import jax import jax.numpy as jnp from jax.experimental import sparse def foo1(wb, x, y):  first item is a tuple with W first and B second     w, b = wb     return ((w @ x + b)  y).sum() def foo2(bw, x, y):  first item is a tuple with B first and W second     b, w = bw     return ((w @ x + b)  y).sum() rng = jax.random.PRNGKey(0) keys = jax.random.split(rng, 4) w = jax.random.normal(keys[0], (3, 3)) b = jax.random.normal(keys[1], (3,)) x = jax.random.normal(keys[2], (3,)) y = jax.random.normal(keys[3], (3,)) """""" ``` Here are the outputs for the different tests: ```python  normal jax.grad, W first and B second jax.grad(foo1)((w, b), x, y) (Array([[0.47994015,  0.42577833,  0.765658  ],         [0.47994015,  0.42577833,  0.765658  ],         [0.47994015,  0.42577833,  0.765658  ]], dtype=float32),  Array([1., 1., 1.], dtype=float32))  ============================  normal jax.grad, B first and W second jax.grad(foo2)((b, w), x, y) (Array([1., 1., 1.], dtype=float32),  Array([[0.47994015,  0.42577833,  0.765658  ],         [0.47994015,  0.42577833,  0.765658  ],         [0.47994015,  0.42577833,  0.765658  ]], dtype=float32))  ============================  sparse.grad, W first and B second. only the gradient with respect to W is returned! sparse.grad(foo1)((w, b), x, y) Array([[0.47994015,  0.42577833,  0.765658  ],        [0.47994015,  0.42577833,  0.765658  ],        [0.47994015,  0.42577833,  0.765658  ]], dtype=float32)  ============================  sparse.grad, B first and W second. only the gradient with respect to B is returned! sparse.grad(foo2)((b, w), x, y) Array([1., 1., 1.], dtype=float32) ```  What jax/jaxlib version are you using? jax v0.4.13  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-28T21:12:56Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/16582,Have there been any updates on this bug or hints about where it may originate from?,"Hey  sorry for being silent here. This is a bug in how `sparse.grad` is implemented. I don't think we have any plans to fix it at the moment: `jax.experimental.sparse` is experimental, and you should expect it to have some rough edges.","It looks like this is bug in the logic for postprocessing gradients. https://github.com/google/jax/blob/c1f234a95cb0932cd23ad63a9ddbe0a8d43333b7/jax/experimental/sparse/ad.pyL69L71 Currently, if `argnums` indexes a single pytree, such as a dictionary of parameters, this triggers only the first of the computed gradients to be returned. The logic doesn't account for pytrees being unpacked into multiple arguments when flattened. I opened a draft PR which passes the current sparse tests and should match the behavior of `jax.grad()` when argnums indexes a pytree. The current sparse testing lacks coverage of this pytree repacking behavior (clearly), so I think writing tests for that is the next step if this looks reasonable.",Added some tests and took the PR out of draft.
9393,"以下是一个github上的jax下的一个issue, 标题是(Gradient of `sqrtm`)， 内容是 ( Description I require taking gradients with respect to the square root of a matrix. So, I have turned to `jax.scipy.linalg.sqrtm`. Based on previous discussions and issues on this repo, I understand that it is only implemented on CPU. I can accept this for now  I can always do a callback when necessary and just eat the computational overhead. But, I am getting an error when trying to calculate gradients w.r.t. this operation. For example, see the following minimal example: ```python import jax.numpy as jnp from jax.scipy.linalg import sqrtm from jax import grad arr = jnp.ones((2, 2)) sqrt_arr = sqrtm(arr)  This works grad_sqrt_arr = grad(sqrtm)(arr)  This does not work ``` This yields the following error: ``` Traceback (most recent call last):   File """", line 1, in    File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 935, in sqrtm     return _sqrtm(A)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 915, in _sqrtm     T, Z = schur(A, output='complex')   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 206, in schur     return _schur(a, output)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 199, in _schur     return lax_linalg.schur(a) jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: _schur_jvp_rule() got an unexpected keyword argument 'select_callable' The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/api.py"", line 647, in grad_f     _, g = value_and_grad_f(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/api.py"", line 723, in value_and_grad_f     ans, vjp_py = _vjp(f_partial, *dyn_args, reduce_axes=reduce_axes)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/api.py"", line 2208, in _vjp     out_primal, out_vjp = ad.vjp(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 139, in vjp     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 128, in linearize     jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 777, in trace_to_jaxpr_nounits     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 935, in sqrtm     return _sqrtm(A)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 250, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 163, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 2677, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 315, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1465, in _pjit_jvp     jaxpr_jvp, is_nz_tangents_out = ad.jvp_jaxpr(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 699, in jvp_jaxpr     return _jvp_jaxpr(jaxpr, tuple(nonzeros), tuple(instantiate))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 709, in _jvp_jaxpr     jaxpr_out, avals_out, literals_out = pe.trace_to_jaxpr_dynamic(f_jvp, avals_in)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 229, in jaxpr_as_fun     return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 448, in eval_jaxpr     ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 2677, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 315, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1465, in _pjit_jvp     jaxpr_jvp, is_nz_tangents_out = ad.jvp_jaxpr(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 699, in jvp_jaxpr     return _jvp_jaxpr(jaxpr, tuple(nonzeros), tuple(instantiate))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 709, in _jvp_jaxpr     jaxpr_out, avals_out, literals_out = pe.trace_to_jaxpr_dynamic(f_jvp, avals_in)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 229, in jaxpr_as_fun     return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 448, in eval_jaxpr     ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 315, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params) jax._src.traceback_util.UnfilteredStackTrace: TypeError: _schur_jvp_rule() got an unexpected keyword argument 'select_callable' The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 935, in sqrtm     return _sqrtm(A) TypeError: _schur_jvp_rule() got an unexpected keyword argument 'select_callable' ```  What jax/jaxlib version are you using? jax v0.4.13, jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python 3.9.7, Ubuntu 18.04.6  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Gradient of `sqrtm`," Description I require taking gradients with respect to the square root of a matrix. So, I have turned to `jax.scipy.linalg.sqrtm`. Based on previous discussions and issues on this repo, I understand that it is only implemented on CPU. I can accept this for now  I can always do a callback when necessary and just eat the computational overhead. But, I am getting an error when trying to calculate gradients w.r.t. this operation. For example, see the following minimal example: ```python import jax.numpy as jnp from jax.scipy.linalg import sqrtm from jax import grad arr = jnp.ones((2, 2)) sqrt_arr = sqrtm(arr)  This works grad_sqrt_arr = grad(sqrtm)(arr)  This does not work ``` This yields the following error: ``` Traceback (most recent call last):   File """", line 1, in    File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 935, in sqrtm     return _sqrtm(A)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 915, in _sqrtm     T, Z = schur(A, output='complex')   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 206, in schur     return _schur(a, output)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 199, in _schur     return lax_linalg.schur(a) jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: _schur_jvp_rule() got an unexpected keyword argument 'select_callable' The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/api.py"", line 647, in grad_f     _, g = value_and_grad_f(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/api.py"", line 723, in value_and_grad_f     ans, vjp_py = _vjp(f_partial, *dyn_args, reduce_axes=reduce_axes)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/api.py"", line 2208, in _vjp     out_primal, out_vjp = ad.vjp(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 139, in vjp     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 128, in linearize     jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 777, in trace_to_jaxpr_nounits     jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 935, in sqrtm     return _sqrtm(A)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 250, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 163, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 2677, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 315, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1465, in _pjit_jvp     jaxpr_jvp, is_nz_tangents_out = ad.jvp_jaxpr(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 699, in jvp_jaxpr     return _jvp_jaxpr(jaxpr, tuple(nonzeros), tuple(instantiate))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 709, in _jvp_jaxpr     jaxpr_out, avals_out, literals_out = pe.trace_to_jaxpr_dynamic(f_jvp, avals_in)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 229, in jaxpr_as_fun     return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 448, in eval_jaxpr     ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 2677, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 315, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1465, in _pjit_jvp     jaxpr_jvp, is_nz_tangents_out = ad.jvp_jaxpr(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 699, in jvp_jaxpr     return _jvp_jaxpr(jaxpr, tuple(nonzeros), tuple(instantiate))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 709, in _jvp_jaxpr     jaxpr_out, avals_out, literals_out = pe.trace_to_jaxpr_dynamic(f_jvp, avals_in)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2155, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2177, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 188, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 229, in jaxpr_as_fun     return eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, *args)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 448, in eval_jaxpr     ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 315, in process_primitive     primal_out, tangent_out = jvp(primals_in, tangents_in, **params) jax._src.traceback_util.UnfilteredStackTrace: TypeError: _schur_jvp_rule() got an unexpected keyword argument 'select_callable' The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/ryan/.local/lib/python3.9/sitepackages/jax/_src/scipy/linalg.py"", line 935, in sqrtm     return _sqrtm(A) TypeError: _schur_jvp_rule() got an unexpected keyword argument 'select_callable' ```  What jax/jaxlib version are you using? jax v0.4.13, jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python 3.9.7, Ubuntu 18.04.6  NVIDIA GPU info _No response_",2023-06-28T15:16:15Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16579,"Thanks for the report. There is indeed a missing argument in the schur JVP implementation, but unfortunately all it's preventing is a `NotImplementedError`: https://github.com/google/jax/blob/f463437c7ee81f915d2404d302b6bc5b32ecffbe/jax/_src/lax/linalg.pyL2113L2116 I don't think this is trival to implement unfortunately; see some related discussion at https://github.com/google/jax/issues/669."
504,"以下是一个github上的jax下的一个issue, 标题是(Finish pile_map support enough for a basic transformer example)， 内容是 (The remaining operations to cover were  Slicing a nonragged axis when ragged axes are present  Einsum where both tensor and contraction dimensions are ragged Also improved the error message that happens when trying to broadcast a ragged axis to a rectangular one)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Finish pile_map support enough for a basic transformer example,The remaining operations to cover were  Slicing a nonragged axis when ragged axes are present  Einsum where both tensor and contraction dimensions are ragged Also improved the error message that happens when trying to broadcast a ragged axis to a rectangular one,2023-06-23T17:15:45Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/16541,Side note: maybe rename pile_map to something else? Just google `piles` and you'll see what I am talking about ;)," Yes, that's on the agenda."
1361,"以下是一个github上的jax下的一个issue, 标题是(`experimental.odeint` custom gradient rule breaks with integer arguments)， 内容是 ( Description I know that `experimental.odeint` is in a sense discontinued, but maybe there is an easy fix for this? MWE: ```python from jax.experimental.ode import odeint import jax import jax.numpy as jnp def f(y, t, arg1, arg2):     return y*arg1 + arg2 y0 = jnp.array([2., 3.]) ts = jnp.array([0.0, 1.0]) arg1 = jnp.array(0.2) arg2 = jnp.array(1, dtype=int) sol = odeint(f, y0, ts, arg1, arg2) def get_sol(arg1):      note that arg2 is captured so we don't differentiate wrt it.     sol = odeint(f, y0, ts, arg1, arg2)     return sol jax.jacobian(get_sol)(arg1) ``` This happens because odeint's custom gradient rule attempts to compute the gradient wrt all arguments, even `arg2` which is an integer and I was not trying to actually differentiate. While this might seem nonsensical, this case shows up if you try to integrate with odeint a differential equation defined by a sparse matrix, which has the structure encoded in integers.  What jax/jaxlib version are you using? jax 0.4.11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`experimental.odeint` custom gradient rule breaks with integer arguments," Description I know that `experimental.odeint` is in a sense discontinued, but maybe there is an easy fix for this? MWE: ```python from jax.experimental.ode import odeint import jax import jax.numpy as jnp def f(y, t, arg1, arg2):     return y*arg1 + arg2 y0 = jnp.array([2., 3.]) ts = jnp.array([0.0, 1.0]) arg1 = jnp.array(0.2) arg2 = jnp.array(1, dtype=int) sol = odeint(f, y0, ts, arg1, arg2) def get_sol(arg1):      note that arg2 is captured so we don't differentiate wrt it.     sol = odeint(f, y0, ts, arg1, arg2)     return sol jax.jacobian(get_sol)(arg1) ``` This happens because odeint's custom gradient rule attempts to compute the gradient wrt all arguments, even `arg2` which is an integer and I was not trying to actually differentiate. While this might seem nonsensical, this case shows up if you try to integrate with odeint a differential equation defined by a sparse matrix, which has the structure encoded in integers.  What jax/jaxlib version are you using? jax 0.4.11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-22T09:36:52Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/16517
778,"以下是一个github上的jax下的一个issue, 标题是(Preserve integer dtype in broadcast_one_to_all)， 内容是 (I was trying to broadcast PRNGKeys using `jax.experimental.multihost_utils.broadcast_one_to_all` while in 64 bit mode, which resulted in invalid keys because the `uint32` arrays were promoted to `uint64`. This PR fixes this. reproducer: ```python import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True)  jax.distributed.initialize(...) x = jnp.ones(1, dtype=jnp.uint32) y = jax.experimental.multihost_utils.broadcast_one_to_all(x) print(x.dtype, y.dtype) ``` output before: `uint32 uint64` output after this change: `uint32 uint32`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Preserve integer dtype in broadcast_one_to_all,"I was trying to broadcast PRNGKeys using `jax.experimental.multihost_utils.broadcast_one_to_all` while in 64 bit mode, which resulted in invalid keys because the `uint32` arrays were promoted to `uint64`. This PR fixes this. reproducer: ```python import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True)  jax.distributed.initialize(...) x = jnp.ones(1, dtype=jnp.uint32) y = jax.experimental.multihost_utils.broadcast_one_to_all(x) print(x.dtype, y.dtype) ``` output before: `uint32 uint64` output after this change: `uint32 uint32`",2023-06-21T21:08:00Z,,closed,0,9,https://github.com/jax-ml/jax/issues/16511,"Hey , is this the recommended practice for other APIs in JAX too? Will this break a bunch of stuff? Is there a flag that the user can set which would give them this behavior?","Looking at this more closely, I think this is intended because you are setting the x64 flag to True so things will get promoted. I think this is WAI.","It's intended that `int32` gets cast to `int64` in summation aggregates. This was bakedin to `jax.numpy.sum` very early, because it's what `numpy.sum` does. We looked at changing that a while ago (the `promote_integers` argument was something I added as an intended backwardcompatibilty shim after we had decided to do this), but midway through the planned change there was a lot of pushback because it introduced the possibility of silent integer overflows in some cases (integer overflows are already a danger for other integer operations virtually everywhere already, but I digress) Whether it makes sense that `broadcast_one_to_all` calls `jnp.sum` directly and inherits its dtype behavior is less clear to me. I don't think that this PR should be merged asis, though: it would be confusing if `psum` had different type semantics from `sum`. Perhaps `psum` should gain a `promote_integers` argument? And perhaps `broadcast_one_to_all` should have different dtype semantics than `sum`? I'm not sure.","> Looking at this more closely, I think this is intended because you are setting the x64 flag to True so things will get promoted. I think this is WAI. Sorry, I think I my initial example was not clear enough (simplified too much), here is the reproducer for the actual bug I am trying to solve: ```python import jax import jax.numpy as jnp jax.config.update(""jax_enable_x64"", True)  even fails if we are not distributed  jax.distributed.initialize(...)   kx = jax.random.PRNGKey(0) print(jax.random.uniform(kx)) ky = jax.experimental.multihost_utils.broadcast_one_to_all(kx) print(jax.random.uniform(ky)) ``` which fails with the following error: ``` TypeError: JAX encountered invalid PRNG key data: expected key_data.dtype = uint32; got dtype=uint64 ``` Feel free to close this PR, and let me know if I should open an issue instead, if that is easier for you.",We should never be calling `jnp.sum` on PRNG Keys. Yash – is it possible to implement `broadcast_one_to_all` without invoking summation? Why is summation necessary?,"Note that in the future, adding or summing PRNG keys will result in an error, because semantically PRNG keys do not support addition. (You can set `jax_enable_custom_prng=True` to see how this will manifest). This tells me that either (1) `broadcast_one_to_all` is semantically incompatible with PRNG keys, or (2) `broadcast_one_to_all` does not require addition/summation of data.","Maybe broadcast_one_to_all is not the right solution for whatever you are trying to do. Can you please open a discussions topic and I can tag in relevant folks to help? Please explain what you are trying to do when you do open the discussions topic rather than just the error you are seeing. (maybe also close this PR?) > broadcast_one_to_all does not require addition/summation of data. No, that's not possible. We need to run a collective to allreduce across all the hosts so that the broadcasting can happen.","OK, then I think we can safely say that `broadcast_one_to_all` is not an appropriate operation for PRNG keys.","> Maybe broadcast_one_to_all is not the right solution for whatever you are trying to do. Can you please open a discussions topic and I can tag in relevant folks to help? Please explain what you are trying to do when you do open the discussions topic rather than just the error you are seeing. (maybe also close this PR?) I realized that for my usecase I am better off broadcasting the seed, instead of the PRNGkey. There it is no problem if the type is promoted, therefore no further discussion will be needed. I still think it is inconsistent if a broadcast function changes the type, but so be it. Thanks for the help so far, I will close this PR."
940,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: module 'jax.interpreters.xla' has no attribute 'DeviceArray')， 内容是 ( Description Hi, I am trying to save the model in. my network, but it appears an error in the title. ``` def save(self, save_path: str):     os.makedirs(os.path.dirname(save_path), exist_ok=True)     with open(save_path, 'wb') as f:         f.write(flax.serialization.to_bytes(self.params)) ``` error track: ``` module 'jax.interpreters.xla' has no attribute 'DeviceArray'   File ""common.py"", line 95, in save     f.write(flax.serialization.to_bytes(self.params)) AttributeError: module 'jax.interpreters.xla' has no attribute 'DeviceArray' ```  What jax/jaxlib version are you using? 0.4.12  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info 3090)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AttributeError: module 'jax.interpreters.xla' has no attribute 'DeviceArray'," Description Hi, I am trying to save the model in. my network, but it appears an error in the title. ``` def save(self, save_path: str):     os.makedirs(os.path.dirname(save_path), exist_ok=True)     with open(save_path, 'wb') as f:         f.write(flax.serialization.to_bytes(self.params)) ``` error track: ``` module 'jax.interpreters.xla' has no attribute 'DeviceArray'   File ""common.py"", line 95, in save     f.write(flax.serialization.to_bytes(self.params)) AttributeError: module 'jax.interpreters.xla' has no attribute 'DeviceArray' ```  What jax/jaxlib version are you using? 0.4.12  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info 3090",2023-06-21T13:10:43Z,bug needs info,closed,0,7,https://github.com/jax-ml/jax/issues/16505,"Well, that's correct, `DeviceArray` was removed from JAX several versions ago. The question is what other code is trying to refer to it. It's not really possible to tell based on the information you gave, but perhaps try updating other packages like `flax`?","Thanks for your reply. I am trying to save the checkpoint.  Should I discard this approach to save the checkpoint rather than turn to other solutions, like this URL? I have updated the package to `flax==0.4.2`. Could you please give me a quick example about how to save and load the model at the stateoftheart JAX version(`0.4.12`)? ","Well, what you are doing may work. I think the problem is you have an older version of some package that refers to the old JAX type. I think in particular it is likely your`flax` version is much too old: the current release is 0.6.11.","> Thanks for your reply. I am trying to save the checkpoint. Should I discard this approach to save the checkpoint rather than turn to other solutions, like this URL? I have updated the package to `flax==0.4.2`. >  > Could you please give me a quick example about how to save and load the model at the stateoftheart JAX version(`0.4.12`)? Did you solve this problem?","> > Thanks for your reply. I am trying to save the checkpoint. Should I discard this approach to save the checkpoint rather than turn to other solutions, like this URL? I have updated the package to `flax==0.4.2`. > > Could you please give me a quick example about how to save and load the model at the stateoftheart JAX version(`0.4.12`)? >  > Did you solve this problem? Yes. I uninstall the flax and reinstalled `flax==0.6.11` and `jax==0.4.12`. 3090. (do not update the `flax==0.6.11` model)",My code is using flax.optim optimizers and I am using flax==0.5.1. I am unable to checkpoint using orbax ( unexpected attribute .step) or the legacy api  (same error as mentioned in the thread) . Can't update to flax 0.6.0 or greater since there won't be any optim support. How do I save the train state in this case? ,Hi   you'll probably have more luck getting this question answered at https://github.com/google/flax/discussions. Best of luck!
4492,"以下是一个github上的jax下的一个issue, 标题是(GPU: Out of memory)， 内容是 ( Description > Using following batch size 512 > Using following batch size 512 > Number of parameters being optimized: 325764 > I0621 19:37:57.948089 140590213324928 checkpoints.py:252] Restoring checkpoint from /home/gaoa/FreeNeRFstage2/DietNeRFpytorch/dietnerf/logs/checkpoint_25000 > 20230621 19:38:15.226954: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.38GiB (rounded to 1477187584)requested by op > 20230621 19:38:15.227661: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:468] ________________________________________________________________________________________________ > 20230621 19:38:15.227864: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2036] Execution of replica 0 failed: Resource exhausted: Out of memory while trying to allocate 1477187472 bytes. > Traceback (most recent call last): > File ""train.py"", line 531, in > app.run(main) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/absl/app.py"", line 312, in run > _run_main(main, args) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/absl/app.py"", line 258, in _run_main > sys.exit(main(argv)) > File ""train.py"", line 380, in main > tvnorm_loss_weight, > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/_src/traceback_util.py"", line 183, in reraise_with_filtered_traceback > return fun(*args, **kwargs) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/_src/api.py"", line 1654, in f_pmapped > global_arg_shapes=tuple(global_arg_shapes_flat)) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/core.py"", line 1620, in bind > return call_bind(self, fun, *args, **params) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/core.py"", line 1551, in call_bind > outs = primitive.process(top_trace, fun, tracers, params) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/core.py"", line 1623, in process > return trace.process_map(self, fun, tracers, params) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/core.py"", line 606, in process_call > return primitive.impl(f, *tracers, **params) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/interpreters/pxla.py"", line 637, in xla_pmap_impl > return compiled_fun(*args) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/interpreters/pxla.py"", line 1152, in execute_replicated > out_bufs = compiled.execute_sharded_on_local_devices(input_bufs) > jax._src.traceback_util.UnfilteredStackTrace: RuntimeError: Resource exhausted: Out of memory while trying to allocate 1477187472 bytes. >  > The stack trace below excludes JAXinternal frames. > The preceding is the original exception that occurred, unmodified. >  > The above exception was the direct cause of the following exception: >  > Traceback (most recent call last): > File ""train.py"", line 531, in > app.run(main) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/absl/app.py"", line 312, in run > _run_main(main, args) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/absl/app.py"", line 258, in _run_main > sys.exit(main(argv)) > File ""train.py"", line 380, in main > tvnorm_loss_weight, > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/interpreters/pxla.py"", line 1152, in execute_replicated > out_bufs = compiled.execute_sharded_on_local_devices(input_bufs) > RuntimeError: Resource exhausted: Out of memory while trying to allocate 1477187472 bytes. At first, I set the 'XLA_PYTHON_CLIENT_PREALLOCATE' be False, then I restore a checkpoint(size 3.73MB, a 6layers, width=256 MLP), the GPU memory increase from 500MB to about 24GB, and then throw a error OOM, then I set 'XLA_PYTHON_CLIENT_PREALLOCATE' be True, the code works and the memory is still 24GB. I am very confused about this, why a checkpoint can take up so much memory？So could you please give me some suggestion about this?  What jax/jaxlib version are you using? jax0.2.16    jaxlib0.1.68  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info RTX 3090 _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,GPU: Out of memory," Description > Using following batch size 512 > Using following batch size 512 > Number of parameters being optimized: 325764 > I0621 19:37:57.948089 140590213324928 checkpoints.py:252] Restoring checkpoint from /home/gaoa/FreeNeRFstage2/DietNeRFpytorch/dietnerf/logs/checkpoint_25000 > 20230621 19:38:15.226954: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.38GiB (rounded to 1477187584)requested by op > 20230621 19:38:15.227661: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:468] ________________________________________________________________________________________________ > 20230621 19:38:15.227864: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2036] Execution of replica 0 failed: Resource exhausted: Out of memory while trying to allocate 1477187472 bytes. > Traceback (most recent call last): > File ""train.py"", line 531, in > app.run(main) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/absl/app.py"", line 312, in run > _run_main(main, args) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/absl/app.py"", line 258, in _run_main > sys.exit(main(argv)) > File ""train.py"", line 380, in main > tvnorm_loss_weight, > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/_src/traceback_util.py"", line 183, in reraise_with_filtered_traceback > return fun(*args, **kwargs) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/_src/api.py"", line 1654, in f_pmapped > global_arg_shapes=tuple(global_arg_shapes_flat)) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/core.py"", line 1620, in bind > return call_bind(self, fun, *args, **params) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/core.py"", line 1551, in call_bind > outs = primitive.process(top_trace, fun, tracers, params) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/core.py"", line 1623, in process > return trace.process_map(self, fun, tracers, params) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/core.py"", line 606, in process_call > return primitive.impl(f, *tracers, **params) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/interpreters/pxla.py"", line 637, in xla_pmap_impl > return compiled_fun(*args) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/interpreters/pxla.py"", line 1152, in execute_replicated > out_bufs = compiled.execute_sharded_on_local_devices(input_bufs) > jax._src.traceback_util.UnfilteredStackTrace: RuntimeError: Resource exhausted: Out of memory while trying to allocate 1477187472 bytes. >  > The stack trace below excludes JAXinternal frames. > The preceding is the original exception that occurred, unmodified. >  > The above exception was the direct cause of the following exception: >  > Traceback (most recent call last): > File ""train.py"", line 531, in > app.run(main) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/absl/app.py"", line 312, in run > _run_main(main, args) > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/absl/app.py"", line 258, in _run_main > sys.exit(main(argv)) > File ""train.py"", line 380, in main > tvnorm_loss_weight, > File ""/home/gaoa/anaconda3/envs/freenerf/lib/python3.7/sitepackages/jax/interpreters/pxla.py"", line 1152, in execute_replicated > out_bufs = compiled.execute_sharded_on_local_devices(input_bufs) > RuntimeError: Resource exhausted: Out of memory while trying to allocate 1477187472 bytes. At first, I set the 'XLA_PYTHON_CLIENT_PREALLOCATE' be False, then I restore a checkpoint(size 3.73MB, a 6layers, width=256 MLP), the GPU memory increase from 500MB to about 24GB, and then throw a error OOM, then I set 'XLA_PYTHON_CLIENT_PREALLOCATE' be True, the code works and the memory is still 24GB. I am very confused about this, why a checkpoint can take up so much memory？So could you please give me some suggestion about this?  What jax/jaxlib version are you using? jax0.2.16    jaxlib0.1.68  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info RTX 3090 _No response_",2023-06-21T11:50:22Z,bug,open,2,0,https://github.com/jax-ml/jax/issues/16504
295,"以下是一个github上的jax下的一个issue, 标题是([sparse] support custom JVP in sparsify)， 内容是 (At some point this stopped working, but we didn't have test coverage for it.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[sparse] support custom JVP in sparsify,"At some point this stopped working, but we didn't have test coverage for it.",2023-06-21T09:36:41Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16503
335,"以下是一个github上的jax下的一个issue, 标题是(Fix test failure in array_interoperability_test due to 64-bit dtype squashing.)， 内容是 (Fix test failure in array_interoperability_test due to 64bit dtype squashing.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix test failure in array_interoperability_test due to 64-bit dtype squashing.,Fix test failure in array_interoperability_test due to 64bit dtype squashing.,2023-06-20T19:48:56Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16495
384,"以下是一个github上的jax下的一个issue, 标题是([XLA:Python] Fix __cuda_array_interface__.)， 内容是 ([XLA:Python] Fix __cuda_array_interface__. Adds a test for __cuda_array_interface__ that does not depend on cupy. Fixes https://github.com/google/jax/issues/16440)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[XLA:Python] Fix __cuda_array_interface__.,[XLA:Python] Fix __cuda_array_interface__. Adds a test for __cuda_array_interface__ that does not depend on cupy. Fixes https://github.com/google/jax/issues/16440,2023-06-20T15:02:52Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16490
3710,"以下是一个github上的jax下的一个issue, 标题是(Scan over parallelized xmap)， 内容是 ( Description I am performing a computation that requires an outer scan, but the inner computations can be vectorized. Previously, I was just using `vmap`, but to limit memory I have been experimenting with `pmap` and `xmap`. I am on a machine with 2 x 40 GB A100s. I quickly realized that I could not `pmap` inside of the `scan`. So, I turned to `xmap`. However, I am getting some unexpected behavior.  First, the following code *works*: ```python import pdb import numpy as onp import jax import jax.numpy as jnp from jax import jit, vmap, pmap from jax.experimental.maps import xmap from jax.sharding import Mesh device_axis_names = ('x') mesh_devices = onp.array(jax.devices()) assert(len(mesh_devices) == 2 and len(mesh_devices.shape) == 1) mesh_def = (mesh_devices, device_axis_names) def foo(x):     return x + 3 def test_xmap():     xs = jnp.arange(12).reshape(2, 1)      Option 1: Unparallelized version                                                                     foo_xmapped = xmap(foo, in_axes={0: 'batch'}, out_axes={0: 'batch'})                                 ys = foo_xmapped(xs)                                                                                 Option 2: Parallelized version                                                                      foo_xmapped = xmap(foo, in_axes={0: 'batch'}, out_axes={0: 'batch'},                        axis_resources={'batch': 'x'})     with Mesh(*mesh_def):         ys = foo_xmapped(xs)     return test_xmap() ``` Both Options 1 and 2 work. However, when I try to place the same computation in a scan, I get an error with Option 2: ```python def test_xmap_scan():     def scan_fn(sm, i):         xs = jnp.arange(12).reshape(2, 1)          Option 1: Unparallelized version                                                                     foo_xmapped = xmap(foo, in_axes={0: 'batch'}, out_axes={0: 'batch'})                                 ys = foo_xmapped(xs)                                                                                 Option 2: Parallelized version                                                                      foo_xmapped = xmap(foo, in_axes={0: 'batch'}, out_axes={0: 'batch'}, axis_resources={'batch'\ : 'x'})         ys = foo_xmapped(xs)         sm += ys.sum()         return sm, None     with Mesh(*mesh_def):         fin_sm, _ = jax.lax.scan(scan_fn, 0.0, jnp.arange(4))     return fin_sm test_xmap_scan() ``` This yields the following error: ``` Traceback (most recent call last):   File ""/home/ryan/scan_pmap_test.py"", line 115, in      test_xmap_scan()   File ""/home/ryan/scan_pmap_test.py"", line 100, in test_xmap_scan     fin_sm, _ = jax.lax.scan(scan_fn, 0.0, jnp.arange(4))   File ""/home/ryan/scan_pmap_test.py"", line 94, in scan_fn     ys = foo_xmapped(xs) jax._src.source_info_util.JaxStackTraceBeforeTransformation: NameError: unbound axis name: x. The following axis names (e.g. defined by pmap) are available to collective operations: [] ``` To be clear, I understand that this computation doesn't require a `scan`, but it is a minimal example of my failing use case. Practically, I need a way to distribute computation within a scan over multiple devices.  I have also seen `shmap` but haven't tried it yet. Thank you in advance for your help.  What jax/jaxlib version are you using? jax v0.4.11, jaxlib v0.4.11+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16, Debian 11  NVIDIA GPU info ``` Mon Jun 19 16:33:12 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Scan over parallelized xmap," Description I am performing a computation that requires an outer scan, but the inner computations can be vectorized. Previously, I was just using `vmap`, but to limit memory I have been experimenting with `pmap` and `xmap`. I am on a machine with 2 x 40 GB A100s. I quickly realized that I could not `pmap` inside of the `scan`. So, I turned to `xmap`. However, I am getting some unexpected behavior.  First, the following code *works*: ```python import pdb import numpy as onp import jax import jax.numpy as jnp from jax import jit, vmap, pmap from jax.experimental.maps import xmap from jax.sharding import Mesh device_axis_names = ('x') mesh_devices = onp.array(jax.devices()) assert(len(mesh_devices) == 2 and len(mesh_devices.shape) == 1) mesh_def = (mesh_devices, device_axis_names) def foo(x):     return x + 3 def test_xmap():     xs = jnp.arange(12).reshape(2, 1)      Option 1: Unparallelized version                                                                     foo_xmapped = xmap(foo, in_axes={0: 'batch'}, out_axes={0: 'batch'})                                 ys = foo_xmapped(xs)                                                                                 Option 2: Parallelized version                                                                      foo_xmapped = xmap(foo, in_axes={0: 'batch'}, out_axes={0: 'batch'},                        axis_resources={'batch': 'x'})     with Mesh(*mesh_def):         ys = foo_xmapped(xs)     return test_xmap() ``` Both Options 1 and 2 work. However, when I try to place the same computation in a scan, I get an error with Option 2: ```python def test_xmap_scan():     def scan_fn(sm, i):         xs = jnp.arange(12).reshape(2, 1)          Option 1: Unparallelized version                                                                     foo_xmapped = xmap(foo, in_axes={0: 'batch'}, out_axes={0: 'batch'})                                 ys = foo_xmapped(xs)                                                                                 Option 2: Parallelized version                                                                      foo_xmapped = xmap(foo, in_axes={0: 'batch'}, out_axes={0: 'batch'}, axis_resources={'batch'\ : 'x'})         ys = foo_xmapped(xs)         sm += ys.sum()         return sm, None     with Mesh(*mesh_def):         fin_sm, _ = jax.lax.scan(scan_fn, 0.0, jnp.arange(4))     return fin_sm test_xmap_scan() ``` This yields the following error: ``` Traceback (most recent call last):   File ""/home/ryan/scan_pmap_test.py"", line 115, in      test_xmap_scan()   File ""/home/ryan/scan_pmap_test.py"", line 100, in test_xmap_scan     fin_sm, _ = jax.lax.scan(scan_fn, 0.0, jnp.arange(4))   File ""/home/ryan/scan_pmap_test.py"", line 94, in scan_fn     ys = foo_xmapped(xs) jax._src.source_info_util.JaxStackTraceBeforeTransformation: NameError: unbound axis name: x. The following axis names (e.g. defined by pmap) are available to collective operations: [] ``` To be clear, I understand that this computation doesn't require a `scan`, but it is a minimal example of my failing use case. Practically, I need a way to distribute computation within a scan over multiple devices.  I have also seen `shmap` but haven't tried it yet. Thank you in advance for your help.  What jax/jaxlib version are you using? jax v0.4.11, jaxlib v0.4.11+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16, Debian 11  NVIDIA GPU info ``` Mon Jun 19 16:33:12 2023        ++  ++ ```",2023-06-19T16:33:30Z,bug,closed,2,3,https://github.com/jax-ml/jax/issues/16476,Just checking in on this!  ,"Hi, I am encountering the same error. Have there been any updates on this ?  I think the same bug was mentioned in this issue: https://github.com/google/jax/discussions/15807","xmap was removed from JAX, so this is obsolete"
302,"以下是一个github上的jax下的一个issue, 标题是(Documentation improvements for XlaCallModule.disabled_checks.)， 内容是 (Documentation improvements for XlaCallModule.disabled_checks.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Documentation improvements for XlaCallModule.disabled_checks.,Documentation improvements for XlaCallModule.disabled_checks.,2023-06-18T12:49:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16469
512,"以下是一个github上的jax下的一个issue, 标题是(Disable a test that fails under Windows with a NumPy exception.)， 内容是 ( I think this is another one for you to look at when you get to it. I think as written this test depends on a particular integer overflow behavior that differs between platforms, but I'm not sure we should be relying on the integer overflow behavior in the first place.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Disable a test that fails under Windows with a NumPy exception.," I think this is another one for you to look at when you get to it. I think as written this test depends on a particular integer overflow behavior that differs between platforms, but I'm not sure we should be relying on the integer overflow behavior in the first place.",2023-06-16T15:03:35Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16453
2122,"以下是一个github上的jax下的一个issue, 标题是(Add `ClosedJaxpr.{const,in,out}vars` forwarding properties.)， 内容是 (`ClosedJaxpr` already defines many properties (like `in_avals` and `out_avals` and `effects`) that forward properties from its underlying `Jaxpr`. This PR adds a few more: ```python class ClosedJaxpr:   ...      def constvars(self):     return self.jaxpr.constvars      def invars(self):     return self.jaxpr.invars      def outvars(self):     return self.jaxpr.outvars ``` Ducktyped functions like `_check_call` now work with `call_jaxpr: ClosedJaxpr` values without any other changes. Previously, they would fall with an error: ```   File "".../venv/lib/python3.11/sitepackages/jax/_src/core.py"", line 2713, in check_jaxpr     _check_jaxpr(ctx_factory, jaxpr)   File "".../venv/lib/python3.11/sitepackages/jax/_src/core.py"", line 2788, in _check_jaxpr     out_type, eqn_effects = _check_call(ctx_factory, prim, in_atoms,                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""...//venv/lib/python3.11/sitepackages/jax/_src/core.py"", line 2904, in _check_call     if len(in_atoms) != len(call_jaxpr.invars):                             ^^^^^^^^^^^^^^^^^ jax._src.traceback_util.UnfilteredStackTrace: AttributeError: 'ClosedJaxpr' object has no attribute 'invars' The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified. ``` Adding these properties is cleaner than adding adhoc `isinstance(call_jaxpr, ClosedJaxpr)` checks in functions like `core._check_call`. Note: `core._check_call` isn't currently called for `closed_call_p` since it has its own custom typechecking rule. This change would make typechecking work for `closed_call_p` even if `custom_typechecks[closed_call_p]` were removed.  This is helpful for a JAXbased research project that adds a custom primitive subclassing `ClosedCallPrimitive`, but that wants to avoid defining a new typechecking rule and reuse `core._check_call` instead.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Add `ClosedJaxpr.{const,in,out}vars` forwarding properties.","`ClosedJaxpr` already defines many properties (like `in_avals` and `out_avals` and `effects`) that forward properties from its underlying `Jaxpr`. This PR adds a few more: ```python class ClosedJaxpr:   ...      def constvars(self):     return self.jaxpr.constvars      def invars(self):     return self.jaxpr.invars      def outvars(self):     return self.jaxpr.outvars ``` Ducktyped functions like `_check_call` now work with `call_jaxpr: ClosedJaxpr` values without any other changes. Previously, they would fall with an error: ```   File "".../venv/lib/python3.11/sitepackages/jax/_src/core.py"", line 2713, in check_jaxpr     _check_jaxpr(ctx_factory, jaxpr)   File "".../venv/lib/python3.11/sitepackages/jax/_src/core.py"", line 2788, in _check_jaxpr     out_type, eqn_effects = _check_call(ctx_factory, prim, in_atoms,                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""...//venv/lib/python3.11/sitepackages/jax/_src/core.py"", line 2904, in _check_call     if len(in_atoms) != len(call_jaxpr.invars):                             ^^^^^^^^^^^^^^^^^ jax._src.traceback_util.UnfilteredStackTrace: AttributeError: 'ClosedJaxpr' object has no attribute 'invars' The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified. ``` Adding these properties is cleaner than adding adhoc `isinstance(call_jaxpr, ClosedJaxpr)` checks in functions like `core._check_call`. Note: `core._check_call` isn't currently called for `closed_call_p` since it has its own custom typechecking rule. This change would make typechecking work for `closed_call_p` even if `custom_typechecks[closed_call_p]` were removed.  This is helpful for a JAXbased research project that adds a custom primitive subclassing `ClosedCallPrimitive`, but that wants to avoid defining a new typechecking rule and reuse `core._check_call` instead.",2023-06-16T06:16:31Z,,open,0,4,https://github.com/jax-ml/jax/issues/16446,"Two more ideas below.  1. Remove custom typechecking rule for `closed_call_p` I wonder if it would make sense to remove the custom typechecking rule for `closed_call_p` below? https://github.com/google/jax/blob/d16b8581caa4ccb5ec2dab60b876a3104ba8035f/jax/_src/core.pyL2691L2696 `_check_closed_call` seems to do strictly less than than `_check_call`: https://github.com/google/jax/blob/d16b8581caa4ccb5ec2dab60b876a3104ba8035f/jax/_src/core.pyL2900L2935 And since `custom_typechecks[closed_call_p] = _check_closed_call` is defined, `_check_call` isn't used for `closed_call_p`: https://github.com/google/jax/blob/d16b8581caa4ccb5ec2dab60b876a3104ba8035f/jax/_src/core.pyL2788L2794"," 2. Define `ClosedJaxpr.{const,in,out}vars` properties Currently, this PR makes this change to `_check_call` to support `closed_call_p` in addition to `call_p`: ```python    Handle `closed_call`.   if isinstance(call_jaxpr, ClosedJaxpr):     call_jaxpr = call_jaxpr.jaxpr   if len(in_atoms) != len(call_jaxpr.invars):     ... ``` This avoids the following error: ```   File "".../venv/lib/python3.11/sitepackages/jax/_src/core.py"", line 2713, in check_jaxpr     _check_jaxpr(ctx_factory, jaxpr)   File "".../venv/lib/python3.11/sitepackages/jax/_src/core.py"", line 2788, in _check_jaxpr     out_type, eqn_effects = _check_call(ctx_factory, prim, in_atoms,                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""...//venv/lib/python3.11/sitepackages/jax/_src/core.py"", line 2904, in _check_call     if len(in_atoms) != len(call_jaxpr.invars):                             ^^^^^^^^^^^^^^^^^ jax._src.traceback_util.UnfilteredStackTrace: AttributeError: 'ClosedJaxpr' object has no attribute 'invars' The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified. ``` Instead, we could fix the error by defining more forwarding properties on `ClosedJaxpr`. ```python class ClosedJaxpr:   ...      def constvars(self):     return self.jaxpr.constvars      def invars(self):     return self.jaxpr.invars      def outvars(self):     return self.jaxpr.outvars ``` I confirmed that this works, and this seems preferable to me, especially since `ClosedJaxpr` already has a bunch of other forwarding properties based on its underlying `Jaxpr`. Could someone please confirm?","I went ahead and implemented + pushed idea (2) above, since I think it's a cleaner solution. Feel free to comment otherwise. >  2. Define `ClosedJaxpr.{const,in,out}vars` properties > ... >  > Instead, we could fix the error by defining more forwarding properties on `ClosedJaxpr`. >  > ```python > class ClosedJaxpr: >   ... >  >    >   def constvars(self): >     return self.jaxpr.constvars >  >    >   def invars(self): >     return self.jaxpr.invars >  >    >   def outvars(self): >     return self.jaxpr.outvars > ```",I think there is a reason why those properties don't exist on ClosedJaxpr. Maybe  remembers why? Doing the isinstance check is fine I think since that pattern exists in other places too.
1481,"以下是一个github上的jax下的一个issue, 标题是(nested gmres does not work)， 内容是 ( Description I am building an iterative solver. Naturally some of my linear operators are defined iteratively using gmres. A simple example would be ``` def a(x):      this function is essentially the linear operator x > 2*x     return gmres(lambda i: i, 2 * x)[0] ``` On top of this linear operator I can define another linear operator ``` def b(x):      this function is essentially the inverse of a     return gmres(a, x)[0] ``` However, jax would throw an error message whenever i am trying to call `b`.  Here is the full script:  ``` import jax from jax import numpy as np from functools import partial gmres = partial(jax.scipy.sparse.linalg.gmres, tol=1e3, restart=10, solve_method='batched') def a(x):      this function is essentially the linear operator x> 2*x     return gmres(lambda i: i, 2 * x)[0] def b(x):      this function is essentially the inverse of a      but it does not work...     return gmres(a, x)[0] if __name__ == '__main__':     t = np.ones((10,))     b(t) ``` And here is the error message > TypeError: Value UndefinedPrimal(ShapedArray(float32[])) with type  is not a valid JAX type  What jax/jaxlib version are you using? jax v0.4.12  Which accelerator(s) are you using? cpu  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,nested gmres does not work," Description I am building an iterative solver. Naturally some of my linear operators are defined iteratively using gmres. A simple example would be ``` def a(x):      this function is essentially the linear operator x > 2*x     return gmres(lambda i: i, 2 * x)[0] ``` On top of this linear operator I can define another linear operator ``` def b(x):      this function is essentially the inverse of a     return gmres(a, x)[0] ``` However, jax would throw an error message whenever i am trying to call `b`.  Here is the full script:  ``` import jax from jax import numpy as np from functools import partial gmres = partial(jax.scipy.sparse.linalg.gmres, tol=1e3, restart=10, solve_method='batched') def a(x):      this function is essentially the linear operator x> 2*x     return gmres(lambda i: i, 2 * x)[0] def b(x):      this function is essentially the inverse of a      but it does not work...     return gmres(a, x)[0] if __name__ == '__main__':     t = np.ones((10,))     b(t) ``` And here is the error message > TypeError: Value UndefinedPrimal(ShapedArray(float32[])) with type  is not a valid JAX type  What jax/jaxlib version are you using? jax v0.4.12  Which accelerator(s) are you using? cpu  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-15T20:41:13Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/16441,Curiously it seems to work fine if you use `cg` or `bicgstab` for either `a` or `b` (or both) so it seems to be limited to `gmres` not other iterative linear solvers. Under the hood both `cg` and `bicgstab` use similar infrastructure that `gmres` doesn't.,"You may like to try Lineax, which is our new morecomprehensive solution for linear solvers. I've just checked and it appears to handle this case without issues.","Assuming this is still a bug, I did some digging and I figured out that the issue is probably caused when the solver calls linear_transpose at the end of the solve. For some reason, as opposed to every other time this function is called, when it generates the jaxpr (line 2282 in api.py), despite having, as far as I can tell, the same parameters, it produces a jaxpr with a nonzero length of equations (`len(jaxpr.eqns) > 0`) and produces invars that are not in the constvars. This produces a problem further down the line when backwards_pass is called (line 181 in ad.py). This method generates a mapping of sorts between the constvars of the provided jaxpr and some other constants passed into the function itself. Later, the method iterates over every equation in jaxpr.eqns, and as part of that loop, it tries to read the invars from the mapping it generated earlier. If the invar is not defined in the mapping, it gets an UndefinedPrimal by default. Later, when concrete_aval is called on the values, it tries to concrete_aval on the UndefinedPrimal, which fails because apparently an UndefinedPrimal does not have the `""__jax_array__"" `attribute, which causes the method to error out. I am not sure if UndefinedPrimals are supposed to have that attribute (they probably aren't) and I have no idea why the jaxpr generation method just suddenly decided to throw in a bunch of extra invars and constvars. Every other call I caught made a jaxpr with 0 constvars, 1 invar (a), and 0 equations, which didn't cause issues later in the backwards_pass method since it tried to iterate over the equations and immediately stopped since there aren't any. Once again, as far as I can tell, the inputs into the jaxpr generation method are identical each time.","Alright, the above is not true. The error happens when the function a is called on the pvals, but why is a returning a jaxpr anyway? Shouldn't it just return a single value?","I noticed this when working on some other stuff, but it seems like its a more general problem using `gmres` with `fori_loop` in the linear function: ```python def bfun(x):     out = jnp.zeros_like(x)     def body(i, out):         out = out.at[i].set(3*x[i])         return out     return jax.lax.fori_loop(0, x.size, body, out) jax.scipy.sparse.linalg.gmres(bfun, jnp.arange(5).astype(float)) ``` gives the following assertion error: ``` AssertionError                            Traceback (most recent call last) Cell In[108], line 8       5         return out       6     return jax.lax.fori_loop(0, x.size, body, out) > 8 jax.scipy.sparse.linalg.gmres(bfun, jnp.arange(5).astype(float)) File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/scipy/sparse/linalg.py:704, in gmres(A, b, x0, tol, atol, restart, maxiter, M, solve_method)     702 def _solve(A, b):     703   return _gmres_solve(A, b, x0, atol, ptol, restart, maxiter, M, gmres_func) > 704 x = lax.custom_linear_solve(A, b, solve=_solve, transpose_solve=_solve)     706 failed = jnp.isnan(_norm(x))     707 info = jnp.where(failed, x=1, y=0)     [... skipping hidden 12 frame] File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/lax/control_flow/loops.py:718, in _scan_transpose(reduce_axes, cts, reverse, length, num_consts, num_carry, jaxpr, linear, unroll, *args)     716 ires, _ = split_list(consts, [num_ires])     717 _, eres = split_list(xs, [sum(xs_lin)]) > 718 assert not any(ad.is_undefined_primal(r) for r in ires)     719 assert not any(ad.is_undefined_primal(r) for r in eres)     721 carry_avals, y_avals = split_list(jaxpr.out_avals, [num_carry]) AssertionError: "
4027,"以下是一个github上的jax下的一个issue, 标题是(jax `__cuda_array_interface__` not working)， 内容是 ( Description ```python import numpy as	np import jax.numpy as jnp from numba import cuda import cupy .jit def _sum_nomask(w, res, ind):     start = cuda.grid(1)     stride = cuda.gridsize(1)     n = w.shape[0]     tot = 0.0     for i in range(start, n, stride):         if w[i] > 0:             tot += w[i]     cuda.atomic.add(res, ind, tot) if __name__ == ""__main__"":     arr = cupy.arange(10000, dtype=np.float32)     res = cupy.zeros(2, dtype=np.float32)     _sum_nomask500, 32     print(""cupy:"", res)     arr = jnp.arange(10000, dtype=jnp.float32)     res = jnp.zeros(2, dtype=jnp.float32)     _sum_nomask500, 32     print(""jax:"", res) ``` I get ```bash $ python test_jax_numba.py  cupy: [49995040.        0.] Traceback (most recent call last):   File ""/home/mrbecker/test_jax_numba.py"", line 29, in      _sum_nomask500, 32   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 542, in __call__     return self.dispatcher.call(args, self.griddim, self.blockdim,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 676, in call     kernel = _dispatcher.Dispatcher._cuda_call(self, *args)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 683, in _compile_for_args     argtypes = [self.typeof_pyval(a) for a in args]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 683, in      argtypes = [self.typeof_pyval(a) for a in args]                 ^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 690, in typeof_pyval     return typeof(val, Purpose.argument)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/core/typing/typeof.py"", line 33, in typeof     ty = typeof_impl(val, c)          ^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/functools.py"", line 909, in wrapper     return dispatch(args[0].__class__)(*args, **kw)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/core/typing/typeof.py"", line 46, in typeof_impl     tp = _typeof_buffer(val, c)          ^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/core/typing/typeof.py"", line 69, in _typeof_buffer     m = memoryview(val)         ^^^^^^^^^^^^^^^ BufferError: INVALID_ARGUMENT: Python buffer protocol is only defined for CPU buffers. ``` Futher if you try to access the cuda array interface attribute another error comes up. (This may be a red herring.) ``` In [1]: import jax In [2]: a = jax.numpy.arange(10) In [3]: a Out[3]: Array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32) In [4]: a.__cuda_array_interface__  TypeError                                 Traceback (most recent call last) TypeError: Unregistered type : absl::lts_20230125::StatusOr The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last) Cell In[4], line 1 > 1 a.__cuda_array_interface__ TypeError: Unable to convert function return value to a Python type! The signature was 	(arg0: xla::PyArray) > absl::lts_20230125::StatusOr ```  What jax/jaxlib version are you using? jax and jaxlib 0.4.12  Which accelerator(s) are you using? GPU  Additional system info linux w/ nvidia  NVIDIA GPU info ``` $ nvidiasmi Thu Jun 15 14:35:41 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax `__cuda_array_interface__` not working," Description ```python import numpy as	np import jax.numpy as jnp from numba import cuda import cupy .jit def _sum_nomask(w, res, ind):     start = cuda.grid(1)     stride = cuda.gridsize(1)     n = w.shape[0]     tot = 0.0     for i in range(start, n, stride):         if w[i] > 0:             tot += w[i]     cuda.atomic.add(res, ind, tot) if __name__ == ""__main__"":     arr = cupy.arange(10000, dtype=np.float32)     res = cupy.zeros(2, dtype=np.float32)     _sum_nomask500, 32     print(""cupy:"", res)     arr = jnp.arange(10000, dtype=jnp.float32)     res = jnp.zeros(2, dtype=jnp.float32)     _sum_nomask500, 32     print(""jax:"", res) ``` I get ```bash $ python test_jax_numba.py  cupy: [49995040.        0.] Traceback (most recent call last):   File ""/home/mrbecker/test_jax_numba.py"", line 29, in      _sum_nomask500, 32   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 542, in __call__     return self.dispatcher.call(args, self.griddim, self.blockdim,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 676, in call     kernel = _dispatcher.Dispatcher._cuda_call(self, *args)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 683, in _compile_for_args     argtypes = [self.typeof_pyval(a) for a in args]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 683, in      argtypes = [self.typeof_pyval(a) for a in args]                 ^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/cuda/dispatcher.py"", line 690, in typeof_pyval     return typeof(val, Purpose.argument)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/core/typing/typeof.py"", line 33, in typeof     ty = typeof_impl(val, c)          ^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/functools.py"", line 909, in wrapper     return dispatch(args[0].__class__)(*args, **kw)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/core/typing/typeof.py"", line 46, in typeof_impl     tp = _typeof_buffer(val, c)          ^^^^^^^^^^^^^^^^^^^^^^   File ""/home/mrbecker/miniforge3/envs/jaxnumba/lib/python3.11/sitepackages/numba/core/typing/typeof.py"", line 69, in _typeof_buffer     m = memoryview(val)         ^^^^^^^^^^^^^^^ BufferError: INVALID_ARGUMENT: Python buffer protocol is only defined for CPU buffers. ``` Futher if you try to access the cuda array interface attribute another error comes up. (This may be a red herring.) ``` In [1]: import jax In [2]: a = jax.numpy.arange(10) In [3]: a Out[3]: Array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32) In [4]: a.__cuda_array_interface__  TypeError                                 Traceback (most recent call last) TypeError: Unregistered type : absl::lts_20230125::StatusOr The above exception was the direct cause of the following exception: TypeError                                 Traceback (most recent call last) Cell In[4], line 1 > 1 a.__cuda_array_interface__ TypeError: Unable to convert function return value to a Python type! The signature was 	(arg0: xla::PyArray) > absl::lts_20230125::StatusOr ```  What jax/jaxlib version are you using? jax and jaxlib 0.4.12  Which accelerator(s) are you using? GPU  Additional system info linux w/ nvidia  NVIDIA GPU info ``` $ nvidiasmi Thu Jun 15 14:35:41 2023        ++  ++ ```",2023-06-15T19:35:56Z,bug NVIDIA GPU,closed,0,2,https://github.com/jax-ml/jax/issues/16440,I did some bisecting and this appears to have broken sometime around 0.4.3 or 0.4.4 when abseil moved from 20220623.0 to 20230125.0.,"https://github.com/google/jax/pull/16490 and https://github.com/openxla/xla/pull/3704 will fix, in the next jaxlib release."
5645,"以下是一个github上的jax下的一个issue, 标题是(jax only recognized 1 out of 8 gpus)， 内容是 ( Description I'm trying to use jax with 8 nvidia a100 GPUs and jax.devices only recognizes 1 of them. In a fresh virtual environment I have executed the following command.  As you can see there are 8 A100 running CUDA Version: 12.0  ```(jax_env) willb260130134:~$ nvidiasmi Thu Jun 15 18:53:23 2023        ++  ++ ``` pip is upgraded ``` (jax_env) willb260130134:~$ pip install upgrade pip Requirement already satisfied: pip in ./jax_env/lib/python3.8/sitepackages (23.1.2) ``` installing jax[cuda] following https://github.com/google/jaxinstallation ``` (jax_env) willb260130134:~$ pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Requirement already satisfied: jax[cuda12_pip] in ./jax_env/lib/python3.8/sitepackages (0.4.8) Collecting jax[cuda12_pip]   Using cached jax0.4.12py3noneany.whl Requirement already satisfied: mldtypes>=0.1.0 in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (0.2.0) Requirement already satisfied: numpy>=1.21 in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (1.24.3) Requirement already satisfied: opteinsum in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (3.3.0) Requirement already satisfied: scipy>=1.7 in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (1.10.1) Requirement already satisfied: importlibmetadata>=4.6 in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (6.6.0) INFO: pip is looking at multiple versions of jax[cuda12pip] to determine which version is compatible with other requirements. This could take a while.   Using cached jax0.4.11py3noneany.whl Collecting jaxlib==0.4.11+cuda12.cudnn88 (from jax[cuda12_pip])   Using cached https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.11%2Bcuda12.cudnn88cp38cp38manylinux2014_x86_64.whl (170.8 MB) Collecting nvidiacublascu12 (from jax[cuda12_pip])   Downloading nvidia_cublas_cu1212.1.3.1py3nonemanylinux1_x86_64.whl (410.6 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 6.1 MB/s eta 0:00:00 Collecting nvidiacudacupticu12 (from jax[cuda12_pip])   Downloading nvidia_cuda_cupti_cu1212.1.105py3nonemanylinux1_x86_64.whl (14.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 44.5 MB/s eta 0:00:00 Collecting nvidiacudanvcccu12 (from jax[cuda12_pip])   Downloading nvidia_cuda_nvcc_cu1212.1.105py3nonemanylinux1_x86_64.whl (20.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/20.0 MB 29.9 MB/s eta 0:00:00 Collecting nvidiacudaruntimecu12 (from jax[cuda12_pip])   Downloading nvidia_cuda_runtime_cu1212.1.105py3nonemanylinux1_x86_64.whl (823 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 9.6 MB/s eta 0:00:00 Collecting nvidiacudnncu12>=8.9 (from jax[cuda12_pip])   Downloading nvidia_cudnn_cu128.9.2.26py3nonemanylinux1_x86_64.whl (731.7 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 3.5 MB/s eta 0:00:00 Collecting nvidiacufftcu12 (from jax[cuda12_pip])   Downloading nvidia_cufft_cu1211.0.2.54py3nonemanylinux1_x86_64.whl (121.6 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 14.5 MB/s eta 0:00:00 Collecting nvidiacusolvercu12 (from jax[cuda12_pip])   Downloading nvidia_cusolver_cu1211.4.5.107py3nonemanylinux1_x86_64.whl (124.2 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 6.7 MB/s eta 0:00:00 Collecting nvidiacusparsecu12 (from jax[cuda12_pip])   Downloading nvidia_cusparse_cu1212.1.0.106py3nonemanylinux1_x86_64.whl (196.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 10.5 MB/s eta 0:00:00 Requirement already satisfied: zipp>=0.5 in ./jax_env/lib/python3.8/sitepackages (from importlibmetadata>=4.6>jax[cuda12_pip]) (3.15.0) Collecting nvidianvjitlinkcu12 (from nvidiacusolvercu12>jax[cuda12_pip])   Downloading nvidia_nvjitlink_cu1212.1.105py3nonemanylinux1_x86_64.whl (19.8 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/19.8 MB 37.9 MB/s eta 0:00:00 Installing collected packages: nvidianvjitlinkcu12, nvidiacufftcu12, nvidiacudaruntimecu12, nvidiacudanvcccu12, nvidiacudacupticu12, nvidiacublascu12, nvidiacusparsecu12, nvidiacudnncu12, jaxlib, jax, nvidiacusolvercu12   Attempting uninstall: jaxlib     Found existing installation: jaxlib 0.4.7+cuda11.cudnn86     Uninstalling jaxlib0.4.7+cuda11.cudnn86:       Successfully uninstalled jaxlib0.4.7+cuda11.cudnn86   Attempting uninstall: jax     Found existing installation: jax 0.4.8     Uninstalling jax0.4.8:       Successfully uninstalled jax0.4.8 Successfully installed jax0.4.11 jaxlib0.4.11+cuda12.cudnn88 nvidiacublascu1212.1.3.1 nvidiacudacupticu1212.1.105 nvidiacudanvcccu1212.1.105 nvidiacudaruntimecu1212.1.105 nvidiacudnncu128.9.2.26 nvidiacufftcu1211.0.2.54 nvidiacusolvercu1211.4.5.107 nvidiacusparsecu1212.1.0.106 nvidianvjitlinkcu1212.1.105 ``` Here you can see that jax.devices is only recognizing 1 of the gpu (or at least that's what you'd think) ``` (jax_env) willb260130134:~$ python Python 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.devices() [gpu(id=0)] >>> jax.numpy.array(0) Array(0, dtype=int32, weak_type=True)```  What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax only recognized 1 out of 8 gpus," Description I'm trying to use jax with 8 nvidia a100 GPUs and jax.devices only recognizes 1 of them. In a fresh virtual environment I have executed the following command.  As you can see there are 8 A100 running CUDA Version: 12.0  ```(jax_env) willb260130134:~$ nvidiasmi Thu Jun 15 18:53:23 2023        ++  ++ ``` pip is upgraded ``` (jax_env) willb260130134:~$ pip install upgrade pip Requirement already satisfied: pip in ./jax_env/lib/python3.8/sitepackages (23.1.2) ``` installing jax[cuda] following https://github.com/google/jaxinstallation ``` (jax_env) willb260130134:~$ pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Requirement already satisfied: jax[cuda12_pip] in ./jax_env/lib/python3.8/sitepackages (0.4.8) Collecting jax[cuda12_pip]   Using cached jax0.4.12py3noneany.whl Requirement already satisfied: mldtypes>=0.1.0 in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (0.2.0) Requirement already satisfied: numpy>=1.21 in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (1.24.3) Requirement already satisfied: opteinsum in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (3.3.0) Requirement already satisfied: scipy>=1.7 in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (1.10.1) Requirement already satisfied: importlibmetadata>=4.6 in ./jax_env/lib/python3.8/sitepackages (from jax[cuda12_pip]) (6.6.0) INFO: pip is looking at multiple versions of jax[cuda12pip] to determine which version is compatible with other requirements. This could take a while.   Using cached jax0.4.11py3noneany.whl Collecting jaxlib==0.4.11+cuda12.cudnn88 (from jax[cuda12_pip])   Using cached https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.11%2Bcuda12.cudnn88cp38cp38manylinux2014_x86_64.whl (170.8 MB) Collecting nvidiacublascu12 (from jax[cuda12_pip])   Downloading nvidia_cublas_cu1212.1.3.1py3nonemanylinux1_x86_64.whl (410.6 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 6.1 MB/s eta 0:00:00 Collecting nvidiacudacupticu12 (from jax[cuda12_pip])   Downloading nvidia_cuda_cupti_cu1212.1.105py3nonemanylinux1_x86_64.whl (14.1 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 44.5 MB/s eta 0:00:00 Collecting nvidiacudanvcccu12 (from jax[cuda12_pip])   Downloading nvidia_cuda_nvcc_cu1212.1.105py3nonemanylinux1_x86_64.whl (20.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/20.0 MB 29.9 MB/s eta 0:00:00 Collecting nvidiacudaruntimecu12 (from jax[cuda12_pip])   Downloading nvidia_cuda_runtime_cu1212.1.105py3nonemanylinux1_x86_64.whl (823 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 9.6 MB/s eta 0:00:00 Collecting nvidiacudnncu12>=8.9 (from jax[cuda12_pip])   Downloading nvidia_cudnn_cu128.9.2.26py3nonemanylinux1_x86_64.whl (731.7 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 3.5 MB/s eta 0:00:00 Collecting nvidiacufftcu12 (from jax[cuda12_pip])   Downloading nvidia_cufft_cu1211.0.2.54py3nonemanylinux1_x86_64.whl (121.6 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 14.5 MB/s eta 0:00:00 Collecting nvidiacusolvercu12 (from jax[cuda12_pip])   Downloading nvidia_cusolver_cu1211.4.5.107py3nonemanylinux1_x86_64.whl (124.2 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 6.7 MB/s eta 0:00:00 Collecting nvidiacusparsecu12 (from jax[cuda12_pip])   Downloading nvidia_cusparse_cu1212.1.0.106py3nonemanylinux1_x86_64.whl (196.0 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 10.5 MB/s eta 0:00:00 Requirement already satisfied: zipp>=0.5 in ./jax_env/lib/python3.8/sitepackages (from importlibmetadata>=4.6>jax[cuda12_pip]) (3.15.0) Collecting nvidianvjitlinkcu12 (from nvidiacusolvercu12>jax[cuda12_pip])   Downloading nvidia_nvjitlink_cu1212.1.105py3nonemanylinux1_x86_64.whl (19.8 MB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/19.8 MB 37.9 MB/s eta 0:00:00 Installing collected packages: nvidianvjitlinkcu12, nvidiacufftcu12, nvidiacudaruntimecu12, nvidiacudanvcccu12, nvidiacudacupticu12, nvidiacublascu12, nvidiacusparsecu12, nvidiacudnncu12, jaxlib, jax, nvidiacusolvercu12   Attempting uninstall: jaxlib     Found existing installation: jaxlib 0.4.7+cuda11.cudnn86     Uninstalling jaxlib0.4.7+cuda11.cudnn86:       Successfully uninstalled jaxlib0.4.7+cuda11.cudnn86   Attempting uninstall: jax     Found existing installation: jax 0.4.8     Uninstalling jax0.4.8:       Successfully uninstalled jax0.4.8 Successfully installed jax0.4.11 jaxlib0.4.11+cuda12.cudnn88 nvidiacublascu1212.1.3.1 nvidiacudacupticu1212.1.105 nvidiacudanvcccu1212.1.105 nvidiacudaruntimecu1212.1.105 nvidiacudnncu128.9.2.26 nvidiacufftcu1211.0.2.54 nvidiacusolvercu1211.4.5.107 nvidiacusparsecu1212.1.0.106 nvidianvjitlinkcu1212.1.105 ``` Here you can see that jax.devices is only recognizing 1 of the gpu (or at least that's what you'd think) ``` (jax_env) willb260130134:~$ python Python 3.8.10 (default, May 26 2023, 14:05:08)  [GCC 9.4.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax >>> jax.devices() [gpu(id=0)] >>> jax.numpy.array(0) Array(0, dtype=int32, weak_type=True)```  What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-15T19:03:07Z,bug NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/16438,Can you confirm you don't have `CUDA_VISIBLE_DEVICES` set in your enviroment?,"Setting it resolved the problem, thank you!","Closing, since that seems like it's working as intended."
1319,"以下是一个github上的jax下的一个issue, 标题是(Metal: Failed Assertion  / Crash when trying to create arrays in double-precision)， 内容是 ( Description There seems to be an issue with casting for the edge case of size zero arrays. ```python from jax import config config.update(""jax_enable_x64"", True) import jax.numpy as jnp import numpy as np jnp.array([1.0])            OK : Array([1.], dtype=float64) jnp.array(np.double(1.0))   OK : Array(1., dtype=float64) jnp.array(jnp.double(1.0))  Crash jnp.array(1.0)              Crash   ``` The last two commands will crash the kernel ``` Metal device set to: Apple M1 systemMemory: 16.00 GB maxCacheSize: 5.33 GB /AppleInternal/Library/BuildRoots/c2cb9645dafc11edaa266ec1e3b3f7b3/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphExecutable.mm:1377: failed assertion `Incompatible element type for parameter at index 0, mlir module expected element type f64 but received f32' ```  What jax/jaxlib version are you using? jax 0.4.11, jaxlib 0.4.10, jaxmetal 0.0.2  Which accelerator(s) are you using? Apple GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Metal: Failed Assertion  / Crash when trying to create arrays in double-precision," Description There seems to be an issue with casting for the edge case of size zero arrays. ```python from jax import config config.update(""jax_enable_x64"", True) import jax.numpy as jnp import numpy as np jnp.array([1.0])            OK : Array([1.], dtype=float64) jnp.array(np.double(1.0))   OK : Array(1., dtype=float64) jnp.array(jnp.double(1.0))  Crash jnp.array(1.0)              Crash   ``` The last two commands will crash the kernel ``` Metal device set to: Apple M1 systemMemory: 16.00 GB maxCacheSize: 5.33 GB /AppleInternal/Library/BuildRoots/c2cb9645dafc11edaa266ec1e3b3f7b3/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphExecutable.mm:1377: failed assertion `Incompatible element type for parameter at index 0, mlir module expected element type f64 but received f32' ```  What jax/jaxlib version are you using? jax 0.4.11, jaxlib 0.4.10, jaxmetal 0.0.2  Which accelerator(s) are you using? Apple GPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-15T15:40:38Z,bug Apple GPU (Metal) plugin,open,0,8,https://github.com/jax-ml/jax/issues/16435,"Just to confirm, this is on the Apple metal backend, yes? (All of this should work fine on nonexperimental backends)","I think the Metal plugin simply doesn't support f64, but it should report it more gracefully."," , that's right. Metal doesn't have FP64 support, we will add support to fail more gracefully."," , this issue is not related to Empty shapes. We don't have support for FP64 and we will add errorchecks early to exit gracefully.","Thanks for the prompt reply, I understand the issue now.",">  , that's right. Metal doesn't have FP64 support, we will add support to fail more gracefully. Are you planning on adding FP64 support? I would be really keen on that.","While we are looking into the issue, FP64 support will NOT be there for sometime. ", thank you for letting me know. 
522,"以下是一个github上的jax下的一个issue, 标题是(Custom PRNG: improve test coverage when enable_custom_prng=false)， 内容是 (We're now moving to a world where custom PRNG should exist sidebyside with the old PRNG implementation. This change improves test coverage for that, by enabling relevant custom PRNG tests even when the flag is set to False. Part of CC(RNGs: key types and custom implementations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Custom PRNG: improve test coverage when enable_custom_prng=false,"We're now moving to a world where custom PRNG should exist sidebyside with the old PRNG implementation. This change improves test coverage for that, by enabling relevant custom PRNG tests even when the flag is set to False. Part of CC(RNGs: key types and custom implementations)",2023-06-15T10:41:17Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16428
1141,"以下是一个github上的jax下的一个issue, 标题是(`jax` v0.4.12 with CUDA 12 depends on non-existent `jaxlib` version)， 内容是 ( Description **Problem:** The `jax` library `v0.4.12` with CUDA 12 locally installed depends on `jaxlib` `v0.4.12` using CUDNN 8.8: https://github.com/google/jax/blob/4c02f2c748a498a8c18e084e6c418afc5f6ec991/setup.pyL22L27 But for `jaxlib v0.4.12` there are only releases for CUDNN 8.9: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html, for example `jaxlib0.4.12+cuda12.cudnn89cp311cp311manylinux2014_x86_64.whl` Therefor, `jax` cannot be installed on Linux with a local CUDA 12 installation because neither having CUDNN 8.8 nor 8.9 will help because the dependencies of `jax` itself cannot be installed.  **Solution:** Publish a built for `jaxlib v0.4.12` for CUDNN 8.8 on the registry. Thank you in advance!  What jax/jaxlib version are you using? jax v0.4.12, jaxlib v0.4.12  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info RTX 3060 Ti)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,`jax` v0.4.12 with CUDA 12 depends on non-existent `jaxlib` version," Description **Problem:** The `jax` library `v0.4.12` with CUDA 12 locally installed depends on `jaxlib` `v0.4.12` using CUDNN 8.8: https://github.com/google/jax/blob/4c02f2c748a498a8c18e084e6c418afc5f6ec991/setup.pyL22L27 But for `jaxlib v0.4.12` there are only releases for CUDNN 8.9: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html, for example `jaxlib0.4.12+cuda12.cudnn89cp311cp311manylinux2014_x86_64.whl` Therefor, `jax` cannot be installed on Linux with a local CUDA 12 installation because neither having CUDNN 8.8 nor 8.9 will help because the dependencies of `jax` itself cannot be installed.  **Solution:** Publish a built for `jaxlib v0.4.12` for CUDNN 8.8 on the registry. Thank you in advance!  What jax/jaxlib version are you using? jax v0.4.12, jaxlib v0.4.12  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info RTX 3060 Ti",2023-06-15T01:40:35Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16421,"Thanks for the report! I think this is a duplicate of CC(Installation Bug: No Matching distribution found for jaxlib==0.4.12+cuda12.cudnn88; extra == ""cuda12_local"" (from jax[cuda12_local]))","> Thanks for the report! I think this is a duplicate of CC(Installation Bug: No Matching distribution found for jaxlib==0.4.12+cuda12.cudnn88; extra == ""cuda12_local"" (from jax[cuda12_local])) Thank you for the quick reply! Sorry, I was mainly looking through the open issues, not the closed ones. I could fix it by installing it from the repo directory, or downgrading CuDNN to 8.8 and using `0.4.10` (`0.4.11` had some issue with my CuDNN). Best, Moritz"
616,"以下是一个github上的jax下的一个issue, 标题是(JAX CUDA 12 wheel is outdated)， 内容是 ( Description I am unable to install the latest version of JAX 0.4.12 through the command: ``` pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Version 0.4.11 is installed instead.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX CUDA 12 wheel is outdated," Description I am unable to install the latest version of JAX 0.4.12 through the command: ``` pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Version 0.4.11 is installed instead.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info ++  ++",2023-06-14T16:56:17Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16412,"Thanks for the report! I think this is a duplicate of CC(Installation Bug: No Matching distribution found for jaxlib==0.4.12+cuda12.cudnn88; extra == ""cuda12_local"" (from jax[cuda12_local]))."
374,"以下是一个github上的jax下的一个issue, 标题是(CI nightly: update nightly wheel location)， 内容是 (It looks like this is now the recommended location for nightly numpy/scipy wheels (see https://github.com/scipy/scipy/issues/18675issuecomment1590697109))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,CI nightly: update nightly wheel location,It looks like this is now the recommended location for nightly numpy/scipy wheels (see https://github.com/scipy/scipy/issues/18675issuecomment1590697109),2023-06-14T09:32:36Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16400,"nightly CI failure is expected (it's tracked in CC(⚠️ Nightly upstreamdev CI failed ⚠️)) but the installation of the nightly libraries succeeded before the expected test failure, which is the relevant piece here."
459,"以下是一个github上的jax下的一个issue, 标题是(Fix test failure in LaxBackedNumpyTest.testFrexp4 on Windows.)， 内容是 (NumPy is inconsistent between platforms on what it returns for the exponent of an infinite input. On Linux/Mac it returns 0, and on Windows it returns 1. Normalize the test reference result to use 0 in this case.  FYI.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix test failure in LaxBackedNumpyTest.testFrexp4 on Windows.,"NumPy is inconsistent between platforms on what it returns for the exponent of an infinite input. On Linux/Mac it returns 0, and on Windows it returns 1. Normalize the test reference result to use 0 in this case.  FYI.",2023-06-14T00:44:12Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16393,I can locally verify this fixed the tests.
262,"以下是一个github上的jax下的一个issue, 标题是(Implement einsum for (at least some combinations) of arrays with ragged dimensions)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Implement einsum for (at least some combinations) of arrays with ragged dimensions,,2023-06-13T21:06:53Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/16386
1006,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Add native_lowering_disabled_checks parameter to jax2tf.convert)， 内容是 (Previously, we had a boolean parameter `native_lowering_strict_checks` that was disabling all safety checks. This mechanism had several disadvantages:   * the mechanism did not differentiate between different safety checks. E.g., in order to disable checking of the custom call targets, one had to disable checking for all custom call targets, and also the checking that the serialization and execution platforms are the same.   * the mechanism operated only at serialization time. Now, the XlaCallModule supports a `disabled_checks` attribute to control which safety checks should be disabled. Here we replace the `native_serialization_strict_checks` with `native_serialization_disabled_checks`, whose values are sequences of disabled check descriptors.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Add native_lowering_disabled_checks parameter to jax2tf.convert,"Previously, we had a boolean parameter `native_lowering_strict_checks` that was disabling all safety checks. This mechanism had several disadvantages:   * the mechanism did not differentiate between different safety checks. E.g., in order to disable checking of the custom call targets, one had to disable checking for all custom call targets, and also the checking that the serialization and execution platforms are the same.   * the mechanism operated only at serialization time. Now, the XlaCallModule supports a `disabled_checks` attribute to control which safety checks should be disabled. Here we replace the `native_serialization_strict_checks` with `native_serialization_disabled_checks`, whose values are sequences of disabled check descriptors.",2023-06-11T13:12:46Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/16347,   PTAL,"> I assume that we don't expect `native_serialization_disabled_checks` to be long. Otherwise, there are a few places where we are doing linear scan over this list, which we might want to fix to use `set` instead. Indeed, I expect that this will be used rarely and will be short. E.g., there are no uses anymore in Google of `native_serialization_strict_checks` (there were a few in the past, temporarily)."
2129,"以下是一个github上的jax下的一个issue, 标题是(Custom forward and reverse rule for function via transpose hitting python error)， 内容是 ( Description I was chatting with  about how to register both a custom vjp and jvp for use in https://github.com/EnzymeAD/EnzymeJAX (a plugin for Jax to import foreign code as a layer using Enzyme, testing out C++ to begin with). So far I have separate functions for use in a vjp/vjp since JaX isn't able to do both a custom forward and reverse registration on a single function. A conversation with  led me to trying to  making a custom transpose. I couldn't find docs on this so please forgive my ignorance here. I tried to use the custom_transpose in a variety of ways that all seemed to lead to errors that I don't quite understand. For example, one such attempt: ```python (jax.custom_jvp, nondiff_argnums=(0, 1, 2, 3)) .custom_transpose def cpp_fwd_internal(source: str, fn:str, argv: Sequence[str], out_shapes: Sequence[jax.core.ShapedArray], *args):   return _enzyme_primal_p.bind(       *args, source=source, fn=fn, argv=argv, out_shapes=out_shapes) .def_transpose def ft(*args):   print(args)   return 3. * args[1] ... ```  will hit ```python wmoses:~/git/EnzymeJaX (main) $ python test/test.py  Traceback (most recent call last):   File ""/home/wmoses/git/EnzymeJaX/test/test.py"", line 3, in      from enzyme_jax import cpp_fwd, cpp_rev   File ""/home/wmoses/anaconda3/lib/python3.9/sitepackages/enzyme_jax/__init__.py"", line 1, in      from enzyme_jax.primitives import cpp_fwd, cpp_rev   File ""/home/wmoses/anaconda3/lib/python3.9/sitepackages/enzyme_jax/primitives.py"", line 265, in      def cpp_fwd_internal(source: str, fn:str, argv: Sequence[str], out_shapes: Sequence[jax.core.ShapedArray], *args): TypeError: 'module' object is not callable ```  What jax/jaxlib version are you using? commit 21fc6e0229e0f5f1cb5f1f69d2c3daa2e5c2ca11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Custom forward and reverse rule for function via transpose hitting python error," Description I was chatting with  about how to register both a custom vjp and jvp for use in https://github.com/EnzymeAD/EnzymeJAX (a plugin for Jax to import foreign code as a layer using Enzyme, testing out C++ to begin with). So far I have separate functions for use in a vjp/vjp since JaX isn't able to do both a custom forward and reverse registration on a single function. A conversation with  led me to trying to  making a custom transpose. I couldn't find docs on this so please forgive my ignorance here. I tried to use the custom_transpose in a variety of ways that all seemed to lead to errors that I don't quite understand. For example, one such attempt: ```python (jax.custom_jvp, nondiff_argnums=(0, 1, 2, 3)) .custom_transpose def cpp_fwd_internal(source: str, fn:str, argv: Sequence[str], out_shapes: Sequence[jax.core.ShapedArray], *args):   return _enzyme_primal_p.bind(       *args, source=source, fn=fn, argv=argv, out_shapes=out_shapes) .def_transpose def ft(*args):   print(args)   return 3. * args[1] ... ```  will hit ```python wmoses:~/git/EnzymeJaX (main) $ python test/test.py  Traceback (most recent call last):   File ""/home/wmoses/git/EnzymeJaX/test/test.py"", line 3, in      from enzyme_jax import cpp_fwd, cpp_rev   File ""/home/wmoses/anaconda3/lib/python3.9/sitepackages/enzyme_jax/__init__.py"", line 1, in      from enzyme_jax.primitives import cpp_fwd, cpp_rev   File ""/home/wmoses/anaconda3/lib/python3.9/sitepackages/enzyme_jax/primitives.py"", line 265, in      def cpp_fwd_internal(source: str, fn:str, argv: Sequence[str], out_shapes: Sequence[jax.core.ShapedArray], *args): TypeError: 'module' object is not callable ```  What jax/jaxlib version are you using? commit 21fc6e0229e0f5f1cb5f1f69d2c3daa2e5c2ca11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_  ",2023-06-09T19:54:31Z,bug,closed,0,12,https://github.com/jax-ml/jax/issues/16336,"I was briefly using `custom_transpose` for the same purpose but eventually ran into issues. The issue that I filed about this shows a working example CC(`AssertionError` in `jax.custom_transpose` if linearized) . Currently, I am using something along the lines of ```python3 def f(ry, x):     r, y = ry     return y**2 * x / r def t(ry, t):     r, y = ry     return y**2 * t / r (jax.custom_jvp, nondiff_argnums=(2, )) def div(x, y, denom):     return linear_call(f, t, (denom, y), x) .defjvp def div_jvp(denom, primals, tangents):     x, y = primals     x_tan, y_tan = tangents     print(tangents)     return div(x, y, denom), div(x_tan, y, denom) d = partial(div, denom=3., y=10.) jax.jvp(d, (6., ), (2., )) jax.vjp(d, 6.)1 ``` Unfortunately, `linear_call` does not support batching nor higherorder derivatives :( (which doesn't matter to Enzyme in this case though)",bump  any updates?,"Custom transposition is (very gradual) work in progress, and not yet functioning. It's not documented for that reason, and I think shouldn't have it exposed as a public symbol altogether; most attempts to use it today will raise errors. I should figure out how to do that so as to to avoid confusion. The remaining question is: what can we do for your use case without involving `custom_transpose`? One answer is to set up a custom primitive with a transposition rule. Custom primitives usually take more work, since they need a rule for every transformation. But, looking at your example (plus guessing from this being Enzyme), I gather you're already quite familiar with setting up custom primitives. Is that right? If so, what do you think of setting up one more for this purpose? To be a bit more precise, what I have in mind is that you use a custom primitive in place of where you were trying to use custom transpose. This primitive would be bound from within the JVP rule of another operation.","Yeah I think that would be effective here (perhaps even more so). In particular, it would be nice to also integrate Enzyme's batching/vector mode into vmap. This also aligns nicely with some other ongoing MLIR work as well (). For this C FFI test, we actually already define primitives already (see https://github.com/EnzymeAD/EnzymeJAX/blob/59207f8911100e426b9ca989983a41a047bcf388/enzyme_jax/primitives.pyL285 ), so it's more a matter of understanding how to register the transformations for those primitives. ","I followed this guide (https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html) to move AD onto the primitives themselves (https://github.com/EnzymeAD/EnzymeJAX/pull/6/files). An issue I have come across for reverse mode (forward mode works fine) is that I would need to specify a custom modified forward and reverse pass (with tape passed to it), which doesn't seem to have nice support.  Looking at how custom_vjp is handled, it looks like its manually written in? https://github.com/google/jax/blob/f4eed78e9079aece4feffe6ab31e7c76b6826e7d/jax/_src/interpreters/ad.pyL390","We don't define VJPs for primitives (see for instance this note). A primitive can have a JVP rule and it can have a transpose rule. What I had in mind here was that you make one custom primitive `P1` whose JVP rule binds another primitive `P2`. You endow `P2` with a transposition rule. Together, these define the reversemode (i.e. VJP) behavior of the original primitive `P1`.","Yeah, and that style of custom rule is now implemented in the sample PR (https://github.com/EnzymeAD/EnzymeJAX/pull/6). The sole issue with this at the moment, which I'm curious if you have thoughts on, is as follows: In both forward and reverse mode, the registration mechanism requires redundantly computing the forward pass results (as well as potentially preserving primal values further into the reverse pass). For example, in forward mode, Enzyme can compute both the primal and derivative together in the same custom op. Unfortunately, having the generated forward mode op return both the primal and tangent, breaks the custom transpose (https://github.com/EnzymeAD/EnzymeJAX/blob/fa8893b4c230e096e8acb412ebaad60cd80f49cc/enzyme_jax/primitives.pyL307). In particular the error recieved is as follows (replace that return with simply res): ```python Traceback (most recent call last):   File ""/Users/wmoses/git/EnzymeJAX/test/test.py"", line 43, in      primals, f_vjp = jax.vjp(do_something, ones)   File ""/Users/wmoses/opt/anaconda3/lib/python3.9/sitepackages/jax/_src/api.py"", line 2164, in vjp     return _vjp(   File ""/Users/wmoses/opt/anaconda3/lib/python3.9/sitepackages/jax/_src/api.py"", line 2173, in _vjp     out_primal, out_vjp = ad.vjp(   File ""/Users/wmoses/opt/anaconda3/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 139, in vjp     out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)   File ""/Users/wmoses/opt/anaconda3/lib/python3.9/sitepackages/jax/_src/interpreters/ad.py"", line 130, in linearize     assert all(out_primal_pval.is_known() for out_primal_pval in out_primals_pvals) AssertionError ``` Notably in forward mode (aka without the custom tangent) that's fine, presumably since it doesn't have the unknown value. Therefore, the remedy I found at the moment to allow the custom_tangent to work is to duplicate the forward pass for the return. A similar issue occurs for forward mode, wherein Enzyme has its own augmented forward pass (that generates a tape of the minimum computed values needed for reverse, as well as any desired outputs). We need that tape for the reverse pass, so we have to run the augmented forward. However, doing this during the transposition itself puts this in the reverse pass, unnecessarily keeping around all of the primal values until that point, in addition to unnecessarily recomputing the primal (wherein that would have been potentially save redundant forwardpass computation). See below for an example: ```python .jit def g(a, b, x, y):     primals, f_vjp = jax.vjp(do_something, a, b)     return primals, f_vjp((x, y)) print(g.lower(ones, twos, x, y).compiler_ir(dialect=""stablehlo"")) print(g.lower(ones, twos, x, y).compiler_ir(dialect=""mhlo"")) ``` ```mlir module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {jax.arg_info = ""a"", mhlo.sharding = ""{replicated}""}, %arg1: tensor {jax.arg_info = ""b"", mhlo.sharding = ""{replicated}""}, %arg2: tensor {jax.arg_info = ""x"", mhlo.sharding = ""{replicated}""}, %arg3: tensor {jax.arg_info = ""y"", mhlo.sharding = ""{replicated}""}) > (tensor {jax.result_info = ""[0][0]""}, tensor {jax.result_info = ""[0][1]""}, tensor {jax.result_info = ""[1][0]""}, tensor {jax.result_info = ""[1][1]""}) {     %0 = mhlo.constant dense : tensor     %1:2 = mhlo.custom_call .primal(%0, %arg0, %arg1) {backend_config = """"} : (tensor, tensor, tensor) > (tensor, tensor)     %2 = mhlo.constant dense : tensor     %3:3 = mhlo.custom_call .aug(%2, %arg0, %arg1) {backend_config = """"} : (tensor, tensor, tensor) > (tensor, tensor, tensor)     %4 = mhlo.constant dense : tensor     %5:2 = mhlo.custom_call .rev(%4, %3 CC(Explicit tuples are not valid function parameters in Python 3), %arg2, %arg3) {backend_config = """"} : (tensor, tensor, tensor, tensor) > (tensor, tensor)     return %1 CC(未找到相关数据), %1 CC(Python 3 compatibility issues), %5 CC(未找到相关数据), %5 CC(Python 3 compatibility issues) : tensor, tensor, tensor, tensor   } } ```","> For example, in forward mode, Enzyme can compute both the primal and derivative together in the same custom op. I think I see what you mean now. For jax to generate a VJP from a JVP, we need the JVP to be ""unzippable"" (i.e. support partial evaluation over primals, for unknown tangents). You'll have to tell me if this is possible with Enzyme, but one way to design this would be for Enzyme to supply to Python more than one underlying operation. Namely: * For forwardmode:   * A JVP op `O1`, mapping (primal inputs, tangent inputs) > (primal outputs, tangents outputs) efficiently. I believe this is what you currently have. * For reversemode:   * A ""forward"" op `O2`, mapping (primal inputs, tangent inputs) > (primal outputs, residuals), and   * A ""reverse"" op `O3`, mapping (residuals, cotangent outputs) > cotangent inputs Then you can set up two JAX primitives `P1` and `P2` like we said before: * `P1` has a JVP rule that calls out to `O1` * `P1` has a partial evaluation rule that calls out to `O2` and stages out a bind of `P2` * `P2` has a transposition rule that calls out to `O3` Is this reasonable?","Yeah I think that design remedies the redundant computation (with the possible exception of tape preservation which I need to think more on). But if there are two primitives, would that not force users to have one primitive for forward mode support as distinct from reverse mode? Or is it possible upon say the MLIR lowering phase to check if there is a transpose interpreter in the stack and switch between the forward and reverse mode lowered primitives? Or if you have a different alternative.","Oh never mind I missed the custom partial evaluation rule, I think that would fix the double user primitive issue. Let me take a stab at that. ","Got it fully working, thanks!","Great to hear. We'll try to improve the interfaces here with time (custom transpose, primitives, etc.)."
600,"以下是一个github上的jax下的一个issue, 标题是(profiler_test.py fixes and add coverage to Cloud TPU CI)， 内容是 (* Add deps to test requirements, including in new `collectprofilerequirements.txt` (to avoid adding tensorflow to `testrequirements.txt`). * Use correct Python executable `ProfilerTest.test_remote_profiler` (`python` sometimes defaults to python2) * Run computations for longer in `ProfilerTest.test_remote_profiler`, othewise `collect_profile` sometimes misses it.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,profiler_test.py fixes and add coverage to Cloud TPU CI,"* Add deps to test requirements, including in new `collectprofilerequirements.txt` (to avoid adding tensorflow to `testrequirements.txt`). * Use correct Python executable `ProfilerTest.test_remote_profiler` (`python` sometimes defaults to python2) * Run computations for longer in `ProfilerTest.test_remote_profiler`, othewise `collect_profile` sometimes misses it.",2023-06-09T18:03:19Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16333
391,"以下是一个github上的jax下的一个issue, 标题是(Increase precision in LaxBackedScipySpatialTransformTests.testRotationApply)， 内容是 (Increase precision in LaxBackedScipySpatialTransformTests.testRotationApply Otherwise the test fails due to small numerical differences.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Increase precision in LaxBackedScipySpatialTransformTests.testRotationApply,Increase precision in LaxBackedScipySpatialTransformTests.testRotationApply Otherwise the test fails due to small numerical differences.,2023-06-08T21:35:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16325
627,"以下是一个github上的jax下的一个issue, 标题是(Expand RaggedAxis representation to support raggedly-shaped broadcasting)， 内容是 (Main content:  Change `RaggedAxis` to allow multiple ragged axes keyed to one stacked axis, e.g., a stack of matrices with different sizes in each dimension.  Update the batching rules for `broadcast_in_dim` and the implementation of `broadcast_to` to support ragged arguments and target shapes. I flagged two particular points of uncertainty using `TODO(reviewer)` comments.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Expand RaggedAxis representation to support raggedly-shaped broadcasting,"Main content:  Change `RaggedAxis` to allow multiple ragged axes keyed to one stacked axis, e.g., a stack of matrices with different sizes in each dimension.  Update the batching rules for `broadcast_in_dim` and the implementation of `broadcast_to` to support ragged arguments and target shapes. I flagged two particular points of uncertainty using `TODO(reviewer)` comments.",2023-06-08T20:21:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16324
1556,"以下是一个github上的jax下的一个issue, 标题是([XLA:Python] Fix incorrect code source information under Python 3.11.)， 内容是 ([XLA:Python] Fix incorrect code source information under Python 3.11. Do not multiply the result of PyFrame_GetLasti() by sizeof(_Py_CODEUNIT), because the CPython implementation already does this inside PyFrame_GetLasti(). * In CPython versions 3.9 or earlier, the f_lasti value in a PyFrameObject was in bytes. * In CPython 3.10, f_lasti was changed to be in code units, which required multiplying it by sizeof(_Py_CODEUNIT) before passing it to functions like PyCode_Addr2Line(). https://docs.python.org/3/whatsnew/3.10.htmlchangesinthecapi * In CPython 3.11, direct access to the representation of the PyFrameObject was removed from the headers, requiring the use of PyFrame_GetLasti() (https://docs.python.org/3/whatsnew/3.11.htmlpyframeobject311hiding). This function multiplies by sizeof(_Py_CODEUNIT) internally (https://github.com/python/cpython/blob/deaf509e8fc6e0363bd6f26d52ad42f976ec42f2/Objects/frameobject.cL1353) so there is no need for the caller to do this multiplication. It is difficult to write a good test for this, since the only symptom is slightly inaccurate code line information. This issue was found under a debug mode build of CPython (https://docs.python.org/3/using/configure.htmlpythondebugbuild), where PyCode_Addr2Line() has additional checks for out of range lasti values.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[XLA:Python] Fix incorrect code source information under Python 3.11.,"[XLA:Python] Fix incorrect code source information under Python 3.11. Do not multiply the result of PyFrame_GetLasti() by sizeof(_Py_CODEUNIT), because the CPython implementation already does this inside PyFrame_GetLasti(). * In CPython versions 3.9 or earlier, the f_lasti value in a PyFrameObject was in bytes. * In CPython 3.10, f_lasti was changed to be in code units, which required multiplying it by sizeof(_Py_CODEUNIT) before passing it to functions like PyCode_Addr2Line(). https://docs.python.org/3/whatsnew/3.10.htmlchangesinthecapi * In CPython 3.11, direct access to the representation of the PyFrameObject was removed from the headers, requiring the use of PyFrame_GetLasti() (https://docs.python.org/3/whatsnew/3.11.htmlpyframeobject311hiding). This function multiplies by sizeof(_Py_CODEUNIT) internally (https://github.com/python/cpython/blob/deaf509e8fc6e0363bd6f26d52ad42f976ec42f2/Objects/frameobject.cL1353) so there is no need for the caller to do this multiplication. It is difficult to write a good test for this, since the only symptom is slightly inaccurate code line information. This issue was found under a debug mode build of CPython (https://docs.python.org/3/using/configure.htmlpythondebugbuild), where PyCode_Addr2Line() has additional checks for out of range lasti values.",2023-06-08T13:42:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16312
1827,"以下是一个github上的jax下的一个issue, 标题是(Add `jax_debug_log_modules` config option.)， 内容是 (This can be used to enable debug logging for specific files (e.g. `JAX_DEBUG_LOG_MODULES=""jax._src.xla_bridge,jax._src.dispatch""`) or all jax (`JAX_DEBUG_LOG_MODULES=""jax""`). Example output: ``` $ JAX_DEBUG_LOG_MODULES=jax python3 c ""import jax; jax.numpy.add(1,1)"" DEBUG:20230607 00:27:57,399:jax._src.xla_bridge:352: No jax_plugins namespace packages available DEBUG:20230607 00:27:57,488:jax._src.path:29: etils.epath found. Using etils.epath for file I/O. DEBUG:20230607 00:27:57,663:jax._src.dispatch:272: Finished tracing + transforming fn for pjit in 0.0005719661712646484 sec DEBUG:20230607 00:27:57,664:jax._src.xla_bridge:590: Initializing backend 'tpu' DEBUG:20230607 00:28:00,502:jax._src.xla_bridge:602: Backend 'tpu' initialized DEBUG:20230607 00:28:00,502:jax._src.xla_bridge:590: Initializing backend 'cpu' DEBUG:20230607 00:28:00,542:jax._src.xla_bridge:602: Backend 'cpu' initialized DEBUG:20230607 00:28:00,544:jax._src.interpreters.pxla:1890: Compiling fn for with global shapes and types [ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})). DEBUG:20230607 00:28:00,547:jax._src.dispatch:272: Finished jaxpr to MLIR module conversion jit(fn) in 0.0023522377014160156 sec DEBUG:20230607 00:28:00,547:jax._src.xla_bridge:140: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]] DEBUG:20230607 00:28:00,571:jax._src.dispatch:272: Finished XLA compilation of jit(fn) in 0.023587703704833984 sec ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add `jax_debug_log_modules` config option.,"This can be used to enable debug logging for specific files (e.g. `JAX_DEBUG_LOG_MODULES=""jax._src.xla_bridge,jax._src.dispatch""`) or all jax (`JAX_DEBUG_LOG_MODULES=""jax""`). Example output: ``` $ JAX_DEBUG_LOG_MODULES=jax python3 c ""import jax; jax.numpy.add(1,1)"" DEBUG:20230607 00:27:57,399:jax._src.xla_bridge:352: No jax_plugins namespace packages available DEBUG:20230607 00:27:57,488:jax._src.path:29: etils.epath found. Using etils.epath for file I/O. DEBUG:20230607 00:27:57,663:jax._src.dispatch:272: Finished tracing + transforming fn for pjit in 0.0005719661712646484 sec DEBUG:20230607 00:27:57,664:jax._src.xla_bridge:590: Initializing backend 'tpu' DEBUG:20230607 00:28:00,502:jax._src.xla_bridge:602: Backend 'tpu' initialized DEBUG:20230607 00:28:00,502:jax._src.xla_bridge:590: Initializing backend 'cpu' DEBUG:20230607 00:28:00,542:jax._src.xla_bridge:602: Backend 'cpu' initialized DEBUG:20230607 00:28:00,544:jax._src.interpreters.pxla:1890: Compiling fn for with global shapes and types [ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated})). DEBUG:20230607 00:28:00,547:jax._src.dispatch:272: Finished jaxpr to MLIR module conversion jit(fn) in 0.0023522377014160156 sec DEBUG:20230607 00:28:00,547:jax._src.xla_bridge:140: get_compile_options: num_replicas=1 num_partitions=1 device_assignment=[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]] DEBUG:20230607 00:28:00,571:jax._src.dispatch:272: Finished XLA compilation of jit(fn) in 0.023587703704833984 sec ```",2023-06-07T22:17:52Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16304,"Ah good catch, added the BUILD rule."
7946,"以下是一个github上的jax下的一个issue, 标题是(Cannot bind to primitive Zero(AbstractToken()))， 内容是 ( Description In `mpi4jax` we make heavy use of tokens to prevent XLA to reorder our MPI calls, which is particularly a problem on XLA:GPU. A standard function would look something like ```python f(a):   ta = create_token()   b, tb = mpi_fun(a, token=ta)   c, tc = mpi_fun2(a, token=tb)   d = b+c   return d ``` and we do not return the token because in general `jax.jit` functions cannot return them, but still, a strong order is enforced within the compiled function because of the tokens. When we transpose our functions with `jax.linear_transpose`, we expect also the transpose function to enforce a strong ordering in the reverse order, for example: ```python f_t = jax.linear_transpose(f)  should match roughly def f_t(d_t):   b_t = d_t , c_t = d_t   tc_t = Zero(AbstractToken)  automatically there because we dont return tc   a_t, tb_t = mpi_fun2_transpose(c_t, token=tc_t)   a_t2, ta_t = mpi_fun(b_t, token= tb_t)   _ = create_token_transpose(ta_t)    return a_t + a_t2 ``` However, when mpi4jax attempts to bind the transposed token `Zero(AbstractToken)` an error is raised saying that it cannot be binded because XLA does not know how to represent it. I suspect that the correct way to treat the `Zero(AbstractToken)` should be exactly like a standard token, such that `tc_t` in the example above is    either a normal token that is passed around to enforce ordering in the transposed program.   either when binding a `Zero(AbstractToken)` you follow the same path as when binding a normal token an example code of how this impact mpi4jax can be had by installing the branch `pv/fixtoken` by for example running `pip install git+https://github.com/mpi4jax/mpi4jax.git/fixtoken` and then using the following MWE: ```python from mpi4py import MPI import jax import jax.numpy as jnp import numpy as np comm = MPI.COMM_WORLD rank = comm.Get_rank() size = comm.Get_size() from mpi4jax import allreduce arr = jnp.ones((3, 2)) _arr = arr.copy() def f(x):     (res,) = jax.linear_transpose(lambda x: allreduce(x, op=MPI.SUM)[0], arr)(x)     return res res = jax.jit(f)(arr) ``` which raises the error: ```python TypeError: Argument 'Zero(AbstractToken())' of type '' is not a valid JAX type ```     Open for the whole stack trace ```python ~/Dropbox/Ricerca/Codes/Python/mpi4jax pv/fixtoken 35s python3.11.2 ❯ python ex.py Traceback (most recent call last):   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 39, in      arr = jnp.ones((3, 2))   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 1, in test_allreduce_transpose   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 1, in    File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/mpi4jax/_src/collective_ops/allreduce.py"", line 1, in allreduce jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: Argument 'Zero(AbstractToken())' of type '' is not a valid JAX type The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 22, in      test_allreduce_transpose()   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 19, in test_allreduce_transpose     (res,) = jax.linear_transpose(lambda x: allreduce(x, op=MPI.SUM)[0], arr)(_arr)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/api.py"", line 2270, in transposed_fun     in_cts = ad.backward_pass(jaxpr, reduce_axes, True, const, dummies, out_cts)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 253, in backward_pass     cts_out = get_primitive_transpose(eqn.primitive)(               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/mpi4jax/_src/collective_ops/allreduce.py"", line 209, in mpi_allreduce_transpose_rule     res, token = mpi_allreduce_p.bind(                  ^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 131, in apply_primitive     in_avals, in_shardings = util.unzip2([arg_spec(a) for a in args])                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 131, in      in_avals, in_shardings = util.unzip2([arg_spec(a) for a in args])                                           ^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 102, in arg_spec     aval = xla.abstractify(x)            ^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/interpreters/xla.py"", line 200, in abstractify     raise TypeError(f""Argument '{x}' of type '{type(x)}' is not a valid JAX type"") jax._src.traceback_util.UnfilteredStackTrace: TypeError: Argument 'Zero(AbstractToken())' of type '' is not a valid JAX type The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 22, in      test_allreduce_transpose()   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 19, in test_allreduce_transpose     (res,) = jax.linear_transpose(lambda x: allreduce(x, op=MPI.SUM)[0], arr)(_arr)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/mpi4jax/_src/collective_ops/allreduce.py"", line 209, in mpi_allreduce_transpose_rule     res, token = mpi_allreduce_p.bind(                  ^^^^^^^^^^^^^^^^^^^^^ TypeError: Argument 'Zero(AbstractToken())' of type '' is not a valid JAX type ```    What jax/jaxlib version are you using? jax 0.4.11 jaxlib 0.4.11  Which accelerator(s) are you using? CPU  Additional system info MacOs M1  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot bind to primitive Zero(AbstractToken())," Description In `mpi4jax` we make heavy use of tokens to prevent XLA to reorder our MPI calls, which is particularly a problem on XLA:GPU. A standard function would look something like ```python f(a):   ta = create_token()   b, tb = mpi_fun(a, token=ta)   c, tc = mpi_fun2(a, token=tb)   d = b+c   return d ``` and we do not return the token because in general `jax.jit` functions cannot return them, but still, a strong order is enforced within the compiled function because of the tokens. When we transpose our functions with `jax.linear_transpose`, we expect also the transpose function to enforce a strong ordering in the reverse order, for example: ```python f_t = jax.linear_transpose(f)  should match roughly def f_t(d_t):   b_t = d_t , c_t = d_t   tc_t = Zero(AbstractToken)  automatically there because we dont return tc   a_t, tb_t = mpi_fun2_transpose(c_t, token=tc_t)   a_t2, ta_t = mpi_fun(b_t, token= tb_t)   _ = create_token_transpose(ta_t)    return a_t + a_t2 ``` However, when mpi4jax attempts to bind the transposed token `Zero(AbstractToken)` an error is raised saying that it cannot be binded because XLA does not know how to represent it. I suspect that the correct way to treat the `Zero(AbstractToken)` should be exactly like a standard token, such that `tc_t` in the example above is    either a normal token that is passed around to enforce ordering in the transposed program.   either when binding a `Zero(AbstractToken)` you follow the same path as when binding a normal token an example code of how this impact mpi4jax can be had by installing the branch `pv/fixtoken` by for example running `pip install git+https://github.com/mpi4jax/mpi4jax.git/fixtoken` and then using the following MWE: ```python from mpi4py import MPI import jax import jax.numpy as jnp import numpy as np comm = MPI.COMM_WORLD rank = comm.Get_rank() size = comm.Get_size() from mpi4jax import allreduce arr = jnp.ones((3, 2)) _arr = arr.copy() def f(x):     (res,) = jax.linear_transpose(lambda x: allreduce(x, op=MPI.SUM)[0], arr)(x)     return res res = jax.jit(f)(arr) ``` which raises the error: ```python TypeError: Argument 'Zero(AbstractToken())' of type '' is not a valid JAX type ```     Open for the whole stack trace ```python ~/Dropbox/Ricerca/Codes/Python/mpi4jax pv/fixtoken 35s python3.11.2 ❯ python ex.py Traceback (most recent call last):   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 39, in      arr = jnp.ones((3, 2))   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 1, in test_allreduce_transpose   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 1, in    File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/mpi4jax/_src/collective_ops/allreduce.py"", line 1, in allreduce jax._src.source_info_util.JaxStackTraceBeforeTransformation: TypeError: Argument 'Zero(AbstractToken())' of type '' is not a valid JAX type The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 22, in      test_allreduce_transpose()   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 19, in test_allreduce_transpose     (res,) = jax.linear_transpose(lambda x: allreduce(x, op=MPI.SUM)[0], arr)(_arr)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/api.py"", line 2270, in transposed_fun     in_cts = ad.backward_pass(jaxpr, reduce_axes, True, const, dummies, out_cts)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/interpreters/ad.py"", line 253, in backward_pass     cts_out = get_primitive_transpose(eqn.primitive)(               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/mpi4jax/_src/collective_ops/allreduce.py"", line 209, in mpi_allreduce_transpose_rule     res, token = mpi_allreduce_p.bind(                  ^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 131, in apply_primitive     in_avals, in_shardings = util.unzip2([arg_spec(a) for a in args])                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 131, in      in_avals, in_shardings = util.unzip2([arg_spec(a) for a in args])                                           ^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 102, in arg_spec     aval = xla.abstractify(x)            ^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Documents/pythonenvs/mpi4jax/python3.11.2/lib/python3.11/sitepackages/jax/_src/interpreters/xla.py"", line 200, in abstractify     raise TypeError(f""Argument '{x}' of type '{type(x)}' is not a valid JAX type"") jax._src.traceback_util.UnfilteredStackTrace: TypeError: Argument 'Zero(AbstractToken())' of type '' is not a valid JAX type The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 22, in      test_allreduce_transpose()   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/ex.py"", line 19, in test_allreduce_transpose     (res,) = jax.linear_transpose(lambda x: allreduce(x, op=MPI.SUM)[0], arr)(_arr)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/Users/filippo.vicentini/Dropbox/Ricerca/Codes/Python/mpi4jax/mpi4jax/_src/collective_ops/allreduce.py"", line 209, in mpi_allreduce_transpose_rule     res, token = mpi_allreduce_p.bind(                  ^^^^^^^^^^^^^^^^^^^^^ TypeError: Argument 'Zero(AbstractToken())' of type '' is not a valid JAX type ```    What jax/jaxlib version are you using? jax 0.4.11 jaxlib 0.4.11  Which accelerator(s) are you using? CPU  Additional system info MacOs M1  NVIDIA GPU info _No response_",2023-06-07T21:26:03Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/16303,"After diving deep into jax's internal, I managed to devise this possible fix: Every time you try to construct a `Zero(AbstractToken)` build a token instead. so simply editing this `read_cotangent` function  to ```python   from jax._src.core import (AbstractToken, Token)   def read_cotangent(v):     if isinstance(v.aval, AbstractToken):       return ct_env.pop(v, Token())     else:       return ct_env.pop(v, Zero(v.aval)) ``` fixes my reproducer above.  Of course this might not be semantically correct but I'm sure you know more about it..."," if the Issue is not very clear please let me know, and I can try to further clarify it. This is a big blocker for us..","Thanks for the ping! I managed not to notice until just now. > because in general jax.jit functions cannot return them Can you say more, and/or share a reproducer? If these are JAX tokens then they should be returnable from jitted functions, otherwise that's a JAX bug (even if it's not the main bug you're talking about). As for the main issue, I understand the general outline, but I need to look at mpi4jax more closely, or alternatively set up a toy model, to understand better. I have two gut reactions: 1. taking a narrow pigeonholed view, anywhere you see a symbolic zero `Zero(AbstractToken)`, i.e. in a JVP or transpose rule (not in ad.py's backward_pass), you probably want to instantiate it so that it's no longer symbolic; but in the bigger picture... 2. I don't think we want to rely on tangentsoftokens to be tokenlike at all, since throughout JAX's AD system we assume tangent types are vectorspacelike, in particular in that they have zero elements which have the behavior that any linear function applied to them is zero. In particular I don't think the fix in this comment is on the right track, unfortunately. > an example code of how this impact mpi4jax can be had by installing the branch pv/fixtoken by for example running pip install git+https://github.com/mpi4jax/mpi4jax.git/fixtoken and then using the following MWE: Where is the token in this example?","Thanks for answering! In the example I shared above the token is automatically generated by mpi4jax, but let me share an example that is more clear. I hope you don't mind installing mpi4jax (unfortunately tokens are used nowhere in jax so I can't build a reproducer there. The reproducer is the following: ```python import jax import jax.numpy as jnp import mpi4jax from mpi4py import MPI def f(a, b):     token_a = jax.lax.create_token()     c, token_b = mpi4jax.allreduce(a, MPI.SUM, token=token_a)     d, token_c = mpi4jax.allreduce(b, MPI.SUM, token=token_b)     e = c+d     return d x = jnp.ones(1) y = jnp.ones(1) r = f(x, y)  jax.make_jaxpr(f)(x, y)  jax.make_jaxpr(jax.linear_transpose(f, x, y))(r) jax.linear_transpose(f, x, y)(r) ``` Let me comment on what is going on in here by inspecting the jaxpr: ```python In [4]: jax.make_jaxpr(f)(x,y) Out[4]: { lambda ; a:f32[1] b:f32[1]. let     c:Tok = create_token     d:f32[1] e:Tok = allreduce_mpi[       comm=       op=       transpose=False     ] a c     f:f32[1] _:Tok = allreduce_mpi[       comm=       op=       transpose=False     ] b e     _:f32[1] = add d f   in (f,) } ``` You can see that I have two calls to the primitive `allreduce_mpi`, which is defined in here. This primitive takes two inputs: the array to be reduced and a token to prevent reordering. Now, what would be the correct transposition of this jaxpr?  I would assume is the execution in reverse of the operations. The transposition rule is defined `here` for master and it is essentially: ```python def mpi_allreduce_transpose_rule(tan_args, *x_args, op, comm,):     _, token = x_args     x_tan, token_tan = tan_args     res, token = mpi_allreduce_transpose_p.bind(         x_tan, token, op=op, comm=comm,     )     return res, token_tan ``` **notice that I bind the primal token instead of the tangent token. Is this correct?** It seems not, as this fails with error ```python File ~/Documents/pythonenvs/mpi4jax/python3.11.1/lib/python3.11/sitepackages/jax/_src/core.py:1326, in concrete_aval(x)    1324 if hasattr(x, '__jax_array__'):    1325   return concrete_aval(x.__jax_array__()) > 1326 raise TypeError(f""Value {repr(x)} with type {type(x)} is not a valid JAX ""    1327                  ""type"") TypeError: Value UndefinedPrimal(AbstractToken()) with type  is not a valid JAX type ``` Another reason suggesting me that I should bind the tangent of the token instead of the primal token here is that I would like to get in the linear transposition an execution order that is reverted, which I only get by binding the tangent token to the tangent primitive. Does this make sense? So in the branch `mpi4jax/fixtoken` I tried to modify the transposition rule to read ```python def mpi_allreduce_transpose_rule(tan_args, *x_args, op, comm):     _, _ = x_args     x_tan, token_tan = tan_args     res, token = mpi_allreduce_transpose_p.bind(         x_tan, token_tan, op=op, comm=comm,     )     return res, token ``` but this fails as well with the error I shared in the original post, namely ```python File ~/Documents/pythonenvs/mpi4jax/python3.11.1/lib/python3.11/sitepackages/jax/_src/core.py:1326, in concrete_aval(x)    1324 if hasattr(x, '__jax_array__'):    1325   return concrete_aval(x.__jax_array__()) > 1326 raise TypeError(f""Value {repr(x)} with type {type(x)} is not a valid JAX ""    1327                  ""type"") TypeError: Value Zero(AbstractToken()) with type  is not a valid JAX type ```",">>because in general jax.jit functions cannot return them? >Can you say more, and/or share a reproducer? If these are JAX tokens then they should be returnable from jitted functions, otherwise that's a JAX bug (even if it's not the main bug you're talking about). Apparently I was not up to date, and it seems that it is now possible to return tokens (I remember about a year ago it was not possible).  However it will still error if you try to transpose a token: ```python import jax import jax.numpy as jnp import mpi4jax from mpi4py import MPI def f(a):     token_a = jax.lax.create_token()     b, token_b = mpi4jax.allreduce(a, MPI.SUM, token=token_a)     return b, token_b x = jnp.ones(1) r,s = f(x) jax.make_jaxpr(f)(x) jax.make_jaxpr(jax.linear_transpose(f, x))(r) ``` that fails with  ```python File ~/Documents/pythonenvs/mpi4jax/python3.11.1/lib/python3.11/sitepackages/jax/_src/dtypes.py:530, in dtype(x, canonicalize)     528     dt = np.result_type(x)     529   except TypeError as err: > 530     raise TypeError(f""Cannot determine dtype of {x}"") from err     531 if dt not in _jax_dtype_set and not is_opaque_dtype(dt):     532   raise TypeError(f""Value '{x}' with dtype {dt} is not a valid JAX array ""     533                   ""type. Only arrays of numeric types are supported by JAX."") TypeError: Cannot determine dtype of AbstractToken() ``` Though that's a different bug from what I originally reported and I'm not so worried about this one because tokens usually remain inside the jitted functions...",> I hope you don't mind installing mpi4jax I don't mind at all! Thanks for the detailed repro and info. I'll take a look...,I haven't had a chance yet :/ I expect I can in the next 48 hours or so.,thanks for the update! looking forwards for a reply , any luck? Can I do anything to help you nail this problem down?, pretty please 🥹?
1837,"以下是一个github上的jax下的一个issue, 标题是(Metal plugin - failed to legalize operation 'mhlo.convolution')， 内容是 ( Description Encountered this on Apple M2 Pro after following instructions from https://developer.apple.com/metal/jax/, and then trying to get https://github.com/sanchitgandhi/whisperjax to run. Steps for reproducing: * Compile and install metal jax following instructions from: https://developer.apple.com/metal/jax/ * Install whisperjax with: `pip install git+https://github.com/sanchitgandhi/whisperjax.git` ```python from whisper_jax import FlaxWhisperPipline  instantiate pipeline pipeline = FlaxWhisperPipline(""openai/whisperlargev2"")  JIT compile the forward call  slow, but we only do once text = pipeline(""audio.mp3"") ```` Leads to the following error: ```bash jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError:  UNKNOWN: /Users/pere/jaxmetal/lib/python3.10/sitepackages/whisper_jax/layers.py:1236:0:  error: failed to legalize operation 'mhlo.convolution' /Users/pere/jaxmetal/lib/python3.10/sitepackages/whisper_jax/layers.py:1236:0: note: see current operation:  %111 = ""mhlo.convolution""(%110, >)  {batch_group_count = 1 : i64, dimension_numbers = mhlo.conv[b, 0, f]>, feature_group_count = 1 : i64, lhs_dilation = dense :  tensor, padding = dense :  tensor, precision_config = [mhlo,  mhlo],  rhs_dilation = dense :  tensor, window_reversal = dense :  tensor,  window_strides = dense : tensor} : (tensor, tensor) > tensor ``` Maybe this operation isnt implemented yet?    .   What jax/jaxlib version are you using? jaxlibv0.4.10  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Metal plugin - failed to legalize operation 'mhlo.convolution'," Description Encountered this on Apple M2 Pro after following instructions from https://developer.apple.com/metal/jax/, and then trying to get https://github.com/sanchitgandhi/whisperjax to run. Steps for reproducing: * Compile and install metal jax following instructions from: https://developer.apple.com/metal/jax/ * Install whisperjax with: `pip install git+https://github.com/sanchitgandhi/whisperjax.git` ```python from whisper_jax import FlaxWhisperPipline  instantiate pipeline pipeline = FlaxWhisperPipline(""openai/whisperlargev2"")  JIT compile the forward call  slow, but we only do once text = pipeline(""audio.mp3"") ```` Leads to the following error: ```bash jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError:  UNKNOWN: /Users/pere/jaxmetal/lib/python3.10/sitepackages/whisper_jax/layers.py:1236:0:  error: failed to legalize operation 'mhlo.convolution' /Users/pere/jaxmetal/lib/python3.10/sitepackages/whisper_jax/layers.py:1236:0: note: see current operation:  %111 = ""mhlo.convolution""(%110, >)  {batch_group_count = 1 : i64, dimension_numbers = mhlo.conv[b, 0, f]>, feature_group_count = 1 : i64, lhs_dilation = dense :  tensor, padding = dense :  tensor, precision_config = [mhlo,  mhlo],  rhs_dilation = dense :  tensor, window_reversal = dense :  tensor,  window_strides = dense : tensor} : (tensor, tensor) > tensor ``` Maybe this operation isnt implemented yet?    .   What jax/jaxlib version are you using? jaxlibv0.4.10  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-07T20:29:55Z,enhancement Apple GPU (Metal) plugin,closed,3,11,https://github.com/jax-ml/jax/issues/16302,,"Thanks   for filing the issue, we will take a look.  ",This is a case of the dimension_numbers we didn't support. We will look into expanding our conversion patterns for convolution op.  ,"By the way, this also happens when you try to train a very simple neural net with `flax`, e.g., the one in the tutorial: https://github.com/google/flax/tree/main/examples/mnist/", Thanks. Any rough estimates on a time frame here?,Do you know if there is any workaround? Thanks!,"Maybe I am wrong here, but for me this looks like lacking support for a standard 1d conv layers. Is this correct? If so, this is blocking a lot of different use cases. For me it blocks a very interesting use of Whisper on Mac. I have no idea of the complexity of implementing this, but is there any chance it could be prioritised?    ","Has this been abandoned?  Can I help? Quite the deal breaker that we cannot invert matrices, do 3d convolutions, eigenvector decomposition, etc. ","  Could you give an update on this? I am still unable to use jaxmetal to run whisperjax, even using v0.0.5. What of the needed operations is expected in the near future?",The issue is lacking of conv1d support. Will look into adding a conversion sequence to shape it to conv2d. ,The issue should be fixed in jaxmetal 0.0.7. Pls reopen it if otherwise. 
542,"以下是一个github上的jax下的一个issue, 标题是(Fix test failures in JAX under NumPy 1.25.0rc1.)， 内容是 (Fix test failures in JAX under NumPy 1.25.0rc1. `jnp.finfo(...)` of an Array type yields: ``` TypeError: unhashable type: 'ArrayImpl' ``` However, `np.finfo(...)` no longer accepts NumPy arrays as input either, so it would be consistent to require the user to pass a dtype where they are currently passing an array.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix test failures in JAX under NumPy 1.25.0rc1.,"Fix test failures in JAX under NumPy 1.25.0rc1. `jnp.finfo(...)` of an Array type yields: ``` TypeError: unhashable type: 'ArrayImpl' ``` However, `np.finfo(...)` no longer accepts NumPy arrays as input either, so it would be consistent to require the user to pass a dtype where they are currently passing an array.",2023-06-07T16:53:41Z,,closed,0,3,https://github.com/jax-ml/jax/issues/16297,We're going to hold off submitting this because https://github.com/numpy/numpy/issues/23867 might resolve this., https://github.com/numpy/numpy/issues/23867 has been resolved via https://github.com/numpy/numpy/pull/14847. The changes in this PR might be good anyway. `jnp.finfo(alpha_k.dtype)` is a bit faster than `jnp.finfo(alpha_k)` (although I have no idea whether that part of the code is performance critical)," Thanks for the fix! Yes, it seems like this might be a tiny bit better anyway. I'll submit it."
5203,"以下是一个github上的jax下的一个issue, 标题是(Vanilla installation fails: HASHES)， 内容是 ( Description This Dockerfile  ``` FROM nvidia/cuda:12.1.1baseubuntu22.04  install dependencies via pip RUN apt update && apt install y python3pip python3 RUN pip3 install upgrade pip  CUDA 12 installation  Note: wheels only available on linux. RUN pip3 install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` **Occasionally** (almost every time) throws this error at the last step:  > [4/6] RUN pip3 install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html: CC(未找到相关数据) 0.479 Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html CC(未找到相关数据) 0.738 Collecting jax[cuda12_pip] CC(未找到相关数据) 0.795   Downloading jax0.4.11.tar.gz (1.3 MB) CC(未找到相关数据) 0.831      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 37.2 MB/s eta 0:00:00 CC(未找到相关数据) 0.911   Installing build dependencies: started CC(未找到相关数据) 2.243   Installing build dependencies: finished with status 'done' CC(未找到相关数据) 2.244   Getting requirements to build wheel: started CC(未找到相关数据) 2.342   Getting requirements to build wheel: finished with status 'done' CC(未找到相关数据) 2.343   Preparing metadata (pyproject.toml): started CC(未找到相关数据) 2.451   Preparing metadata (pyproject.toml): finished with status 'done' CC(未找到相关数据) 2.515 Collecting mldtypes>=0.1.0 (from jax[cuda12_pip]) CC(未找到相关数据) 2.518   Downloading ml_dtypes0.2.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB) CC(未找到相关数据) 2.532      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 80.2 MB/s eta 0:00:00 CC(未找到相关数据) 2.766 Collecting numpy>=1.21 (from jax[cuda12_pip]) CC(未找到相关数据) 2.769   Downloading numpy1.24.3cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB) CC(未找到相关数据) 2.981      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 76.4 MB/s eta 0:00:00 CC(未找到相关数据) 3.032 Collecting opteinsum (from jax[cuda12_pip]) CC(未找到相关数据) 3.035   Downloading opt_einsum3.3.0py3noneany.whl (65 kB) CC(未找到相关数据) 3.038      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 34.3 MB/s eta 0:00:00 CC(未找到相关数据) 3.189 Collecting scipy>=1.7 (from jax[cuda12_pip]) CC(未找到相关数据) 3.193   Downloading scipy1.10.1cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB) CC(未找到相关数据) 3.572      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.4/34.4 MB 70.9 MB/s eta 0:00:00 CC(未找到相关数据) 3.680 Collecting jaxlib==0.4.11+cuda12.cudnn88 (from jax[cuda12_pip]) CC(未找到相关数据) 3.955   Downloading https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.11%2Bcuda12.cudnn88cp310cp310manylinux2014_x86_64.whl (170.8 MB) CC(未找到相关数据) 9.235      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.8/170.8 MB 20.5 MB/s eta 0:00:00 CC(未找到相关数据) 9.374 Collecting nvidiacublascu12 (from jax[cuda12_pip]) CC(未找到相关数据) 9.377   Downloading nvidia_cublas_cu1212.1.3.1py3nonemanylinux1_x86_64.whl (410.6 MB) CC(未找到相关数据) 14.09      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 15.6 MB/s eta 0:00:00 CC(未找到相关数据) 14.38 Collecting nvidiacudacupticu12 (from jax[cuda12_pip]) CC(未找到相关数据) 14.38   Downloading nvidia_cuda_cupti_cu1212.1.105py3nonemanylinux1_x86_64.whl (14.1 MB) CC(未找到相关数据) 14.54      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 84.7 MB/s eta 0:00:00 CC(未找到相关数据) 14.59 Collecting nvidiacudanvcccu12 (from jax[cuda12_pip]) CC(未找到相关数据) 14.59   Downloading nvidia_cuda_nvcc_cu1212.1.105py3nonemanylinux1_x86_64.whl (20.0 MB) CC(未找到相关数据) 14.81      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/20.0 MB 84.1 MB/s eta 0:00:00 CC(未找到相关数据) 14.87 Collecting nvidiacudaruntimecu12 (from jax[cuda12_pip]) CC(未找到相关数据) 14.87   Downloading nvidia_cuda_runtime_cu1212.1.105py3nonemanylinux1_x86_64.whl (823 kB) CC(未找到相关数据) 14.88      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 87.4 MB/s eta 0:00:00 CC(未找到相关数据) 14.92 Collecting nvidiacudnncu12>=8.9 (from jax[cuda12_pip]) CC(未找到相关数据) 14.92   Downloading nvidia_cudnn_cu128.9.2.26py3nonemanylinux1_x86_64.whl (731.7 MB) CC(未找到相关数据) 15.18      ━                                       24.7/731.7 MB 84.1 MB/s eta 0:00:09 CC(未找到相关数据) 15.20 ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them. CC(未找到相关数据) 15.20     nvidiacudnncu12>=8.9 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu128.9.2.26py3nonemanylinux1_x86_64.whl (from jax[cuda12_pip]): CC(未找到相关数据) 15.20         Expected sha256 5ccb288774fdfb07a7e7025ffec286971c06d8d7b4fb162525334616d7629ff9 CC(未找到相关数据) 15.20              Got        564176f093ec503350285f8b312eb696913d5efb63b7320f5a87e6ab29edae23 CC(未找到相关数据) 15.20  **Note:** adding nocachedir does not help.  What jax/jaxlib version are you using? jax0.4.11  Which accelerator(s) are you using? NVIDIA RTX 4090  Additional system info _No response_  NVIDIA GPU info Wed Jun  7 10:24:30 2023        ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Vanilla installation fails: HASHES," Description This Dockerfile  ``` FROM nvidia/cuda:12.1.1baseubuntu22.04  install dependencies via pip RUN apt update && apt install y python3pip python3 RUN pip3 install upgrade pip  CUDA 12 installation  Note: wheels only available on linux. RUN pip3 install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` **Occasionally** (almost every time) throws this error at the last step:  > [4/6] RUN pip3 install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html: CC(未找到相关数据) 0.479 Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html CC(未找到相关数据) 0.738 Collecting jax[cuda12_pip] CC(未找到相关数据) 0.795   Downloading jax0.4.11.tar.gz (1.3 MB) CC(未找到相关数据) 0.831      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 37.2 MB/s eta 0:00:00 CC(未找到相关数据) 0.911   Installing build dependencies: started CC(未找到相关数据) 2.243   Installing build dependencies: finished with status 'done' CC(未找到相关数据) 2.244   Getting requirements to build wheel: started CC(未找到相关数据) 2.342   Getting requirements to build wheel: finished with status 'done' CC(未找到相关数据) 2.343   Preparing metadata (pyproject.toml): started CC(未找到相关数据) 2.451   Preparing metadata (pyproject.toml): finished with status 'done' CC(未找到相关数据) 2.515 Collecting mldtypes>=0.1.0 (from jax[cuda12_pip]) CC(未找到相关数据) 2.518   Downloading ml_dtypes0.2.0cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB) CC(未找到相关数据) 2.532      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 80.2 MB/s eta 0:00:00 CC(未找到相关数据) 2.766 Collecting numpy>=1.21 (from jax[cuda12_pip]) CC(未找到相关数据) 2.769   Downloading numpy1.24.3cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB) CC(未找到相关数据) 2.981      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 76.4 MB/s eta 0:00:00 CC(未找到相关数据) 3.032 Collecting opteinsum (from jax[cuda12_pip]) CC(未找到相关数据) 3.035   Downloading opt_einsum3.3.0py3noneany.whl (65 kB) CC(未找到相关数据) 3.038      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 34.3 MB/s eta 0:00:00 CC(未找到相关数据) 3.189 Collecting scipy>=1.7 (from jax[cuda12_pip]) CC(未找到相关数据) 3.193   Downloading scipy1.10.1cp310cp310manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB) CC(未找到相关数据) 3.572      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.4/34.4 MB 70.9 MB/s eta 0:00:00 CC(未找到相关数据) 3.680 Collecting jaxlib==0.4.11+cuda12.cudnn88 (from jax[cuda12_pip]) CC(未找到相关数据) 3.955   Downloading https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.11%2Bcuda12.cudnn88cp310cp310manylinux2014_x86_64.whl (170.8 MB) CC(未找到相关数据) 9.235      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.8/170.8 MB 20.5 MB/s eta 0:00:00 CC(未找到相关数据) 9.374 Collecting nvidiacublascu12 (from jax[cuda12_pip]) CC(未找到相关数据) 9.377   Downloading nvidia_cublas_cu1212.1.3.1py3nonemanylinux1_x86_64.whl (410.6 MB) CC(未找到相关数据) 14.09      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 15.6 MB/s eta 0:00:00 CC(未找到相关数据) 14.38 Collecting nvidiacudacupticu12 (from jax[cuda12_pip]) CC(未找到相关数据) 14.38   Downloading nvidia_cuda_cupti_cu1212.1.105py3nonemanylinux1_x86_64.whl (14.1 MB) CC(未找到相关数据) 14.54      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 84.7 MB/s eta 0:00:00 CC(未找到相关数据) 14.59 Collecting nvidiacudanvcccu12 (from jax[cuda12_pip]) CC(未找到相关数据) 14.59   Downloading nvidia_cuda_nvcc_cu1212.1.105py3nonemanylinux1_x86_64.whl (20.0 MB) CC(未找到相关数据) 14.81      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/20.0 MB 84.1 MB/s eta 0:00:00 CC(未找到相关数据) 14.87 Collecting nvidiacudaruntimecu12 (from jax[cuda12_pip]) CC(未找到相关数据) 14.87   Downloading nvidia_cuda_runtime_cu1212.1.105py3nonemanylinux1_x86_64.whl (823 kB) CC(未找到相关数据) 14.88      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 87.4 MB/s eta 0:00:00 CC(未找到相关数据) 14.92 Collecting nvidiacudnncu12>=8.9 (from jax[cuda12_pip]) CC(未找到相关数据) 14.92   Downloading nvidia_cudnn_cu128.9.2.26py3nonemanylinux1_x86_64.whl (731.7 MB) CC(未找到相关数据) 15.18      ━                                       24.7/731.7 MB 84.1 MB/s eta 0:00:09 CC(未找到相关数据) 15.20 ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them. CC(未找到相关数据) 15.20     nvidiacudnncu12>=8.9 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu128.9.2.26py3nonemanylinux1_x86_64.whl (from jax[cuda12_pip]): CC(未找到相关数据) 15.20         Expected sha256 5ccb288774fdfb07a7e7025ffec286971c06d8d7b4fb162525334616d7629ff9 CC(未找到相关数据) 15.20              Got        564176f093ec503350285f8b312eb696913d5efb63b7320f5a87e6ab29edae23 CC(未找到相关数据) 15.20  **Note:** adding nocachedir does not help.  What jax/jaxlib version are you using? jax0.4.11  Which accelerator(s) are you using? NVIDIA RTX 4090  Additional system info _No response_  NVIDIA GPU info Wed Jun  7 10:24:30 2023        ++  ++",2023-06-07T10:31:55Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16293,"I don't think this is a JAX issue. JAX isn't using a requirements file, does not specify hashes of its dependencies, and this is occurring with one of NVIDIA's packages. Does the same thing happen if you simply install `pip install nvidiacudnncu12` in your `Dockerfile` instead of installing JAX? From google searching the error message, it might be an issue with stale data in the pip cache in that docker image. Try rebuilding the docker image, or adding ``` pip cache purge ``` before the `pip install` command? For what it's worth, if you are using the `cuda12_pip` installation, you don't need to use a CUDA docker image. You just need  the NVIDIA driver and to use `nvidiadocker` to run the container.","You're right, I discovered it is related to my ISP, even things like pip install torch fail. Using a vpn solves the issue for the moment. Thanks anyways and sorry for this."
1386,"以下是一个github上的jax下的一个issue, 标题是(JAX Error when run on CPU without connection to TPU)， 内容是 ( Description I tried running the following very simple python code using JAX on my local device, without any connection to a GPU or TPU. ``` import jax import jax.numpy as jnp x = jnp.arange(10) print(x) ``` However, I received the following error, which I have not been able to resolve by myself. Any ideas on how to fix it? ``` UserWarning: cloud_tpu_init failed: ConnectionError(MaxRetryError(""HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/agentworkernumber (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 2] Name or service not known'))""))  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report "" WARNING: Logging before InitGoogle() is written to STDERR I0000 00:00:1686133010.112154  337255 common_lib.cc:156] Failed to fetch URL on try 1 out of 6: Couldn't connect to server ```  What jax/jaxlib version are you using? v0.3.25  Which accelerator(s) are you using? CPU  Additional system info Ubuntu  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,JAX Error when run on CPU without connection to TPU," Description I tried running the following very simple python code using JAX on my local device, without any connection to a GPU or TPU. ``` import jax import jax.numpy as jnp x = jnp.arange(10) print(x) ``` However, I received the following error, which I have not been able to resolve by myself. Any ideas on how to fix it? ``` UserWarning: cloud_tpu_init failed: ConnectionError(MaxRetryError(""HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/agentworkernumber (Caused by NewConnectionError(': Failed to establish a new connection: [Errno 2] Name or service not known'))""))  This a JAX bug; please report an issue at https://github.com/google/jax/issues   _warn(f""cloud_tpu_init failed: {repr(exc)}\n This a JAX bug; please report "" WARNING: Logging before InitGoogle() is written to STDERR I0000 00:00:1686133010.112154  337255 common_lib.cc:156] Failed to fetch URL on try 1 out of 6: Couldn't connect to server ```  What jax/jaxlib version are you using? v0.3.25  Which accelerator(s) are you using? CPU  Additional system info Ubuntu  NVIDIA GPU info _No response_",2023-06-07T10:24:22Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/16292,0.3.25 is fairly old at this point. Can you try with the current release (0.4.11)?,"Running it on 0.4.11 just causes the following message to repeat six times. ``` WARNING: Logging before InitGoogle() is written to STDERR I0000 00:00:1686146493.777501  408669 common_lib.cc:156] Failed to fetch URL on try 1 out of 6: Couldn't connect to server ``` Then this comes up, after which the messaes from above repeat again. ``` Failed to get 'tpuenv' from instance metadata: Couldn't connect to server ``` This one also appears after a while. Any ideas? ``` WARNING: could not determine TPU accelerator type. Set env var TPU_ACCELERATOR_TYPE to set manually. TPU runtime may not be properly initialized. ```",How did you install jaxlib?,"Also, can you share `pip list`'s output? Do you have `libtpu` installed?",I just used ``` pip install jaxlib ``` I do have `libtpunightly` installed. ,"Ah! Uninstall `libtpunightly` if you don't have a TPU.  Can we fail more gracefully in this case? It's not necessarily a JAX bug, merely ""you don't have a TPU"" here..."
1431,"以下是一个github上的jax下的一个issue, 标题是(Failed to launch CUDA kernel when multiplying bool matrices with large batch size.)， 内容是 ( Description Here is an minimum example to reproduce the bug. Tested using a single RTX 3090. ```python import jax from jax import jit, vmap import jax.numpy as jnp  def f(adj, mat):     return adj @ mat / jnp.sum(adj, axis=1)[:, jnp.newaxis] adj = jnp.ones((1024 * 100, 10, 10), dtype=bool) mat = jnp.ones((1024 * 100, 10, 100), dtype=float) jax.jit(vmap(f))(adj, mat) ``` The code will result in the following error when compiling: ```bash Traceback (most recent call last):   File ""/****/bug.py"", line 12, in      jax.jit(vmap(f))(adj, mat) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to launch CUDA kernel: triton_gemm_dot_0 with block dimensions: 128x1x1 and grid dimensions: 4x1x102400 and shared memory size: 65536: CUDA_ERROR_INVALID_VALUE: invalid argument ``` Here adj is an adjacency matrix of type bool and mat is just a random matrix. Setting adj to float or avoid using `@` by using a combination of `vmap` and `jnp.sum` could solve this problem.  What jax/jaxlib version are you using? jax v0.4.11, jaxlib 0.4.11+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Failed to launch CUDA kernel when multiplying bool matrices with large batch size.," Description Here is an minimum example to reproduce the bug. Tested using a single RTX 3090. ```python import jax from jax import jit, vmap import jax.numpy as jnp  def f(adj, mat):     return adj @ mat / jnp.sum(adj, axis=1)[:, jnp.newaxis] adj = jnp.ones((1024 * 100, 10, 10), dtype=bool) mat = jnp.ones((1024 * 100, 10, 100), dtype=float) jax.jit(vmap(f))(adj, mat) ``` The code will result in the following error when compiling: ```bash Traceback (most recent call last):   File ""/****/bug.py"", line 12, in      jax.jit(vmap(f))(adj, mat) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: Failed to launch CUDA kernel: triton_gemm_dot_0 with block dimensions: 128x1x1 and grid dimensions: 4x1x102400 and shared memory size: 65536: CUDA_ERROR_INVALID_VALUE: invalid argument ``` Here adj is an adjacency matrix of type bool and mat is just a random matrix. Setting adj to float or avoid using `@` by using a combination of `vmap` and `jnp.sum` could solve this problem.  What jax/jaxlib version are you using? jax v0.4.11, jaxlib 0.4.11+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info ``` ++  ++ ```",2023-06-07T02:52:29Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16286,"Thanks for the report, I filed an XLA bug. If you need a workaround until it is fixed, try setting the environment variable `XLA_FLAGS=xla_gpu_enable_triton_gemm=false`","https://github.com/openxla/xla/pull/3530 fixed this, and should be in the next jaxlib release."
1585,"以下是一个github上的jax下的一个issue, 标题是(Possible memory leak / OOM when closing over large constant in jitted loop)， 内容是 ( Description I'm getting consistent out of memory errors when jitting functions that close over large constant arrays. It seems similar to CC(Memory leak in compiled loops closing over large constants) but only occurs when compiling the closed over function.  MWE: ```python import numpy as np import jax import jax.numpy as jnp ns = 900 ne = 1000 nq = 800 bigmat = jnp.array(np.random.random((ne,nq,ns))) x = jnp.empty(ns) def foo(x, bigmat):     def body(i, f):         gx = bigmat[i]          do some other stuff that isn't important         f = f.at[i].set(gx.sum())         return f     f = jnp.zeros(x.shape[0])     f = jax.lax.fori_loop(0, 1, body, f)     return f  this runs fine, no OOM issue f = foo(x, bigmat)  this also runs fine, basically the same runtime as expected since the  fori_loop is already compiled and thats most of the cost f = jax.jit(foo)(x, bigmat).block_until_ready()  this causes OOM f = jax.jit(lambda y: foo(y, bigmat))(x) ``` Also curiously, if `bigmat` is replaced with zeros or ones, it seems to work fine, suggesting there's some lower level compiler optimization going on.  What jax/jaxlib version are you using? jax=0.4.2, jaxlib=0.4.2  Which accelerator(s) are you using? tried on intel/AMD cpus, nvidia v100 and a100 gpus  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Possible memory leak / OOM when closing over large constant in jitted loop," Description I'm getting consistent out of memory errors when jitting functions that close over large constant arrays. It seems similar to CC(Memory leak in compiled loops closing over large constants) but only occurs when compiling the closed over function.  MWE: ```python import numpy as np import jax import jax.numpy as jnp ns = 900 ne = 1000 nq = 800 bigmat = jnp.array(np.random.random((ne,nq,ns))) x = jnp.empty(ns) def foo(x, bigmat):     def body(i, f):         gx = bigmat[i]          do some other stuff that isn't important         f = f.at[i].set(gx.sum())         return f     f = jnp.zeros(x.shape[0])     f = jax.lax.fori_loop(0, 1, body, f)     return f  this runs fine, no OOM issue f = foo(x, bigmat)  this also runs fine, basically the same runtime as expected since the  fori_loop is already compiled and thats most of the cost f = jax.jit(foo)(x, bigmat).block_until_ready()  this causes OOM f = jax.jit(lambda y: foo(y, bigmat))(x) ``` Also curiously, if `bigmat` is replaced with zeros or ones, it seems to work fine, suggesting there's some lower level compiler optimization going on.  What jax/jaxlib version are you using? jax=0.4.2, jaxlib=0.4.2  Which accelerator(s) are you using? tried on intel/AMD cpus, nvidia v100 and a100 gpus  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-06T18:53:42Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/16278,Try upgrading your jax and jaxlib version to 0.4.11 and see if it still happens?,"I tried a few different versions of jax/jaxlib with similar behavior. The exact max size of the big array varies slightly with different versions but in all cases the array should fit into memory many times over, but runs out when compiling. Also, in the cases where the compilation does succeed in the closed over case, the compilation time is 1050x longer than in the nonclosed over case, which also seems excessive."
15258,"以下是一个github上的jax下的一个issue, 标题是(LLVM worker thread limit)， 内容是 ( Description Recently we have been seeing a lot of crashed jobs on our cluster, with the following error message: ``` LLVM ERROR: pthread_create failed: Resource temporarily unavailable ``` caused by the jobs hitting the thread limit per user per node (set to 4096). Further investigation revealed that this is caused by jax spawning hundreds/thousands of llvmworker threads, and never exiting them. This happens whenever we run a script which contains many compilations, and a certain subset of jax primitives, e.g. qr decomposition. Here is a minimal working example to reproduce the aforementioned behaviour: ```python import jax import jax.numpy as jnp from functools import partial (jax.jit, static_argnums=0) def expensive_computation(f, x):     x = f(x)     q, r = jnp.linalg.qr(x)     return q.sum()+r.sum() x = jnp.ones((16,16)) i =1 while True:     i = i+1     if i%1000 ==0: print(i)     f = lambda x: x trigger recompilation     u = jax.block_until_ready(expensive_computation(f, x)) ``` After a few thousand iterations, ```top Hp $(pgrep python3)``` shows ~6100 llvmworker threads which have been spawned: ``` top  12:04:00 up  3:05,  3 users,  load average: 1.04, 0.52, 0.26 Threads: 6178 total,   1 running, 6177 sleeping,   0 stopped,   0 zombie %Cpu(s): 33.7 us,  1.0 sy,  0.0 ni, 65.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st MiB Mem :  60000.0 total,  56564.2 free,   3173.1 used,    262.8 buff/cache MiB Swap:      0.0 total,      0.0 free,      0.0 used.  56826.9 avail Mem      PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                                                              507363 clemens   20   0   52.8g   2.9g 103472 R  99.0   5.0   3:05.13 python3                                                                                              507364 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.06 python3                                                                                              507365 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.07 python3                                                                                              507367 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507368 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507369 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507370 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507371 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507372 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507373 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507374 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507375 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507376 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507377 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507378 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507379 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507380 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507381 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507382 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507383 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507384 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507385 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507386 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507387 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507388 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507390 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.02 python3                                                                                              507391 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.06 python3                                                                                              507392 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.07 python3                                                                                              521501 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521502 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521503 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521507 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521508 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521509 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521513 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521514 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521515 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521519 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521520 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521521 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521525 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521526 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521527 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521531 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521532 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521533 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521537 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521538 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521539 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521543 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521544 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521545 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521549 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521550 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521551 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521555 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521556 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521557 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521561 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521562 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521563 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521567 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521568 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521569 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521573 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521574 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521575 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521579 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521580 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521581 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521585 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521586 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521587 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2 ...                             ``` (this was run locally, in an lxc container limited to 3 cpu cores) A few more remarks:  if the `jnp.linalg.qr` decomposition is replaced with some normal jax function, e.g. `jax.lax.sin` the issue does not happen  as can be seen from the thread names (repeated indices), they are not all spawned by the same `llvm::ThreadPool`, but somewhere else (I am not familiar enough with XLA to find where) Context:  possibly related to https://github.com/google/jax/issues/15819 and https://github.com/google/jax/issues/16215  this issue is different from https://github.com/google/jax/issues/1539 and https://github.com/google/jax/issues/15866  , as we are not worried about the number of threads jax is using for doing the acutal computations (which can be limited via cpu pinning e.g. taskset) but rather about the compilation threads Let me know if this is the right place or if I should open an issue on https://github.com/openxla/xla instead.  What jax/jaxlib version are you using? jax 0.4.11, jaxlib 0.4.11  Which accelerator(s) are you using? CPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,LLVM worker thread limit," Description Recently we have been seeing a lot of crashed jobs on our cluster, with the following error message: ``` LLVM ERROR: pthread_create failed: Resource temporarily unavailable ``` caused by the jobs hitting the thread limit per user per node (set to 4096). Further investigation revealed that this is caused by jax spawning hundreds/thousands of llvmworker threads, and never exiting them. This happens whenever we run a script which contains many compilations, and a certain subset of jax primitives, e.g. qr decomposition. Here is a minimal working example to reproduce the aforementioned behaviour: ```python import jax import jax.numpy as jnp from functools import partial (jax.jit, static_argnums=0) def expensive_computation(f, x):     x = f(x)     q, r = jnp.linalg.qr(x)     return q.sum()+r.sum() x = jnp.ones((16,16)) i =1 while True:     i = i+1     if i%1000 ==0: print(i)     f = lambda x: x trigger recompilation     u = jax.block_until_ready(expensive_computation(f, x)) ``` After a few thousand iterations, ```top Hp $(pgrep python3)``` shows ~6100 llvmworker threads which have been spawned: ``` top  12:04:00 up  3:05,  3 users,  load average: 1.04, 0.52, 0.26 Threads: 6178 total,   1 running, 6177 sleeping,   0 stopped,   0 zombie %Cpu(s): 33.7 us,  1.0 sy,  0.0 ni, 65.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st MiB Mem :  60000.0 total,  56564.2 free,   3173.1 used,    262.8 buff/cache MiB Swap:      0.0 total,      0.0 free,      0.0 used.  56826.9 avail Mem      PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                                                              507363 clemens   20   0   52.8g   2.9g 103472 R  99.0   5.0   3:05.13 python3                                                                                              507364 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.06 python3                                                                                              507365 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.07 python3                                                                                              507367 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507368 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507369 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507370 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507371 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507372 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507373 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507374 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507375 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507376 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507377 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507378 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507379 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507380 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507381 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507382 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507383 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507384 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507385 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507386 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507387 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507388 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 python3                                                                                              507390 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.02 python3                                                                                              507391 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.06 python3                                                                                              507392 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.07 python3                                                                                              521501 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521502 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521503 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521507 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521508 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521509 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521513 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521514 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521515 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521519 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521520 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521521 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521525 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521526 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521527 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521531 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521532 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521533 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521537 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521538 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521539 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521543 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521544 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521545 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521549 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521550 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521551 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521555 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521556 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521557 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521561 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521562 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521563 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521567 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521568 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521569 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521573 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521574 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521575 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521579 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521580 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521581 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2                                                                                        521585 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker0                                                                                        521586 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker1                                                                                        521587 clemens   20   0   52.8g   2.9g 103472 S   0.0   5.0   0:00.00 llvmworker2 ...                             ``` (this was run locally, in an lxc container limited to 3 cpu cores) A few more remarks:  if the `jnp.linalg.qr` decomposition is replaced with some normal jax function, e.g. `jax.lax.sin` the issue does not happen  as can be seen from the thread names (repeated indices), they are not all spawned by the same `llvm::ThreadPool`, but somewhere else (I am not familiar enough with XLA to find where) Context:  possibly related to https://github.com/google/jax/issues/15819 and https://github.com/google/jax/issues/16215  this issue is different from https://github.com/google/jax/issues/1539 and https://github.com/google/jax/issues/15866  , as we are not worried about the number of threads jax is using for doing the acutal computations (which can be limited via cpu pinning e.g. taskset) but rather about the compilation threads Let me know if this is the right place or if I should open an issue on https://github.com/openxla/xla instead.  What jax/jaxlib version are you using? jax 0.4.11, jaxlib 0.4.11  Which accelerator(s) are you using? CPU",2023-06-06T13:43:14Z,bug,closed,1,2,https://github.com/jax-ml/jax/issues/16272,"Thanks for the simple reproduction. It made debugging easy. It appears we were keeping a small MLIRowned threadpool alive accidentally. CC(Disable threading in MLIR contexts.) disables the MLIR threading support, which we don't really care about anyway, and with that change the number of threads no longer grows with your test case. Hope that helps!",I can confirm that the fix seems to work. Thanks!
588,"以下是一个github上的jax下的一个issue, 标题是(Improve the logging mechanism for `XlaCallModule` op lowering)， 内容是 (Improve the logging mechanism for `XlaCallModule` op lowering * Use `mlir::StatusScopedDiagnosticHandler` to propagate error messages from MLIR to `absl::Status`. * Use `applyTensorflowAndCLOptions` to set up crash reproducer. * Use `DumpMlirOpToFile` to dump MLIR modules to `TF_DUMP_GRAPH_PREFIX` as files, which makes it easier to copy & paste.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Improve the logging mechanism for `XlaCallModule` op lowering,"Improve the logging mechanism for `XlaCallModule` op lowering * Use `mlir::StatusScopedDiagnosticHandler` to propagate error messages from MLIR to `absl::Status`. * Use `applyTensorflowAndCLOptions` to set up crash reproducer. * Use `DumpMlirOpToFile` to dump MLIR modules to `TF_DUMP_GRAPH_PREFIX` as files, which makes it easier to copy & paste.",2023-06-04T18:30:39Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16251,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
718,"以下是一个github上的jax下的一个issue, 标题是(shard_map replication rule for `shard_map conv_general_dilated` not implemented)， 内容是 (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Trying to use a model that uses `conv_general_dilated` raises the following exception: ``` UnfilteredStackTrace: NotImplementedError: No replication rule for conv_general_dilated. As a workaround, pass the `check_rep=False` argument to `shard_map`. To get this fixed, open an issue at https://github.com/google/jax/issues ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,shard_map replication rule for `shard_map conv_general_dilated` not implemented,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Trying to use a model that uses `conv_general_dilated` raises the following exception: ``` UnfilteredStackTrace: NotImplementedError: No replication rule for conv_general_dilated. As a workaround, pass the `check_rep=False` argument to `shard_map`. To get this fixed, open an issue at https://github.com/google/jax/issues ```",2023-06-04T03:26:14Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/16249,We fixed this in CC([shardmap] add conv replication rules)!
388,"以下是一个github上的jax下的一个issue, 标题是(Update `type` output of arrays to be real)， 内容是 (Update `type` output of arrays to be real `type(a_jax_array)` does not return `jax.Array`, it returns `tensorflow.compiler.xla.python.xla_extension.ArrayImpl` instead.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update `type` output of arrays to be real,"Update `type` output of arrays to be real `type(a_jax_array)` does not return `jax.Array`, it returns `tensorflow.compiler.xla.python.xla_extension.ArrayImpl` instead.",2023-06-02T16:27:42Z,,closed,0,2,https://github.com/jax-ml/jax/issues/16237,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Closing Copybara created PR due to inactivity
1065,"以下是一个github上的jax下的一个issue, 标题是(Wrong definition of interval length in zoom linesearch)， 内容是 ( Description My issue is in the definition of the interval length in the zoom linesearch in jax/_src/scipy/optimize/line_search.py on line 113.  The interpolation method aims at finding a point `a_j` lying in the interval with boundary points `a_lo`, `a_hi`, that is `[a_lo, a_hi]` or `[a_hi, a_lo]` depending on their order, as stated in Algorithm 3.6 of Wright and Nocedal, 'Numerical Optimization', 1999. A priori nothing prevents the algorithm as stated by Wright and Nocedal to have `a_hi  a + cchk) & (a_j_cubic  b`, that is we may select a point outside the desired interval.  Note that the second issue (wrong checks for cubic and quadratic) seems to be also present in Scipy's original code line 544, line 562 and line 564. This issue can simply be fixed by defining `dalpha = jnp.abs(state.a_hi  state.a_lo)` on line 113.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Wrong definition of interval length in zoom linesearch," Description My issue is in the definition of the interval length in the zoom linesearch in jax/_src/scipy/optimize/line_search.py on line 113.  The interpolation method aims at finding a point `a_j` lying in the interval with boundary points `a_lo`, `a_hi`, that is `[a_lo, a_hi]` or `[a_hi, a_lo]` depending on their order, as stated in Algorithm 3.6 of Wright and Nocedal, 'Numerical Optimization', 1999. A priori nothing prevents the algorithm as stated by Wright and Nocedal to have `a_hi  a + cchk) & (a_j_cubic  b`, that is we may select a point outside the desired interval.  Note that the second issue (wrong checks for cubic and quadratic) seems to be also present in Scipy's original code line 544, line 562 and line 564. This issue can simply be fixed by defining `dalpha = jnp.abs(state.a_hi  state.a_lo)` on line 113.",2023-06-02T16:23:31Z,bug,open,0,7,https://github.com/jax-ml/jax/issues/16236,"Brilliant and detailed report, thank you!  are you available to take a look at this, since IIUC you have some context on the code? If not, I can take a look.","Sorry, I have not had the time to dive into line search. We've discussed this a few times, but I think the best longterm solution for JAX is probably to drop `jax.scipy.optimize` entirely and recommend JAXopt instead. At this point I believe JAXopt has a strict superset of optimization algorithms in JAX, including its own line search. CC   ","That sounds like a good idea for the long term, though perhaps we can also fix this bug in the near term, since  has basically told us the oneline fix. Concretely I think we just need someone to grok and check vroulet's analysis here, then apply the prescribed fix.","I'm actually working on the zoom linesearch in JAXOpt to reformat it as the other linesearches.  Once done, I can run the associated tests in JAXOpt to check the resulting algorithm.  I believe the oneline fix is sufficient.  who adapted the code from Scipy may confirm.",Note that the zoom linesearch in JAXopt supports pytrees while the one in JAX doesn't.,"Nice find. Let me verify, though I have no doubt in your finding.  P.s. It's funny how very small yet significant things like this can persist in code. Seen it many times in ""hardened"" optimisation code. ","For the record, below is a minimal example, where the current implementation fails while the scipy implementation succeeds. For the longterm solution (using JAXOpt) I recoded the zoom_linesearch in JAXOpt and attached the corresponding PR. Once merged, you may drop jax.scipy.optimize for the long term but the oneline fix may be a simple patch for the short term.  By the way, the `a_rec` variable in the current implementation is not well set in one case (when `a_hi` is set to `a_lo`, and `a_lo` is set to `a_j`, `a_rec` should be set to `a_hi` not `a_lo` as currently implemented). But this only affects the cubic interpolation and won't create any bugs (it may just fail to select the cubic interpolation at times where it could have used it).  I corrected it in the new implementation in JAXOpt.  ``` import jax from jax._src.scipy.optimize.line_search import line_search import scipy.optimize def fun(x):     return x**2  Descent direction p chosen such that, with x+p   the first trial of the algorithm,   1. p>0 (valid descent direction)  2. x+p satisifies sufficient decrease  3. x+p does not satisfy small curvature  4. f'(x+p) > 0  As a result, the first trial starts with high < low  The jax implementation fails while scipy's  implementation works. x = 1. p = 1.95*x   res_jax = line_search(fun, x, p) print(res_jax.failed)   True res_scipy = scipy.optimize.line_search(fun, jax.grad(fun), x, p) print(res_scipy[0] == None)  False, None would mean failure ```"
776,"以下是一个github上的jax下的一个issue, 标题是(`jax.jit(..., static_argnums=...)` keeps strong (not weak) reference in cache, causing memory leak)， 内容是 ( Description ```python import gc import weakref import jax class A: 	pass def f(x, y): 	return x f = jax.jit(f, static_argnums=1) f(1, A) ref = weakref.ref(A) del A gc.collect() print(ref)    ``` Commenting out the `f(1, A)` line instead produces ``. This usecase turns up in tests, when creating and destroying many JIT'd functions.  What jax/jaxlib version are you using? 4.11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"`jax.jit(..., static_argnums=...)` keeps strong (not weak) reference in cache, causing memory leak"," Description ```python import gc import weakref import jax class A: 	pass def f(x, y): 	return x f = jax.jit(f, static_argnums=1) f(1, A) ref = weakref.ref(A) del A gc.collect() print(ref)    ``` Commenting out the `f(1, A)` line instead produces ``. This usecase turns up in tests, when creating and destroying many JIT'd functions.  What jax/jaxlib version are you using? 4.11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-01T21:09:51Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/16226,"It _could_ be argued that this is working as intended: caching *in general* is based on static _values_, not static object identities. That's what lets us get a cache hit on evaluating the expression `jit(f, static_argnums=0)(n())` twice where `n = lambda: 256 + (lambda: 1)()` (I had to embellish that to defeat some CPython optimizations). With caching based on values, in general whether an object with a particular value goes out of scope doesn't matter, since we can usually construct another object with the same value and thus the cache entry isn't dead. (Cache entries are dead, and hence ""leaked"", if they become unreachable.) This particular example doesn't fit neatly into that mold because `A` inherits the `type` hashable definition, where equality is based on object id. That is, after deleting `A` it's _hard_ to construct another object with the same value and thus hard to get a cache hit. It's hard, but technically not impossible: Python can reuse object ids. So, _technically_ the cache entry is not totally dead; it's just hard to reach, and perhaps impossible to reach deterministically without using a C extension. (This is probably a good reason not to put equalitybyobjectid instances into cache keys if the cache can outlive the object instances, as I think CPython really just checks pointer value and not anything else.) Anyway, pedantry aside, I agree this is probably an undesirable behavior for objects where equality is based on object id, and we may want to revise it. But (presumably) we don't want to break the behavior for other objects, like (noninterned) ints or strings or tuples or frozen dataclasses, where equal values can be reconstructed (and hence cache entries hit) even when the original object goes out of scope. How do we tell the difference? I'm not sure how to do it in a complete and sound way. One heuristic might be to check for each static argument `x` whether `x.__eq__ is object.__eq__`, and if so only hold a weak reference.  WDYT?","> One heuristic might be to check for each static argument x whether `x.__eq__ is object.__eq__`, and if so only hold a weak reference. Agreed  I think this something like this is probably a reasonable heuristic, and that a heuristic is probably as good as it gets. Two other important heuristic cases that come to mind:  If `type(x) is types.FunctionType`. In practice I *think* it's possible to create equal nonidentical functions (equal `__code__` etc.) but in practice that's weird and functions are my most common example of a static argument with this problem.  Nested examples of the above, e.g. `(,)`. (Or I suppose any pytree, with at least one heuristicmatching leaf.)"
7317,"以下是一个github上的jax下的一个issue, 标题是(ImportError: cannot import name 'xla_call_p' from 'jax.interpreters.xla' )， 内容是 ( Description **When i ran the below code,it came a import error** ```python import scanpy as sc import numpy as np import matplotlib.pyplot as plt import matplotlib as mpl import cell2location from matplotlib import rcParams rcParams['pdf.fonttype'] = 42  enables correct plotting of text for PDFs ``` ``` Global seed set to 0 ``` ```pytb  ImportError                               Traceback (most recent call last) Cell In[2], line 6       3 import matplotlib.pyplot as plt       4 import matplotlib as mpl > 6 import cell2location       8 from matplotlib import rcParams       9 rcParams['pdf.fonttype'] = 42  enables correct plotting of text for PDFs File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/cell2location/__init__.py:9       6 from rich.logging import RichHandler       7 from torch.distributions import biject_to, transform_to > 9 from . import models      10 from .run_colocation import run_colocation      12  https://github.com/pythonpoetry/poetry/pull/2366issuecomment652418094      13  https://github.com/pythonpoetry/poetry/issues/144issuecomment623927302 File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/cell2location/models/__init__.py:1 > 1 from ._cell2location_model import Cell2location       2 from ._cell2location_module import (       3     LocationModelLinearDependentWMultiExperimentLocationBackgroundNormLevelGeneAlphaPyroModel,       4 )       5 from ._cell2location_WTA_model import Cell2location_WTA File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/cell2location/models/_cell2location_model.py:11       9 from pyro.infer import Trace_ELBO, TraceEnum_ELBO      10 from pyro.nn import PyroModule > 11 from scvi import REGISTRY_KEYS      12 from scvi.data import AnnDataManager      13 from scvi.data.fields import (      14     CategoricalJointObsField,      15     CategoricalObsField,    (...)      18     NumericalObsField,      19 ) File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/__init__.py:10       7 from ._settings import settings       9  this import needs to come after prior imports to prevent circular import > 10 from . import autotune, data, model, external, utils      12 from importlib.metadata import version      14 package_name = ""scvitools"" File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/autotune/__init__.py:1 > 1 from ._manager import TuneAnalysis, TunerManager       2 from ._tuner import ModelTuner       3 from ._types import Tunable, TunableMixin File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/autotune/_manager.py:22      20 from scvi._types import AnnOrMuData      21 from scvi.data._constants import _SETUP_ARGS_KEY, _SETUP_METHOD_NAME > 22 from scvi.model.base import BaseModelClass      23 from scvi.utils import InvalidParameterError      25 from ._defaults import COLORS, COLUMN_KWARGS, DEFAULTS, TUNABLE_TYPES File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/model/__init__.py:2       1 from . import utils > 2 from ._amortizedlda import AmortizedLDA       3 from ._autozi import AUTOZI       4 from ._condscvi import CondSCVI File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/model/_amortizedlda.py:14      12 from scvi.data import AnnDataManager      13 from scvi.data.fields import LayerField > 14 from scvi.module import AmortizedLDAPyroModule      15 from scvi.utils import setup_anndata_dsp      17 from .base import BaseModelClass, PyroSviTrainMixin File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/module/__init__.py:1 > 1 from ._amortizedlda import AmortizedLDAPyroModule       2 from ._autozivae import AutoZIVAE       3 from ._classifier import Classifier File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/module/_amortizedlda.py:14      12 from scvi._constants import REGISTRY_KEYS      13 from scvi.autotune._types import Tunable > 14 from scvi.module.base import PyroBaseModuleClass, auto_move_data      15 from scvi.nn import Encoder      17 _AMORTIZED_LDA_PYRO_MODULE_NAME = ""amortized_lda"" File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/module/base/__init__.py:1 > 1 from ._base_module import (       2     BaseMinifiedModeModuleClass,       3     BaseModuleClass,       4     JaxBaseModuleClass,       5     LossOutput,       6     PyroBaseModuleClass,       7     TrainStateWithState,       8 )       9 from ._decorators import auto_move_data, flax_configure      11 __all__ = [      12     ""BaseModuleClass"",      13     ""LossOutput"",    (...)      19     ""BaseMinifiedModeModuleClass"",      20 ] File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/module/base/_base_module.py:18      16 from jax import random      17 from jaxlib.xla_extension import Device > 18 from numpyro.distributions import Distribution      19 from pyro.infer.predictive import Predictive      20 from torch import nn File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/numpyro/__init__.py:6       1  Copyright Contributors to the Pyro project.       2  SPDXLicenseIdentifier: Apache2.0       4 import logging > 6 from numpyro import compat, diagnostics, distributions, handlers, infer, ops, optim       7 from numpyro.distributions.distribution import enable_validation, validation_enabled       8 from numpyro.infer.inspect import render_model File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/numpyro/infer/__init__.py:5       1  Copyright Contributors to the Pyro project.       2  SPDXLicenseIdentifier: Apache2.0       4 from numpyro.infer.barker import BarkerMH > 5 from numpyro.infer.elbo import (       6     ELBO,       7     RenyiELBO,       8     Trace_ELBO,       9     TraceEnum_ELBO,      10     TraceGraph_ELBO,      11     TraceMeanField_ELBO,      12 )      13 from numpyro.infer.hmc import HMC, NUTS      14 from numpyro.infer.hmc_gibbs import HMCECS, DiscreteHMCGibbs, HMCGibbs File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/numpyro/infer/elbo.py:24      17 from numpyro.handlers import replay, seed, substitute, trace      18 from numpyro.infer.util import (      19     _without_rsample_stop_gradient,      20     get_importance_trace,      21     is_identically_one,      22     log_density,      23 ) > 24 from numpyro.ops.provenance import eval_provenance      25 from numpyro.util import _validate_model, check_model_guide_match, find_stack_level      28 class ELBO: File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/numpyro/ops/provenance.py:10       8 from jax.interpreters.partial_eval import trace_to_jaxpr_dynamic       9 from jax.interpreters.pxla import xla_pmap_p > 10 from jax.interpreters.xla import xla_call_p      11 import jax.linear_util as lu      12 import jax.numpy as jnp **ImportError: cannot import name 'xla_call_p' from 'jax.interpreters.xla'**  ``` Is this an error caused by the latest version v0.4.11? Thanks a lot!  What jax/jaxlib version are you using? jax v0.4.11;jaxlib v0.4.11  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,ImportError: cannot import name 'xla_call_p' from 'jax.interpreters.xla' ," Description **When i ran the below code,it came a import error** ```python import scanpy as sc import numpy as np import matplotlib.pyplot as plt import matplotlib as mpl import cell2location from matplotlib import rcParams rcParams['pdf.fonttype'] = 42  enables correct plotting of text for PDFs ``` ``` Global seed set to 0 ``` ```pytb  ImportError                               Traceback (most recent call last) Cell In[2], line 6       3 import matplotlib.pyplot as plt       4 import matplotlib as mpl > 6 import cell2location       8 from matplotlib import rcParams       9 rcParams['pdf.fonttype'] = 42  enables correct plotting of text for PDFs File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/cell2location/__init__.py:9       6 from rich.logging import RichHandler       7 from torch.distributions import biject_to, transform_to > 9 from . import models      10 from .run_colocation import run_colocation      12  https://github.com/pythonpoetry/poetry/pull/2366issuecomment652418094      13  https://github.com/pythonpoetry/poetry/issues/144issuecomment623927302 File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/cell2location/models/__init__.py:1 > 1 from ._cell2location_model import Cell2location       2 from ._cell2location_module import (       3     LocationModelLinearDependentWMultiExperimentLocationBackgroundNormLevelGeneAlphaPyroModel,       4 )       5 from ._cell2location_WTA_model import Cell2location_WTA File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/cell2location/models/_cell2location_model.py:11       9 from pyro.infer import Trace_ELBO, TraceEnum_ELBO      10 from pyro.nn import PyroModule > 11 from scvi import REGISTRY_KEYS      12 from scvi.data import AnnDataManager      13 from scvi.data.fields import (      14     CategoricalJointObsField,      15     CategoricalObsField,    (...)      18     NumericalObsField,      19 ) File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/__init__.py:10       7 from ._settings import settings       9  this import needs to come after prior imports to prevent circular import > 10 from . import autotune, data, model, external, utils      12 from importlib.metadata import version      14 package_name = ""scvitools"" File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/autotune/__init__.py:1 > 1 from ._manager import TuneAnalysis, TunerManager       2 from ._tuner import ModelTuner       3 from ._types import Tunable, TunableMixin File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/autotune/_manager.py:22      20 from scvi._types import AnnOrMuData      21 from scvi.data._constants import _SETUP_ARGS_KEY, _SETUP_METHOD_NAME > 22 from scvi.model.base import BaseModelClass      23 from scvi.utils import InvalidParameterError      25 from ._defaults import COLORS, COLUMN_KWARGS, DEFAULTS, TUNABLE_TYPES File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/model/__init__.py:2       1 from . import utils > 2 from ._amortizedlda import AmortizedLDA       3 from ._autozi import AUTOZI       4 from ._condscvi import CondSCVI File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/model/_amortizedlda.py:14      12 from scvi.data import AnnDataManager      13 from scvi.data.fields import LayerField > 14 from scvi.module import AmortizedLDAPyroModule      15 from scvi.utils import setup_anndata_dsp      17 from .base import BaseModelClass, PyroSviTrainMixin File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/module/__init__.py:1 > 1 from ._amortizedlda import AmortizedLDAPyroModule       2 from ._autozivae import AutoZIVAE       3 from ._classifier import Classifier File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/module/_amortizedlda.py:14      12 from scvi._constants import REGISTRY_KEYS      13 from scvi.autotune._types import Tunable > 14 from scvi.module.base import PyroBaseModuleClass, auto_move_data      15 from scvi.nn import Encoder      17 _AMORTIZED_LDA_PYRO_MODULE_NAME = ""amortized_lda"" File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/module/base/__init__.py:1 > 1 from ._base_module import (       2     BaseMinifiedModeModuleClass,       3     BaseModuleClass,       4     JaxBaseModuleClass,       5     LossOutput,       6     PyroBaseModuleClass,       7     TrainStateWithState,       8 )       9 from ._decorators import auto_move_data, flax_configure      11 __all__ = [      12     ""BaseModuleClass"",      13     ""LossOutput"",    (...)      19     ""BaseMinifiedModeModuleClass"",      20 ] File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/scvi/module/base/_base_module.py:18      16 from jax import random      17 from jaxlib.xla_extension import Device > 18 from numpyro.distributions import Distribution      19 from pyro.infer.predictive import Predictive      20 from torch import nn File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/numpyro/__init__.py:6       1  Copyright Contributors to the Pyro project.       2  SPDXLicenseIdentifier: Apache2.0       4 import logging > 6 from numpyro import compat, diagnostics, distributions, handlers, infer, ops, optim       7 from numpyro.distributions.distribution import enable_validation, validation_enabled       8 from numpyro.infer.inspect import render_model File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/numpyro/infer/__init__.py:5       1  Copyright Contributors to the Pyro project.       2  SPDXLicenseIdentifier: Apache2.0       4 from numpyro.infer.barker import BarkerMH > 5 from numpyro.infer.elbo import (       6     ELBO,       7     RenyiELBO,       8     Trace_ELBO,       9     TraceEnum_ELBO,      10     TraceGraph_ELBO,      11     TraceMeanField_ELBO,      12 )      13 from numpyro.infer.hmc import HMC, NUTS      14 from numpyro.infer.hmc_gibbs import HMCECS, DiscreteHMCGibbs, HMCGibbs File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/numpyro/infer/elbo.py:24      17 from numpyro.handlers import replay, seed, substitute, trace      18 from numpyro.infer.util import (      19     _without_rsample_stop_gradient,      20     get_importance_trace,      21     is_identically_one,      22     log_density,      23 ) > 24 from numpyro.ops.provenance import eval_provenance      25 from numpyro.util import _validate_model, check_model_guide_match, find_stack_level      28 class ELBO: File ~/miniconda3/envs/cell2loc_env/lib/python3.9/sitepackages/numpyro/ops/provenance.py:10       8 from jax.interpreters.partial_eval import trace_to_jaxpr_dynamic       9 from jax.interpreters.pxla import xla_pmap_p > 10 from jax.interpreters.xla import xla_call_p      11 import jax.linear_util as lu      12 import jax.numpy as jnp **ImportError: cannot import name 'xla_call_p' from 'jax.interpreters.xla'**  ``` Is this an error caused by the latest version v0.4.11? Thanks a lot!  What jax/jaxlib version are you using? jax v0.4.11;jaxlib v0.4.11  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_",2023-06-01T11:59:57Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16214,"Hi  this is expected. `xla_call_p` has been deprecated since JAX v0.4.4, and was removed in JAX version 0.4.11. See the Change Log for more information. If you have code that still uses `xla_call_p`, you can install JAX version 0.4.10 or older, but I'd suggest updating your code if possible.","By the way, it looks like numpyro has already fixed the issue here: https://github.com/pyroppl/numpyro/pull/1595"
230,"以下是一个github上的jax下的一个issue, 标题是(custom prng: add shard methods to PRNGKeyArrayImpl)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,custom prng: add shard methods to PRNGKeyArrayImpl,,2023-06-01T10:52:12Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16213
1051,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Improve compile-time shape checking. )， 内容是 (JAX shape polymorphism relies on implicit assumptions. For example, when tracing with input specification `(a, a)`, we assume that the first two dimensions have the same size greater or equal to 1. Here we extend the checking that these assumptions hold. When we call an `Exported` module from jax, with `jax_export.call_exported` we check these assumptions statically. However, when we stage an `Exported` using `XlaCallModule` to be called from TensorFlow, or when we use TF graph serialization we need to check these assumptions when we execute and compile the op (that is when the shapes are available). To prepare for this compiletime shape checking we add `Exported.shape_check_module` to produce a serialized MLIR module containing the shape checking code. This will be added in a future change to `XlaCallModule`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Improve compile-time shape checking. ,"JAX shape polymorphism relies on implicit assumptions. For example, when tracing with input specification `(a, a)`, we assume that the first two dimensions have the same size greater or equal to 1. Here we extend the checking that these assumptions hold. When we call an `Exported` module from jax, with `jax_export.call_exported` we check these assumptions statically. However, when we stage an `Exported` using `XlaCallModule` to be called from TensorFlow, or when we use TF graph serialization we need to check these assumptions when we execute and compile the op (that is when the shapes are available). To prepare for this compiletime shape checking we add `Exported.shape_check_module` to produce a serialized MLIR module containing the shape checking code. This will be added in a future change to `XlaCallModule`.",2023-06-01T10:13:40Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16211, PTAL
593,"以下是一个github上的jax下的一个issue, 标题是(Some cleanup of logging in XlaCallModule.)， 内容是 (Some cleanup of logging in XlaCallModule. Previously, we were using XLA_VLOG_LINES to ensure that the whole module is logged even when large. This had the unpleasant sideeffect that each line of the module was logged separately, including the filename and line number. This made the log large and prevented copypasting the module. Now we use `if (VLOG_IS_ON()) LogModule`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Some cleanup of logging in XlaCallModule.,"Some cleanup of logging in XlaCallModule. Previously, we were using XLA_VLOG_LINES to ensure that the whole module is logged even when large. This had the unpleasant sideeffect that each line of the module was logged separately, including the filename and line number. This made the log large and prevented copypasting the module. Now we use `if (VLOG_IS_ON()) LogModule`.",2023-06-01T09:22:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16210
8560,"以下是一个github上的jax下的一个issue, 标题是(Calling a pjitted function repeatedly with the same inputs errors in multiprocessing execution)， 内容是 ( Description Hi jax team! This issue came up when I was trying to benchmark jax inference where I would pjit a few different model calls and call the function multiple times with the same input in a for loop, but I would get an error only when multiprocessing was enabled. I can reproduce this in this nightly container:  `ghcr.io/nvidia/t5x:nightly20230522` But I've also been able to reproduce it in another older nightly container. Everything runs fine when I run it like this: ```bash CUDA_VISIBLE_DEVICES=0,1 python repro.py ``` But it fails with ``` ValueError: INVALID_ARGUMENT: Executable expected shape f32[1,2,2]{2,1,0} for argument 2 but got incompatible shape f32[2,2,2]{2,1,0} ``` When run like this: ```bash NUM_PROCESSES=2 for i in $(seq 0 $((NUM_PROCESSES1))); do   NUM_PROCESSES=$NUM_PROCESSES CUDA_VISIBLE_DEVICES=$i PROCESS_ID=$i python repro.py &  done ``` I have noticed that if I remove these lines ```python     print(f'SECOND dummy_batch={jax.tree_map(jnp.shape,dummy_batch)}')     out = predict_fn(init_ts.params, dummy_batch)     print(f'SECOND out={jax.tree_map(jnp.shape,out)}') ``` the test succeeds, but I'm not sure why and it doesn't solve my original problem, which was running the pjitted function multiple times to obtain a benchmark. Here's `repro.py` ```python import jax import jax.numpy as jnp import os from t5x import partitioning, utils from t5x.models import BaseModel NUM_PROCESSES = int(os.getenv('NUM_PROCESSES', '1')) PROCESS_ID = int(os.getenv('PROCESS_ID', '0')) jax.distributed.initialize(         coordinator_address='localhost:12345',         num_processes=NUM_PROCESSES,         process_id=PROCESS_ID, ) from typing import Any, Callable, List, Optional, Sequence, Tuple, Union import jax.numpy as jnp import numpy as np from flax import linen as nn from flax.linen.dtypes import canonicalize_dtype, promote_dtype from flax.linen.initializers import lecun_normal, zeros from flax.linen.linear import _canonicalize_tuple, _conv_dimension_numbers, _normalize_axes, canonicalize_padding from flax.linen.module import Module, compact, merge_param   pylint: disable=gmultipleimport from flax.linen.partitioning import param_with_axes, variable_with_axes, with_sharding_constraint from jax import ShapedArray, eval_shape, lax, random PRNGKey = Any Array = Any Shape = Tuple[int, ...] Dtype = Any PrecisionLike = Union[None, str, lax.Precision, Tuple[str, str],                       Tuple[lax.Precision, lax.Precision]] PaddingLike = Union[str, int, Sequence[Union[int, Tuple[int, int]]]] Activation = Callable[..., Array]  Parameter initializers. Initializer = Callable[[PRNGKey, Shape, Dtype], Array] Axes = Union[int, Any] default_kernel_init = lecun_normal() default_embed_init = nn.initializers.variance_scaling(     1.0, 'fan_in', 'normal', out_axis=0) class Dense(Module):   """"""A linear transformation applied over the last dimension of the input.   Attributes:     features: the number of output features.     use_bias: whether to add a bias to the output (default: True).     dtype: the dtype of the computation (default: infer from input and params).     param_dtype: the dtype passed to parameter initializers (default: float32).     precision: numerical precision of the computation see `jax.lax.Precision`       for details.     kernel_init: initializer function for the weight matrix.     bias_init: initializer function for the bias.   """"""   features: int   use_bias: bool = True   dtype: Optional[Dtype] = None   param_dtype: Dtype = jnp.float32   precision: PrecisionLike = None   kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init   bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = zeros   kernel_axes: Tuple[str, ...] = ()   bias_axes: Tuple[str, ...] = ()      def __call__(self, inputs: Array) > Array:     """"""Applies a linear transformation to the inputs along the last dimension.     Args:       inputs: The ndarray to be transformed.     Returns:       The transformed input.     """"""     kernel = param_with_axes('kernel',                              self.kernel_init,                              (jnp.shape(inputs)[1], self.features),                              self.param_dtype,                              axes=self.kernel_axes)     if self.use_bias:       bias = param_with_axes('bias', self.bias_init, (self.features,),                              self.param_dtype,                              axes=self.bias_axes)     else:       bias = None     inputs, kernel, bias = promote_dtype(inputs, kernel, bias, dtype=self.dtype)     y = lax.dot_general(inputs, kernel,                         (((inputs.ndim  1,), (0,)), ((), ())),                         precision=self.precision)     if bias is not None:       y += jnp.reshape(bias, (1,) * (y.ndim  1) + (1,))     return y def restore_t5x_checkpoint_state(partitioner: partitioning.PjitPartitioner, t5x_model: BaseModel, input_shapes: dict, input_types: dict, ckpt_dir: str = None, from_scratch: bool = True):     train_state_initializer = utils.TrainStateInitializer(           optimizer_def=None,   Do not load optimizer state.           init_fn=t5x_model.get_initial_variables,           input_shapes=input_shapes,           input_types=input_types,           partitioner=partitioner)     train_state_axes = train_state_initializer.train_state_axes     if from_scratch:         init_ts = train_state_initializer.from_scratch(jax.random.PRNGKey(0))     else:         restore_cfg = utils.RestoreCheckpointConfig(ckpt_dir, mode='specific', dtype='float32')         init_ts = train_state_initializer.from_checkpoint([restore_cfg], init_rng=None)     return init_ts, train_state_axes def test_matmul(dense_feat, batch_size, inner_dim, outer_dim):     partitioner = partitioning.PjitPartitioner(         num_partitions=1,         model_parallel_submesh=None,         logical_axis_rules=partitioning.standard_logical_axis_rules()     )     class DenseModel(BaseModel):         def __init__(self):             self.module = Dense(features=dense_feat)         def get_initial_variables(self, rng, input_shapes, input_types):             return self.module.init(                 rng,                 jnp.ones(input_shapes['inputs'], dtype=input_types['inputs']),             )         def predict_batch_with_aux(self, params, batch, rng):             return self.module.apply(                 {'params': params},                 rngs=rng,                 inputs=batch['inputs'],             ), None         def loss_fn(self):             raise NotImplementedError         def score_batch(self, params, batch, return_intermediates: bool = False):             return NotImplementedError     model = DenseModel()     dummy_batch = {'inputs': jnp.ones((batch_size,inner_dim,outer_dim), dtype=jnp.float32)}     init_ts, train_state_axes = restore_t5x_checkpoint_state(         partitioner=partitioner,         t5x_model=model,         input_shapes=jax.tree_map(lambda x: x.shape, dummy_batch),         input_types=jax.tree_map(lambda x: x.dtype, dummy_batch),         from_scratch=True,     )     partitioned_predict_fn = partitioner.partition(model.predict_batch,                                                    in_axis_resources=(train_state_axes.params, partitioner.data_partition_spec),                                                    out_axis_resources=None)     predict_fn = lambda params, batch: jax.block_until_ready(partitioned_predict_fn(params, batch))     print(f'FIRST dummy_batch={jax.tree_map(jnp.shape,dummy_batch)}')     out = predict_fn(init_ts.params, dummy_batch)     print(f'FIRST out={jax.tree_map(jnp.shape,out)}')     print(f'SECOND dummy_batch={jax.tree_map(jnp.shape,dummy_batch)}')     out = predict_fn(init_ts.params, dummy_batch)     print(f'SECOND out={jax.tree_map(jnp.shape,out)}') import itertools for dense_feat, batch_size, inner_dim, outer_dim in itertools.product([2,16],[2,16],[2,16],[2,16]):     print((dense_feat, batch_size, inner_dim, outer_dim))     test_matmul(dense_feat=dense_feat, batch_size=batch_size, inner_dim=inner_dim, outer_dim=outer_dim) print('yay!') ``` FYI:    What jax/jaxlib version are you using? jax 0.4.9 (from source: 63d87c6c3df2de514a41f6f6aff5340dad12cc35)  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Ubuntu  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Calling a pjitted function repeatedly with the same inputs errors in multiprocessing execution," Description Hi jax team! This issue came up when I was trying to benchmark jax inference where I would pjit a few different model calls and call the function multiple times with the same input in a for loop, but I would get an error only when multiprocessing was enabled. I can reproduce this in this nightly container:  `ghcr.io/nvidia/t5x:nightly20230522` But I've also been able to reproduce it in another older nightly container. Everything runs fine when I run it like this: ```bash CUDA_VISIBLE_DEVICES=0,1 python repro.py ``` But it fails with ``` ValueError: INVALID_ARGUMENT: Executable expected shape f32[1,2,2]{2,1,0} for argument 2 but got incompatible shape f32[2,2,2]{2,1,0} ``` When run like this: ```bash NUM_PROCESSES=2 for i in $(seq 0 $((NUM_PROCESSES1))); do   NUM_PROCESSES=$NUM_PROCESSES CUDA_VISIBLE_DEVICES=$i PROCESS_ID=$i python repro.py &  done ``` I have noticed that if I remove these lines ```python     print(f'SECOND dummy_batch={jax.tree_map(jnp.shape,dummy_batch)}')     out = predict_fn(init_ts.params, dummy_batch)     print(f'SECOND out={jax.tree_map(jnp.shape,out)}') ``` the test succeeds, but I'm not sure why and it doesn't solve my original problem, which was running the pjitted function multiple times to obtain a benchmark. Here's `repro.py` ```python import jax import jax.numpy as jnp import os from t5x import partitioning, utils from t5x.models import BaseModel NUM_PROCESSES = int(os.getenv('NUM_PROCESSES', '1')) PROCESS_ID = int(os.getenv('PROCESS_ID', '0')) jax.distributed.initialize(         coordinator_address='localhost:12345',         num_processes=NUM_PROCESSES,         process_id=PROCESS_ID, ) from typing import Any, Callable, List, Optional, Sequence, Tuple, Union import jax.numpy as jnp import numpy as np from flax import linen as nn from flax.linen.dtypes import canonicalize_dtype, promote_dtype from flax.linen.initializers import lecun_normal, zeros from flax.linen.linear import _canonicalize_tuple, _conv_dimension_numbers, _normalize_axes, canonicalize_padding from flax.linen.module import Module, compact, merge_param   pylint: disable=gmultipleimport from flax.linen.partitioning import param_with_axes, variable_with_axes, with_sharding_constraint from jax import ShapedArray, eval_shape, lax, random PRNGKey = Any Array = Any Shape = Tuple[int, ...] Dtype = Any PrecisionLike = Union[None, str, lax.Precision, Tuple[str, str],                       Tuple[lax.Precision, lax.Precision]] PaddingLike = Union[str, int, Sequence[Union[int, Tuple[int, int]]]] Activation = Callable[..., Array]  Parameter initializers. Initializer = Callable[[PRNGKey, Shape, Dtype], Array] Axes = Union[int, Any] default_kernel_init = lecun_normal() default_embed_init = nn.initializers.variance_scaling(     1.0, 'fan_in', 'normal', out_axis=0) class Dense(Module):   """"""A linear transformation applied over the last dimension of the input.   Attributes:     features: the number of output features.     use_bias: whether to add a bias to the output (default: True).     dtype: the dtype of the computation (default: infer from input and params).     param_dtype: the dtype passed to parameter initializers (default: float32).     precision: numerical precision of the computation see `jax.lax.Precision`       for details.     kernel_init: initializer function for the weight matrix.     bias_init: initializer function for the bias.   """"""   features: int   use_bias: bool = True   dtype: Optional[Dtype] = None   param_dtype: Dtype = jnp.float32   precision: PrecisionLike = None   kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init   bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = zeros   kernel_axes: Tuple[str, ...] = ()   bias_axes: Tuple[str, ...] = ()      def __call__(self, inputs: Array) > Array:     """"""Applies a linear transformation to the inputs along the last dimension.     Args:       inputs: The ndarray to be transformed.     Returns:       The transformed input.     """"""     kernel = param_with_axes('kernel',                              self.kernel_init,                              (jnp.shape(inputs)[1], self.features),                              self.param_dtype,                              axes=self.kernel_axes)     if self.use_bias:       bias = param_with_axes('bias', self.bias_init, (self.features,),                              self.param_dtype,                              axes=self.bias_axes)     else:       bias = None     inputs, kernel, bias = promote_dtype(inputs, kernel, bias, dtype=self.dtype)     y = lax.dot_general(inputs, kernel,                         (((inputs.ndim  1,), (0,)), ((), ())),                         precision=self.precision)     if bias is not None:       y += jnp.reshape(bias, (1,) * (y.ndim  1) + (1,))     return y def restore_t5x_checkpoint_state(partitioner: partitioning.PjitPartitioner, t5x_model: BaseModel, input_shapes: dict, input_types: dict, ckpt_dir: str = None, from_scratch: bool = True):     train_state_initializer = utils.TrainStateInitializer(           optimizer_def=None,   Do not load optimizer state.           init_fn=t5x_model.get_initial_variables,           input_shapes=input_shapes,           input_types=input_types,           partitioner=partitioner)     train_state_axes = train_state_initializer.train_state_axes     if from_scratch:         init_ts = train_state_initializer.from_scratch(jax.random.PRNGKey(0))     else:         restore_cfg = utils.RestoreCheckpointConfig(ckpt_dir, mode='specific', dtype='float32')         init_ts = train_state_initializer.from_checkpoint([restore_cfg], init_rng=None)     return init_ts, train_state_axes def test_matmul(dense_feat, batch_size, inner_dim, outer_dim):     partitioner = partitioning.PjitPartitioner(         num_partitions=1,         model_parallel_submesh=None,         logical_axis_rules=partitioning.standard_logical_axis_rules()     )     class DenseModel(BaseModel):         def __init__(self):             self.module = Dense(features=dense_feat)         def get_initial_variables(self, rng, input_shapes, input_types):             return self.module.init(                 rng,                 jnp.ones(input_shapes['inputs'], dtype=input_types['inputs']),             )         def predict_batch_with_aux(self, params, batch, rng):             return self.module.apply(                 {'params': params},                 rngs=rng,                 inputs=batch['inputs'],             ), None         def loss_fn(self):             raise NotImplementedError         def score_batch(self, params, batch, return_intermediates: bool = False):             return NotImplementedError     model = DenseModel()     dummy_batch = {'inputs': jnp.ones((batch_size,inner_dim,outer_dim), dtype=jnp.float32)}     init_ts, train_state_axes = restore_t5x_checkpoint_state(         partitioner=partitioner,         t5x_model=model,         input_shapes=jax.tree_map(lambda x: x.shape, dummy_batch),         input_types=jax.tree_map(lambda x: x.dtype, dummy_batch),         from_scratch=True,     )     partitioned_predict_fn = partitioner.partition(model.predict_batch,                                                    in_axis_resources=(train_state_axes.params, partitioner.data_partition_spec),                                                    out_axis_resources=None)     predict_fn = lambda params, batch: jax.block_until_ready(partitioned_predict_fn(params, batch))     print(f'FIRST dummy_batch={jax.tree_map(jnp.shape,dummy_batch)}')     out = predict_fn(init_ts.params, dummy_batch)     print(f'FIRST out={jax.tree_map(jnp.shape,out)}')     print(f'SECOND dummy_batch={jax.tree_map(jnp.shape,dummy_batch)}')     out = predict_fn(init_ts.params, dummy_batch)     print(f'SECOND out={jax.tree_map(jnp.shape,out)}') import itertools for dense_feat, batch_size, inner_dim, outer_dim in itertools.product([2,16],[2,16],[2,16],[2,16]):     print((dense_feat, batch_size, inner_dim, outer_dim))     test_matmul(dense_feat=dense_feat, batch_size=batch_size, inner_dim=inner_dim, outer_dim=outer_dim) print('yay!') ``` FYI:    What jax/jaxlib version are you using? jax 0.4.9 (from source: 63d87c6c3df2de514a41f6f6aff5340dad12cc35)  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Ubuntu  NVIDIA GPU info ++  ++",2023-05-31T04:56:02Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/16198,"Do you have a minimal repro? One guess I have is in multiprocess, make sure that all shapes you pass to pjit are global.","Hi  . By minimal repro, do you mean like instructions to run the above script endtoend?","Hi! can you repro this with jax and jaxlib 0.4.20? If yes, please provide a minimal repro: https://en.wikipedia.org/wiki/Minimal_reproducible_example:~:text=In%20computing%2C%20a%20minimal%20reproducible,to%20be%20demonstrated%20and%20reproduced."
1613,"以下是一个github上的jax下的一个issue, 标题是(When the inferred step is integer-valued, have `linspace()` return integer-valued results)， 内容是 (_I assume this feature request is a nogo given optimisation ramifications(?), but thought I'd open this for posterity's sake._ See the different results you get when using  `int64` in the following example:[^1] ```python >>> from jax import numpy as jnp >>> jnp.linspace(1, 7, 7) Array([1., 2., 3., 4., 5., 6., 7.], dtype=float64) >>> jnp.linspace(1, 7, 7, dtype=jnp.int64) Array([1, 2, 3, 3, 4, 5, 7], dtype=int64)                ~  ~  ~       expected 4, 5, 6 ``` I see now that the `jnp.linspace(1, 7, 7)` result doesn't consist of only integer values, only that nonints get repr'd as ints (which is understandable behaviour). ```python  >>> x = jnp.linspace(1, 7, 7) >>> x == jnp.asarray([1., 2., 3., 4., 5., 6., 7.], dtype=jnp.float64) Array([ True,  True,  True, False, False, False,  True], dtype=bool)                            ~~~~~  ~~~~~  ~~~~~ >>> x[4] Array(5., dtype=float64) >>> x[4]  5 Array(8.8817842e16, dtype=float64) ``` My question is if its possible for `linspace()` to give integervalued results when the the inferred linspace step is integervalued, like NumPy (at least seemingly more consistently) does. ```python >>> np.linspace(1, 7, 7)[4]  5 0.0 ``` I ran into this issue today when trying `jax.numpy` with code initially written for NumPy. [^1]: at least locally, `jax==HEAD`/`jaxlib==0.4.10`, Ubuntu 22.04, on CPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"When the inferred step is integer-valued, have `linspace()` return integer-valued results","_I assume this feature request is a nogo given optimisation ramifications(?), but thought I'd open this for posterity's sake._ See the different results you get when using  `int64` in the following example:[^1] ```python >>> from jax import numpy as jnp >>> jnp.linspace(1, 7, 7) Array([1., 2., 3., 4., 5., 6., 7.], dtype=float64) >>> jnp.linspace(1, 7, 7, dtype=jnp.int64) Array([1, 2, 3, 3, 4, 5, 7], dtype=int64)                ~  ~  ~       expected 4, 5, 6 ``` I see now that the `jnp.linspace(1, 7, 7)` result doesn't consist of only integer values, only that nonints get repr'd as ints (which is understandable behaviour). ```python  >>> x = jnp.linspace(1, 7, 7) >>> x == jnp.asarray([1., 2., 3., 4., 5., 6., 7.], dtype=jnp.float64) Array([ True,  True,  True, False, False, False,  True], dtype=bool)                            ~~~~~  ~~~~~  ~~~~~ >>> x[4] Array(5., dtype=float64) >>> x[4]  5 Array(8.8817842e16, dtype=float64) ``` My question is if its possible for `linspace()` to give integervalued results when the the inferred linspace step is integervalued, like NumPy (at least seemingly more consistently) does. ```python >>> np.linspace(1, 7, 7)[4]  5 0.0 ``` I ran into this issue today when trying `jax.numpy` with code initially written for NumPy. [^1]: at least locally, `jax==HEAD`/`jaxlib==0.4.10`, Ubuntu 22.04, on CPU",2023-05-30T15:48:28Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/16183,"Thanks for the request – you probably know this already, but the issue is that floating point arithmetic is only an approximation of realvalued arithmetic, and so you generally should not use exact equality checks with floatingpoint values. Regarding the specific request to return exact integers when possible – I don't think this is feasible. But I think we could make the results with `dtype='int64'` more reasonable."
2704,"以下是一个github上的jax下的一个issue, 标题是(jax.numpy.interp has poor performance on TPUs)， 内容是 ( Description Hi,  I recently discovered that jax.numpy.interp has poor performance on TPUs. More specifically, I am trying to perform 1D interpolation on a flattened tensor with an original shape of (128, 384, 384). After JIT, this operation takes 0.26 second on my laptop CPU, and 0.26 second on a Colab T4 GPU. However, the same operation would take 4.30 seconds on a TPUv3 core, which is way too slow. Since interpolation / linear look up table is a common operation for people working on computer vision, maybe we should improve the performance of this operation on TPU? I have attached a script to reproduce this problem  please let me know if I can provide anything that would be helpful. ``` python  import jax import time import jax.numpy as jnp def adjust_channel_given_channel(img, delta):     """"""Adjust the value of ref_channel given the value of to_adjust_channel and the delta""""""     ref_channel = 0     to_adjust_channel = 1     img_channels = jnp.split(img, 3, axis=1)     x = jnp.linspace(0.0, 1.0, delta.shape[1])      Delta is an 1D look up table for the ref_channel,       so we first flatten the ref_channel,       interpolate the delta, and then reshape it back.     ref_channel_flat = img_channels[ref_channel].flatten()     calculated_delta = jnp.interp(ref_channel_flat, x, delta)     calculated_delta = jnp.reshape(calculated_delta, img_channels[to_adjust_channel].shape)     img_channels[to_adjust_channel] = jnp.add(img_channels[to_adjust_channel], calculated_delta)     img = jnp.concatenate(img_channels, axis=1)     return img  Make it support batched input & JIT to improve performance. adjust_channel_given_channel_batched = jax.vmap(adjust_channel_given_channel, in_axes=(0, 0)) adjust_channel_given_channel_batched_jit = jax.jit(adjust_channel_given_channel_batched) if __name__ == '__main__':     img = jnp.ones((128, 384, 384, 3), dtype=jnp.float32)     delta = jnp.ones((128, 64), dtype=jnp.float32)      Warm up JIT.     adjust_channel_given_channel_batched_jit(img, delta).block_until_ready()     start_time = time.time()     adjust_channel_given_channel_batched_jit(img, delta).block_until_ready()     print(f'Total time: {time.time()  start_time} seconds')      0.26 seconds on laptop CPU, 0.26 second on Colab T4 GPU, 4.30 seconds on a single TPU v3 core. ```  What jax/jaxlib version are you using? jax v0.4.10, jaxlib v0.4.10  Which accelerator(s) are you using? TPU  Additional system info TPU VM tpuv38  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.numpy.interp has poor performance on TPUs," Description Hi,  I recently discovered that jax.numpy.interp has poor performance on TPUs. More specifically, I am trying to perform 1D interpolation on a flattened tensor with an original shape of (128, 384, 384). After JIT, this operation takes 0.26 second on my laptop CPU, and 0.26 second on a Colab T4 GPU. However, the same operation would take 4.30 seconds on a TPUv3 core, which is way too slow. Since interpolation / linear look up table is a common operation for people working on computer vision, maybe we should improve the performance of this operation on TPU? I have attached a script to reproduce this problem  please let me know if I can provide anything that would be helpful. ``` python  import jax import time import jax.numpy as jnp def adjust_channel_given_channel(img, delta):     """"""Adjust the value of ref_channel given the value of to_adjust_channel and the delta""""""     ref_channel = 0     to_adjust_channel = 1     img_channels = jnp.split(img, 3, axis=1)     x = jnp.linspace(0.0, 1.0, delta.shape[1])      Delta is an 1D look up table for the ref_channel,       so we first flatten the ref_channel,       interpolate the delta, and then reshape it back.     ref_channel_flat = img_channels[ref_channel].flatten()     calculated_delta = jnp.interp(ref_channel_flat, x, delta)     calculated_delta = jnp.reshape(calculated_delta, img_channels[to_adjust_channel].shape)     img_channels[to_adjust_channel] = jnp.add(img_channels[to_adjust_channel], calculated_delta)     img = jnp.concatenate(img_channels, axis=1)     return img  Make it support batched input & JIT to improve performance. adjust_channel_given_channel_batched = jax.vmap(adjust_channel_given_channel, in_axes=(0, 0)) adjust_channel_given_channel_batched_jit = jax.jit(adjust_channel_given_channel_batched) if __name__ == '__main__':     img = jnp.ones((128, 384, 384, 3), dtype=jnp.float32)     delta = jnp.ones((128, 64), dtype=jnp.float32)      Warm up JIT.     adjust_channel_given_channel_batched_jit(img, delta).block_until_ready()     start_time = time.time()     adjust_channel_given_channel_batched_jit(img, delta).block_until_ready()     print(f'Total time: {time.time()  start_time} seconds')      0.26 seconds on laptop CPU, 0.26 second on Colab T4 GPU, 4.30 seconds on a single TPU v3 core. ```  What jax/jaxlib version are you using? jax v0.4.10, jaxlib v0.4.10  Which accelerator(s) are you using? TPU  Additional system info TPU VM tpuv38  NVIDIA GPU info _No response_",2023-05-30T15:14:58Z,enhancement performance contributions welcome jax.numpy,open,1,9,https://github.com/jax-ml/jax/issues/16182,"I updated my code a bit and leveraged the fact that the range that needs to be interpolated is known, and each bin is evenly spaced. This results in significant speed up on GPUs, but the TPU implementation is still very slow.  ```python import jax import time import jax.numpy as jnp def fast_interp(x, xp_min, xp_max, fp):      The official jnp.interp is very slow becuase it uses searchsorted.      Therefore, we leverage the fact that the fp is linearly increasing, evenly spaced, and has a known range      to make this operation much faster.     eps = 1e6     n = fp.shape[1]     i = (x  xp_min) / (xp_max  xp_min) * n     i = jnp.clip(i, eps, n  1.0  eps)   Avoid index out of range     i_lower = jnp.floor(i).astype(jnp.int32)     i_upper = jnp.minimum(i_lower + 1, n  1)     w_upper = i  i_lower     w_lower = 1.0  w_upper     return w_lower * fp[i_lower] + w_upper * fp[i_upper] def adjust_channel_given_channel(img, delta):     """"""Adjust the value of ref_channel given the value of to_adjust_channel and the delta""""""     ref_channel = 0     to_adjust_channel = 1     img_channels = jnp.split(img, 3, axis=1)      Delta is an 1D look up table for the ref_channel,       so we first flatten the ref_channel,       interpolate the delta, and then reshape it back.     ref_channel_flat = img_channels[ref_channel].flatten()     calculated_delta = fast_interp(ref_channel_flat, 0.0, 1.0, delta)     calculated_delta = jnp.reshape(calculated_delta, img_channels[to_adjust_channel].shape)     img_channels[to_adjust_channel] = jnp.add(img_channels[to_adjust_channel], calculated_delta)     img = jnp.concatenate(img_channels, axis=1)     return img  Make it support batched input & JIT to improve performance. adjust_channel_given_channel_batched = jax.vmap(adjust_channel_given_channel, in_axes=(0, 0)) adjust_channel_given_channel_batched_jit = jax.jit(adjust_channel_given_channel_batched) if __name__ == '__main__':     img = jnp.ones((128, 384, 384, 3), dtype=jnp.float32)     delta = jnp.ones((128, 64), dtype=jnp.float32)      Warm up JIT.     adjust_channel_given_channel_batched_jit(img, delta).block_until_ready()     start_time = time.time()     adjust_channel_given_channel_batched_jit(img, delta).block_until_ready()     print(f'Total time: {time.time()  start_time} seconds')      0.038 seconds on my laptop CPU, 0.003 seconds on Colab T4, 0.86 seconds on a single TPUv3 core from TPUv38 VM. ```","We have a version of `interp` that uses a dot product rather than indexing, which we find to be much faster on TPUs... happy to share.", It would be awesome if you can share an interp implementation using dot product. Much appreciated!,"You can use this for now:  ```python  Copyright 2023 Google LLC.  SPDXLicenseIdentifier: Apache2.0 import jax.numpy as jnp def dot_interp(x, xp, fp):   """"""Interpolate with a dot product instead of indexing.   This is often much faster than the default jnp.interp on TPUs.   Note: This function expects a scalar `x`. If you need multiple `x`   values, use vmap over the first argument.   """"""    TODO(shoyer): upstream this into jnp.interp   n = len(xp)   i = jnp.arange(n)   dx = xp[1:]  xp[:1]   delta = x  xp[:1]   w = delta / dx   w_left = jnp.pad(1  w, [(0, 1)])   w_right = jnp.pad(w, [(1, 0)])   u = jnp.searchsorted(xp, x, side='right', method='compare_all')   u = jnp.clip(u, 1, n  1)   weights = w_left * (i == (u  1)) + w_right * (i == u)   weights = jnp.where(x  xp[1], i == (n  1), weights)   return jnp.dot(weights, fp, precision='highest') ```","The root of this poor performance comes from the fact that `interp` uses `searchsorted`, and `searchsorted` is not a primitive recognized by the compiler. Because of this, we must choose an implementation for it at the JAX level, and the default implementation is one that performs poorly on TPU. I'm not sure how to address this in general without making binary search a primitive for which the compiler can choose the optimial operation given the hardware. Unfortunately, it doesn't look like there's sufficient appetite to handle `searchsorted` natively in HLO. Given that, your best bet is probably to tune the implementation to the hardware manually, as  does in his answer.","There are actually two key tricks for efficient interpretation on TPUs: 1. Faster searchsorted with `method='compare_all'` 2. Replacing array indexing with a dot product This combination is much faster, at each for interpolating along relatively small dimensions (less than 100 elements or so). The main downside is higher memory usage, which can usually be mitigated by also using `jax.remat`. My suggested fix for JAX would be to add options for selecting these algorithms to `jnp.interp`.","Hi  thank you for providing your interp implementation using dot product! It seems to work perfectly when `x` and `fp` has the same shape, but it would stop working if `x` is longer than `fp`. I think the current `jax.numpy.interp` allows for x to have different sizes from `fp`. ","If you need multiple `x` points, use `vmap` to vectorize over the first argument.",It looks like PR CC(Make searchsorted a primitive) might address the `searchsorted` issue.
738,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Add a polymorphic shape refinement MLIR pass accessible to JAX Python.)， 内容是 ([shape_poly] Add a polymorphic shape refinement MLIR pass accessible to JAX Python. At the moment we can run the StableHLO module lowered by jax2tf with polymorphic shapes only with jax2tf, because the tf.XlaCallModule op has the necessary shape refinement logic (which is necessary to legalize the StableHLO module with dynamic shapes to MHLO). Here we expose the shape refinement MLIR transformation to JAX Python. For now this is used only in a test in jax_export_test.py.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Add a polymorphic shape refinement MLIR pass accessible to JAX Python.,"[shape_poly] Add a polymorphic shape refinement MLIR pass accessible to JAX Python. At the moment we can run the StableHLO module lowered by jax2tf with polymorphic shapes only with jax2tf, because the tf.XlaCallModule op has the necessary shape refinement logic (which is necessary to legalize the StableHLO module with dynamic shapes to MHLO). Here we expose the shape refinement MLIR transformation to JAX Python. For now this is used only in a test in jax_export_test.py.",2023-05-30T13:52:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16181
1353,"以下是一个github上的jax下的一个issue, 标题是(Bad return values from `jax.random.poisson` when `lam` is `jnp.inf` )， 内容是 ( Description ```python import jax.random as jr import jax.numpy as jnp jr.poisson(key, jnp.inf, shape=(2, )) ``` Output ``` Array([2147483647,          0], dtype=int32) ``` As above, `+inf` input leads to output 0, looks like an overflow occurs, for `lam = inf`, seems `214748364` should be a right answer here? A *large* output of  a `+inf` rate or expectation is more convincing here. BTW, for numpy `inf` will lead to a value error: ``` ValueError: lam value too large ``` For `tensorflowprobability` `inf` produces `inf`s ```python Poisson(jnp.inf)sample.(seed=key, sample_shape=(10, )) ``` Output ``` Array([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf], dtype=float32) ``` and maybe for compatibility with `numpy`, `jax` chooses `integer `return dtype, while `tfp` chooses `float`, as discussed by  https://github.com/google/jax/pull/16134pullrequestreview1446642709, seems a float return dtype may be more reasonable...   What jax/jaxlib version are you using? jax v0.4.10, jaxlib v0.4.10  Which accelerator(s) are you using? CPU  Additional system info Mac, Python 3.10.9  NVIDIA GPU info None)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bad return values from `jax.random.poisson` when `lam` is `jnp.inf` ," Description ```python import jax.random as jr import jax.numpy as jnp jr.poisson(key, jnp.inf, shape=(2, )) ``` Output ``` Array([2147483647,          0], dtype=int32) ``` As above, `+inf` input leads to output 0, looks like an overflow occurs, for `lam = inf`, seems `214748364` should be a right answer here? A *large* output of  a `+inf` rate or expectation is more convincing here. BTW, for numpy `inf` will lead to a value error: ``` ValueError: lam value too large ``` For `tensorflowprobability` `inf` produces `inf`s ```python Poisson(jnp.inf)sample.(seed=key, sample_shape=(10, )) ``` Output ``` Array([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf], dtype=float32) ``` and maybe for compatibility with `numpy`, `jax` chooses `integer `return dtype, while `tfp` chooses `float`, as discussed by  https://github.com/google/jax/pull/16134pullrequestreview1446642709, seems a float return dtype may be more reasonable...   What jax/jaxlib version are you using? jax v0.4.10, jaxlib v0.4.10  Which accelerator(s) are you using? CPU  Additional system info Mac, Python 3.10.9  NVIDIA GPU info None",2023-05-27T14:38:09Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/16164,"Thanks for the report. JAX has some constraints here, namely:  we cannot raise a runtime error triggered by the value of an input – this is quite similar, for example, to the reason JAX cannot raise IndexErrors for outofbound indices.  we cannot change the type of the output based on the value of the input, so only integer arrays are an option. With that in mind, given that the API returns an integer array, all we can do is choose some valid integer value to return when lambda is too large. There's no good return value here, but the largest representable integer is probably the least bad option. Alternatively, we could change the API contract so that it returns a float array, in which case we could use `NaN` or `inf` for invalid outputs (though this would require a deprecation cycle for the API change, so would be somewhat complicated) – what do you think?","Hi , thanks for your reply! In my opinion, JAX may *utlimately* support a float array return, there're some reasons you mentioned, `nan` or `inf` are more intuitive and mathematical than `1`, other lib (`tfp`) and framework (`torch`) return a float array too (but seems bug exists too 🤔 https://github.com/pytorch/pytorch/issues/102811), and for my use case, log transformation is often used.  Surely it's not a *must* and depends on personal taste and may *impact* latter features, such as *what type will other discrete distributions(binomial, multinomial) return?*. Consistency between these apis seems important too."
1265,"以下是一个github上的jax下的一个issue, 标题是([JAX] Use MLIR argument locations instead of a bespoke jax.arg_info attribute.)， 内容是 ([JAX] Use MLIR argument locations instead of a bespoke jax.arg_info attribute. https://github.com/llvm/llvmproject/commit/514dddbeba643e32310c508a15d7b6ff42f2c461 allowed for specifying argument Locations in the MLIR Python bindings. We should use them, in the form of a Name location, rather than making up our own attribute. Example of new output: ``` In [1]: import jax In [2]: ir = jax.jit(lambda x, y: x + y).lower(7, 3).compiler_ir() In [3]: ir.operation.print(enable_debug_info=True) loc1 = loc(""x"") loc2 = loc(""y"") module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.sharding = ""{replicated}""} loc(""x""), %arg1: tensor {mhlo.sharding = ""{replicated}""} loc(""y"")) > (tensor {jax.result_info = """"}) {     %0 = stablehlo.add %arg0, %arg1 : tensor loc(loc4)     return %0 : tensor loc(loc)   } loc(loc) } loc(loc) loc = loc(unknown) loc3 = loc("""":1:0) loc4 = loc(""jit()/jit(main)/add""(loc3)) ``` Note debug information must be enabled.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[JAX] Use MLIR argument locations instead of a bespoke jax.arg_info attribute.,"[JAX] Use MLIR argument locations instead of a bespoke jax.arg_info attribute. https://github.com/llvm/llvmproject/commit/514dddbeba643e32310c508a15d7b6ff42f2c461 allowed for specifying argument Locations in the MLIR Python bindings. We should use them, in the form of a Name location, rather than making up our own attribute. Example of new output: ``` In [1]: import jax In [2]: ir = jax.jit(lambda x, y: x + y).lower(7, 3).compiler_ir() In [3]: ir.operation.print(enable_debug_info=True) loc1 = loc(""x"") loc2 = loc(""y"") module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {mhlo.sharding = ""{replicated}""} loc(""x""), %arg1: tensor {mhlo.sharding = ""{replicated}""} loc(""y"")) > (tensor {jax.result_info = """"}) {     %0 = stablehlo.add %arg0, %arg1 : tensor loc(loc4)     return %0 : tensor loc(loc)   } loc(loc) } loc(loc) loc = loc(unknown) loc3 = loc("""":1:0) loc4 = loc(""jit()/jit(main)/add""(loc3)) ``` Note debug information must be enabled.",2023-05-26T20:18:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16160
929,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Add partial support for call_exported with polymorphic shapes)， 内容是 (Until now the jax_export.call_exported did not allow calling functions that were exported with polymorphic shapes. We now add that support, including resolving the dimension variables of the called function in terms of the shapes at the call site (which themselves may include dimension variables), and then computing the output shape of the called function. The support is partial in that we can export a JAX function that calls an exported polymorphic function, but we cannot invoke it. This is because we do not yet have access to the shape refinement machinery that XlaCallModule uses. For now, we use XlaCallModule for invoking exported that includes shape polymorphism.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Add partial support for call_exported with polymorphic shapes,"Until now the jax_export.call_exported did not allow calling functions that were exported with polymorphic shapes. We now add that support, including resolving the dimension variables of the called function in terms of the shapes at the call site (which themselves may include dimension variables), and then computing the output shape of the called function. The support is partial in that we can export a JAX function that calls an exported polymorphic function, but we cannot invoke it. This is because we do not yet have access to the shape refinement machinery that XlaCallModule uses. For now, we use XlaCallModule for invoking exported that includes shape polymorphism.",2023-05-26T09:03:02Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16148, PTAL
430,"以下是一个github上的jax下的一个issue, 标题是(Add new `called_index` to custom_call  `tf.backend_config` DictAttr.)， 内容是 (Add new `called_index` to custom_call  `tf.backend_config` DictAttr. Here, `called_index` indicates the tf concrete function index in the `function_list` of the parent XLACallModule.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add new `called_index` to custom_call  `tf.backend_config` DictAttr.,"Add new `called_index` to custom_call  `tf.backend_config` DictAttr. Here, `called_index` indicates the tf concrete function index in the `function_list` of the parent XLACallModule.",2023-05-24T21:05:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16114
8045,"以下是一个github上的jax下的一个issue, 标题是(`FAILED_PRECONDITION: DNN library initialization failed.` when importing tensorflow before jax, with `set_visible_devices`)， 内容是 ( Description Hello, the following code  ``` import jax.numpy as jnp import jax import jax.random as random  rng2 = random.PRNGKey(0) import tensorflow as tf tf.config.experimental.set_visible_devices([], 'GPU') rng = random.PRNGKey(1) ``` results in  ``` 20230524 10:06:54.948939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20230524 10:06:56.435690: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:427] Loaded runtime CuDNN library: 8.6.0 but source was compiled with: 8.8.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. Traceback (most recent call last):   File ""/tmp/test.py"", line 10, in      rng = random.PRNGKey(1)           ^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/random.py"", line 137, in PRNGKey     key = prng.seed_with_impl(impl, seed)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 320, in seed_with_impl     return random_seed(seed, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 734, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 746, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 751, in random_seed_impl_base     return seed(seeds)            ^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 980, in threefry_seed     return _threefry_seed(seed)            ^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 208, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(                                           ^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 155, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 2633, in bind     return self.bind_with_trace(top_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1088, in _pjit_call_impl     always_lower=False, lowering_platform=None).compile()                                                 ^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2313, in compile     executable = UnloadedMeshExecutable.from_hlo(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2633, in from_hlo     xla_executable, compile_options = _cached_compilation(                                       ^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 495, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 463, in backend_compile     return backend.compile(built_c, compile_options=options)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/tmp/test.py"", line 10, in      rng = random.PRNGKey(1)           ^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/random.py"", line 137, in PRNGKey     key = prng.seed_with_impl(impl, seed)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 320, in seed_with_impl     return random_seed(seed, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 734, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ``` It seems like tensorflow is preallocating the memory even though it should not see the GPU. However, if I uncomment the line ``` rng2 = random.PRNGKey(0) ``` before tensorflow is imported, the code will run.  What jax/jaxlib version are you using? ``` >>> jax.__version__ '0.4.10' >>> jaxlib.__version__ '0.4.10' >>> tf.__version__ '2.12.0' ``` I installed jax with the CUDA libraries from pip, via ``` pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```  Which accelerator(s) are you using? GPU  Additional system info Manjaro Linux  NVIDIA GPU info ```` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"`FAILED_PRECONDITION: DNN library initialization failed.` when importing tensorflow before jax, with `set_visible_devices`"," Description Hello, the following code  ``` import jax.numpy as jnp import jax import jax.random as random  rng2 = random.PRNGKey(0) import tensorflow as tf tf.config.experimental.set_visible_devices([], 'GPU') rng = random.PRNGKey(1) ``` results in  ``` 20230524 10:06:54.948939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT 20230524 10:06:56.435690: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:427] Loaded runtime CuDNN library: 8.6.0 but source was compiled with: 8.8.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. Traceback (most recent call last):   File ""/tmp/test.py"", line 10, in      rng = random.PRNGKey(1)           ^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/random.py"", line 137, in PRNGKey     key = prng.seed_with_impl(impl, seed)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 320, in seed_with_impl     return random_seed(seed, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 734, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 746, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 751, in random_seed_impl_base     return seed(seeds)            ^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 980, in threefry_seed     return _threefry_seed(seed)            ^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 208, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(                                           ^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 155, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 2633, in bind     return self.bind_with_trace(top_trace, args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/pjit.py"", line 1088, in _pjit_call_impl     always_lower=False, lowering_platform=None).compile()                                                 ^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2313, in compile     executable = UnloadedMeshExecutable.from_hlo(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2633, in from_hlo     xla_executable, compile_options = _cached_compilation(                                       ^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 495, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 463, in backend_compile     return backend.compile(built_c, compile_options=options)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/tmp/test.py"", line 10, in      rng = random.PRNGKey(1)           ^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/random.py"", line 137, in PRNGKey     key = prng.seed_with_impl(impl, seed)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 320, in seed_with_impl     return random_seed(seed, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/prng.py"", line 734, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/jakiw/Programs/miniconda3/envs/ml/lib/python3.11/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ``` It seems like tensorflow is preallocating the memory even though it should not see the GPU. However, if I uncomment the line ``` rng2 = random.PRNGKey(0) ``` before tensorflow is imported, the code will run.  What jax/jaxlib version are you using? ``` >>> jax.__version__ '0.4.10' >>> jaxlib.__version__ '0.4.10' >>> tf.__version__ '2.12.0' ``` I installed jax with the CUDA libraries from pip, via ``` pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```  Which accelerator(s) are you using? GPU  Additional system info Manjaro Linux  NVIDIA GPU info ```` ++  ++++ ```",2023-05-24T08:08:26Z,bug,open,2,3,https://github.com/jax-ml/jax/issues/16107,Have you solved the problem? I also encountered the same problem,"Hi, I am encountering the same problem.  My details: ```python  test.py import tensorflow as tf import jax print(f'{tf.__version__=}') print(f'{jax.__version__=}') ds = tf.data.Dataset.from_tensor_slices([1,2,3]) rng_key = jax.random.PRNGKey(42) ``` ```python python test.py 20230611 21:15:57.913384: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performancecritical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. /usr/lib/python3/distpackages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (3.0.4) doesn't match a supported version!   warnings.warn(""urllib3 ({}) or chardet ({}) doesn't match a supported "" tf.__version__='2.12.0' jax.__version__='0.4.12' 20230611 21:15:59.844151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:15:59.849216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:15:59.849632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:15:59.850620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:15:59.850816: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:15:59.851098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:16:00.521037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:16:00.521318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:16:00.521560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:16:00.521793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7306 MB memory:  > device: 0, name: NVIDIA GeForce GTX 1070 with MaxQ Design, pci bus id: 0000:01:00.0, compute capability: 6.1 20230611 21:16:00.637590: I external/xla/xla/service/service.cc:168] XLA service 0x2de8000 initialized for platform Interpreter (this does not guarantee that XLA will be used). Devices: 20230611 21:16:00.637614: I external/xla/xla/service/service.cc:176]   StreamExecutor device (0): Interpreter,  20230611 21:16:00.639394: I external/xla/xla/pjrt/tfrt_cpu_pjrt_client.cc:458] TfrtCpuClient created. 20230611 21:16:00.639733: I external/xla/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfsbuspciL344L355 20230611 21:16:00.640012: I external/xla/xla/service/service.cc:168] XLA service 0x1e92a1d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices: 20230611 21:16:00.640044: I external/xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1070 with MaxQ Design, Compute Capability 6.1 20230611 21:16:00.640507: I external/xla/xla/pjrt/gpu/se_gpu_pjrt_client.cc:627] Using BFC allocator. 20230611 21:16:00.640567: I external/xla/xla/pjrt/gpu/gpu_helpers.cc:105] XLA backend allocating 6380912640 bytes on device 0 for BFCAllocator. 20230611 21:16:00.642166: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.94GiB (6380912640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.642628: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.35GiB (5742821376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.643078: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.81GiB (5168539136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.643511: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.33GiB (4651684864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.643933: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.90GiB (4186516224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.644372: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.51GiB (3767864576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.644811: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.16GiB (3391078144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.645271: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.84GiB (3051970304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.645669: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.56GiB (2746773248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.646121: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.30GiB (2472095744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.646555: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.07GiB (2224886016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.647032: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.86GiB (2002397440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.647493: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.68GiB (1802157824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.647963: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.51GiB (1621942016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.648418: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.36GiB (1459747840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.648848: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.22GiB (1313773056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.649286: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.10GiB (1182395904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.649721: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1014.86MiB (1064156416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.650138: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 913.37MiB (957740800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.650554: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 822.04MiB (861966848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.650993: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 739.83MiB (775770112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.651440: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 665.85MiB (698193152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.651876: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 599.26MiB (628374016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.652291: I external/xla/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 539.34MiB (565536768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory 20230611 21:16:00.718717: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20230611 21:16:00.718777: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 5177344 bytes free, 8507883520 bytes total. Traceback (most recent call last):   File ""tests/haiku_jax_test.py"", line 8, in      rng_key = jax.random.PRNGKey(42)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/random.py"", line 155, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 707, in random_seed_impl_base     return seed(seeds)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 936, in threefry_seed     return _threefry_seed(seed)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/pjit.py"", line 250, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/usr/local/lib/python3.8/distpackages/jax/_src/pjit.py"", line 163, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/core.py"", line 2652, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/pjit.py"", line 1193, in _pjit_call_impl     return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,   File ""/usr/local/lib/python3.8/distpackages/jax/_src/pjit.py"", line 1177, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(   File ""/usr/local/lib/python3.8/distpackages/jax/_src/pjit.py"", line 1110, in _pjit_call_impl_python     compiled = _pjit_lower(   File ""/usr/local/lib/python3.8/distpackages/jax/_src/interpreters/pxla.py"", line 2329, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/usr/local/lib/python3.8/distpackages/jax/_src/interpreters/pxla.py"", line 2651, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/usr/local/lib/python3.8/distpackages/jax/_src/interpreters/pxla.py"", line 2561, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py"", line 497, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/usr/local/lib/python3.8/distpackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py"", line 465, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""tests/haiku_jax_test.py"", line 8, in      rng_key = jax.random.PRNGKey(42)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/random.py"", line 155, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 707, in random_seed_impl_base     return seed(seeds)   File ""/usr/local/lib/python3.8/distpackages/jax/_src/prng.py"", line 936, in threefry_seed     return _threefry_seed(seed) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. 20230611 21:16:00.892125: I external/xla/xla/pjrt/tfrt_cpu_pjrt_client.cc:461] TfrtCpuClient destroyed. ```","I did some tests changing the order of statements.  Each of these tests is a standalone independent launch of python.   It does not matter what order tf and jax are imported.  If the 'RT' command is run, something in the environment is changed after that so that the 'RJ' command produces the 'FAILED_PRECONDITION' error: Environment: ```bash docker run gpus all it tensorflow/tensorflow:latestgpu bash $ pip install upgrade ""jax[cuda11_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Mnemonic:  `I=import, R=run, T=tensorflow, J=jax` Script variations.  Each one is run inside the docker container as described above, using `python test.py` ```python  RESULTS:  IT, IJ, RT, RJ  FAILED_PRECONDITION  IT, IJ, RJ, RT  works   IT, RT, IJ, RJ  FAILED_PRECONDITION   IJ, IT, RT, RJ  FAILED_PRECONDITION   IJ, RJ, IT, RT  works   IJ, IT, RJ      works   IJ, IT, RT      works  import tensorflow as tf                            IT ds = tf.data.Dataset.from_tensor_slices([1,2,3])   RT import jax                                         IJ rng_key = jax.random.PRNGKey(42)                   RJ ```"
992,"以下是一个github上的jax下的一个issue, 标题是(JVP softmax implementation is missing a stop_gradient, leading to training instability)， 内容是 ( Description In jax/_src/nn/functions.py there's a new implementation of JVPbased softmax. Recently my team has been struggling with exploding gradients on training Transformer models, which we didn't have before. After bisecting all changes in JAX and other libraries we use, we narrowed it down to this change. The new softmax implementation is missing a `stop_gradient` operator around the x_max evaluation, which was done on the deprecated softmax implementation, and is highlighted as a canonical use case:  https://www.tensorflow.org/api_docs/python/tf/stop_gradient  What jax/jaxlib version are you using? jax v0.4.10  Which accelerator(s) are you using? TPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,"JVP softmax implementation is missing a stop_gradient, leading to training instability"," Description In jax/_src/nn/functions.py there's a new implementation of JVPbased softmax. Recently my team has been struggling with exploding gradients on training Transformer models, which we didn't have before. After bisecting all changes in JAX and other libraries we use, we narrowed it down to this change. The new softmax implementation is missing a `stop_gradient` operator around the x_max evaluation, which was done on the deprecated softmax implementation, and is highlighted as a canonical use case:  https://www.tensorflow.org/api_docs/python/tf/stop_gradient  What jax/jaxlib version are you using? jax v0.4.10  Which accelerator(s) are you using? TPU  Additional system info Linux  NVIDIA GPU info _No response_",2023-05-23T14:25:50Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16093,"Just adding some context we discussed in chat: after adding the `custom_jvp`, the `stop_gradient` in the `softmax` implementation is always a noop, since we no longer autodiff through the `softmax` implementation. Just to be extra sure, I checked:  The assert triggers if we remove the custom_jvp rule, but with the custom_jvp rule in place it doesn't."
2972,"以下是一个github上的jax下的一个issue, 标题是(Add (optional) ordered effects for `jax2tf.call_tf`)， 内容是 (Add (optional) ordered effects for `jax2tf.call_tf` This allows users to express nested TensorFlow computation that must be ordered during execution. It leverages the existing JAX effects system to model such side effects and lower them to use XLA tokens. With this change, `jax2tf.call_tf(ordered=True)` can be used to generate ordered TF calls. This has the following behavior: * With `call_tf_graph=True`, this generates a custom call op with the following differences: (1) a `!stablehlo.token` argument/result is prepended to each custom call's argument/result list and (2) `tf.backend_config` has an additional `has_token_input_output = true` entry. * Without `call_tf_graph=True`, this raises a `NotImplementedError()`. For this, `jax_export.py` makes sure that dummy arguments/results added for ordered effects are not exposed to the public interface by passing constant values in a wrapper function. Because of this, adding ordered effects to jax2tfed computation no longer causes calling convention changes and can be safely allowed. Example StableHLO produced from the added test: ``` module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {jax.arg_info = ""x"", mhlo.sharding = ""{replicated}""}) > (tensor {jax.result_info = """"}) {     %0 = stablehlo.constant dense : tensor     %1:2 = call (%0, %arg0) : (tensor, tensor) > (tensor, tensor)     return %1 CC(Python 3 compatibility issues) : tensor   }   func.func private (%arg0: tensor {jax.token = true}, %arg1: tensor {jax.arg_info = ""x"", mhlo.sharding = ""{replicated}""}) > (tensor {jax.token = true}, tensor {jax.result_info = """"}) {     %0 = stablehlo.create_token : !stablehlo.token     %1 = stablehlo.constant dense : tensor     %2:3 = stablehlo.while(%iterArg = %0, %iterArg_0 = %1, %iterArg_1 = %arg1) : !stablehlo.token, tensor, tensor      cond {       %4 = stablehlo.constant dense : tensor       %5 = stablehlo.compare  LT, %iterArg_0, %4,  SIGNED : (tensor, tensor) > tensor       stablehlo.return %5 : tensor     } do {       %4 = stablehlo.custom_call .call_tf_function(%iterArg, %iterArg_1) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {caller_name = ""__inference_callable_flat_tf_10"", has_token_input_output = true}} : (!stablehlo.token, tensor) > !stablehlo.token       %5 = stablehlo.constant dense : tensor       %6 = stablehlo.add %iterArg_1, %5 : tensor       %7 = stablehlo.constant dense : tensor       %8 = stablehlo.add %iterArg_0, %7 : tensor       stablehlo.return %4, %8, %6 : !stablehlo.token, tensor, tensor     }     %3 = stablehlo.constant dense : tensor     return %3, %2 CC(Explicit tuples are not valid function parameters in Python 3) : tensor, tensor   } } ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add (optional) ordered effects for `jax2tf.call_tf`,"Add (optional) ordered effects for `jax2tf.call_tf` This allows users to express nested TensorFlow computation that must be ordered during execution. It leverages the existing JAX effects system to model such side effects and lower them to use XLA tokens. With this change, `jax2tf.call_tf(ordered=True)` can be used to generate ordered TF calls. This has the following behavior: * With `call_tf_graph=True`, this generates a custom call op with the following differences: (1) a `!stablehlo.token` argument/result is prepended to each custom call's argument/result list and (2) `tf.backend_config` has an additional `has_token_input_output = true` entry. * Without `call_tf_graph=True`, this raises a `NotImplementedError()`. For this, `jax_export.py` makes sure that dummy arguments/results added for ordered effects are not exposed to the public interface by passing constant values in a wrapper function. Because of this, adding ordered effects to jax2tfed computation no longer causes calling convention changes and can be safely allowed. Example StableHLO produced from the added test: ``` module  attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {   func.func public (%arg0: tensor {jax.arg_info = ""x"", mhlo.sharding = ""{replicated}""}) > (tensor {jax.result_info = """"}) {     %0 = stablehlo.constant dense : tensor     %1:2 = call (%0, %arg0) : (tensor, tensor) > (tensor, tensor)     return %1 CC(Python 3 compatibility issues) : tensor   }   func.func private (%arg0: tensor {jax.token = true}, %arg1: tensor {jax.arg_info = ""x"", mhlo.sharding = ""{replicated}""}) > (tensor {jax.token = true}, tensor {jax.result_info = """"}) {     %0 = stablehlo.create_token : !stablehlo.token     %1 = stablehlo.constant dense : tensor     %2:3 = stablehlo.while(%iterArg = %0, %iterArg_0 = %1, %iterArg_1 = %arg1) : !stablehlo.token, tensor, tensor      cond {       %4 = stablehlo.constant dense : tensor       %5 = stablehlo.compare  LT, %iterArg_0, %4,  SIGNED : (tensor, tensor) > tensor       stablehlo.return %5 : tensor     } do {       %4 = stablehlo.custom_call .call_tf_function(%iterArg, %iterArg_1) {api_version = 2 : i32, has_side_effect = true, tf.backend_config = {caller_name = ""__inference_callable_flat_tf_10"", has_token_input_output = true}} : (!stablehlo.token, tensor) > !stablehlo.token       %5 = stablehlo.constant dense : tensor       %6 = stablehlo.add %iterArg_1, %5 : tensor       %7 = stablehlo.constant dense : tensor       %8 = stablehlo.add %iterArg_0, %7 : tensor       stablehlo.return %4, %8, %6 : !stablehlo.token, tensor, tensor     }     %3 = stablehlo.constant dense : tensor     return %3, %2 CC(Explicit tuples are not valid function parameters in Python 3) : tensor, tensor   } } ```",2023-05-23T00:24:13Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16089,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
1023,"以下是一个github上的jax下的一个issue, 标题是(Extend plugin discovery to also include entry-points.)， 内容是 (This effectively implements a mix of option 2 and option 3 from https://packaging.python.org/en/latest/guides/creatinganddiscoveringplugins/ as a pragmatic way to cover all packaging cases. The namespace/path based iteration works for situations where code has not been packaged and is present on the PYTHONPATH, whereas the advertised entrypoints work around setuptools/pkgutil issues that make it impossible to reliably iterate over installed modules in certain scenarios (noted for editable installs which use a custom finder that does not implement iter_modules()). A plugin entrypoint can be advertised in setup.py (or equivalent pyproject.toml) with something like: ```     entry_points={         ""jax_plugins"": [           ""openxlacpu = jax_plugins.openxla_cpu"",         ],     } ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Extend plugin discovery to also include entry-points.,"This effectively implements a mix of option 2 and option 3 from https://packaging.python.org/en/latest/guides/creatinganddiscoveringplugins/ as a pragmatic way to cover all packaging cases. The namespace/path based iteration works for situations where code has not been packaged and is present on the PYTHONPATH, whereas the advertised entrypoints work around setuptools/pkgutil issues that make it impossible to reliably iterate over installed modules in certain scenarios (noted for editable installs which use a custom finder that does not implement iter_modules()). A plugin entrypoint can be advertised in setup.py (or equivalent pyproject.toml) with something like: ```     entry_points={         ""jax_plugins"": [           ""openxlacpu = jax_plugins.openxla_cpu"",         ],     } ```",2023-05-19T21:14:56Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16073, FYI
2107,"以下是一个github上的jax下的一个issue, 标题是(Support static arguments for any function transformation)， 内容是 (Following up on https://github.com/google/jax/issues/15504, I ended up needing a more general solution because I kept running into the same issue with other function transformations that often don't support static arguments. My solution ended up being this this snippet that adds static argument support to any transformation. A meta transformation! :D ```python def static_support(transform):   def new_transform(fun, *args, static=(), **kwargs):     assert isinstance(static, (list, tuple)), static     cache = {}     def new_function(*args2, **kwargs2):       sta = {k: v for k, v in kwargs2.items() if k in static}       dyn = {k: v for k, v in kwargs2.items() if k not in static}       key = hash(tuple(sta.get(n, '_default') for n in static))       if key not in cache:         specialized = bind(fun, **sta)         specialized.__name__ = fun.__name__         cache[key] = transform(specialized, *args, **kwargs)       return cachekey     return new_function   return new_transform fun = static_support(jit)(fun, static=['static_arg']) fun = static_support(checkify)(fun, static=['static_arg'])  ... ``` A simplifying design choice here is that static arguments are required to be passed as keyword arguments and cannot be passed as positional arguments, which I find much easier to specify. When the transformed function is already the output of some other transformation, counting argument names can be painful. I'm wondering whether including a helper like the one above would be a useful addition, simplification, and unification for JAX? It might simplify the existing JAX transformations, provide a unified API to users, and allow users to use thirdparty transformations that may not support static arguments. Longer term, users could even be asked to apply the meta transformation themselves instead of repeating the API across the provided transformations.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Support static arguments for any function transformation,"Following up on https://github.com/google/jax/issues/15504, I ended up needing a more general solution because I kept running into the same issue with other function transformations that often don't support static arguments. My solution ended up being this this snippet that adds static argument support to any transformation. A meta transformation! :D ```python def static_support(transform):   def new_transform(fun, *args, static=(), **kwargs):     assert isinstance(static, (list, tuple)), static     cache = {}     def new_function(*args2, **kwargs2):       sta = {k: v for k, v in kwargs2.items() if k in static}       dyn = {k: v for k, v in kwargs2.items() if k not in static}       key = hash(tuple(sta.get(n, '_default') for n in static))       if key not in cache:         specialized = bind(fun, **sta)         specialized.__name__ = fun.__name__         cache[key] = transform(specialized, *args, **kwargs)       return cachekey     return new_function   return new_transform fun = static_support(jit)(fun, static=['static_arg']) fun = static_support(checkify)(fun, static=['static_arg'])  ... ``` A simplifying design choice here is that static arguments are required to be passed as keyword arguments and cannot be passed as positional arguments, which I find much easier to specify. When the transformed function is already the output of some other transformation, counting argument names can be painful. I'm wondering whether including a helper like the one above would be a useful addition, simplification, and unification for JAX? It might simplify the existing JAX transformations, provide a unified API to users, and allow users to use thirdparty transformations that may not support static arguments. Longer term, users could even be asked to apply the meta transformation themselves instead of repeating the API across the provided transformations.",2023-05-19T19:34:46Z,enhancement,open,1,2,https://github.com/jax-ml/jax/issues/16071,"You might find Equinox interesting, which does something similar: it has `equinox.filter_{jit, grad, ...}`, which automatically determines dynamic vs static arguments based on their type. After all, it mostly only makes sense to JIT wrt arrays, to grad wrt floatingpoint arrays, etc.","Thanks Patrick, I've heard of equinox and think it has a lot of interesting ideas. I use Ninjax."
908,"以下是一个github上的jax下的一个issue, 标题是(Add MLIR side effects to `tf.XlaCallModule`.)， 内容是 (Add MLIR side effects to `tf.XlaCallModule`. With `jax2tf` native serialization, the StableHLO module embedded in `tf.XlaCallModule` may contain `stablehlo.custom_call` calling TF host callback functions. In this case, the `stablehlo.custom_call`s will be lowered to `stablehlo.send` and `stablehlo.recv`, so `tf.XlaCallModule` has `TF_SendSideEffect` and `TF_RecvSideEffect`. This CL 1. replaces the `Pure` trait with `MemoryEffects` trait in the automatically generated `tf.XlaCallModule` op's definition. 2. sets `isStateful` in `XlaCallModule`'s op declaration. 2. updates TF side effect analysis to recursively analyze the TF host callback functions called by `tf.XlaCallModule`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add MLIR side effects to `tf.XlaCallModule`.,"Add MLIR side effects to `tf.XlaCallModule`. With `jax2tf` native serialization, the StableHLO module embedded in `tf.XlaCallModule` may contain `stablehlo.custom_call` calling TF host callback functions. In this case, the `stablehlo.custom_call`s will be lowered to `stablehlo.send` and `stablehlo.recv`, so `tf.XlaCallModule` has `TF_SendSideEffect` and `TF_RecvSideEffect`. This CL 1. replaces the `Pure` trait with `MemoryEffects` trait in the automatically generated `tf.XlaCallModule` op's definition. 2. sets `isStateful` in `XlaCallModule`'s op declaration. 2. updates TF side effect analysis to recursively analyze the TF host callback functions called by `tf.XlaCallModule`.",2023-05-18T21:36:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16061
411,"以下是一个github上的jax下的一个issue, 标题是(PRNGKeyArray: add several missing attributes & methods)， 内容是 (These are properties ant methods of `ArrayImpl` that we need to define on `PRNGKeyArrayImpl` for full compatibility. Leaving some of the more difficult/ambiguous ones as a TODO.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,PRNGKeyArray: add several missing attributes & methods,These are properties ant methods of `ArrayImpl` that we need to define on `PRNGKeyArrayImpl` for full compatibility. Leaving some of the more difficult/ambiguous ones as a TODO.,2023-05-17T21:48:50Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/16047
1576,"以下是一个github上的jax下的一个issue, 标题是(GPU profiling: cuptiGetTimestamp: error 999 )， 内容是 ( Description I have been trying to profile a program running on my A100 GPU on Google Cloud. When I wrap my computation with ``` with jax.profiler.trace(""/tmp/jaxtrace"", create_perfetto_link=True): ``` as described in the docs I get out a perfetto link, but it doesn't have any GPU information. Looking in the logs, I see lines like this: ``` 20230516 21:01:05.541307: E external/xla/xla/backends/profiler/gpu/cupti_error_manager.cc:137] cuptiGetTimestamp: error 999:  20230516 21:01:05.541339: E external/xla/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error. 20230516 21:01:05.541345: E external/xla/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error. 20230516 21:01:05.541350: E external/xla/xla/backends/profiler/gpu/cupti_tracer.cc:1722] function cupti_interface_>Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error  ``` Any idea how to further debug or fix this? (I tested the workaround in https://github.com/google/jax/issues/13477 but it did not fix things)  What jax/jaxlib version are you using? jax=0.4.8, jaxlib=0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU (A100)  Additional system info 22.04 Ubuntu  NVIDIA GPU info https://gist.github.com/silvasean/2f6da6081642c59b9e77711f3a472fc1)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,GPU profiling: cuptiGetTimestamp: error 999 ," Description I have been trying to profile a program running on my A100 GPU on Google Cloud. When I wrap my computation with ``` with jax.profiler.trace(""/tmp/jaxtrace"", create_perfetto_link=True): ``` as described in the docs I get out a perfetto link, but it doesn't have any GPU information. Looking in the logs, I see lines like this: ``` 20230516 21:01:05.541307: E external/xla/xla/backends/profiler/gpu/cupti_error_manager.cc:137] cuptiGetTimestamp: error 999:  20230516 21:01:05.541339: E external/xla/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error. 20230516 21:01:05.541345: E external/xla/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error. 20230516 21:01:05.541350: E external/xla/xla/backends/profiler/gpu/cupti_tracer.cc:1722] function cupti_interface_>Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error  ``` Any idea how to further debug or fix this? (I tested the workaround in https://github.com/google/jax/issues/13477 but it did not fix things)  What jax/jaxlib version are you using? jax=0.4.8, jaxlib=0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU (A100)  Additional system info 22.04 Ubuntu  NVIDIA GPU info https://gist.github.com/silvasean/2f6da6081642c59b9e77711f3a472fc1",2023-05-16T21:48:13Z,bug needs info,closed,0,5,https://github.com/jax-ml/jax/issues/16030,"Did you install CUDA using the `pip` packages? If so, can you try updating to the latest release (0.4.10)? I think this may have already been fixed by https://github.com/google/jax/commit/75d0f6522d3e4623e7fd5bd512645e4cbab6b10d which added a missing `cupti` dependency.","Thanks Peter. I see `nvidiacudacupticu11` already installed in my venv: ``` $ python m pip list | grep nvidia nvidiacublascu11                11.11.3.6 nvidiacudacupticu11            11.8.87 nvidiacudanvcccu11             11.8.89 nvidiacudaruntimecu11          11.8.89 nvidiacudnncu11                 8.9.0.131 nvidiacufftcu11                 10.9.0.58 nvidiacusolvercu11              11.4.1.48 nvidiacusparsecu11              11.7.5.86 ``` However, I also have CUDA installed locally. I suspect the local one is being picked, since `LD_LIBRARY_PATH=/usr/local/cuda11.7/extras/CUPTI/lib64/` appears to fix the issue. Is there a way to observe or control which CUDA installation is being picked?","I just want to check: did you actually update your jaxlib to the latest release (0.4.10)? You also need to pick up this commit: https://github.com/openxla/xla/commit/c2319a85f3b7d115e57f65ebe2deb67fc349f067 Fundamentally we'll look in the location given by the `RPATH`, and then look in your `LD_LIBRARY_PATH`.","No, I did not update, since I found a workaround (and I find these things to be fragile as they get updated  it sounds like the packaging situation is getting a lot more robust though so I'm excited to see that change!). When you say `RPATH`  which specific ELF file is most relevant here? I looked in xla_extension.so but see no RPATH, only RUNPATH (very similar, except the order in which they are searched). (and indeed, the cupti path is missing) ```  0x000000000000001d (RUNPATH)            Library runpath: [$ORIGIN/../nvidia/cuda_runtime/lib:$ORIGIN/../nvidia/cublas/lib:$ORIGIN/../nvidia/cufft/lib:$ORIGIN/../nvidia/cudnn/lib:$ORIGIN/../nvidia/cusolver/lib] ``` Overall, I think I have a solution for me locally here, and it looks like there is work in progress/landed to make this not an issue going forward, so feel free to close! Thanks for your help Peter!","Err, yes, it's `RUNPATH`. If you update to the current release, then you'll see the `cupti` path in the `RUNPATH` as well. Closing, since the issue is solved already in the current release."
590,"以下是一个github上的jax下的一个issue, 标题是(add a jax.nn.softmax_alt with alternative differentiation behavior)， 内容是 (Following up on CC(add custom_jvp for jax.nn.softmax), we found that at least one user's code worked better with the old differentiation approach. So we decided to keep it alive! Instead of relying on a flag, users can just apply `jax.nn.softmax_alt` to get the old behavior. (The flag is still here for now, though I'll delete it in followup.))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,add a jax.nn.softmax_alt with alternative differentiation behavior,"Following up on CC(add custom_jvp for jax.nn.softmax), we found that at least one user's code worked better with the old differentiation approach. So we decided to keep it alive! Instead of relying on a flag, users can just apply `jax.nn.softmax_alt` to get the old behavior. (The flag is still here for now, though I'll delete it in followup.)",2023-05-15T21:45:05Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16015, talked me out of this...
941,"以下是一个github上的jax下的一个issue, 标题是(Slicing a CPU-placed jax array results in unnecessary host-to-device transfers)， 内容是 ( Description When one slices a CPUplaced array, jax keeps the result on a CPU, but, nevertheless, seems to send slice indices to TPU. Steps to reproduce: ``` import jax import numpy as np with jax.transfer_guard('disallow'):   x = jax.device_put(np.zeros((8,)), jax.devices(""cpu"")[0])   y = x[2:4] ``` Error: ``` XlaRuntimeError: INVALID_ARGUMENT: Disallowed hosttodevice transfer: aval=ShapedArray(int32[]), dst_sharding=SingleDeviceSharding(device=TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)) ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Slicing a CPU-placed jax array results in unnecessary host-to-device transfers," Description When one slices a CPUplaced array, jax keeps the result on a CPU, but, nevertheless, seems to send slice indices to TPU. Steps to reproduce: ``` import jax import numpy as np with jax.transfer_guard('disallow'):   x = jax.device_put(np.zeros((8,)), jax.devices(""cpu"")[0])   y = x[2:4] ``` Error: ``` XlaRuntimeError: INVALID_ARGUMENT: Disallowed hosttodevice transfer: aval=ShapedArray(int32[]), dst_sharding=SingleDeviceSharding(device=TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)) ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-14T14:58:41Z,bug,open,1,3,https://github.com/jax-ml/jax/issues/16002,"I'm also affected by this. The simplest reproduction example is: ```python jax.config.update(""jax_transfer_guard"", ""disallow"") x = jnp.zeros(())   Raise Disallowed hosttodevice transfer ``` That raises: ``` XlaRuntimeError: INVALID_ARGUMENT: Disallowed hosttodevice transfer: aval=ShapedArray(float32[]), dst_sharding=GSPMDSharding({replicated}) ```",We are almost close to a fix. It should happen in the next 2 weeks or so.,"I can still reproduce this ```python jax.config.update(""jax_transfer_guard"", ""allow"") x = jnp.zeros(())   Raise Disallowed hosttodevice transfer ```"
844,"以下是一个github上的jax下的一个issue, 标题是(`jax.custom_jvp` promotes tangent-of-integer to integer (not float0))， 内容是 ( Description ```python import jax .custom_jvp def f(x, y):     return x + y .defjvp def f_jvp(primals, tangents):     _, ty = tangents     assert ty.dtype != jax.numpy.int32 jax.jvp(lambda x: f(x, 1), (1.,), (1.,)) ``` This will fail the assert statement. I came across this when calling `jax.jvp(some_other_fn, ...)` inside the `defjvp` rule. This was complaining that an integer was given an integer tangent.  What jax/jaxlib version are you using? JAX 0.4.10, jaxlib 0.4.10  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.custom_jvp` promotes tangent-of-integer to integer (not float0)," Description ```python import jax .custom_jvp def f(x, y):     return x + y .defjvp def f_jvp(primals, tangents):     _, ty = tangents     assert ty.dtype != jax.numpy.int32 jax.jvp(lambda x: f(x, 1), (1.,), (1.,)) ``` This will fail the assert statement. I came across this when calling `jax.jvp(some_other_fn, ...)` inside the `defjvp` rule. This was complaining that an integer was given an integer tangent.  What jax/jaxlib version are you using? JAX 0.4.10, jaxlib 0.4.10  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-14T08:17:33Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/16000,"Thanks for raising this. IIRC we did this intentionally at first when we introduced float0, I think because preexisting custom_jvp code didn't know about float0. But as you point out it's currently inconsistent with `jax.jvp` itself, so we should make these coherent somehow...  "
1036,"以下是一个github上的jax下的一个issue, 标题是(sparse-sparse matrix multiply creates unnecessary zero entries)， 内容是 ( Description When multiplying two sparse BCOO matrices it seems the result always stores explicit zeroentries even when the corresponding row/column of `a` and `b` are all zero: ```python import jax import numpy as np a = jax.experimental.sparse.BCOO.fromdense(np.diag([1., 2.])) b = jax.experimental.sparse.BCOO.fromdense(np.diag([3., 4.])) (a @ b).data, (a @ b).indices >>> (Array([3., 0., 0., 8.], dtype=float64),      Array([[0, 0],             [0, 1],             [1, 0],             [1, 1]], dtype=int32)) ``` Expected output: ```python >>> (Array([3., 8.], dtype=float64),      Array([[0, 0],             [1, 1]], dtype=int32)) ```  What jax/jaxlib version are you using? 0.4.8  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,sparse-sparse matrix multiply creates unnecessary zero entries," Description When multiplying two sparse BCOO matrices it seems the result always stores explicit zeroentries even when the corresponding row/column of `a` and `b` are all zero: ```python import jax import numpy as np a = jax.experimental.sparse.BCOO.fromdense(np.diag([1., 2.])) b = jax.experimental.sparse.BCOO.fromdense(np.diag([3., 4.])) (a @ b).data, (a @ b).indices >>> (Array([3., 0., 0., 8.], dtype=float64),      Array([[0, 0],             [0, 1],             [1, 0],             [1, 1]], dtype=int32)) ``` Expected output: ```python >>> (Array([3., 8.], dtype=float64),      Array([[0, 0],             [1, 1]], dtype=int32)) ```  What jax/jaxlib version are you using? 0.4.8  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-13T21:02:05Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/15997,"Hi  thanks for the report! This is working as intended. You're correct that sparsesparse matmul often results in more stored elements than are strictly required, but those extra stored arguments are necessary due to the constraints of JAX's compilation model, which requires array shapes (and in this case the size of the sparse matrix buffers) to be known at compile time. The issue is that the sparse matrix indices are only known at runtime, so the output buffers must be able to handle the worst case. When multiplying two matrices with number of specified elements `a.nse` and `b.nse`, the worst case is an output with `out.nse = a.nse * b.nse` (an easy way to imagine this is if the first matrix has all entries in a single column, and the second matrices has all elements in a single row). In anything but this worst case, the result will be padded with zeros. To handle this, you have two options: 1) Call `out.sum_duplicates()` on the result of the matmul, outside JIT, in order to sum and remove duplicated entries. It might look like this: ```python out = (a @ b).sum_duplicates() print(out.data)  [3. 8.] print(out.indices)  [[0 0]   [1 1]] ``` 2) If appropriate, you can use a structured sparse representation (e.g. with `n_batch=1` on the leftmost input) such that the output *nse* will be more constrained. Hope that helps!","ah I see, that makes sense! Would it somehow be possible to manually set the number of specified elements for the output? eg in this case I'm computing `Bi = S.T @ Ai @ S` for a bunch of very sparse matrices that are too large to store densely on the gpu but I know `Bi.nse == Ai.nse`.",How do you *know* that the output has the same nse as the input? Could you encode that knowledge by using structured sparsity for the `S` matrix (i.e. option 2 in my answer above)?,"The `Ai`s are nonzero only on subblocks (different for every i) and `S = [[D, b], [0, 1]]` where `D` is diagonal I ended up getting around the issue by simply rescaling the elements of `Ai` before constructing the sparse matrix, so no need for matrixmatrix multiplies :smile:  In case it's useful here's a basic example to illustrate, goes OOM on my 12GB GPU: ```python import numpy as np import jax.numpy as jnp from jax.experimental import sparse def get_inds(n, block_size):     block_inds = np.random.choice(n  1, block_size  1, replace=False)     block_inds = np.hstack([np.sort(block_inds), n  1])     return block_inds[np.array(list(np.ndindex(block_size, block_size)))] n = 48 n_batch = 3000 block_size = 5 A = sparse.bcoo_concatenate([     sparse.BCOO(         (             np.random.randn(block_size * block_size),             get_inds(n, block_size)         ),         shape=(n, n),     )[None]     for _ in range(n_batch) ], dimension=0) S = sparse.BCOO.fromdense(np.block([     [np.diag(np.random.randn(n  1)), np.random.randn(n  1)[:, None]],     [np.zeros((1, n  1)), 1.] ])) A_scaled = (A @ S).transpose((0, 2, 1)) @ S ```"
620,"以下是一个github上的jax下的一个issue, 标题是(Refactor approx_top_k lowering to make it easier to understand)， 内容是 (Refactor approx_top_k lowering to make it easier to understand There have been two recent changes in this area: 1) migrating TPU lowering from XLA fallback to MLIR, 2) migrating lowering for other platforms from XLA fallback to MLIR. As I was trying to understand whether the versioning code makes sense, I had a hard time doing that. I think this refactoring makes this easier.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Refactor approx_top_k lowering to make it easier to understand,"Refactor approx_top_k lowering to make it easier to understand There have been two recent changes in this area: 1) migrating TPU lowering from XLA fallback to MLIR, 2) migrating lowering for other platforms from XLA fallback to MLIR. As I was trying to understand whether the versioning code makes sense, I had a hard time doing that. I think this refactoring makes this easier.",2023-05-12T19:02:08Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/15988, for review., for review. I'm not familiar the mhlo related lowering logic.
10459,"以下是一个github上的jax下的一个issue, 标题是(ROCM build is broken)， 内容是 (I was able to run the Hessian code using rocm/jaxbuild. I have not run the tests on that docker image.   Fixing Dockerfile.ms is probably not worth it I was in the middle of putting together a minimal pull request to fix this when I realized that most of the tests were broken and that basically, no one has tested the ROCm build in quite some time, which means that there's no planned support from Google  or at least that AMD ROCm support is not considered ""essential"" in the same way that CUDA/TPU support would be.  That's quite frustrating because screwing with ROCm while learning the python ecosystem is about all I've done for a month and I cannot afford a GPU. I haven't really written a line of python during that time. If this issue seems a little scattered, it's because I pulled the content from my notes.  Description I built JAX ```sh ./build/rocm/ci_build.sh keep_image runtime bash c ""./build/rocm/build_rocm.sh"" ``` And launched: ```sh sudo docker run it device=/dev/kfd device=/dev/dri securityopt seccomp=unconfined groupadd video entrypoint /bin/bash jaxrocm:latest ```  Initial Problems Seeing this in =step 17/21=. This is similar to RadeonOpenCompute/ROCm/issues CC(Relax test tolerance for core_test jvp tests.) ``` W: Conflicting distribution: https://repo.radeon.com/rocm/apt/5.4 ubuntu InRelease (expected ubuntu but got focal) WARNING: apt does not have a stable CLI interface. Use with caution in scripts. ... debconf: delaying package configuration, since aptutils is not installed ``` And then in =Step 21/21= after git clone, the image learns that numpy 1.20 was installed.   Fixed Dockerfile.ms by bumping numpy to 0.21.6 Change =Dockerfile.ms= to fix the pip version  google/jax CC([ROCm] jaxrocm runtime/ci dockerfile multistages): pullreq to change build_ci.sh to default to multistage  both dockerfiles should have numpy bumped  `build_ci.sh` comments should change to indicate Dockerfile.ms is default  Bazel Build Warnings I changed numpy to 0.21.6 and the build script succeed. ``` WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found ```  Running Tests Start the container with `${PWD}/jax` bound to the `/workspace` volume. ```sh sudo docker run it device=/dev/kfd device=/dev/dri \      securityopt seccomp=unconfined groupadd video \      volume=${PWD}/jax:/workspace \      e 'HSA_OVERRIDE_GFX_VERSION=10.3.0' \      entrypoint ""/bin/bash"" jaxrocm:latest ``` I ran the tests after removing the `x` argument in `python ./build/rocm/run_single_gpu.py` 32 Tests Failed ```  Captured stderr call  20230512 05:43:25.807625: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.gemm' failed: Not implemented on ROCm. ...  Captured stderr call  20230512 05:43:26.279208: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.gemm' failed: Not implemented on ROCm. =========================== short test summary info ============================ FAILED tests/ann_test.py::AnnTest::test_approx_max_k0  jaxlib.xla_extension.... ... FAILED tests/ann_test.py::AnnTest::test_approx_min_k0  jaxlib.xla_extension.... ... FAILED tests/ann_test.py::AnnTest::test_approx_min_k9  jaxlib.xla_extension.... FAILED tests/ann_test.py::AnnTest::test_pmap0  jaxlib.xla_extension.XlaRunti... ...  FAILED tests/ann_test.py::AnnTest::test_pmap9  jaxlib.xla_extension.XlaRunti... FAILED tests/ann_test.py::AnnTest::test_vmap_after  jaxlib.xla_extension.Xla... FAILED tests/ann_test.py::AnnTest::test_vmap_before  jaxlib.xla_extension.Xl... =================== 32 failed, 10 passed, 96 rerun in 34.09s =================== ``` No pmap?  .... isn't that just clojure's r/fold? python is the wrong language... and lifting to the network for distributed programming is why haskell was written (or so I've always imagined)  I Couldn't Run The Hessian Example The main issue for me was that I was unable to pushfwd/pullback in order to do hessians. Error: Hessian not implemented on ROCm ```python >>> J = jacfwd(f)(W) >>> J = jacrev(f)(W) 20230512 05:47:40.848932: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.gemm' failed: Not implemented on ROCm. CC(Roll forward with fix: Remove the original python function `fun_` from C++ PjitFunction, as the destroying `fun_` may yield the thread in some cases, which causes error during deleting the python object of PjitFunction.)  ```  What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? GPU  Additional system info Linux e5fc65f21cda 6.3.1zen11zen CC(Python 3 compatibility issues) ZEN SMP PREEMPT_DYNAMIC Mon, 01 May 2023 17:42:12 +0000 x86_64 x86_64 x86_64 GNU/Linux  NVIDIA GPU info rocmsmi ``` ======================= ROCm System Management Interface ======================= ================================= Concise Info ================================= GPU  Temp (DieEdge)  AvgPwr  SCLK    MCLK     Fan     Perf  PwrCap  VRAM%  GPU% 0    55.0c           26.0W   500Mhz  1000Mhz  32.55%  auto  211.0W   16%   0% ================================================================================ ============================= End of ROCm SMI Log ============================== ``` rocminfo ``` ROCk module is loaded ===================== HSA System Attributes ===================== Runtime Version:         1.1 System Timestamp Freq.:  1000.000000MHz Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count) Machine Model:           LARGE System Endianness:       LITTLE ========== HSA Agents ========== ******* Agent 1 ******* Name:                    AMD Ryzen 9 5950X 16Core Processor Uuid:                    CPUXX Marketing Name:          AMD Ryzen 9 5950X 16Core Processor Vendor Name:             CPU Feature:                 None specified Profile:                 FULL_PROFILE Float Round Mode:        NEAR Max Queue Number:        0(0x0) Queue Min Size:          0(0x0) Queue Max Size:          0(0x0) Queue Type:              MULTI Node:                    0 Device Type:             CPU Cache Info: L1:                      32768(0x8000) KB Chip ID:                 0(0x0) ASIC Revision:           0(0x0) Cacheline Size:          64(0x40) Max Clock Freq. (MHz):   3400 BDFID:                   0 Internal Node ID:        0 Compute Unit:            32 SIMDs per CU:            0 Shader Engines:          0 Shader Arrs. per Eng.:   0 WatchPts on Addr. Ranges:1 Features:                None Pool Info: Pool 1 Segment:                 GLOBAL; FLAGS: FINE GRAINED Size:                    32771392(0x1f40d40) KB Allocatable:             TRUE Alloc Granule:           4KB Alloc Alignment:         4KB Accessible by all:       TRUE Pool 2 Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED Size:                    32771392(0x1f40d40) KB Allocatable:             TRUE Alloc Granule:           4KB Alloc Alignment:         4KB Accessible by all:       TRUE Pool 3 Segment:                 GLOBAL; FLAGS: COARSE GRAINED Size:                    32771392(0x1f40d40) KB Allocatable:             TRUE Alloc Granule:           4KB Alloc Alignment:         4KB Accessible by all:       TRUE ISA Info: ******* Agent 2 ******* Name:                    gfx1030 Uuid:                    GPUXX Marketing Name:          AMD Radeon RX 6700 XT Vendor Name:             AMD Feature:                 KERNEL_DISPATCH Profile:                 BASE_PROFILE Float Round Mode:        NEAR Max Queue Number:        128(0x80) Queue Min Size:          64(0x40) Queue Max Size:          131072(0x20000) Queue Type:              MULTI Node:                    1 Device Type:             GPU Cache Info: L1:                      16(0x10) KB L2:                      3072(0xc00) KB L3:                      98304(0x18000) KB Chip ID:                 29663(0x73df) ASIC Revision:           0(0x0) Cacheline Size:          64(0x40) Max Clock Freq. (MHz):   2855 BDFID:                   2816 Internal Node ID:        1 Compute Unit:            40 SIMDs per CU:            2 Shader Engines:          4 Shader Arrs. per Eng.:   2 WatchPts on Addr. Ranges:4 Features:                KERNEL_DISPATCH Fast F16 Operation:      TRUE Wavefront Size:          32(0x20) Workgroup Max Size:      1024(0x400) Workgroup Max Size per Dimension: x                        1024(0x400) y                        1024(0x400) z                        1024(0x400) Max Waves Per CU:        32(0x20) Max Workitem Per CU:    1024(0x400) Grid Max Size:           4294967295(0xffffffff) Grid Max Size per Dimension: x                        4294967295(0xffffffff) y                        4294967295(0xffffffff) z                        4294967295(0xffffffff) Max fbarriers/Workgrp:   32 Pool Info: Pool 1 Segment:                 GLOBAL; FLAGS: COARSE GRAINED Size:                    12566528(0xbfc000) KB Allocatable:             TRUE Alloc Granule:           4KB Alloc Alignment:         4KB Accessible by all:       FALSE Pool 2 Segment:                 GROUP Size:                    64(0x40) KB Allocatable:             FALSE Alloc Granule:           0KB Alloc Alignment:         0KB Accessible by all:       FALSE ISA Info: ISA 1 Name:                    amdgcnamdamdhsagfx1030 Machine Models:          HSA_MACHINE_MODEL_LARGE Profiles:                HSA_PROFILE_BASE Default Rounding Mode:   NEAR Default Rounding Mode:   NEAR Fast f16:                TRUE Workgroup Max Size:      1024(0x400) Workgroup Max Size per Dimension: x                        1024(0x400) y                        1024(0x400) z                        1024(0x400) Grid Max Size:           4294967295(0xffffffff) Grid Max Size per Dimension: x                        4294967295(0xffffffff) y                        4294967295(0xffffffff) z                        4294967295(0xffffffff) FBarrier Max Size:       32 *** Done *** ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",large language model,ROCM build is broken,"I was able to run the Hessian code using rocm/jaxbuild. I have not run the tests on that docker image.   Fixing Dockerfile.ms is probably not worth it I was in the middle of putting together a minimal pull request to fix this when I realized that most of the tests were broken and that basically, no one has tested the ROCm build in quite some time, which means that there's no planned support from Google  or at least that AMD ROCm support is not considered ""essential"" in the same way that CUDA/TPU support would be.  That's quite frustrating because screwing with ROCm while learning the python ecosystem is about all I've done for a month and I cannot afford a GPU. I haven't really written a line of python during that time. If this issue seems a little scattered, it's because I pulled the content from my notes.  Description I built JAX ```sh ./build/rocm/ci_build.sh keep_image runtime bash c ""./build/rocm/build_rocm.sh"" ``` And launched: ```sh sudo docker run it device=/dev/kfd device=/dev/dri securityopt seccomp=unconfined groupadd video entrypoint /bin/bash jaxrocm:latest ```  Initial Problems Seeing this in =step 17/21=. This is similar to RadeonOpenCompute/ROCm/issues CC(Relax test tolerance for core_test jvp tests.) ``` W: Conflicting distribution: https://repo.radeon.com/rocm/apt/5.4 ubuntu InRelease (expected ubuntu but got focal) WARNING: apt does not have a stable CLI interface. Use with caution in scripts. ... debconf: delaying package configuration, since aptutils is not installed ``` And then in =Step 21/21= after git clone, the image learns that numpy 1.20 was installed.   Fixed Dockerfile.ms by bumping numpy to 0.21.6 Change =Dockerfile.ms= to fix the pip version  google/jax CC([ROCm] jaxrocm runtime/ci dockerfile multistages): pullreq to change build_ci.sh to default to multistage  both dockerfiles should have numpy bumped  `build_ci.sh` comments should change to indicate Dockerfile.ms is default  Bazel Build Warnings I changed numpy to 0.21.6 and the build script succeed. ``` WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found ```  Running Tests Start the container with `${PWD}/jax` bound to the `/workspace` volume. ```sh sudo docker run it device=/dev/kfd device=/dev/dri \      securityopt seccomp=unconfined groupadd video \      volume=${PWD}/jax:/workspace \      e 'HSA_OVERRIDE_GFX_VERSION=10.3.0' \      entrypoint ""/bin/bash"" jaxrocm:latest ``` I ran the tests after removing the `x` argument in `python ./build/rocm/run_single_gpu.py` 32 Tests Failed ```  Captured stderr call  20230512 05:43:25.807625: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.gemm' failed: Not implemented on ROCm. ...  Captured stderr call  20230512 05:43:26.279208: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.gemm' failed: Not implemented on ROCm. =========================== short test summary info ============================ FAILED tests/ann_test.py::AnnTest::test_approx_max_k0  jaxlib.xla_extension.... ... FAILED tests/ann_test.py::AnnTest::test_approx_min_k0  jaxlib.xla_extension.... ... FAILED tests/ann_test.py::AnnTest::test_approx_min_k9  jaxlib.xla_extension.... FAILED tests/ann_test.py::AnnTest::test_pmap0  jaxlib.xla_extension.XlaRunti... ...  FAILED tests/ann_test.py::AnnTest::test_pmap9  jaxlib.xla_extension.XlaRunti... FAILED tests/ann_test.py::AnnTest::test_vmap_after  jaxlib.xla_extension.Xla... FAILED tests/ann_test.py::AnnTest::test_vmap_before  jaxlib.xla_extension.Xl... =================== 32 failed, 10 passed, 96 rerun in 34.09s =================== ``` No pmap?  .... isn't that just clojure's r/fold? python is the wrong language... and lifting to the network for distributed programming is why haskell was written (or so I've always imagined)  I Couldn't Run The Hessian Example The main issue for me was that I was unable to pushfwd/pullback in order to do hessians. Error: Hessian not implemented on ROCm ```python >>> J = jacfwd(f)(W) >>> J = jacrev(f)(W) 20230512 05:47:40.848932: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.gemm' failed: Not implemented on ROCm. CC(Roll forward with fix: Remove the original python function `fun_` from C++ PjitFunction, as the destroying `fun_` may yield the thread in some cases, which causes error during deleting the python object of PjitFunction.)  ```  What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? GPU  Additional system info Linux e5fc65f21cda 6.3.1zen11zen CC(Python 3 compatibility issues) ZEN SMP PREEMPT_DYNAMIC Mon, 01 May 2023 17:42:12 +0000 x86_64 x86_64 x86_64 GNU/Linux  NVIDIA GPU info rocmsmi ``` ======================= ROCm System Management Interface ======================= ================================= Concise Info ================================= GPU  Temp (DieEdge)  AvgPwr  SCLK    MCLK     Fan     Perf  PwrCap  VRAM%  GPU% 0    55.0c           26.0W   500Mhz  1000Mhz  32.55%  auto  211.0W   16%   0% ================================================================================ ============================= End of ROCm SMI Log ============================== ``` rocminfo ``` ROCk module is loaded ===================== HSA System Attributes ===================== Runtime Version:         1.1 System Timestamp Freq.:  1000.000000MHz Sig. Max Wait Duration:  18446744073709551615 (0xFFFFFFFFFFFFFFFF) (timestamp count) Machine Model:           LARGE System Endianness:       LITTLE ========== HSA Agents ========== ******* Agent 1 ******* Name:                    AMD Ryzen 9 5950X 16Core Processor Uuid:                    CPUXX Marketing Name:          AMD Ryzen 9 5950X 16Core Processor Vendor Name:             CPU Feature:                 None specified Profile:                 FULL_PROFILE Float Round Mode:        NEAR Max Queue Number:        0(0x0) Queue Min Size:          0(0x0) Queue Max Size:          0(0x0) Queue Type:              MULTI Node:                    0 Device Type:             CPU Cache Info: L1:                      32768(0x8000) KB Chip ID:                 0(0x0) ASIC Revision:           0(0x0) Cacheline Size:          64(0x40) Max Clock Freq. (MHz):   3400 BDFID:                   0 Internal Node ID:        0 Compute Unit:            32 SIMDs per CU:            0 Shader Engines:          0 Shader Arrs. per Eng.:   0 WatchPts on Addr. Ranges:1 Features:                None Pool Info: Pool 1 Segment:                 GLOBAL; FLAGS: FINE GRAINED Size:                    32771392(0x1f40d40) KB Allocatable:             TRUE Alloc Granule:           4KB Alloc Alignment:         4KB Accessible by all:       TRUE Pool 2 Segment:                 GLOBAL; FLAGS: KERNARG, FINE GRAINED Size:                    32771392(0x1f40d40) KB Allocatable:             TRUE Alloc Granule:           4KB Alloc Alignment:         4KB Accessible by all:       TRUE Pool 3 Segment:                 GLOBAL; FLAGS: COARSE GRAINED Size:                    32771392(0x1f40d40) KB Allocatable:             TRUE Alloc Granule:           4KB Alloc Alignment:         4KB Accessible by all:       TRUE ISA Info: ******* Agent 2 ******* Name:                    gfx1030 Uuid:                    GPUXX Marketing Name:          AMD Radeon RX 6700 XT Vendor Name:             AMD Feature:                 KERNEL_DISPATCH Profile:                 BASE_PROFILE Float Round Mode:        NEAR Max Queue Number:        128(0x80) Queue Min Size:          64(0x40) Queue Max Size:          131072(0x20000) Queue Type:              MULTI Node:                    1 Device Type:             GPU Cache Info: L1:                      16(0x10) KB L2:                      3072(0xc00) KB L3:                      98304(0x18000) KB Chip ID:                 29663(0x73df) ASIC Revision:           0(0x0) Cacheline Size:          64(0x40) Max Clock Freq. (MHz):   2855 BDFID:                   2816 Internal Node ID:        1 Compute Unit:            40 SIMDs per CU:            2 Shader Engines:          4 Shader Arrs. per Eng.:   2 WatchPts on Addr. Ranges:4 Features:                KERNEL_DISPATCH Fast F16 Operation:      TRUE Wavefront Size:          32(0x20) Workgroup Max Size:      1024(0x400) Workgroup Max Size per Dimension: x                        1024(0x400) y                        1024(0x400) z                        1024(0x400) Max Waves Per CU:        32(0x20) Max Workitem Per CU:    1024(0x400) Grid Max Size:           4294967295(0xffffffff) Grid Max Size per Dimension: x                        4294967295(0xffffffff) y                        4294967295(0xffffffff) z                        4294967295(0xffffffff) Max fbarriers/Workgrp:   32 Pool Info: Pool 1 Segment:                 GLOBAL; FLAGS: COARSE GRAINED Size:                    12566528(0xbfc000) KB Allocatable:             TRUE Alloc Granule:           4KB Alloc Alignment:         4KB Accessible by all:       FALSE Pool 2 Segment:                 GROUP Size:                    64(0x40) KB Allocatable:             FALSE Alloc Granule:           0KB Alloc Alignment:         0KB Accessible by all:       FALSE ISA Info: ISA 1 Name:                    amdgcnamdamdhsagfx1030 Machine Models:          HSA_MACHINE_MODEL_LARGE Profiles:                HSA_PROFILE_BASE Default Rounding Mode:   NEAR Default Rounding Mode:   NEAR Fast f16:                TRUE Workgroup Max Size:      1024(0x400) Workgroup Max Size per Dimension: x                        1024(0x400) y                        1024(0x400) z                        1024(0x400) Grid Max Size:           4294967295(0xffffffff) Grid Max Size per Dimension: x                        4294967295(0xffffffff) y                        4294967295(0xffffffff) z                        4294967295(0xffffffff) FBarrier Max Size:       32 *** Done *** ```",2023-05-12T06:12:08Z,bug contributions welcome AMD GPU,closed,0,8,https://github.com/jax-ml/jax/issues/15983,"Are JAX tracers limited to optimizing python calls to XLA? Or does the trace analysis select from binding to different possible XLA functionality based on the JIT optimization process? If it does, then my comment about pmap is totally wrong. I see that XLA optimizes or interacts with PTX bytecode. And I've found the super helpful `autodidax.md`. Can code written with JAX benefit from AOT compilation?  Anyways, I'm just glad I can run the code on my GPU.","amd amd can you perhaps take a look? Note that we (Google) don't ship a ROCM build of JAX: one main reason for this is we have no way to test it. Without CI testing it is expected that it will break frequently. The ROCM build is community supported: we welcome PRs to fix it, but it's something the community maintains.",/,"This is a bit of a scattershot issue description but if you are having trouble building, there are some prebuilt options: Dockerhub:      https://hub.docker.com/r/rocm/jaxbuild whl files:      https://pypi.org/project/jaxlibrocm/     https://github.com/ROCmSoftwarePlatform/jax/releases/download/jaxlibv0.4.6rocm55/jaxlib0.4.6.550cp39cp39manylinux2014_x86_64.whl Also note the consumer boards are not strictly supported by ROCM at this point, but generally should work. ","thanks for responding. sorry, I'm just a bit frustrated. i'm finally able to use my new computer for programming and i'm running into issues.  I started to use the rocm/jaxbuild image, but the python i end up with can't run jupyter. it wasn't built with sqlite. here's the repository i'm running with: dcunited001/nbjax I think I can just add the wheels in on top of the rcom/tensorflow image. i'll try again and update the issue.  is there a custom branch with changes to XLA/JAX that the jaxlibrocm wheel and rocm/jaxbuild image are built with? ... ah ok, there are other release tags. i had only checked the main branch. i'm a bit new to python, so I just hadn't thought about releasing the wheels as release artifacts. also,  i would create a pull request, but it's a thin set of changes. i wasn't sure what version of numpy should be specified in `Dockerfile.ms`. I spec'd `1.21.6`, but I ended up with `1.24.3` after the build which is downgraded to `1.22.4` after running a few pip installs.","So I've got the docker image running with jaxlibrocm, tensorflowrocm and jupyter.  I'm starting with `JAX_PLATFORMS=cpu,rocm jupyterlab`, but when I run this: ```python  this returns CPU/GPU devices import tensorflow as tf tf.config.list_physical_devices()  this either gives a ""No GPU/TPU"" warning or throws the error below import jax.numpy as jnp from jax import config as jcfg from jax import grad, jit, vmap from jax import random key = random.PRNGKey(0) jcfg.jax_platforms ``` Then I'm getting the error below I think I need to check out the ROCmSoftwarePlatform/jax.4.6 tag to figure out how to get past this error. ``` File ~/.local/lib/python3.9/sitepackages/jax/_src/lib/xla_bridge.py:350, in backends()     348 if config.jax_platforms:     349   err_msg += "" (set JAX_PLATFORMS='' to automatically choose an available backend)"" > 350   raise RuntimeError(err_msg)     351 else:     352   _backends_errors[platform] = str(err) RuntimeError: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (set JAX_PLATFORMS='' to automatically choose an available backend) ``` I'm trying to resolve by tracing through these references/modules in the jax codebase. I could definitely use help, but I should be able to figure this stuff out.  ```python from jax._src.lib import xla_client from jax._src.lib import xla_bridge from jax._src.config import flags FLAGS = flags.FLAGS FLAGS.jax_platflorm_name xla_bridge.is_known_platform('rocm')  True xla_client.CompileOptions() xla_bridge.backends() ```",**Try this wheel** https://github.com/ROCmSoftwarePlatform/jax/releases/download/jaxlibv0.4.6rocm55/jaxlib0.4.6.550cp39cp39manylinux2014_x86_64.whl P.S. Building jaxlib for ROCm is a bit tricky. You also need to pull in XLA changes to build and there is ROCm fork for that as well.,"thanks   one of the biggest problems i've had in figuring out how to use the ROCm stuff is trying to learn how the builds/dependencies differ between the upstream tensorflow/etc repositories and the ROCmSoftwarePlatform repositories. i think diffing across the repositories checked out to specific repo's helps alot. the process i'm using is in orgbabel scripts here. using repo is a little heavy handed, but it just makes managing the repository clones easier. the resulting diff here jaxrocm046.diff. i fixed the link. i'm not sure if it's complete, since it gives an error code of 1."
8367,"以下是一个github上的jax下的一个issue, 标题是(grad of shard_map of lax.all_to_all crashes)， 内容是 ( Description The forward transformation works, but not the backwards pass:  ```python import functools import chex import jax from jax.experimental import shard_map from jax import lax import jax.numpy as jnp import numpy as np chex.set_n_cpu_devices(4) P = jax.sharding.PartitionSpec shmap = shard_map.shard_map mesh = jax.sharding.Mesh(jax.devices(), axis_names=['x']) .partial(     shmap, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x') ) def reshard(x):   return lax.all_to_all(x, 'x', split_axis=1, concat_axis=0, tiled=True) def loss(x):   return reshard(x).sum() in_sharding = jax.sharding.NamedSharding(mesh, P('x', None)) x = jax.device_put(np.arange(64).reshape(8, 8), in_sharding) print(x.sharding)   NamedSharding(mesh={'x': 4}, spec=PartitionSpec('x', None)) y = reshard(x) print(y.sharding)   NamedSharding(mesh={'x': 4}, spec=PartitionSpec(None, 'x')) np.testing.assert_array_equal(y, x) print(jax.grad(loss)(1.0 * x)) ``` `ValueError: all_to_all requires the size of the mapped axis axis_name to equal x.shape[split_axis], but they are 4 and 8 respectively.`  ``` ValueError                                Traceback (most recent call last)  in ()      32 np.testing.assert_array_equal(y, x)      33  > 34 print(jax.grad(loss)(1.0 * x))   errors third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     164     __tracebackhide__ = True     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e:     168       mode = _filtering_mode() third_party/py/jax/_src/api.py in grad_f(*args, **kwargs)     644        645   def grad_f(*args, **kwargs): > 646     _, g = value_and_grad_f(*args, **kwargs)     647     return g     648  third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     164     __tracebackhide__ = True     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e:     168       mode = _filtering_mode() third_party/py/jax/_src/api.py in value_and_grad_f(*args, **kwargs)     726     _check_scalar(ans)     727     tree_map(partial(_check_output_dtype_grad, holomorphic), ans) > 728     g = vjp_py(lax_internal._one(ans))     729     g = g[0] if isinstance(argnums, int) else g     730     if not has_aux: third_party/py/jax/_src/tree_util.py in __call__(self, *args, **kw)     301      302   def __call__(self, *args, **kw): > 303     return self.fun(*args, **kw)     304      305   def __hash__(self): third_party/py/jax/_src/api.py in _vjp_pullback_wrapper(name, cotangent_dtypes, cotangent_shapes, io_tree, fun, *py_args_)    2102           ""must be the same as the shape of corresponding primal input ""    2103           f""{ct_shape}."") > 2104   ans = fun(*args)    2105   return tree_unflatten(out_tree, ans)    2106  third_party/py/jax/_src/tree_util.py in __call__(self, *args, **kw)     301      302   def __call__(self, *args, **kw): > 303     return self.fun(*args, **kw)     304      305   def __hash__(self): third_party/py/jax/_src/interpreters/ad.py in unbound_vjp(pvals, jaxpr, consts, *cts)     144     cts = tuple(ct for ct, pval in zip(cts, pvals) if not pval.is_known())     145     dummy_args = [UndefinedPrimal(v.aval) for v in jaxpr.invars] > 146     arg_cts = backward_pass(jaxpr, reduce_axes, True, consts, dummy_args, cts)     147     return map(instantiate_zeros, arg_cts)     148  third_party/py/jax/_src/interpreters/ad.py in backward_pass(jaxpr, reduce_axes, transform_stack, consts, primals_in, cotangents_in)     251               reduce_axes, cts_in, *invals, **eqn.params)     252         else: > 253           cts_out = get_primitive_transpose(eqn.primitive)(     254               cts_in, *invals, **eqn.params)     255         cts_out = [Zero(v.aval) for v in eqn.invars] if cts_out is Zero else cts_out third_party/py/jax/experimental/shard_map.py in _shard_map_transpose(out_cts, jaxpr, mesh, in_names, out_names, check_rep, *args)    1061     return tuple(names for names, nz in zip(in_names, nz_arg_cts()) if nz)    1062  > 1063   out_flat = shard_map_p.bind(    1064       fun_trans_flat, *all_args, mesh=mesh, in_names=tuple(new_in_names),    1065       out_names_thunk=new_out_names_thunk, check_rep=check_rep) third_party/py/jax/experimental/shard_map.py in bind(self, fun, mesh, in_names, out_names_thunk, check_rep, *args)     345      346     tracers = map(top_trace.full_raise, args) > 347     outs = top_trace.process_shard_map(   pytype: disable=attributeerror     348         shard_map_p, fun, tracers, mesh=mesh, in_names=in_names,     349         out_names_thunk=new_out_names_thunk, check_rep=check_rep) third_party/py/jax/experimental/shard_map.py in _shard_map_impl(***failed resolving arguments***)     552       t = main.with_cur_sublevel()     553       in_tracers = map(partial(ShardMapTracer, t), in_rep, args) > 554       ans = fun.call_wrapped(*in_tracers)     555       out_tracers = map(t.full_raise, ans)     556       outs_, out_rep = unzip2((t.val, t.rep) for t in out_tracers) third_party/py/jax/_src/linear_util.py in call_wrapped(self, *args, **kwargs)     186      187     try: > 188       ans = self.f(*args, **dict(self.params, **kwargs))     189     except:     190        Some transformations yield from inside context managers, so we have to third_party/py/jax/experimental/shard_map.py in fun_trans(out_cts, args)    1044         pe.close_jaxpr(jaxpr), map(ad.is_undefined_primal, args), False)    1045     res_reshaped = core.jaxpr_as_fun(jaxpr_known)(*res) > 1046     out = ad.backward_pass(    1047         jaxpr_unknown.jaxpr, (), False, (), (*res_reshaped, *undefs), out_cts    1048     ) third_party/py/jax/_src/interpreters/ad.py in backward_pass(jaxpr, reduce_axes, transform_stack, consts, primals_in, cotangents_in)     251               reduce_axes, cts_in, *invals, **eqn.params)     252         else: > 253           cts_out = get_primitive_transpose(eqn.primitive)(     254               cts_in, *invals, **eqn.params)     255         cts_out = [Zero(v.aval) for v in eqn.invars] if cts_out is Zero else cts_out third_party/py/jax/_src/interpreters/ad.py in linear_transpose2(transpose_rule, cotangent, *args, **kwargs)     518      519 def linear_transpose2(transpose_rule, cotangent, *args, **kwargs): > 520   return Zero if type(cotangent) is Zero else transpose_rule(cotangent, *args, **kwargs)     521      522  third_party/py/jax/_src/lax/parallel.py in _all_to_all_transpose_rule(cts, x, axis_name, split_axis, concat_axis, axis_index_groups)     993      994 def _all_to_all_transpose_rule(cts, x, axis_name, split_axis, concat_axis, axis_index_groups): > 995   return (all_to_all(     996       cts,     997       axis_name=axis_name, third_party/py/jax/_src/lax/parallel.py in all_to_all(x, axis_name, split_axis, concat_axis, axis_index_groups, tiled)     403     return result     404  > 405   return tree_util.tree_map(bind, x)     406      407 def axis_index(axis_name): third_party/py/jax/_src/tree_util.py in tree_map(f, tree, is_leaf, *rest)     208   leaves, treedef = tree_flatten(tree, is_leaf)     209   all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest] > 210   return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))     211      212 def build_tree(treedef: PyTreeDef, xs: Any) > Any: third_party/py/jax/_src/tree_util.py in (.0)     208   leaves, treedef = tree_flatten(tree, is_leaf)     209   all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest] > 210   return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))     211      212 def build_tree(treedef: PyTreeDef, xs: Any) > Any: third_party/py/jax/_src/lax/parallel.py in bind(x, split_axis, concat_axis)     387         msg = (""all_to_all requires the size of the mapped axis axis_name to ""     388                ""equal x.shape[split_axis], but they are {} and {} respectively."") > 389         raise ValueError(msg.format(group_size, x.shape[split_axis]))     390       if split_axis   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,grad of shard_map of lax.all_to_all crashes," Description The forward transformation works, but not the backwards pass:  ```python import functools import chex import jax from jax.experimental import shard_map from jax import lax import jax.numpy as jnp import numpy as np chex.set_n_cpu_devices(4) P = jax.sharding.PartitionSpec shmap = shard_map.shard_map mesh = jax.sharding.Mesh(jax.devices(), axis_names=['x']) .partial(     shmap, mesh=mesh, in_specs=(P('x', None),), out_specs=P(None, 'x') ) def reshard(x):   return lax.all_to_all(x, 'x', split_axis=1, concat_axis=0, tiled=True) def loss(x):   return reshard(x).sum() in_sharding = jax.sharding.NamedSharding(mesh, P('x', None)) x = jax.device_put(np.arange(64).reshape(8, 8), in_sharding) print(x.sharding)   NamedSharding(mesh={'x': 4}, spec=PartitionSpec('x', None)) y = reshard(x) print(y.sharding)   NamedSharding(mesh={'x': 4}, spec=PartitionSpec(None, 'x')) np.testing.assert_array_equal(y, x) print(jax.grad(loss)(1.0 * x)) ``` `ValueError: all_to_all requires the size of the mapped axis axis_name to equal x.shape[split_axis], but they are 4 and 8 respectively.`  ``` ValueError                                Traceback (most recent call last)  in ()      32 np.testing.assert_array_equal(y, x)      33  > 34 print(jax.grad(loss)(1.0 * x))   errors third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     164     __tracebackhide__ = True     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e:     168       mode = _filtering_mode() third_party/py/jax/_src/api.py in grad_f(*args, **kwargs)     644        645   def grad_f(*args, **kwargs): > 646     _, g = value_and_grad_f(*args, **kwargs)     647     return g     648  third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     164     __tracebackhide__ = True     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e:     168       mode = _filtering_mode() third_party/py/jax/_src/api.py in value_and_grad_f(*args, **kwargs)     726     _check_scalar(ans)     727     tree_map(partial(_check_output_dtype_grad, holomorphic), ans) > 728     g = vjp_py(lax_internal._one(ans))     729     g = g[0] if isinstance(argnums, int) else g     730     if not has_aux: third_party/py/jax/_src/tree_util.py in __call__(self, *args, **kw)     301      302   def __call__(self, *args, **kw): > 303     return self.fun(*args, **kw)     304      305   def __hash__(self): third_party/py/jax/_src/api.py in _vjp_pullback_wrapper(name, cotangent_dtypes, cotangent_shapes, io_tree, fun, *py_args_)    2102           ""must be the same as the shape of corresponding primal input ""    2103           f""{ct_shape}."") > 2104   ans = fun(*args)    2105   return tree_unflatten(out_tree, ans)    2106  third_party/py/jax/_src/tree_util.py in __call__(self, *args, **kw)     301      302   def __call__(self, *args, **kw): > 303     return self.fun(*args, **kw)     304      305   def __hash__(self): third_party/py/jax/_src/interpreters/ad.py in unbound_vjp(pvals, jaxpr, consts, *cts)     144     cts = tuple(ct for ct, pval in zip(cts, pvals) if not pval.is_known())     145     dummy_args = [UndefinedPrimal(v.aval) for v in jaxpr.invars] > 146     arg_cts = backward_pass(jaxpr, reduce_axes, True, consts, dummy_args, cts)     147     return map(instantiate_zeros, arg_cts)     148  third_party/py/jax/_src/interpreters/ad.py in backward_pass(jaxpr, reduce_axes, transform_stack, consts, primals_in, cotangents_in)     251               reduce_axes, cts_in, *invals, **eqn.params)     252         else: > 253           cts_out = get_primitive_transpose(eqn.primitive)(     254               cts_in, *invals, **eqn.params)     255         cts_out = [Zero(v.aval) for v in eqn.invars] if cts_out is Zero else cts_out third_party/py/jax/experimental/shard_map.py in _shard_map_transpose(out_cts, jaxpr, mesh, in_names, out_names, check_rep, *args)    1061     return tuple(names for names, nz in zip(in_names, nz_arg_cts()) if nz)    1062  > 1063   out_flat = shard_map_p.bind(    1064       fun_trans_flat, *all_args, mesh=mesh, in_names=tuple(new_in_names),    1065       out_names_thunk=new_out_names_thunk, check_rep=check_rep) third_party/py/jax/experimental/shard_map.py in bind(self, fun, mesh, in_names, out_names_thunk, check_rep, *args)     345      346     tracers = map(top_trace.full_raise, args) > 347     outs = top_trace.process_shard_map(   pytype: disable=attributeerror     348         shard_map_p, fun, tracers, mesh=mesh, in_names=in_names,     349         out_names_thunk=new_out_names_thunk, check_rep=check_rep) third_party/py/jax/experimental/shard_map.py in _shard_map_impl(***failed resolving arguments***)     552       t = main.with_cur_sublevel()     553       in_tracers = map(partial(ShardMapTracer, t), in_rep, args) > 554       ans = fun.call_wrapped(*in_tracers)     555       out_tracers = map(t.full_raise, ans)     556       outs_, out_rep = unzip2((t.val, t.rep) for t in out_tracers) third_party/py/jax/_src/linear_util.py in call_wrapped(self, *args, **kwargs)     186      187     try: > 188       ans = self.f(*args, **dict(self.params, **kwargs))     189     except:     190        Some transformations yield from inside context managers, so we have to third_party/py/jax/experimental/shard_map.py in fun_trans(out_cts, args)    1044         pe.close_jaxpr(jaxpr), map(ad.is_undefined_primal, args), False)    1045     res_reshaped = core.jaxpr_as_fun(jaxpr_known)(*res) > 1046     out = ad.backward_pass(    1047         jaxpr_unknown.jaxpr, (), False, (), (*res_reshaped, *undefs), out_cts    1048     ) third_party/py/jax/_src/interpreters/ad.py in backward_pass(jaxpr, reduce_axes, transform_stack, consts, primals_in, cotangents_in)     251               reduce_axes, cts_in, *invals, **eqn.params)     252         else: > 253           cts_out = get_primitive_transpose(eqn.primitive)(     254               cts_in, *invals, **eqn.params)     255         cts_out = [Zero(v.aval) for v in eqn.invars] if cts_out is Zero else cts_out third_party/py/jax/_src/interpreters/ad.py in linear_transpose2(transpose_rule, cotangent, *args, **kwargs)     518      519 def linear_transpose2(transpose_rule, cotangent, *args, **kwargs): > 520   return Zero if type(cotangent) is Zero else transpose_rule(cotangent, *args, **kwargs)     521      522  third_party/py/jax/_src/lax/parallel.py in _all_to_all_transpose_rule(cts, x, axis_name, split_axis, concat_axis, axis_index_groups)     993      994 def _all_to_all_transpose_rule(cts, x, axis_name, split_axis, concat_axis, axis_index_groups): > 995   return (all_to_all(     996       cts,     997       axis_name=axis_name, third_party/py/jax/_src/lax/parallel.py in all_to_all(x, axis_name, split_axis, concat_axis, axis_index_groups, tiled)     403     return result     404  > 405   return tree_util.tree_map(bind, x)     406      407 def axis_index(axis_name): third_party/py/jax/_src/tree_util.py in tree_map(f, tree, is_leaf, *rest)     208   leaves, treedef = tree_flatten(tree, is_leaf)     209   all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest] > 210   return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))     211      212 def build_tree(treedef: PyTreeDef, xs: Any) > Any: third_party/py/jax/_src/tree_util.py in (.0)     208   leaves, treedef = tree_flatten(tree, is_leaf)     209   all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest] > 210   return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))     211      212 def build_tree(treedef: PyTreeDef, xs: Any) > Any: third_party/py/jax/_src/lax/parallel.py in bind(x, split_axis, concat_axis)     387         msg = (""all_to_all requires the size of the mapped axis axis_name to ""     388                ""equal x.shape[split_axis], but they are {} and {} respectively."") > 389         raise ValueError(msg.format(group_size, x.shape[split_axis]))     390       if split_axis   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-12T05:48:02Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/15982
662,"以下是一个github上的jax下的一个issue, 标题是([ANN] Update approx_{max,min}_k jvp to make it more TPU-friendly.)， 内容是 (Scatters/gathers are very expensive, so instead we take a dot product with the appropriate onehot representation of the indices on the tangent. Also: * Updated tests to consider varying reduction_dimension * Removed possibility of cyclic dependency in debugging.py by removing a large jnp import, replacing it with a numpy_lax import. Speeds up jvp of T5X XLsized dot product, followed by approx_max_k, by 50x on TPU.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[ANN] Update approx_{max,min}_k jvp to make it more TPU-friendly.","Scatters/gathers are very expensive, so instead we take a dot product with the appropriate onehot representation of the indices on the tangent. Also: * Updated tests to consider varying reduction_dimension * Removed possibility of cyclic dependency in debugging.py by removing a large jnp import, replacing it with a numpy_lax import. Speeds up jvp of T5X XLsized dot product, followed by approx_max_k, by 50x on TPU.",2023-05-11T21:31:16Z,pull ready,open,0,3,https://github.com/jax-ml/jax/issues/15977,"How do I replicate this failure using pytests? ``` [ RUN      ] AnnTest.test_autodiff24 (dtype=, shape=(2, 1, 4), reduction_dimension=1, k=3, is_max_k=False) [  FAILED  ] AnnTest.test_autodiff24 (dtype=, shape=(2, 1, 4), reduction_dimension=1, k=3, is_max_k=False) ```",Nevermind; I was able to replicate.  Debugging.,``` export JAX_NUM_GENERATED_CASES=50 pytest n auto tests/ann_test.py k AnnTest ``` Now passes.  Rebased my PR.
8427,"以下是一个github上的jax下的一个issue, 标题是(Results mismatch between different convolution algorithms (CuDNN 8.3))， 内容是 ( Description I've been encountering a CuDNN lossofprecision error with a JAX/Flax script (error log below). Strangely, the script worked before and then I started getting this error even though no system changes have been made. Perhaps relatedly, the same script has been running into OOM errors even though it didn't before. I'm running on 4x NVIDIA RTX A6000 GPUs from a Singularity container (.def file pasted below) that has CUDA 11.6 / cuDNN 8.3 installed. I've also tested on cuDNN 8.4 and run into the same issue. Any help would be greatly appreciated! Log: ``` 20230511 02:45:27.996215: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 0: 4.29605e+06 vs 3.50164e+06 20230511 02:45:27.996275: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 1: 4.29557e+06 vs 3.50089e+06 20230511 02:45:27.996283: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 2: 4.29567e+06 vs 3.50124e+06 20230511 02:45:27.996290: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 3: 4.29613e+06 vs 3.50168e+06 20230511 02:45:27.996298: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 4: 4.29621e+06 vs 3.50163e+06 20230511 02:45:27.996304: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 5: 4.29554e+06 vs 3.50056e+06 20230511 02:45:27.996312: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 6: 4.2959e+06 vs 3.50146e+06 20230511 02:45:27.996320: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 7: 4.29521e+06 vs 3.50084e+06 20230511 02:45:27.996330: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 8: 4.29673e+06 vs 3.50221e+06 20230511 02:45:27.996337: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 9: 4.29579e+06 vs 3.50107e+06 20230511 02:45:27.996352: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:764] Results mismatch between different convolution algorithms. This is likely a bug/unexpected loss of precision in cudnn. %cudnnconvbwfilter.165 = (f32[1,1,4,4]{3,2,1,0}, u8[0]{0}) customcall(f32[1024,1,132,132]{3,2,1,0} %bitcast.78862, f32[1024,1,129,129]{3,2,1,0} %bitcast.78868), window={size=4x4}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_name=""pmap(step_fn)/jit(main)/jit(step_fn)/transpose(jvp(checkpointedbwd))/while/body/cond/branch_1_fun/transpose(jvp(checkpointedbwd))/while/body/cond/branch_1_fun/transpose(jvp(NCSNpp))/Downsample_1/Conv2d_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(1, 0, 2, 3), rhs_spec=(1, 0, 2, 3), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]"" source_file=""/WORKDIR/models/up_or_down_sampling.py"" source_line=280}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" for eng20{k2=6,k3=0} vs eng57{k2=5,k13=1,k14=2,k18=1,k23=0} 20230511 02:45:27.996365: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:250] Device: NVIDIA RTX A6000 20230511 02:45:27.996371: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:251] Platform: Compute Capability 8.6 20230511 02:45:27.996377: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:252] Driver: 11060 (INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"") 20230511 02:45:27.996383: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:253] Runtime:  20230511 02:45:27.996392: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:260] cudnn version: 8.3.2 20230511 02:45:51.545059: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 0: 8.45972e+06 vs 5.71454e+06 20230511 02:45:51.545125: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 1: 8.459e+06 vs 5.71383e+06 20230511 02:45:51.545132: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 2: 8.45956e+06 vs 5.7141e+06 20230511 02:45:51.545140: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 3: 8.45975e+06 vs 5.71441e+06 20230511 02:45:51.545148: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 4: 8.45838e+06 vs 5.71266e+06 20230511 02:45:51.545155: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 5: 8.45897e+06 vs 5.71342e+06 20230511 02:45:51.545161: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 6: 8.45842e+06 vs 5.7137e+06 20230511 02:45:51.545172: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 7: 8.4587e+06 vs 5.7134e+06 20230511 02:45:51.545181: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 8: 8.45907e+06 vs 5.71338e+06 20230511 02:45:51.545188: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 9: 8.45827e+06 vs 5.71331e+06 20230511 02:45:51.545211: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:764] Results mismatch between different convolution algorithms. This is likely a bug/unexpected loss of precision in cudnn. %cudnnconvbwfilter.229 = (f32[1,1,4,4]{3,2,1,0}, u8[0]{0}) customcall(f32[2048,1,131,131]{3,2,1,0} %bitcast.83451, f32[2048,1,128,128]{3,2,1,0} %bitcast.83457), window={size=4x4}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_name=""pmap(step_fn)/jit(main)/jit(step_fn)/transpose(jvp(checkpointedbwd))/while/body/cond/branch_1_fun/transpose(jvp(checkpointedbwd))/while/body/cond/branch_1_fun/transpose(jvp(NCSNpp))/ResnetBlockBigGANpp_32/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(1, 0, 2, 3), rhs_spec=(1, 0, 2, 3), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]"" source_file=""/WORKDIR/score_flow/models/up_or_down_sampling.py"" source_line=280}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" for eng20{k2=6,k3=0} vs eng57{k2=5,k13=1,k14=2,k18=1,k23=0} 20230511 02:45:51.545224: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:250] Device: NVIDIA RTX A6000 20230511 02:45:51.545233: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:251] Platform: Compute Capability 8.6 20230511 02:45:51.545239: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:252] Driver: 11060 (INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"") 20230511 02:45:51.545245: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:253] Runtime:  20230511 02:45:51.545254: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:260] cudnn version: 8.3.2 ``` Singularity .def: ``` bootstrap: docker from: nvcr.io/nvidia/tensorflow:22.02tf2py3  Python 3.8  CUDA 11.6.0  cuDNN 8.3.2  TensorFlow 2.7.0  TensorBoard 2.7.0 %post   export DEBIAN_FRONTEND=noninteractive   aptget update   aptget y upgrade    Install git and pip   aptget y install gitall   aptget y install python3pip   aptget y install wget    Pip installations.   pip install upgrade pip   pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html   pip install flax equinox diffrax optax   pip install tensorflowprobability==0.15.0 tensorflowdatasets   pip install tensorflow_io   pip install mlcollections   pip install Pillow   pip install scikitlearn   pip install scikitimage   pip install matplotlib   pip install scipy   pip install imageio   pip install notebook   pip install ipywidgets   pip install astropy   pip install seaborn   unset DEBIAN_FRONTEND %environment    export XLA_FLAGS=xla_gpu_cuda_data_dir=/usr/lib/cuda   export TF_FORCE_GPU_ALLOW_GROWTH=true   export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH   export PATH=/opt/conda/bin:$PATH   export LC_ALL=C ```  What jax/jaxlib version are you using? jax v0.4.8  Which accelerator(s) are you using? GPU  Additional system info Python 3.8, CUDA 11.60, cuDNN 8.3.2  NVIDIA GPU info ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Results mismatch between different convolution algorithms (CuDNN 8.3)," Description I've been encountering a CuDNN lossofprecision error with a JAX/Flax script (error log below). Strangely, the script worked before and then I started getting this error even though no system changes have been made. Perhaps relatedly, the same script has been running into OOM errors even though it didn't before. I'm running on 4x NVIDIA RTX A6000 GPUs from a Singularity container (.def file pasted below) that has CUDA 11.6 / cuDNN 8.3 installed. I've also tested on cuDNN 8.4 and run into the same issue. Any help would be greatly appreciated! Log: ``` 20230511 02:45:27.996215: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 0: 4.29605e+06 vs 3.50164e+06 20230511 02:45:27.996275: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 1: 4.29557e+06 vs 3.50089e+06 20230511 02:45:27.996283: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 2: 4.29567e+06 vs 3.50124e+06 20230511 02:45:27.996290: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 3: 4.29613e+06 vs 3.50168e+06 20230511 02:45:27.996298: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 4: 4.29621e+06 vs 3.50163e+06 20230511 02:45:27.996304: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 5: 4.29554e+06 vs 3.50056e+06 20230511 02:45:27.996312: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 6: 4.2959e+06 vs 3.50146e+06 20230511 02:45:27.996320: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 7: 4.29521e+06 vs 3.50084e+06 20230511 02:45:27.996330: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 8: 4.29673e+06 vs 3.50221e+06 20230511 02:45:27.996337: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 9: 4.29579e+06 vs 3.50107e+06 20230511 02:45:27.996352: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:764] Results mismatch between different convolution algorithms. This is likely a bug/unexpected loss of precision in cudnn. %cudnnconvbwfilter.165 = (f32[1,1,4,4]{3,2,1,0}, u8[0]{0}) customcall(f32[1024,1,132,132]{3,2,1,0} %bitcast.78862, f32[1024,1,129,129]{3,2,1,0} %bitcast.78868), window={size=4x4}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_name=""pmap(step_fn)/jit(main)/jit(step_fn)/transpose(jvp(checkpointedbwd))/while/body/cond/branch_1_fun/transpose(jvp(checkpointedbwd))/while/body/cond/branch_1_fun/transpose(jvp(NCSNpp))/Downsample_1/Conv2d_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(1, 0, 2, 3), rhs_spec=(1, 0, 2, 3), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]"" source_file=""/WORKDIR/models/up_or_down_sampling.py"" source_line=280}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" for eng20{k2=6,k3=0} vs eng57{k2=5,k13=1,k14=2,k18=1,k23=0} 20230511 02:45:27.996365: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:250] Device: NVIDIA RTX A6000 20230511 02:45:27.996371: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:251] Platform: Compute Capability 8.6 20230511 02:45:27.996377: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:252] Driver: 11060 (INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"") 20230511 02:45:27.996383: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:253] Runtime:  20230511 02:45:27.996392: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:260] cudnn version: 8.3.2 20230511 02:45:51.545059: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 0: 8.45972e+06 vs 5.71454e+06 20230511 02:45:51.545125: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 1: 8.459e+06 vs 5.71383e+06 20230511 02:45:51.545132: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 2: 8.45956e+06 vs 5.7141e+06 20230511 02:45:51.545140: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 3: 8.45975e+06 vs 5.71441e+06 20230511 02:45:51.545148: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 4: 8.45838e+06 vs 5.71266e+06 20230511 02:45:51.545155: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 5: 8.45897e+06 vs 5.71342e+06 20230511 02:45:51.545161: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 6: 8.45842e+06 vs 5.7137e+06 20230511 02:45:51.545172: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 7: 8.4587e+06 vs 5.7134e+06 20230511 02:45:51.545181: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 8: 8.45907e+06 vs 5.71338e+06 20230511 02:45:51.545188: E external/xla/xla/service/gpu/buffer_comparator.cc:731] Difference at 9: 8.45827e+06 vs 5.71331e+06 20230511 02:45:51.545211: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:764] Results mismatch between different convolution algorithms. This is likely a bug/unexpected loss of precision in cudnn. %cudnnconvbwfilter.229 = (f32[1,1,4,4]{3,2,1,0}, u8[0]{0}) customcall(f32[2048,1,131,131]{3,2,1,0} %bitcast.83451, f32[2048,1,128,128]{3,2,1,0} %bitcast.83457), window={size=4x4}, dim_labels=bf01_oi01>bf01, custom_call_target=""__cudnn$convBackwardFilter"", metadata={op_name=""pmap(step_fn)/jit(main)/jit(step_fn)/transpose(jvp(checkpointedbwd))/while/body/cond/branch_1_fun/transpose(jvp(checkpointedbwd))/while/body/cond/branch_1_fun/transpose(jvp(NCSNpp))/ResnetBlockBigGANpp_32/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(1, 0, 2, 3), rhs_spec=(1, 0, 2, 3), out_spec=(2, 3, 0, 1)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]"" source_file=""/WORKDIR/score_flow/models/up_or_down_sampling.py"" source_line=280}, backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}"" for eng20{k2=6,k3=0} vs eng57{k2=5,k13=1,k14=2,k18=1,k23=0} 20230511 02:45:51.545224: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:250] Device: NVIDIA RTX A6000 20230511 02:45:51.545233: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:251] Platform: Compute Capability 8.6 20230511 02:45:51.545239: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:252] Driver: 11060 (INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1"") 20230511 02:45:51.545245: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:253] Runtime:  20230511 02:45:51.545254: E external/xla/xla/service/gpu/gpu_conv_algorithm_picker.cc:260] cudnn version: 8.3.2 ``` Singularity .def: ``` bootstrap: docker from: nvcr.io/nvidia/tensorflow:22.02tf2py3  Python 3.8  CUDA 11.6.0  cuDNN 8.3.2  TensorFlow 2.7.0  TensorBoard 2.7.0 %post   export DEBIAN_FRONTEND=noninteractive   aptget update   aptget y upgrade    Install git and pip   aptget y install gitall   aptget y install python3pip   aptget y install wget    Pip installations.   pip install upgrade pip   pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html   pip install flax equinox diffrax optax   pip install tensorflowprobability==0.15.0 tensorflowdatasets   pip install tensorflow_io   pip install mlcollections   pip install Pillow   pip install scikitlearn   pip install scikitimage   pip install matplotlib   pip install scipy   pip install imageio   pip install notebook   pip install ipywidgets   pip install astropy   pip install seaborn   unset DEBIAN_FRONTEND %environment    export XLA_FLAGS=xla_gpu_cuda_data_dir=/usr/lib/cuda   export TF_FORCE_GPU_ALLOW_GROWTH=true   export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH   export PATH=/opt/conda/bin:$PATH   export LC_ALL=C ```  What jax/jaxlib version are you using? jax v0.4.8  Which accelerator(s) are you using? GPU  Additional system info Python 3.8, CUDA 11.60, cuDNN 8.3.2  NVIDIA GPU info ``` ++  ++++ ```",2023-05-11T21:23:24Z,bug NVIDIA GPU,closed,0,1,https://github.com/jax-ml/jax/issues/15976,"We no longer ship new wheels with CUDNN 8.3 support. (JAX 0.4.8 was the last release.) The earliest version we support is CUDNN 8.6, but I'd encourage you to upgrade to the latest version (8.9). The `pip` wheel installation should make this easy to do. Closing, because I believe this is a bug with an old/no longer supported CUDNN version. Please reopen if you see it with an up to date CUDNN!"
1425,"以下是一个github上的jax下的一个issue, 标题是(Segfault with `jax.clear_caches`)， 内容是 ( Description Calling `jax.clear_caches`: ``` Fatal Python error: Segmentation fault Current thread 0x00007cb4dbcbd180 (most recent call first):   File "".../jax/_src/util.py"", line 309 in clear_all_weakref_lru_caches   File "".../jax/_src/api.py"", line 2894 in clear_caches ... ``` Notably I also had to exclude the weakref stuff from my previous manual equivalent: ``` for module_name, module in sys.modules.copy().items():     if module_name.startswith(""jax""):         if module_name not in [""jax.interpreters.partial_eval""]:             for obj_name in dir(module):                 obj = getattr(module, obj_name)                 if hasattr(obj, ""cache_clear""):                     try:                         print(f""Clearing {obj}"")                         if ""Weakref"" not in type(obj).__name__:                             obj.cache_clear()                     except Exception:                         pass ``` so it's probably something to do with that. I can probably set up a repro using the Diffrax test suite if that'd be helpful.  What jax/jaxlib version are you using? JAX 0.4.9, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.9  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Segfault with `jax.clear_caches`," Description Calling `jax.clear_caches`: ``` Fatal Python error: Segmentation fault Current thread 0x00007cb4dbcbd180 (most recent call first):   File "".../jax/_src/util.py"", line 309 in clear_all_weakref_lru_caches   File "".../jax/_src/api.py"", line 2894 in clear_caches ... ``` Notably I also had to exclude the weakref stuff from my previous manual equivalent: ``` for module_name, module in sys.modules.copy().items():     if module_name.startswith(""jax""):         if module_name not in [""jax.interpreters.partial_eval""]:             for obj_name in dir(module):                 obj = getattr(module, obj_name)                 if hasattr(obj, ""cache_clear""):                     try:                         print(f""Clearing {obj}"")                         if ""Weakref"" not in type(obj).__name__:                             obj.cache_clear()                     except Exception:                         pass ``` so it's probably something to do with that. I can probably set up a repro using the Diffrax test suite if that'd be helpful.  What jax/jaxlib version are you using? JAX 0.4.9, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.9  NVIDIA GPU info _No response_",2023-05-11T19:05:14Z,bug needs info,closed,0,5,https://github.com/jax-ml/jax/issues/15973,Possibly related: CC(`jax.clear_backends` causes doublefree and `SIGABRT`.),Please add a repro...,Closing because we don't have a repro. Please reopen if we have a way to recreate the problem!,kidger will reopen if/when he can tell us how to reproduce it...,"If that's useful, this can be reproduced using Diffrax, as per patrickkidger/diffrax CC(Implement np.take (70).)."
7259,"以下是一个github上的jax下的一个issue, 标题是(Assertion error for TPU VM within `unsynced_user_spec`)， 内容是 ( Description Hey all, I have an issue when trying to run flaxformer with jax on a TPU VM V28 using the base software version. On CPU this runs perfectly but on TPU I have run into this issue consistently even when trying to shift `flaxformer`, `flax` and `jax` versions. I feel it could be an issue with sharding but am not well versed enough to dig into it. Thanks! Logs: ```python File ""/home/AdamG012/t5x/t5x/train.py"", line 835, in      config_utils.run(main)   File ""/home/AdamG012/temp/t5x/t5x/config_utils.py"", line 214, in run     gin_utils.run(main)   File ""/home/AdamG012/temp/t5x/t5x/gin_utils.py"", line 129, in run     app.run(   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/AdamG012/t5x/t5x/train.py"", line 788, in main     _main(argv)   File ""/home/AdamG012/t5x/t5x/train.py"", line 830, in _main     train_using_gin()   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/gin/config.py"", line 1605, in gin_wrapper     utils.augment_exception_message_and_reraise(e, err_str)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/gin/utils.py"", line 41, in augment_exception_message_and_reraise     raise proxy.with_traceback(exception.__traceback__) from None   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/gin/config.py"", line 1582, in gin_wrapper     return fn(*new_args, **new_kwargs)   File ""/home/AdamG012/t5x/t5x/train.py"", line 614, in train     trainer.compile_train(dummy_batch)   File ""/home/AdamG012/temp/t5x/t5x/trainer.py"", line 538, in compile_train     self._compiled_train_step = self._partitioner.compile(   File ""/home/AdamG012/temp/t5x/t5x/partitioning.py"", line 805, in compile     return partitioned_fn.lower(*args).compile()   File ""/home/AdamG012/temp/t5x/t5x/partitioning.py"", line 770, in lower     return self._pjitted_fn.lower(*args, **kwargs)   File ""/home/AdamG012/temp/t5x/t5x/trainer.py"", line 899, in train_step     return train_with_lr(   File ""/home/AdamG012/temp/t5x/t5x/trainer.py"", line 824, in train_with_lr     accumulate_grads_microbatched(model, train_state, batch, dropout_rng,   File ""/home/AdamG012/temp/t5x/t5x/trainer.py"", line 673, in accumulate_grads_microbatched     (_, metrics), grad_accum = grad_fn(train_state.params, batch, dropout_rng)   File ""/home/AdamG012/temp/t5x/t5x/contrib/moe/models.py"", line 101, in loss_fn     logits, state = self._compute_logits(   File ""/home/AdamG012/temp/t5x/t5x/models.py"", line 458, in _compute_logits     return self.module.apply(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 1465, in __call__     encoded = self.encode(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 1331, in encode     return self.encoder(   pytype: disable=attributeerror   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 904, in __call__     encoder_outputs = self.encode_from_continuous_inputs(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 858, in encode_from_continuous_inputs     encoder_outputs = self.encoder(   File ""/home/AdamG012/t5x/t5x/flaxformer/transformer_common.py"", line 108, in __call__     return self.apply_range_of_layers(0, None, inputs, *args, **kwargs)   File ""/home/AdamG012/t5x/t5x/flaxformer/transformer_common.py"", line 130, in apply_range_of_layers     current_activations = layer(current_activations, *args, **kwargs)   pytype: disable=notcallable   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_architecture.py"", line 81, in __call__     y = super().__call__(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 346, in __call__     y = self.mlp(y, enable_dropout=enable_dropout)   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 228, in __call__     outputs = self._mask_and_dispatch_to_experts(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 433, in _mask_and_dispatch_to_experts     expert_outputs = self._call_experts(expert_inputs, enable_dropout, **kwargs)   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 597, in _call_experts     outputs = layer_fn(self.expert, inputs)   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 593, in layer_fn     return self._filter_inputs(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 687, in _filter_inputs     return mapped_expert(   File ""/home/AdamG012/t5x/t5x/flaxformer/components/dense.py"", line 337, in __call__     x = dense(self.intermediate_dim, dense_name, inputs,   File ""/home/AdamG012/t5x/t5x/flaxformer/components/dense.py"", line 309, in dense     return DenseGeneral(   File ""/home/AdamG012/t5x/t5x/flaxformer/components/dense.py"", line 140, in __call__     kernel = partitioning.param_with_axes(   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/flax/linen/partitioning.py"", line 148, in param_with_axes     module_param = with_sharding_constraint(module_param,   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/flax/linen/spmd.py"", line 272, in with_logical_constraint     return jax.tree_util.tree_map(   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/flax/linen/spmd.py"", line 251, in _with_sharding_constraint_one_fallback     return _with_sharding_constraint(x, jax.sharding.PartitionSpec(*mesh_axes), mesh=mesh)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/flax/linen/spmd.py"", line 229, in _with_sharding_constraint     return pjit.with_sharding_constraint(x, axis_resources)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 253, in _from_parsed_pspec     return cls(mesh, parsed_pspec.get_partition_spec(), parsed_pspec)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 221, in _preprocess     _check_mesh_resource_axis(self.mesh, self._parsed_pspec)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 127, in _check_mesh_resource_axis     raise ValueError(f""Resource axis: {e.args[0]} of {parsed_pspec.user_spec} is ""   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 817, in user_spec     return self.unsynced_user_spec(SpecSync.IN_SYNC)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 830, in unsynced_user_spec     raise AssertionError(f""Please open a bug report! ({self.sync} >= {min_sync})"") AssertionError: Please open a bug report! (0 >= 2)   In call to configurable 'train' () ```  What jax/jaxlib version are you using? jax v0.4.9, jaxlib v0.4.9  Which accelerator(s) are you using? TPU  Additional system info Linux, Python version 3.9.5  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Assertion error for TPU VM within `unsynced_user_spec`," Description Hey all, I have an issue when trying to run flaxformer with jax on a TPU VM V28 using the base software version. On CPU this runs perfectly but on TPU I have run into this issue consistently even when trying to shift `flaxformer`, `flax` and `jax` versions. I feel it could be an issue with sharding but am not well versed enough to dig into it. Thanks! Logs: ```python File ""/home/AdamG012/t5x/t5x/train.py"", line 835, in      config_utils.run(main)   File ""/home/AdamG012/temp/t5x/t5x/config_utils.py"", line 214, in run     gin_utils.run(main)   File ""/home/AdamG012/temp/t5x/t5x/gin_utils.py"", line 129, in run     app.run(   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/AdamG012/t5x/t5x/train.py"", line 788, in main     _main(argv)   File ""/home/AdamG012/t5x/t5x/train.py"", line 830, in _main     train_using_gin()   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/gin/config.py"", line 1605, in gin_wrapper     utils.augment_exception_message_and_reraise(e, err_str)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/gin/utils.py"", line 41, in augment_exception_message_and_reraise     raise proxy.with_traceback(exception.__traceback__) from None   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/gin/config.py"", line 1582, in gin_wrapper     return fn(*new_args, **new_kwargs)   File ""/home/AdamG012/t5x/t5x/train.py"", line 614, in train     trainer.compile_train(dummy_batch)   File ""/home/AdamG012/temp/t5x/t5x/trainer.py"", line 538, in compile_train     self._compiled_train_step = self._partitioner.compile(   File ""/home/AdamG012/temp/t5x/t5x/partitioning.py"", line 805, in compile     return partitioned_fn.lower(*args).compile()   File ""/home/AdamG012/temp/t5x/t5x/partitioning.py"", line 770, in lower     return self._pjitted_fn.lower(*args, **kwargs)   File ""/home/AdamG012/temp/t5x/t5x/trainer.py"", line 899, in train_step     return train_with_lr(   File ""/home/AdamG012/temp/t5x/t5x/trainer.py"", line 824, in train_with_lr     accumulate_grads_microbatched(model, train_state, batch, dropout_rng,   File ""/home/AdamG012/temp/t5x/t5x/trainer.py"", line 673, in accumulate_grads_microbatched     (_, metrics), grad_accum = grad_fn(train_state.params, batch, dropout_rng)   File ""/home/AdamG012/temp/t5x/t5x/contrib/moe/models.py"", line 101, in loss_fn     logits, state = self._compute_logits(   File ""/home/AdamG012/temp/t5x/t5x/models.py"", line 458, in _compute_logits     return self.module.apply(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 1465, in __call__     encoded = self.encode(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 1331, in encode     return self.encoder(   pytype: disable=attributeerror   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 904, in __call__     encoder_outputs = self.encode_from_continuous_inputs(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 858, in encode_from_continuous_inputs     encoder_outputs = self.encoder(   File ""/home/AdamG012/t5x/t5x/flaxformer/transformer_common.py"", line 108, in __call__     return self.apply_range_of_layers(0, None, inputs, *args, **kwargs)   File ""/home/AdamG012/t5x/t5x/flaxformer/transformer_common.py"", line 130, in apply_range_of_layers     current_activations = layer(current_activations, *args, **kwargs)   pytype: disable=notcallable   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_architecture.py"", line 81, in __call__     y = super().__call__(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/t5/t5_architecture.py"", line 346, in __call__     y = self.mlp(y, enable_dropout=enable_dropout)   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 228, in __call__     outputs = self._mask_and_dispatch_to_experts(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 433, in _mask_and_dispatch_to_experts     expert_outputs = self._call_experts(expert_inputs, enable_dropout, **kwargs)   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 597, in _call_experts     outputs = layer_fn(self.expert, inputs)   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 593, in layer_fn     return self._filter_inputs(   File ""/home/AdamG012/t5x/t5x/flaxformer/architectures/moe/moe_layers.py"", line 687, in _filter_inputs     return mapped_expert(   File ""/home/AdamG012/t5x/t5x/flaxformer/components/dense.py"", line 337, in __call__     x = dense(self.intermediate_dim, dense_name, inputs,   File ""/home/AdamG012/t5x/t5x/flaxformer/components/dense.py"", line 309, in dense     return DenseGeneral(   File ""/home/AdamG012/t5x/t5x/flaxformer/components/dense.py"", line 140, in __call__     kernel = partitioning.param_with_axes(   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/flax/linen/partitioning.py"", line 148, in param_with_axes     module_param = with_sharding_constraint(module_param,   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/flax/linen/spmd.py"", line 272, in with_logical_constraint     return jax.tree_util.tree_map(   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/flax/linen/spmd.py"", line 251, in _with_sharding_constraint_one_fallback     return _with_sharding_constraint(x, jax.sharding.PartitionSpec(*mesh_axes), mesh=mesh)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/flax/linen/spmd.py"", line 229, in _with_sharding_constraint     return pjit.with_sharding_constraint(x, axis_resources)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 253, in _from_parsed_pspec     return cls(mesh, parsed_pspec.get_partition_spec(), parsed_pspec)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 221, in _preprocess     _check_mesh_resource_axis(self.mesh, self._parsed_pspec)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 127, in _check_mesh_resource_axis     raise ValueError(f""Resource axis: {e.args[0]} of {parsed_pspec.user_spec} is ""   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 817, in user_spec     return self.unsynced_user_spec(SpecSync.IN_SYNC)   File ""/home/AdamG012/t5x/t5_env/lib/python3.9/sitepackages/jax/_src/sharding_impls.py"", line 830, in unsynced_user_spec     raise AssertionError(f""Please open a bug report! ({self.sync} >= {min_sync})"") AssertionError: Please open a bug report! (0 >= 2)   In call to configurable 'train' () ```  What jax/jaxlib version are you using? jax v0.4.9, jaxlib v0.4.9  Which accelerator(s) are you using? TPU  Additional system info Linux, Python version 3.9.5  NVIDIA GPU info _No response_",2023-05-11T12:32:25Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/15964,Cna you please provide a minimal repro?,"Hey  , Thanks, no problem! I am just using this configuration of the main branches for each of `flax`, `flaxformer`: ```bash git clone branch=main https://github.com/googleresearch/t5x cd t5x python3 m pip install e '.[tpu]' f \   https://storage.googleapis.com/jaxreleases/libtpu_releases.html git clone https://github.com/google/flaxformer.git cd flaxformer pip3 install '.[testing]' cd .. mv flaxformer flaxformermain mv flaxformermain/flaxformer . ``` Then I ran a modified wmt `gin` script to include `tokens_choose_small.gin` as the base: ```python from __gin__ import dynamic_registration import __main__ as train_script import seqio from t5.data import mixtures from t5x import models from t5x import partitioning from t5x import utils include 'flaxformer/t5x/configs/moe/models/tokens_choose_small.gin' include ""t5x/configs/runs/pretrain.gin"" MIXTURE_OR_TASK_NAME = ""wmt_t2t_ende_v003"" TASK_FEATURE_LENGTHS = {""inputs"": 256, ""targets"": 256} TRAIN_STEPS = 50000 DROPOUT_RATE = 0.0 train/utils.DatasetConfig:   batch_size = 128   use_cached = False   pack = True   seed = 0 train_eval/utils.DatasetConfig:   batch_size = 128   use_cached = False   pack = True   seed = 0 infer_eval/utils.DatasetConfig:   mixture_or_task_name = %MIXTURE_OR_TASK_NAME   task_feature_lengths = None   compute max   split = ""validation""   seed = 0   batch_size = 128   shuffle = False   use_cached = False train_script.train:   eval_period = 500   eval_steps = 20   random_seed = 0   use_hardware_rng = True   infer_eval_dataset_cfg = /utils.DatasetConfig()   inference_evaluator_cls = .Evaluator seqio.Evaluator:   logger_cls = [.PyLoggingLogger, .TensorBoardLogger, .JSONLogger]   num_examples = None   Use all examples in the infer_eval dataset.   use_memory_cache = True utils.SaveCheckpointConfig:   period = 5000   checkpoint frequency  `num_decodes` is equivalent to a beam size in a beam search decoding. models.EncoderDecoderModel.predict_batch_with_aux.num_decodes = 4 partitioning.PjitPartitioner.num_partitions = 2 utils.create_learning_rate_scheduler:   factors = 'constant * rsqrt_decay'   base_learning_rate = 1.0   warmup_steps = 10000 ``` The script to train is just: ```bash GIN_FILE=""flaxformer/t5x/configs/moe/runs/base_wmt_from_scratch.gin""; INNER_MODEL_NAME=$(date +%Y%m%d%s); GOOGLE_CLOUD_BUCKET_NAME=""""; TFDS_DATA_DIR=""gs://${GOOGLE_CLOUD_BUCKET_NAME}/data""; TFDS_DATA_DIR=""${HOME}/data""; MODEL_DIR=""gs://${GOOGLE_CLOUD_BUCKET_NAME}/models/${INNER_MODEL_NAME}""; T5X_DIR=""$""; export TFDS_DATA_DIR; export MODEL_DIR; tfds build wmt_t2t_translate ""${TFDS_DATA_DIR}"" python3 ""${T5X_DIR}/t5x/train.py"" \ 	gin_file=""${GIN_FILE}"" \ 	gin.MODEL_DIR=\""""${MODEL_DIR}""\"" \         gin.NUM_MODEL_PARTITIONS=1 \ 	tfds_data_dir=\""""${TFDS_DATA_DIR}""\""; ```",All good this was fixed with the latest flaxformer update and some modifications. Thanks!, could you please share what modifications you have done to make it work? I am facing the same issue with the latest flaxformer. 
1816,"以下是一个github上的jax下的一个issue, 标题是(A100 8 GPUs extremely slow compared to a single A100)， 内容是 ( Description TL;DR When I run a t5x script using a A1008 GPU machine it is much slower compared to running the same script on a single A100 machine. There are many available configurations for running language model training using the t5x library which is based on jax/flax. To reproduce this issue one can take any of the training scripts run it on a single A100 and then do the same on a A100 8 GPU machine. I also cached the tfds dataset and read it from local disk to minimize any overhead and to just compare performance when switching from 1 GPU to 8 GPU single node training. The model I use doesn't require model parallelism it only suppose to data parallelism, e.g. you can pick up any model size up to (m)T5 XL (3.7B) to reproduce. Here are the stats I got during my training runs: ```  On A100 8 GPU machine  extremely slow / slow compile warning seqs_per_second_per_core=0.156312  On single A100 machine / runs fine  seqs_per_second_per_core=~6 ``` ``` ******************************** [Compiling module pjit_train_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. ******************************** ``` I am not able to attach the file as zst format is not supported and zip compressed folder is too large.  What jax/jaxlib version are you using? jax==0.4.9 jaxlib==0.4.9+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] :: Anaconda, Inc. on linux  NVIDIA GPU info ``` Thu May 11 09:01:07 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",large language model,A100 8 GPUs extremely slow compared to a single A100," Description TL;DR When I run a t5x script using a A1008 GPU machine it is much slower compared to running the same script on a single A100 machine. There are many available configurations for running language model training using the t5x library which is based on jax/flax. To reproduce this issue one can take any of the training scripts run it on a single A100 and then do the same on a A100 8 GPU machine. I also cached the tfds dataset and read it from local disk to minimize any overhead and to just compare performance when switching from 1 GPU to 8 GPU single node training. The model I use doesn't require model parallelism it only suppose to data parallelism, e.g. you can pick up any model size up to (m)T5 XL (3.7B) to reproduce. Here are the stats I got during my training runs: ```  On A100 8 GPU machine  extremely slow / slow compile warning seqs_per_second_per_core=0.156312  On single A100 machine / runs fine  seqs_per_second_per_core=~6 ``` ``` ******************************** [Compiling module pjit_train_step] Very slow compile? If you want to file a bug, run with envvar XLA_FLAGS=xla_dump_to=/tmp/foo and attach the results. ******************************** ``` I am not able to attach the file as zst format is not supported and zip compressed folder is too large.  What jax/jaxlib version are you using? jax==0.4.9 jaxlib==0.4.9+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16 (main, Mar  8 2023, 14:00:05)  [GCC 11.2.0] :: Anaconda, Inc. on linux  NVIDIA GPU info ``` Thu May 11 09:01:07 2023        ++  ++ ```",2023-05-11T09:17:13Z,bug NVIDIA GPU,open,0,4,https://github.com/jax-ml/jax/issues/15962,"You could have an issue(software more likely, but also hardware) with the 8 GPUs machine. Can you run the single GPU version on the 8 gpu machine? To limit the GPUs seen by the process, you can do: ``` CUDA_VISIBLE_DEVICES=0 python ... ``` Also, can you give the spec of the 8 GPU computer? the number of CPU and CPU type, RAM, ... It is SMX GPU from the post above.","Could you please share the full run log (as a gist etc.)? On the 8GPU machine, did you start a single process, or multiple processes via cluster management tools like SLURM?","  Thanks for replying quickly. Let me give more details below by trying to reproduce the issue on 2 GPU A100 machine (I couldn't get a 8 A100 this time I will try again with it once available): TL;DR It worked as intended in a 2 A100 GPU  more details below.   Software Stack Running umt5 xl model. Model implementation uses jax pjit and according to README should support TPU (most efficientbut pricey) and GPU clusters. I am not using SLURM yet, right now for initial benchmarking I am using a script to do single node training with 8 GPUs where distribution strategy is only data parallelism. Later, I plan to try a larger model on single node and also move the training to a GPU cluster based on the priceperformance compared to TPUs.  Training script: ```bash !/bin/bash  set train hyperparams unset LD_PRELOAD  set environment variables PROJECT_DIR=${HOME}""/"" T5X_DIR=${HOME}""/t5x""   directory where the t5x is cloned. TFDS_DATA_DIR="""" export PYTHONPATH=${PROJECT_DIR} export XLA_PYTHON_CLIENT_PREALLOCATE=true export XLA_PYTHON_CLIENT_MEM_FRACTION=0.9  default is less around 61/80 GB python3 ${T5X_DIR}/t5x/train.py \   gin_search_paths=${PROJECT_DIR} \   gin_file=""xl_umt5_pretrain.gin"" \   tfds_data_dir=${TFDS_DATA_DIR} ``` Content of `xl_umt5_pretrain.gin`: ```python from __gin__ import dynamic_registration import tasks import __main__ as train_script from t5x import utils from t5x import partitioning from t5x import trainer import seqio include ""t5x/examples/scalable_t5/umt5/pretrain_xl.gin"" MODEL_DIR = """" MIXTURE_OR_TASK_NAME = """" TRAIN_EVAL_MIXTURE_OR_TASK_NAME = """" LOSS_NORMALIZING_FACTOR = None  LOSS_NORMALIZING_FACTOR = ""NUM_REAL_TARGET_TOKENS"" DROPOUT_RATE = 0.0 TRAIN_STEPS = 1_011_200   1_000_000 pretrained steps + 11_200 prefinetuning TASK_FEATURE_LENGTHS = {""inputs"": 512, ""targets"": 512} RANDOM_SEED = 42  BATCH_SIZE = 1024  BATCH_SIZE = 16 BATCH_SIZE = 128  16 x 8 = 128 with 8 A100 GPUs SHUFFLE_TRAIN_EXAMPLES = False  shuffles files for tfds data source USE_CACHED_TASKS = True   Training specification overrides  train_script.train:   eval_period = 2000   stats_period = 20   eval_steps = 20   random_seed = %RANDOM_SEED  (used for dropout) but just to be sure in general   train_dataset_cfg = /utils.DatasetConfig()    train_eval_dataset_cfg = /utils.DatasetConfig()   train_eval_dataset_cfg = None  gradient accumulation not working?   infer_eval_dataset_cfg = None   run_eval_before_training = False  OOM with microbatches when True   partitioner = .PjitPartitioner() partitioning.PjitPartitioner:   model_parallel_submesh = (1,1,1,1)   num_partitions = None trainer.Trainer:   num_microbatches = 0  1024 / 32 = 32 (bs 32 tpu v38) utils.create_learning_rate_scheduler:   factors = 'rsqrt_decay'   step_offset = 960_000  step = 1_000_000  960_000, 1 / sqrt(40_000) = 5e3 utils.DatasetConfig:   seed = 42  read_config.shuffle_seed in the case of TfdsDataSource utils.SaveCheckpointConfig:   period = 2000   save_dataset = True    keep = 5  default is None which keeps all checkpoints utils.RestoreCheckpointConfig:   mode = 'specific'   dtype= 'bfloat16'   path = 'gs://t5data/pretrained_models/t5x/umt5_xl/checkpoint_1000000'   assignment_map = None   strict = True   dtype = None ```  Machine Specs lscpu.info.txt cpu.info.txt Base Ubuntu 20.04 Dependency Installation Steps: 1) CUDA: ``` wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cudakeyring_1.01_all.deb sudo dpkg i cudakeyring_1.01_all.deb sudo aptget update sudo aptget y install cuda ``` 2) Miniconda: ``` wget https://repo.anaconda.com/miniconda/Miniconda3latestLinuxx86_64.sh ... install miniconda conda create n myenv python=3.9 ``` 3) Verify jax with GPUs: ``` python c ""import jax; print(jax.device_put(jax.numpy.ones(1), device=jax.devices('gpu')[0]).device())"" python c ""import jax; print(jax.devices())"" ``` ``` gpu:0 [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0), StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)] ``` 4) T5X: ``` git clone https://github.com/googleresearch/t5x.git cd t5x pip install e .  also install dependencies pip install t5 ``` 5) Download cached dataset to machine. 6) Run single node training scripts  Running with 1 device visible with `CUDA_VISIBLE_DEVICES=0` ``` PROJECT_DIR= T5X_DIR= TFDS_DATA_DIR= export PYTHONPATH=${PROJECT_DIR}  Setting XLA flags export XLA_FLAGS=""xla_gpu_simplify_all_fp_conversions \ xla_gpu_all_reduce_combine_threshold_bytes=136314880 ${XLA_FLAGS}\ xla_dump_to=xla_run_dump"" export CUDA_VISIBLE_DEVICES=0 export XLA_PYTHON_CLIENT_PREALLOCATE=true export XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 python3 ${T5X_DIR}/t5x/train.py \   gin_search_paths=${PROJECT_DIR} \   gin_file=""xl_umt5_pretrain.gin"" \   tfds_data_dir=${TFDS_DATA_DIR} ``` Adding training logs: single_gpu_train.log  Running with all 2 devices (assuming data parallelism via JAX pjit) ``` PROJECT_DIR= T5X_DIR= TFDS_DATA_DIR= export PYTHONPATH=${PROJECT_DIR}  Setting XLA flags export XLA_FLAGS=""xla_gpu_simplify_all_fp_conversions \ xla_gpu_all_reduce_combine_threshold_bytes=136314880 ${XLA_FLAGS}\ xla_dump_to=xla_run_dump"" export CUDA_VISIBLE_DEVICES=0,1 export XLA_PYTHON_CLIENT_PREALLOCATE=true export XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 python3 ${T5X_DIR}/t5x/train.py \   gin_search_paths=${PROJECT_DIR} \   gin_file=""xl_umt5_pretrain.gin"" \   tfds_data_dir=${TFDS_DATA_DIR} ``` Adding training logs: two_gpus_train.log Performance per core / gpu is similar but need to try again with 8 GPUs. 1 gpu training: `timing/seqs_per_second_per_core=5.6435` 2 gpu training: `timing/seqs_per_second_per_core=4.85872`","Is this still a problem? We've done lots of work on LLM training in JAX, and we expect it to be screamingly fast these days on A100 GPUs."
1369,"以下是一个github上的jax下的一个issue, 标题是(Jax requires driver version 520.61.5, although any >= 450.80.02 is specified for CUDA 11)， 内容是 ( Description Hello! Thanks for your hard work! Faced the following problem: I installed jax like pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html my Driver Version is 470.182.03 (>= 450.80.02 for CUDA 11 on Linux). But when I try: import jax num_devices = jax.device_count() I get: 20230511 05:14:42.079662: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected 20230511 05:14:42.079796: E external/xla/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 470.182.3 does not match DSO version 520.61.5  cannot find working devices in this configuration No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Unfortunately my graphics card does not support the latest drivers than 470  What jax/jaxlib version are you using? jax==0.4.9 jaxlib==0.4.9+cuda11.cudnn86  Which accelerator(s) are you using? Tesla K80  Additional system info docker image nvidia/cuda:11.8.0cudnn8develubuntu18.04  NVIDIA GPU info ++ )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"Jax requires driver version 520.61.5, although any >= 450.80.02 is specified for CUDA 11"," Description Hello! Thanks for your hard work! Faced the following problem: I installed jax like pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html my Driver Version is 470.182.03 (>= 450.80.02 for CUDA 11 on Linux). But when I try: import jax num_devices = jax.device_count() I get: 20230511 05:14:42.079662: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected 20230511 05:14:42.079796: E external/xla/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 470.182.3 does not match DSO version 520.61.5  cannot find working devices in this configuration No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Unfortunately my graphics card does not support the latest drivers than 470  What jax/jaxlib version are you using? jax==0.4.9 jaxlib==0.4.9+cuda11.cudnn86  Which accelerator(s) are you using? Tesla K80  Additional system info docker image nvidia/cuda:11.8.0cudnn8develubuntu18.04  NVIDIA GPU info ++ ",2023-05-11T05:19:45Z,bug NVIDIA GPU,closed,0,2,https://github.com/jax-ml/jax/issues/15961,"Not sure what's going on here. From NVIDIA's documentation, I think the combination of driver and runtime should work. Given K80 is a datacenter GPU, you could try NVIDIA's CUDA forward compatibility packages (https://docs.nvidia.com/deploy/cudacompatibility/index.htmlforwardcompatibilitytitle) Note that K80 is pretty old: e.g., NVIDIA has dropped support for Kepler GPUs in CUDA 12.","JAX has dropped support for Kepler series GPUs, so we're not going to be able to take any actions here."
4896,"以下是一个github上的jax下的一个issue, 标题是(v0.4.9 Build fails on linux arm)， 内容是 ( Description To compile tfdf for arm we need jaxlib. Due to no linux arm64 wheels being published, i am trying to build from source. This fails at `jaxlibv.0.4.9`, i have tried to use the XLA repo at head instead of the pinned one, which brpught no luck. ```Dockerfile FROM python:3.10buster RUN git clone b jaxlibv0.4.9 https://github.com/google/jax WORKDIR /jax RUN pip install numpy wheel RUN python build/build.py ``` ```bash $ docker buildx build platform linux/arm64 . CC(未找到相关数据) 891.5 In file included from external/xla/xla/hlo/ir/hlo_instruction.h:43, CC(未找到相关数据) 891.5                  from external/xla/xla/hlo/ir/hlo_computation.h:35, CC(未找到相关数据) 891.5                  from external/xla/xla/hlo/ir/dfs_hlo_visitor_with_default.h:24, CC(未找到相关数据) 891.5                  from external/xla/xla/service/topk_rewriter.h:24, CC(未找到相关数据) 891.5                  from external/xla/xla/service/topk_rewriter.cc:16: CC(未找到相关数据) 891.5 external/xla/xla/comparison_util.h:74:60: note: declared here CC(未找到相关数据) 891.5    enum class [[deprecated(""Use PrimitiveType and Order"")]] Type : uint8_t{ CC(未找到相关数据) 891.5                                                             ^~~~ CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc: In instantiation of 'xla::HasIota(xla::HloSortInstruction*, xla::HloInstruction*):: [with auto:5 = std::initializer_list]': CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc:177:67:   required from here CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc:174:78: error: no matching function for call to 'xla::match::detail::ShapePattern >::WithDims(std::initializer_list&)' CC(未找到相关数据) 891.5      return m::Iota().WithShape(m::Shape().WithElementType(S32).WithDims(dims)); CC(未找到相关数据) 891.5                                                                               ^ CC(未找到相关数据) 891.5 In file included from external/xla/xla/service/topk_rewriter.cc:30: CC(未找到相关数据) 891.5 external/xla/xla/service/pattern_matcher.h:1073:18: note: candidate: 'constexpr auto xla::match::detail::ShapePattern::WithDims(absl::lts_20230125::Span) const [with ShapeType = const xla::Shape; Impl = xla::match::detail::AllOfPattern]' CC(未找到相关数据) 891.5    constexpr auto WithDims(absl::Span dims) const { CC(未找到相关数据) 891.5                   ^~~~~~~~ CC(未找到相关数据) 891.5 external/xla/xla/service/pattern_matcher.h:1073:18: note:   no known conversion for argument 1 from 'std::initializer_list' to 'absl::lts_20230125::Span' CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc: In function 'bool xla::HasIota(xla::HloSortInstruction*, xla::HloInstruction*)': CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc:177:68: error: invalid use of void expression CC(未找到相关数据) 891.5           Match(sort>operand(1), m::Broadcast(match_iota(sort_dims))); CC(未找到相关数据) 891.5                                                                     ^ CC(未找到相关数据) 891.5 In file included from external/xla/xla/hlo/ir/hlo_computation.h:35, CC(未找到相关数据) 891.5                  from external/xla/xla/hlo/ir/dfs_hlo_visitor_with_default.h:24, CC(未找到相关数据) 891.5                  from external/xla/xla/service/topk_rewriter.h:24, CC(未找到相关数据) 891.5                  from external/xla/xla/service/topk_rewriter.cc:16: CC(未找到相关数据) 891.5 external/xla/xla/hlo/ir/hlo_instruction.h: In member function 'virtual absl::lts_20230125::Span xla::HloInstruction::dimensions() const': CC(未找到相关数据) 891.5 external/xla/xla/hlo/ir/hlo_instruction.h:1934:19: warning: control reaches end of nonvoid function [Wreturntype] CC(未找到相关数据) 891.5      LOG(FATAL)  CC(未找到相关数据) 892.4     main() CC(未找到相关数据) 892.4   File ""/jax/build/build.py"", line 559, in main CC(未找到相关数据) 892.4     shell(command) CC(未找到相关数据) 892.4   File ""/jax/build/build.py"", line 53, in shell CC(未找到相关数据) 892.4     output = subprocess.check_output(cmd) CC(未找到相关数据) 892.4   File ""/usr/local/lib/python3.10/subprocess.py"", line 421, in check_output CC(未找到相关数据) 892.4     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True, CC(未找到相关数据) 892.4   File ""/usr/local/lib/python3.10/subprocess.py"", line 526, in run CC(未找到相关数据) 892.4     raise CalledProcessError(retcode, process.args, CC(未找到相关数据) 892.4 subprocess.CalledProcessError: Command '['./bazel5.1.1linuxarm64', 'run', 'verbose_failures=true', ':build_wheel', '', 'output_path=/jax/dist', 'cpu=aarch64']' returned nonzero exit status 1. ``` The actual issue seems to be with xla, but unfourtunately it is beyond my understanding and skills to fix the build, so any help is appreciated!  What jax/jaxlib version are you using? jax v.0.4.9  Which accelerator(s) are you using? _No response_  Additional system info python3.10, arm64  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,v0.4.9 Build fails on linux arm," Description To compile tfdf for arm we need jaxlib. Due to no linux arm64 wheels being published, i am trying to build from source. This fails at `jaxlibv.0.4.9`, i have tried to use the XLA repo at head instead of the pinned one, which brpught no luck. ```Dockerfile FROM python:3.10buster RUN git clone b jaxlibv0.4.9 https://github.com/google/jax WORKDIR /jax RUN pip install numpy wheel RUN python build/build.py ``` ```bash $ docker buildx build platform linux/arm64 . CC(未找到相关数据) 891.5 In file included from external/xla/xla/hlo/ir/hlo_instruction.h:43, CC(未找到相关数据) 891.5                  from external/xla/xla/hlo/ir/hlo_computation.h:35, CC(未找到相关数据) 891.5                  from external/xla/xla/hlo/ir/dfs_hlo_visitor_with_default.h:24, CC(未找到相关数据) 891.5                  from external/xla/xla/service/topk_rewriter.h:24, CC(未找到相关数据) 891.5                  from external/xla/xla/service/topk_rewriter.cc:16: CC(未找到相关数据) 891.5 external/xla/xla/comparison_util.h:74:60: note: declared here CC(未找到相关数据) 891.5    enum class [[deprecated(""Use PrimitiveType and Order"")]] Type : uint8_t{ CC(未找到相关数据) 891.5                                                             ^~~~ CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc: In instantiation of 'xla::HasIota(xla::HloSortInstruction*, xla::HloInstruction*):: [with auto:5 = std::initializer_list]': CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc:177:67:   required from here CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc:174:78: error: no matching function for call to 'xla::match::detail::ShapePattern >::WithDims(std::initializer_list&)' CC(未找到相关数据) 891.5      return m::Iota().WithShape(m::Shape().WithElementType(S32).WithDims(dims)); CC(未找到相关数据) 891.5                                                                               ^ CC(未找到相关数据) 891.5 In file included from external/xla/xla/service/topk_rewriter.cc:30: CC(未找到相关数据) 891.5 external/xla/xla/service/pattern_matcher.h:1073:18: note: candidate: 'constexpr auto xla::match::detail::ShapePattern::WithDims(absl::lts_20230125::Span) const [with ShapeType = const xla::Shape; Impl = xla::match::detail::AllOfPattern]' CC(未找到相关数据) 891.5    constexpr auto WithDims(absl::Span dims) const { CC(未找到相关数据) 891.5                   ^~~~~~~~ CC(未找到相关数据) 891.5 external/xla/xla/service/pattern_matcher.h:1073:18: note:   no known conversion for argument 1 from 'std::initializer_list' to 'absl::lts_20230125::Span' CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc: In function 'bool xla::HasIota(xla::HloSortInstruction*, xla::HloInstruction*)': CC(未找到相关数据) 891.5 external/xla/xla/service/topk_rewriter.cc:177:68: error: invalid use of void expression CC(未找到相关数据) 891.5           Match(sort>operand(1), m::Broadcast(match_iota(sort_dims))); CC(未找到相关数据) 891.5                                                                     ^ CC(未找到相关数据) 891.5 In file included from external/xla/xla/hlo/ir/hlo_computation.h:35, CC(未找到相关数据) 891.5                  from external/xla/xla/hlo/ir/dfs_hlo_visitor_with_default.h:24, CC(未找到相关数据) 891.5                  from external/xla/xla/service/topk_rewriter.h:24, CC(未找到相关数据) 891.5                  from external/xla/xla/service/topk_rewriter.cc:16: CC(未找到相关数据) 891.5 external/xla/xla/hlo/ir/hlo_instruction.h: In member function 'virtual absl::lts_20230125::Span xla::HloInstruction::dimensions() const': CC(未找到相关数据) 891.5 external/xla/xla/hlo/ir/hlo_instruction.h:1934:19: warning: control reaches end of nonvoid function [Wreturntype] CC(未找到相关数据) 891.5      LOG(FATAL)  CC(未找到相关数据) 892.4     main() CC(未找到相关数据) 892.4   File ""/jax/build/build.py"", line 559, in main CC(未找到相关数据) 892.4     shell(command) CC(未找到相关数据) 892.4   File ""/jax/build/build.py"", line 53, in shell CC(未找到相关数据) 892.4     output = subprocess.check_output(cmd) CC(未找到相关数据) 892.4   File ""/usr/local/lib/python3.10/subprocess.py"", line 421, in check_output CC(未找到相关数据) 892.4     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True, CC(未找到相关数据) 892.4   File ""/usr/local/lib/python3.10/subprocess.py"", line 526, in run CC(未找到相关数据) 892.4     raise CalledProcessError(retcode, process.args, CC(未找到相关数据) 892.4 subprocess.CalledProcessError: Command '['./bazel5.1.1linuxarm64', 'run', 'verbose_failures=true', ':build_wheel', '', 'output_path=/jax/dist', 'cpu=aarch64']' returned nonzero exit status 1. ``` The actual issue seems to be with xla, but unfourtunately it is beyond my understanding and skills to fix the build, so any help is appreciated!  What jax/jaxlib version are you using? jax v.0.4.9  Which accelerator(s) are you using? _No response_  Additional system info python3.10, arm64  NVIDIA GPU info _No response_",2023-05-11T04:53:33Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15960,This was fixed by https://github.com/openxla/xla/commit/be1cee3e4aeee55ea5487d15398d92d9e21e19b2
761,"以下是一个github上的jax下的一个issue, 标题是(XLA_PYTHON_CLIENT_MEM_FRACTION=.XX gives me gpu error)， 内容是 ( Description I am trying to calculate self attention values with large image such size 720x1280. it always make huge OOM error so I am trying to use XLA_PYTHON_CLIENT_MEM_FRACTION=.XX < this environment. but It makes another error that can not found its GPU even though jax can find GPU before add the code. I want to know how to escape from this situation thank you.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info python 3.7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XLA_PYTHON_CLIENT_MEM_FRACTION=.XX gives me gpu error, Description I am trying to calculate self attention values with large image such size 720x1280. it always make huge OOM error so I am trying to use XLA_PYTHON_CLIENT_MEM_FRACTION=.XX < this environment. but It makes another error that can not found its GPU even though jax can find GPU before add the code. I want to know how to escape from this situation thank you.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info python 3.7  NVIDIA GPU info _No response_,2023-05-09T09:11:28Z,bug NVIDIA GPU,open,0,1,https://github.com/jax-ml/jax/issues/15926,Can you provide a command and/or code snippet that minimally reproduces the issue?
2471,"以下是一个github上的jax下的一个issue, 标题是(Jit unnecessarily re-compiles with `jax.device_put(x, sharding)`)， 内容是 ( Description I was loading some pretrained parameters in JAX and found my `train_step` got compiled more than once unexpectedly. I was able to reproduce with the following simple snippet ```python from contextlib import contextmanager import numpy as np import jax, jax.numpy as jnp  def print_barrier(name: str):     import sys     print(f"" ENTER {name} "", file=sys.stderr)     yield     print(f"" EXIT  {name} "", file=sys.stderr) .jit def test(x):     return x if __name__ == '__main__':     x_jax = jnp.zeros((1,))     x_np = np.zeros((1,))     x_np2jax = jax.device_put(x_np)     x_np2jax_sharded = jax.device_put(x_np, x_jax.sharding)     jax.config.update(""jax_log_compiles"", True)     with print_barrier(""jax.Array""):         test(x_jax)     with print_barrier(""np.ndarray""):         test(x_np)     with print_barrier(""np.ndarray > jax.Array""):         test(x_np2jax)     with print_barrier(""np.ndarray > jax.Array (with sharding)""):         test(x_np2jax_sharded)     jax.config.update(""jax_log_compiles"", False)     test_aot = test.lower(x_jax).compile()     test_aot(x_jax)     test_aot(x_np)     test_aot(x_np2jax)     test_aot(x_np2jax_sharded) ``` finishes without error and logs to stderr the following ```  ENTER jax.Array  Finished tracing + transforming test for pjit in 0.0008282661437988281 sec Compiling test for with global shapes and types [ShapedArray(float32[1])]. Argument mapping: (GSPMDSharding({replicated}),).  EXIT  jax.Array   ENTER np.ndarray   EXIT  np.ndarray   ENTER np.ndarray > jax.Array   EXIT  np.ndarray > jax.Array   ENTER np.ndarray > jax.Array (with sharding)  Compiling test for with global shapes and types [ShapedArray(float32[1])]. Argument mapping: (GSPMDSharding({replicated}),).  EXIT  np.ndarray > jax.Array (with sharding)  ``` The fourth call `test(x_np2jax_sharded)` triggers jit recompilation, but I did not find it necessary. Two compilations have identical log and jaxpr. I did not find any difference between `x_np2jax` and `x_np2jax_sharded`. And the AOT version below could finish without error.  What jax/jaxlib version are you using? jax v0.4.8, jaxlib v0.4.7  Which accelerator(s) are you using? GPU  Additional system info Python 3.9  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Jit unnecessarily re-compiles with `jax.device_put(x, sharding)`"," Description I was loading some pretrained parameters in JAX and found my `train_step` got compiled more than once unexpectedly. I was able to reproduce with the following simple snippet ```python from contextlib import contextmanager import numpy as np import jax, jax.numpy as jnp  def print_barrier(name: str):     import sys     print(f"" ENTER {name} "", file=sys.stderr)     yield     print(f"" EXIT  {name} "", file=sys.stderr) .jit def test(x):     return x if __name__ == '__main__':     x_jax = jnp.zeros((1,))     x_np = np.zeros((1,))     x_np2jax = jax.device_put(x_np)     x_np2jax_sharded = jax.device_put(x_np, x_jax.sharding)     jax.config.update(""jax_log_compiles"", True)     with print_barrier(""jax.Array""):         test(x_jax)     with print_barrier(""np.ndarray""):         test(x_np)     with print_barrier(""np.ndarray > jax.Array""):         test(x_np2jax)     with print_barrier(""np.ndarray > jax.Array (with sharding)""):         test(x_np2jax_sharded)     jax.config.update(""jax_log_compiles"", False)     test_aot = test.lower(x_jax).compile()     test_aot(x_jax)     test_aot(x_np)     test_aot(x_np2jax)     test_aot(x_np2jax_sharded) ``` finishes without error and logs to stderr the following ```  ENTER jax.Array  Finished tracing + transforming test for pjit in 0.0008282661437988281 sec Compiling test for with global shapes and types [ShapedArray(float32[1])]. Argument mapping: (GSPMDSharding({replicated}),).  EXIT  jax.Array   ENTER np.ndarray   EXIT  np.ndarray   ENTER np.ndarray > jax.Array   EXIT  np.ndarray > jax.Array   ENTER np.ndarray > jax.Array (with sharding)  Compiling test for with global shapes and types [ShapedArray(float32[1])]. Argument mapping: (GSPMDSharding({replicated}),).  EXIT  np.ndarray > jax.Array (with sharding)  ``` The fourth call `test(x_np2jax_sharded)` triggers jit recompilation, but I did not find it necessary. Two compilations have identical log and jaxpr. I did not find any difference between `x_np2jax` and `x_np2jax_sharded`. And the AOT version below could finish without error.  What jax/jaxlib version are you using? jax v0.4.8, jaxlib v0.4.7  Which accelerator(s) are you using? GPU  Additional system info Python 3.9  NVIDIA GPU info _No response_",2023-05-09T05:28:41Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/15924,The recompilation is because `x_np2jax_sharded` is sharded which would lead to a different HLO than the nonsharded one. You can check that by printing `test_aot.as_text()` for both sharded and unsharded and they will be different.," Thanks for your reply, but they are no different. There are several concerns. 1. You mentioned that `x_np2jax_sharded` is sharded, but I set its sharding to be identical to `x_jax`'s sharding, I guess if `x_jax` is nonsharded, so should `x_np2jax_sharded` be. 2. **The whole snippet above run without error**, so `test_aot` is compatible with all four inputs. 3. The HLOs are identical, verified by `assert test.lower(x_jax).compile().as_text() == test.lower(x_np2jax_sharded).compile().as_text()` A bit of use case for more context: I would like to load some pretrained parameters in NumPy format to an already initialized and possibly sharded network parameter (by haiku), so I roughly do `params[key] = jax.device_put(new_param, params[key].sharding)`, but got jit recompiles. This is nonfatal, only slowing down a long running program for about 10 seconds, but it would be better to get things clear.",Ohh I missed that the sharding was the same. I'll take a look and report back,"I don't see any recompilation: ``` In [4]: .jit    ...: def test(x):    ...:     return x    ...: In [5]: x_jax = jnp.zeros((1,))    ...: x_np = np.zeros((1,))    ...: x_np2jax = jax.device_put(x_np)    ...: x_np2jax_sharded = jax.device_put(x_np, x_jax.sharding) No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) In [6]: jax.config.update(""jax_log_compiles"", True) In [7]: test(x_np2jax) Finished tracing + transforming test for pjit in 0.000431060791015625 sec Out[7]: Array([0.], dtype=float32) In [8]: test(x_np2jax_sharded) Out[8]: Array([0.], dtype=float32) In [9]: test(x_jax) Out[9]: Array([0.], dtype=float32) ```",Maybe upgrade to jax and jaxlib 0.4.9 and see if you see something different?,"Indeed, with v0.4.9 released 2 hours ago this problem went away! So this issue is considered fixed and I will close this issue. But I am a bit interested with Jax internals, so could you point to a specific commit that fixes this issue?  ",The fix was over a lot of commits. https://github.com/google/jax/commit/5d2f4530940f247fc7a4a6bc7e0f2b41fa39f7fb was the main one that fixed these kind of problems.
9559,"以下是一个github上的jax下的一个issue, 标题是(JAX internal assertion fails with vmap(shard_map(...)) when setting axis_name and spmd_axis_name)， 内容是 (If I set both `axis_name` and `spmd_axis_name` on `vmap`, I get an opaque internal assertion error from shard_map. I'm not sure if this is user error or a genuine bug, because spmd_axis_name is undocumented. After doing some testing, it does appear that `spmd_axis_name` _also_ effectively sets `axis_name` (in most cases?), so maybe that is enough. But if you aren't supposed to set both of these arguments on `vmap`, an informative error raised much earlier would have saved me hours of debugging. To reproduce: ```python import chex import jax import jax.numpy as jnp import numpy as np from jax.experimental import shard_map chex.set_n_cpu_devices(4) P = jax.sharding.PartitionSpec mesh = jax.sharding.Mesh(np.reshape(jax.devices(), (2, 2)), axis_names=['x', 'y']) x = jax.device_put(jnp.ones((2, 2))) y = jax.vmap(   shard_map.shard_map(       lambda x: x ** 2,       mesh=mesh,       in_specs=(P('y'),),       out_specs=P('y')   ),   axis_name='x',   spmd_axis_name='x', )(x) ``` Raises:  ```  UnfilteredStackTrace                      Traceback (most recent call last)  in ()      12  > 13 y = jax.vmap(      14   shard_map.shard_map( third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e: third_party/py/jax/_src/api.py in vmap_f(*args, **kwargs)    1240                   _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, ""vmap"")) > 1241     out_flat = batching.batch(    1242         flat_fun, axis_name, axis_size_, in_axes_flat, third_party/py/jax/_src/linear_util.py in call_wrapped(self, *args, **kwargs)     187     try: > 188       ans = self.f(*args, **dict(self.params, **kwargs))     189     except: third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e: third_party/py/jax/experimental/shard_map.py in wrapped(*args)     119     try: > 120       out_flat = shard_map_p.bind(     121           flat_fun, *args_flat, mesh=mesh, in_names=in_names_flat, third_party/py/jax/experimental/shard_map.py in bind(self, fun, mesh, in_names, out_names_thunk, check_rep, *args)     346     tracers = map(top_trace.full_raise, args) > 347     outs = top_trace.process_shard_map(   pytype: disable=attributeerror     348         shard_map_p, fun, tracers, mesh=mesh, in_names=in_names, third_party/py/jax/experimental/shard_map.py in _shard_map_batch(trace, prim, fun, in_tracers, mesh, in_names, out_names_thunk, check_rep)     855                     out_names_thunk=new_out_names_thunk, check_rep=check_rep) > 856   out_vals = prim.bind(fun, *in_vals, **new_params)     857   make_tracer = partial(batching.BatchTracer, trace, third_party/py/jax/experimental/shard_map.py in bind(self, fun, mesh, in_names, out_names_thunk, check_rep, *args)     346     tracers = map(top_trace.full_raise, args) > 347     outs = top_trace.process_shard_map(   pytype: disable=attributeerror     348         shard_map_p, fun, tracers, mesh=mesh, in_names=in_names, third_party/py/jax/experimental/shard_map.py in _shard_map_impl(***failed resolving arguments***)     553       in_tracers = map(partial(ShardMapTracer, t), in_rep, args) > 554       ans = fun.call_wrapped(*in_tracers)     555       out_tracers = map(t.full_raise, ans) third_party/py/jax/_src/linear_util.py in call_wrapped(self, *args, **kwargs)     187     try: > 188       ans = self.f(*args, **dict(self.params, **kwargs))     189     except:  in (x)      14   shard_map.shard_map( > 15       lambda x: x ** 2,      16       mesh=mesh, third_party/py/jax/_src/numpy/array_methods.py in op(self, *args)     789   def op(self, *args): > 790     return getattr(self.aval, f""_{name}"")(self, *args)     791   return op third_party/py/jax/_src/numpy/array_methods.py in deferring_binary_op(self, other)     257     if isinstance(other, _accepted_binop_types): > 258       return binary_op(*args)     259     if isinstance(other, _rejected_binop_types): third_party/py/jax/_src/numpy/ufuncs.py in power(x1, x2)     343       x1, = promote_dtypes_numeric(x1) > 344       return lax.integer_pow(x1, x2)     345   return _power(x1, x2) third_party/py/jax/_src/lax/lax.py in integer_pow(x, y)     372   r""""""Elementwise power: :math:`x^y`, where :math:`y` is a fixed integer."""""" > 373   return integer_pow_p.bind(x, y=y)     374  third_party/py/jax/_src/core.py in bind(self, *args, **params)     379             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 380     return self.bind_with_trace(find_top_trace(args), args, params)     381  third_party/py/jax/_src/core.py in bind_with_trace(self, trace, args, params)     382   def bind_with_trace(self, trace, args, params): > 383     out = trace.process_primitive(self, map(trace.full_raise, args), params)     384     return map(full_lower, out) if self.multiple_results else full_lower(out) third_party/py/jax/_src/interpreters/batching.py in process_primitive(self, primitive, tracers, params)     360     else: > 361       frame = self.get_frame(vals_in, dims_in)     362       batched_primitive = self.get_primitive_batcher(primitive, frame) third_party/py/jax/_src/interpreters/batching.py in get_frame(self, vals, dims)     345     frame = core.axis_frame(self.axis_name, self.main) > 346     assert axis_size is None or axis_size == frame.size, (axis_size, frame.size)     347     assert frame.main_trace is self.main UnfilteredStackTrace: AssertionError: (1, 2) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: AssertionError                            Traceback (most recent call last)  in ()      11 x = jax.device_put(jnp.ones((2, 2)))      12  > 13 y = jax.vmap(      14   shard_map.shard_map(      15       lambda x: x ** 2,  in (x)      13 y = jax.vmap(      14   shard_map.shard_map( > 15       lambda x: x ** 2,      16       mesh=mesh,      17       in_specs=(P('y'),), third_party/py/jax/_src/numpy/array_methods.py in op(self, *args)     788 def _forward_operator_to_aval(name):     789   def op(self, *args): > 790     return getattr(self.aval, f""_{name}"")(self, *args)     791   return op     792  third_party/py/jax/_src/numpy/array_methods.py in deferring_binary_op(self, other)     256     args = (other, self) if swap else (self, other)     257     if isinstance(other, _accepted_binop_types): > 258       return binary_op(*args)     259     if isinstance(other, _rejected_binop_types):     260       raise TypeError(f""unsupported operand type(s) for {opchar}: "" third_party/py/jax/_src/numpy/ufuncs.py in power(x1, x2)     342     else:     343       x1, = promote_dtypes_numeric(x1) > 344       return lax.integer_pow(x1, x2)     345   return _power(x1, x2)     346  AssertionError: (1, 2) ```  The end of my traceback looks like:  ```  File ""third_party/py/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback    return fun(*args, **kwargs)  File ""third_party/py/jax/experimental/shard_map.py"", line 120, in wrapped    out_flat = shard_map_p.bind(  File ""third_party/py/jax/experimental/shard_map.py"", line 347, in bind    outs = top_trace.process_shard_map(   pytype: disable=attributeerror  File ""third_party/py/jax/experimental/shard_map.py"", line 856, in _shard_map_batch    out_vals = prim.bind(fun, *in_vals, **new_params)  File ""third_party/py/jax/experimental/shard_map.py"", line 347, in bind    outs = top_trace.process_shard_map(   pytype: disable=attributeerror  File ""third_party/py/jax/experimental/shard_map.py"", line 404, in _shard_map_staging    jaxpr, out_avals_generic, consts = pe.trace_to_subjaxpr_dynamic(f, main, in_avals_)  File ""third_party/py/jax/_src/interpreters/partial_eval.py"", line 2172, in trace_to_subjaxpr_dynamic    ans = fun.call_wrapped(*in_tracers_)  File ""third_party/py/jax/_src/linear_util.py"", line 188, in call_wrapped    ans = self.f(*args, **dict(self.params, **kwargs))  File ""third_party/py/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback    return fun(*args, **kwargs)  File ""third_party/py/jax/_src/pjit.py"", line 208, in cache_miss    outs, out_flat, out_tree, args_flat = _python_pjit_helper(  File ""third_party/py/jax/_src/pjit.py"", line 155, in _python_pjit_helper    out_flat = pjit_p.bind(*args_flat, **params)  File ""third_party/py/jax/_src/core.py"", line 2633, in bind    return self.bind_with_trace(top_trace, args, params)  File ""third_party/py/jax/_src/core.py"", line 383, in bind_with_trace    out = trace.process_primitive(self, map(trace.full_raise, args), params)  File ""third_party/py/jax/_src/interpreters/batching.py"", line 361, in process_primitive    frame = self.get_frame(vals_in, dims_in)  File ""third_party/py/jax/_src/interpreters/batching.py"", line 346, in get_frame    assert axis_size is None or axis_size == frame.size, (axis_size, frame.size) jax._src.traceback_util.UnfilteredStackTrace: AssertionError: (1, 4) ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,JAX internal assertion fails with vmap(shard_map(...)) when setting axis_name and spmd_axis_name,"If I set both `axis_name` and `spmd_axis_name` on `vmap`, I get an opaque internal assertion error from shard_map. I'm not sure if this is user error or a genuine bug, because spmd_axis_name is undocumented. After doing some testing, it does appear that `spmd_axis_name` _also_ effectively sets `axis_name` (in most cases?), so maybe that is enough. But if you aren't supposed to set both of these arguments on `vmap`, an informative error raised much earlier would have saved me hours of debugging. To reproduce: ```python import chex import jax import jax.numpy as jnp import numpy as np from jax.experimental import shard_map chex.set_n_cpu_devices(4) P = jax.sharding.PartitionSpec mesh = jax.sharding.Mesh(np.reshape(jax.devices(), (2, 2)), axis_names=['x', 'y']) x = jax.device_put(jnp.ones((2, 2))) y = jax.vmap(   shard_map.shard_map(       lambda x: x ** 2,       mesh=mesh,       in_specs=(P('y'),),       out_specs=P('y')   ),   axis_name='x',   spmd_axis_name='x', )(x) ``` Raises:  ```  UnfilteredStackTrace                      Traceback (most recent call last)  in ()      12  > 13 y = jax.vmap(      14   shard_map.shard_map( third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e: third_party/py/jax/_src/api.py in vmap_f(*args, **kwargs)    1240                   _mapped_axis_size(fun, in_tree, args_flat, in_axes_flat, ""vmap"")) > 1241     out_flat = batching.batch(    1242         flat_fun, axis_name, axis_size_, in_axes_flat, third_party/py/jax/_src/linear_util.py in call_wrapped(self, *args, **kwargs)     187     try: > 188       ans = self.f(*args, **dict(self.params, **kwargs))     189     except: third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e: third_party/py/jax/experimental/shard_map.py in wrapped(*args)     119     try: > 120       out_flat = shard_map_p.bind(     121           flat_fun, *args_flat, mesh=mesh, in_names=in_names_flat, third_party/py/jax/experimental/shard_map.py in bind(self, fun, mesh, in_names, out_names_thunk, check_rep, *args)     346     tracers = map(top_trace.full_raise, args) > 347     outs = top_trace.process_shard_map(   pytype: disable=attributeerror     348         shard_map_p, fun, tracers, mesh=mesh, in_names=in_names, third_party/py/jax/experimental/shard_map.py in _shard_map_batch(trace, prim, fun, in_tracers, mesh, in_names, out_names_thunk, check_rep)     855                     out_names_thunk=new_out_names_thunk, check_rep=check_rep) > 856   out_vals = prim.bind(fun, *in_vals, **new_params)     857   make_tracer = partial(batching.BatchTracer, trace, third_party/py/jax/experimental/shard_map.py in bind(self, fun, mesh, in_names, out_names_thunk, check_rep, *args)     346     tracers = map(top_trace.full_raise, args) > 347     outs = top_trace.process_shard_map(   pytype: disable=attributeerror     348         shard_map_p, fun, tracers, mesh=mesh, in_names=in_names, third_party/py/jax/experimental/shard_map.py in _shard_map_impl(***failed resolving arguments***)     553       in_tracers = map(partial(ShardMapTracer, t), in_rep, args) > 554       ans = fun.call_wrapped(*in_tracers)     555       out_tracers = map(t.full_raise, ans) third_party/py/jax/_src/linear_util.py in call_wrapped(self, *args, **kwargs)     187     try: > 188       ans = self.f(*args, **dict(self.params, **kwargs))     189     except:  in (x)      14   shard_map.shard_map( > 15       lambda x: x ** 2,      16       mesh=mesh, third_party/py/jax/_src/numpy/array_methods.py in op(self, *args)     789   def op(self, *args): > 790     return getattr(self.aval, f""_{name}"")(self, *args)     791   return op third_party/py/jax/_src/numpy/array_methods.py in deferring_binary_op(self, other)     257     if isinstance(other, _accepted_binop_types): > 258       return binary_op(*args)     259     if isinstance(other, _rejected_binop_types): third_party/py/jax/_src/numpy/ufuncs.py in power(x1, x2)     343       x1, = promote_dtypes_numeric(x1) > 344       return lax.integer_pow(x1, x2)     345   return _power(x1, x2) third_party/py/jax/_src/lax/lax.py in integer_pow(x, y)     372   r""""""Elementwise power: :math:`x^y`, where :math:`y` is a fixed integer."""""" > 373   return integer_pow_p.bind(x, y=y)     374  third_party/py/jax/_src/core.py in bind(self, *args, **params)     379             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 380     return self.bind_with_trace(find_top_trace(args), args, params)     381  third_party/py/jax/_src/core.py in bind_with_trace(self, trace, args, params)     382   def bind_with_trace(self, trace, args, params): > 383     out = trace.process_primitive(self, map(trace.full_raise, args), params)     384     return map(full_lower, out) if self.multiple_results else full_lower(out) third_party/py/jax/_src/interpreters/batching.py in process_primitive(self, primitive, tracers, params)     360     else: > 361       frame = self.get_frame(vals_in, dims_in)     362       batched_primitive = self.get_primitive_batcher(primitive, frame) third_party/py/jax/_src/interpreters/batching.py in get_frame(self, vals, dims)     345     frame = core.axis_frame(self.axis_name, self.main) > 346     assert axis_size is None or axis_size == frame.size, (axis_size, frame.size)     347     assert frame.main_trace is self.main UnfilteredStackTrace: AssertionError: (1, 2) The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: AssertionError                            Traceback (most recent call last)  in ()      11 x = jax.device_put(jnp.ones((2, 2)))      12  > 13 y = jax.vmap(      14   shard_map.shard_map(      15       lambda x: x ** 2,  in (x)      13 y = jax.vmap(      14   shard_map.shard_map( > 15       lambda x: x ** 2,      16       mesh=mesh,      17       in_specs=(P('y'),), third_party/py/jax/_src/numpy/array_methods.py in op(self, *args)     788 def _forward_operator_to_aval(name):     789   def op(self, *args): > 790     return getattr(self.aval, f""_{name}"")(self, *args)     791   return op     792  third_party/py/jax/_src/numpy/array_methods.py in deferring_binary_op(self, other)     256     args = (other, self) if swap else (self, other)     257     if isinstance(other, _accepted_binop_types): > 258       return binary_op(*args)     259     if isinstance(other, _rejected_binop_types):     260       raise TypeError(f""unsupported operand type(s) for {opchar}: "" third_party/py/jax/_src/numpy/ufuncs.py in power(x1, x2)     342     else:     343       x1, = promote_dtypes_numeric(x1) > 344       return lax.integer_pow(x1, x2)     345   return _power(x1, x2)     346  AssertionError: (1, 2) ```  The end of my traceback looks like:  ```  File ""third_party/py/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback    return fun(*args, **kwargs)  File ""third_party/py/jax/experimental/shard_map.py"", line 120, in wrapped    out_flat = shard_map_p.bind(  File ""third_party/py/jax/experimental/shard_map.py"", line 347, in bind    outs = top_trace.process_shard_map(   pytype: disable=attributeerror  File ""third_party/py/jax/experimental/shard_map.py"", line 856, in _shard_map_batch    out_vals = prim.bind(fun, *in_vals, **new_params)  File ""third_party/py/jax/experimental/shard_map.py"", line 347, in bind    outs = top_trace.process_shard_map(   pytype: disable=attributeerror  File ""third_party/py/jax/experimental/shard_map.py"", line 404, in _shard_map_staging    jaxpr, out_avals_generic, consts = pe.trace_to_subjaxpr_dynamic(f, main, in_avals_)  File ""third_party/py/jax/_src/interpreters/partial_eval.py"", line 2172, in trace_to_subjaxpr_dynamic    ans = fun.call_wrapped(*in_tracers_)  File ""third_party/py/jax/_src/linear_util.py"", line 188, in call_wrapped    ans = self.f(*args, **dict(self.params, **kwargs))  File ""third_party/py/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback    return fun(*args, **kwargs)  File ""third_party/py/jax/_src/pjit.py"", line 208, in cache_miss    outs, out_flat, out_tree, args_flat = _python_pjit_helper(  File ""third_party/py/jax/_src/pjit.py"", line 155, in _python_pjit_helper    out_flat = pjit_p.bind(*args_flat, **params)  File ""third_party/py/jax/_src/core.py"", line 2633, in bind    return self.bind_with_trace(top_trace, args, params)  File ""third_party/py/jax/_src/core.py"", line 383, in bind_with_trace    out = trace.process_primitive(self, map(trace.full_raise, args), params)  File ""third_party/py/jax/_src/interpreters/batching.py"", line 361, in process_primitive    frame = self.get_frame(vals_in, dims_in)  File ""third_party/py/jax/_src/interpreters/batching.py"", line 346, in get_frame    assert axis_size is None or axis_size == frame.size, (axis_size, frame.size) jax._src.traceback_util.UnfilteredStackTrace: AssertionError: (1, 4) ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-08T03:54:13Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/15905,"~~I'm seeing indications that setting `spmd_axis_name` sometimes but does not always set `axis_name` inside the vmapped functions for use with parallel lax APIs. Not sure if that's also a bug, but we definitely do need both types of axis names for some of our model variations :)~~ EDIT: I think I was mistaken here! `spmd_axis_name` does not set `axis_name`","Indeed I don't think we've figured out exactly what `spmd_axis_name` should mean :) Here are some potential options: 1. Act just like a regular axis_name, except in interactions with specific primitives, like `shmap` and `with_sharding_constraint`, do something different; in particular ""just like"" would include ""work with collectives"". Also setting `axis_name` would then be an error. 2. Only `axis_name` works with collectives, and `spmd_axis_name` does not, so if you want collectives you better set both. I think the first makes sense. I'm not sure the second is coherent. Is it valid to set both but set them to different names? If not then isn't that redundant information? If so what does that mean?  can you weigh in, since IIRC you added `spmd_axis_name`?",I think this is just a bug. spmd_axis_name is just supposed to add this axis to any with_sharding_constraint along the vmapped axis. The same thing should happen for shard_map and it should get added to any vmapped axis (but the outer vmap axis should just be available inside the collective).,"There are two things here: 1. the bug in the OP is just that the vmapped axis size changes from the caller of the shmap to the body, yet we don't update the axis env with the new size; however 2. there are general (not shmaprelated) ambiguities in what spmd_axis_name means (e.g. is it valid to set both axis_name and spmd_axis_name, etc). I can fix the former, but can you weigh in on the latter?","Ah, for 2 spmd_axis_name should not be visible inside. I think it is just a way to map the axis_name to an axis of the mesh. If it is visible, then that should be a bug. Note that spmd_axis_name could also be a tuple of names (which it does not make sense to reference)."
5327,"以下是一个github上的jax下的一个issue, 标题是(Operations between replicated arrays on different meshes fails if mesh devices have different orders )， 内容是 ( Description Operations between replicated arrays created using different meshes can succeed, but only if the different meshes happen to have the same internal device order (apparently C contiguous order, for multidimensional meshes). This surprised me because the `sharding` attributes on these arrays is a `GSPMDSharding({replicated})` object, for which the public API suggests device order doesn't matter: `.device_set` and `.addressable_devices` are `set` objects. Could the underlying device assignment checks also be made to be order invariant, e.g., by using sets instead of tuples, or by sorting devices on replicated arrays? Example code to reproduce: ```python import functools import chex import jax import numpy as np chex.set_n_cpu_devices(4) P = jax.sharding.PartitionSpec .partial(jax.jit, static_argnames=['mesh']) def replicated(x, mesh):   sharding = jax.sharding.NamedSharding(mesh, P(None))   return jax.lax.with_sharding_constraint(x, sharding) devices = jax.devices() devices_2d = np.reshape(devices, (2, 2)) mesh = jax.sharding.Mesh(devices, axis_names=['x']) mesh2 = jax.sharding.Mesh(devices_2d, axis_names=['x', 'y']) mesh3 = jax.sharding.Mesh(devices_2d.T, axis_names=['x', 'y']) x = replicated(np.zeros(4), mesh) y = replicated(np.zeros(4), mesh2) z = replicated(np.zeros(4), mesh3) print(x.sharding)   GSPMDSharding({replicated}) print(x  y)   succeeds print(x  z)   fails ```  ``` DeviceAssignmentMismatchError             Traceback (most recent call last) third_party/py/jax/_src/pjit.py in _python_pjit_helper(fun, infer_params_fn, *args, **kwargs)     154   try: > 155     out_flat = pjit_p.bind(*args_flat, **params)     156   except pxla.DeviceAssignmentMismatchError as e: 12 frames third_party/py/jax/_src/core.py in bind(self, *args, **params)    2632                  else axis_main.with_cur_sublevel()) > 2633     return self.bind_with_trace(top_trace, args, params)    2634  third_party/py/jax/_src/core.py in bind_with_trace(self, trace, args, params)     382   def bind_with_trace(self, trace, args, params): > 383     out = trace.process_primitive(self, map(trace.full_raise, args), params)     384     return map(full_lower, out) if self.multiple_results else full_lower(out) third_party/py/jax/_src/core.py in process_primitive(self, primitive, tracers, params)     789   def process_primitive(self, primitive, tracers, params): > 790     return primitive.impl(*tracers, **params)     791  third_party/py/jax/_src/pjit.py in _pjit_call_impl(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)    1068  > 1069   in_shardings = _resolve_in_shardings(    1070       args, in_shardings, out_shardings, third_party/py/jax/_src/pjit.py in _resolve_in_shardings(args, pjit_in_shardings, out_shardings, pjit_mesh)    1001    same. > 1002   pxla._get_and_check_device_assignment(    1003       it.chain( third_party/py/jax/_src/interpreters/pxla.py in _get_and_check_device_assignment(shardings, devices)    1783       if first_sharding_info[0] != arr_device_assignment: > 1784         raise DeviceAssignmentMismatchError([    1785             DeviceAssignmentMismatch(*first_sharding_info), DeviceAssignmentMismatchError: [DeviceAssignmentMismatch(da=(CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3)), m_type=, source_info=None), DeviceAssignmentMismatch(da=(CpuDevice(id=0), CpuDevice(id=2), CpuDevice(id=1), CpuDevice(id=3)), m_type=, source_info=None)] During handling of the above exception, another exception occurred: UnfilteredStackTrace                      Traceback (most recent call last)  in ()      34 print(x  y)   succeeds > 35 print(x  z)   fails third_party/py/jax/_src/numpy/array_methods.py in deferring_binary_op(self, other)     257     if isinstance(other, _accepted_binop_types): > 258       return binary_op(*args)     259     if isinstance(other, _rejected_binop_types): third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e: third_party/py/jax/_src/pjit.py in cache_miss(*args, **kwargs)     207   def cache_miss(*args, **kwargs): > 208     outs, out_flat, out_tree, args_flat = _python_pjit_helper(     209         fun, infer_params_fn, *args, **kwargs) third_party/py/jax/_src/pjit.py in _python_pjit_helper(fun, infer_params_fn, *args, **kwargs)     162         fun_name, fails, args_flat, api_name, arg_names) > 163     raise ValueError(msg) from None     164   outs = tree_unflatten(out_tree, out_flat) UnfilteredStackTrace: ValueError: Received incompatible devices for jitted computation. Got argument x1 of jax.numpy.subtract with shape float32[4] and device ids [0, 1, 2, 3] on platform CPU and argument x2 of jax.numpy.subtract with shape float32[4] and device ids [0, 2, 1, 3] on platform CPU ```     What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Operations between replicated arrays on different meshes fails if mesh devices have different orders ," Description Operations between replicated arrays created using different meshes can succeed, but only if the different meshes happen to have the same internal device order (apparently C contiguous order, for multidimensional meshes). This surprised me because the `sharding` attributes on these arrays is a `GSPMDSharding({replicated})` object, for which the public API suggests device order doesn't matter: `.device_set` and `.addressable_devices` are `set` objects. Could the underlying device assignment checks also be made to be order invariant, e.g., by using sets instead of tuples, or by sorting devices on replicated arrays? Example code to reproduce: ```python import functools import chex import jax import numpy as np chex.set_n_cpu_devices(4) P = jax.sharding.PartitionSpec .partial(jax.jit, static_argnames=['mesh']) def replicated(x, mesh):   sharding = jax.sharding.NamedSharding(mesh, P(None))   return jax.lax.with_sharding_constraint(x, sharding) devices = jax.devices() devices_2d = np.reshape(devices, (2, 2)) mesh = jax.sharding.Mesh(devices, axis_names=['x']) mesh2 = jax.sharding.Mesh(devices_2d, axis_names=['x', 'y']) mesh3 = jax.sharding.Mesh(devices_2d.T, axis_names=['x', 'y']) x = replicated(np.zeros(4), mesh) y = replicated(np.zeros(4), mesh2) z = replicated(np.zeros(4), mesh3) print(x.sharding)   GSPMDSharding({replicated}) print(x  y)   succeeds print(x  z)   fails ```  ``` DeviceAssignmentMismatchError             Traceback (most recent call last) third_party/py/jax/_src/pjit.py in _python_pjit_helper(fun, infer_params_fn, *args, **kwargs)     154   try: > 155     out_flat = pjit_p.bind(*args_flat, **params)     156   except pxla.DeviceAssignmentMismatchError as e: 12 frames third_party/py/jax/_src/core.py in bind(self, *args, **params)    2632                  else axis_main.with_cur_sublevel()) > 2633     return self.bind_with_trace(top_trace, args, params)    2634  third_party/py/jax/_src/core.py in bind_with_trace(self, trace, args, params)     382   def bind_with_trace(self, trace, args, params): > 383     out = trace.process_primitive(self, map(trace.full_raise, args), params)     384     return map(full_lower, out) if self.multiple_results else full_lower(out) third_party/py/jax/_src/core.py in process_primitive(self, primitive, tracers, params)     789   def process_primitive(self, primitive, tracers, params): > 790     return primitive.impl(*tracers, **params)     791  third_party/py/jax/_src/pjit.py in _pjit_call_impl(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)    1068  > 1069   in_shardings = _resolve_in_shardings(    1070       args, in_shardings, out_shardings, third_party/py/jax/_src/pjit.py in _resolve_in_shardings(args, pjit_in_shardings, out_shardings, pjit_mesh)    1001    same. > 1002   pxla._get_and_check_device_assignment(    1003       it.chain( third_party/py/jax/_src/interpreters/pxla.py in _get_and_check_device_assignment(shardings, devices)    1783       if first_sharding_info[0] != arr_device_assignment: > 1784         raise DeviceAssignmentMismatchError([    1785             DeviceAssignmentMismatch(*first_sharding_info), DeviceAssignmentMismatchError: [DeviceAssignmentMismatch(da=(CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3)), m_type=, source_info=None), DeviceAssignmentMismatch(da=(CpuDevice(id=0), CpuDevice(id=2), CpuDevice(id=1), CpuDevice(id=3)), m_type=, source_info=None)] During handling of the above exception, another exception occurred: UnfilteredStackTrace                      Traceback (most recent call last)  in ()      34 print(x  y)   succeeds > 35 print(x  z)   fails third_party/py/jax/_src/numpy/array_methods.py in deferring_binary_op(self, other)     257     if isinstance(other, _accepted_binop_types): > 258       return binary_op(*args)     259     if isinstance(other, _rejected_binop_types): third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e: third_party/py/jax/_src/pjit.py in cache_miss(*args, **kwargs)     207   def cache_miss(*args, **kwargs): > 208     outs, out_flat, out_tree, args_flat = _python_pjit_helper(     209         fun, infer_params_fn, *args, **kwargs) third_party/py/jax/_src/pjit.py in _python_pjit_helper(fun, infer_params_fn, *args, **kwargs)     162         fun_name, fails, args_flat, api_name, arg_names) > 163     raise ValueError(msg) from None     164   outs = tree_unflatten(out_tree, out_flat) UnfilteredStackTrace: ValueError: Received incompatible devices for jitted computation. Got argument x1 of jax.numpy.subtract with shape float32[4] and device ids [0, 1, 2, 3] on platform CPU and argument x2 of jax.numpy.subtract with shape float32[4] and device ids [0, 2, 1, 3] on platform CPU ```     What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-07T04:39:44Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15903,I am trying to lift that restriction (actively) but it is hard and also might lead to inefficiencies so it will require more testing than usual. But currently those restrictions exist. Is this blocking you? I can suggest a workaround!,"Glad you're working on this! I was able to figure out my own workaround pretty quickly once I figured out what was going on. (In my case, I have two meshes, one of which is 1D. So I can just copy the device order for the 1D mesh from the ND mesh.)"
6666,"以下是一个github上的jax下的一个issue, 标题是(triton_autotuner: Rounding modifier required for instruction 'cvt')， 内容是 ( Description Getting the following error when trying to run code on a A100 80GB Google Cloud Debian Deep Learning image (c0deeplearningcommoncu113v20230501debian10). This code is tested and works on TPU (using t5x library). I don't know if this error is related to my setup but after creating the instance before running the code these are the steps I took: 1) Created a new conda environment with py3.9 2) Install latest jax cuda `pip install jax[cuda] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`, which is 4.8.0 as of writing. 3)  Clone t5x library and install editable local version `e git+https://github.com/googleresearch/t5x.gitegg=t5x`. 4) Install extra dep:`pip install t5`. 5) Upgrade CUDNN library to 8.6.0 as jax complained it requires at least that version by manually downloading cudnnlinuxx86_648.6.0.163_cuda11archive.tar.xz and then running the following: ``` $ sudo cp cudnn*archive/include/cudnn*.h /usr/local/cuda/include  $ sudo cp P cudnn*archive/lib/libcudnn* /usr/local/cuda/lib64  $ sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* ```  6) Verified GPU is usable by jax: ``` >>> import jax >>> jax.device_put(jax.numpy.ones(1), device=jax.devices('gpu')[0]).device() StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0) ``` The following is the error I get when running a t5x pretraining script using train.py. ``` Traceback (most recent call last):   File ""/home/keremturgutlu/t5x/t5x/train.py"", line 835, in      config_utils.run(main)   File ""/home/keremturgutlu/t5x/t5x/config_utils.py"", line 214, in run     gin_utils.run(main)   File ""/home/keremturgutlu/t5x/t5x/gin_utils.py"", line 129, in run     app.run(   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/keremturgutlu/t5x/t5x/train.py"", line 788, in main     _main(argv)   File ""/home/keremturgutlu/t5x/t5x/train.py"", line 830, in _main     train_using_gin()   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/gin/config.py"", line 1605, in gin_wrapper     utils.augment_exception_message_and_reraise(e, err_str)   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/gin/utils.py"", line 41, in augment_exception_message_and_reraise     raise proxy.with_traceback(exception.__traceback__) from None   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/gin/config.py"", line 1582, in gin_wrapper     return fn(*new_args, **new_kwargs)   File ""/home/keremturgutlu/t5x/t5x/train.py"", line 614, in train     trainer.compile_train(dummy_batch)   File ""/home/keremturgutlu/t5x/t5x/trainer.py"", line 538, in compile_train     self._compiled_train_step = self._partitioner.compile(   File ""/home/keremturgutlu/t5x/t5x/partitioning.py"", line 805, in compile     return partitioned_fn.lower(*args).compile()   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/stages.py"", line 600, in compile     self._lowering.compile(**kw),   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2836, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 3048, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with nonzero error code 65280, output: ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 234; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 238; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 242; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 246; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 250; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 254; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 258; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 262; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 266; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 270; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 274; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 278; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 282; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 286; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 290; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 294; error   : Rounding modifier required for instruction 'cvt' ptxas fatal   : Ptx assembly aborted due to errors ```  What jax/jaxlib version are you using? jax==0.4.7 jaxlib==0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16  ++ ``` ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052021 NVIDIA Corporation Built on Mon_May__3_19:15:13_PDT_2021 Cuda compilation tools, release 11.3, V11.3.109 Build cuda_11.3.r11.3/compiler.29920130_0 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,triton_autotuner: Rounding modifier required for instruction 'cvt'," Description Getting the following error when trying to run code on a A100 80GB Google Cloud Debian Deep Learning image (c0deeplearningcommoncu113v20230501debian10). This code is tested and works on TPU (using t5x library). I don't know if this error is related to my setup but after creating the instance before running the code these are the steps I took: 1) Created a new conda environment with py3.9 2) Install latest jax cuda `pip install jax[cuda] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`, which is 4.8.0 as of writing. 3)  Clone t5x library and install editable local version `e git+https://github.com/googleresearch/t5x.gitegg=t5x`. 4) Install extra dep:`pip install t5`. 5) Upgrade CUDNN library to 8.6.0 as jax complained it requires at least that version by manually downloading cudnnlinuxx86_648.6.0.163_cuda11archive.tar.xz and then running the following: ``` $ sudo cp cudnn*archive/include/cudnn*.h /usr/local/cuda/include  $ sudo cp P cudnn*archive/lib/libcudnn* /usr/local/cuda/lib64  $ sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* ```  6) Verified GPU is usable by jax: ``` >>> import jax >>> jax.device_put(jax.numpy.ones(1), device=jax.devices('gpu')[0]).device() StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0) ``` The following is the error I get when running a t5x pretraining script using train.py. ``` Traceback (most recent call last):   File ""/home/keremturgutlu/t5x/t5x/train.py"", line 835, in      config_utils.run(main)   File ""/home/keremturgutlu/t5x/t5x/config_utils.py"", line 214, in run     gin_utils.run(main)   File ""/home/keremturgutlu/t5x/t5x/gin_utils.py"", line 129, in run     app.run(   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/keremturgutlu/t5x/t5x/train.py"", line 788, in main     _main(argv)   File ""/home/keremturgutlu/t5x/t5x/train.py"", line 830, in _main     train_using_gin()   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/gin/config.py"", line 1605, in gin_wrapper     utils.augment_exception_message_and_reraise(e, err_str)   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/gin/utils.py"", line 41, in augment_exception_message_and_reraise     raise proxy.with_traceback(exception.__traceback__) from None   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/gin/config.py"", line 1582, in gin_wrapper     return fn(*new_args, **new_kwargs)   File ""/home/keremturgutlu/t5x/t5x/train.py"", line 614, in train     trainer.compile_train(dummy_batch)   File ""/home/keremturgutlu/t5x/t5x/trainer.py"", line 538, in compile_train     self._compiled_train_step = self._partitioner.compile(   File ""/home/keremturgutlu/t5x/t5x/partitioning.py"", line 805, in compile     return partitioned_fn.lower(*args).compile()   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/stages.py"", line 600, in compile     self._lowering.compile(**kw),   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2836, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 3048, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/opt/conda/envs/myenv/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: ptxas exited with nonzero error code 65280, output: ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 234; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 238; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 242; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 246; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 250; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 254; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 258; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 262; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 266; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 270; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 274; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 278; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 282; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 286; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 290; error   : Rounding modifier required for instruction 'cvt' ptxas /var/tmp/tempfilegpub325a549192145fb10904b7b5a, line 294; error   : Rounding modifier required for instruction 'cvt' ptxas fatal   : Ptx assembly aborted due to errors ```  What jax/jaxlib version are you using? jax==0.4.7 jaxlib==0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16  ++ ``` ``` nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052021 NVIDIA Corporation Built on Mon_May__3_19:15:13_PDT_2021 Cuda compilation tools, release 11.3, V11.3.109 Build cuda_11.3.r11.3/compiler.29920130_0 ```",2023-05-07T01:57:07Z,bug NVIDIA GPU,open,0,11,https://github.com/jax-ml/jax/issues/15900, How do we know that it originates in the triton_autotuner?, Is it hard to propagate C++ stack trace along with the error?,">  How do we know that it originates in the triton_autotuner? I am not 100% if that's the root cause but I should've probably pasted this as well: ``` [triton_autotuner.cc:271] failure: internal: ptxas exited with nonzero error code 65280, output: ptxas /var/tmp/tempfilegpu3b3b9d27291935fb10408aefa4, line 234; error : rounding modifier required for instruction 'cvt' ``` I was able to successfully run the code with from scratch nvidia driver, cuda (12.1), cudnn and jax installation 1) Launched a A100 in Google Cloud with base ubuntu 18.04 image. 2) Install latest nvidia driver with cuda 12.1. 3) Install miniconda and create a conda env. 4) Install jax and cudnn. ```pip install upgrade pip  CUDA 12 installation  Note: wheels only available on linux. pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` 5) Install t5x from source and install t5.  ``` ++  ++ ``` nvcc not installed.","Oh this is CUDA12, probably that would explain it  we had some bugs filed on CUDA12 before.","I think this is actually a case of too *old* a CUDA installation, not the other way around. The image is named `c0deeplearningcommoncu113v20230501debian10`: note ""cu113"". JAX is built for CUDA 11.8 (or CUDA 12), and if I recall correctly Ampere GPU support wasn't added until longer after 11.3. Can you update to CUDA 11.8 or newer? Note `nvidiasmi` reports the CUDA version of the driver, not the installed libraries. You need both to be sufficiently new.",Sorry if it was not clear but what I wanted mention was issue was fixed when I installed cuda 12 from scratch instead using the Google Cloud image.,"Recently got this error (might be related to 0.4.9 release looking into it): ``` (myenv) keremturgutlu:~$ python c ""import jax; print(jax.device_put(jax.numpy.ones(1), device=jax.devices('gpu')[0]).device())"" 20230510 07:12:19.701059: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR Traceback (most recent call last):   File """", line 1, in    File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2092, in ones     return lax.full(shape, 1, _jnp_dtype(dtype))   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 1190, in full     return broadcast(fill_value, shape)   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 756, in broadcast     return broadcast_in_dim(operand, tuple(sizes) + np.shape(operand), dims)   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 784, in broadcast_in_dim     return broadcast_in_dim_p.bind(   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 117, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 208, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 254, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2816, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 3028, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/keremturgutlu/miniconda3/envs/myenv/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ``` **Edit**: Tried again by recreating a new instance, and I wasn't able to reproduce the error.","Getting similar when trying to run a custom model on an A6000 with `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` over Runpod's PyTorch or Tensorflow images.  Tried both cuda11 and cuda12, same issue. https://pastebin.com/raw/MUsYZje8 Update: Seems to only happen with bfloat16. Works fine with float32"," Do you have another copy of `ptxas` installed? Is there one in your `PATH`? My strong suspicion is still ""we are finding an ancient `ptxas`""."," I don't, but it should be whatever is used here https://hub.docker.com/r/runpod/pytorch/","  ``` ptxas: NVIDIA (R) Ptx optimizing assembler Copyright (c) 20052022 NVIDIA Corporation Built on Tue_Mar__8_18:17:32_PST_2022 Cuda compilation tools, release 11.6, V11.6.124 Build cuda_11.6.r11.6/compiler.31057947_0 ``` After some testing, it appears to be caused by me accidentally mixing `bflaot16` values with `float32` ones.  Seems a check for that is missing somewhere prior to assembly."
2391,"以下是一个github上的jax下的一个issue, 标题是(Flax output mismatch for multi-dimensional batch input on GPUs)， 内容是 ( Description Hi, I am trying to run a simple MLP on A100 GPU with multidimensional batch inputs of shape *(b1, b2, input_dim)* and output shape *(b1, b2, 1)*. Flax outputs when passing the entire input *(b1, b2, input_dim)* v/s passing a single input *(1, 1, input_dim)* iteratively are not matching. When I run the same code example on CPU or run the equivalent PyTorch version, it matches exactly. Please see the minimal code example, colab link and outputs in the issue below: https://github.com/google/flax/issues/3084 ```py import os os.environ['CUDA_VISIBLE_DEVICES'] = '0' import numpy as np  Flax imports import jax.random as jr import jax.numpy as jnp import flax.linen as nn  PyTorch imports import torch  Common constants b1, b2 = 2, 3 input_dim = 2 hidden_dim = 2 output_dim = 1 batch_shape = (b1, b2)  Flax code tiny_model = nn.Sequential([nn.Dense(hidden_dim), nn.Dense(output_dim)]) tiny_params = tiny_model.init(jr.PRNGKey(1234), jnp.ones((*batch_shape, input_dim))) x = jr.normal(jr.PRNGKey(5678), (*batch_shape, input_dim)) batch_out = tiny_model.apply(tiny_params, x) individual_out = np.zeros_like(batch_out) for i in range(b1):     for j in range(b2):         individual_out[i:i+1, j:j+1, :] = tiny_model.apply(tiny_params, x[i:i+1, j:j+1, :]) print(f""Flax output match: {jnp.all(batch_out == individual_out)}"") display(batch_out.squeeze().tolist(), individual_out.squeeze().tolist())  PyTorch code torch.manual_seed(1234) model = torch.nn.Sequential(torch.nn.Linear(input_dim, hidden_dim), torch.nn.Linear(hidden_dim, output_dim)) batch_out = model(torch.tensor(x.tolist())) individual_out = torch.ones_like(batch_out) for i in range(b1):     for j in range(b2):         individual_out[i, j, :] = model(torch.tensor(x[i:i+1, j:j+1, :].tolist())).squeeze() print(f""PyTorch output match: {torch.all(batch_out == individual_out)}"") display(batch_out.squeeze().tolist(), individual_out.squeeze().tolist()) ```  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` Sat May  6 13:25:07 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Flax output mismatch for multi-dimensional batch input on GPUs," Description Hi, I am trying to run a simple MLP on A100 GPU with multidimensional batch inputs of shape *(b1, b2, input_dim)* and output shape *(b1, b2, 1)*. Flax outputs when passing the entire input *(b1, b2, input_dim)* v/s passing a single input *(1, 1, input_dim)* iteratively are not matching. When I run the same code example on CPU or run the equivalent PyTorch version, it matches exactly. Please see the minimal code example, colab link and outputs in the issue below: https://github.com/google/flax/issues/3084 ```py import os os.environ['CUDA_VISIBLE_DEVICES'] = '0' import numpy as np  Flax imports import jax.random as jr import jax.numpy as jnp import flax.linen as nn  PyTorch imports import torch  Common constants b1, b2 = 2, 3 input_dim = 2 hidden_dim = 2 output_dim = 1 batch_shape = (b1, b2)  Flax code tiny_model = nn.Sequential([nn.Dense(hidden_dim), nn.Dense(output_dim)]) tiny_params = tiny_model.init(jr.PRNGKey(1234), jnp.ones((*batch_shape, input_dim))) x = jr.normal(jr.PRNGKey(5678), (*batch_shape, input_dim)) batch_out = tiny_model.apply(tiny_params, x) individual_out = np.zeros_like(batch_out) for i in range(b1):     for j in range(b2):         individual_out[i:i+1, j:j+1, :] = tiny_model.apply(tiny_params, x[i:i+1, j:j+1, :]) print(f""Flax output match: {jnp.all(batch_out == individual_out)}"") display(batch_out.squeeze().tolist(), individual_out.squeeze().tolist())  PyTorch code torch.manual_seed(1234) model = torch.nn.Sequential(torch.nn.Linear(input_dim, hidden_dim), torch.nn.Linear(hidden_dim, output_dim)) batch_out = model(torch.tensor(x.tolist())) individual_out = torch.ones_like(batch_out) for i in range(b1):     for j in range(b2):         individual_out[i, j, :] = model(torch.tensor(x[i:i+1, j:j+1, :].tolist())).squeeze() print(f""PyTorch output match: {torch.all(batch_out == individual_out)}"") display(batch_out.squeeze().tolist(), individual_out.squeeze().tolist()) ```  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` Sat May  6 13:25:07 2023        ++  ++ ```",2023-05-06T07:53:43Z,bug NVIDIA GPU,open,0,3,https://github.com/jax-ml/jax/issues/15898,"I think this is just a numerical preision problem, not a bug. eg change the print statement to ``` print(f""Flax output match: {jnp.allclose(batch_out, individual_out, atol=1e3)}"") ``` and it says True on GPU (and CPU). Similarly,  ``` m = jnp.max(batch_out  individual_out) print(m)  0.000106453896 ```","That's right, Dr. . However, I guess it should exactly match given that precision (float32) is not changed between batch and individual versions of code.  may let us know his views on this from the domain perspective.",Hi zeel  Looks like the precision issue on GPU with batch input has been resolved in later versions of JAX. I tested the provided code using JAX version 0.4.33 on colab GPU and JAX 0.4.35 on cloud VM having GPU A100. I could not find any output mismatch in both the cases. Please find the screenshot of output on cloud VM with A100 GPU. !image Attaching a colab gist for reference. Could you please test with latest JAX and Flax versions and check if the issue still persists? Thank you.
1619,"以下是一个github上的jax下的一个issue, 标题是(RNG slows down data parallel training)， 内容是 ( Discussed in https://github.com/google/jax/discussions/15783  Originally posted by **jjyyxx** April 27, 2023 I was working with a transformer model in jax and haiku, and found that dropout greatly slows down data parallel training, the main training step looks like ```python self._train_state, batch_scalars = self._train_step(train_key, self._train_state, batch) ``` where  Sharding is created with `sharding = jax.sharding.PositionalSharding(jax.devices())`, containing GPU:0 and GPU:1  `train_key` is a `PRNGKeyArray`, not sharded  `self._train_state` is a PyTree of params and opt_states, replicated with `jax.device_put(train_state, sharding.replicate())`  `batch` is a PyTree of data and labels, sharded with `jax.device_put(batch, sharding)` Every operation in this model (except final loss reduction) is independent between each sample in batch, so this should be trivially data parallel. Without `x = hk.dropout(hk.next_rng_key(), self.dropout, x)` (boils down to a `jax.random.split` and a `jax.random.bernoulli`), every thing works well (Single device: 4.2 it/s, Two devices: 7.5 it/s). But when dropout is enabled (called 20 times), I got  Single device: 3.75 it/s  Two devices: 1.9 it/s  Two devices with `jax.config.update('jax_threefry_partitionable', True)`: 5.32 it/s (I was aware of the document) which is far from expected. Did I miss somthing? Could this performance be optimized?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,RNG slows down data parallel training," Discussed in https://github.com/google/jax/discussions/15783  Originally posted by **jjyyxx** April 27, 2023 I was working with a transformer model in jax and haiku, and found that dropout greatly slows down data parallel training, the main training step looks like ```python self._train_state, batch_scalars = self._train_step(train_key, self._train_state, batch) ``` where  Sharding is created with `sharding = jax.sharding.PositionalSharding(jax.devices())`, containing GPU:0 and GPU:1  `train_key` is a `PRNGKeyArray`, not sharded  `self._train_state` is a PyTree of params and opt_states, replicated with `jax.device_put(train_state, sharding.replicate())`  `batch` is a PyTree of data and labels, sharded with `jax.device_put(batch, sharding)` Every operation in this model (except final loss reduction) is independent between each sample in batch, so this should be trivially data parallel. Without `x = hk.dropout(hk.next_rng_key(), self.dropout, x)` (boils down to a `jax.random.split` and a `jax.random.bernoulli`), every thing works well (Single device: 4.2 it/s, Two devices: 7.5 it/s). But when dropout is enabled (called 20 times), I got  Single device: 3.75 it/s  Two devices: 1.9 it/s  Two devices with `jax.config.update('jax_threefry_partitionable', True)`: 5.32 it/s (I was aware of the document) which is far from expected. Did I miss somthing? Could this performance be optimized?",2023-05-05T21:01:26Z,bug performance,open,0,6,https://github.com/jax-ml/jax/issues/15895,", do you have a minimal code example that reproduces this?", I could give it a try. But can a snippet containing haiku be considered minimal? I suspect at least a mediumsized model could reveal the difference. But I will try it first anyway.,"```python """""" conda create p .conda/jax python=3.9 numpy scipy ipykernel conda activate .conda/jax pip install ""jax[cuda11_cudnn86]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html pip install optax dmhaiku  Run on Single GPU CUDA_VISIBLE_DEVICES=0 python rng.py  Run on Multi GPU CUDA_VISIBLE_DEVICES=0,1 python rng.py """""" from typing import NamedTuple import jax, jax.numpy as jnp import haiku as hk import optax class TrainState(NamedTuple):     params: hk.Params     opt_state: optax.OptState class Batch(NamedTuple):     x: jax.Array     y: jax.Array def model_fn(x: jax.Array) > jax.Array:     def dropout(x: jax.Array) > jax.Array:          return x         return hk.dropout(hk.next_rng_key(), 0.1, x)     def attn(x: jax.Array) > jax.Array:         residual = x         x = hk.MultiHeadAttention(8, 64, w_init=hk.initializers.VarianceScaling(1.0), model_size=256)(x, x, x)         x = hk.LayerNorm(axis=1, create_scale=True, create_offset=True)(x)         x = dropout(x)         x += residual         return x     def ff(x: jax.Array) > jax.Array:         residual = x         x = hk.Linear(256 * 4)(x)         x = jax.nn.relu(x)         x = dropout(x)         x = hk.Linear(256)(x)         x = jax.nn.relu(x)         x += residual         return x     for _ in range(10):         x = attn(x)         x = ff(x)     x = hk.Linear(10)(x)     x = jnp.max(x, axis=2)     return x transformed = hk.transform(model_fn) optimizer = optax.adam(1e3) .jit def init_fn(key: jax.random.KeyArray, batch: Batch) > TrainState:     x, _ = batch     params = transformed.init(key, x)     opt_state = optimizer.init(params)     return TrainState(params, opt_state) def loss_fn(params: hk.Params, key: jax.random.KeyArray, batch: Batch) > jax.Array:     x, y = batch     logits = transformed.apply(params, key, x)     return optax.softmax_cross_entropy(logits=logits, labels=y).mean() .jit def step_fn(key: jax.random.KeyArray, state: TrainState, batch: Batch) > TrainState:     params, opt_state = state     grads = jax.grad(loss_fn)(params, key, batch)     updates, new_opt_state = optimizer.update(grads, opt_state)     new_params = optax.apply_updates(params, updates)     return TrainState(new_params, new_opt_state) if __name__ == ""__main__"":     master_key = jax.random.PRNGKey(42)     init_key, step_key = jax.random.split(master_key)     batch_size = 256     batch = Batch(         x=jnp.zeros((batch_size, 64, 256)),         y=jnp.zeros((batch_size, 10)),     )     state = init_fn(init_key, batch)     sharding = jax.sharding.PositionalSharding(jax.devices())     batch = jax.device_put(batch, jax.tree_util.tree_map(lambda x: sharding.reshape(1, *[1] * (x.ndim1)), batch))     state = jax.device_put(state, sharding.replicate())     state = step_fn(step_key, state, batch)      Time it     import time     start = time.perf_counter()     n = 100     for _ in range(n):         state = step_fn(step_key, state, batch)     end = time.perf_counter()     elapsed = end  start     per_step_ms = elapsed / n * 1000     print(f""Time per step: {per_step_ms:.1f} ms"") ```  A hopefully not too long code example (dispatch cost for small models obscure the difference). Compare `return x` with `return hk.dropout(hk.next_rng_key(), 0.1, x)`, then run single & multi GPU training to see the difference. My local experiment on two RTX 3090 Ti shows ",Thank you!,"Try using `jax.lax.with_sharding_constraint` on dropout? Specifically try this in `def dropout`: `return jax.lax.with_sharding_constraint(kh.dropout(...), some_sharding)` "," I am afraid it's not a direct sharding issue. I further simplify the `dropout` function to this ```python     def dropout(x: jax.Array) > jax.Array:         def print_sharding(x: jax.Array, name: str):             if not hk.running_init():                 jax.debug.inspect_array_sharding(x, callback=lambda sharding: print(name, sharding))         print_sharding(x, ""x before"")         keep = jax.random.bernoulli(jax.random.PRNGKey(42), 0.9, shape=x.shape)         print_sharding(keep, ""keep"")         x *= keep         print_sharding(x, ""x after"")         return x ``` For two GPU case, all calls to `dropout` print ``` x before GSPMDSharding({devices=[2,1,1]0,1}) keep GSPMDSharding({devices=[2,1,1]0,1}) x after GSPMDSharding({devices=[2,1,1]0,1}) ``` It should also be noted that even a constant PRNG key `jax.random.PRNGKey(42)` for `bernoulli` could reproduce this. _But of course this slowdown will go away with_ ```python         with jax.ensure_compile_time_eval():             keep = jax.random.bernoulli(jax.random.PRNGKey(42), 0.9, shape=x.shape)          The following line is optional         keep = jax.lax.with_sharding_constraint(keep, jax.sharding.PositionalSharding(jax.devices()).reshape(2, 1, 1)) ``` _Offtopic: could you leave some comment about CC(Simple data parallelism via jax.Array feels unergonomic to use)? The sharding attached to an array must be in a ""broadcastable"" form, which is especially counterintuitive to use with PyTree?_"
1656,"以下是一个github上的jax下的一个issue, 标题是(jax doesn't find multiple gpu's on A100 migs)， 内容是 ( Description When trying to use `pmap` on an A100 80GB that has been split into two 40GB multiinstance GPUs (migs) it doesn't seem to work, `jax.devices()` only finds a single device. Wrote an minimal example: ```python import jax import jax.numpy as jnp print(f""local device count: {jax.local_device_count()}"") print(f""local devices: {jax.local_devices()}"") print(f""device count: {jax.device_count()}"") print(f""devices: {jax.devices()}"") data = jnp.arange(jax.device_count()) print(f""pmap: {jax.pmap(lambda x: x**2)(data)}"") ``` Here's an example output requesting 2 40GB migs ``` local device count: 1 local devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] device count: 1 devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] pmap: [0] ``` Here's the output on a 2 regular 40GB A100s ``` local device count: 2 local devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0), StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)] device count: 2 devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0), StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)] pmap: [0 1] ``` Any ideas why this could be happening?  What jax/jaxlib version are you using? jax 0.4.2  Which accelerator(s) are you using? A100 40GB and A100 40GB mig  Additional system info _No response_  NVIDIA GPU info Can't run this right now, will update when I have access.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax doesn't find multiple gpu's on A100 migs," Description When trying to use `pmap` on an A100 80GB that has been split into two 40GB multiinstance GPUs (migs) it doesn't seem to work, `jax.devices()` only finds a single device. Wrote an minimal example: ```python import jax import jax.numpy as jnp print(f""local device count: {jax.local_device_count()}"") print(f""local devices: {jax.local_devices()}"") print(f""device count: {jax.device_count()}"") print(f""devices: {jax.devices()}"") data = jnp.arange(jax.device_count()) print(f""pmap: {jax.pmap(lambda x: x**2)(data)}"") ``` Here's an example output requesting 2 40GB migs ``` local device count: 1 local devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] device count: 1 devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] pmap: [0] ``` Here's the output on a 2 regular 40GB A100s ``` local device count: 2 local devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0), StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)] device count: 2 devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0), StreamExecutorGpuDevice(id=1, process_index=0, slice_index=0)] pmap: [0 1] ``` Any ideas why this could be happening?  What jax/jaxlib version are you using? jax 0.4.2  Which accelerator(s) are you using? A100 40GB and A100 40GB mig  Additional system info _No response_  NVIDIA GPU info Can't run this right now, will update when I have access.",2023-05-03T10:06:23Z,bug NVIDIA GPU,closed,0,2,https://github.com/jax-ml/jax/issues/15843,"a Were you able to understand this any better? I'm seeing similar, with 4 A100s split 7ways each with MiG, and Jax only seeing a single device. Here's what I have: ```bash > echo $CUDA_VISIBLE_DEVICES MIGff9ad03d6c8b5f72b0dc70b79aef1bd9,MIG877ee044c0f354bfbb04ef9353d2094c,MIG6175e6006777518b96ea1a51f93610a0,MIG4cd9f19e2fb45477ab15617e911674e4,MIGd08f0b6ed16a536d8f5326f8850b783a,MIG97271b5c3850592c8f807c9940e5edab,MIGc3263cfa42b057118f3db038c7b2f56a,MIG922aabb95ec75c9d85cb21c1916668f4,MIGdd8c0a3182d5505cb04d9c774935ee48,MIG9895b8bdb1ca5d54b9809266de212174,MIGbcbfa7cf3d315d4184d56e3db56ff9bf,MIG750f2c47c72959fd9cf9cea7247506a9,MIGb2d615acf5bd53f09ba1e517e52916fe,MIG03c3ad0d18d15e94bd32067e3a833d9d,MIG5f0f94baeffb59a889c0cd97af7faccf,MIG687e9009be785f55835c20d664b40c6b,MIG21b2c53e6c605f6c88cf2f08b7cbd240,MIG671e55fff9b1514cae6843e549796f0e,MIG10440183cf2c5b5d814a62ea4b517e2f,MIG861fe3a6880451b19ee795c9d02a52f4,MIG89986c59054c53d58dcace9503cbb7a4,MIG4e663d9c62c456738844fc8e3aed670b,MIGc880443460f25c86b94f435d38989c9d,MIG062e8f77da1c5804a13cc37b8323f5ea,MIGfc87c0aace415badbcf886f3ddddf55e,MIGba86e355101451e4a5f9402117564a07,MIGa2c0b32d7ddd56edbf5e91ce51a05a4e,MIGbd3400c1f6b0533690aa8bf82c490ec6 > ipython Python 3.10.9  +++++ ```","This is a CUDA limitation, not a JAX issue per se. Unfortunately MIG was not designed with multiGPU in mind, limiting itself to use cases using less than a full GPU.  So for example it does not create a new CUDA device index for each MIG instance.  To solve this would require work in CUDA first, but then probably also NCCL after that, if not also finally in JAX itself. But beyond that, there are no fast MIGtoMIG communication mechanisms implemented either, so even if the application could use multiMIG in the way you're describing, it would likely see a communication bottleneck between them.  NCCL might end up needing to roundtrip the communications through system memory for example."
296,"以下是一个github上的jax下的一个issue, 标题是(Bump XLACallModule to version 5 and add the function_list.)， 内容是 (Bump XLACallModule to version 5 and add the function_list.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Bump XLACallModule to version 5 and add the function_list.,Bump XLACallModule to version 5 and add the function_list.,2023-05-02T21:08:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15829
1089,"以下是一个github上的jax下的一个issue, 标题是(Fully-Fused MLP implementation)， 内容是 (Dear all, Developers from Nvidia have recently fused MLP network in a large mega kernel and called it ""FullyFused MLP"", which is 5x10x faster than the equivalent MLP implementation from PyTorch or TensorFlow. People currently use these fullyfused MLPs to accelerate training and inference of ML applications, particularly they use them extensively in neural computer graphics to obtain high gains in speed. I would like to know do you maybe plan to implement it or do you have an equivalent implementation? (Im not sure if this can be achieved by simply applying `jax.jit` on normal MLP). I think it is a nice feature, which will enrich the jax library and make it more attractive for machine learning / computer graphics researchers and engineers.  References: [1] Realtime Neural Radiance Caching for Path Tracing [2] Technical Paper [3] Nvidia's Fully fused MLP implementation)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fully-Fused MLP implementation,"Dear all, Developers from Nvidia have recently fused MLP network in a large mega kernel and called it ""FullyFused MLP"", which is 5x10x faster than the equivalent MLP implementation from PyTorch or TensorFlow. People currently use these fullyfused MLPs to accelerate training and inference of ML applications, particularly they use them extensively in neural computer graphics to obtain high gains in speed. I would like to know do you maybe plan to implement it or do you have an equivalent implementation? (Im not sure if this can be achieved by simply applying `jax.jit` on normal MLP). I think it is a nice feature, which will enrich the jax library and make it more attractive for machine learning / computer graphics researchers and engineers.  References: [1] Realtime Neural Radiance Caching for Path Tracing [2] Technical Paper [3] Nvidia's Fully fused MLP implementation",2023-05-02T13:51:18Z,enhancement,closed,2,2,https://github.com/jax-ml/jax/issues/15823,"The advantages of this approach mainly apply to smaller networks with widths up to 128 and completely disappear with MLPs that have width 256 and higher. Initially, the training advantage was about 2x, but this gap has only diminished over time. Though it might be interesting to pinpoint the precise performance difference, developing a custom fully fused MLP in JAX appears unwarranted. The main reason is that the method is highly specialized and the benefits are not substantial enough. Furthermore, attempting such an integration would conflict with JAX's core values such as flexibility and simplicity. Introducing a highly specialized solution would undermine these fundamental principles.",  Thanks for your answer!
9584,"以下是一个github上的jax下的一个issue, 标题是(Segmentation fault in _xla_gc_callback)， 内容是 ( Description I don't have a MWE, but I have a minimum change to a test that reproduces the segmentation fault. Simply cloning the repository and running the test should reproduce the segmentation fault: ```zsh pip install poetry git clone git.com:NeilGirdhar/efax.git cd efax git switch segfault poetry install poetry shell pytest tests/test_distributions.py x ``` gives ``` =============================================================================================================================================== test session starts ================================================================================================================================================ platform linux  Python 3.11.3, pytest7.3.1, pluggy1.0.0 rootdir: /home/neil/src configfile: pyproject.toml plugins: xdist3.2.1, anyio3.6.2 collected 52 items                                                                                                                                                                                                                                                                                                  tests/test_distributions.py .^CException ignored in:  Traceback (most recent call last):   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/jax/_src/lib/__init__.py"", line 97, in _xla_gc_callback     def _xla_gc_callback(*args): KeyboardInterrupt:  Fatal Python error: Segmentation fault Current thread 0x00007ff75a4adb80 (most recent call first):   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 151 in f_locals   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 242 in locals   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 833 in repr_traceback_entry   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 873 in repr_traceback   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 946 in repr_excinfo   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 669 in getrepr   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/nodes.py"", line 484 in _repr_failure_py   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/python.py"", line 1833 in repr_failure   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/reports.py"", line 359 in from_item_and_call   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/runner.py"", line 368 in pytest_runtest_makereport   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/runner.py"", line 224 in call_and_report   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/runner.py"", line 133 in runtestprotocol   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/runner.py"", line 114 in pytest_runtest_protocol   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/main.py"", line 348 in pytest_runtestloop   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/main.py"", line 323 in _main   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/main.py"", line 269 in wrap_session   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/main.py"", line 316 in pytest_cmdline_main   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/config/__init__.py"", line 166 in main   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/config/__init__.py"", line 189 in console_main   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/bin/pytest"", line 8 in  Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, jaxlib.cpu_feature_guard, scipy._lib._ccallback_c, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ellip_harm_2, numpy.linalg.lapack_lite, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._statlib, scipy.stats._mvn, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont (total: 115) ``` Switching back to main should work fine.  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.3)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Segmentation fault in _xla_gc_callback," Description I don't have a MWE, but I have a minimum change to a test that reproduces the segmentation fault. Simply cloning the repository and running the test should reproduce the segmentation fault: ```zsh pip install poetry git clone git.com:NeilGirdhar/efax.git cd efax git switch segfault poetry install poetry shell pytest tests/test_distributions.py x ``` gives ``` =============================================================================================================================================== test session starts ================================================================================================================================================ platform linux  Python 3.11.3, pytest7.3.1, pluggy1.0.0 rootdir: /home/neil/src configfile: pyproject.toml plugins: xdist3.2.1, anyio3.6.2 collected 52 items                                                                                                                                                                                                                                                                                                  tests/test_distributions.py .^CException ignored in:  Traceback (most recent call last):   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/jax/_src/lib/__init__.py"", line 97, in _xla_gc_callback     def _xla_gc_callback(*args): KeyboardInterrupt:  Fatal Python error: Segmentation fault Current thread 0x00007ff75a4adb80 (most recent call first):   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 151 in f_locals   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 242 in locals   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 833 in repr_traceback_entry   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 873 in repr_traceback   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 946 in repr_excinfo   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/_code/code.py"", line 669 in getrepr   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/nodes.py"", line 484 in _repr_failure_py   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/python.py"", line 1833 in repr_failure   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/reports.py"", line 359 in from_item_and_call   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/runner.py"", line 368 in pytest_runtest_makereport   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/runner.py"", line 224 in call_and_report   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/runner.py"", line 133 in runtestprotocol   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/runner.py"", line 114 in pytest_runtest_protocol   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/main.py"", line 348 in pytest_runtestloop   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/main.py"", line 323 in _main   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/main.py"", line 269 in wrap_session   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/main.py"", line 316 in pytest_cmdline_main   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_callers.py"", line 39 in _multicall   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_manager.py"", line 80 in _hookexec   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/pluggy/_hooks.py"", line 265 in __call__   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/config/__init__.py"", line 166 in main   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/lib/python3.11/sitepackages/_pytest/config/__init__.py"", line 189 in console_main   File ""/home/neil/.cache/pypoetry/virtualenvs/efaxZssyUsLUpy3.11/bin/pytest"", line 8 in  Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, jaxlib.cpu_feature_guard, scipy._lib._ccallback_c, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ellip_harm_2, numpy.linalg.lapack_lite, scipy.spatial._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, scipy.optimize._minpack2, scipy.optimize._group_columns, scipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, scipy.optimize._zeros, scipy.optimize.__nnls, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.linalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integrate._lsoda, scipy.special.cython_special, scipy.stats._stats, scipy.stats.beta_ufunc, scipy.stats._boost.beta_ufunc, scipy.stats.binom_ufunc, scipy.stats._boost.binom_ufunc, scipy.stats.nbinom_ufunc, scipy.stats._boost.nbinom_ufunc, scipy.stats.hypergeom_ufunc, scipy.stats._boost.hypergeom_ufunc, scipy.stats.ncf_ufunc, scipy.stats._boost.ncf_ufunc, scipy.stats.ncx2_ufunc, scipy.stats._boost.ncx2_ufunc, scipy.stats.nct_ufunc, scipy.stats._boost.nct_ufunc, scipy.stats.skewnorm_ufunc, scipy.stats._boost.skewnorm_ufunc, scipy.stats.invgauss_ufunc, scipy.stats._boost.invgauss_ufunc, scipy.interpolate._fitpack, scipy.interpolate.dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._rgi_cython, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._statlib, scipy.stats._mvn, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._rcont.rcont (total: 115) ``` Switching back to main should work fine.  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.3",2023-05-01T17:53:21Z,bug,open,1,5,https://github.com/jax-ml/jax/issues/15810,"I have a similar problem with my code, and I can reproduce your problem on macOS 13.3.1, Arm64, Python 3.11.0, same jax and jaxlib versions", Thank you for confirming it!,"This is a known issue with the old Fortran 77 implementation of COBYLA, which is buggy and not maintained anymore. See https://github.com/scipy/scipy/issues/18118 and https://github.com/libprima/primabugfixes. ",Thanks for clarifying  !  I wonder if Jax must depend on COBYLA?,"Hi  , Yes if Jax involves optimization without derivatives. For such problems, Powell's methods are the most widely used ones. For instance, see Section 1 of a recent paper on Powell's solvers as well as the Google searches of COBYLA and BOBYQA. Note that, even though the old Fortran 77 implementation of the aforementioned solvers is truly a masterpiece, it contains many bugs (mostly due to the language itself), which can lead to segmentation faults (as you observed) or infinite loops. For example, see Section 4.4 of the above paper and many GitHub issues. It is strongly discouraged to use the Fortran 77 version of these solvers anymore. Instead, it is advised to switch to the modernized and improved implementation offered by PRIMA.  If you need help with these solvers, I will be very happy to assist.  Thanks and regards, Zaikun"
324,"以下是一个github上的jax下的一个issue, 标题是(Add new attribute `function_list` to XLACallModule and bump the version.)， 内容是 (Add new attribute `function_list` to XLACallModule and bump the version.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add new attribute `function_list` to XLACallModule and bump the version.,Add new attribute `function_list` to XLACallModule and bump the version.,2023-04-29T04:27:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15793
389,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Simplify back_compat_test.py to use jax_export mechanisms to run)， 内容是 ([jax2tf] Simplify back_compat_test.py to use jax_export mechanisms to run the serialized module, instead of relying on tf.XlaCallModule.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Simplify back_compat_test.py to use jax_export mechanisms to run,"[jax2tf] Simplify back_compat_test.py to use jax_export mechanisms to run the serialized module, instead of relying on tf.XlaCallModule.",2023-04-28T14:25:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15787
349,"以下是一个github上的jax下的一个issue, 标题是(Make sure jax2tf back_compat test capture non-backward compabible XlaCallModule)， 内容是 (Make sure jax2tf back_compat test capture nonbackward compabible XlaCallModule op changes.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Make sure jax2tf back_compat test capture non-backward compabible XlaCallModule,Make sure jax2tf back_compat test capture nonbackward compabible XlaCallModule op changes.,2023-04-28T07:32:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15784
2406,"以下是一个github上的jax下的一个issue, 标题是(Notice a different between jax.image.resize and F.interpolate when using ""bicubic"")， 内容是 ( Description Hi, I am trying to convert a Pytorch model to JAX model, and I find there are some implementations different in ""bicubic"".  Here is a script I have used to confirm that the outputs are different.  ``` import torch import torch.nn as nn import jax.numpy as jnp import jax import numpy as np input_arr = jnp.array([     [0, 1, 2, 3, 4],     [5, 6, 7, 8, 9], ]) / 9 torch_input_arr = torch.from_numpy(np.array(input_arr)).unsqueeze(0).unsqueeze(0) output = jax.image.resize(     image=input_arr,     shape=(4,10),     method=""bicubic"") interpolate_without_align_corners = torch.nn.functional.interpolate(     torch_input_arr.float(),     size=(4, 10),     mode=""bicubic"",     align_corners=False, ) print(""jax: "", output) print(""torch interpolate without align corners"",interpolate_without_align_corners) ``` from the prints I get: ``` jax:  [[0.05882353 0.03036592  0.0298608   0.08986928  0.14542481  0.20098035    0.25653598  0.31654444  0.37677112  0.40522873]  [ 0.10527544  0.13373306  0.19395977  0.25396827  0.30952382  0.36507937    0.42063496  0.48064342  0.54087013  0.5693277 ]  [ 0.4306723   0.45912993  0.5193566   0.57936513  0.6349207   0.69047624    0.74603176  0.8060403   0.86626697  0.89472455]  [ 0.5947712   0.62322885  0.6834555   0.74346405  0.79901963  0.85457516    0.9101307   0.9701392   1.0303658   1.0588235 ]] torch interpolate without align corners tensor([[[[0.0703, 0.0373,  0.0156,  0.0855,  0.1306,  0.1966,  0.2418,             0.3116,  0.3646,  0.3976],           [ 0.1141,  0.1471,  0.2001,  0.2700,  0.3151,  0.3811,  0.4262,             0.4961,  0.5490,  0.5820],           [ 0.4180,  0.4510,  0.5039,  0.5738,  0.6189,  0.6849,  0.7300,             0.7999,  0.8529,  0.8859],           [ 0.6024,  0.6354,  0.6884,  0.7582,  0.8034,  0.8694,  0.9145,             0.9844,  1.0373,  1.0703]]]]) ``` I tried some other resizing methods. e.g., linear and bilinear, and they look fine. Does anyone have a workaround for it?   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Notice a different between jax.image.resize and F.interpolate when using ""bicubic"""," Description Hi, I am trying to convert a Pytorch model to JAX model, and I find there are some implementations different in ""bicubic"".  Here is a script I have used to confirm that the outputs are different.  ``` import torch import torch.nn as nn import jax.numpy as jnp import jax import numpy as np input_arr = jnp.array([     [0, 1, 2, 3, 4],     [5, 6, 7, 8, 9], ]) / 9 torch_input_arr = torch.from_numpy(np.array(input_arr)).unsqueeze(0).unsqueeze(0) output = jax.image.resize(     image=input_arr,     shape=(4,10),     method=""bicubic"") interpolate_without_align_corners = torch.nn.functional.interpolate(     torch_input_arr.float(),     size=(4, 10),     mode=""bicubic"",     align_corners=False, ) print(""jax: "", output) print(""torch interpolate without align corners"",interpolate_without_align_corners) ``` from the prints I get: ``` jax:  [[0.05882353 0.03036592  0.0298608   0.08986928  0.14542481  0.20098035    0.25653598  0.31654444  0.37677112  0.40522873]  [ 0.10527544  0.13373306  0.19395977  0.25396827  0.30952382  0.36507937    0.42063496  0.48064342  0.54087013  0.5693277 ]  [ 0.4306723   0.45912993  0.5193566   0.57936513  0.6349207   0.69047624    0.74603176  0.8060403   0.86626697  0.89472455]  [ 0.5947712   0.62322885  0.6834555   0.74346405  0.79901963  0.85457516    0.9101307   0.9701392   1.0303658   1.0588235 ]] torch interpolate without align corners tensor([[[[0.0703, 0.0373,  0.0156,  0.0855,  0.1306,  0.1966,  0.2418,             0.3116,  0.3646,  0.3976],           [ 0.1141,  0.1471,  0.2001,  0.2700,  0.3151,  0.3811,  0.4262,             0.4961,  0.5490,  0.5820],           [ 0.4180,  0.4510,  0.5039,  0.5738,  0.6189,  0.6849,  0.7300,             0.7999,  0.8529,  0.8859],           [ 0.6024,  0.6354,  0.6884,  0.7582,  0.8034,  0.8694,  0.9145,             0.9844,  1.0373,  1.0703]]]]) ``` I tried some other resizing methods. e.g., linear and bilinear, and they look fine. Does anyone have a workaround for it?   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-27T01:00:52Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/15768,"I'm not sure what's up here. The output of JAX exactly matches Pillow and TensorFlow for the same bicubic resize, so I don't think we're doing something wrong here, but PyTorch must be using a different convention for something as opposed to all the other systems."," I could use your input here. I looked into this a bit more. There are two groups of behaviors for cubic upsampling: * pillow, JAX, Tensorflow all agree (almost exactly) * PyTorch (with `align_corners=False`) and OpenCV agree (almost exactly) I think there are two different things happening here:  JAX and PyTorch use different cubic interpolation kernels JAX/pillow/TF use the Keys cubic kernel with A = 0.5 (https://en.wikipedia.org/wiki/Bicubic_interpolationBicubic_convolution_algorithm). PyTorch/OpenCV uses a cubic kernel with A = 0.75. At least some users consider this a bug (https://github.com/opencv/opencv/issues/17720 ), and that set of parameters suffers from ringing artifacts [^mitchell]. [^mitchell]: Mitchell, D.P. and Netravali, A.N., 1988. Reconstruction filters in computergraphics. ACM Siggraph Computer Graphics, 22(4), pp.221228. It's actually quite easy to get JAX to use the PyTorch/OpenCV kernel, if we use: ``` def _fill_pytorch_cubic_kernel(x):   out = ((1.25 * x  2.25) * x) * x + 1.   out = jnp.where(x >= 1., ((0.75 * x + 5*0.75) * x  8 * 0.75) * x + 4*0.75, out)   return jnp.where(x >= 2., 0., out) ``` in place of https://github.com/google/jax/blob/e51d12cdef0fd118e6f8cc3357cd8f95a68a79fa/jax/_src/image/scale.pyL36 This solves the mismatch except for a band of 34 pixels near the edge of the image. This is because:  JAX and PyTorch use different approaches for edge padding Cubic interpolation requires sampling pixel values from outside the original input image. JAX/pillow/TF handle this by truncating the convolutional kernel and rescaling it to keep its total weight 1. I believe PyTorch handles this by edgepadding (i.e., repeating the value on the edge). This leads to slightly different values in a 34 pixel band near each edge. I'll look into this a bit further, we can probably replicate the PyTorch behavior (optionally) with a bit more work.","I'm curious if it's actually important to you that we replicate PyTorch's behavior exactly, or you merely noticed it is different and were curious about it.","Hi  Thanks for digging into these differences! I actually tried to reproduce some research works in JAX and noticed there are some performance degrades, then I started to look into some lowerlevel implementations and found out this : )"," I don't know about pytorch but your description of JAX's resize sounds correct, and yes there are different choices for the bicuibic kernel. The other thing to keep in mind is that when downsampling JAX's resize will do antialiasing by default.", I'm particularly curious about the edge padding behavior. How do the OpenCV/PyTorch behaviors compare with what JAX/TF/pillow do?,I should add: it's actually slightly annoying to mimic the PyTorch edge padding behavior *because* of the antialiasing we do on downsampling. The antialiasing means the kernel can be of an arbitrary width measured in input pixels.,"TF is the same: https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/refs/heads/master/tensorflow/core/kernels/image/scale_and_translate_op.(未找到相关数据) I think we could mimic the PyTorch edge behavior if we wanted to, no, we'd just shift the kernel weight to that last pixel and make it zero outside? That said, I'm skeptical this will cause large enough differences to break models. ","Hi  , thanks for digging into this and the great write up.  I have a similar issue but I want to replicate JAX's behaviour in Torch.  Based on your findings, is it possible to do this with an appropriate set of args to `jax.image.resize`? Thanks!",">  I could use your input here. >  > I looked into this a bit more. There are two groups of behaviors for cubic upsampling: >  > * pillow, JAX, Tensorflow all agree (almost exactly) > * PyTorch (with `align_corners=False`) and OpenCV agree (almost exactly) >  > I think there are two different things happening here: >  >  JAX and PyTorch use different cubic interpolation kernels > JAX/pillow/TF use the Keys cubic kernel with A = 0.5 (https://en.wikipedia.org/wiki/Bicubic_interpolationBicubic_convolution_algorithm). >  > PyTorch/OpenCV uses a cubic kernel with A = 0.75. At least some users consider this a bug ([opencv/opencv CC(MAINT Use a generator expression in tuple([... for ... in ...]))](https://github.com/opencv/opencv/issues/17720) ), and that set of parameters suffers from ringing artifacts 1. >  > It's actually quite easy to get JAX to use the PyTorch/OpenCV kernel, if we use: >  > ``` > def _fill_pytorch_cubic_kernel(x): >   out = ((1.25 * x  2.25) * x) * x + 1. >   out = jnp.where(x >= 1., ((0.75 * x + 5*0.75) * x  8 * 0.75) * x + 4*0.75, out) >   return jnp.where(x >= 2., 0., out) > ``` >  > in place of >  > https://github.com/google/jax/blob/e51d12cdef0fd118e6f8cc3357cd8f95a68a79fa/jax/_src/image/scale.pyL36 >  > This solves the mismatch except for a band of 34 pixels near the edge of the image. This is because: >  >  JAX and PyTorch use different approaches for edge padding > Cubic interpolation requires sampling pixel values from outside the original input image. JAX/pillow/TF handle this by truncating the convolutional kernel and rescaling it to keep its total weight 1. I believe PyTorch handles this by edgepadding (i.e., repeating the value on the edge). This leads to slightly different values in a 34 pixel band near each edge. >  > I'll look into this a bit further, we can probably replicate the PyTorch behavior (optionally) with a bit more work. >  >  Footnotes > 1. Mitchell, D.P. and Netravali, A.N., 1988. Reconstruction filters in computergraphics. ACM Siggraph Computer Graphics, 22(4), pp.221228. ↩ Hi , it would be really nice if this work is continued. I'm trying to convert a pytorch model to flax and this ""Bicubic interpolate"" layer is the only part that prevents the exact replication of the model.  Thank you."
324,"以下是一个github上的jax下的一个issue, 标题是(Add new attribute `function_list` to XLACallModule and bump the version.)， 内容是 (Add new attribute `function_list` to XLACallModule and bump the version.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add new attribute `function_list` to XLACallModule and bump the version.,Add new attribute `function_list` to XLACallModule and bump the version.,2023-04-27T00:41:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15767
4554,"以下是一个github上的jax下的一个issue, 标题是(Using `ir.F64Type()` for `hlo.CustomCallOp`'s `out_type`)， 内容是 ( Description Hi! I have been trying to bind my c++ method into a jax primitive. The C++ backend essentially takes in an array and outputs a double. I was unable to make it work with specifying the `out_type` as `ir.F64Type()` which is what I would expect the type to be if my c++ backend function returns a double. What I ended up with is the following code below: ```python from functools import partial import sys import numpy as np import jax import jax.numpy as jnp import jax._src.test_util as jtu from pyleftfield import pylefty, Registrations from jax import core, dtypes from jax.interpreters import xla, mlir from jax.lib import xla_client from jax.interpreters.mlir import ir from jaxlib.hlo_helpers import custom_call from jax.abstract_arrays import ShapedArray   Inspired from:    https://github.com/dfm/extendingjax/    https://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html  __all__ = [""pylefty_ll""]  Register functions defined in pylefty as custom call target for CPUs for _name, _value in Registrations().items():         xla_client.register_custom_call_target(_name, _value, platform=""cpu"")  This function exposes the primitive to user code and this is the only  publicfacing function in this module def pylefty_ll(shat):     return _pylefty_ll_prim.bind(shat) _pylefty_ll_fwd_p = core.Primitive(""pylefty_ll_fwd"") _pylefty_ll_fwd_p.multiple_results = False _pylefty_ll_fwd_p.def_impl(partial(xla.apply_primitive, _pylefty_ll_fwd_p)) def pylefty_ll_fwd(shat):     output, invvar = _pylefty_ll_prim.bind(shat)     return output  ctx  mlir context  shat  JAX primitive argument def _pylefty_ll_fwd_lowering(ctx, shat):     shat_aval = ctx.avals_in[0]  only one argument here     np_dtype = np.dtype(shat_aval.dtype)      Check the type is np.float64, since we're working only with that!     assert np_dtype == np.float64     shat_type = ir.RankedTensorType(shat.type)     shat_shape = shat_type.shape     layout = tuple(range(len(shat_shape)1,1,1))     out = custom_call(         b""pylefty_ll_forward_f64"",          Output of the XLACPU_EFTlikelihood call is just double         out_types=[ir.RankedTensorType.get((), ir.F64Type.get())],          Inputs of the XLACPU_EFTlikelihood         operands=[shat],          Layout specification         operand_layouts=[layout],          Result is just a number!         result_layouts=[()]     )     return (out,) def _pylefty_ll_fwd_abstract(shat):     shat_dtype = dtypes.canonicalize_dtype(shat.dtype)     assert shat_dtype == jnp.dtype(np.float64)     return ShapedArray((), jnp.dtype(np.float64)) _pylefty_ll_prim = core.Primitive(""pylefty_ll"") _pylefty_ll_prim.multiple_results = False _pylefty_ll_prim.def_impl(partial(xla.apply_primitive, _pylefty_ll_prim)) _pylefty_ll_prim.def_abstract_eval(_pylefty_ll_fwd_abstract) mlir.register_lowering(     _pylefty_ll_prim,     _pylefty_ll_fwd_lowering,     platform=""cpu"" ) ```  So I ended up used the `RankedTensorType`, since if I replace the line: ```python  out_types=[ir.RankedTensorType.get((), ir.F64Type.get())], ``` with ```python  out_types=[ir.F64Type.get())], ``` I get that `mlir` doesn't recognize the type, i.e. the error I obtain is ```text 'stablehlo.custom_call' op result CC(未找到相关数据) must be tensor of f8E4M3FN type or f8E5M2 type or 16bit float or 32bit float or 64bit float or bfloat16 type or pred (AKA boolean or 1bit integer) or 4/8/16/32/64bit signless integer or 4/8/16/32/64bit unsigned integer or complex type with 32bit float or 64bit float elements or 4/8/16/32bit uniform quantized signed integer or 4/8/16/32bit uniform quantized unsigned integer values or token or nested tuple with any combination of tensor of f8E4M3FN type or f8E5M2 type or 16bit float or 32bit float or 64bit float or bfloat16 type or pred (AKA boolean or 1bit integer) or 4/8/16/32/64bit signless integer or 4/8/16/32/64bit unsigned integer or complex type with 32bit float or 64bit float elements or 4/8/16/32bit uniform quantized signed integer or 4/8/16/32bit uniform quantized unsigned integer values or token values, but got 'f64' ``` I don't understand why `f64` is not accepted as a viable type?  What jax/jaxlib version are you using? jax v0.4.8,  jaxlib v0.4.7  Which accelerator(s) are you using? CPU  Additional system info python3.9, x86_64 GNU/Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Using `ir.F64Type()` for `hlo.CustomCallOp`'s `out_type`," Description Hi! I have been trying to bind my c++ method into a jax primitive. The C++ backend essentially takes in an array and outputs a double. I was unable to make it work with specifying the `out_type` as `ir.F64Type()` which is what I would expect the type to be if my c++ backend function returns a double. What I ended up with is the following code below: ```python from functools import partial import sys import numpy as np import jax import jax.numpy as jnp import jax._src.test_util as jtu from pyleftfield import pylefty, Registrations from jax import core, dtypes from jax.interpreters import xla, mlir from jax.lib import xla_client from jax.interpreters.mlir import ir from jaxlib.hlo_helpers import custom_call from jax.abstract_arrays import ShapedArray   Inspired from:    https://github.com/dfm/extendingjax/    https://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html  __all__ = [""pylefty_ll""]  Register functions defined in pylefty as custom call target for CPUs for _name, _value in Registrations().items():         xla_client.register_custom_call_target(_name, _value, platform=""cpu"")  This function exposes the primitive to user code and this is the only  publicfacing function in this module def pylefty_ll(shat):     return _pylefty_ll_prim.bind(shat) _pylefty_ll_fwd_p = core.Primitive(""pylefty_ll_fwd"") _pylefty_ll_fwd_p.multiple_results = False _pylefty_ll_fwd_p.def_impl(partial(xla.apply_primitive, _pylefty_ll_fwd_p)) def pylefty_ll_fwd(shat):     output, invvar = _pylefty_ll_prim.bind(shat)     return output  ctx  mlir context  shat  JAX primitive argument def _pylefty_ll_fwd_lowering(ctx, shat):     shat_aval = ctx.avals_in[0]  only one argument here     np_dtype = np.dtype(shat_aval.dtype)      Check the type is np.float64, since we're working only with that!     assert np_dtype == np.float64     shat_type = ir.RankedTensorType(shat.type)     shat_shape = shat_type.shape     layout = tuple(range(len(shat_shape)1,1,1))     out = custom_call(         b""pylefty_ll_forward_f64"",          Output of the XLACPU_EFTlikelihood call is just double         out_types=[ir.RankedTensorType.get((), ir.F64Type.get())],          Inputs of the XLACPU_EFTlikelihood         operands=[shat],          Layout specification         operand_layouts=[layout],          Result is just a number!         result_layouts=[()]     )     return (out,) def _pylefty_ll_fwd_abstract(shat):     shat_dtype = dtypes.canonicalize_dtype(shat.dtype)     assert shat_dtype == jnp.dtype(np.float64)     return ShapedArray((), jnp.dtype(np.float64)) _pylefty_ll_prim = core.Primitive(""pylefty_ll"") _pylefty_ll_prim.multiple_results = False _pylefty_ll_prim.def_impl(partial(xla.apply_primitive, _pylefty_ll_prim)) _pylefty_ll_prim.def_abstract_eval(_pylefty_ll_fwd_abstract) mlir.register_lowering(     _pylefty_ll_prim,     _pylefty_ll_fwd_lowering,     platform=""cpu"" ) ```  So I ended up used the `RankedTensorType`, since if I replace the line: ```python  out_types=[ir.RankedTensorType.get((), ir.F64Type.get())], ``` with ```python  out_types=[ir.F64Type.get())], ``` I get that `mlir` doesn't recognize the type, i.e. the error I obtain is ```text 'stablehlo.custom_call' op result CC(未找到相关数据) must be tensor of f8E4M3FN type or f8E5M2 type or 16bit float or 32bit float or 64bit float or bfloat16 type or pred (AKA boolean or 1bit integer) or 4/8/16/32/64bit signless integer or 4/8/16/32/64bit unsigned integer or complex type with 32bit float or 64bit float elements or 4/8/16/32bit uniform quantized signed integer or 4/8/16/32bit uniform quantized unsigned integer values or token or nested tuple with any combination of tensor of f8E4M3FN type or f8E5M2 type or 16bit float or 32bit float or 64bit float or bfloat16 type or pred (AKA boolean or 1bit integer) or 4/8/16/32/64bit signless integer or 4/8/16/32/64bit unsigned integer or complex type with 32bit float or 64bit float elements or 4/8/16/32bit uniform quantized signed integer or 4/8/16/32bit uniform quantized unsigned integer values or token values, but got 'f64' ``` I don't understand why `f64` is not accepted as a viable type?  What jax/jaxlib version are you using? jax v0.4.8,  jaxlib v0.4.7  Which accelerator(s) are you using? CPU  Additional system info python3.9, x86_64 GNU/Linux  NVIDIA GPU info _No response_",2023-04-26T18:13:41Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15758,"This is an stablehlo MLIR dialect issue, not a JAX issue, but I believe they simply only allow RankedTensor types as inputs or outputs to most operators. If you want a scalar, you need to wrap it a `RankedTensor`. I should add: we are hoping to add a more userfriendly FFI so you don't have to write your own stablehlo lowerings in most cases.","Thanks for answering!  > This is an stablehlo MLIR dialect issue, not a JAX issue, but I believe they simply only allow RankedTensor types as inputs or outputs to most operators. If you want a scalar, you need to wrap it a RankedTensor. Yes I guess it is more of a MLIR dialect issue than pure JAX issue, I hoped I could try to better understand what MLIR really wants through posting this issue. Where can I find more on exact objects defined within the mlir dialect and their attributes? I am not sure what is the correct material to read, so I usually play with the debugger and try to guess what objects are needed, so it would be nice to properly read up on what exactly is happening under the hood when doing the MLIR lowering. "
1641,"以下是一个github上的jax下的一个issue, 标题是(JAX 0.4.8 on CUDA 12.0, H100, Driver: 525.60.13  yields CUDNN_STATUS_INTERNAL_ERROR)， 内容是 ( Description Hi there,  On H100 GPU with CUDA 12.0, NvidiaDriver 525.60.13 Jax seems to have some issues. First a sample to reproduce and below that the error message: `import jax.numpy as jnp a = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) b = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) result = jnp.matmul(a, b) print(""Result: "", result)` 20230426 07:08:38.181946: W external/xla/xla/stream_executor/cuda/cuda_dnn.cc:397] There was an error before creating cudnn handle: cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found. 20230426 07:08:38.182026: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR Traceback (most recent call last):   File ""bug_report_script.py"", line 7, in      result = jnp.matmul(a, b) ..... ..... jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. As an additional note, I observed a similar error on the A100, but then it was due to the GPU being memorywise occupied. In this case the GPU is completely free. Thank you in advance. Best Sebastien  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU H100  Additional system info Ubuntu 22.04 LTS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"JAX 0.4.8 on CUDA 12.0, H100, Driver: 525.60.13  yields CUDNN_STATUS_INTERNAL_ERROR"," Description Hi there,  On H100 GPU with CUDA 12.0, NvidiaDriver 525.60.13 Jax seems to have some issues. First a sample to reproduce and below that the error message: `import jax.numpy as jnp a = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) b = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) result = jnp.matmul(a, b) print(""Result: "", result)` 20230426 07:08:38.181946: W external/xla/xla/stream_executor/cuda/cuda_dnn.cc:397] There was an error before creating cudnn handle: cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found. 20230426 07:08:38.182026: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR Traceback (most recent call last):   File ""bug_report_script.py"", line 7, in      result = jnp.matmul(a, b) ..... ..... jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. As an additional note, I observed a similar error on the A100, but then it was due to the GPU being memorywise occupied. In this case the GPU is completely free. Thank you in advance. Best Sebastien  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU H100  Additional system info Ubuntu 22.04 LTS  NVIDIA GPU info _No response_",2023-04-26T07:24:29Z,bug NVIDIA GPU,closed,0,7,https://github.com/jax-ml/jax/issues/15752,Hi Sebastien! Is this on a cloud platform or your local setup?,"Hi, Thank you for your reply! A vendor setup the H100 server and we accessing it via ssh for testing purposes.","When I run the example as given in the OP on H100, JAX hangs for a long time and then gives the same error: ``` E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 20230428 22:43:31.202338: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 525.85.12 Traceback (most recent call last):   File ""/usr/local/lib/python3.10/distpackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/usr/local/lib/python3.10/distpackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ```",Will look into that further.,"The example works with CUDA 12.1: ```  nvidiasmi Sat Apr 29 02:04:19 2023        ++  ++  python Python 3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import jax.numpy as jnp >>> a = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) >>> b = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) >>> result = jnp.matmul(a, b) >>> print(""Result: "", result) Result:  [[12 18 24]  [30 45 60]  [12 18 24]] ```","Is this still an issue? Given yhtang@'s comments, I suspect everything is fine now. Support for H100 has also matured significantly since April.","There was no update since November, and we routinely run on H100, so I think this problem was fixed by either a newer CUDA or a newer JAX."
977,"以下是一个github上的jax下的一个issue, 标题是(Make shard_map_test compatible with custom_prng)， 内容是 (Tested: ``` $ JAX_ENABLE_CUSTOM_PRNG=1 pytest tests/shard_map_test.py k test_prngkeyarray_eager ===================================== test session starts ===================================== platform darwin  Python 3.8.2, pytest7.2.2, pluggy0.13.1 rootdir: /Users/vanderplas/github/google/jax, configfile: pyproject.toml plugins: env0.6.2, xdist3.2.1, cov2.10.0, forked1.3.0, reportlog0.1.2 collected 103 items / 102 deselected / 1 selected                                              tests/shard_map_test.py .                                                               [100%] ============================== 1 passed, 102 deselected in 2.01s ============================== ``` There's no CI test coverage for this yet, but we're getting there...)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Make shard_map_test compatible with custom_prng,"Tested: ``` $ JAX_ENABLE_CUSTOM_PRNG=1 pytest tests/shard_map_test.py k test_prngkeyarray_eager ===================================== test session starts ===================================== platform darwin  Python 3.8.2, pytest7.2.2, pluggy0.13.1 rootdir: /Users/vanderplas/github/google/jax, configfile: pyproject.toml plugins: env0.6.2, xdist3.2.1, cov2.10.0, forked1.3.0, reportlog0.1.2 collected 103 items / 102 deselected / 1 selected                                              tests/shard_map_test.py .                                                               [100%] ============================== 1 passed, 102 deselected in 2.01s ============================== ``` There's no CI test coverage for this yet, but we're getting there...",2023-04-25T18:40:36Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15744
7293,"以下是一个github上的jax下的一个issue, 标题是( DNN library initialization failed.)， 内容是 ( Description I tried run AlphaFoldMultimer on GPU, after the parameters and inputs are loaded, the running stopped by raising: `jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ` I can't find out the exact error information from the error message I have. Following the errors: > I0425 13:55:15.099713 140484539389760 af_multimer_multiseed.py:552] Running model model_1_multimer_v2_pred_0 I0425 13:55:15.100178 140484539389760 model.py:165] Running predict with shape(feat) = {'aatype': (247,), 'residue_index': (247,), 'seq_length': (), 'msa': (4096, 247), 'num_alignments': (), 'template_all_atom_positions': (4, 247, 37, 3), 'template_all_atom_mask': (4, 247, 37), 'template_aatype': (4, 247), 'asym_id': (247,), 'sym_id': (247,), 'entity_id': (247,), 'deletion_matrix': (4096, 247), 'deletion_mean': (247,), 'all_atom_mask': (247, 37), 'all_atom_positions': (247, 37, 3), 'assembly_num_chains': (), 'entity_mask': (247,), 'num_templates': (), 'cluster_bias_mask': (4096,), 'bert_mask': (4096, 247), 'seq_mask': (247,), 'msa_mask': (4096, 247)} 20230425 13:55:17.465601: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.4.1 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. Traceback (most recent call last):   File ""/home/user/project/alphafold/multiseed/af_multimer_multiseed.py"", line 676, in      app.run(main)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/user/project/alphafold/multiseed/af_multimer_multiseed.py"", line 666, in main     predict_structure(FLAGS.output_dir, model_runners, random_seed, multimer_feature, FLAGS.num_prediction, FLAGS.model_preset, FLAGS.enable_single_seed, FLAGS.single_seed)   File ""/home/user/project/alphafold/multiseed/af_multimer_multiseed.py"", line 564, in predict_structure     prediction_result = model_runner.predict(processed_feature_dict,   File ""/home/user/project/alphafold/alphafold/alphafold/model/model.py"", line 167, in predict     result = self.apply(self.params, jax.random.PRNGKey(random_seed), feat)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/random.py"", line 136, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 270, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 561, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 573, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 578, in random_seed_impl_base     return seed(seeds)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 813, in threefry_seed     lax.shift_right_logical(seed, lax_internal._const(seed, 32)))   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/lax/lax.py"", line 458, in shift_right_logical     return shift_right_logical_p.bind(x, y)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 117, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 208, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 254, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2816, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 3028, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.  What jax/jaxlib version are you using? jax version 0.4.8 jaxlib version: 0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi, DNN library initialization failed.," Description I tried run AlphaFoldMultimer on GPU, after the parameters and inputs are loaded, the running stopped by raising: `jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ` I can't find out the exact error information from the error message I have. Following the errors: > I0425 13:55:15.099713 140484539389760 af_multimer_multiseed.py:552] Running model model_1_multimer_v2_pred_0 I0425 13:55:15.100178 140484539389760 model.py:165] Running predict with shape(feat) = {'aatype': (247,), 'residue_index': (247,), 'seq_length': (), 'msa': (4096, 247), 'num_alignments': (), 'template_all_atom_positions': (4, 247, 37, 3), 'template_all_atom_mask': (4, 247, 37), 'template_aatype': (4, 247), 'asym_id': (247,), 'sym_id': (247,), 'entity_id': (247,), 'deletion_matrix': (4096, 247), 'deletion_mean': (247,), 'all_atom_mask': (247, 37), 'all_atom_positions': (247, 37, 3), 'assembly_num_chains': (), 'entity_mask': (247,), 'num_templates': (), 'cluster_bias_mask': (4096,), 'bert_mask': (4096, 247), 'seq_mask': (247,), 'msa_mask': (4096, 247)} 20230425 13:55:17.465601: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.4.1 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. Traceback (most recent call last):   File ""/home/user/project/alphafold/multiseed/af_multimer_multiseed.py"", line 676, in      app.run(main)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/user/project/alphafold/multiseed/af_multimer_multiseed.py"", line 666, in main     predict_structure(FLAGS.output_dir, model_runners, random_seed, multimer_feature, FLAGS.num_prediction, FLAGS.model_preset, FLAGS.enable_single_seed, FLAGS.single_seed)   File ""/home/user/project/alphafold/multiseed/af_multimer_multiseed.py"", line 564, in predict_structure     prediction_result = model_runner.predict(processed_feature_dict,   File ""/home/user/project/alphafold/alphafold/alphafold/model/model.py"", line 167, in predict     result = self.apply(self.params, jax.random.PRNGKey(random_seed), feat)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/random.py"", line 136, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 270, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 561, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 573, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 578, in random_seed_impl_base     return seed(seeds)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/prng.py"", line 813, in threefry_seed     lax.shift_right_logical(seed, lax_internal._const(seed, 32)))   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/lax/lax.py"", line 458, in shift_right_logical     return shift_right_logical_p.bind(x, y)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 117, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 208, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 254, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2816, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 3028, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/user/miniconda3/envs/alpha_mul_v3/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.  What jax/jaxlib version are you using? jax version 0.4.8 jaxlib version: 0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ++  ++",2023-04-25T17:59:04Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15742,"The error message explains the problem: ```  Loaded runtime CuDNN library: 8.4.1 but source was compiled with: 8.6.0. CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library. ``` Upgrade your CuDNN installation?",">  Thanks! Problem solved. I upgraded the installation by running `pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Following https://github.com/google/jaxinstallation"
2987,"以下是一个github上的jax下的一个issue, 标题是(Seemingly odd behaviour with make_jaxpr)， 内容是 (Hello, I am trying to understand exactly how autodiff works with JAX. I have an example function: ``` def fnc_jax(x1, x2):     return (jnp.divide(x1,x2)  jnp.exp(x2))*(jnp.sin(jnp.divide(x1,x2)) + jnp.divide(x1,x2)  jnp.exp(x2)) ``` Now I do `jax.make_jaxpr(fnc_jax)(1.0,1.0)`, which gives me the following output: ``` { lambda ; a:f32[] b:f32[]. let     c:f32[] = div a b     d:f32[] = exp b     e:f32[] = sub c d     f:f32[] = div a b     g:f32[] = sin f     h:f32[] = div a b     i:f32[] = add g h     j:f32[] = exp b     k:f32[] = sub i j     l:f32[] = mul e k   in (l,) } ``` Question 1: Why is it that c, e, and h for example are doing the exact same computation? Wouldn't a better way be if we had something like this: ``` def fnc_jax_alt(x1, x2):     a = x1/x2           b = np.exp(x2)          c = np.sin(a)     d = a  b     e = c + d     g = d * e     return g ``` which gives the same output for a given input as `fnc_jax`, but doesnt repeat computations confirmed using `jax.make_jaxpr(fnc_jax_alt)(1.0,1.0)`. Is this a feature or a bug?  Another confusion I have is when I do `jax.make_jaxpr(jax.grad(fnc_jax))(1.0,1.0)`: ``` { lambda ; a:f32[] b:f32[]. let     c:f32[] = div a b     d:f32[] = exp b     e:f32[] = sub c d     f:f32[] = div a b     g:f32[] = sin f     h:f32[] = cos f     i:f32[] = div a b     j:f32[] = add g i     k:f32[] = exp b     l:f32[] = sub j k     _:f32[] = mul e l     m:f32[] = mul e 1.0     n:f32[] = mul 1.0 l     o:f32[] = div m b     p:f32[] = mul m h     q:f32[] = div p b     r:f32[] = add_any o q     s:f32[] = div n b     t:f32[] = add_any r s   in (t,) } ``` Question 2: Apart from repeated computations, what confuses me is what exactly is going on here? It seems to me like it is doing something of a forward differentiation. But if I understand correctly from the documentation, `jax.grad` performs reversemode differentiation by default. Am I wrong or misunderstanding the make_jaxpr? If it is forwardmode differentiation, then should we not have something like the following: ``` { lambda ; a:f32[] b:f32[]. let     c:f32[] = div a b     d:f32[] = exp b     e:f32[] = sin c     f:f32[] = sub c d     g:f32[] = add e f     _:f32[] = mul f g     h:f32[] = div 1.0 b     i:f32[] = cos c     j:f32[] = mul i h     k:f32[] = sub h 0.0     l:f32[] = add j k     m:f32[] = mul k g     n:f32[] = mul f l     o:f32[] = add m n   in (o,) } ``` If it is reversemode differentiation, how can I get a `make_jaxpr` for forwardmode? Apologies in advance for my misunderstandings. I would appreciate some clarity on these questions! Regards, Jay Sandesara  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Seemingly odd behaviour with make_jaxpr,"Hello, I am trying to understand exactly how autodiff works with JAX. I have an example function: ``` def fnc_jax(x1, x2):     return (jnp.divide(x1,x2)  jnp.exp(x2))*(jnp.sin(jnp.divide(x1,x2)) + jnp.divide(x1,x2)  jnp.exp(x2)) ``` Now I do `jax.make_jaxpr(fnc_jax)(1.0,1.0)`, which gives me the following output: ``` { lambda ; a:f32[] b:f32[]. let     c:f32[] = div a b     d:f32[] = exp b     e:f32[] = sub c d     f:f32[] = div a b     g:f32[] = sin f     h:f32[] = div a b     i:f32[] = add g h     j:f32[] = exp b     k:f32[] = sub i j     l:f32[] = mul e k   in (l,) } ``` Question 1: Why is it that c, e, and h for example are doing the exact same computation? Wouldn't a better way be if we had something like this: ``` def fnc_jax_alt(x1, x2):     a = x1/x2           b = np.exp(x2)          c = np.sin(a)     d = a  b     e = c + d     g = d * e     return g ``` which gives the same output for a given input as `fnc_jax`, but doesnt repeat computations confirmed using `jax.make_jaxpr(fnc_jax_alt)(1.0,1.0)`. Is this a feature or a bug?  Another confusion I have is when I do `jax.make_jaxpr(jax.grad(fnc_jax))(1.0,1.0)`: ``` { lambda ; a:f32[] b:f32[]. let     c:f32[] = div a b     d:f32[] = exp b     e:f32[] = sub c d     f:f32[] = div a b     g:f32[] = sin f     h:f32[] = cos f     i:f32[] = div a b     j:f32[] = add g i     k:f32[] = exp b     l:f32[] = sub j k     _:f32[] = mul e l     m:f32[] = mul e 1.0     n:f32[] = mul 1.0 l     o:f32[] = div m b     p:f32[] = mul m h     q:f32[] = div p b     r:f32[] = add_any o q     s:f32[] = div n b     t:f32[] = add_any r s   in (t,) } ``` Question 2: Apart from repeated computations, what confuses me is what exactly is going on here? It seems to me like it is doing something of a forward differentiation. But if I understand correctly from the documentation, `jax.grad` performs reversemode differentiation by default. Am I wrong or misunderstanding the make_jaxpr? If it is forwardmode differentiation, then should we not have something like the following: ``` { lambda ; a:f32[] b:f32[]. let     c:f32[] = div a b     d:f32[] = exp b     e:f32[] = sin c     f:f32[] = sub c d     g:f32[] = add e f     _:f32[] = mul f g     h:f32[] = div 1.0 b     i:f32[] = cos c     j:f32[] = mul i h     k:f32[] = sub h 0.0     l:f32[] = add j k     m:f32[] = mul k g     n:f32[] = mul f l     o:f32[] = add m n   in (o,) } ``` If it is reversemode differentiation, how can I get a `make_jaxpr` for forwardmode? Apologies in advance for my misunderstandings. I would appreciate some clarity on these questions! Regards, Jay Sandesara  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-25T15:26:30Z,question,closed,0,6,https://github.com/jax-ml/jax/issues/15736," For your Q1: Eager mode (not jitted) JAX runs `fnc_jax` opbyop, so indeed the computations are duplicated, and `jax.make_jaxpr` only reflects this fact. However, if you jit your function, CSE will kickin, and `divide` only occurred once: ```python print(jit(fnc_jax).lower(1., 1.).compile().as_text()) ``` ``` HloModule jit_fnc_jax, entry_computation_layout={(f64[],f64[])>f64[]}, allow_spmd_sharding_propagation_to_output={true} fused_computation {   param_0.2 = f64[] parameter(0)   param_1.4 = f64[] parameter(1)   exponential.0 = f64[] exponential(param_1.4)   subtract.1 = f64[] subtract(param_0.2, exponential.0)   sine.0 = f64[] sine(param_0.2)   add.0 = f64[] add(sine.0, param_0.2)   subtract.0 = f64[] subtract(add.0, exponential.0)   ROOT multiply.0 = f64[] multiply(subtract.1, subtract.0) } ENTRY main.13 {   Arg_0.1 = f64[] parameter(0), sharding={replicated}   Arg_1.2 = f64[] parameter(1), sharding={replicated}   divide.3 = f64[] divide(Arg_0.1, Arg_1.2)   ROOT fusion = f64[] fusion(divide.3, Arg_1.2), kind=kLoop, calls=fused_computation } ``` Assuming your functions in JAX will be jitted, you generally don't need to worry about duplicate computation, and you can code it the way that looks cleanest/makes most sense conceptually.","> Question 1: Why is it that c, e, and h for example are doing the exact same computation?  The reason for this is that jaxprs do not produce optimized code, they merely produce an intermediate representation of *the code you wrote*. Because you computed `jnp.divide(x, y)` three times, the computation is represented three times in the jaxpr. Never fear, though: when you JITcompile the code, the compiler recognizes these repeated operations and will deduplicate them, freeing you from having to worry about doing so manually. You can see this by printing the compiled HLO, though it's admittedly harder to read than the jaxpr: ```python print(jax.jit(fnc_jax).lower(1.0, 1.0).compile().as_text()) ``` ``` %fused_computation (param_0.2: f32[], param_1.4: f32[]) > f32[] {   %param_0.2 = f32[] parameter(0)   %param_1.4 = f32[] parameter(1)   %exponential.0 = f32[] exponential(f32[] %param_1.4), metadata={op_name=""jit(fnc_jax)/jit(main)/exp"" source_file="""" source_line=5}   %subtract.1 = f32[] subtract(f32[] %param_0.2, f32[] %exponential.0), metadata={op_name=""jit(fnc_jax)/jit(main)/sub"" source_file="""" source_line=5}   %sine.0 = f32[] sine(f32[] %param_0.2), metadata={op_name=""jit(fnc_jax)/jit(main)/sin"" source_file="""" source_line=5}   %add.0 = f32[] add(f32[] %sine.0, f32[] %param_0.2), metadata={op_name=""jit(fnc_jax)/jit(main)/add"" source_file="""" source_line=5}   %subtract.0 = f32[] subtract(f32[] %add.0, f32[] %exponential.0), metadata={op_name=""jit(fnc_jax)/jit(main)/sub"" source_file="""" source_line=5}   ROOT %multiply.0 = f32[] multiply(f32[] %subtract.1, f32[] %subtract.0), metadata={op_name=""jit(fnc_jax)/jit(main)/mul"" source_file="""" source_line=5} } ENTRY %main.13 (Arg_0.1: f32[], Arg_1.2: f32[]) > f32[] {   %Arg_0.1 = f32[] parameter(0), sharding={replicated}   %Arg_1.2 = f32[] parameter(1), sharding={replicated}   %divide.3 = f32[] divide(f32[] %Arg_0.1, f32[] %Arg_1.2), metadata={op_name=""jit(fnc_jax)/jit(main)/div"" source_file="""" source_line=5}   ROOT %fusion = f32[] fusion(f32[] %divide.3, f32[] %Arg_1.2), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit(fnc_jax)/jit(main)/mul"" source_file="""" source_line=5} } ``` > Question 2: Apart from repeated computations, what confuses me is what exactly is going on here?  `jax.grad` is implemented in terms of reversemode differentiation, so the generated jaxpr reflects the gradient computed in reverse mode. There's no exact forwardmode equivalent of `jax.grad`, but you can get roughly the same thing by computing the jaxpr of `jax.jacfwd`: ```python print(jax.make_jaxpr(jax.jacfwd(fnc_jax))(1.0,1.0)) ``` ``` { lambda ; a:f32[] b:f32[]. let     c:i32[1,1] = iota[dimension=0 dtype=int32 shape=(1, 1)]      d:i32[1,1] = add c 0     e:i32[1,1] = iota[dimension=1 dtype=int32 shape=(1, 1)]      f:bool[1,1] = eq d e     g:f32[1,1] = convert_element_type[new_dtype=float32 weak_type=False] f     h:f32[1,1] = slice[limit_indices=(1, 1) start_indices=(0, 0) strides=None] g     i:f32[1] = reshape[dimensions=None new_sizes=(1,)] h     j:f32[1] = convert_element_type[new_dtype=float32 weak_type=True] i     k:f32[] = div a b     l:f32[1] = div j b     m:f32[] = exp b     n:f32[] = sub k m     o:f32[] = div a b     p:f32[1] = div j b     q:f32[] = sin o     r:f32[] = cos o     s:f32[1] = mul p r     t:f32[] = div a b     u:f32[1] = div j b     v:f32[] = add q t     w:f32[1] = add s u     x:f32[] = exp b     y:f32[] = sub v x     _:f32[] = mul n y     z:f32[1] = mul l y     ba:f32[1] = mul n w     bb:f32[1] = add_any z ba     bc:f32[1] = slice[limit_indices=(1,) start_indices=(0,) strides=None] bb     bd:f32[] = reshape[dimensions=None new_sizes=()] bc   in (bd,) } ``` The `iota` and `slice` stuff in the beginning comes from the fact that `jacfwd` is more general than `grad`, but the rest reflects the unoptimized forwardmode gradient.",Thanks a lot  and  for the clear explanations! ,"Sorry to reopen, but I have one follow up question: As you suggested, I perform forwardmode autodiff using jacfwd and reverse mode using jacrev.  From what I understand, jacrev should be far more efficient when computing the gradient of a scalar valued function with a large input. However, for a toy computation like the following, I see the opposite! Toy function (really doesn't do anything useful, just made it this way so I can pass arbitrarily large arrays): ``` def fn(tuple_arr):     x = 1.0     for param in tuple_arr:         x *= (param**2param**3param)     return x ``` Timing it: ``` %%timeit r1 n1 jax.jacrev(fn)(jnp.array(np.ones(1000))).block_until_ready() ``` Output: ``` 46 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) ``` ``` %%timeit r1 n1 jax.jacfwd(fn)(jnp.array(np.ones(1000))).block_until_ready() ``` Output: ``` 26.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each) ``` Is this something to do with the function itself? Or that I am using CPU, which might be struggling with memory transfers? Considering that reversemode uses more memory?","A good rule of thumb is that any time you're writing `for` loops over array elements, you're going to end up with a very inefficient implementation. I'd probably write your function this way, in which case you see the computational characteristics you'd expect. ```python import jax  import jax.numpy as jnp def fn(tuple_arr):   return jnp.prod(tuple_arr ** 2  tuple_arr ** 3  tuple_arr) %timeit jax.jacrev(fn)(jnp.ones(1000)).block_until_ready()  18 ms ± 2.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) %timeit jax.jacfwd(fn)(jnp.ones(1000)).block_until_ready()  46.2 ms ± 8.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) ``` Maybe something about autodiffing a 1000step unrolled loop makes jacrev less efficient than jacfwd? I'm not sure. In any case, that's a pattern you should avoid when possible.",Thank you for the clarification!
6860,"以下是一个github上的jax下的一个issue, 标题是(slow function/gradient evaluation for complex-valued parameters)， 内容是 ( Description Hello everybody, thank you very much for all your work on jax!  I noticed a performance issue while writing a likelihood function. The issue is possibly a version of CC(matmul slow for complex dtypes). The function looks something like this: ``` def dummyLikelihood(T, data): 	sumOfAmps = data @ T 	return np.sum(np.log(np.abs(sumOfAmps)**2)) ``` [I attached a selfcontained example at the end] Where data is of shape (nmbEvents, nDim) and dtype is complex128 and T is of length nDim also of dtype complex128. Typical values are nmbEvents ~ 100000; nDim ~ 1000. From my understanding the calculation should be mostly limited by memory bandwidth. I noticed a slow down (for the jitted version of the function) as compared to an implementation using autograd or original numpy (both using openBLAS). The same is true for the gradient/value_and_grad of the function. The slowdown disappears as soon as I switch to float. I found an implementation that is much faster (even than the original numpy implementation) by using complex dtypes only internally: ```  def dummyLikelihood_jnp_fast(T_real, data_re, data_im):     T = T_real[::2] + 1.j*T_real[1::2]     data = data_re + 1.j*data_im     sumOfAmps = data @ T     return jnp.sum(jnp.log(jnp.real(jnp.abs(sumOfAmps)**2))) ``` However, the gradient/value_and_grad of this function is then even slower than the original complexvalued implementation. I got the following measurements with my example code: ``` timing likelihood numpy: dtype is: complex128 average evaluation time (s):     0.0439 timing likelihood autograd: dtype is: complex128 average evaluation time (s):     0.0433 timing likelihood jax: dtype is: complex128 average evaluation time (s):     0.0644 timing likelihood numpy: dtype is: float64 average evaluation time (s):     0.0214 timing likelihood autograd: dtype is: float64 average evaluation time (s):     0.0213 timing likelihood jax: dtype is: float64 average evaluation time (s):     0.0213 timing value_and_grad autograd: dtype is: complex128 average evaluation time (s):     0.0877 timing value_and_grad jax: dtype is: complex128 average evaluation time (s):     0.5855 timing value_and_grad autograd: dtype is: float64 average evaluation time (s):     0.0447 timing value_and_grad jax: dtype is: float64 average evaluation time (s):     0.0406 ``` I hope that I am not missing something here. Thank you in advance for your help. Example: ```  switch between float64 and float32 useFloat64 = True  realisitic number of events nmbEvents = 100000  realistic number of dimensions nDim = 800 import os  disable multithreading by default: we do not want to allow openBLAS to be faster just by multithreading os.environ['OPENBLAS_NUM_THREADS'] = ""1"" from jax import jit import jaxlib import jax from jax.config import config config.update('jax_platform_name', 'cpu') config.update(""jax_enable_x64"", useFloat64) from jax import numpy as jnp from autograd import numpy as anp import numpy as onp dtype=onp.float64 if useFloat64 else onp.float32 from jax import value_and_grad as jax_value_and_grad from autograd import value_and_grad as autograd_value_and_grad import time def time_function(testName, fun, T, data, nIter=10):     print(""timing {0}:"".format(testName))     print(""dtype is: {0}"".format(str(T.dtype)))     block = type(T[0]) is jaxlib.xla_extension.ArrayImpl      initial call to trigger compile for jax     val = fun(T, data)     if type(val) is tuple:         isVgrad=True     else:         isVgrad=False     if block:         vblock = val[0] if isVgrad else val         vblock.block_until_ready()       start = time.time()     for i in range(nIter):         val = fun(T, data)         if block:             vblock = val[0] if isVgrad else val             vblock.block_until_ready()     end = time.time()     print(""average evaluation time (s): {:10.4f}"".format((end  start)/nIter))     print()  dummy likelihoods  def dummyLikelihood_numpy(T, data): 	sumOfAmps = data @ T 	return onp.sum(onp.log(onp.abs(sumOfAmps)**2)) def dummyLikelihood_autograd(T, data): 	sumOfAmps = data @ T 	return anp.sum(anp.log(anp.abs(sumOfAmps)**2)) dummyLikelihood_autograd_vgr = autograd_value_and_grad(dummyLikelihood_autograd)  def dummyLikelihood_jax(T, data):     sumOfAmps = data @ T     return jnp.sum(jnp.log(jnp.abs(sumOfAmps)**2)) dummyLikelihood_jax_vgr = jit(jax_value_and_grad(dummyLikelihood_jax))  create pseudodata onp_data_re = onp.random.rand(nmbEvents,nDim).astype(dtype) onp_data_im = onp.random.rand(nmbEvents,nDim).astype(dtype) onp_data = onp_data_re + 1.j * onp_data_im onp_T_re = onp.random.rand(nDim).astype(dtype) onp_T_im = onp.random.rand(nDim).astype(dtype)  onp_T = onp_T_re + 1.j*onp_T_im  copy to autograd anp_data    = anp.array(onp_data,       copy=True) anp_T       = anp.array(onp_T,          copy=True) anp_data_re    = anp.array(onp_data_re, copy=True) anp_T_re       = anp.array(onp_T_re,    copy=True)  copy to jax jnp_data    = jnp.array(onp_data,       copy=True) jnp_T       = jnp.array(onp_T,          copy=True) jnp_data_re    = jnp.array(onp_data_re, copy=True) jnp_T_re       = jnp.array(onp_T_re,    copy=True) functionList = [                     {'testName': 'likelihood numpy', 'fun': dummyLikelihood_numpy, 'T': onp_T, 'data': onp_data},                     {'testName': 'likelihood autograd','fun': dummyLikelihood_autograd, 'T': anp_T, 'data': anp_data},                     {'testName': 'likelihood jax', 'fun': dummyLikelihood_jax, 'T': jnp_T, 'data': jnp_data},                     {'testName': 'likelihood numpy', 'fun': dummyLikelihood_numpy, 'T': onp_T_re, 'data': onp_data_re},                     {'testName': 'likelihood autograd', 'fun': dummyLikelihood_autograd, 'T': anp_T_re, 'data': anp_data_re},                     {'testName': 'likelihood jax', 'fun': dummyLikelihood_jax, 'T': jnp_T_re, 'data': jnp_data_re},                     {'testName': 'value_and_grad autograd', 'fun': dummyLikelihood_autograd_vgr, 'T': anp_T, 'data': anp_data},                     {'testName': 'value_and_grad jax', 'fun': dummyLikelihood_jax_vgr, 'T': jnp_T, 'data': jnp_data},                     {'testName': 'value_and_grad autograd', 'fun': dummyLikelihood_autograd_vgr, 'T': anp_T_re, 'data': anp_data_re},                     {'testName': 'value_and_grad jax', 'fun': dummyLikelihood_jax_vgr, 'T': jnp_T_re, 'data': jnp_data_re},                 ] for test in functionList:     time_function(**test) ```  What jax/jaxlib version are you using? jax 0.4.8/jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.10; Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,slow function/gradient evaluation for complex-valued parameters," Description Hello everybody, thank you very much for all your work on jax!  I noticed a performance issue while writing a likelihood function. The issue is possibly a version of CC(matmul slow for complex dtypes). The function looks something like this: ``` def dummyLikelihood(T, data): 	sumOfAmps = data @ T 	return np.sum(np.log(np.abs(sumOfAmps)**2)) ``` [I attached a selfcontained example at the end] Where data is of shape (nmbEvents, nDim) and dtype is complex128 and T is of length nDim also of dtype complex128. Typical values are nmbEvents ~ 100000; nDim ~ 1000. From my understanding the calculation should be mostly limited by memory bandwidth. I noticed a slow down (for the jitted version of the function) as compared to an implementation using autograd or original numpy (both using openBLAS). The same is true for the gradient/value_and_grad of the function. The slowdown disappears as soon as I switch to float. I found an implementation that is much faster (even than the original numpy implementation) by using complex dtypes only internally: ```  def dummyLikelihood_jnp_fast(T_real, data_re, data_im):     T = T_real[::2] + 1.j*T_real[1::2]     data = data_re + 1.j*data_im     sumOfAmps = data @ T     return jnp.sum(jnp.log(jnp.real(jnp.abs(sumOfAmps)**2))) ``` However, the gradient/value_and_grad of this function is then even slower than the original complexvalued implementation. I got the following measurements with my example code: ``` timing likelihood numpy: dtype is: complex128 average evaluation time (s):     0.0439 timing likelihood autograd: dtype is: complex128 average evaluation time (s):     0.0433 timing likelihood jax: dtype is: complex128 average evaluation time (s):     0.0644 timing likelihood numpy: dtype is: float64 average evaluation time (s):     0.0214 timing likelihood autograd: dtype is: float64 average evaluation time (s):     0.0213 timing likelihood jax: dtype is: float64 average evaluation time (s):     0.0213 timing value_and_grad autograd: dtype is: complex128 average evaluation time (s):     0.0877 timing value_and_grad jax: dtype is: complex128 average evaluation time (s):     0.5855 timing value_and_grad autograd: dtype is: float64 average evaluation time (s):     0.0447 timing value_and_grad jax: dtype is: float64 average evaluation time (s):     0.0406 ``` I hope that I am not missing something here. Thank you in advance for your help. Example: ```  switch between float64 and float32 useFloat64 = True  realisitic number of events nmbEvents = 100000  realistic number of dimensions nDim = 800 import os  disable multithreading by default: we do not want to allow openBLAS to be faster just by multithreading os.environ['OPENBLAS_NUM_THREADS'] = ""1"" from jax import jit import jaxlib import jax from jax.config import config config.update('jax_platform_name', 'cpu') config.update(""jax_enable_x64"", useFloat64) from jax import numpy as jnp from autograd import numpy as anp import numpy as onp dtype=onp.float64 if useFloat64 else onp.float32 from jax import value_and_grad as jax_value_and_grad from autograd import value_and_grad as autograd_value_and_grad import time def time_function(testName, fun, T, data, nIter=10):     print(""timing {0}:"".format(testName))     print(""dtype is: {0}"".format(str(T.dtype)))     block = type(T[0]) is jaxlib.xla_extension.ArrayImpl      initial call to trigger compile for jax     val = fun(T, data)     if type(val) is tuple:         isVgrad=True     else:         isVgrad=False     if block:         vblock = val[0] if isVgrad else val         vblock.block_until_ready()       start = time.time()     for i in range(nIter):         val = fun(T, data)         if block:             vblock = val[0] if isVgrad else val             vblock.block_until_ready()     end = time.time()     print(""average evaluation time (s): {:10.4f}"".format((end  start)/nIter))     print()  dummy likelihoods  def dummyLikelihood_numpy(T, data): 	sumOfAmps = data @ T 	return onp.sum(onp.log(onp.abs(sumOfAmps)**2)) def dummyLikelihood_autograd(T, data): 	sumOfAmps = data @ T 	return anp.sum(anp.log(anp.abs(sumOfAmps)**2)) dummyLikelihood_autograd_vgr = autograd_value_and_grad(dummyLikelihood_autograd)  def dummyLikelihood_jax(T, data):     sumOfAmps = data @ T     return jnp.sum(jnp.log(jnp.abs(sumOfAmps)**2)) dummyLikelihood_jax_vgr = jit(jax_value_and_grad(dummyLikelihood_jax))  create pseudodata onp_data_re = onp.random.rand(nmbEvents,nDim).astype(dtype) onp_data_im = onp.random.rand(nmbEvents,nDim).astype(dtype) onp_data = onp_data_re + 1.j * onp_data_im onp_T_re = onp.random.rand(nDim).astype(dtype) onp_T_im = onp.random.rand(nDim).astype(dtype)  onp_T = onp_T_re + 1.j*onp_T_im  copy to autograd anp_data    = anp.array(onp_data,       copy=True) anp_T       = anp.array(onp_T,          copy=True) anp_data_re    = anp.array(onp_data_re, copy=True) anp_T_re       = anp.array(onp_T_re,    copy=True)  copy to jax jnp_data    = jnp.array(onp_data,       copy=True) jnp_T       = jnp.array(onp_T,          copy=True) jnp_data_re    = jnp.array(onp_data_re, copy=True) jnp_T_re       = jnp.array(onp_T_re,    copy=True) functionList = [                     {'testName': 'likelihood numpy', 'fun': dummyLikelihood_numpy, 'T': onp_T, 'data': onp_data},                     {'testName': 'likelihood autograd','fun': dummyLikelihood_autograd, 'T': anp_T, 'data': anp_data},                     {'testName': 'likelihood jax', 'fun': dummyLikelihood_jax, 'T': jnp_T, 'data': jnp_data},                     {'testName': 'likelihood numpy', 'fun': dummyLikelihood_numpy, 'T': onp_T_re, 'data': onp_data_re},                     {'testName': 'likelihood autograd', 'fun': dummyLikelihood_autograd, 'T': anp_T_re, 'data': anp_data_re},                     {'testName': 'likelihood jax', 'fun': dummyLikelihood_jax, 'T': jnp_T_re, 'data': jnp_data_re},                     {'testName': 'value_and_grad autograd', 'fun': dummyLikelihood_autograd_vgr, 'T': anp_T, 'data': anp_data},                     {'testName': 'value_and_grad jax', 'fun': dummyLikelihood_jax_vgr, 'T': jnp_T, 'data': jnp_data},                     {'testName': 'value_and_grad autograd', 'fun': dummyLikelihood_autograd_vgr, 'T': anp_T_re, 'data': anp_data_re},                     {'testName': 'value_and_grad jax', 'fun': dummyLikelihood_jax_vgr, 'T': jnp_T_re, 'data': jnp_data_re},                 ] for test in functionList:     time_function(**test) ```  What jax/jaxlib version are you using? jax 0.4.8/jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.10; Linux  NVIDIA GPU info _No response_",2023-04-25T14:23:10Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/15735,"My results on  Python 3.10.10, Linux 6.2.2, Intel(R) Core(TM) i52520M CPU @ 2.50GHz. JAX is three times slower than the autograd version of value_and_gradient. ``` timing likelihood numpy: dtype is: complex128 average evaluation time (s):     0.2172 timing likelihood autograd: dtype is: complex128 average evaluation time (s):     0.2159 timing likelihood jax: dtype is: complex128 average evaluation time (s):     0.1312 timing likelihood numpy: dtype is: float64 average evaluation time (s):     0.0930 timing likelihood autograd: dtype is: float64 average evaluation time (s):     0.0926 timing likelihood jax: dtype is: float64 average evaluation time (s):     0.0426 timing value_and_grad autograd: dtype is: complex128 average evaluation time (s):     0.3901 timing value_and_grad jax: dtype is: complex128 average evaluation time (s):     0.9244 timing value_and_grad autograd: dtype is: float64 average evaluation time (s):     0.1719 timing value_and_grad jax: dtype is: float64 average evaluation time (s):     0.0849 ```"
492,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Add support for calling an exported JAX function)， 内容是 (Previously, the only way we could use an Exported object was via tf.XlaCallModule. Here we add support for calling the Exported object directly from JAX, without TF. There is support for custom gradients, and pytrees, but not for shape polymorphism (yet).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Add support for calling an exported JAX function,"Previously, the only way we could use an Exported object was via tf.XlaCallModule. Here we add support for calling the Exported object directly from JAX, without TF. There is support for custom gradients, and pytrees, but not for shape polymorphism (yet).",2023-04-25T11:32:42Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/15733,  PTAL
706,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Fix float0 representation under native serialization)， 内容是 (Previously, jax2tf used int32 zeros for float0 while the JAX native lowering uses bool. The actual type does not make any difference for graph serialization because TF will not use the values. But for native serialization this meant that we passed wrongdtype arguments to the exported modules. This error was masked by the shape refinement logic in XlaCallModule, which set the type of the module formal arguments to be the same as the type of the actual arguments.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Fix float0 representation under native serialization,"Previously, jax2tf used int32 zeros for float0 while the JAX native lowering uses bool. The actual type does not make any difference for graph serialization because TF will not use the values. But for native serialization this meant that we passed wrongdtype arguments to the exported modules. This error was masked by the shape refinement logic in XlaCallModule, which set the type of the module formal arguments to be the same as the type of the actual arguments.",2023-04-25T11:22:22Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/15732, PTAL,  PTAL since Junwhan is OOO
1925,"以下是一个github上的jax下的一个issue, 标题是(shard_map gives strange ValueError when called with the wrong number of arguments)， 内容是 ( Description ```python import jax import jax.numpy as jnp from jax.experimental import shard_map import chex import functools chex.set_n_cpu_devices(2) mesh = jax.sharding.Mesh(jax.devices(), axis_names=('x',)) spec = jax.sharding.PartitionSpec('x') sharding = jax.sharding.NamedSharding(mesh, spec) .partial(     shard_map.shard_map, mesh=mesh, in_specs=(spec, spec), out_specs=spec, ) def f(x, y):   return x + y x = jax.device_put(jnp.arange(8), sharding) f(x) ``` Results in the error: ``` ValueError: Tuple arity mismatch: 1 != 2; tuple: (Array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32),). During handling of the above exception, another exception occurred: UnfilteredStackTrace                      Traceback (most recent call last) UnfilteredStackTrace: ValueError: pytree structure error: different lengths of tuple at key path     shard_map in_specs At that key path, the prefix pytree shard_map in_specs has a subtree of type tuple of length 2, but the full pytree has a subtree of the same type but of length 1. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified. ``` The right way to call this function is `f(x, y)`, of course. I expected to see an error message like what I get for supplying the wrong number of arguments to a standard Python function: ```python def f(x, y):   return x + y f(x)   TypeError: f() missing 1 required positional argument: 'y' ``` (this is what I see for jax.jit decorated functions.)  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,shard_map gives strange ValueError when called with the wrong number of arguments," Description ```python import jax import jax.numpy as jnp from jax.experimental import shard_map import chex import functools chex.set_n_cpu_devices(2) mesh = jax.sharding.Mesh(jax.devices(), axis_names=('x',)) spec = jax.sharding.PartitionSpec('x') sharding = jax.sharding.NamedSharding(mesh, spec) .partial(     shard_map.shard_map, mesh=mesh, in_specs=(spec, spec), out_specs=spec, ) def f(x, y):   return x + y x = jax.device_put(jnp.arange(8), sharding) f(x) ``` Results in the error: ``` ValueError: Tuple arity mismatch: 1 != 2; tuple: (Array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32),). During handling of the above exception, another exception occurred: UnfilteredStackTrace                      Traceback (most recent call last) UnfilteredStackTrace: ValueError: pytree structure error: different lengths of tuple at key path     shard_map in_specs At that key path, the prefix pytree shard_map in_specs has a subtree of type tuple of length 2, but the full pytree has a subtree of the same type but of length 1. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified. ``` The right way to call this function is `f(x, y)`, of course. I expected to see an error message like what I get for supplying the wrong number of arguments to a standard Python function: ```python def f(x, y):   return x + y f(x)   TypeError: f() missing 1 required positional argument: 'y' ``` (this is what I see for jax.jit decorated functions.)  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-25T03:36:13Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/15728,I hit the same issue.
976,"以下是一个github上的jax下的一个issue, 标题是(Cannot use jit for @classmethod)， 内容是 ( Description I'm trying to `jax.jit` a  but it does not work. ``` import jax import jax.numpy as jnp from functools import partial class A:     def __init__(self):         pass          (jax.jit, static_argnums=0)   this one does not work as well     .jit     def zeros(shape):         return jnp.zeros(shape) print(A.zeros((2, 3))) ``` I got `TypeError: Cannot interpret value of type  as an abstract array; it does not have a dtype attribute` If I use `(jax.jit, static_argnums=0)` instead of `.jit`, I got `TypeError: A.zeros() takes 1 positional argument but 2 were given`.  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info python 3.11.3, Ubuntu 18.04  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot use jit for @classmethod," Description I'm trying to `jax.jit` a  but it does not work. ``` import jax import jax.numpy as jnp from functools import partial class A:     def __init__(self):         pass          (jax.jit, static_argnums=0)   this one does not work as well     .jit     def zeros(shape):         return jnp.zeros(shape) print(A.zeros((2, 3))) ``` I got `TypeError: Cannot interpret value of type  as an abstract array; it does not have a dtype attribute` If I use `(jax.jit, static_argnums=0)` instead of `.jit`, I got `TypeError: A.zeros() takes 1 positional argument but 2 were given`.  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info python 3.11.3, Ubuntu 18.04  NVIDIA GPU info _No response_",2023-04-25T02:54:07Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/15727,"try this ``` class A:     def __init__(self):         pass          (jax.jit, static_argnums=(0,1))     def zeros(cls, shape):         return jnp.zeros(shape) ```","Thank you very much! I'm getting into a different but relevant problem: ``` import jax import jax.numpy as jnp from functools import partial class A:     def __init__(self):         pass          (jax.jit, static_argnums=(0, 1))     def zeros(cls, shape):         return jnp.zeros(shape)          (jax.jit, static_argnums=(0, 1))     def concatenate(cls, arrays):         return jnp.concatenate(arrays) .jit def concatenate(arrays):     return jnp.concatenate(arrays) print(A.zeros((2, 3))) print(concatenate((jnp.array([0, 1]), jnp.array([0, 1])))) print(A.concatenate((jnp.array([0, 1]), jnp.array([0, 1])))) ``` The first two lines work well, but I got `TypeError: unhashable type: 'ArrayImpl'` for the last line.","For the above example, I somehow find `(jax.jit, static_argnums=0)` works for `A.concatenate`. Why do I have to use different arguments for the two similar functions?","`static_argnums` indicates which arguments to a JITcompiled function should be static. Array properties like shapes and dtypes must be static, while arrays themselves cannot be static. With that in mind, the reason that `static_argnums=(0, 1)` is appropriate for the first function is that argument `1` is a shape (which must be static). The reason that `static_argnums=(0, 1)` is not appropriate for the second function is because argument `1` is an array or list of arrays (which must not be static). For more conceptual background on these topics, you might find this doc useful: https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html"
244,"以下是一个github上的jax下的一个issue, 标题是(Prngkey asarray)， 内容是 (Builds on CC(PRNGKeyArrayImpl: add aval property))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Prngkey asarray,Builds on CC(PRNGKeyArrayImpl: add aval property),2023-04-21T23:53:03Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/15706
277,"以下是一个github上的jax下的一个issue, 标题是(PRNGKeyArrayImpl: add aval property)， 内容是 (This makes it more readily compatible with jax.numpy routines.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,PRNGKeyArrayImpl: add aval property,This makes it more readily compatible with jax.numpy routines.,2023-04-21T23:24:40Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/15705,"I think since `pytype_aval_mappings` uses the attribute now, existing test coverage is sufficient, what do you think?",Sounds good. Merge away!
752,"以下是一个github上的jax下的一个issue, 标题是(Reland: [XLA:Python] Add buffer protocol support to jax.Array)， 内容是 (Reland: [XLA:Python] Add buffer protocol support to jax.Array We supported the buffer protocol on the older DeviceArray class; port that support to jax.Array. The previous attempt was reverted because it led to a C++ CHECK failure if the buffer was deleted while an external Python reference was held. Change the CPU PJRT client to keep the underlying buffer alive as long as there are external references, which is what the contract of Delete() says it will do. Fixes https://github.com/google/jax/issues/14713)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Reland: [XLA:Python] Add buffer protocol support to jax.Array,"Reland: [XLA:Python] Add buffer protocol support to jax.Array We supported the buffer protocol on the older DeviceArray class; port that support to jax.Array. The previous attempt was reverted because it led to a C++ CHECK failure if the buffer was deleted while an external Python reference was held. Change the CPU PJRT client to keep the underlying buffer alive as long as there are external references, which is what the contract of Delete() says it will do. Fixes https://github.com/google/jax/issues/14713",2023-04-21T14:20:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15697
286,"以下是一个github上的jax下的一个issue, 标题是(Include the device_kind in the compilation cache key.)， 内容是 (Include the device_kind in the compilation cache key.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Include the device_kind in the compilation cache key.,Include the device_kind in the compilation cache key.,2023-04-20T02:06:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15678
1894,"以下是一个github上的jax下的一个issue, 标题是(jacfwd fails on a function that returns BCOO)， 内容是 ( Description This code returns a sparse weight vector for linearly interpolating the input point `x` on a onedimensional regular grid: ``` def w(x, M) > BCOO:     r""linear interpolation weights for x \in [0, 1] with M equispaced grid points""     z = jnp.linspace(0, 1, M)     i = jnp.searchsorted(z, x)  1     delta = 1 / (M  1)     w = (x  z[i]) / delta     data = jnp.array([1.  w, w])     inds = jnp.array([[i, i + 1]]).T     return BCOO((data, inds), shape=(M,)) ``` The primal output does what I expect: ``` >>> w(.3, 5) BCOO(float32[5], nse=2) >>> w(.3, 5).todense()   Array([0.        , 0.79999995, 0.20000005, 0.        , 0.        ],      dtype=float32) ``` The Jacobian of above should be a sparse vector with entries `[0, 1/delta, 1/delta, 0, 0]`. Indeed, this is what I get if I cast the output to dense: ``` >>> jacfwd(lambda x: w(x, 5).todense())(.3) Array([ 0., 4.,  4.,  0.,  0.], dtype=float32) ``` However, for some reason jacfwd chokes if the function returns a `BCOO`: ``` >>> jacfwd(w)(.3, .5) ... File ~/opt/py310/lib/python3.10/sitepackages/jax/experimental/sparse/bcoo.py:146, in _validate_bcoo_indices(indices, shape)     145 def _validate_bcoo_indices(indices: Buffer, shape: Sequence[int]) > BCOOProperties: > 146   assert jnp.issubdtype(indices.dtype, jnp.integer)     147   shape = tuple(shape)     148   nse, n_sparse = indices.shape[2:] AssertionError:  ``` Is this just a limitation of the current sparse matrix implementation? Or is there some sort of workaround?  What jax/jaxlib version are you using? jax v0.4.8, jaxlib v0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.10  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jacfwd fails on a function that returns BCOO," Description This code returns a sparse weight vector for linearly interpolating the input point `x` on a onedimensional regular grid: ``` def w(x, M) > BCOO:     r""linear interpolation weights for x \in [0, 1] with M equispaced grid points""     z = jnp.linspace(0, 1, M)     i = jnp.searchsorted(z, x)  1     delta = 1 / (M  1)     w = (x  z[i]) / delta     data = jnp.array([1.  w, w])     inds = jnp.array([[i, i + 1]]).T     return BCOO((data, inds), shape=(M,)) ``` The primal output does what I expect: ``` >>> w(.3, 5) BCOO(float32[5], nse=2) >>> w(.3, 5).todense()   Array([0.        , 0.79999995, 0.20000005, 0.        , 0.        ],      dtype=float32) ``` The Jacobian of above should be a sparse vector with entries `[0, 1/delta, 1/delta, 0, 0]`. Indeed, this is what I get if I cast the output to dense: ``` >>> jacfwd(lambda x: w(x, 5).todense())(.3) Array([ 0., 4.,  4.,  0.,  0.], dtype=float32) ``` However, for some reason jacfwd chokes if the function returns a `BCOO`: ``` >>> jacfwd(w)(.3, .5) ... File ~/opt/py310/lib/python3.10/sitepackages/jax/experimental/sparse/bcoo.py:146, in _validate_bcoo_indices(indices, shape)     145 def _validate_bcoo_indices(indices: Buffer, shape: Sequence[int]) > BCOOProperties: > 146   assert jnp.issubdtype(indices.dtype, jnp.integer)     147   shape = tuple(shape)     148   nse, n_sparse = indices.shape[2:] AssertionError:  ``` Is this just a limitation of the current sparse matrix implementation? Or is there some sort of workaround?  What jax/jaxlib version are you using? jax v0.4.8, jaxlib v0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.10  NVIDIA GPU info _No response_",2023-04-18T18:34:37Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15654,"This is expected, unfortunately. It's not possible to override the semantics of `jax.jacfwd` to work correctly for sparse outputs. This should give you the result you're after: ```python mat = w(0.3, 5) mat.data = jax.jacfwd(lambda x, M: w(x, M).data)(0.3, 5) print(mat.todense())  [ 0. 4.  4.  0.  0.] ``` Note that we have alternative autodiff operator definitions for sparse matrices, e.g. `jax.experimental.sparse.jacfwd`, but they are currently only implemented for sparse inputs, not sparse outputs."
415,"以下是一个github上的jax下的一个issue, 标题是(Import jax.experimental.compilation_cache.compilation_cache by default.)， 内容是 (Import jax.experimental.compilation_cache.compilation_cache by default. This is to fix users who were relying on this module being imported as part of 'import jax'.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Import jax.experimental.compilation_cache.compilation_cache by default.,Import jax.experimental.compilation_cache.compilation_cache by default. This is to fix users who were relying on this module being imported as part of 'import jax'.,2023-04-18T14:49:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15651
2388,"以下是一个github上的jax下的一个issue, 标题是(Apparent Floating Point Error)， 内容是 ( Description Hello, I have been working on optimizing some routines for some machine learning methods we would like to apply to our work, but I seem to be getting some difference between the original numpy version of this code, and my, likely, semi optimized version in Jax. I attach an example which is reproducible on my machine below. If I am missing something somewhere, I apologize, this is my first pass at using Jax ```python import jax from jax import random, vmap, jit import jax.numpy as jnp import numpy as np  Numpy def np_computeMeanVec(X, y, n):     mean_vectors = []     for cl in n:         mask = y==cl         mean_vectors.append(np.mean(X[mask], axis=0))     return np.stack(mean_vectors)  JAX def get_mask(y: jnp.ndarray, n: int):     mask = y==n     return mask.flatten() def computeMeanVec(X: jnp.ndarray, mask: jnp.ndarray) > jnp.ndarray:     """"""Compues the ddimensional mean vectors for the class defined by mask     Args:         X (jnp.ndarray): Feature matrix         mask (jnp.ndarray): Mask defining class membership     Returns:         jnp.ndarray: mean feature vector for class defined by mask     """"""     return jnp.mean(X, where=mask.reshape(1,1), axis=0) vmapMask = jit(vmap(get_mask, in_axes=(None,0))) vmapCMV = jit(vmap(computeMeanVec, in_axes=(None,1))) def computeAllMeanVec(X, y, n):     mask = vmapMask(y.reshape(1,1), n).T     return vmapCMV(X,mask) if __name__ == ""__main__"":     key = jax.random.PRNGKey(42)      offending float is 4.2950333e+09     X = np.full((400,2), 4.2950333e+09).astype(np.float32)     y = np.full((1,400), 2)[0].astype(np.int32)     n = np.unique(y).astype(np.int32)      Reference     mean_org = np_computeMeanVec(X, y, n)     X = jnp.array(X, dtype=jnp.float32)     y = jnp.array(y, dtype=jnp.int32)     n = jnp.array(n, dtype=jnp.int32)      JAX Optimized     mean_vectors = computeAllMeanVec(X, y, n)      err > [[24064. 24064.]]     err = mean_vectors  mean_org         print(err) ``` Thanks, Alex  What jax/jaxlib version are you using? jax v0.4.8  Which accelerator(s) are you using? GPU  Additional system info Python 3.11.3, Ubuntu 22.04  NVIDIA GPU info ``` Mon Apr 17 22:09:24 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Apparent Floating Point Error," Description Hello, I have been working on optimizing some routines for some machine learning methods we would like to apply to our work, but I seem to be getting some difference between the original numpy version of this code, and my, likely, semi optimized version in Jax. I attach an example which is reproducible on my machine below. If I am missing something somewhere, I apologize, this is my first pass at using Jax ```python import jax from jax import random, vmap, jit import jax.numpy as jnp import numpy as np  Numpy def np_computeMeanVec(X, y, n):     mean_vectors = []     for cl in n:         mask = y==cl         mean_vectors.append(np.mean(X[mask], axis=0))     return np.stack(mean_vectors)  JAX def get_mask(y: jnp.ndarray, n: int):     mask = y==n     return mask.flatten() def computeMeanVec(X: jnp.ndarray, mask: jnp.ndarray) > jnp.ndarray:     """"""Compues the ddimensional mean vectors for the class defined by mask     Args:         X (jnp.ndarray): Feature matrix         mask (jnp.ndarray): Mask defining class membership     Returns:         jnp.ndarray: mean feature vector for class defined by mask     """"""     return jnp.mean(X, where=mask.reshape(1,1), axis=0) vmapMask = jit(vmap(get_mask, in_axes=(None,0))) vmapCMV = jit(vmap(computeMeanVec, in_axes=(None,1))) def computeAllMeanVec(X, y, n):     mask = vmapMask(y.reshape(1,1), n).T     return vmapCMV(X,mask) if __name__ == ""__main__"":     key = jax.random.PRNGKey(42)      offending float is 4.2950333e+09     X = np.full((400,2), 4.2950333e+09).astype(np.float32)     y = np.full((1,400), 2)[0].astype(np.int32)     n = np.unique(y).astype(np.int32)      Reference     mean_org = np_computeMeanVec(X, y, n)     X = jnp.array(X, dtype=jnp.float32)     y = jnp.array(y, dtype=jnp.int32)     n = jnp.array(n, dtype=jnp.int32)      JAX Optimized     mean_vectors = computeAllMeanVec(X, y, n)      err > [[24064. 24064.]]     err = mean_vectors  mean_org         print(err) ``` Thanks, Alex  What jax/jaxlib version are you using? jax v0.4.8  Which accelerator(s) are you using? GPU  Additional system info Python 3.11.3, Ubuntu 22.04  NVIDIA GPU info ``` Mon Apr 17 22:09:24 2023        ++  ++ ```",2023-04-18T02:34:09Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/15646,"Hi  thanks for the question. It's important here to look at not just the difference, but also the relative difference: ```python print(mean_org)   numpy result  [[4.295057e+09 4.295057e+09]] print(mean_vectors)   jax result  [[4.2950333e+09 4.2950333e+09]] err = mean_vectors  mean_org     print(err)   difference  [[23552. 23552.]] print(err / mean_vectors)   relative difference  [[5.483543e06 5.483543e06]] ``` Your numpy and JAX results differ by about 1 part in $10^6$. You're using float32 values, and float32 has a 23bit mantissa, meaning that single operations will normally have errors on order of 1 part in $2^{23}$, which is roughly 1 part in $10^7$. Since your operation involves accumulation, it will consiste of multiple such operations, which makes it look like your output is consistent with the normal roundoff error of floating point operations. What do you think?","That makes sense. I assumed that it was something to do with precision and rounding of the values, given the very suspicious difference of 23552. Just wanted to make sure I wasn't experiencing any unintended behaviour. Thanks!","Thanks  another note: if your computation requires more accuracy, you could enable 64bit precision and then you'll have several orders of magnitude more precision to work with."
7915,"以下是一个github上的jax下的一个issue, 标题是(Include the return path when reporting an overflow on a `jit` function output)， 内容是 (Returning an overflowing integer from a `jit`ed function leads to a very cryptic error: ``` In [1]: import jax In [2]: jax.jit(lambda x: (x, 2**64))(7)  OverflowError                             Traceback (most recent call last) Cell In[2], line 1 > 1 jax.jit(lambda x: (x, 2**64))(7)     [... skipping hidden 17 frame] File ~/p/jax/jax/_src/dtypes.py:183, in _scalar_type_to_dtype(typ, value)     181 if typ is int and value is not None:     182   if value  np.iinfo(dtype).max: > 183     raise OverflowError(f""Python int {value} too large to convert to {dtype}"")     184 return dtype OverflowError: Python int 18446744073709551616 too large to convert to int32 In [4]: jax.config.update('jax_traceback_filtering', 'off') In [5]: jax.jit(lambda x: (x, 2**64))(7)  OverflowError                             Traceback (most recent call last) Cell In[5], line 1 > 1 jax.jit(lambda x: (x, 2**64))(7)     [... skipping hidden 1 frame] File ~/p/jax/jax/_src/pjit.py:208, in _cpp_pjit..cache_miss(*args, **kwargs)     206      207 def cache_miss(*args, **kwargs): > 208   outs, out_flat, out_tree, args_flat = _python_pjit_helper(     209       fun, infer_params_fn, *args, **kwargs)     211   executable = _read_most_recent_pjit_call_executable()     213   use_fastpath = (     214       executable is not None and     215       isinstance(executable, pxla.MeshExecutable) and    (...)     221       all(isinstance(x, xc.ArrayImpl) for x in out_flat)     222   ) File ~/p/jax/jax/_src/pjit.py:150, in _python_pjit_helper(fun, infer_params_fn, *args, **kwargs)     149 def _python_pjit_helper(fun, infer_params_fn, *args, **kwargs): > 150   args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(     151       *args, **kwargs)     152   for arg in args_flat:     153     dispatch.check_arg(arg) File ~/p/jax/jax/_src/api.py:301, in jit..infer_params(*args, **kwargs)     294 def infer_params(*args, **kwargs):     295   pjit_info_args = pjit.PjitInfo(     296       fun=fun, in_shardings=in_shardings,     297       out_shardings=out_shardings, static_argnums=static_argnums,     298       static_argnames=static_argnames, donate_argnums=donate_argnums,     299       device=device, backend=backend, keep_unused=keep_unused,     300       inline=inline, resource_env=None, abstracted_axes=abstracted_axes) > 301   return pjit.common_infer_params(pjit_info_args, *args, **kwargs) File ~/p/jax/jax/_src/pjit.py:469, in common_infer_params(***failed resolving arguments***)     464   in_type = in_avals = tuple(avals)     466 canonicalized_in_shardings_flat = _process_in_axis_resources(     467     hashable_pytree(in_shardings), in_avals, in_tree, resource_env) > 469 jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(     470     flat_fun, hashable_pytree(out_shardings), in_type, dbg,     471     HashableFunction(out_tree, closure=()),     472     HashableFunction(res_paths, closure=()))     474 assert len(explicit_args) == len(canonicalized_in_shardings_flat)     476 if config.jax_dynamic_shapes: File ~/p/jax/jax/_src/pjit.py:927, in _pjit_jaxpr(fun, out_shardings_thunk, in_type, debug_info, out_tree, result_paths)     925 def _pjit_jaxpr(fun, out_shardings_thunk, in_type, debug_info, out_tree,     926                 result_paths): > 927   jaxpr, final_consts, out_type = _create_pjit_jaxpr(     928       fun, in_type, debug_info, result_paths)     929   canonicalized_out_shardings_flat = _check_and_canonicalize_out_shardings(     930       out_shardings_thunk, out_tree, tuple(out_type))     931    lu.cache needs to be able to create weakrefs to outputs, so we can't return a plain tuple File ~/p/jax/jax/_src/linear_util.py:345, in cache..memoized_fun(fun, *args)     343   fun.populate_stores(stores)     344 else: > 345   ans = call(fun, *args)     346   cache[key] = (ans, fun.stores)     348 return ans File ~/p/jax/jax/_src/pjit.py:883, in _create_pjit_jaxpr(fun, in_type, debug_info, out_paths)     880     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic2(     881         lu.annotate(fun, in_type), debug_info=pe_debug)     882   else: > 883     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(     884         fun, in_type, debug_info=pe_debug)     886 if not config.jax_dynamic_shapes:     887   jaxpr = jaxpr_debug_info(jaxpr, debug_info, out_paths()) File ~/p/jax/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File ~/p/jax/jax/_src/interpreters/partial_eval.py:2149, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2147 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2148   main.jaxpr_stack = ()   type: ignore > 2149   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2150     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2151   del main, fun    2152 return jaxpr, out_avals, consts File ~/p/jax/jax/_src/interpreters/partial_eval.py:2172, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2170 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep]    2171 ans = fun.call_wrapped(*in_tracers_) > 2172 out_tracers = map(trace.full_raise, ans)    2173 jaxpr, consts = frame.to_jaxpr(out_tracers)    2174 del fun, main, trace, frame, in_tracers, out_tracers, ans File ~/p/jax/jax/_src/util.py:109, in safe_map(f, *args)     107 for arg in args[1:]:     108   assert len(arg) == n, f'length mismatch: {list(map(len, args))}' > 109 return list(map(f, *args)) File ~/p/jax/jax/_src/core.py:473, in Trace.full_raise(self, val)     471   val = val.dimension_as_value()     472 if not isinstance(val, Tracer): > 473   return self.pure(val)     474 val._assert_live()     475 level = self.level File ~/p/jax/jax/_src/interpreters/partial_eval.py:1798, in DynamicJaxprTrace.new_const(self, c)    1796 tracer = self.frame.constid_to_tracer.get(id(c))    1797 if tracer is None: > 1798   aval = raise_to_shaped(get_aval(c), weak_type=dtypes.is_weakly_typed(c))    1799   aval = self._lift_tracers_in_aval(aval)    1800   tracer = self._new_const(aval, c) File ~/p/jax/jax/_src/core.py:1378, in get_aval(x)    1376   return x.aval    1377 else: > 1378   return concrete_aval(x) File ~/p/jax/jax/_src/core.py:1367, in concrete_aval(x)    1365 for typ in type(x).__mro__:    1366   handler = pytype_aval_mappings.get(typ) > 1367   if handler: return handler(x)    1368 if hasattr(x, '__jax_array__'):    1369   return concrete_aval(x.__jax_array__()) File ~/p/jax/jax/_src/abstract_arrays.py:75, in _make_concrete_python_scalar(t, x)      74 def _make_concrete_python_scalar(t, x): > 75   dtype = dtypes._scalar_type_to_dtype(t, x)      76   weak_type = dtypes.is_weakly_typed(x)      77   return canonical_concrete_aval(np.array(x, dtype=dtype), weak_type=weak_type) File ~/p/jax/jax/_src/dtypes.py:183, in _scalar_type_to_dtype(typ, value)     181 if typ is int and value is not None:     182   if value  np.iinfo(dtype).max: > 183     raise OverflowError(f""Python int {value} too large to convert to {dtype}"")     184 return dtype OverflowError: Python int 18446744073709551616 too large to convert to int32 ``` It would be much better if there were a way we could report the return value path. There is of course one minor difficulty: we don't know the return value path until after tracing completes! Any ideas? The best idea I have would be to return some kind of error tracer, which might allow us to delay reporting the error until after the output pytree is known.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Include the return path when reporting an overflow on a `jit` function output,"Returning an overflowing integer from a `jit`ed function leads to a very cryptic error: ``` In [1]: import jax In [2]: jax.jit(lambda x: (x, 2**64))(7)  OverflowError                             Traceback (most recent call last) Cell In[2], line 1 > 1 jax.jit(lambda x: (x, 2**64))(7)     [... skipping hidden 17 frame] File ~/p/jax/jax/_src/dtypes.py:183, in _scalar_type_to_dtype(typ, value)     181 if typ is int and value is not None:     182   if value  np.iinfo(dtype).max: > 183     raise OverflowError(f""Python int {value} too large to convert to {dtype}"")     184 return dtype OverflowError: Python int 18446744073709551616 too large to convert to int32 In [4]: jax.config.update('jax_traceback_filtering', 'off') In [5]: jax.jit(lambda x: (x, 2**64))(7)  OverflowError                             Traceback (most recent call last) Cell In[5], line 1 > 1 jax.jit(lambda x: (x, 2**64))(7)     [... skipping hidden 1 frame] File ~/p/jax/jax/_src/pjit.py:208, in _cpp_pjit..cache_miss(*args, **kwargs)     206      207 def cache_miss(*args, **kwargs): > 208   outs, out_flat, out_tree, args_flat = _python_pjit_helper(     209       fun, infer_params_fn, *args, **kwargs)     211   executable = _read_most_recent_pjit_call_executable()     213   use_fastpath = (     214       executable is not None and     215       isinstance(executable, pxla.MeshExecutable) and    (...)     221       all(isinstance(x, xc.ArrayImpl) for x in out_flat)     222   ) File ~/p/jax/jax/_src/pjit.py:150, in _python_pjit_helper(fun, infer_params_fn, *args, **kwargs)     149 def _python_pjit_helper(fun, infer_params_fn, *args, **kwargs): > 150   args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(     151       *args, **kwargs)     152   for arg in args_flat:     153     dispatch.check_arg(arg) File ~/p/jax/jax/_src/api.py:301, in jit..infer_params(*args, **kwargs)     294 def infer_params(*args, **kwargs):     295   pjit_info_args = pjit.PjitInfo(     296       fun=fun, in_shardings=in_shardings,     297       out_shardings=out_shardings, static_argnums=static_argnums,     298       static_argnames=static_argnames, donate_argnums=donate_argnums,     299       device=device, backend=backend, keep_unused=keep_unused,     300       inline=inline, resource_env=None, abstracted_axes=abstracted_axes) > 301   return pjit.common_infer_params(pjit_info_args, *args, **kwargs) File ~/p/jax/jax/_src/pjit.py:469, in common_infer_params(***failed resolving arguments***)     464   in_type = in_avals = tuple(avals)     466 canonicalized_in_shardings_flat = _process_in_axis_resources(     467     hashable_pytree(in_shardings), in_avals, in_tree, resource_env) > 469 jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(     470     flat_fun, hashable_pytree(out_shardings), in_type, dbg,     471     HashableFunction(out_tree, closure=()),     472     HashableFunction(res_paths, closure=()))     474 assert len(explicit_args) == len(canonicalized_in_shardings_flat)     476 if config.jax_dynamic_shapes: File ~/p/jax/jax/_src/pjit.py:927, in _pjit_jaxpr(fun, out_shardings_thunk, in_type, debug_info, out_tree, result_paths)     925 def _pjit_jaxpr(fun, out_shardings_thunk, in_type, debug_info, out_tree,     926                 result_paths): > 927   jaxpr, final_consts, out_type = _create_pjit_jaxpr(     928       fun, in_type, debug_info, result_paths)     929   canonicalized_out_shardings_flat = _check_and_canonicalize_out_shardings(     930       out_shardings_thunk, out_tree, tuple(out_type))     931    lu.cache needs to be able to create weakrefs to outputs, so we can't return a plain tuple File ~/p/jax/jax/_src/linear_util.py:345, in cache..memoized_fun(fun, *args)     343   fun.populate_stores(stores)     344 else: > 345   ans = call(fun, *args)     346   cache[key] = (ans, fun.stores)     348 return ans File ~/p/jax/jax/_src/pjit.py:883, in _create_pjit_jaxpr(fun, in_type, debug_info, out_paths)     880     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic2(     881         lu.annotate(fun, in_type), debug_info=pe_debug)     882   else: > 883     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(     884         fun, in_type, debug_info=pe_debug)     886 if not config.jax_dynamic_shapes:     887   jaxpr = jaxpr_debug_info(jaxpr, debug_info, out_paths()) File ~/p/jax/jax/_src/profiler.py:314, in annotate_function..wrapper(*args, **kwargs)     311 (func)     312 def wrapper(*args, **kwargs):     313   with TraceAnnotation(name, **decorator_kwargs): > 314     return func(*args, **kwargs)     315   return wrapper File ~/p/jax/jax/_src/interpreters/partial_eval.py:2149, in trace_to_jaxpr_dynamic(fun, in_avals, debug_info, keep_inputs)    2147 with core.new_main(DynamicJaxprTrace, dynamic=True) as main:   type: ignore    2148   main.jaxpr_stack = ()   type: ignore > 2149   jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(    2150     fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)    2151   del main, fun    2152 return jaxpr, out_avals, consts File ~/p/jax/jax/_src/interpreters/partial_eval.py:2172, in trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs, debug_info)    2170 in_tracers_ = [t for t, keep in zip(in_tracers, keep_inputs) if keep]    2171 ans = fun.call_wrapped(*in_tracers_) > 2172 out_tracers = map(trace.full_raise, ans)    2173 jaxpr, consts = frame.to_jaxpr(out_tracers)    2174 del fun, main, trace, frame, in_tracers, out_tracers, ans File ~/p/jax/jax/_src/util.py:109, in safe_map(f, *args)     107 for arg in args[1:]:     108   assert len(arg) == n, f'length mismatch: {list(map(len, args))}' > 109 return list(map(f, *args)) File ~/p/jax/jax/_src/core.py:473, in Trace.full_raise(self, val)     471   val = val.dimension_as_value()     472 if not isinstance(val, Tracer): > 473   return self.pure(val)     474 val._assert_live()     475 level = self.level File ~/p/jax/jax/_src/interpreters/partial_eval.py:1798, in DynamicJaxprTrace.new_const(self, c)    1796 tracer = self.frame.constid_to_tracer.get(id(c))    1797 if tracer is None: > 1798   aval = raise_to_shaped(get_aval(c), weak_type=dtypes.is_weakly_typed(c))    1799   aval = self._lift_tracers_in_aval(aval)    1800   tracer = self._new_const(aval, c) File ~/p/jax/jax/_src/core.py:1378, in get_aval(x)    1376   return x.aval    1377 else: > 1378   return concrete_aval(x) File ~/p/jax/jax/_src/core.py:1367, in concrete_aval(x)    1365 for typ in type(x).__mro__:    1366   handler = pytype_aval_mappings.get(typ) > 1367   if handler: return handler(x)    1368 if hasattr(x, '__jax_array__'):    1369   return concrete_aval(x.__jax_array__()) File ~/p/jax/jax/_src/abstract_arrays.py:75, in _make_concrete_python_scalar(t, x)      74 def _make_concrete_python_scalar(t, x): > 75   dtype = dtypes._scalar_type_to_dtype(t, x)      76   weak_type = dtypes.is_weakly_typed(x)      77   return canonical_concrete_aval(np.array(x, dtype=dtype), weak_type=weak_type) File ~/p/jax/jax/_src/dtypes.py:183, in _scalar_type_to_dtype(typ, value)     181 if typ is int and value is not None:     182   if value  np.iinfo(dtype).max: > 183     raise OverflowError(f""Python int {value} too large to convert to {dtype}"")     184 return dtype OverflowError: Python int 18446744073709551616 too large to convert to int32 ``` It would be much better if there were a way we could report the return value path. There is of course one minor difficulty: we don't know the return value path until after tracing completes! Any ideas? The best idea I have would be to return some kind of error tracer, which might allow us to delay reporting the error until after the output pytree is known.",2023-04-17T17:29:24Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/15631
4984,"以下是一个github上的jax下的一个issue, 标题是(NotImplementedError: Call to scatter_(update/add/multiply/min/max) cannot be converted with enable_xla=False)， 内容是 ( Description I've got a JAX function that I'm trying to fuse with a Keras model and convert everything to TFLite. To my understanding the best way to do that is to convert my function to a TensorFlow Concrete Function, merge it with my Keras model to another Concrete Function, and eventually convert it to TFlite using `TFLiteConverter.from_concrete_functions`. Problem is, getting a Concrete Function from my JAX function fails with the errors below. The weird thing is that converting this function to TFLite directly with `TFLiteConverter.experimental_from_jax` works! I assume it has something to do with conversion directly to HLO (reference), but still, is it possible to get the same behavior for `from_concrete_functions`? ```python import numpy as np import jax.numpy as jnp from jax.experimental import jax2tf import numpy.typing as npt import tensorflow as tf grid_min = np.array([1.5, 0, 0.2], dtype=np.float32) grid_max = np.array([1.5, 3, 2], dtype=np.float32) grid_res = np.array([0.15, 0.15, 0.1], dtype=np.float32) image_shape = ((grid_max + grid_res  grid_min) / grid_res).astype(np.int32)  start, end, step x_range = np.array([0.995, 0.995, 0.058500], dtype=np.float32) y_range = np.array([0.995, 0.995, 0.058500], dtype=np.float32) z_range = np.array([0, 6.5169, 0.2327], dtype=np.float32) def preprocess(     dx_idx,     dy_idx,     r_idx,     snr,     sensor_loc,     rotation_mat,     arena_min,     arena_max, ) > npt.NDArray:      convert to spherical coordinates     dx = x_range[0] + (dx_idx  1) * x_range[2]     dy = y_range[0] + (dy_idx  1) * y_range[2]     r = z_range[0] + (r_idx  1) * z_range[2]      convert to cartesian coordinates     peaks = jnp.stack([dx, dy, jnp.sqrt(1  dx * dx  dy * dy)]) * r     peaks = peaks.T      rotate and translate the peaks     peaks = peaks @ rotation_mat + sensor_loc      mask out peaks that are outside of the arena     arena_mask = ((arena_min <= peaks) & (peaks <= arena_max)).all(axis=1)      convert to image coordinates     peaks = jnp.floor((peaks  grid_min) / grid_res).astype(np.int32)      mask out peaks that are outside of the arena     image_mask = ((0 <= peaks) & (peaks < image_shape)).all(axis=1)      clip to zero peaks that are outside of the arena and image     mask = arena_mask & image_mask     x = jnp.where(mask, peaks[:, 0], 0)     y = jnp.where(mask, peaks[:, 1], 0)     z = jnp.where(mask, peaks[:, 2], 0)     snr = jnp.where(mask, snr, 0)      create the image     image = jnp.zeros(image_shape, dtype=np.float32)     image.at[x, y, z].set(snr)     return image dx_idx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32) dy_idx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32) r_idx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32) snr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32) sensor_loc = np.array([0, 0, 1.5], dtype=np.float32) rotation_mat = np.array([[0.7071, 0, 0.7071], [0.7071, 0, 0.7071], [0, 1, 0]], dtype=np.float32) arena_min = np.array([1, 0.3, 0], dtype=np.float32) arena_max = np.array([1, 2, 1.8], dtype=np.float32) converter = tf.lite.TFLiteConverter.experimental_from_jax(     [preprocess],     [         [             (""dx_idx"", dx_idx),             (""dy_idx"", dy_idx),             (""r_idx"", r_idx),             (""snr"", snr),             (""sensor_loc"", sensor_loc),             (""rotation_mat"", rotation_mat),             (""arena_min"", arena_min),             (""arena_max"", arena_max),         ]     ], ) process_tflite = converter.convert()   works! preprocess_converted = jax2tf.convert(preprocess, enable_xla=False) preprocess_tf = tf.function(preprocess_converted, autograph=False, jit_compile=True) preprocess_tf_concrete = preprocess_tf.get_concrete_function(   fails!     tf.TensorSpec(shape=dx_idx.shape, name=""dx_idx""),     tf.TensorSpec(shape=dy_idx.shape, name=""dy_idx""),     tf.TensorSpec(shape=r_idx.shape, name=""r_idx""),     tf.TensorSpec(shape=snr.shape, name=""snr""),     tf.TensorSpec(shape=sensor_loc.shape, name=""sensor_loc""),     tf.TensorSpec(shape=rotation_mat.shape, name=""rotation_mat""),     tf.TensorSpec(shape=arena_min.shape, name=""arena_min""),     tf.TensorSpec(shape=arena_max.shape, name=""arena_max""), ) converter = tf.lite.TFLiteConverter.from_concrete_functions([preprocess_tf_concrete], preprocess_tf) process_tflite = converter.convert() ``` Note: if it disable XLA (`enable_xla=True`) the conversion seemingly works but the final model I'm getting from `from_concrete_functions` has an empty graph. Thank you.  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7  Which accelerator(s) are you using? CPU  Additional system info WSL2 over Windows 11  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,NotImplementedError: Call to scatter_(update/add/multiply/min/max) cannot be converted with enable_xla=False," Description I've got a JAX function that I'm trying to fuse with a Keras model and convert everything to TFLite. To my understanding the best way to do that is to convert my function to a TensorFlow Concrete Function, merge it with my Keras model to another Concrete Function, and eventually convert it to TFlite using `TFLiteConverter.from_concrete_functions`. Problem is, getting a Concrete Function from my JAX function fails with the errors below. The weird thing is that converting this function to TFLite directly with `TFLiteConverter.experimental_from_jax` works! I assume it has something to do with conversion directly to HLO (reference), but still, is it possible to get the same behavior for `from_concrete_functions`? ```python import numpy as np import jax.numpy as jnp from jax.experimental import jax2tf import numpy.typing as npt import tensorflow as tf grid_min = np.array([1.5, 0, 0.2], dtype=np.float32) grid_max = np.array([1.5, 3, 2], dtype=np.float32) grid_res = np.array([0.15, 0.15, 0.1], dtype=np.float32) image_shape = ((grid_max + grid_res  grid_min) / grid_res).astype(np.int32)  start, end, step x_range = np.array([0.995, 0.995, 0.058500], dtype=np.float32) y_range = np.array([0.995, 0.995, 0.058500], dtype=np.float32) z_range = np.array([0, 6.5169, 0.2327], dtype=np.float32) def preprocess(     dx_idx,     dy_idx,     r_idx,     snr,     sensor_loc,     rotation_mat,     arena_min,     arena_max, ) > npt.NDArray:      convert to spherical coordinates     dx = x_range[0] + (dx_idx  1) * x_range[2]     dy = y_range[0] + (dy_idx  1) * y_range[2]     r = z_range[0] + (r_idx  1) * z_range[2]      convert to cartesian coordinates     peaks = jnp.stack([dx, dy, jnp.sqrt(1  dx * dx  dy * dy)]) * r     peaks = peaks.T      rotate and translate the peaks     peaks = peaks @ rotation_mat + sensor_loc      mask out peaks that are outside of the arena     arena_mask = ((arena_min <= peaks) & (peaks <= arena_max)).all(axis=1)      convert to image coordinates     peaks = jnp.floor((peaks  grid_min) / grid_res).astype(np.int32)      mask out peaks that are outside of the arena     image_mask = ((0 <= peaks) & (peaks < image_shape)).all(axis=1)      clip to zero peaks that are outside of the arena and image     mask = arena_mask & image_mask     x = jnp.where(mask, peaks[:, 0], 0)     y = jnp.where(mask, peaks[:, 1], 0)     z = jnp.where(mask, peaks[:, 2], 0)     snr = jnp.where(mask, snr, 0)      create the image     image = jnp.zeros(image_shape, dtype=np.float32)     image.at[x, y, z].set(snr)     return image dx_idx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32) dy_idx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32) r_idx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32) snr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32) sensor_loc = np.array([0, 0, 1.5], dtype=np.float32) rotation_mat = np.array([[0.7071, 0, 0.7071], [0.7071, 0, 0.7071], [0, 1, 0]], dtype=np.float32) arena_min = np.array([1, 0.3, 0], dtype=np.float32) arena_max = np.array([1, 2, 1.8], dtype=np.float32) converter = tf.lite.TFLiteConverter.experimental_from_jax(     [preprocess],     [         [             (""dx_idx"", dx_idx),             (""dy_idx"", dy_idx),             (""r_idx"", r_idx),             (""snr"", snr),             (""sensor_loc"", sensor_loc),             (""rotation_mat"", rotation_mat),             (""arena_min"", arena_min),             (""arena_max"", arena_max),         ]     ], ) process_tflite = converter.convert()   works! preprocess_converted = jax2tf.convert(preprocess, enable_xla=False) preprocess_tf = tf.function(preprocess_converted, autograph=False, jit_compile=True) preprocess_tf_concrete = preprocess_tf.get_concrete_function(   fails!     tf.TensorSpec(shape=dx_idx.shape, name=""dx_idx""),     tf.TensorSpec(shape=dy_idx.shape, name=""dy_idx""),     tf.TensorSpec(shape=r_idx.shape, name=""r_idx""),     tf.TensorSpec(shape=snr.shape, name=""snr""),     tf.TensorSpec(shape=sensor_loc.shape, name=""sensor_loc""),     tf.TensorSpec(shape=rotation_mat.shape, name=""rotation_mat""),     tf.TensorSpec(shape=arena_min.shape, name=""arena_min""),     tf.TensorSpec(shape=arena_max.shape, name=""arena_max""), ) converter = tf.lite.TFLiteConverter.from_concrete_functions([preprocess_tf_concrete], preprocess_tf) process_tflite = converter.convert() ``` Note: if it disable XLA (`enable_xla=True`) the conversion seemingly works but the final model I'm getting from `from_concrete_functions` has an empty graph. Thank you.  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7  Which accelerator(s) are you using? CPU  Additional system info WSL2 over Windows 11  NVIDIA GPU info _No response_",2023-04-16T22:16:50Z,enhancement,open,0,6,https://github.com/jax-ml/jax/issues/15627, Is this bug still current?,"Yes, still current with no apparent way overcoming it. I'll be happy to contribute a fix if you'll be able to guide me through.", to see if these is a fix.,"  Hi, I got back to this today and found out that `experimental_from_jax` is being deprecated and `jax2tf.convert` is the way to go now, which means I don't have a lot of time until I'll have to migrate. Can I help in some way with this issue?", What is the status of TFLite support for StableHLO? I would like to deprecate the `enable_xla=False` path altogether.,"Hi  ,  TFLite has migrated to use native serialization as the default.  We can simply save as as TFSavedModel via `tf.saved_model.save`  containing StableHLO ops as the intermediate format between JAX and TFLite. Then TFLite converter API will pick that up  `converter = tf.lite.TFLiteConverter.from_saved_model('/some/directory') tflite_model = converter.convert()`. We will update our public documentation pretty soon."
1859,"以下是一个github上的jax下的一个issue, 标题是(``JAX`` conflicts with ``PyTorch`` at import time)， 内容是 ( Description When importing ``jax`` and ``pytorch``, namely ```python import torch import os os.environ['XLA_FLAGS'] = 'xla_gpu_force_compilation_parallelism=1' import jax ``` I get the following error ```bash Traceback (most recent call last):   File ""debug_script.py"", line 6, in      import jax   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jax/__init__.py"", line 35, in      from jax import config as _config_module   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jax/config.py"", line 17, in      from jax._src.config import config   noqa: F401   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jax/_src/config.py"", line 27, in      from jax._src import lib   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jax/_src/lib/__init__.py"", line 87, in      import jaxlib.xla_client as xla_client   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jaxlib/xla_client.py"", line 25, in      from . import xla_extension as _xla ImportError: /lib/x86_64linuxgnu/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jaxlib/xla_extension.so) ``` I thought it might be an error linked to the version of ``libstdc++``, but it appears to be not the case. Indeed, the problem disappears when swapping the import order of them.  What jax/jaxlib version are you using? jax v`0.3.25`, jaxlib v`0.3.22`  Which accelerator(s) are you using? GPU  Additional system info Python 3.7, OS Linux Ubuntu 64bit  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",glm,``JAX`` conflicts with ``PyTorch`` at import time," Description When importing ``jax`` and ``pytorch``, namely ```python import torch import os os.environ['XLA_FLAGS'] = 'xla_gpu_force_compilation_parallelism=1' import jax ``` I get the following error ```bash Traceback (most recent call last):   File ""debug_script.py"", line 6, in      import jax   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jax/__init__.py"", line 35, in      from jax import config as _config_module   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jax/config.py"", line 17, in      from jax._src.config import config   noqa: F401   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jax/_src/config.py"", line 27, in      from jax._src import lib   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jax/_src/lib/__init__.py"", line 87, in      import jaxlib.xla_client as xla_client   File ""/home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jaxlib/xla_client.py"", line 25, in      from . import xla_extension as _xla ImportError: /lib/x86_64linuxgnu/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/badrmoufad/anaconda3/envs/skglmgpu/lib/python3.7/sitepackages/jaxlib/xla_extension.so) ``` I thought it might be an error linked to the version of ``libstdc++``, but it appears to be not the case. Indeed, the problem disappears when swapping the import order of them.  What jax/jaxlib version are you using? jax v`0.3.25`, jaxlib v`0.3.22`  Which accelerator(s) are you using? GPU  Additional system info Python 3.7, OS Linux Ubuntu 64bit  NVIDIA GPU info ``` ++  ++ ```",2023-04-16T10:54:21Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15624,"This sort of thing may happen if you mix packages installed by anaconda with packages installed via `pip`. One possible workaround: it might work to import JAX first, before `torch`. The other suggestion I have is: install both JAX and Torch from anaconda (condaforge for JAX, see the README), or install both from `pip`.",I would stick with inversing the order of imports as the conflict between both packages persists despite installing them from `conda`. Thanks for your response!
954,"以下是一个github上的jax下的一个issue, 标题是(Provide access to keepalives when calling jax.jit)， 内容是 (Custom Calls return a pointer of the serialized descriptor to jax.jit, and the jax.jitted function saves this pointer as an internal state, giving this pointer the same lifetime as the function. When calling jax.jit to get the lowered IR, we lose the keepalives list of pointers. Unless we store the entire jax.jitted function, the custom call pointers get released, giving a segfault when trying to access the descriptor through the (now released) pointer. Storing the entire jax.jitted function becomes a problem when the model is big, as we have to store this memory when all we really need is the keepalives list. A good solution would be to store the keepalives in the jax.stages.Lowered object, to which we have access.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Provide access to keepalives when calling jax.jit,"Custom Calls return a pointer of the serialized descriptor to jax.jit, and the jax.jitted function saves this pointer as an internal state, giving this pointer the same lifetime as the function. When calling jax.jit to get the lowered IR, we lose the keepalives list of pointers. Unless we store the entire jax.jitted function, the custom call pointers get released, giving a segfault when trying to access the descriptor through the (now released) pointer. Storing the entire jax.jitted function becomes a problem when the model is big, as we have to store this memory when all we really need is the keepalives list. A good solution would be to store the keepalives in the jax.stages.Lowered object, to which we have access.",2023-04-11T09:20:26Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/15536
222,"以下是一个github上的jax下的一个issue, 标题是(fix typo in functions.py)， 内容是 (Indicies > Indices)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fix typo in functions.py,Indicies > Indices,2023-04-11T02:34:08Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/15531,Thank you!
7298,"以下是一个github上的jax下的一个issue, 标题是(pip installation: GPU (CUDA, installed via pip) not working for me with Brax)， 内容是 ( Description pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html jax installation using the command above is not working with the latest Brax: https://github.com/google/brax For some reason it complains on runtime CuDNN version: ``` 20230409 22:29:02.848922: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.3.2 but source was compiled with: 8.8.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. Traceback (most recent call last):   File ""/home/viktorm/Projects/rl_games/runner.py"", line 68, in      runner.run(args)   File ""/home/viktorm/Projects/rl_games/rl_games/torch_runner.py"", line 119, in run     self.run_train(args)   File ""/home/viktorm/Projects/rl_games/rl_games/torch_runner.py"", line 97, in run_train     agent = self.algo_factory.create(self.algo_name, base_name='run', params=self.params)   File ""/home/viktorm/Projects/rl_games/rl_games/common/object_factory.py"", line 15, in create     return builder(**kwargs)   File ""/home/viktorm/Projects/rl_games/rl_games/torch_runner.py"", line 36, in      self.algo_factory.register_builder('a2c_continuous', lambda **kwargs : a2c_continuous.A2CAgent(**kwargs))   File ""/home/viktorm/Projects/rl_games/rl_games/algos_torch/a2c_continuous.py"", line 16, in __init__     a2c_common.ContinuousA2CBase.__init__(self, base_name, params)   File ""/home/viktorm/Projects/rl_games/rl_games/common/a2c_common.py"", line 1056, in __init__     A2CBase.__init__(self, base_name, params)   File ""/home/viktorm/Projects/rl_games/rl_games/common/a2c_common.py"", line 119, in __init__     self.vec_env = vecenv.create_vec_env(self.env_name, self.num_actors, **self.env_config)   File ""/home/viktorm/Projects/rl_games/rl_games/common/vecenv.py"", line 222, in create_vec_env     return vecenv_configvec_env_name   File ""/home/viktorm/Projects/rl_games/rl_games/common/vecenv.py"", line 227, in      register('BRAX', lambda config_name, num_actors, **kwargs: BraxEnv(config_name, num_actors, **kwargs))   File ""/home/viktorm/Projects/rl_games/rl_games/envs/brax.py"", line 43, in __init__     self.env = envs.create(env_name=self.env_name, batch_size=self.num_envs, backend=self.sim_backend)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/envs/__init__.py"", line 95, in create     env = _envsenv_name   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/envs/ant.py"", line 190, in __init__     sys = mjcf.load(path)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/io/mjcf.py"", line 512, in load     return load_model(mj)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/io/mjcf.py"", line 384, in load_model     geoms = [   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/io/mjcf.py"", line 385, in      jax.tree_map(lambda *x: jp.stack(x), *g) for g in geom_groups.values()   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 210, in tree_map     return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 210, in      return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/io/mjcf.py"", line 385, in      jax.tree_map(lambda *x: jp.stack(x), *g) for g in geom_groups.values()   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1715, in stack     new_arrays.append(expand_dims(a, axis))   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 872, in expand_dims     return lax.expand_dims(a, axis)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 1313, in expand_dims     return broadcast_in_dim(array, result_shape, broadcast_dims)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 784, in broadcast_in_dim     return broadcast_in_dim_p.bind(   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 117, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 208, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 254, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2816, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 3028, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ```  What jax/jaxlib version are you using? jax 0.4.8  Which accelerator(s) are you using? RTX 4090  Additional system info Python 3.9 Ubuntu 22.04  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,"pip installation: GPU (CUDA, installed via pip) not working for me with Brax"," Description pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html jax installation using the command above is not working with the latest Brax: https://github.com/google/brax For some reason it complains on runtime CuDNN version: ``` 20230409 22:29:02.848922: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.3.2 but source was compiled with: 8.8.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. Traceback (most recent call last):   File ""/home/viktorm/Projects/rl_games/runner.py"", line 68, in      runner.run(args)   File ""/home/viktorm/Projects/rl_games/rl_games/torch_runner.py"", line 119, in run     self.run_train(args)   File ""/home/viktorm/Projects/rl_games/rl_games/torch_runner.py"", line 97, in run_train     agent = self.algo_factory.create(self.algo_name, base_name='run', params=self.params)   File ""/home/viktorm/Projects/rl_games/rl_games/common/object_factory.py"", line 15, in create     return builder(**kwargs)   File ""/home/viktorm/Projects/rl_games/rl_games/torch_runner.py"", line 36, in      self.algo_factory.register_builder('a2c_continuous', lambda **kwargs : a2c_continuous.A2CAgent(**kwargs))   File ""/home/viktorm/Projects/rl_games/rl_games/algos_torch/a2c_continuous.py"", line 16, in __init__     a2c_common.ContinuousA2CBase.__init__(self, base_name, params)   File ""/home/viktorm/Projects/rl_games/rl_games/common/a2c_common.py"", line 1056, in __init__     A2CBase.__init__(self, base_name, params)   File ""/home/viktorm/Projects/rl_games/rl_games/common/a2c_common.py"", line 119, in __init__     self.vec_env = vecenv.create_vec_env(self.env_name, self.num_actors, **self.env_config)   File ""/home/viktorm/Projects/rl_games/rl_games/common/vecenv.py"", line 222, in create_vec_env     return vecenv_configvec_env_name   File ""/home/viktorm/Projects/rl_games/rl_games/common/vecenv.py"", line 227, in      register('BRAX', lambda config_name, num_actors, **kwargs: BraxEnv(config_name, num_actors, **kwargs))   File ""/home/viktorm/Projects/rl_games/rl_games/envs/brax.py"", line 43, in __init__     self.env = envs.create(env_name=self.env_name, batch_size=self.num_envs, backend=self.sim_backend)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/envs/__init__.py"", line 95, in create     env = _envsenv_name   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/envs/ant.py"", line 190, in __init__     sys = mjcf.load(path)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/io/mjcf.py"", line 512, in load     return load_model(mj)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/io/mjcf.py"", line 384, in load_model     geoms = [   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/io/mjcf.py"", line 385, in      jax.tree_map(lambda *x: jp.stack(x), *g) for g in geom_groups.values()   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 210, in tree_map     return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/tree_util.py"", line 210, in      return treedef.unflatten(f(*xs) for xs in zip(*all_leaves))   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/brax/io/mjcf.py"", line 385, in      jax.tree_map(lambda *x: jp.stack(x), *g) for g in geom_groups.values()   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 1715, in stack     new_arrays.append(expand_dims(a, axis))   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 872, in expand_dims     return lax.expand_dims(a, axis)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 1313, in expand_dims     return broadcast_in_dim(array, result_shape, broadcast_dims)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 784, in broadcast_in_dim     return broadcast_in_dim_p.bind(   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 117, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 208, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 254, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2816, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 3028, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/viktorm/anaconda3/envs/warp39/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ```  What jax/jaxlib version are you using? jax 0.4.8  Which accelerator(s) are you using? RTX 4090  Additional system info Python 3.9 Ubuntu 22.04  NVIDIA GPU info ++  ++",2023-04-10T05:35:51Z,bug,closed,2,3,https://github.com/jax-ml/jax/issues/15508,"Well, the message `Loaded runtime CuDNN library: 8.3.2` tells you the problem: JAX was trying to load CuDNN, but found a really old version (8.3) when it tried. Usually this means that you have already loaded an older CuDNN into your process (e.g., importing PyTorch before importing JAX is one common way this can happen, since PyTorch usually bundles an older CuDNN). The other way it can happen is that an older CuDNN is first in your `LD_LIBRARY_PATH`, but I think that's unlikely to be the case here since you used the `pip` installation of JAX. Try searching for other CuDNN installations (files name `*cudnn*`) on your system? This is probably not something we can fix from the JAX end.","same issue, it seems because the current version of pytorch does not support cudnn8.8 or higher version"," You might do well to install a CPUonly version of PyTorch, if the goal is to use that in the context of a GPUusing JAX program."
533,"以下是一个github上的jax下的一个issue, 标题是(Support static argnames/argnums to checkify)， 内容是 (I tried using checkify today but it's not compatible with my code base because I'm relying on static argnames/argnums for some of the jitted functions. After wrapping the jitted functions with checkify, those inputs are misinterpreted as dynamic arguments and JAX raises an error because they aren't JAX types.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Support static argnames/argnums to checkify,"I tried using checkify today but it's not compatible with my code base because I'm relying on static argnames/argnums for some of the jitted functions. After wrapping the jitted functions with checkify, those inputs are misinterpreted as dynamic arguments and JAX raises an error because they aren't JAX types.",2023-04-10T01:08:16Z,enhancement,closed,0,7,https://github.com/jax-ml/jax/issues/15504,"Thanks for raising this! > those inputs are misinterpreted as dynamic arguments Well, `static_argnums`/`static_argnames` only affects the particular `jit` call to which those arguments are passed. In particular, they don't change the meaning of the body. So if I understand correctly, this issue isn't checkifyspecific: for any `f` where `jit(checkify(f), static_argnums=(0,))` would fail with the error you describe, I believe `jit(jit(f), static_argnums=(0,))` would fail in the same way. In both cases, the function `g` to which the `jit(g, static_argnums=(0,))` is applied only accepts pytrees of jax types (ie arrays) as arguments. So, there are (at least) two possible problem statements here, and I just want to figure out which one you have in mind: 1. make both `jit(jit(f), static_argnums=(0,))` and `jit(checkify(f), static_argnums=(0,))` (and `scan` etc) Just Work, by changing the meaning of the outer `jit`'s `static_argnums` to affect the body as well; or 2. change `checkify` only so that this issue doesn't arise (e.g. maybe `checkify` should try to avoid assuming  _any_ inputs are jaxtypes, independent of any caller's `static_argnums`). Did you have one of these in mind?", ,"I was thinking of your option 2, changing `checkify` to avoid assuming that all its inputs must be jaxtypes. More generally, there could be a solution along the lines of this? ```python3 def with_static(transform, argname):   specialized = {}   def wrapped(fn, *args, **kwargs):     value = kwargs.pop(argname, '_default')     if value not in specialized:       specialized[value] = transform(functools.partial(fn, **{argname: value}))     return specializedvalue   return wrapped fn = with_static(jit, 'string_arg')(fn)   Same as jit(fn, static_argnames='string_arg') fn = with_static(checkify, 'string_arg')(fn) fn(array_arg=jnp.zeros(), string_arg='bar') ```","Actually, this line isnt quite right: ```python fn = with_static(jit, 'string_arg')(fn)   Same as jit(fn, static_argnames='string_arg') ``` `jit`'s `static_argnums`/`static_argnames` can't be implemented in user code (i.e. on top of a `jax.jit` API which doesn't itself have those options). If you try using the solution quoted here, you'll always get retraces/recompiles between two applications of `fn` even when passing the same value for `string_arg`, because you're creating a fresh callable object (the `partial` instance) every time. That's why `jit` has `static_argnums`/`static_argnames` built in: not as a convenience but because it's the only way to get the caching behavior we want. This kind of caching approach can work better with `checkify`, though I'm a bit certain because it now operates differently from e.g. `jax.grad` (basically it has some internal caching which makes it a bit different). I'd have to think about it. I guess a third option is: 3. add `static_argnums` / `static_argnames` to `checkify`, perhaps just as a convenience (though it's not as convenient as it Just Working). I'd rather make it Just Work if we can!",I don't quite follow. The `functools.partial()` in my call only happens when the specialization value isn't already in the cache dictionary. So it would only retrace once per unique value for `string_arg`.,"Sorry, I misread your code. (Actually I think it's a bit buggy in that you want to curry the `wrapped` fun on extra level and not get both `fn` and `args`/`kwargs` at the same time.) You're right that you can get cache hits for equal values with this approach, so that part of my comment was mistaken. But now you have separate jit caches (so e.g. we can't get the same eviction logic we would get from having a single cache). In general my only point was just that `with_static(jit, 'string_arg')(fn)` isn't _exactly_ the same as `jit(fn, static_argnames='string_arg')`. Luckily, CC(Checkify: close over all arguments.) will solve the issue by making things Just Work! With that `checkify` will no longer place any constraints on the arguments passed to the `checkify`decorated function. (Internally all the arguments are just closed over; we checked that the caching issues I was concerned about, basically the things CC(checkify: cache jaxpr formation so we don't always retrace) and CC(Fix checkify caching with nested call primitives) were fixing, no longer apply thanks to 6ec9082.)","Awesome! Thanks also for the explanation, that makes sense."
1484,"以下是一个github上的jax下的一个issue, 标题是(Bias initialization based on the input dimension of the layer)， 内容是 ( Description Hey,  I was trying to initialize the biases of my layers according the way it's done in PyTorch's default way of doing so, for linear layers: ```python if self.bias is not None:     fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)     bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0     init.uniform_(self.bias, bound, bound) ``` but I realized it's almost impossible to do so in a clean way using jax. As in an initializer, I only have access to the shape of the parameter being initialized, which for bias translates to the output dimension of a layer. I imagine this was intended (https://github.com/google/jax/issues/2075issuecomment578465814, https://github.com/google/flax/issues/2749) but I also imagine this could be annoying for a lot of people like me, who would like to initialize their biases based on the input dimension in some way (let's say to numerically test some theoretical analysis). Is there any recommended way of initializing the biases like this? If not, is this supposed to be supported in the future at all? Thanks!  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bias initialization based on the input dimension of the layer," Description Hey,  I was trying to initialize the biases of my layers according the way it's done in PyTorch's default way of doing so, for linear layers: ```python if self.bias is not None:     fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)     bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0     init.uniform_(self.bias, bound, bound) ``` but I realized it's almost impossible to do so in a clean way using jax. As in an initializer, I only have access to the shape of the parameter being initialized, which for bias translates to the output dimension of a layer. I imagine this was intended (https://github.com/google/jax/issues/2075issuecomment578465814, https://github.com/google/flax/issues/2749) but I also imagine this could be annoying for a lot of people like me, who would like to initialize their biases based on the input dimension in some way (let's say to numerically test some theoretical analysis). Is there any recommended way of initializing the biases like this? If not, is this supposed to be supported in the future at all? Thanks!  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-09T22:16:45Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15501,"I think this is probably more of a flax issue, than jax. I'm closing this and have opened the same issue in the flax repo: https://github.com/google/flax/issues/3019"
382,"以下是一个github上的jax下的一个issue, 标题是([shard-map] better rep-rule-not-implemented error)， 内容是 (Copy of CC([shardmap] better reprulenotimplemented error) and CC([shardmap] better reprulenotimplemented error), trying to trick copybara into working...)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard-map] better rep-rule-not-implemented error,"Copy of CC([shardmap] better reprulenotimplemented error) and CC([shardmap] better reprulenotimplemented error), trying to trick copybara into working...",2023-04-09T04:29:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15495
334,"以下是一个github上的jax下的一个issue, 标题是([shard-map] better rep-rule-not-implemented error)， 内容是 (Copy of CC([shardmap] better reprulenotimplemented error), trying to trick copybara into doing its job...)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard-map] better rep-rule-not-implemented error,"Copy of CC([shardmap] better reprulenotimplemented error), trying to trick copybara into doing its job...",2023-04-09T03:59:35Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15494
1400,"以下是一个github上的jax下的一个issue, 标题是(Unexpected behavior when multiplying Jax boolean with jnp.inf)， 内容是 ( Description Hello! I recently was struggling to find a bug in my code, when I realized the problem came from some weird behavior from Jax. Below, note the inconsistency in evaluation when mutliplying `jnp.array(False)` with arrays of length >1: ``` import jax.numpy as jnp f = jnp.array(False) t = jnp.array(True) inf=jnp.inf a = jnp.array([inf]) b = jnp.array([inf, inf])   Scalar multiplication (what I used as a baseline for 'normal') print( t * inf )   > Array(inf, dtype=float32) print( f * inf )   > Array(0., dtype=float32)  Array multiplication print( t * a )   normal behavior > Array([inf], dtype=float32) print( f * a )   normal behavior > Array([0.], dtype=float32) print( t * b )  normal behavior > Array([inf, inf], dtype=float32) print( f * b )  unexpected behavior! > Array([nan, nan], dtype=float32) ``` It seems like there is a low probability that this behavior is intentional, so I decided to open this issue. Thanks in advance for any help!  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.12, using Ubuntu on WSL2  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unexpected behavior when multiplying Jax boolean with jnp.inf," Description Hello! I recently was struggling to find a bug in my code, when I realized the problem came from some weird behavior from Jax. Below, note the inconsistency in evaluation when mutliplying `jnp.array(False)` with arrays of length >1: ``` import jax.numpy as jnp f = jnp.array(False) t = jnp.array(True) inf=jnp.inf a = jnp.array([inf]) b = jnp.array([inf, inf])   Scalar multiplication (what I used as a baseline for 'normal') print( t * inf )   > Array(inf, dtype=float32) print( f * inf )   > Array(0., dtype=float32)  Array multiplication print( t * a )   normal behavior > Array([inf], dtype=float32) print( f * a )   normal behavior > Array([0.], dtype=float32) print( t * b )  normal behavior > Array([inf, inf], dtype=float32) print( f * b )  unexpected behavior! > Array([nan, nan], dtype=float32) ``` It seems like there is a low probability that this behavior is intentional, so I decided to open this issue. Thanks in advance for any help!  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.12, using Ubuntu on WSL2  NVIDIA GPU info ``` ++  ++ ```",2023-04-08T18:02:54Z,bug XLA,closed,0,4,https://github.com/jax-ml/jax/issues/15492,I actually find it super weird that `jnp.array(False) * jnp.inf` gives 0.,"This looks like an incorrect optimization in XLA: ``` In [1]: print(jax.jit(lambda x, y: x*y).lower(f, inf).as_text(dialect=""hlo"")) HloModule jit__lambda_, entry_computation_layout={(pred[],f32[])>f32[]} ENTRY main.5 {   Arg_0.1 = pred[] parameter(0), sharding={replicated}   convert.3 = f32[] convert(Arg_0.1)   Arg_1.2 = f32[] parameter(1), sharding={replicated}   ROOT multiply.4 = f32[] multiply(convert.3, Arg_1.2) } In[2]: print(jax.jit(lambda x, y: x*y).lower(f, inf).compile().as_text()) HloModule jit__lambda_, entry_computation_layout={(pred[],f32[])>f32[]}, allow_spmd_sharding_propagation_to_output={true} ENTRY %main.5 (Arg_0.1: pred[], Arg_1.2: f32[]) > f32[] {   %Arg_0.1 = pred[] parameter(0), sharding={replicated}   %Arg_1.2 = f32[] parameter(1), sharding={replicated}   %constant.1 = f32[] constant(0)   ROOT %select = f32[] select(pred[] %Arg_0.1, f32[] %Arg_1.2, f32[] %constant.1), metadata={op_name=""jit()/jit(main)/mul"" source_file="""" source_line=1} } ```",Possible duplicate: CC(Multiplying Nan by False give 0. instead of NaN),Closing as duplicate of CC(Multiplying Nan by False give 0. instead of NaN)
993,"以下是一个github上的jax下的一个issue, 标题是(Is it possible to implement Betainc gradient with respect to a and b?)， 内容是 (I was trying to sample from `pymc` package using `numpyro` backend (which in turns use `jax`) and got the following error ``` ValueError: Betainc gradient with respect to a and b not supported. ``` which pointed to this line of code After searching a bit there seems to be a way to calculate this value from this paper in section 3.1 on equation 14, and 29. However the math is a bit over my head   For Generalized Hypergeometric Function 2F1 can be found in `scipy.special.hyp2f1`, and 2F1 & 3F2 implementation can be found in `mpmath` package, and Incomplete Beta Function is  $\psi(x)$ is the digamma function i.e.   [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Is it possible to implement Betainc gradient with respect to a and b?,"I was trying to sample from `pymc` package using `numpyro` backend (which in turns use `jax`) and got the following error ``` ValueError: Betainc gradient with respect to a and b not supported. ``` which pointed to this line of code After searching a bit there seems to be a way to calculate this value from this paper in section 3.1 on equation 14, and 29. However the math is a bit over my head   For Generalized Hypergeometric Function 2F1 can be found in `scipy.special.hyp2f1`, and 2F1 & 3F2 implementation can be found in `mpmath` package, and Incomplete Beta Function is  $\psi(x)$ is the digamma function i.e.   [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2023-04-08T14:21:20Z,enhancement,open,3,2,https://github.com/jax-ml/jax/issues/15487,"I think I can work on this. Actually, I love working on implementations of challenging mathematical functions that are optimized for modern hardware like GPU and TPU. Last year, I made some contributions to TensorFlow Probability in this regard:  Partial derivatives of the regularized incomplete beta function with respect to `a` and `b` [link]  Inverse of the regularized incomplete beta function and its partial derivatives [link]  Improved CDF and quantile functions of the Student's Tdistribution, respectively `stdtr` and `stdtrit` (more accurate than the corresponding SciPy implementation) [link] You can use TensorFlow Probability to compute the partial derivatives of `betainc` with respect to all its parameters: ```python from jax import grad import jax.numpy as jnp from tensorflow_probability.substrates import jax as tfp a = 0.5 b = 4. x = 0.1 grad(tfp.math.betainc, argnums=[0, 1, 2])(a, b, x) ``` This produces: ``` (Array(0.68118393, dtype=float32, weak_type=True),  Array(0.06340793, dtype=float32, weak_type=True),  Array(2.5214243, dtype=float32, weak_type=True)) ``` : Considering the cost of maintaining this implementation, do you think it's worth writing a specific JAX implementation of the partial derivatives of `betainc`? If so, could you provide some examples of optimized custom gradient implementations that are well documented and tested (and that can help me write idiomatic JAX code)?","Just adding to the usefulness of this: The derivatives of the incomplete Beta are necessary to compute the gradient of the CDF of the Beta, which in turn is the canonical way to differentiate through the Beta , as per the ""Implicit reparametrization gradients"" paper: https://proceedings.neurips.cc/paper_files/paper/2018/hash/92c8c96e4c37100777c7190b76d28233Abstract.html"
1135,"以下是一个github上的jax下的一个issue, 标题是(Deprecation warning when parsing nightly packages)， 内容是 ( Description When running `pip` with `f https://storage.googleapis.com/jaxreleases/jaxlib_nightly_releases.html` to install nightlies, I'm getting this deprecation warning: ``` DEPRECATION: The HTML index page being used (https://storage.googleapis.com/jaxreleases/jaxlib_nightly_releases.html) is not a proper HTML 5 document. This is in violation of PEP 503 which requires these pages to be wellformed HTML 5 documents. Please reach out to the owners of this index page, and ask them to update this index page to a valid HTML 5 document. pip 22.2 will enforce this behaviour change. Discussion can be found at https://github.com/pypa/pip/issues/10825 ``` I'm using pip 22.0.2. Probably best to fix whatever it is angry about soon.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Deprecation warning when parsing nightly packages," Description When running `pip` with `f https://storage.googleapis.com/jaxreleases/jaxlib_nightly_releases.html` to install nightlies, I'm getting this deprecation warning: ``` DEPRECATION: The HTML index page being used (https://storage.googleapis.com/jaxreleases/jaxlib_nightly_releases.html) is not a proper HTML 5 document. This is in violation of PEP 503 which requires these pages to be wellformed HTML 5 documents. Please reach out to the owners of this index page, and ask them to update this index page to a valid HTML 5 document. pip 22.2 will enforce this behaviour change. Discussion can be found at https://github.com/pypa/pip/issues/10825 ``` I'm using pip 22.0.2. Probably best to fix whatever it is angry about soon.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-07T22:17:02Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15476,"Thanks for the report – this looks like a duplicate of CC(Please provide PEP 503 compliant indices for CUDA versions of packages), but is still something we need to address.",Closing in favor of https://github.com/google/jax/issues/5410
4161,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: module 'jax' has no attribute 'Array')， 内容是 ( Description Use this notebook from https://github.com/kingoflolz/meshtransformerjax.git to reproduce the issue.  What jax/jaxlib version are you using? flax0.6.9 jax0.4.8 optax0.1.5.dev0 chex0.1.7  Which accelerator(s) are you using? cloud TPU  Additional system info python 3.8.10  NVIDIA GPU info I have installed the git development version of `flax`, `jax` , `optax` and `chex`, but I am still getting `AttributeError: module 'jax' has no attribute 'Array'` ```  AttributeError                            Traceback (most recent call last) Cell In[15], line 6       4 from jax.experimental import maps       5 import numpy as np > 6 import optax       7 import transformers       9 from mesh_transformer.checkpoint import read_ckpt_lowmem File ~/.local/lib/python3.8/sitepackages/optax/__init__.py:17       1  Copyright 2019 DeepMind Technologies Limited. All Rights Reserved.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      13  limitations under the License.      14  ==============================================================================      15 """"""Optax: composable gradient processing and optimization, in JAX."""""" > 17 from optax import experimental      18 from optax._src.alias import adabelief      19 from optax._src.alias import adafactor File ~/.local/lib/python3.8/sitepackages/optax/experimental/__init__.py:20       1  Copyright 2021 DeepMind Technologies Limited. All Rights Reserved.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      13  limitations under the License.      14  ==============================================================================      15 """"""Experimental features in Optax.      16       17 Features may be removed or modified at any time.      18 """""" > 20 from optax._src.experimental.complex_valued import split_real_and_imaginary      21 from optax._src.experimental.complex_valued import SplitRealAndImaginaryState      22 from optax._src.experimental.extra_args import GradientTransformationWithExtraArgs File ~/.local/lib/python3.8/sitepackages/optax/_src/experimental/complex_valued.py:32      15 """"""Complexvalued optimization.      16       17 When using `split_real_and_imaginary` to wrap an optimizer, we split the complex    (...)      27 See details at https://github.com/deepmind/optax/issues/196      28 """"""      30 from typing import NamedTuple, Union > 32 import chex      33 import jax      34 import jax.numpy as jnp File ~/.local/lib/python3.8/sitepackages/chex/__init__.py:17       1  Copyright 2020 DeepMind Technologies Limited. All Rights Reserved.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      13  limitations under the License.      14  ==============================================================================      15 """"""Chex: Testing made fun, in JAX!"""""" > 17 from chex._src.asserts import assert_axis_dimension      18 from chex._src.asserts import assert_axis_dimension_comparator      19 from chex._src.asserts import assert_axis_dimension_gt File ~/.local/lib/python3.8/sitepackages/chex/_src/asserts.py:26      23 import unittest      24 from unittest import mock > 26 from chex._src import asserts_internal as _ai      27 from chex._src import pytypes      28 import jax File ~/.local/lib/python3.8/sitepackages/chex/_src/asserts_internal.py:34      31 from typing import Any, Sequence, Union, Callable, List, Optional, Set, Tuple, Type      33 from absl import logging > 34 from chex._src import pytypes      35 import jax      36 from jax.experimental import checkify File ~/.local/lib/python3.8/sitepackages/chex/_src/pytypes.py:27      24 ArrayNumpy = np.ndarray      26  For instance checking, use `isinstance(x, jax.Array)`. > 27 ArrayDevice = jax.Array      29  Types for backward compatibility.      30 ArraySharded = jax.Array AttributeError: module 'jax' has no attribute 'Array' ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,AttributeError: module 'jax' has no attribute 'Array'," Description Use this notebook from https://github.com/kingoflolz/meshtransformerjax.git to reproduce the issue.  What jax/jaxlib version are you using? flax0.6.9 jax0.4.8 optax0.1.5.dev0 chex0.1.7  Which accelerator(s) are you using? cloud TPU  Additional system info python 3.8.10  NVIDIA GPU info I have installed the git development version of `flax`, `jax` , `optax` and `chex`, but I am still getting `AttributeError: module 'jax' has no attribute 'Array'` ```  AttributeError                            Traceback (most recent call last) Cell In[15], line 6       4 from jax.experimental import maps       5 import numpy as np > 6 import optax       7 import transformers       9 from mesh_transformer.checkpoint import read_ckpt_lowmem File ~/.local/lib/python3.8/sitepackages/optax/__init__.py:17       1  Copyright 2019 DeepMind Technologies Limited. All Rights Reserved.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      13  limitations under the License.      14  ==============================================================================      15 """"""Optax: composable gradient processing and optimization, in JAX."""""" > 17 from optax import experimental      18 from optax._src.alias import adabelief      19 from optax._src.alias import adafactor File ~/.local/lib/python3.8/sitepackages/optax/experimental/__init__.py:20       1  Copyright 2021 DeepMind Technologies Limited. All Rights Reserved.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      13  limitations under the License.      14  ==============================================================================      15 """"""Experimental features in Optax.      16       17 Features may be removed or modified at any time.      18 """""" > 20 from optax._src.experimental.complex_valued import split_real_and_imaginary      21 from optax._src.experimental.complex_valued import SplitRealAndImaginaryState      22 from optax._src.experimental.extra_args import GradientTransformationWithExtraArgs File ~/.local/lib/python3.8/sitepackages/optax/_src/experimental/complex_valued.py:32      15 """"""Complexvalued optimization.      16       17 When using `split_real_and_imaginary` to wrap an optimizer, we split the complex    (...)      27 See details at https://github.com/deepmind/optax/issues/196      28 """"""      30 from typing import NamedTuple, Union > 32 import chex      33 import jax      34 import jax.numpy as jnp File ~/.local/lib/python3.8/sitepackages/chex/__init__.py:17       1  Copyright 2020 DeepMind Technologies Limited. All Rights Reserved.       2        3  Licensed under the Apache License, Version 2.0 (the ""License"");    (...)      13  limitations under the License.      14  ==============================================================================      15 """"""Chex: Testing made fun, in JAX!"""""" > 17 from chex._src.asserts import assert_axis_dimension      18 from chex._src.asserts import assert_axis_dimension_comparator      19 from chex._src.asserts import assert_axis_dimension_gt File ~/.local/lib/python3.8/sitepackages/chex/_src/asserts.py:26      23 import unittest      24 from unittest import mock > 26 from chex._src import asserts_internal as _ai      27 from chex._src import pytypes      28 import jax File ~/.local/lib/python3.8/sitepackages/chex/_src/asserts_internal.py:34      31 from typing import Any, Sequence, Union, Callable, List, Optional, Set, Tuple, Type      33 from absl import logging > 34 from chex._src import pytypes      35 import jax      36 from jax.experimental import checkify File ~/.local/lib/python3.8/sitepackages/chex/_src/pytypes.py:27      24 ArrayNumpy = np.ndarray      26  For instance checking, use `isinstance(x, jax.Array)`. > 27 ArrayDevice = jax.Array      29  Types for backward compatibility.      30 ArraySharded = jax.Array AttributeError: module 'jax' has no attribute 'Array' ```",2023-04-07T17:05:08Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/15466,I think the issue is that that notebook downgrades jax: ```  jax 0.2.12 is required due to a regression with xmap in 0.2.13 !pip install meshtransformerjax/ jax==0.2.12 tensorflow==2.5.0 ``` That would explain why `jax.Array` does not exist.,"I had retried without the following lines, but still the same error ```python !/bin/python3 m pip install r meshtransformerjax/requirements.txt  jax 0.2.12 is required due to a regression with xmap in 0.2.13 !/bin/python3 m pip install u meshtransformerjax/ tensorflow` ```",Show the output of `pip list` ?,Here you go: ``` Package                  Version   abslpy                  0.15.0 acme                     1.1.0 aiohttp                  3.8.3 aiohttpcors             0.7.0 aioredis                 2.0.1 aiosignal                1.3.1 antlr4python3runtime   4.9.3 anyio                    3.6.2 appdirs                  1.4.4 asttokens                2.2.1 astunparse               1.6.3 asynctimeout            4.0.2 attrs                    19.3.0 Automat                  0.8.0 backcall                 0.2.0 bcrypt                   4.0.1 blessed                  1.20.0 blinker                  1.4 cachedproperty          1.5.2 cachetools               4.2.4 certbot                  0.40.0 certbotnginx            0.40.0 certifi                  2019.11.28 cffi                     1.15.1 chardet                  3.0.4 charsetnormalizer       2.0.12 chex                     0.1.7 clang                    5.0 click                    7.1.2 cloudinit               22.2 cloudtpuclient         0.10 cloudpickle              1.3.0 colorama                 0.4.3 colorful                 0.5.5 comm                     0.1.2 commandnotfound        0.3 commonmark               0.9.1 ConfigArgParse           0.13.0 configobj                5.0.6 constantly               15.1.0 contourpy                1.0.6 cryptography             40.0.1 cycler                   0.11.0 Cython                   0.29.14 DataProperty             0.55.0 datasets                 2.8.0 dbuspython              1.2.16 debugpy                  1.6.5 decorator                5.1.1 dill                     0.3.6 distlib                  0.3.4 distro                   1.4.0 distroinfo              0.23ubuntu1 dmhaiku                 0.0.5 dmtree                  0.1.8 dockerpycreds           0.4.0 einops                   0.3.2 entrypoints              0.3 etils                    1.0.0 evaluate                 0.4.0 exceptiongroup           1.1.0 executing                1.2.0 fabric                   2.6.0 fastapi                  0.95.0 filelock                 3.7.1 Flask                    1.1.4 flatbuffers              1.12 flax                     0.6.9 fonttools                4.38.0 frozenlist               1.3.3 fsspec                   2022.11.0 ftfy                     6.1.1 functimeout             4.3.5 future                   0.18.2 gast                     0.4.0 gitdb                    4.0.10 GitPython                3.1.31 googleapicore          1.34.0 googleapipythonclient 1.8.0 googleauth              1.35.0 googleauthhttplib2     0.1.0 googleauthoauthlib     0.4.6 googlecloudcore        1.7.3 googlecloudstorage     1.36.2 googlecrc32c            1.5.0 googlepasta             0.2.0 googleresumablemedia   1.3.3 googleapiscommonprotos 1.58.0 gpustat                  1.1 grpcio                   1.53.0 h11                      0.14.0 h5py                     3.1.0 httplib2                 0.14.0 huggingfacehub          0.11.1 hyperlink                19.0.0 idna                     2.8 importlibmetadata       6.0.0 importlibresources      5.10.2 incremental              16.10.1 iniconfig                2.0.0 intelopenmp             2022.1.0 invoke                   1.7.3 ipykernel                6.20.1 ipython                  8.8.0 itsdangerous             1.1.0 jax                      0.4.8 jaxlib                   0.4.7 jedi                     0.18.2 Jinja2                   2.10.1 jmp                      0.0.4 joblib                   1.2.0 josepy                   1.2.0 jsonlines                3.1.0 jsonpatch                1.22 jsonpointer              2.0 jsonschema               3.2.0 jupyter_client           7.4.8 jupyter_core             5.1.3 keras                    2.6.0 kerasnightly            2.5.0.dev2021032900 KerasPreprocessing      1.1.2 keyring                  18.0.1 kiwisolver               1.4.4 languageselector        0.1 launchpadlib             1.10.13 lazr.restfulclient       0.14.2 lazr.uri                 1.0.3 libtpunightly           0.1.dev20230327 lmdataformat            0.0.20 lmeval                  0.3.0 Markdown                 3.4.1 MarkupSafe               2.1.1 matplotlib               3.6.2 matplotlibinline        0.1.6 mbstrdecoder             1.1.2 meshtransformer         0.0.0 mkl                      2022.1.0 mklinclude              2022.1.0 mldtypes                0.0.4 mock                     3.0.5 moreitertools           4.2.0 msgpack                  1.0.4 multidict                6.0.4 multiprocess             0.70.14 nestasyncio             1.5.6 netifaces                0.10.4 nltk                     3.8.1 numexpr                  2.8.4 numpy                    1.24.2 nvidiacublascu11       11.10.3.66 nvidiacudanvrtccu11   11.7.99 nvidiacudaruntimecu11 11.7.99 nvidiacudnncu11        8.5.0.96 nvidiamlpy             11.525.112 oauth2client             4.1.3 oauthlib                 3.1.0 omegaconf                2.3.0 openai                   0.27.4 opencensus               0.11.2 opencensuscontext       0.1.3 opteinsum               3.3.0 optax                    0.1.5.dev0 orbax                    0.1.0 orbaxcheckpoint         0.1.8 packaging                23.0 pandas                   1.4.4 paramiko                 3.1.0 parsedatetime            2.4 parso                    0.8.3 pathlib                  1.0.1 pathlib2                 2.3.7.post1 pathtools                0.1.2 pathvalidate             2.5.2 pathy                    0.10.1 pbr                      5.4.5 petl                     1.7.12 pexpect                  4.6.0 pickleshare              0.7.5 Pillow                   9.4.0 pip                      23.0.1 platformdirs             2.5.2 pluggy                   1.0.0 portalocker              2.7.0 prometheusclient        0.16.0 prompttoolkit           3.0.36 protobuf                 3.19.6 psutil                   5.9.4 pureeval                0.2.2 pyspy                   0.3.14 pyarrow                  10.0.1 pyasn1                   0.4.2 pyasn1modules           0.2.1 pybind11                 2.10.4 pycountry                22.3.5 pycparser                2.21 pydantic                 1.9.2 Pygments                 2.14.0 PyGObject                3.36.0 PyHamcrest               1.9.0 PyICU                    2.4.2 PyJWT                    1.7.1 pymacaroons              0.13.0 PyNaCl                   1.5.0 pyOpenSSL                19.0.0 pyparsing                2.4.6 pyRFC3339                1.1 pyrsistent               0.15.5 pyserial                 3.4 pytablewriter            0.64.2 pytest                   7.2.0 pythonapt               2.0.0+ubuntu0.20.4.7 pythondateutil          2.8.2 pythondebian            0.1.36ubuntu1 pytz                     2022.7 PyYAML                   5.4.1 pyzmq                    24.0.1 ray                      1.4.1 redis                    4.5.4 regex                    2022.10.31 requests                 2.25.1 requestsoauthlib        1.3.1 requeststoolbelt        0.8.0 requestsunixsocket      0.2.0 responses                0.18.0 rich                     13.0.1 rougescore              0.1.2 rsa                      4.9 sacrebleu                1.5.0 scikitlearn             1.2.2 scipy                    1.10.0 SecretStorage            2.3.1 sentencepiece            0.1.97 sentrysdk               1.19.1 serviceidentity         18.1.0 setproctitle             1.3.2 setuptools               62.3.2 simplejson               3.16.0 six                      1.15.0 smartopen               6.2.0 smmap                    5.0.0 sniffio                  1.3.0 sos                      4.3 sqlitedict               2.1.0 sshimportid            5.10 stackdata               0.6.2 starlette                0.26.1 systemdpython           234 tabledata                1.3.1 tabulate                 0.9.0 tbb                      2021.6.0 tcolorpy                 0.1.2 tensorboard              2.6.0 tensorboarddataserver  0.6.1 tensorboardpluginwit   1.8.1 tensorflow               2.5.0 tensorflowcpu           2.6.5 tensorflowestimator     2.6.0 tensorstore              0.1.35 termcolor                1.1.0 threadpoolctl            3.1.0 tokenizers               0.13.2 tomli                    2.0.1 toolz                    0.12.0 torch                    1.13.1 torchxla                1.13 tornado                  6.2 tqdm                     4.64.1 tqdmmultiprocess        0.0.11 traitlets                5.8.1 transformers             4.26.1 Twisted                  18.9.0 typepy                   1.3.0 typer                    0.7.0 typing_extensions        4.5.0 ubuntuadvantagetools   27.8 ufw                      0.36 ujson                    5.7.0 unattendedupgrades      0.1 uritemplate              3.0.1 urllib3                  1.26.15 uvicorn                  0.21.1 virtualenv               20.14.1 wadllib                  1.3.3 wandb                    0.14.1 wcwidth                  0.2.5 Werkzeug                 1.0.1 wheel                    0.40.0 wrapt                    1.12.1 xxhash                   3.2.0 yarl                     1.8.2 zipp                     3.11.0 zope.component           4.3.0 zope.event               4.4 zope.hookable            5.0.0 zope.interface           4.7.1 zstandard                0.20.0 ```,"I just killed and restarted the jupyter kernel, it seems to work for now given that it now points to some other different error.  "
1527,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Refactor and simplify jax2tf.py)， 内容是 (This is the next step in the refactoring and simplifying jax2tf. An additional goal is to move to jax_export.py as much pureJAX functionalit as possible so that jax_export can start to become a standalone API for native serialization. Previous PRs in this series:    CC([shape_poly] Refactor the computation of the dimension variables in native serialization) and CC([shape_poly] Improved computation of dimension variables for native serialization): refactoring computation of dimension variables    CC([jax2tf] Cleanup of handling of tf.custom_gradient) and CC([jax2tf] Refactor the gradient machinery for native serialization): refactor handling of tf.custom_gradient (moved VJP serialization to jax_export) Here we do the following:   * we move the handling of in/out pytrees to jax_export. This allows the removal of a workaround that introduced a trampoline for the `lower` function for native lowering.   * we move most of the code to set up the input abstract values into jax_export (`poly_spec` and `poly_specs`)   * we separate out the implementation details for graph and native serialization into two classes `GraphSerializationImpl` and `NativeSerializationImpl`. This simplifies the core of the jax2tf.convert. This is a pure refactoring, it should not change any existing behavior for jax2tf.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Refactor and simplify jax2tf.py,"This is the next step in the refactoring and simplifying jax2tf. An additional goal is to move to jax_export.py as much pureJAX functionalit as possible so that jax_export can start to become a standalone API for native serialization. Previous PRs in this series:    CC([shape_poly] Refactor the computation of the dimension variables in native serialization) and CC([shape_poly] Improved computation of dimension variables for native serialization): refactoring computation of dimension variables    CC([jax2tf] Cleanup of handling of tf.custom_gradient) and CC([jax2tf] Refactor the gradient machinery for native serialization): refactor handling of tf.custom_gradient (moved VJP serialization to jax_export) Here we do the following:   * we move the handling of in/out pytrees to jax_export. This allows the removal of a workaround that introduced a trampoline for the `lower` function for native lowering.   * we move most of the code to set up the input abstract values into jax_export (`poly_spec` and `poly_specs`)   * we separate out the implementation details for graph and native serialization into two classes `GraphSerializationImpl` and `NativeSerializationImpl`. This simplifies the core of the jax2tf.convert. This is a pure refactoring, it should not change any existing behavior for jax2tf.",2023-04-07T14:22:18Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/15457,"  PTAL. This is a pretty big refactoring. I will land this when I return from vacation, after April 24th."
1804,"以下是一个github上的jax下的一个issue, 标题是(Add missing exports to jax.config and jax.debug)， 内容是 ( Description Add missing exports to jax.config and jax.debug (and any other such files). See CC(Consider defining public API explicitly) for context. Example: ```python3 from jax.config import config from jax.debug import print, callback, breakpoint ``` mypy yields ``` example.py:1: error: Module ""jax.config"" does not explicitly export attribute ""config""  [attrdefined] example.py:2: error: Module ""jax.debug"" does not explicitly export attribute ""print""  [attrdefined] example.py:2: error: Module ""jax.debug"" does not explicitly export attribute ""callback""  [attrdefined] example.py:2: error: Module ""jax.debug"" does not explicitly export attribute ""breakpoint""  [attrdefined] ``` pyright yields ```   /Users/carlos/Desktop/example.py:1:24  error: ""config"" is not exported from module ""jax.config""     Import from ""jax._src.config"" instead (reportPrivateImportUsage)   /Users/carlos/Desktop/example.py:2:23  error: ""print"" is not exported from module ""jax.debug""     Import from ""jax._src.debugging"" instead (reportPrivateImportUsage)   /Users/carlos/Desktop/example.py:2:30  error: ""callback"" is not exported from module ""jax.debug""     Import from ""jax._src.debugging"" instead (reportPrivateImportUsage)   /Users/carlos/Desktop/example.py:2:40  error: ""breakpoint"" is not exported from module ""jax.debug""     Import from ""jax._src.debugger.core"" instead (reportPrivateImportUsage) ```  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.2, macOS 11.7.4  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add missing exports to jax.config and jax.debug," Description Add missing exports to jax.config and jax.debug (and any other such files). See CC(Consider defining public API explicitly) for context. Example: ```python3 from jax.config import config from jax.debug import print, callback, breakpoint ``` mypy yields ``` example.py:1: error: Module ""jax.config"" does not explicitly export attribute ""config""  [attrdefined] example.py:2: error: Module ""jax.debug"" does not explicitly export attribute ""print""  [attrdefined] example.py:2: error: Module ""jax.debug"" does not explicitly export attribute ""callback""  [attrdefined] example.py:2: error: Module ""jax.debug"" does not explicitly export attribute ""breakpoint""  [attrdefined] ``` pyright yields ```   /Users/carlos/Desktop/example.py:1:24  error: ""config"" is not exported from module ""jax.config""     Import from ""jax._src.config"" instead (reportPrivateImportUsage)   /Users/carlos/Desktop/example.py:2:23  error: ""print"" is not exported from module ""jax.debug""     Import from ""jax._src.debugging"" instead (reportPrivateImportUsage)   /Users/carlos/Desktop/example.py:2:30  error: ""callback"" is not exported from module ""jax.debug""     Import from ""jax._src.debugging"" instead (reportPrivateImportUsage)   /Users/carlos/Desktop/example.py:2:40  error: ""breakpoint"" is not exported from module ""jax.debug""     Import from ""jax._src.debugger.core"" instead (reportPrivateImportUsage) ```  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.2, macOS 11.7.4  NVIDIA GPU info _No response_",2023-04-07T03:01:01Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/15453,"> ```python > from jax.config import config > ``` I think this is not meant to be a supported import. This is the supported way to get the same thing: ```python from jax import config ``` (yes, it's strange that we rewrite the `config` module with the `config` object: we need to clean that up at some point) > ```python > from jax.debug import print, callback, breakpoint > ``` We should fix these. `breakpoint` is easy (we just need to add `as breakpoint`), but I'm not sure how to tell `mypy` that `callback` and `print` are meant to be exported, because they are renamed on import: https://github.com/google/jax/blob/87a1fea1c71b9b059b286c86bda4942cea766e3a/jax/debug.pyL14L20 Do you know how to tell mypy that the renamed  imports are meant as exported names?","  > I think this is not meant to be a supported import. In that case, should the docs be updated to reflect this? For example, here it currently says: > You can manually set the jax_enable_x64 configuration flag at startup: > ``` again, this only works on startup! > from jax.config import config > config.update(""jax_enable_x64"", True) > ``` > Do you know how to tell mypy that the renamed imports are meant as exported names? Unfortunately, I do not.","Yeah, that should probably be updated."," I'm also getting an error for `jax.distributed.initialize`: ``` error: ""initialize"" is not exported from module ""jax.distributed"" ``` Should I open a new issue?"
4037,"以下是一个github上的jax下的一个issue, 标题是(JEP: Allow finite difference method for non-jax differentiable functions)， 内容是 ( Motivation: Make JAX relevant to many more users JAX provides a powerful generalpurpose tool for automatic differentiation, but it usually requires that users write code that is JAXtraceable endtoend.  Significant numbers of scientific and industrial applications involve large, legacy codebases where the lift to transfer the system to endtoend JAX is prohibitively high. In other cases, users are tied into proprietary software, or the underlying software is not written in python, and also find themselves unable to readily convert the underlying code to JAX. Without JAX, the standard method for performing optimization involves computing derivatives using _finite difference methods_. While these can be integrated into JAX using custom functions, the process is cumbersome, which significantly limits the set of users able to integrate JAX into their work. This JEP proposes a simple method for computing numerical derivatives in JAX. I expect that this change would expand the potential user base of JAX substantially, and could drive adoption of JAX across both academia and industry.  Proposal: A decorator that computes JAX derivatives using finite differences Let's start with an example. ```python from jax import grad from jax import numpy as jnp from jax.experimental.finite_difference import jax_finite_difference import numpy as old_np   Not jaxtraceable  def rosenbach2(x, y):     """"""Compute the Rosenbach function for two variables.""""""     return old_np.pow(1x, 2) + 100*old_np.pow(yold_np.pow(x, 2), 2) def rosenbach3(x, y, z):     """"""Compute the Rosenbach function for three variables.""""""     return rosenbach2(x, y) + rosenbach2(y, z) value, grad = jax.value_and_grad(rosenbach3)(1., 2., 3.)  ``` By wrapping the function `rosenbach2` in `jax_finite_difference`, it will become completely compatible with JAX's automatic differentiation tooling, and works with other JAX primitives such as `vmap`.  Additional options will be available for power users who may want to specify the step size, or forward vs center vs backward mode.  This is feasible. I have working, tested code that does the above for any function that accepts and returns JAX `Array`s. If there is interest in this JEP, I will happily make a PR.  Limits of this JEP  This proposal will not support XLA out of the box The initial proposal does not aim to support XLA for finite differences. While it should be possible to overcome this limitation using a JAX Foreign Function Interface (FFI) [Issue CC([FFI]: Add JEP for FFI), PR], it would be best to wait until the FFI is finalized before implementing XLA for finite differences.  The downsides of `float32` increase with finite differences Using singleprecision (32bit) floating point numbers in finite differences may lead to unacceptably large errors in many cases. While this is not a foregone conclusion—many functions can be differentiated just fine with 32bit floating point—we probably want to plan for mitigation strategies, e.g.,    Warning the users if they are using less than doubleprecision; or    Making it easy to automatically convert to doubleprecision for the finite difference. The second strategy would likely be more important when using FFI in conjunction with XLA in later work. At this stage a warning may be all that's needed.  Related JEPs The proposed Foreign Function Interface [Issue CC([FFI]: Add JEP for FFI), PR] will provide a method that allows JAX code to call out to external code code in the course of derivative computation. However, it does not create a method for computing derivatives—those must still be defined by the user.  However, we expect that the FFI combined with our finitedifference method would enable ""the dream"": nearlyarbitrary user code fullyintegrated with JAX using a single decorator.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JEP: Allow finite difference method for non-jax differentiable functions," Motivation: Make JAX relevant to many more users JAX provides a powerful generalpurpose tool for automatic differentiation, but it usually requires that users write code that is JAXtraceable endtoend.  Significant numbers of scientific and industrial applications involve large, legacy codebases where the lift to transfer the system to endtoend JAX is prohibitively high. In other cases, users are tied into proprietary software, or the underlying software is not written in python, and also find themselves unable to readily convert the underlying code to JAX. Without JAX, the standard method for performing optimization involves computing derivatives using _finite difference methods_. While these can be integrated into JAX using custom functions, the process is cumbersome, which significantly limits the set of users able to integrate JAX into their work. This JEP proposes a simple method for computing numerical derivatives in JAX. I expect that this change would expand the potential user base of JAX substantially, and could drive adoption of JAX across both academia and industry.  Proposal: A decorator that computes JAX derivatives using finite differences Let's start with an example. ```python from jax import grad from jax import numpy as jnp from jax.experimental.finite_difference import jax_finite_difference import numpy as old_np   Not jaxtraceable  def rosenbach2(x, y):     """"""Compute the Rosenbach function for two variables.""""""     return old_np.pow(1x, 2) + 100*old_np.pow(yold_np.pow(x, 2), 2) def rosenbach3(x, y, z):     """"""Compute the Rosenbach function for three variables.""""""     return rosenbach2(x, y) + rosenbach2(y, z) value, grad = jax.value_and_grad(rosenbach3)(1., 2., 3.)  ``` By wrapping the function `rosenbach2` in `jax_finite_difference`, it will become completely compatible with JAX's automatic differentiation tooling, and works with other JAX primitives such as `vmap`.  Additional options will be available for power users who may want to specify the step size, or forward vs center vs backward mode.  This is feasible. I have working, tested code that does the above for any function that accepts and returns JAX `Array`s. If there is interest in this JEP, I will happily make a PR.  Limits of this JEP  This proposal will not support XLA out of the box The initial proposal does not aim to support XLA for finite differences. While it should be possible to overcome this limitation using a JAX Foreign Function Interface (FFI) [Issue CC([FFI]: Add JEP for FFI), PR], it would be best to wait until the FFI is finalized before implementing XLA for finite differences.  The downsides of `float32` increase with finite differences Using singleprecision (32bit) floating point numbers in finite differences may lead to unacceptably large errors in many cases. While this is not a foregone conclusion—many functions can be differentiated just fine with 32bit floating point—we probably want to plan for mitigation strategies, e.g.,    Warning the users if they are using less than doubleprecision; or    Making it easy to automatically convert to doubleprecision for the finite difference. The second strategy would likely be more important when using FFI in conjunction with XLA in later work. At this stage a warning may be all that's needed.  Related JEPs The proposed Foreign Function Interface [Issue CC([FFI]: Add JEP for FFI), PR] will provide a method that allows JAX code to call out to external code code in the course of derivative computation. However, it does not create a method for computing derivatives—those must still be defined by the user.  However, we expect that the FFI combined with our finitedifference method would enable ""the dream"": nearlyarbitrary user code fullyintegrated with JAX using a single decorator.",2023-04-06T00:41:14Z,enhancement,open,2,12,https://github.com/jax-ml/jax/issues/15425,"Thanks for the proposal! We've discussed something along these lines amongst ourselves at various points. It's a good idea! Do you have an idea for the API you'd propose? When this has come up before, our thought was to make it a convenience wrapper around `jax.custom_jvp`. That suggests an implementation using `custom_jvp` as well.","I actually implemented it using fullon custom primitives following this guide. Since it seems like there's interest, I'll make the PR."," any reason not to use a `custom_jvp` or `custom_vjp`? Those are a lot simpler than a custom primitive, and I think it'd be the preferred approach unless I'm missing something.",In particular `custom_jvp` would support both forward and reversemode AD.,"Hello, for the context of API design, I have a small WIP library for finite difference, I implemented a finite difference version of `jax.grad` with couple of options colab ```python import jax  from jax import numpy as jnp import numpy as old_np   Not jaxtraceable import finitediffx as fdx import functools as ft  from jax.experimental import enable_x64 with enable_x64():     .fgrad     .fgrad     def np_rosenbach2(x, y):         """"""Compute the Rosenbach function for two variables.""""""         return old_np.power(1x, 2) + 100*old_np.power(yold_np.power(x, 2), 2)     .partial(fdx.fgrad, derivative=2)     def np2_rosenbach2(x, y):         """"""Compute the Rosenbach function for two variables.""""""         return old_np.power(1x, 2) + 100*old_np.power(yold_np.power(x, 2), 2)     .grad      .grad     def jnp_rosenbach2(x, y):         """"""Compute the Rosenbach function for two variables.""""""         return jnp.power(1x, 2) + 100*jnp.power(yjnp.power(x, 2), 2)     print(np_rosenbach2(1.,2.))     print(np_rosenbach2(1.,2.))     print(jnp_rosenbach2(1., 2.)) 402.0000951997936 402.0000951997936 402.0 ```","As far as I understand we can also use `fdx.fgrad` with `custom_jvp` and `pure_callback`  to make nontraceable code work with `jax` transformations. ```python import functools as ft import jax import jax.numpy as jnp import numpy as onp import finitediffx as fdx def wrap_pure_callback(func):     .wraps(func)     def wrapper(*args, **kwargs):         args = [jnp.asarray(arg) for arg in args]         func_ = lambda *args, **kwargs: func(*args, **kwargs).astype(args[0].dtype)         result_shape_dtype = jax.ShapeDtypeStruct(             shape=jnp.broadcast_shapes(*[arg.shape for arg in args]),             dtype=args[0].dtype,         )         return jax.pure_callback(             func_, result_shape_dtype, *args, **kwargs, vectorized=True         )     return wrapper def define_finitdiff_jvp(func):     func = jax.custom_jvp(func)     .defjvp     def func_jvp(primals, tangents):         primal_out = func(*primals)         tangent_out = sum(             fdx.fgrad(func, argnums=i)(*primals) * dot for i, dot in enumerate(tangents)         )         return jnp.array(primal_out), jnp.array(tangent_out)     return func .jit   def np_rosenbach2(x, y):     """"""Compute the Rosenbach function for two variables.""""""     return onp.power(1  x, 2) + 100 * onp.power(y  onp.power(x, 2), 2) .jit def jnp_rosenbach2(x, y):     """"""Compute the Rosenbach function for two variables.""""""     return jnp.power(1  x, 2) + 100 * jnp.power(y  jnp.power(x, 2), 2) print(jax.value_and_grad(np_rosenbach2, argnums=0)(1.0, 2.0)) print(jax.value_and_grad(jnp_rosenbach2, argnums=0)(1.0, 2.0)) print(jax.value_and_grad(np_rosenbach2, argnums=1)(1.0, 2.0)) print(jax.value_and_grad(jnp_rosenbach2, argnums=1)(1.0, 2.0)) print(jax.vmap(jax.grad(np_rosenbach2), in_axes=(0, None))(jnp.array([1.0, 2.0, 3.0, 0.2]), 2.0)) print(jax.vmap(jax.grad(jnp_rosenbach2), in_axes=(0, None))(jnp.array([1.0, 2.0, 3.0, 0.2]), 2.0)) (Array(100., dtype=float32), Array(399.9948, dtype=float32, weak_type=True)) (Array(100., dtype=float32, weak_type=True), Array(400., dtype=float32, weak_type=True)) (Array(100., dtype=float32), Array(199.97772, dtype=float32, weak_type=True)) (Array(100., dtype=float32, weak_type=True), Array(200., dtype=float32, weak_type=True)) [399.9948  1601.8411  8403.304   158.45016] [400.      1602.      8404.      158.40001] ```","> As far as I understand  So understated! Seems like you've written a package that does most of the work here. :) Given this, is an implementation of finitedifferences in the core JAX package (e.g., under `jax.experimental`) desired? I'm inclined to think that this would be a very valuable addition to the JAX ecosystem, either here or some sister project, because of how useful I'm finding the tool when using third party scientific libraries. FYI, I'm happy to collaborate on a PR  if that's of interest. I'll try to get something up in the next day or so for more commentI've had some busy days in the last week.  I'll look into using `custom_vjp` and `custom_jvp`—I recall having a reason I didn't use those to start with, but it's possible that I just missed something.","To be concrete about the API , here are some guiding principles that I have in mind: 0. A reasonable name. Personally, I think a decorator named `jaxify` is appropriate because it gets at ""what it means for most users"", that is, make their existing code work with JAX. Other names like `jax_finite_difference` are also OK, but they are based on implementation and are a bit more technical. 1. Oneline decorator that ""just works"" for 80% of cases, and provides sensible error messages for the most common issues (e.g., not abiding by the API). 2. More powerful options for ~1519% of the remaining cases. For example:      Step size control      Mode choice (e.g., central, forward, and backward modes)      Allowing different steps sizes or modes per argument. 4. Reasonably efficient, so that, e.g., forward mode uses significantly fewer function evaluations than central mode. 5. Consistent with the existing API. I like the idea that it has the same requirements as `pure_callback` (as  helpfully suggested). 6. It might be nice to warn users when the use of this decorator has kept them from having full hardware acceleration, though that may be best left as a feature for `pure_callback`. (Not sure if this is easy or hard.) 7. Welldocumented, e.g., a notebook with documentation for use.  API Examples  The ""80%"" use case Most use cases should start with a simple decorator: ```python import jax from jax.experimental.jaxify import jaxify from jax.experimental import enable_x64  def my_func(array1, array2):    return some_complex_math(array1, array2) print(jax.value_and_grad(my_func)(x, y))   Warn about not using 64bit math with enable_x64():     jax.value_and_grad(my_func)(x, y)   No warning ```  Power use cases ```python  The user wants control over the step size (step_size=1e9, mode=""forward"") def my_func(array1, array2):    return some_complex_math(array1, array2)  The user wants perargument control over the step size (step_size=(1e9, 1e3), mode=(""forward"", ""center"")) def my_func(array1, array2):    return some_complex_math(array1, array2) ```", – Why bundle together (a) setting up a derivative rule based on FD with (b) setting up a `pure_callback` call?,"There are two reasons I can think of to support FD for the same class of functions that `pure_callback` supports: equivalence and consistency. (Note that I'm suggesting that we use the same API for the functions, nothing more.) **Equivalence:** The class of functions theoretically supportable by a generic FD technique are precisely those that are theoretically supportable by a `pure_callback` mechansim (that is, pure functions that accept and return numpy arrays).  Think about it: almost by definition, the functions we'd want to finitedifference are not supported directly within JAX's JIT, so computing their values during a generic finitedifference routine requires the use of some mechanism like `pure_callback`. If we manage to write an FD routine that supports more functions than `pure_callback`, we could backport the functionality to `pure_callback` using, for example, the `value` part of `jax.value_and_grad`. Conversely, it's pretty easy to see that—at least in principle—we can write a wrapper that will apply finite differences to pure functions that accept and return numpy arrays. That's, of course, the challenge I've set out for myself here. **Consistency:** Given the close link between the sets of functions supportable by `pure_callback` and those supportable by a generic finite difference routine, we should probably just insist that they _are_ the same and test for it. This will allow us to leverage `pure_callback` and keep the overall API complexity roughly constant.  Example documentation ```python """"""Make a function differentiable with JAX using finite differences. [...] The function must be pure (that it, sideeffect free and deterministic based on its inputs),  and both its inputs and outputs must be ``numpy`` arrays. These requirements are the same as for the ``jax.pure_callback`` function. [...] """""" ``` Note: edited for clarity and added an example docstring.","I'd also be very interested in helping with this. I've done a similar thing using `pure_callback` and `custom_jvp` but I haven't been able to get it to support both forward and reverse mode. If I define a `custom_jvp` for the `pure_callback`, forward mode stuff like `jacfwd` works fine. However, if I try to use `jacrev` I usually get an error  ```JaxStackTraceBeforeTransformation: ValueError: Pure callbacks do not support transpose. Please use `jax.custom_vjp` to use callbacks while taking gradients.``` Using `custom_vjp` allows reverse mode to work, but since a function can't have both `custom_jvp` and `custom_vjp` defined ( CC(Defining both custom_vjp and custom_jvp)), we're limited to one or the other.","  I added a new functionality, `define_fdjvp` here, which satisfies most of the specs you defined. One note, forward, central and backward are replaced by offsets, so for $`\sum A_i*f(x+a_i)/stepsize`$, you only provide `offsets=jnp.array([ai, ... ])` and $`A_i`$  will be calculated based on the other configs."
577,"以下是一个github上的jax下的一个issue, 标题是([4/n] Introduce Module into FunctionDefLibrary.)， 内容是 ([4/n] Introduce Module into FunctionDefLibrary. Retrieve the serialized StableHLO module from FunctionLibraryRuntime. If XlaCallModuleOp has attribute ""module"", it means the StableHLO module is serialized and embedded in the attribute. Otherwise, it must have attribute ""module_name"", which refers to a StableHLO module in the FunctionLibraryRuntime.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[4/n] Introduce Module into FunctionDefLibrary.,"[4/n] Introduce Module into FunctionDefLibrary. Retrieve the serialized StableHLO module from FunctionLibraryRuntime. If XlaCallModuleOp has attribute ""module"", it means the StableHLO module is serialized and embedded in the attribute. Otherwise, it must have attribute ""module_name"", which refers to a StableHLO module in the FunctionLibraryRuntime.",2023-04-05T23:50:01Z,,closed,0,1,https://github.com/jax-ml/jax/issues/15421,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
715,"以下是一个github上的jax下的一个issue, 标题是([3/n] Introduce Module into FunctionDefLibrary.)， 内容是 ([3/n] Introduce Module into FunctionDefLibrary. Add an option `native_serialization_as_module` to jax2tf to serialize the stablehlo explicitly to `FunctionDefLibrary`. In jax2tf native serialization, the stablehlo is embedded as a string attribute of the `XlaCallModuleOp`. When `native_serialization_as_module` is `True`, the stablehlo is serialized as a `ModuleDef` in `FunctionDefLibrary`, then `XlaCallModuleOp` refers to the `ModuleDef` using the module's name as a string attribute.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[3/n] Introduce Module into FunctionDefLibrary.,"[3/n] Introduce Module into FunctionDefLibrary. Add an option `native_serialization_as_module` to jax2tf to serialize the stablehlo explicitly to `FunctionDefLibrary`. In jax2tf native serialization, the stablehlo is embedded as a string attribute of the `XlaCallModuleOp`. When `native_serialization_as_module` is `True`, the stablehlo is serialized as a `ModuleDef` in `FunctionDefLibrary`, then `XlaCallModuleOp` refers to the `ModuleDef` using the module's name as a string attribute.",2023-04-05T23:43:12Z,,closed,0,1,https://github.com/jax-ml/jax/issues/15420,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
3117,"以下是一个github上的jax下的一个issue, 标题是(Taking gradients through jnp.arctan2 seems to be very unstable)， 内容是 ( Description I'm trying to optimise a model and I'm finding that the gradients are very very unstable  no learning rate lets me reliably move in the right direction to reduce the loss.  I have narrowed down the instability to the line where I call `jnp.arctan2`  to calculate angles. I have included below a minimum working example  which demonstrates the issue.  This may well be a misunderstanding on my behalf of the behaviour of `jnp.arctan2`. ``` import jax.numpy as jnp from jax import value_and_grad, random def f(x,mu):     p_x  = jnp.exp(5 * (jnp.cos(jnp.arange(x.shape[1])  mu)  1))     real = jnp.cos(x) @ p_x     imag = jnp.sin(x) @ p_x     return jnp.sum(jnp.arctan2(imag,real))  Change the commented return value to explore behaviout w/ and  w/o arctan2     return jnp.sum(imag + real) def opt_f(num_iter = 10,lr=1e3):     key = random.PRNGKey(0)     x   = (random.uniform(key,shape=(100,100)) * 2 * jnp.pi)  jnp.pi     mu  = 0.0     grad_f = value_and_grad(f,1)     i = 0     while i < num_iter:         f_val,f_grad_x = grad_f(x,mu)         mu = mu  lr * f_grad_x         print(f""iteration {i} value {f_val}"")         i += 1 if __name__ == ""__main__"":     opt_f() ``` If I run this code as is, then the first 10 iterations descend as expected (see below) ``` iteration 0 value 40.33513641357422 iteration 1 value 40.38768768310547 iteration 2 value 40.43447494506836 iteration 3 value 40.47615051269531 iteration 4 value 40.51304626464844 iteration 5 value 40.545711517333984 iteration 6 value 40.5745849609375 iteration 7 value 40.60002899169922 iteration 8 value 40.62242889404297 iteration 9 value 40.64210510253906 ``` But if I change the return value to include the conversion to angle, then the gradients are very unstable and the output fluctuates wildly ``` iteration 0 value 16.434661865234375 iteration 1 value 16.08127784729004     Down iteration 2 value 15.644673347473145 iteration 3 value 21.444284439086914 iteration 4 value 20.929534912109375 iteration 5 value 26.625213623046875 iteration 6 value 19.6180419921875 iteration 7 value 12.849037170410156 iteration 8 value 19.092792510986328 iteration 9 value 19.084476470947266 ``` Reducing the learning rate to `1e7` of course minimises the size of the fluctuations, but the directions still vary from iteration to iteration, see below ``` iteration 0 value 16.434661865234375 iteration 1 value 16.434650421142578 iteration 2 value 16.434680938720703 iteration 3 value 16.434490203857422 iteration 4 value 16.43451690673828 iteration 5 value 16.434511184692383 iteration 6 value 16.434545516967773 iteration 7 value 16.43435287475586 iteration 8 value 16.43438720703125 iteration 9 value 16.434375762939453 ```  What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8, Ubuntu 20.04.5 LTS in WSL2  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Taking gradients through jnp.arctan2 seems to be very unstable," Description I'm trying to optimise a model and I'm finding that the gradients are very very unstable  no learning rate lets me reliably move in the right direction to reduce the loss.  I have narrowed down the instability to the line where I call `jnp.arctan2`  to calculate angles. I have included below a minimum working example  which demonstrates the issue.  This may well be a misunderstanding on my behalf of the behaviour of `jnp.arctan2`. ``` import jax.numpy as jnp from jax import value_and_grad, random def f(x,mu):     p_x  = jnp.exp(5 * (jnp.cos(jnp.arange(x.shape[1])  mu)  1))     real = jnp.cos(x) @ p_x     imag = jnp.sin(x) @ p_x     return jnp.sum(jnp.arctan2(imag,real))  Change the commented return value to explore behaviout w/ and  w/o arctan2     return jnp.sum(imag + real) def opt_f(num_iter = 10,lr=1e3):     key = random.PRNGKey(0)     x   = (random.uniform(key,shape=(100,100)) * 2 * jnp.pi)  jnp.pi     mu  = 0.0     grad_f = value_and_grad(f,1)     i = 0     while i < num_iter:         f_val,f_grad_x = grad_f(x,mu)         mu = mu  lr * f_grad_x         print(f""iteration {i} value {f_val}"")         i += 1 if __name__ == ""__main__"":     opt_f() ``` If I run this code as is, then the first 10 iterations descend as expected (see below) ``` iteration 0 value 40.33513641357422 iteration 1 value 40.38768768310547 iteration 2 value 40.43447494506836 iteration 3 value 40.47615051269531 iteration 4 value 40.51304626464844 iteration 5 value 40.545711517333984 iteration 6 value 40.5745849609375 iteration 7 value 40.60002899169922 iteration 8 value 40.62242889404297 iteration 9 value 40.64210510253906 ``` But if I change the return value to include the conversion to angle, then the gradients are very unstable and the output fluctuates wildly ``` iteration 0 value 16.434661865234375 iteration 1 value 16.08127784729004     Down iteration 2 value 15.644673347473145 iteration 3 value 21.444284439086914 iteration 4 value 20.929534912109375 iteration 5 value 26.625213623046875 iteration 6 value 19.6180419921875 iteration 7 value 12.849037170410156 iteration 8 value 19.092792510986328 iteration 9 value 19.084476470947266 ``` Reducing the learning rate to `1e7` of course minimises the size of the fluctuations, but the directions still vary from iteration to iteration, see below ``` iteration 0 value 16.434661865234375 iteration 1 value 16.434650421142578 iteration 2 value 16.434680938720703 iteration 3 value 16.434490203857422 iteration 4 value 16.43451690673828 iteration 5 value 16.434511184692383 iteration 6 value 16.434545516967773 iteration 7 value 16.43435287475586 iteration 8 value 16.43438720703125 iteration 9 value 16.434375762939453 ```  What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8, Ubuntu 20.04.5 LTS in WSL2  NVIDIA GPU info _No response_",2023-04-05T11:35:55Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15407,"With some further investigation on this minimal example, it seems that there is a sweet spot of learning rate that I had initially missed where the optimisation is stable, so I'll close this issue.  Not sure how to find that sweet spot for the full model but thats the next problem!"
2066,"以下是一个github上的jax下的一个issue, 标题是([shard-map] debugging shmap + prngkey bugs)， 内容是 (fixes CC(jax.random sampling doesn't work inside shard_map) There are a few changes here, since we ran into some things that we wanted to clean up: 1. the `shard_map`related change was that anything lowering from jaxprlevel logical avals their corresponding shardings to HLOlevel (""physical"") types and corresponding HLO OpSharding protos needs to be sure to actually handle the logicaltophysical translation on the shardings, but shmap wasn't doing that, so the changes to `_xla_shard` and `_xla_unshard` handle that; 2. we also didn't have a rep rule for some PRNGrelated primitives, so the `prng.__dict__.values()` change handles that; 3. `PRNGKeyArray` was delegating its `sharding` attribute to the underlying `_base_array.sharding`, but the latter has an incompatible rank (i.e. length of `tile_assignment_dimensions`) because the latter is the sharding for the physical value and doesn't correspond to the logical value, so we instead need to either store the logical sharding (and keep it consistent) or compute it as a function of the physical sharding (which is what this PR does); 4. since we tend to carry lists of avals and corresponding shardings together throughout the lowering code, it makes sense to maintain the invariant that they're either all logical or all physical, but before this PR we were translating logical shardings to physical shardings in `pxla. lower_sharding_computation` (keeping the logical avals) and then only later in the callee `mlir.lower_jaxpr_to_module` and again in _its_ callee `mlir.lower_jaxpr_to_function` lowering the logical avals to physical avals, so in this PR we keep everything consistent, and only lower to physical avals and shardings in `mlir.lower_jaxpr_to_function` when we really need them. Those are some long sentences! Not counting tests, this PR also was a net reduction in LoC!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard-map] debugging shmap + prngkey bugs,"fixes CC(jax.random sampling doesn't work inside shard_map) There are a few changes here, since we ran into some things that we wanted to clean up: 1. the `shard_map`related change was that anything lowering from jaxprlevel logical avals their corresponding shardings to HLOlevel (""physical"") types and corresponding HLO OpSharding protos needs to be sure to actually handle the logicaltophysical translation on the shardings, but shmap wasn't doing that, so the changes to `_xla_shard` and `_xla_unshard` handle that; 2. we also didn't have a rep rule for some PRNGrelated primitives, so the `prng.__dict__.values()` change handles that; 3. `PRNGKeyArray` was delegating its `sharding` attribute to the underlying `_base_array.sharding`, but the latter has an incompatible rank (i.e. length of `tile_assignment_dimensions`) because the latter is the sharding for the physical value and doesn't correspond to the logical value, so we instead need to either store the logical sharding (and keep it consistent) or compute it as a function of the physical sharding (which is what this PR does); 4. since we tend to carry lists of avals and corresponding shardings together throughout the lowering code, it makes sense to maintain the invariant that they're either all logical or all physical, but before this PR we were translating logical shardings to physical shardings in `pxla. lower_sharding_computation` (keeping the logical avals) and then only later in the callee `mlir.lower_jaxpr_to_module` and again in _its_ callee `mlir.lower_jaxpr_to_function` lowering the logical avals to physical avals, so in this PR we keep everything consistent, and only lower to physical avals and shardings in `mlir.lower_jaxpr_to_function` when we really need them. Those are some long sentences! Not counting tests, this PR also was a net reduction in LoC!",2023-04-05T03:26:01Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15399
6727,"以下是一个github上的jax下的一个issue, 标题是(jax.random sampling doesn't work inside shard_map)， 内容是 ( Description I'm trying to sample using a PRNGKey locally on each device, inside a `shard_map`. The following code raises an error.  ```python import jax import jax.numpy as jnp import numpy as np from jax.experimental.shard_map import shard_map assert jax.device_count() == 4 mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4,), ('x',)) sharding = jax.sharding.NamedSharding(mesh, P('x')) rng = jax.random.PRNGKey(0) sharded_rng = jax.random.split(rng, num=4) sharded_rng = jax.device_put(sharded_rng, sharding) def f(key):   return jax.random.randint(key[0], shape=(1, 16), minval=0, maxval=16, dtype=jnp.int32) outputs = shard_map(f, mesh, in_specs=(P('x', None),), out_specs=P('x', None))(sharded_rng) print(outputs.device_buffers) ``` ```python  UnfilteredStackTrace                      Traceback (most recent call last) [](https://colab.corp.google.com/drive/1PUbuHPgfw_YRQiETSlDQmE9PS85pQYQr) in ()      12  > 13 outputs = shard_map(f, mesh, in_specs=(P('x', None),), out_specs=P('x', None))(sharded_rng)      14 print(outputs.device_buffers) 27 frames UnfilteredStackTrace: google3.third_party.tensorflow.compiler.xla.python.xla_extension.XlaRuntimeError: UNKNOWN: :0: error: Number of tile assignment dimensions (excluding subgroups) is different than the input rank. sharding={devices=[4]0,1,2,3}, input_shape=u32[4,2] 	Note: While validating sharding {devices=[4]0,1,2,3} against shape u32[4,2]:     @     0x5563712ac5aa  xla::XlaBuilder::CustomCall()     @     0x5563712bfc85  xla::CustomCall()     @     0x55636b9689ff  ExportXlaOperator()     @     0x55637f37512d  mlir::(anonymous namespace)::ExportXlaOperatorWrapped()     @     0x55636b957009  mlir::(anonymous namespace)::ConvertToHloModule::Lower()     @     0x55636b95a214  mlir::(anonymous namespace)::ConvertToHloModule::LowerBasicBlockAsFunction()     @     0x55636b95b3be  mlir::(anonymous namespace)::ConvertToHloModule::RunOnFunction()     @     0x55636b955b41  mlir::ConvertMlirHloToHlo()     @     0x5563486afabd  xla::MlirToXlaComputation()     @     0x55635f0b66f8  xla::TfrtCpuClient::Compile()     @     0x5563612d1897  xla::ifrt::PjRtLoadedExecutable::Create()     @     0x5563612d065c  xla::ifrt::PjRtCompiler::Compile()     @     0x55635f07ba0e  xla::PyClient::Compile()     @     0x7f0f509fdd4c  pybind11::cpp_function::cpp_function()::{lambda() CC(Python 3 compatibility issues)}::operator()()     @     0x7f0f509fda72  pybind11::detail::argument_loader::call_impl()     @     0x7f0f509fcecb  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::operator()()     @     0x7f0f509fce14  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::__invoke()     @     0x7f0f509bfbaf  pybind11::cpp_function::dispatcher()     @     0x55633f9f6b7c  cfunction_call     @     0x55633fa03703  _PyObject_MakeTpCall     @     0x556346f864c8  method_vectorcall     @     0x55633fa00d82  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa00f8c  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa00545  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633f9ff5df  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa06c16  PyObject_Call     @     0x55633fa00f8c  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector :0: note: see current operation: ""func.return""(%3) : (tensor) > () The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: XlaRuntimeError                           Traceback (most recent call last) google3/third_party/py/jax/_src/prng.py in random_wrap(base_arr, impl)     742 def random_wrap(base_arr, *, impl):     743   _check_prng_key_data(impl, base_arr) > 744   return random_wrap_p.bind(base_arr, impl=impl)     745      746 random_wrap_p = core.Primitive('random_wrap') XlaRuntimeError: UNKNOWN: :0: error: Number of tile assignment dimensions (excluding subgroups) is different than the input rank. sharding={devices=[4]0,1,2,3}, input_shape=u32[4,2] 	Note: While validating sharding {devices=[4]0,1,2,3} against shape u32[4,2]:     @     0x5563712ac5aa  xla::XlaBuilder::CustomCall()     @     0x5563712bfc85  xla::CustomCall()     @     0x55636b9689ff  ExportXlaOperator()     @     0x55637f37512d  mlir::(anonymous namespace)::ExportXlaOperatorWrapped()     @     0x55636b957009  mlir::(anonymous namespace)::ConvertToHloModule::Lower()     @     0x55636b95a214  mlir::(anonymous namespace)::ConvertToHloModule::LowerBasicBlockAsFunction()     @     0x55636b95b3be  mlir::(anonymous namespace)::ConvertToHloModule::RunOnFunction()     @     0x55636b955b41  mlir::ConvertMlirHloToHlo()     @     0x5563486afabd  xla::MlirToXlaComputation()     @     0x55635f0b66f8  xla::TfrtCpuClient::Compile()     @     0x5563612d1897  xla::ifrt::PjRtLoadedExecutable::Create()     @     0x5563612d065c  xla::ifrt::PjRtCompiler::Compile()     @     0x55635f07ba0e  xla::PyClient::Compile()     @     0x7f0f509fdd4c  pybind11::cpp_function::cpp_function()::{lambda() CC(Python 3 compatibility issues)}::operator()()     @     0x7f0f509fda72  pybind11::detail::argument_loader::call_impl()     @     0x7f0f509fcecb  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::operator()()     @     0x7f0f509fce14  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::__invoke()     @     0x7f0f509bfbaf  pybind11::cpp_function::dispatcher()     @     0x55633f9f6b7c  cfunction_call     @     0x55633fa03703  _PyObject_MakeTpCall     @     0x556346f864c8  method_vectorcall     @     0x55633fa00d82  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa00f8c  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa00545  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633f9ff5df  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa06c16  PyObject_Call     @     0x55633fa00f8c  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector :0: note: see current operation: ""func.return""(%3) : (tensor) > () ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU/TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.random sampling doesn't work inside shard_map," Description I'm trying to sample using a PRNGKey locally on each device, inside a `shard_map`. The following code raises an error.  ```python import jax import jax.numpy as jnp import numpy as np from jax.experimental.shard_map import shard_map assert jax.device_count() == 4 mesh = jax.sharding.Mesh(np.array(jax.devices()).reshape(4,), ('x',)) sharding = jax.sharding.NamedSharding(mesh, P('x')) rng = jax.random.PRNGKey(0) sharded_rng = jax.random.split(rng, num=4) sharded_rng = jax.device_put(sharded_rng, sharding) def f(key):   return jax.random.randint(key[0], shape=(1, 16), minval=0, maxval=16, dtype=jnp.int32) outputs = shard_map(f, mesh, in_specs=(P('x', None),), out_specs=P('x', None))(sharded_rng) print(outputs.device_buffers) ``` ```python  UnfilteredStackTrace                      Traceback (most recent call last) [](https://colab.corp.google.com/drive/1PUbuHPgfw_YRQiETSlDQmE9PS85pQYQr) in ()      12  > 13 outputs = shard_map(f, mesh, in_specs=(P('x', None),), out_specs=P('x', None))(sharded_rng)      14 print(outputs.device_buffers) 27 frames UnfilteredStackTrace: google3.third_party.tensorflow.compiler.xla.python.xla_extension.XlaRuntimeError: UNKNOWN: :0: error: Number of tile assignment dimensions (excluding subgroups) is different than the input rank. sharding={devices=[4]0,1,2,3}, input_shape=u32[4,2] 	Note: While validating sharding {devices=[4]0,1,2,3} against shape u32[4,2]:     @     0x5563712ac5aa  xla::XlaBuilder::CustomCall()     @     0x5563712bfc85  xla::CustomCall()     @     0x55636b9689ff  ExportXlaOperator()     @     0x55637f37512d  mlir::(anonymous namespace)::ExportXlaOperatorWrapped()     @     0x55636b957009  mlir::(anonymous namespace)::ConvertToHloModule::Lower()     @     0x55636b95a214  mlir::(anonymous namespace)::ConvertToHloModule::LowerBasicBlockAsFunction()     @     0x55636b95b3be  mlir::(anonymous namespace)::ConvertToHloModule::RunOnFunction()     @     0x55636b955b41  mlir::ConvertMlirHloToHlo()     @     0x5563486afabd  xla::MlirToXlaComputation()     @     0x55635f0b66f8  xla::TfrtCpuClient::Compile()     @     0x5563612d1897  xla::ifrt::PjRtLoadedExecutable::Create()     @     0x5563612d065c  xla::ifrt::PjRtCompiler::Compile()     @     0x55635f07ba0e  xla::PyClient::Compile()     @     0x7f0f509fdd4c  pybind11::cpp_function::cpp_function()::{lambda() CC(Python 3 compatibility issues)}::operator()()     @     0x7f0f509fda72  pybind11::detail::argument_loader::call_impl()     @     0x7f0f509fcecb  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::operator()()     @     0x7f0f509fce14  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::__invoke()     @     0x7f0f509bfbaf  pybind11::cpp_function::dispatcher()     @     0x55633f9f6b7c  cfunction_call     @     0x55633fa03703  _PyObject_MakeTpCall     @     0x556346f864c8  method_vectorcall     @     0x55633fa00d82  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa00f8c  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa00545  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633f9ff5df  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa06c16  PyObject_Call     @     0x55633fa00f8c  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector :0: note: see current operation: ""func.return""(%3) : (tensor) > () The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: XlaRuntimeError                           Traceback (most recent call last) google3/third_party/py/jax/_src/prng.py in random_wrap(base_arr, impl)     742 def random_wrap(base_arr, *, impl):     743   _check_prng_key_data(impl, base_arr) > 744   return random_wrap_p.bind(base_arr, impl=impl)     745      746 random_wrap_p = core.Primitive('random_wrap') XlaRuntimeError: UNKNOWN: :0: error: Number of tile assignment dimensions (excluding subgroups) is different than the input rank. sharding={devices=[4]0,1,2,3}, input_shape=u32[4,2] 	Note: While validating sharding {devices=[4]0,1,2,3} against shape u32[4,2]:     @     0x5563712ac5aa  xla::XlaBuilder::CustomCall()     @     0x5563712bfc85  xla::CustomCall()     @     0x55636b9689ff  ExportXlaOperator()     @     0x55637f37512d  mlir::(anonymous namespace)::ExportXlaOperatorWrapped()     @     0x55636b957009  mlir::(anonymous namespace)::ConvertToHloModule::Lower()     @     0x55636b95a214  mlir::(anonymous namespace)::ConvertToHloModule::LowerBasicBlockAsFunction()     @     0x55636b95b3be  mlir::(anonymous namespace)::ConvertToHloModule::RunOnFunction()     @     0x55636b955b41  mlir::ConvertMlirHloToHlo()     @     0x5563486afabd  xla::MlirToXlaComputation()     @     0x55635f0b66f8  xla::TfrtCpuClient::Compile()     @     0x5563612d1897  xla::ifrt::PjRtLoadedExecutable::Create()     @     0x5563612d065c  xla::ifrt::PjRtCompiler::Compile()     @     0x55635f07ba0e  xla::PyClient::Compile()     @     0x7f0f509fdd4c  pybind11::cpp_function::cpp_function()::{lambda() CC(Python 3 compatibility issues)}::operator()()     @     0x7f0f509fda72  pybind11::detail::argument_loader::call_impl()     @     0x7f0f509fcecb  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::operator()()     @     0x7f0f509fce14  pybind11::cpp_function::initialize()::{lambda() CC(Python 3 compatibility issues)}::__invoke()     @     0x7f0f509bfbaf  pybind11::cpp_function::dispatcher()     @     0x55633f9f6b7c  cfunction_call     @     0x55633fa03703  _PyObject_MakeTpCall     @     0x556346f864c8  method_vectorcall     @     0x55633fa00d82  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa00f8c  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa00545  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633f9ff5df  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector     @     0x55633fa06c16  PyObject_Call     @     0x55633fa00f8c  _PyEval_EvalFrameDefault     @     0x55633f9f70d8  _PyEval_Vector :0: note: see current operation: ""func.return""(%3) : (tensor) > () ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU/TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-05T00:56:43Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/15398,"(Also mentioned in chat) As a workaround, instead of using ""eager"" shmap you can apply a `jax.jit` to avoid the error, e.g. ```python outputs = jax.jit(shard_map(f, mesh, in_specs=(P('x', None),), out_specs=P('x', None)))(sharded_rng) ``` That runs into a different (trivial) issue which requires passing `check_rep=False` to `shard_map`. I'll fix both in a PR.","Simpler repro: replace `f` with `lambda key: prng.random_wrap(key, impl=random.default_prng_impl())` where we do `from jax._src import prng, random`.",This is great! Thank you  for the ultrafast turnaround
398,"以下是一个github上的jax下的一个issue, 标题是(Checkify: instrumentation context manager)， 内容是 (Trying it out on `gather`. notes:  if you don't discharge an instrumented check, should you get an error? probably, but how? add an effect?  `instrument` > `catch`/`catch_error`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Checkify: instrumentation context manager,"Trying it out on `gather`. notes:  if you don't discharge an instrumented check, should you get an error? probably, but how? add an effect?  `instrument` > `catch`/`catch_error`",2023-04-04T14:32:00Z,,open,0,0,https://github.com/jax-ml/jax/issues/15389
4475,"以下是一个github上的jax下的一个issue, 标题是(Cannot install JAX through a proxy server)， 内容是 ( Description Hi, I'm trying to install JAX on a cluster via a proxy server. The following CLI done is : ` python3 m pip install proxy=MYPROXY upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ` I do get the error :  ``` Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda11_local]   Using cached jax0.4.8.tar.gz (1.2 MB)   Installing build dependencies ... error   error: subprocessexitedwitherror   × pip subprocess to install build dependencies did not run successfully.   │ exit code: 1   ╰─> [13 lines of output]       Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       ERROR: Could not find a version that satisfies the requirement setuptools (from versions: none)       ERROR: No matching distribution found for setuptools       [end of output]   note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocessexitedwitherror × pip subprocess to install build dependencies did not run successfully. │ exit code: 1 ╰─> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. ``` When I try CPU version for JAX  `python3 m pip install proxy=MYPROXY upgrade ""jax[cpu]""`  I get the same problem.  I have tried installing other packages on the cluster and had no issue at all. I have installed JAX on my local machine and it worked fine. Both `setuptools` are the same on my cluster and local machine   `setuptools==67.6.7` I would imagine that some build dependencies need an internet connection and does not take into account the argument `proxy` in the CLI for the install.  What jax/jaxlib version are you using? jax==0.4.8  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.9.16, Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot install JAX through a proxy server," Description Hi, I'm trying to install JAX on a cluster via a proxy server. The following CLI done is : ` python3 m pip install proxy=MYPROXY upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ` I do get the error :  ``` Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda11_local]   Using cached jax0.4.8.tar.gz (1.2 MB)   Installing build dependencies ... error   error: subprocessexitedwitherror   × pip subprocess to install build dependencies did not run successfully.   │ exit code: 1   ╰─> [13 lines of output]       Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /jaxreleases/jax_cuda_releases.html       WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(, 'Connection to 134.158.74.101 timed out. (connect timeout=15)')': /simple/setuptools/       ERROR: Could not find a version that satisfies the requirement setuptools (from versions: none)       ERROR: No matching distribution found for setuptools       [end of output]   note: This error originates from a subprocess, and is likely not a problem with pip. error: subprocessexitedwitherror × pip subprocess to install build dependencies did not run successfully. │ exit code: 1 ╰─> See above for output. note: This error originates from a subprocess, and is likely not a problem with pip. ``` When I try CPU version for JAX  `python3 m pip install proxy=MYPROXY upgrade ""jax[cpu]""`  I get the same problem.  I have tried installing other packages on the cluster and had no issue at all. I have installed JAX on my local machine and it worked fine. Both `setuptools` are the same on my cluster and local machine   `setuptools==67.6.7` I would imagine that some build dependencies need an internet connection and does not take into account the argument `proxy` in the CLI for the install.  What jax/jaxlib version are you using? jax==0.4.8  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.9.16, Linux  NVIDIA GPU info _No response_",2023-04-04T11:27:11Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15387,"I think you're running into this issue: https://github.com/pypa/pip/issues/6018 It looks like a bug in pip, over which JAX has no control. There are conflicting reports there, but perhaps you could try updating to the latest pip version to see if it fixes the issue?","Thanks for the reply, my current pip is the latest release `pip 23.0.1`. I'll try to investigate further with the issue you mentionned !  "
5607,"以下是一个github上的jax下的一个issue, 标题是(jit fails for fft)， 内容是 ( Description As part of an effort of performing fft over a sharded array, I've stumbled upon a basic issue: it seems I'm unable to jit fft(). The following MRE: ``` import jax import jax.numpy as jnp def my_fft(a,b):     return jnp.fft.fft(a,b) a = jnp.arange(1000) b = jnp.array(1024)  nonjit is valid: _ = my_fft(a,b)  jit fails: _ = jax.jit(my_fft)(a,b) ``` will successfully execute the nonjit version, yet will fail when trying to jit.    Error message: ``` Traceback (most recent call last):   File ""/home/itayk/git/Sandbox/ItayK/Playground/jax/jitfftissue__Apr23/jitfftissue1.py"", line 11, in      print(jit_fft(a,b))   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 238, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 180, in _python_pjit_helper     args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/api.py"", line 311, in infer_params     return pjit.common_infer_params(pjit_info_args, *args, **kwargs)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 480, in common_infer_params     jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 918, in _pjit_jaxpr     jaxpr, final_consts, out_type = _create_pjit_jaxpr(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 322, in memoized_fun     ans = call(fun, *args)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 874, in _create_pjit_jaxpr     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2049, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2066, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 166, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/itayk/git/Sandbox/ItayK/Playground/jax/jitfftissue__Apr23/jitfftissue1.py"", line 5, in my_fft     return jnp.fft.fft(a,b)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 155, in fft     return _fft_core_1d('fft', xla_client.FftType.FFT, a, n=n, axis=axis,   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 149, in _fft_core_1d     return _fft_core(func_name, fft_type, a, s, axes, norm)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 50, in _fft_core     s = tuple(map(operator.index, s))   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/core.py"", line 583, in __index__     raise TracerIntegerConversionError(self) jax._src.traceback_util.UnfilteredStackTrace: jax._src.errors.TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object Tracedwith See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerIntegerConversionError The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/itayk/git/Sandbox/ItayK/Playground/jax/jitfftissue__Apr23/jitfftissue1.py"", line 11, in      print(jit_fft(a,b))   File ""/home/itayk/git/Sandbox/ItayK/Playground/jax/jitfftissue__Apr23/jitfftissue1.py"", line 5, in my_fft     return jnp.fft.fft(a,b)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 155, in fft     return _fft_core_1d('fft', xla_client.FftType.FFT, a, n=n, axis=axis,   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 149, in _fft_core_1d     return _fft_core(func_name, fft_type, a, s, axes, norm)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 50, in _fft_core     s = tuple(map(operator.index, s)) jax._src.errors.TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object Tracedwith See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerIntegerConversionError ```  Is there a fundamental issue with jit'ing such a complex function, or an alternative way to perform that? Thank you.  What jax/jaxlib version are you using? jax 0.4.8; jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.12, WSL Ubuntu 22.04  NVIDIA GPU info ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jit fails for fft," Description As part of an effort of performing fft over a sharded array, I've stumbled upon a basic issue: it seems I'm unable to jit fft(). The following MRE: ``` import jax import jax.numpy as jnp def my_fft(a,b):     return jnp.fft.fft(a,b) a = jnp.arange(1000) b = jnp.array(1024)  nonjit is valid: _ = my_fft(a,b)  jit fails: _ = jax.jit(my_fft)(a,b) ``` will successfully execute the nonjit version, yet will fail when trying to jit.    Error message: ``` Traceback (most recent call last):   File ""/home/itayk/git/Sandbox/ItayK/Playground/jax/jitfftissue__Apr23/jitfftissue1.py"", line 11, in      print(jit_fft(a,b))   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 238, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 180, in _python_pjit_helper     args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/api.py"", line 311, in infer_params     return pjit.common_infer_params(pjit_info_args, *args, **kwargs)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 480, in common_infer_params     jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 918, in _pjit_jaxpr     jaxpr, final_consts, out_type = _create_pjit_jaxpr(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 322, in memoized_fun     ans = call(fun, *args)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 874, in _create_pjit_jaxpr     jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2049, in trace_to_jaxpr_dynamic     jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/interpreters/partial_eval.py"", line 2066, in trace_to_subjaxpr_dynamic     ans = fun.call_wrapped(*in_tracers_)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/linear_util.py"", line 166, in call_wrapped     ans = self.f(*args, **dict(self.params, **kwargs))   File ""/home/itayk/git/Sandbox/ItayK/Playground/jax/jitfftissue__Apr23/jitfftissue1.py"", line 5, in my_fft     return jnp.fft.fft(a,b)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 155, in fft     return _fft_core_1d('fft', xla_client.FftType.FFT, a, n=n, axis=axis,   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 149, in _fft_core_1d     return _fft_core(func_name, fft_type, a, s, axes, norm)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 50, in _fft_core     s = tuple(map(operator.index, s))   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/core.py"", line 583, in __index__     raise TracerIntegerConversionError(self) jax._src.traceback_util.UnfilteredStackTrace: jax._src.errors.TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object Tracedwith See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerIntegerConversionError The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/home/itayk/git/Sandbox/ItayK/Playground/jax/jitfftissue__Apr23/jitfftissue1.py"", line 11, in      print(jit_fft(a,b))   File ""/home/itayk/git/Sandbox/ItayK/Playground/jax/jitfftissue__Apr23/jitfftissue1.py"", line 5, in my_fft     return jnp.fft.fft(a,b)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 155, in fft     return _fft_core_1d('fft', xla_client.FftType.FFT, a, n=n, axis=axis,   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 149, in _fft_core_1d     return _fft_core(func_name, fft_type, a, s, axes, norm)   File ""/home/itayk/miniconda3/envs/dev_2304/lib/python3.9/sitepackages/jax/_src/numpy/fft.py"", line 50, in _fft_core     s = tuple(map(operator.index, s)) jax._src.errors.TracerIntegerConversionError: The __index__() method was called on the JAX Tracer object Tracedwith See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.TracerIntegerConversionError ```  Is there a fundamental issue with jit'ing such a complex function, or an alternative way to perform that? Thank you.  What jax/jaxlib version are you using? jax 0.4.8; jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.12, WSL Ubuntu 22.04  NVIDIA GPU info ``` ++  ++++ ```",2023-04-04T10:28:16Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15386,"Thanks for the question! The JIT call is failing because the second argument controls the shape of the output, and in JAX output shapes must be static. If you want to JITcompile the FFT operation, one way to do so is to mark the second argument as static: ```python jax.jit(my_fft, static_argnums=1)(a, 1024) ``` You can read more about this in JAX Sharp Bits: Dynamic Shapes.",Got it. Thanks for the quick & informative response!
1111,"以下是一个github上的jax下的一个issue, 标题是(JAX profiler will crash on NV gpu)， 内容是 ( Description I installed the jax through ""pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html "", my OS is ubuntu 20.04, my cuda version is 12.1, cudnn version is 8.8.0, python version is 3.9,  the profiler will crash even on the simple  case. test case: ``` import jax with jax.profiler.trace(""/tmp/tensorboard""):   key = jax.random.PRNGKey(0)   x = jax.random.normal(key, (5000, 5000))   y = x @ x   y.block_until_ready() ~                          ``` log info ``` 20230404 13:20:26.999736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT double free or corruption (!prev) Aborted (core dumped) ```  What jax/jaxlib version are you using? jaxlib    0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info 3.9, ubuntu, cuda12.1, cudnn 8.8.0  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX profiler will crash on NV gpu," Description I installed the jax through ""pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html "", my OS is ubuntu 20.04, my cuda version is 12.1, cudnn version is 8.8.0, python version is 3.9,  the profiler will crash even on the simple  case. test case: ``` import jax with jax.profiler.trace(""/tmp/tensorboard""):   key = jax.random.PRNGKey(0)   x = jax.random.normal(key, (5000, 5000))   y = x @ x   y.block_until_ready() ~                          ``` log info ``` 20230404 13:20:26.999736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT double free or corruption (!prev) Aborted (core dumped) ```  What jax/jaxlib version are you using? jaxlib    0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info 3.9, ubuntu, cuda12.1, cudnn 8.8.0  NVIDIA GPU info ++  ++",2023-04-04T05:13:44Z,bug NVIDIA GPU,closed,3,7,https://github.com/jax-ml/jax/issues/15384,"Looks like there are at least two things going on here: a) on CUDA 11, we're missing a dependency on `cupti` in our `pip` dependencies, and the rpath necessary to find it b) on CUDA 12, I'm not sure, but I agree it crashes as you say.","b) looks to me like a bug in either cupti or the profiler support in XLA. gdb backtrace: ``` (gdb) bt CC(未找到相关数据)  __pthread_kill_implementation (no_tid=0, signo=6, threadid=140460199964672) at ./nptl/pthread_kill.c:44 CC(Python 3 compatibility issues)  __pthread_kill_internal (signo=6, threadid=140460199964672) at ./nptl/pthread_kill.c:78 CC(Explicit tuples are not valid function parameters in Python 3)  __GI___pthread_kill (threadid=140460199964672, signo=signo=6) at ./nptl/pthread_kill.c:89 CC(Undefined name: from ..core import JaxTuple)  0x00007fbf70572476 in __GI_raise (sig=sig=6) at ../sysdeps/posix/raise.c:26 CC(Undefined name: from six.moves import xrange)  0x00007fbf705587f3 in __GI_abort () at ./stdlib/abort.c:79 CC(Building on OSX with CUDA)  0x00007fbf705b96f6 in __libc_message (action=action=do_abort, fmt=fmt=0x7fbf7070bb8c ""%s\n"") at ../sysdeps/posix/libc_fatal.c:155 CC(Made a shim to handle configuration without having absl parse flags)  0x00007fbf705d0d7c in malloc_printerr (str=str=0x7fbf7070e7d0 ""double free or corruption (!prev)"") at ./malloc/malloc.c:5664 CC(Quickish check)  0x00007fbf705d2efc in _int_free (av=0x7fbf70749c80 , p=0x557aa7d043a0, have_lock=) at ./malloc/malloc.c:4591 CC(Quickish check)  0x00007fbf705d54d3 in __GI___libc_free (mem=) at ./malloc/malloc.c:3391 CC(Adding quickstart notebook, and corresponding gitignore rules)  0x00007fbe4670b77a in ?? () from /home/phawkins/.local/lib/python3.10/sitepackages/nvidia/cuda_cupti/lib/libcupti.so.12 CC([JAX] Change semantics of dtype promotion to just call numpy.result_type.) 0x00007fbe466f4a1a in cuptiFinalize () from /home/phawkins/.local/lib/python3.10/sitepackages/nvidia/cuda_cupti/lib/libcupti.so.12 CC(Split out `jax` and `jaxlib` packages) 0x00007fbf5344d9e6 in xla::profiler::CuptiErrorManager::Finalize() () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Update the quickstart notebook.) 0x00007fbf53438e3b in xla::profiler::CuptiTracer::Finalize() () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Fixing logo size so resize is not required) 0x00007fbf5343afbf in xla::profiler::CuptiTracer::Disable() () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Add copyright notice to quickstart notebook.) 0x00007fbf5342b8d0 in xla::profiler::GpuTracer::Stop() () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(rename in_bdims, out_bdims > in_axes, out_axes) 0x00007fbf534605cc in tsl::profiler::ProfilerController::Stop() () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Add wheelbuilding scripts) 0x00007fbf5345f87c in tsl::profiler::ProfilerCollection::Stop() () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Implement np.repeat for scalar repeats.) 0x00007fbf5345f016 in tsl::ProfilerSession::CollectDataInternal(tensorflow::profiler::XSpace*) () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Populate readme) 0x00007fbf5345f134 in tsl::ProfilerSession::CollectData(tensorflow::profiler::XSpace*) () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Notebook showing how to write gufuncs with vmap) 0x00007fbf5342874a in pybind11::cpp_function::initialize, std::allocator > const&) CC(Undefined name: from six.moves import xrange)}, tsl::Status, tsl::ProfilerSession*, std::__cxx11::basic_string, std::allocator > const&, pybind11::name, pybind11::is_method, pybind11::sibling>(xla::BuildProfilerSubmodule(pybind11::module_*)::{lambda(tsl::ProfilerSession*, std::__cxx11::basic_string, std::allocator > const&) CC(Undefined name: from six.moves import xrange)}&&, tsl::Status (*)(tsl::ProfilerSession*, std::__cxx11::basic_string, std::allocator > const&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&) CC(Undefined name: from ..core import JaxTuple)}::_FUN(pybind11::detail::function_call&) () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so CC(Fix link in gufuncs notebook) 0x00007fbf531fb531 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) () from /home/phawkins/.local/lib/python3.10/sitepackages/jaxlib/xla_extension.so ```","is there any workaround, cuda 11 is not working , it will crash with cudnn intilization failed, seem currently it requires cudnn 8.8",Having the same problem; I am using oficial container nvcr.io/nvdlfwea/jax/jax:23.03py3,"Same problem here, jax0.4.8 or 0.4.7 with the GPU, tried cuda 11 or 12.  I get the ""double free or corruption (!prev)"" before the crash, but I also get the ""TFTRT Warning: Could not find TensorRT"" both times even after trying to install tensorrt (but I may have done that incorrectly). Very frustrating as we're trying to find out why many of our routines are taking so very long with the GPU enabled.","This issue is only about a CUDA 12 bug. The workaround is to downgrade to CUDA 11. If you are having problems on CUDA 11, please share details on how to reproduce.","I'm not sure which change fixed it, but I can't reproduce this with JAX 0.4.20 under CUDA 12. I think we can declare this fixed."
5770,"以下是一个github上的jax下的一个issue, 标题是(jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed)， 内容是 ( Description I have a python virtual environment with a clean installation of JAX ```  Installs the wheel compatible with CUDA 12 and cuDNN 8.8 or newer.  Note: wheels only available on linux. pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```  When I run my scripts, they work perfectly, but sometimes I get the following error with a success rate of between 2 and 10 successful executions and between 1 and 3 failed executions  ``` 20230402 16:00:19.964652: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 20230402 16:00:19.964737: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 530.30.2 Traceback (most recent call last):   File ""ddpg_jax_gymnasium_pendulum.py"", line 73, in      key = jax.random.PRNGKey(0)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/random.py"", line 136, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 270, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 561, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 573, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 578, in random_seed_impl_base     return seed(seeds)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 813, in threefry_seed     lax.shift_right_logical(seed, lax_internal._const(seed, 32)))   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/lax/lax.py"", line 458, in shift_right_logical     return shift_right_logical_p.bind(x, y)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 117, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 208, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 254, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2816, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 3028, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ```  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Ubuntu 20.04  NVIDIA GPU info ``` ++  ++ ``` CUDNN version (`/usr/local/cuda/include/cudnn_version.h`) ```c define CUDNN_MAJOR 8 define CUDNN_MINOR 8 define CUDNN_PATCHLEVEL 1 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed," Description I have a python virtual environment with a clean installation of JAX ```  Installs the wheel compatible with CUDA 12 and cuDNN 8.8 or newer.  Note: wheels only available on linux. pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```  When I run my scripts, they work perfectly, but sometimes I get the following error with a success rate of between 2 and 10 successful executions and between 1 and 3 failed executions  ``` 20230402 16:00:19.964652: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 20230402 16:00:19.964737: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 530.30.2 Traceback (most recent call last):   File ""ddpg_jax_gymnasium_pendulum.py"", line 73, in      key = jax.random.PRNGKey(0)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/random.py"", line 136, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 270, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 561, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 573, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 578, in random_seed_impl_base     return seed(seeds)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/prng.py"", line 813, in threefry_seed     lax.shift_right_logical(seed, lax_internal._const(seed, 32)))   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/lax/lax.py"", line 458, in shift_right_logical     return shift_right_logical_p.bind(x, y)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 360, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 363, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/core.py"", line 817, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 117, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 208, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 254, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 2816, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/interpreters/pxla.py"", line 3028, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/toni/Documents/SKRL/envs/env_jax/lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ```  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Ubuntu 20.04  NVIDIA GPU info ``` ++  ++ ``` CUDNN version (`/usr/local/cuda/include/cudnn_version.h`) ```c define CUDNN_MAJOR 8 define CUDNN_MINOR 8 define CUDNN_PATCHLEVEL 1 ```",2023-04-02T14:16:53Z,bug NVIDIA GPU,closed,0,35,https://github.com/jax-ml/jax/issues/15361,What is your OS? Can you confirm you run the scripts sequentially and so there is nothing that is using the GPU in parallel?,"Hi   The OS is Ubuntu 20.04, as indicated above. Btw, I think the problem may be VS Code. After running the script several times to try to get the error to appear, I see that the error only appears (not always but) when I make a modification to the script and save it. There is also the following log. As you can see (by running the `nvidiasmi` command just before executing the script, and after saving it) there is a GPU consumption. The strange thing is that the consumption comes from the python environment  (`env_gym`) configured in the VS Code bottom right pane and not from the python of the sourced environment where jax is installed (`env_jax`) 🤔 ```log (env_jax) toniZBookStudioG8:~/Documents/SKRL/skrl/docs/source/examples/gymnasium$ nvidiasmi; python ddpg_jax_gymnasium_pendulum.py  Mon Apr  3 20:41:40 2023        ++  ++ [skrl:INFO] Environment class: gymnasium.core.Wrapper, gymnasium.utils.record_constructor.RecordConstructorArgs [skrl:INFO] Environment wrapper: Gymnasium 20230403 20:41:43.310989: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 20230403 20:41:43.311060: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 530.30.2 Traceback (most recent call last): ... ```",Thanks for the results. I think we need a way to give a better error to end users.,"Recently a few error message got a little bit better. Closing as I'm not sure what do to more. But if the issue appear again and the error isn't good enough, poke us again.",I also got this error and it was due to GPU reaching its memory limit, Do you have the full error message you had? I would like to improve the error message in that case.,"same error ```bash  XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in ()       3        4  Initialize model weights using dummy tensors. > 5 rng = jax.random.PRNGKey(0)       6 rng, key = jax.random.split(rng)       7 init_img = jnp.ones((4, 224, 224, 5), jnp.float32) 22 frames /usr/local/lib/python3.10/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options, host_callbacks)     469    TODO(sharadmv): remove this fallback when all backends allow `compile`     470    to take in `host_callbacks` > 471   return backend.compile(built_c, compile_options=options)     472      473 _ir_dump_counter = itertools.count() XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ``` I think there also the memory reaching the limit. ```bash Fri May 12 01:50:33 2023        ++  ++ ```",Yep I received the same error message as 24 ,Thanks for the extended error message. But can you share the full output without any truncation? There should be information that should help me. Which JAX version did you use?,"I'm having the same problem but for me it's consistent and I'm unable to run simple Jax code. I only have this problem on my newest system with 4x RTX 4090 GPUs. I have a server A100 and a PC with a 3090ti that work smoothly. Ubuntu 22 across all systems. First installed CUDA 11 from condaforge as suggested, same issue. Then switched to loca installation of CUDA and cudnn. Same problem. After a fresh installation of everything when I run `a = jnp.ones((3,))` I get this error: 20230513 09:04:27.790057: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20230513 09:04:27.790140: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 5853872128 bytes free, 25393692672 bytes total. Traceback (most recent call last):   File """", line 1, in    File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2122, in ones     return lax.full(shape, 1, _jnp_dtype(dtype))            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/lax/lax.py"", line 1203, in full     return broadcast(fill_value, shape)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/lax/lax.py"", line 768, in broadcast     return broadcast_in_dim(operand, tuple(sizes) + np.shape(operand), dims)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/lax/lax.py"", line 796, in broadcast_in_dim     return broadcast_in_dim_p.bind(            ^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 131, in apply_primitive     compiled_fun = xla_primitive_callable(                    ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/util.py"", line 284, in wrapper     return cached(config._trace_context(), *args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/util.py"", line 277, in cached     return f(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 222, in xla_primitive_callable     compiled = _xla_callable_uncached(                ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 252, in _xla_callable_uncached     return computation.compile().unsafe_call            ^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2313, in compile     executable = UnloadedMeshExecutable.from_hlo(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2633, in from_hlo     xla_executable, compile_options = _cached_compilation(                                       ^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 495, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 463, in backend_compile     return backend.compile(built_c, compile_options=options)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",I've also tried to swap my PCs 3090ti (which works properly) with one of the 4090s and I got the exact same error. This used to work in the past. I'm pretty sure the GPU hardware is not the problem and they are functional (tested them on Windows machines).,"Did you cut the output? The error tell to look above for more errors. If there is more outputs, give me all what you have. I'll filter what is useful or not.","I'm getting the same kind of error trying to install jax/jaxlib on an EC2 p2.xlarge (with k80s), to provide solidarity! I can provide more details if useful, but basically running some vanilla installation script of Anaconda and trying different variants of `pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` leads Jax to report seeing the GPU when I check `print(xla_bridge.get_backend().platform)` but gives the DNN error above, otherwise.","(I'm also unable to _any_ Jax code, e.g. `a = jnp.ones((3,))`.)"," Please open new issues rather than appending onto closed ones. However, I think the problem in your case is simple: JAX no longer supports Kepler GPUs in the wheels we release. You can probably rebuild jaxlib from source if you need Kepler support, but note NVIDIA has dropped Kepler support from CUDA 12 and CUDNN 8.9, so this may not remain true for long.", no this is all the output. It's several lines of error are you seeing all of it? I managed to resolve this though. I installed CUDA 11 and cudnn 8.6. In my experiments I also installed the latest version of everything but this was the only version combination that worked for me. Now I'm getting other errors but that's a different problem., Point taken and... Thanks for the help! I just switched over to V100s and _voila_!,"I got the same error, maybe it due to the mismatch between your cuda version and the installed jax. I use ubuntu 20.04 with cuda version as below: !image At start, I installed the newest jax as ```bash pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Then I got the error as reported. So I switched to ```bash pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` Everything works well!","Thanks , for a simple use case I was able to use JAX 0.4.13 and CUDA 11.8 with CUDNN 8.6. I needed to add `/usr/lib/x86_64linuxgnu` to the `LD_LIBRARY_PATH` (installed `libcudnn8` with `aptget`).","I also met this error: The output: 20230731 01:53:45.016563: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:427] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.  XlaRuntimeError                           Traceback (most recent call last) Cell In[4], line 29      26 model = trainer.make_model(nmask)      28 lr_fn, opt = trainer.make_optimizer(steps_per_epoch=len(train_dl)) > 29 state = trainer.create_train_state(jax.random.PRNGKey(0), model, opt)      30 state = checkpoints.restore_checkpoint(ckpt.parent, state) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/random.py:137, in PRNGKey(seed)     134 if np.ndim(seed):     135   raise TypeError(""PRNGKey accepts a scalar seed, but was given an array of""     136                   f""shape {np.shape(seed)} != (). Use jax.vmap for batching"") > 137 key = prng.seed_with_impl(impl, seed)     138 return _return_prng_keys(True, key) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:320, in seed_with_impl(impl, seed)     319 def seed_with_impl(impl: PRNGImpl, seed: Union[int, Array]) > PRNGKeyArrayImpl: > 320   return random_seed(seed, impl=impl) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:734, in random_seed(seeds, impl)     732 else:     733   seeds_arr = jnp.asarray(seeds) > 734 return random_seed_p.bind(seeds_arr, impl=impl) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:380, in Primitive.bind(self, *args, **params)     377 def bind(self, *args, **params):     378   assert (not config.jax_enable_checks or     379           all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 380   return self.bind_with_trace(find_top_trace(args), args, params) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:383, in Primitive.bind_with_trace(self, trace, args, params)     382 def bind_with_trace(self, trace, args, params): > 383   out = trace.process_primitive(self, map(trace.full_raise, args), params)     384   return map(full_lower, out) if self.multiple_results else full_lower(out) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:790, in EvalTrace.process_primitive(self, primitive, tracers, params)     789 def process_primitive(self, primitive, tracers, params): > 790   return primitive.impl(*tracers, **params) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:746, in random_seed_impl(seeds, impl)     744 .def_impl     745 def random_seed_impl(seeds, *, impl): > 746   base_arr = random_seed_impl_base(seeds, impl=impl)     747   return PRNGKeyArrayImpl(impl, base_arr) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:751, in random_seed_impl_base(seeds, impl)     749 def random_seed_impl_base(seeds, *, impl):     750   seed = iterated_vmap_unary(seeds.ndim, impl.seed) > 751   return seed(seeds) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:980, in threefry_seed(seed)     968 def threefry_seed(seed: typing.Array) > typing.Array:     969   """"""Create a single raw threefry PRNG key from an integer seed.     970      971   Args:    (...)     978     first padding out with zeros).     979   """""" > 980   return _threefry_seed(seed)     [... skipping hidden 12 frame] File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/dispatch.py:463, in backend_compile(backend, module, options, host_callbacks)     458   return backend.compile(built_c, compile_options=options,     459                          host_callbacks=host_callbacks)     460  Some backends don't have `host_callbacks` option yet     461  TODO(sharadmv): remove this fallback when all backends allow `compile`     462  to take in `host_callbacks` > 463 return backend.compile(built_c, compile_options=options) XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.  What jax/jaxlib version are you using? Jax0.4.10, jaxlib0.4.10+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.11.4, Ubuntu 22.04  NVIDIA GPU info ++  ++",'s problem was resolved in https://github.com/google/jax/issues/16901. ,"hi, I have similar issue. please help me! the output: 20230905 14:32:56.559501: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20230905 14:32:56.559528: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 6081413120 bytes free, 25438126080 bytes total. Traceback (most recent call last):   File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 453, in      app.run(main)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 428, in main     predict_structure(   File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 214, in predict_structure     prediction_result = model_runner.predict(processed_feature_dict,   File ""/home/wangyun/pre/alphafoldmultimermain/alphafold/model/model.py"", line 167, in predict     result = self.apply(self.params, jax.random.PRNGKey(random_seed), feat)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/random.py"", line 137, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 320, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 732, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 744, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 749, in random_seed_impl_base     return seed(seeds)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 978, in threefry_seed     return _threefry_seed(seed)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 208, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 155, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 2633, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1085, in _pjit_call_impl     compiled = _pjit_lower(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2313, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2633, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 494, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 462, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. What jax/jaxlib version are you using? jax 0.4.9, jaxlib 0.4.9+cuda11.cudnn86 My conda virtual environment: python3.9.0 cudatoolkit               11.8.0              h4ba93d1_12    condaforge cudnn                     8.6.0.163            hed8a83a_0    cudistas But my OS environment:  NVIDIASMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2  ii  cudnnlocalrepoubuntu18048.9.3.28              1.01              amd64        cudnnlocal repository configuration files I try everything what I can , but ……","> What is your OS? Can you confirm you run the scripts sequentially and so there is nothing that is using the GPU in parallel? Hi, I encountered the same problem. When I use A100 to run a single task, it can run normally, but when I submit two tasks at the same time, the above error will be reported. So the reason is that A100 runs two tasks at the same time, will there be a conflict?","> Hi, I encountered the same problem. When I use A100 to run a single task, it can run normally, but when I submit two tasks at the same time, the above error will be reported. So the reason is that A100 runs two tasks at the same time, will there be a conflict? I suppose 2 tasks means 2 process. If not, tell us. By default, JAX will reserve 75% of the GPU memory for the process: https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html So the 2nd process will end up missing GPU memory most of the time. Read that web page to know how to control that 75% memory allocation. If you can lower it to 45% and the first process has enough memory, it will probably work. Otherwise, try a few other values.","> hi, I have similar issue. please help me! >  > the output: 20230905 14:32:56.559501: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20230905 14:32:56.559528: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 6081413120 bytes free, 25438126080 bytes total. Traceback (most recent call last): File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 453, in app.run(main) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/absl/app.py"", line 308, in run _run_main(main, args) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main sys.exit(main(argv)) File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 428, in main predict_structure( File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 214, in predict_structure prediction_result = model_runner.predict(processed_feature_dict, File ""/home/wangyun/pre/alphafoldmultimermain/alphafold/model/model.py"", line 167, in predict result = self.apply(self.params, jax.random.PRNGKey(random_seed), feat) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/random.py"", line 137, in PRNGKey key = prng.seed_with_impl(impl, seed) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 320, in seed_with_impl return random_seed(seed, impl=impl) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 732, in random_seed return random_seed_p.bind(seeds_arr, impl=impl) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 744, in random_seed_impl base_arr = random_seed_impl_base(seeds, impl=impl) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 749, in random_seed_impl_base return seed(seeds) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 978, in threefry_seed return _threefry_seed(seed) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback return fun(*args, **kwargs) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 208, in cache_miss outs, out_flat, out_tree, args_flat = _python_pjit_helper( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 155, in _python_pjit_helper out_flat = pjit_p.bind(*args_flat, **params) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 2633, in bind return self.bind_with_trace(top_trace, args, params) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace out = trace.process_primitive(self, map(trace.full_raise, args), params) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 790, in process_primitive return primitive.impl(*tracers, **params) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1085, in _pjit_call_impl compiled = _pjit_lower( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2313, in compile executable = UnloadedMeshExecutable.from_hlo( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2633, in from_hlo xla_executable, compile_options = _cached_compilation( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation xla_executable = dispatch.compile_or_get_cached( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 494, in compile_or_get_cached return backend_compile(backend, computation, compile_options, File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper return func(*args, **kwargs) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 462, in backend_compile return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. >  > What jax/jaxlib version are you using? jax 0.4.9, jaxlib 0.4.9+cuda11.cudnn86 >  > My conda virtual environment: python3.9.0 cudatoolkit 11.8.0 h4ba93d1_12 condaforge cudnn 8.6.0.163 hed8a83a_0 cudistas >  > But my OS environment: NVIDIASMI 535.54.03 Driver Version: 535.54.03 CUDA Version: 12.2 ii cudnnlocalrepoubuntu18048.9.3.28 1.01 amd64 cudnnlocal repository configuration files >  > I try everything what I can , but …… then, it work. you can look this link (https://blog.csdn.net/2201_75882736/article/details/132812927)"," Hi, I also have the same issue, could anyone please help me? E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found. Traceback (most recent call last):   File ""/bd_byt4090i1/users/state_space_model/DNN/S5/run_train.py"", line 101, in      train(parser.parse_args())   File ""/bd_byt4090i1/users/state_space_model/DNN/S5/s5/train.py"", line 41, in train     key = random.PRNGKey(args.jax_seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/random.py"", line 160, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 707, in random_seed_impl_base     return seed(seeds)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 936, in threefry_seed     return _threefry_seed(seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 250, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 163, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 2677, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 815, in process_primitive     return primitive.impl(*tracers, **params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1203, in _pjit_call_impl     return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1187, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1123, in _pjit_call_impl_python     always_lower=False, lowering_platform=None).compile()   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 2323, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 2645, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 2555, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 497, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 465, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/bd_byt4090i1/users/state_space_model/DNN/S5/run_train.py"", line 101, in      train(parser.parse_args())   File ""/bd_byt4090i1/users/state_space_model/DNN/S5/s5/train.py"", line 41, in train     key = random.PRNGKey(args.jax_seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/random.py"", line 160, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 815, in process_primitive     return primitive.impl(*tracers, **params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 707, in random_seed_impl_base     return seed(seeds)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 936, in threefry_seed     return _threefry_seed(seed) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. My jax version: jax==0.4.13 jaxlib==0.4.13+cuda11.cudnn86 flax==0.7.4 chex==0.1.8 My gpu information: ++  ++ It should be a RTX 2070. Thanks a lot for the help","You are using an old JAX version (0.4.13) and an old driver  that is for CUDA 11.8. Can you update your NVIDIA driver to at least one that support CUDA 11.8, as this is the min version that is currently supported by JAX? JAX is dropping CUDA 11 in the next releases, so if you can update to CUDA12, that would be better.","> You are using an old JAX version (0.4.13) and an old driver that is for CUDA 11.8. > Can you update your NVIDIA driver to at least one that support CUDA 11.8, as this is the min version that is currently supported by JAX? JAX is dropping CUDA 11 in the next releases, so if you can update to CUDA12, that would be better. Thanks a lot. I tried that, and it worked!","Hi, guys. I'm here on by recommendation.  I'm facing a similar issue: ""Not Enough GPU memory? FAILED_PRECONDITION: DNN library initialization failed.""  I tried almost everything suggested on these page to resolve the GPU memory problem: https://github.com/YoshitakaMo/localcolabfold/issues/210 https://github.com/YoshitakaMo/localcolabfold/issues/224 https://github.com/YoshitakaMo/localcolabfold/issues/228 My current jax, cudnn, and nvidiasmi versions are as follows. (Linux Ubuntu 22.04.2 LTS and RTX 4090) ``` $nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052024 NVIDIA Corporation Built on Tue_Feb_27_16:19:38_PST_2024 Cuda compilation tools, release 12.4, V12.4.99 Build cuda_12.4.r12.4/compiler.33961263_0 $python3.10 m pip list +++ ``` Here's the problem I'm encountering: ``` $colabfold_batch templates amber test_A.fasta ./ 20240513 06:17:37,861 Running colabfold 1.5.5 (57b220e028610ba7331ebe1ef9c2d0419992469a) 20240513 06:17:38,189 Running on GPU 20240513 06:17:38,738 Found 9 citations for tools or databases 20240513 06:17:38,738 Query 1/1: pdb_A (length 108) 20240513 06:17:41,729 Sequence 0 found templates: ['1m4u_L', '2r52_B', '6oml_Y', '5vt2_B', '2r53_A', '1lxi_A', '4n1d_A', '7zjf_B', '7zjf_A', '6z3g_A', '3qb4_C', '3qb4_A', '6z3j_A', '2h64_A', '4uhy_A', '1reu_A', '4ui0_A', '2h62_B', '4mid_A', '3bk3_B'] 20240513 06:17:42,533 Setting max_seq=512, max_extra_seq=5120 20240513 06:17:42,674 Could not predict pdb_A. Not Enough GPU memory? FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. 20240513 06:17:42,674 Done ``` I can't find ""the errors above."" Can someone offer any guesses or solutions? ","JAX releases don't support CUDNN 9 yet. Downgrade to CUDNN 8.9 (or build jaxlib from source with CUDNN 9, that works also)"
741,"以下是一个github上的jax下的一个issue, 标题是(Implementation of scipy.ndimage.gaussian_filter1d and gaussian_filter)， 内容是 (Basic implementation of 1D gaussian filter as defined in scipy.ndimage. Only supports a convolution with zero padding. Extended to ND applying successive convolution on each axis. Minimal testing and performance in this colab notebook. It performs poorly compared to scipy implementation since convolution in JAX does not implement FFT based methods yet, but this may be improved in the future ( CC(FFT Convolution)). This is a start for CC(Implementation of ndimage.filters.gaussian_filter) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Implementation of scipy.ndimage.gaussian_filter1d and gaussian_filter,"Basic implementation of 1D gaussian filter as defined in scipy.ndimage. Only supports a convolution with zero padding. Extended to ND applying successive convolution on each axis. Minimal testing and performance in this colab notebook. It performs poorly compared to scipy implementation since convolution in JAX does not implement FFT based methods yet, but this may be improved in the future ( CC(FFT Convolution)). This is a start for CC(Implementation of ndimage.filters.gaussian_filter) ",2023-04-02T11:23:37Z,,open,0,1,https://github.com/jax-ml/jax/issues/15359,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
550,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Removed call_tf tests that are not applicable anymore.)， 内容是 ([jax2tf] Removed call_tf tests that are not applicable anymore. A recent change in TensorFlow makes copies of np.ndarray when they are turned into tf.constant. This means that call_tf cannot guarantee anymore nocopy. Removing those tests, and the paragraph in the documentation that describes this property.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Removed call_tf tests that are not applicable anymore.,"[jax2tf] Removed call_tf tests that are not applicable anymore. A recent change in TensorFlow makes copies of np.ndarray when they are turned into tf.constant. This means that call_tf cannot guarantee anymore nocopy. Removing those tests, and the paragraph in the documentation that describes this property.",2023-04-01T09:35:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15352
1223,"以下是一个github上的jax下的一个issue, 标题是(Added working example of forward-forward algorithm)， 内容是 (The ForwardForward Algorithm was proposed by Geoffrey Hinton in his recent paper ""The ForwardForward Algorithm: Some Preliminary Investigations"" https://arxiv.org/abs/2212.13345 This algorithm proposes a new way of optimizing neural networks using local gradient updates as an alternative to backpropagation. Each layer is trained greedily by calculating its losses and gradients at each step without passing them to other layers in the network. JAX is suitable for implementing such an algorithm because of its powerful autograd framework. Each layer must maintain its own optimizer state and parameters without worrying about the model graph. This is easy to implement using JAX's functional scope. The example trains and evaluates a simple multilayer perceptron network on the MNIST dataset, and achieves a 1.8% test accuracy error compared to the 1.36% reported in the original paper. Further improvements can be achieved with hyperparameter finetuning and changing the model architecture.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Added working example of forward-forward algorithm,"The ForwardForward Algorithm was proposed by Geoffrey Hinton in his recent paper ""The ForwardForward Algorithm: Some Preliminary Investigations"" https://arxiv.org/abs/2212.13345 This algorithm proposes a new way of optimizing neural networks using local gradient updates as an alternative to backpropagation. Each layer is trained greedily by calculating its losses and gradients at each step without passing them to other layers in the network. JAX is suitable for implementing such an algorithm because of its powerful autograd framework. Each layer must maintain its own optimizer state and parameters without worrying about the model graph. This is easy to implement using JAX's functional scope. The example trains and evaluates a simple multilayer perceptron network on the MNIST dataset, and achieves a 1.8% test accuracy error compared to the 1.36% reported in the original paper. Further improvements can be achieved with hyperparameter finetuning and changing the model architecture.",2023-03-31T21:34:36Z,,open,0,0,https://github.com/jax-ml/jax/issues/15348
1231,"以下是一个github上的jax下的一个issue, 标题是(Added example implementation of forward-forward algorithm)， 内容是 (The ForwardForward Algorithm was proposed by Geoffrey Hinton in his recent paper ""The ForwardForward Algorithm: Some Preliminary Investigations"" https://arxiv.org/abs/2212.13345 This algorithm proposes a new way of optimizing neural networks using local gradient updates as an alternative to backpropagation. Each layer is trained greedily by calculating its losses and gradients at each step without passing them to other layers in the network. JAX is suitable for implementing such an algorithm because of its powerful autograd framework. Each layer must maintain its own optimizer state and parameters without worrying about the model graph. This is easy to implement using JAX's functional scope.  The example trains and evaluates a simple multilayer perceptron network on the MNIST dataset, and achieves a 1.8% test accuracy error compared to the 1.36% reported in the original paper. Further improvements can be achieved with hyperparameter finetuning and changing the model architecture.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Added example implementation of forward-forward algorithm,"The ForwardForward Algorithm was proposed by Geoffrey Hinton in his recent paper ""The ForwardForward Algorithm: Some Preliminary Investigations"" https://arxiv.org/abs/2212.13345 This algorithm proposes a new way of optimizing neural networks using local gradient updates as an alternative to backpropagation. Each layer is trained greedily by calculating its losses and gradients at each step without passing them to other layers in the network. JAX is suitable for implementing such an algorithm because of its powerful autograd framework. Each layer must maintain its own optimizer state and parameters without worrying about the model graph. This is easy to implement using JAX's functional scope.  The example trains and evaluates a simple multilayer perceptron network on the MNIST dataset, and achieves a 1.8% test accuracy error compared to the 1.36% reported in the original paper. Further improvements can be achieved with hyperparameter finetuning and changing the model architecture.",2023-03-31T20:08:32Z,,closed,0,1,https://github.com/jax-ml/jax/issues/15346,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
1572,"以下是一个github上的jax下的一个issue, 标题是(Sparse `std` throws error)， 内容是 ( Description Hi all, I ran into an issue when trying to compute the standard deviation along an axis using sparse matrices in `BCOO` format. Here is a MWE,  ```python  import jax.numpy as jnp import jax.experimental.sparse as xsp import jax.random as rdm key = rdm.PRNGKey(1) key, g_key = rdm.split(key) G = xsp.random_bcoo(g_key, (50, 100)) std_sp = xsp.sparsify(jnp.std) std_sp(G, axis=0) ``` throws a `NotImplementedError`, with the bottom of the stacktrace looking like, ``` File ~/miniconda3/lib/python3.9/sitepackages/jax/experimental/sparse/transform.py:608, in _add_sparse(spenv, *spvalues)     606 if X.is_sparse() and Y.is_sparse():     607   if X.shape != Y.shape: > 608     raise NotImplementedError(""Addition between sparse matrices of different shapes."")     609   if X.indices_ref == Y.indices_ref:     610     out_data = lax.add(spenv.data(X), spenv.data(Y)) NotImplementedError: Addition between sparse matrices of different shapes. ``` For now I'm not requiring the results to also be sparse so a workaround is ```python mean_sp = xsp.sparsify(jnp.mean) scale = jnp.sqrt((mean_sp(G ** 2, axis=0)  mean_sp(G, axis=0) ** 2).todense()) ``` Thanks again for all the support and such a great library.  What jax/jaxlib version are you using? 0.4.8 / 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Sparse `std` throws error," Description Hi all, I ran into an issue when trying to compute the standard deviation along an axis using sparse matrices in `BCOO` format. Here is a MWE,  ```python  import jax.numpy as jnp import jax.experimental.sparse as xsp import jax.random as rdm key = rdm.PRNGKey(1) key, g_key = rdm.split(key) G = xsp.random_bcoo(g_key, (50, 100)) std_sp = xsp.sparsify(jnp.std) std_sp(G, axis=0) ``` throws a `NotImplementedError`, with the bottom of the stacktrace looking like, ``` File ~/miniconda3/lib/python3.9/sitepackages/jax/experimental/sparse/transform.py:608, in _add_sparse(spenv, *spvalues)     606 if X.is_sparse() and Y.is_sparse():     607   if X.shape != Y.shape: > 608     raise NotImplementedError(""Addition between sparse matrices of different shapes."")     609   if X.indices_ref == Y.indices_ref:     610     out_data = lax.add(spenv.data(X), spenv.data(Y)) NotImplementedError: Addition between sparse matrices of different shapes. ``` For now I'm not requiring the results to also be sparse so a workaround is ```python mean_sp = xsp.sparsify(jnp.mean) scale = jnp.sqrt((mean_sp(G ** 2, axis=0)  mean_sp(G, axis=0) ** 2).todense()) ``` Thanks again for all the support and such a great library.  What jax/jaxlib version are you using? 0.4.8 / 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_",2023-03-31T17:57:19Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15344,"Thanks for the report – I think this is expected given the current implementation of standard deviation, which effectively computes `(x  x.mean(axis)) ** 2`. Note that if `x` is sparse, then `x  x.mean(axis)` would require densifying the entire array. I think the way you're approaching it (essentially factoring out these operations) is probably the best approach."
1288,"以下是一个github上的jax下的一个issue, 标题是(fix bug from #15335 by checking main_trace tag)， 内容是 (We added the `main_trace` tag field on `core.AxisEnvFrame` so that we could disambiguate which vmap trace instance (""handler"") corresponded to which axis env frame. But in one place, namely `BatchTrace.process_primitive`'s subroutine `BatchTrace.get_frame`, we weren't using it to do that disambiguation: we were only using the (ambiguous, shadowable) name. While CC(add assertions for axis name shadowing bugs) identified the issue and added assertions that we didn't do the _wrong_ thing, this PR fixes the underlying issue and ensures we do the right thing. The assertions are still there; they just don't fire because we now don't get things confused! We should still consider disallowing shadowing:  suggested for example how confusing it might be to write `vmap(hop(fun=lambda x: ...mentions axis name 'i'...), axis_name='i')(x)` where the `hop` implementation calls `vmap(fun, axis_name='i')`. **Now we can enable the shmap test we had to skip!** In addition to enabling that test, I'm going to rename it to be more specific and add two more related ones.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fix bug from #15335 by checking main_trace tag,"We added the `main_trace` tag field on `core.AxisEnvFrame` so that we could disambiguate which vmap trace instance (""handler"") corresponded to which axis env frame. But in one place, namely `BatchTrace.process_primitive`'s subroutine `BatchTrace.get_frame`, we weren't using it to do that disambiguation: we were only using the (ambiguous, shadowable) name. While CC(add assertions for axis name shadowing bugs) identified the issue and added assertions that we didn't do the _wrong_ thing, this PR fixes the underlying issue and ensures we do the right thing. The assertions are still there; they just don't fire because we now don't get things confused! We should still consider disallowing shadowing:  suggested for example how confusing it might be to write `vmap(hop(fun=lambda x: ...mentions axis name 'i'...), axis_name='i')(x)` where the `hop` implementation calls `vmap(fun, axis_name='i')`. **Now we can enable the shmap test we had to skip!** In addition to enabling that test, I'm going to rename it to be more specific and add two more related ones.",2023-03-31T05:14:29Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15337
2364,"以下是一个github上的jax下的一个issue, 标题是(add assertions for axis name shadowing bugs)， 内容是 (Axis name shadowing in `vmap`of`vmap` and `vmap`of`pmap` is buggy, and has been for at least 2 years, and possibly for as long as we've had axis names in vmap. Damn! The basic issue is that we look up the axis frame in the axis environment based on name, e.g. with `core.axis_frame(self.axis_name)` in methods on `BatchTrace`. But if axis names can shadow, that means computing a frame based only on `self.axis_name` could give the wrong one! For example, if we had ```python jax.vmap(jax.pmap(lambda x: 2 * x, axis_name='i'), axis_name='i')(jax.numpy.ones((2, 4))) ``` then in `BatchTrace.process_primitive` when we're about to call the `pjit` batching rule (since `2 * x` leads to a call to the jitted `jnp.multiply`) we get a `core.axis_frame(self.axis_name).size == 4` when we have `in_vals[0].shape[in_dims[0]] == 2`. That is, when applying `BatchTrace.process_primitive` for the (outer) `vmap`, we accidentally pick up the frame for the (inner) `pmap`. Indeed we always get the frame for the inner guy from `core.axis_frame(self.axis_name)`! The two maps have gotten confused! An analogous thing happens with ```python jax.vmap(jax.vmap(lambda x: 2 * x, axis_name='i'), axis_name='i')(jax.numpy.ones((2, 4))) ``` except now we're getting two `vmap`s confused with each other: again `core.axis_frame(self.axis_name)` always gives the frame for the inner `vmap`, even when we're running the outer `vmap`'s `BatchTrace.process_primitive`. This isn't a problem for `pmap`of`vmap` or `pmap`of`pmap` because `pmap` isn't a transformation. **Also, we only noticed this now after the jitpjit merge because it may only be an issue for vmapofvmapofinitialstyle. Actually, we noticed this as a shape error when testing vmapofshmapofinitialstyle.** This PR adds assertions as a _late_ check for whether the confusion has happened. I think a better fix would be to disallow shadowing entirely, by raising an error as soon as a shadowing axis name is attempted to be bound. Or maybe we can allow shadowing and improve our implementation. In any case, I'll send such fixes in followup PRs. For now I just want to rule out silently incorrect behavior.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,add assertions for axis name shadowing bugs,"Axis name shadowing in `vmap`of`vmap` and `vmap`of`pmap` is buggy, and has been for at least 2 years, and possibly for as long as we've had axis names in vmap. Damn! The basic issue is that we look up the axis frame in the axis environment based on name, e.g. with `core.axis_frame(self.axis_name)` in methods on `BatchTrace`. But if axis names can shadow, that means computing a frame based only on `self.axis_name` could give the wrong one! For example, if we had ```python jax.vmap(jax.pmap(lambda x: 2 * x, axis_name='i'), axis_name='i')(jax.numpy.ones((2, 4))) ``` then in `BatchTrace.process_primitive` when we're about to call the `pjit` batching rule (since `2 * x` leads to a call to the jitted `jnp.multiply`) we get a `core.axis_frame(self.axis_name).size == 4` when we have `in_vals[0].shape[in_dims[0]] == 2`. That is, when applying `BatchTrace.process_primitive` for the (outer) `vmap`, we accidentally pick up the frame for the (inner) `pmap`. Indeed we always get the frame for the inner guy from `core.axis_frame(self.axis_name)`! The two maps have gotten confused! An analogous thing happens with ```python jax.vmap(jax.vmap(lambda x: 2 * x, axis_name='i'), axis_name='i')(jax.numpy.ones((2, 4))) ``` except now we're getting two `vmap`s confused with each other: again `core.axis_frame(self.axis_name)` always gives the frame for the inner `vmap`, even when we're running the outer `vmap`'s `BatchTrace.process_primitive`. This isn't a problem for `pmap`of`vmap` or `pmap`of`pmap` because `pmap` isn't a transformation. **Also, we only noticed this now after the jitpjit merge because it may only be an issue for vmapofvmapofinitialstyle. Actually, we noticed this as a shape error when testing vmapofshmapofinitialstyle.** This PR adds assertions as a _late_ check for whether the confusion has happened. I think a better fix would be to disallow shadowing entirely, by raising an error as soon as a shadowing axis name is attempted to be bound. Or maybe we can allow shadowing and improve our implementation. In any case, I'll send such fixes in followup PRs. For now I just want to rule out silently incorrect behavior.",2023-03-31T04:24:27Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/15335,"~I think I introduced this bug in  CC(simplify the implementation of vmap collectives) by splitting out the `_main_trace_for_axis_names` function (and leaving behind a `frame = core.axis_frame(self.axis_name)`.~ Nevermind, the bug existed before that PR too! It only checks the top namematching axis frame too."
885,"以下是一个github上的jax下的一个issue, 标题是(Version matched error while ""pip install -r requirements.txt""?)， 内容是 ( Description I've just transport a new machine and found that using pip install through requirements.txt doesn't working. It seems interesting that it works when I use the script in the README.md. Working: `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Not working: `pip install r requirements.txt` Requirements.txt: ``` f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html jax[cuda11_pip] ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"Version matched error while ""pip install -r requirements.txt""?"," Description I've just transport a new machine and found that using pip install through requirements.txt doesn't working. It seems interesting that it works when I use the script in the README.md. Working: `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Not working: `pip install r requirements.txt` Requirements.txt: ``` f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html jax[cuda11_pip] ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-03-30T02:51:11Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15306,I think it is just my machine issue. Other package has also encounter the same issue. Sorry;,Glad you got it sorted out!
389,"以下是一个github上的jax下的一个issue, 标题是(Add minimal pyproject.toml specifying build system)， 内容是 (Add minimal pyproject.toml specifying build system Replaces CC(Add minimal pyproject.toml specifying build system), Fixes CC(pyproject.toml based wheel builds))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add minimal pyproject.toml specifying build system,"Add minimal pyproject.toml specifying build system Replaces CC(Add minimal pyproject.toml specifying build system), Fixes CC(pyproject.toml based wheel builds)",2023-03-29T15:55:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15291
1558,"以下是一个github上的jax下的一个issue, 标题是(jnp.save in a distributed mannor through GPUDirect)， 内容是 (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. When we load and save using jax, we typically use `jnp.load`, `jnp.save` or `pickle` to serialize the object. However, when we use distributed array, these operations are gathered the data through the host memory which incurs out of memory and ask too much time spending saving checkpoint. Roughly speaking, saving 13B GPT state (param+optimizer state) requires 13min, which is equivalent to 10~100 update steps. If I save the checkpoint every 1000 steps, save operation increase the training time 10% or more. There is a DMA (Direct Memory Access)like technology inside CUDA which enables direct transfer between GPU memory and disk. https://developer.nvidia.com/blog/gpudirectstorage/ It would be better to use these technique to accelerate the research. If the technology is already applied and just can be used with some jit option, please help me. I've just created a snippet but I don't know how to fill the option and whether it will works. ``` .jit(in_sharding=???, static_argnum..?) def save(tensor, path):   jnp.save(tensor, path) .jit(out_sharding=???) def load(path):   return jnp.load(path) ``` Is there any other approach to handle this issue or other threads that struggled with?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,jnp.save in a distributed mannor through GPUDirect,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. When we load and save using jax, we typically use `jnp.load`, `jnp.save` or `pickle` to serialize the object. However, when we use distributed array, these operations are gathered the data through the host memory which incurs out of memory and ask too much time spending saving checkpoint. Roughly speaking, saving 13B GPT state (param+optimizer state) requires 13min, which is equivalent to 10~100 update steps. If I save the checkpoint every 1000 steps, save operation increase the training time 10% or more. There is a DMA (Direct Memory Access)like technology inside CUDA which enables direct transfer between GPU memory and disk. https://developer.nvidia.com/blog/gpudirectstorage/ It would be better to use these technique to accelerate the research. If the technology is already applied and just can be used with some jit option, please help me. I've just created a snippet but I don't know how to fill the option and whether it will works. ``` .jit(in_sharding=???, static_argnum..?) def save(tensor, path):   jnp.save(tensor, path) .jit(out_sharding=???) def load(path):   return jnp.load(path) ``` Is there any other approach to handle this issue or other threads that struggled with?",2023-03-29T03:25:59Z,enhancement NVIDIA GPU,open,0,4,https://github.com/jax-ml/jax/issues/15281,"You might look into https://github.com/google/tensorstore , which is what many of my colleagues use for checkpointing their JAX jobs. It is particularly valuable for *distributed* checkpointing since each worker can save pieces of the checkpoint in parallel.",That is a good candidate! I will consider it.,"I reached the gda serialization in the jax.experimental package. Saving LLaMA 65B with float16 (130GB) takes 4 minutes. Is it a decent time to consume or is there any other way to make it better? Zstd compression algorithm (which is the fastest) is used, but the compression ratio is not that great (after saving, 110GB is saved). Do I removed the compression algorithm inside the tensorstore? I think there is a default algorithm (blosc?) when I removed the compression algorithm.",I found out useful information in the following link.. https://google.github.io/tensorstore/driver/zarr/index.htmljsondriver/zarr.metadata.compressor
818,"以下是一个github上的jax下的一个issue, 标题是(Use modern packaging standards for setuptools)， 内容是 (In setuptools 30.3.0 (December 2016), support was added for specifying setup options in setup.cfg as opposed to arguments to the setup function. Options are merged, and setup.py takes priority when disagreements occur. setup.cfg arguments are superior for a couple reasons. They are:  declarative  no python code is executed to evaluate them  easier to read  capable of doing things that you'd need to opencode in python Of particular note, it is possible to load files or statically parse the AST for a simple assignment, via values such as `attr:` or `file:`. This is a cfgexclusive feature.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Use modern packaging standards for setuptools,"In setuptools 30.3.0 (December 2016), support was added for specifying setup options in setup.cfg as opposed to arguments to the setup function. Options are merged, and setup.py takes priority when disagreements occur. setup.cfg arguments are superior for a couple reasons. They are:  declarative  no python code is executed to evaluate them  easier to read  capable of doing things that you'd need to opencode in python Of particular note, it is possible to load files or statically parse the AST for a simple assignment, via values such as `attr:` or `file:`. This is a cfgexclusive feature.",2023-03-29T02:49:42Z,,open,1,5,https://github.com/jax-ml/jax/issues/15280, from https://github.com/google/jax/issues/15256issuecomment1487730643  > but will be somewhat painful. Our current setup process makes use of a lot of the imperative computation flexibility of `setup.py` files Here's an example that shows you can have the best of both worlds. :),"pyproject.toml is the community standard for specifying project metadata declaritively: I would recommend using that over setup.cfg, which will eventually be deprecated and removed. In this case, you would have to add `""optionaldependencies""` to `dynamic`. All other features are supported in pyproject.toml, either in the `project` table or `tool.setuptools`. `url` will become `project.urls.homepage`.","> or `tool.setuptools`. ... which is still in beta stage, unlike something that has worked robustly and stably for half a decade plus. (I'd love to see it marked as stable, but I'm not sure what the issues involved are.) > over setup.cfg, which will eventually be deprecated and removed. This will almost certainly never ever ever happen, because it would cause large chunks of PyPI to become unbuildable without any hope of vanished maintainers ever fixing things. Additionally, I seem to recall the proposed ""deprecation"" method was actually ""internally run a function that converts cfg to toml, and then run another function to load the toml"", which is a bit of indirection that really boils down to ""setup.cfg is still supported"". It's certainly going to end up removed from the documentation, *eventually*.","`find_packages` is now unused in setup.py: I suggest removing the import  > > or tool.setuptools. > > ... which is still in beta stage The docs aren't particularly clear, but it seems specifically automatic discovery, package data, and dynamic dependencies (from file) are in beta. I suppose its fine to wait for it to be marked as stable.  > > over setup.cfg, which will https://github.com/pypa/setuptools/issues/3214 and removed. > > This will almost certainly never ever ever happen, because it would cause large chunks of PyPI to become unbuildable without any hope of vanished maintainers ever fixing things. Thus the deprecation. As a related example, `distutils` is in the final stages of being removed, despite it being used in ~100% of distributed projects 10 or 15 years ago. A GitHub search shows ~64000 instances of using setup.cfg for metadata declaration right now.","> `find_packages` is now unused in setup.py: I suggest removing the import Good point. While I'm at it, distutils.spawn.find_executable() is a truly old way of performing the role of shutil.which... speaking of deprecated things."
273,"以下是一个github上的jax下的一个issue, 标题是(Add minimal pyproject.toml specifying build system)， 内容是 (Fixes CC(pyproject.toml based wheel builds))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add minimal pyproject.toml specifying build system,Fixes CC(pyproject.toml based wheel builds),2023-03-28T23:31:02Z,pull ready,closed,2,0,https://github.com/jax-ml/jax/issues/15274
1527,"以下是一个github上的jax下的一个issue, 标题是(RuntimeError: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' (set JAX_PLATFORMS='' to automatically choose an available backend))， 内容是 ( Description %%capture !pip install upgrade pip  To support manylinux2010 wheels. !pip install upgrade jax jaxlib  CPUonly !pip install flax !pip install jax !pip install optax !pip install transformers !pip install datasets !pip install U jax[cuda11_cudnn82] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install U jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html !git clone https://github.com/huggingface/transformers.git !python run_t5_mlm_flax.py \ 	output_dir=""./norwegiant5base"" \ 	model_type=""t5"" \ 	config_name=""./norwegiant5base"" \ 	tokenizer_name=""./norwegiant5base"" \ 	dataset_name=""oscar"" \ 	dataset_config_name=""unshuffled_deduplicated_no"" \ 	max_seq_length=""512"" \ 	per_device_train_batch_size=""8"" \ 	per_device_eval_batch_size=""8"" \ 	adafactor \ 	learning_rate=""0.005"" \ 	weight_decay=""0.001"" \ 	warmup_steps=""2000"" \ 	overwrite_output_dir=True \ 	logging_steps=""500"" \ 	save_steps=""10000"" \ 	eval_steps=""2500"" \     num_train_epochs=3 \  What jax/jaxlib version are you using? jax v0.4.7  Which accelerator(s) are you using? TPU  Additional system info Google Colab   NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,RuntimeError: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' (set JAX_PLATFORMS='' to automatically choose an available backend)," Description %%capture !pip install upgrade pip  To support manylinux2010 wheels. !pip install upgrade jax jaxlib  CPUonly !pip install flax !pip install jax !pip install optax !pip install transformers !pip install datasets !pip install U jax[cuda11_cudnn82] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install U jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html !git clone https://github.com/huggingface/transformers.git !python run_t5_mlm_flax.py \ 	output_dir=""./norwegiant5base"" \ 	model_type=""t5"" \ 	config_name=""./norwegiant5base"" \ 	tokenizer_name=""./norwegiant5base"" \ 	dataset_name=""oscar"" \ 	dataset_config_name=""unshuffled_deduplicated_no"" \ 	max_seq_length=""512"" \ 	per_device_train_batch_size=""8"" \ 	per_device_eval_batch_size=""8"" \ 	adafactor \ 	learning_rate=""0.005"" \ 	weight_decay=""0.001"" \ 	warmup_steps=""2000"" \ 	overwrite_output_dir=True \ 	logging_steps=""500"" \ 	save_steps=""10000"" \ 	eval_steps=""2500"" \     num_train_epochs=3 \  What jax/jaxlib version are you using? jax v0.4.7  Which accelerator(s) are you using? TPU  Additional system info Google Colab   NVIDIA GPU info _No response_",2023-03-28T22:29:58Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/15273,"I note you did this: ``` !pip install U jax[cuda11_cudnn82] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install U jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html ``` If your intent is to use TPU, try omitting the CUDA installation? I suspect you ended up with the CUDA jaxlib which does not support TPU.","Also note ""Google Colab"" Note that JAX has dropped support for Colab (Remote) TPUs in 0.4. If you want a hosted notebook with TPUs, you can either (a) downgrade to 0.3.25, or (b) use Kaggle TPUs, which are fully supported by JAX 0.4. Hope that helps!",Closing. Feel free to comment further if the suggestions above don't help.
485,"以下是一个github上的jax下的一个issue, 标题是(JAX doesn't work with cuda/gpu)， 内容是 ( Description Hi all, For a new project, I am trying to install JAX with cuda/gpu support. I installed cuda/cudnn using conda `cudatoolkit==11.7`. The same conda environment contains working pytorch and tensorflow installs, which seem to work on gpu. $ nvidiasmi ```++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JAX doesn't work with cuda/gpu," Description Hi all, For a new project, I am trying to install JAX with cuda/gpu support. I installed cuda/cudnn using conda `cudatoolkit==11.7`. The same conda environment contains working pytorch and tensorflow installs, which seem to work on gpu. $ nvidiasmi ```++  ++ ```",2023-03-28T19:29:18Z,bug NVIDIA GPU,closed,0,16,https://github.com/jax-ml/jax/issues/15268,"``` 20230328 20:24:31.407336: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. ``` This is the key message. Note: we found a CuDNN v8.5, but JAX was built against CuDNN v8.6. You need to update CuDNN. Is there an older CuDNN somewhere on your system?","Thanks for the fast response. Instead, I also tried installing JAX built with a lower CuDNN version from https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html using the command: ``` pip install https://storage.googleapis.com/jaxreleases/cuda11/jaxlib0.4.7+cuda11.cudnn82cp39cp39manylinux2014_x86_64.whl pip install jax==0.4.7 ``` If I understand correctly, this build should work with `cudnn >= 8.2` (judging from the last error, my system has CuDNN v8.5). I am still getting an error, although it's a slightly different one: ``` >>> import jax >>> jax.devices() [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] >>> import jax.numpy as np >>> np.ones(5) 20230328 22:27:30.612627: W external/xla/xla/stream_executor/cuda/cuda_dnn.cc:397] There was an error before creating cudnn handle: cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found. 20230328 22:27:30.612751: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR Traceback (most recent call last):   File """", line 1, in    File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2092, in ones     return lax.full(shape, 1, _jnp_dtype(dtype))   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 1190, in full     return broadcast(fill_value, shape)   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 756, in broadcast     return broadcast_in_dim(operand, tuple(sizes) + np.shape(operand), dims)   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/lax/lax.py"", line 784, in broadcast_in_dim     return broadcast_in_dim_p.bind(   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/core.py"", line 359, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/core.py"", line 362, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/core.py"", line 816, in process_primitive     return primitive.impl(*tracers, **params)   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 117, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 208, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), prim.name,   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 254, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2836, in compile     self._executable = UnloadedMeshExecutable.from_hlo(   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 3048, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 526, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/my_home_dir/anaconda3/envs/gpu/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 471, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ``` Again, this is in the same conda env where pytorch and tensorflow seem to have no issue with finding cudnn. Thanks a lot!","In JAX 0.4.7, there is a new way to install jax that use pip packages for cuda stuff (except the driver). Can you use pip instead of conda to install it? `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` It works for us, but wasn't widely tested. Do this in a fresh env, as it will install cuda 12 and you don't want both cuda to be installed in your env.",This seems to work. Thanks a lot!,"I don't believe this issue should be closed, as I have confirmed that I encounter the same problem when performing either cuda12_local or cuda11_local installations. So far, I have not been able to find a solution. It appears that the local installations are not functioning properly and may require attention.", can you post a repro with `nvidiasmi` output ... etc?,``` Fri Mar 31 17:55:11 2023        ++  ++ ``` Using  nvidia/cuda:11.8.0cudnn8develubuntu20.04 docker image installing JAX  would result in the error.  I just verified that installing CuDNN manually solves it. Could this be a linking error?, what version of cudnn is present in that docker image?,ENV NV_CUDNN_VERSION=8.7.0.84 ENV NV_CUDNN_PACKAGE_NAME=libcudnn8 ENV NV_CUDNN_PACKAGE=libcudnn8=8.7.0.841+cuda11.8 ENV NV_CUDNN_PACKAGE_DEV=libcudnn8dev=8.7.0.841+cuda11.8 ``` /usr/include/x86_64linuxgnu/cudnn_v8.h /usr/include/x86_64linuxgnu/cudnn_version_v8.h /usr/include/x86_64linuxgnu/cudnn_adv_train_v8.h /usr/include/x86_64linuxgnu/cudnn_cnn_train_v8.h /usr/include/x86_64linuxgnu/cudnn_adv_infer_v8.h /usr/include/x86_64linuxgnu/cudnn_backend_v8.h /usr/include/x86_64linuxgnu/cudnn_ops_train_v8.h /usr/include/x86_64linuxgnu/cudnn_cnn_infer_v8.h /usr/include/x86_64linuxgnu/cudnn_ops_infer_v8.h /usr/include/cudnn_ops_train.h /usr/include/cudnn_version.h /usr/include/cudnn_backend.h /usr/include/cudnn_ops_infer.h /usr/include/cudnn_cnn_train.h /usr/include/cudnn_adv_infer.h /usr/include/cudnn_adv_train.h /usr/include/cudnn_cnn_infer.h /usr/include/cudnn.h /usr/src/cudnn_samples_v8 /usr/share/lintian/overrides/libcudnn8 /usr/share/lintian/overrides/libcudnn8dev /usr/share/doc/libcudnn8 /usr/share/doc/libcudnn8dev /usr/local/lib/python3.8/distpackages/jaxlib0.4.7+cuda11.cudnn86.distinfo /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_train_static.a /usr/lib/x86_64linuxgnu/libcudnn_adv_infer_static.a /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so.8 /usr/lib/x86_64linuxgnu/libcudnn_adv_train_static_v8.a /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer_static.a /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer_static_v8.a /usr/lib/x86_64linuxgnu/libcudnn.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train.so /usr/lib/x86_64linuxgnu/libcudnn_ops_train_static.a /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer_static.a /usr/lib/x86_64linuxgnu/libcudnn.so.8 /usr/lib/x86_64linuxgnu/libcudnn.so /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer_static_v8.a /usr/lib/x86_64linuxgnu/libcudnn_adv_train.so /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8 /usr/lib/x86_64linuxgnu/libcudnn_ops_infer.so /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so /usr/lib/x86_64linuxgnu/libcudnn_adv_infer_static_v8.a /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8.7.0 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train_static_v8.a /usr/lib/x86_64linuxgnu/libcudnn_ops_train.so.8 /usr/lib/x86_64linuxgnu/libcudnn_adv_infer.so.8 /usr/lib/x86_64linuxgnu/libcudnn_cnn_train_static.a /usr/lib/x86_64linuxgnu/libcudnn_ops_train_static_v8.a /usr/lib/x86_64linuxgnu/libcudnn_cnn_infer.so.8.7.0 /etc/alternatives/cudnn_version_h /etc/alternatives/libcudnn_adv_infer_so /etc/alternatives/cudnn_cnn_infer_h /etc/alternatives/cudnn_adv_train_h /etc/alternatives/libcudnn_ops_infer_so /etc/alternatives/cudnn_backend_h /etc/alternatives/cudnn_ops_train_h /etc/alternatives/cudnn_cnn_train_h /etc/alternatives/libcudnn_cnn_train_so /etc/alternatives/libcudnn_so /etc/alternatives/libcudnn_cnn_infer_so /etc/alternatives/libcudnn_ops_train_so /etc/alternatives/cudnn_adv_infer_h /etc/alternatives/libcudnn /etc/alternatives/cudnn_ops_infer_h /etc/alternatives/libcudnn_adv_train_so /var/lib/dpkg/info/libcudnn8dev.md5sums /var/lib/dpkg/info/libcudnn8dev.postinst /var/lib/dpkg/info/libcudnn8dev.list /var/lib/dpkg/info/libcudnn8dev.prerm /var/lib/dpkg/info/libcudnn8.md5sums /var/lib/dpkg/info/libcudnn8.list /var/lib/dpkg/alternatives/libcudnn ```," I cannot reproduce. On a cloud VM with an NVIDIA T4 GPU, I did this: ``` $ docker run it  gpus=all nvidia/cuda:11.8.0cudnn8develubuntu20.04 bash ========== == CUDA == ========== CUDA Version 11.8.0 Container image Copyright (c) 20162022, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License. By pulling and using the container, you accept the terms and conditions of this license: https://developer.nvidia.com/ngc/nvidiadeeplearningcontainerlicense A copy of this license is made available in this container at /NGCDLCONTAINERLICENSE for your convenience. root:/ apt update Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B] Get:2 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB] Get:3 http://security.ubuntu.com/ubuntu focalsecurity InRelease [114 kB] Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [969 kB] Get:5 http://archive.ubuntu.com/ubuntu focalupdates InRelease [114 kB] Get:6 http://archive.ubuntu.com/ubuntu focalbackports InRelease [108 kB] Get:7 http://security.ubuntu.com/ubuntu focalsecurity/universe amd64 Packages [1027 kB] Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB] Get:9 http://security.ubuntu.com/ubuntu focalsecurity/main amd64 Packages [2590 kB] Get:10 http://security.ubuntu.com/ubuntu focalsecurity/restricted amd64 Packages [2060 kB] Get:11 http://security.ubuntu.com/ubuntu focalsecurity/multiverse amd64 Packages [28.5 kB] Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB] Get:13 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB] Get:14 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB] Get:15 http://archive.ubuntu.com/ubuntu focalupdates/universe amd64 Packages [1323 kB] Get:16 http://archive.ubuntu.com/ubuntu focalupdates/restricted amd64 Packages [2198 kB] Get:17 http://archive.ubuntu.com/ubuntu focalupdates/multiverse amd64 Packages [31.2 kB] Get:18 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 Packages [3069 kB] Get:19 http://archive.ubuntu.com/ubuntu focalbackports/main amd64 Packages [55.2 kB] Get:20 http://archive.ubuntu.com/ubuntu focalbackports/universe amd64 Packages [28.6 kB] Fetched 26.8 MB in 2s (13.0 MB/s) Reading package lists... Done Building dependency tree Reading state information... Done 26 packages can be upgraded. Run 'apt list upgradable' to see them. root:/ apt install python3pip Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed:   file libexpat1 libexpat1dev libmagicmgc libmagic1 libmpdec2 libpython3dev libpython3stdlib libpython3.8 libpython3.8dev libpython3.8minimal libpython3.8stdlib mimesupport pythonpipwhl python3   python3dev python3distutils python3lib2to3 python3minimal python3pkgresources python3setuptools python3wheel python3.8 python3.8dev python3.8minimal zlib1gdev Suggested packages:   python3doc python3tk python3venv pythonsetuptoolsdoc python3.8venv python3.8doc binfmtsupport The following NEW packages will be installed:   file libexpat1 libexpat1dev libmagicmgc libmagic1 libmpdec2 libpython3dev libpython3stdlib libpython3.8 libpython3.8dev libpython3.8minimal libpython3.8stdlib mimesupport pythonpipwhl python3   python3dev python3distutils python3lib2to3 python3minimal python3pip python3pkgresources python3setuptools python3wheel python3.8 python3.8dev python3.8minimal zlib1gdev 0 upgraded, 27 newly installed, 0 to remove and 26 not upgraded. Need to get 14.4 MB of archives. After this operation, 61.7 MB of additional disk space will be used. Do you want to continue? [Y/n] Get:1 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 libpython3.8minimal amd64 3.8.100ubuntu1~20.04.7 [717 kB] Get:2 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 libexpat1 amd64 2.2.91ubuntu0.6 [74.6 kB] Get:3 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 python3.8minimal amd64 3.8.100ubuntu1~20.04.7 [1903 kB] Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 python3minimal amd64 3.8.20ubuntu2 [23.6 kB] Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 mimesupport all 3.64ubuntu1 [30.6 kB] Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libmpdec2 amd64 2.4.23 [81.1 kB] Get:7 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 libpython3.8stdlib amd64 3.8.100ubuntu1~20.04.7 [1675 kB] Get:8 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 python3.8 amd64 3.8.100ubuntu1~20.04.7 [387 kB] Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libpython3stdlib amd64 3.8.20ubuntu2 [7068 B] Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 python3 amd64 3.8.20ubuntu2 [47.6 kB] Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagicmgc amd64 1:5.384 [218 kB] Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libmagic1 amd64 1:5.384 [75.9 kB] Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 file amd64 1:5.384 [23.3 kB] Get:14 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 python3pkgresources all 45.2.01ubuntu0.1 [130 kB] Get:15 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 libexpat1dev amd64 2.2.91ubuntu0.6 [116 kB] Get:16 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 libpython3.8 amd64 3.8.100ubuntu1~20.04.7 [1626 kB] Get:17 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 libpython3.8dev amd64 3.8.100ubuntu1~20.04.7 [3953 kB] Get:18 http://archive.ubuntu.com/ubuntu focal/main amd64 libpython3dev amd64 3.8.20ubuntu2 [7236 B] Get:19 http://archive.ubuntu.com/ubuntu focalupdates/universe amd64 pythonpipwhl all 20.0.25ubuntu1.8 [1805 kB] Get:20 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 zlib1gdev amd64 1:1.2.11.dfsg2ubuntu1.5 [155 kB] Get:21 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 python3.8dev amd64 3.8.100ubuntu1~20.04.7 [514 kB] Get:22 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 python3lib2to3 all 3.8.100ubuntu1~20.04 [76.3 kB] Get:23 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 python3distutils all 3.8.100ubuntu1~20.04 [141 kB] Get:24 http://archive.ubuntu.com/ubuntu focal/main amd64 python3dev amd64 3.8.20ubuntu2 [1212 B] Get:25 http://archive.ubuntu.com/ubuntu focalupdates/main amd64 python3setuptools all 45.2.01ubuntu0.1 [330 kB] Get:26 http://archive.ubuntu.com/ubuntu focalupdates/universe amd64 python3wheel all 0.34.21ubuntu0.1 [23.9 kB] Get:27 http://archive.ubuntu.com/ubuntu focalupdates/universe amd64 python3pip all 20.0.25ubuntu1.8 [231 kB] Fetched 14.4 MB in 1s (9640 kB/s) debconf: delaying package configuration, since aptutils is not installed Selecting previously unselected package libpython3.8minimal:amd64. (Reading database ... 12894 files and directories currently installed.) Preparing to unpack .../libpython3.8minimal_3.8.100ubuntu1~20.04.7_amd64.deb ... Unpacking libpython3.8minimal:amd64 (3.8.100ubuntu1~20.04.7) ... Selecting previously unselected package libexpat1:amd64. Preparing to unpack .../libexpat1_2.2.91ubuntu0.6_amd64.deb ... Unpacking libexpat1:amd64 (2.2.91ubuntu0.6) ... Selecting previously unselected package python3.8minimal. Preparing to unpack .../python3.8minimal_3.8.100ubuntu1~20.04.7_amd64.deb ... Unpacking python3.8minimal (3.8.100ubuntu1~20.04.7) ... Setting up libpython3.8minimal:amd64 (3.8.100ubuntu1~20.04.7) ... Setting up libexpat1:amd64 (2.2.91ubuntu0.6) ... Setting up python3.8minimal (3.8.100ubuntu1~20.04.7) ... Selecting previously unselected package python3minimal. (Reading database ... 13185 files and directories currently installed.) Preparing to unpack .../0python3minimal_3.8.20ubuntu2_amd64.deb ... Unpacking python3minimal (3.8.20ubuntu2) ... Selecting previously unselected package mimesupport. Preparing to unpack .../1mimesupport_3.64ubuntu1_all.deb ... Unpacking mimesupport (3.64ubuntu1) ... Selecting previously unselected package libmpdec2:amd64. Preparing to unpack .../2libmpdec2_2.4.23_amd64.deb ... Unpacking libmpdec2:amd64 (2.4.23) ... Selecting previously unselected package libpython3.8stdlib:amd64. Preparing to unpack .../3libpython3.8stdlib_3.8.100ubuntu1~20.04.7_amd64.deb ... Unpacking libpython3.8stdlib:amd64 (3.8.100ubuntu1~20.04.7) ... Selecting previously unselected package python3.8. Preparing to unpack .../4python3.8_3.8.100ubuntu1~20.04.7_amd64.deb ... Unpacking python3.8 (3.8.100ubuntu1~20.04.7) ... Selecting previously unselected package libpython3stdlib:amd64. Preparing to unpack .../5libpython3stdlib_3.8.20ubuntu2_amd64.deb ... Unpacking libpython3stdlib:amd64 (3.8.20ubuntu2) ... Setting up python3minimal (3.8.20ubuntu2) ... Selecting previously unselected package python3. (Reading database ... 13587 files and directories currently installed.) Preparing to unpack .../00python3_3.8.20ubuntu2_amd64.deb ... Unpacking python3 (3.8.20ubuntu2) ... Selecting previously unselected package libmagicmgc. Preparing to unpack .../01libmagicmgc_1%3a5.384_amd64.deb ... Unpacking libmagicmgc (1:5.384) ... Selecting previously unselected package libmagic1:amd64. Preparing to unpack .../02libmagic1_1%3a5.384_amd64.deb ... Unpacking libmagic1:amd64 (1:5.384) ... Selecting previously unselected package file. Preparing to unpack .../03file_1%3a5.384_amd64.deb ... Unpacking file (1:5.384) ... Selecting previously unselected package python3pkgresources. Preparing to unpack .../04python3pkgresources_45.2.01ubuntu0.1_all.deb ... Unpacking python3pkgresources (45.2.01ubuntu0.1) ... Selecting previously unselected package libexpat1dev:amd64. Preparing to unpack .../05libexpat1dev_2.2.91ubuntu0.6_amd64.deb ... Unpacking libexpat1dev:amd64 (2.2.91ubuntu0.6) ... Selecting previously unselected package libpython3.8:amd64. Preparing to unpack .../06libpython3.8_3.8.100ubuntu1~20.04.7_amd64.deb ... Unpacking libpython3.8:amd64 (3.8.100ubuntu1~20.04.7) ... Selecting previously unselected package libpython3.8dev:amd64. Preparing to unpack .../07libpython3.8dev_3.8.100ubuntu1~20.04.7_amd64.deb ... Unpacking libpython3.8dev:amd64 (3.8.100ubuntu1~20.04.7) ... Selecting previously unselected package libpython3dev:amd64. Preparing to unpack .../08libpython3dev_3.8.20ubuntu2_amd64.deb ... Unpacking libpython3dev:amd64 (3.8.20ubuntu2) ... Selecting previously unselected package pythonpipwhl. Preparing to unpack .../09pythonpipwhl_20.0.25ubuntu1.8_all.deb ... Unpacking pythonpipwhl (20.0.25ubuntu1.8) ... Selecting previously unselected package zlib1gdev:amd64. Preparing to unpack .../10zlib1gdev_1%3a1.2.11.dfsg2ubuntu1.5_amd64.deb ... Unpacking zlib1gdev:amd64 (1:1.2.11.dfsg2ubuntu1.5) ... Selecting previously unselected package python3.8dev. Preparing to unpack .../11python3.8dev_3.8.100ubuntu1~20.04.7_amd64.deb ... Unpacking python3.8dev (3.8.100ubuntu1~20.04.7) ... Selecting previously unselected package python3lib2to3. Preparing to unpack .../12python3lib2to3_3.8.100ubuntu1~20.04_all.deb ... Unpacking python3lib2to3 (3.8.100ubuntu1~20.04) ... Selecting previously unselected package python3distutils. Preparing to unpack .../13python3distutils_3.8.100ubuntu1~20.04_all.deb ... Unpacking python3distutils (3.8.100ubuntu1~20.04) ... Selecting previously unselected package python3dev. Preparing to unpack .../14python3dev_3.8.20ubuntu2_amd64.deb ... Unpacking python3dev (3.8.20ubuntu2) ... Selecting previously unselected package python3setuptools. Preparing to unpack .../15python3setuptools_45.2.01ubuntu0.1_all.deb ... Unpacking python3setuptools (45.2.01ubuntu0.1) ... Selecting previously unselected package python3wheel. Preparing to unpack .../16python3wheel_0.34.21ubuntu0.1_all.deb ... Unpacking python3wheel (0.34.21ubuntu0.1) ... Selecting previously unselected package python3pip. Preparing to unpack .../17python3pip_20.0.25ubuntu1.8_all.deb ... Unpacking python3pip (20.0.25ubuntu1.8) ... Setting up mimesupport (3.64ubuntu1) ... Setting up libmagicmgc (1:5.384) ... Setting up libmagic1:amd64 (1:5.384) ... Setting up file (1:5.384) ... Setting up libexpat1dev:amd64 (2.2.91ubuntu0.6) ... Setting up zlib1gdev:amd64 (1:1.2.11.dfsg2ubuntu1.5) ... Setting up pythonpipwhl (20.0.25ubuntu1.8) ... Setting up libmpdec2:amd64 (2.4.23) ... Setting up libpython3.8stdlib:amd64 (3.8.100ubuntu1~20.04.7) ... Setting up python3.8 (3.8.100ubuntu1~20.04.7) ... Setting up libpython3stdlib:amd64 (3.8.20ubuntu2) ... Setting up python3 (3.8.20ubuntu2) ... running python rtupdate hooks for python3.8... running python postrtupdate hooks for python3.8... Setting up python3wheel (0.34.21ubuntu0.1) ... Setting up libpython3.8:amd64 (3.8.100ubuntu1~20.04.7) ... Setting up python3lib2to3 (3.8.100ubuntu1~20.04) ... Setting up python3pkgresources (45.2.01ubuntu0.1) ... Setting up python3distutils (3.8.100ubuntu1~20.04) ... Setting up python3setuptools (45.2.01ubuntu0.1) ... Setting up libpython3.8dev:amd64 (3.8.100ubuntu1~20.04.7) ... Setting up python3pip (20.0.25ubuntu1.8) ... Setting up python3.8dev (3.8.100ubuntu1~20.04.7) ... Setting up libpython3dev:amd64 (3.8.20ubuntu2) ... Setting up python3dev (3.8.20ubuntu2) ... Processing triggers for libcbin (2.310ubuntu9.9) ... root:/ pip install q ipython root:/ pip install upgrade ""jax[cuda11_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Collecting jax[cuda11_local]   Downloading jax0.4.8.tar.gz (1.2 MB)       ++ ```",  This part of the error message: ``` 20230328 22:27:30.612627: W external/xla/xla/stream_executor/cuda/cuda_dnn.cc:397] There was an error before creating cudnn handle: cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found. ``` Seem to indicate there is a driver issues. I did a small PR to improve the error message a little bit: https://github.com/openxla/xla/pull/2335 But are you able to use other software on the GPU? Maybe reinstalling the driver can help you.,"Similar errors here with local install of cuda11.8 and cudnn 8.8 on Ubuntu 20.04 WSL2 and then used: pip install upgrade ""jax[cuda11_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html for install. As others have said, Tensorflow and the cudnn samples run without issue. Running any JAX code gives: `E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1207] Failed to get stream capture info: operation not supported E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2432] Execution of replica 0 failed: INVALID_ARGUMENT: stream is uninitialized or in an error state` `++  ++`", WSL is communitysupported. This is not a configuration we test.,https://github.com/openxla/xla/pull/2335 is merged. So the error message should be better in the next release.,">  WSL is communitysupported. This is not a configuration we test. No problem. I will say that I have it running on 3 machines through WSL2 and it has generally ran perfectly. It was just this upgrade with the Titan RTX that seems to have caused issues (upgraded from 11.4 / 8.2 to 12.1 / 8.8 and the latest driver so could just be that combo on that card). Weird issue. 12.1 / 8.8 on the other 2 and they are 2060 and 3050 for GPU, respectively.",I don't think there are any outstanding issues that we have instructions to reproduce in this bug. Please open a new bug with instructions to reproduce if any of these issues still apply!
901,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Refactor the computation of the dimension variables in native serialization)， 内容是 (Currently, JAX native serialization produces a module whose main function takes additional arguments for the values of the dimension variables. These values are then resolved in the XlaCallModule based on a dim_args_spec parameter. We move the code that computes the dimension variables from XlaCallModule to jax_export following pretty much the same technique. This simplifies XlaCallModule and especially its API (the dim_args_spec). So far this is just a refactoring with no semantic changes, but this will allow us to improve the support for dimension variables that occur in linear polynomials, e.g., ""2*b"" rather than just ""b"".)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Refactor the computation of the dimension variables in native serialization,"Currently, JAX native serialization produces a module whose main function takes additional arguments for the values of the dimension variables. These values are then resolved in the XlaCallModule based on a dim_args_spec parameter. We move the code that computes the dimension variables from XlaCallModule to jax_export following pretty much the same technique. This simplifies XlaCallModule and especially its API (the dim_args_spec). So far this is just a refactoring with no semantic changes, but this will allow us to improve the support for dimension variables that occur in linear polynomials, e.g., ""2*b"" rather than just ""b"".",2023-03-28T09:27:05Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/15258,  please take a look,"> LGTM. >  > Does this impact the Refine Shapes pass at all? I'm not too sure how the setup for that pass to be called works.  It does not, before the Refine Shapes the XlaCallModule will add a wrappper. But since dim_args_spec is empty there will be no wrapper added. Recently I have changed the XlaCallModule to run the shape refinement even if dim_args_spec is empty, expecting the change I am making now."
231,"以下是一个github上的jax下的一个issue, 标题是(Fix typo in betaln.py)， 内容是 (implmentation > implementation)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix typo in betaln.py,implmentation > implementation,2023-03-28T05:44:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15255
1263,"以下是一个github上的jax下的一个issue, 标题是(Jax asks for non-existing device numbers)， 内容是 ( Description I am running JAX on a Slurm cluster with `jax.distributed.initialize`. If I ask for a number of process _lower or equal_ to the number of devices available, everything runs fine. When using more process than devices available on a node (trying to have several JAX process use the same GPU concurrently), initializing with `jax.distributed.initialize` leads to it asking for device numbers that do not exist due to this line which asks for a device number equal to the process id.  This fails causing JAX to fall back to `cpu` on some processes and produce this warning: ``` 20220323 20:23:46.386347: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:205] unable to create  StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to  cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal ``` See also this issue.  What jax/jaxlib version are you using? jax0.4.6 jaxlib0.4.6+cuda11.cudnn86  Which accelerator(s) are you using? GPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax asks for non-existing device numbers," Description I am running JAX on a Slurm cluster with `jax.distributed.initialize`. If I ask for a number of process _lower or equal_ to the number of devices available, everything runs fine. When using more process than devices available on a node (trying to have several JAX process use the same GPU concurrently), initializing with `jax.distributed.initialize` leads to it asking for device numbers that do not exist due to this line which asks for a device number equal to the process id.  This fails causing JAX to fall back to `cpu` on some processes and produce this warning: ``` 20220323 20:23:46.386347: W external/org_tensorflow/tensorflow/compiler/xla/service/platform_util.cc:205] unable to create  StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to  cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal ``` See also this issue.  What jax/jaxlib version are you using? jax0.4.6 jaxlib0.4.6+cuda11.cudnn86  Which accelerator(s) are you using? GPU",2023-03-24T23:31:38Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/15214
864,"以下是一个github上的jax下的一个issue, 标题是(Missing version.py file in /jax/jaxlib)， 内容是 ( Description I'm trying to install jaxlib through Poetry using the Github repository :  ``` jaxlib = [     { git = ""git.com:yoziru/jax.git"", tag = ""jaxlibv0.4.6"", markers = ""platform_machine == 'aarch64'"", subdirectory = ""jaxlib""},     { git = ""git.com:google/jax.git"", tag = ""jaxlibv0.4.6"", markers = ""platform_machine != 'aarch64'"", subdirectory = ""jaxlib""} ] ``` However, the install fails because `/jax/jaxlib/setup.py` refers to `/jax/jaxlib/version.py` which does not exist. See here.  What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? CPU  Additional system info MacOS Ventura  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Missing version.py file in /jax/jaxlib," Description I'm trying to install jaxlib through Poetry using the Github repository :  ``` jaxlib = [     { git = ""git.com:yoziru/jax.git"", tag = ""jaxlibv0.4.6"", markers = ""platform_machine == 'aarch64'"", subdirectory = ""jaxlib""},     { git = ""git.com:google/jax.git"", tag = ""jaxlibv0.4.6"", markers = ""platform_machine != 'aarch64'"", subdirectory = ""jaxlib""} ] ``` However, the install fails because `/jax/jaxlib/setup.py` refers to `/jax/jaxlib/version.py` which does not exist. See here.  What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? CPU  Additional system info MacOS Ventura  NVIDIA GPU info _No response_",2023-03-24T14:38:16Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/15202,"`jaxlib` cannot be built using the `setup.py` file. It must be built like this: https://jax.readthedocs.io/en/latest/developer.html The `setup.py` file is just a stub that gets used to construct the wheel, it does not know how to build. Hope that helps!","Alright, thanks. I'll find a way.","The recommended method fails for another reason:  ``` [1 / 1] checking cached actions Analyzing: target //jaxlib/tools:build_wheel (64 packages loaded, 15 targets configured) [1 / 1] checking cached actions Analyzing: target //jaxlib/tools:build_wheel (65 packages loaded, 17 targets configured) [1 / 1] checking cached actions Analyzing: target //jaxlib/tools:build_wheel (68 packages loaded, 257 targets configured) [1 / 1] checking cached actions ERROR: /data/data/com.termux/files/home/.cache/bazel/_bazel_u0_a278/ff0f051e48e545133f94b1262bca6bed/external/local_config_python/BUILD:16:11: in py_runtime rule @//:py3_runtime: Traceback (most recent call last):         File ""/virtual_builtins_bzl/common/python/py_runtime_rule.bzl"", line 40, column 17, in _py_runtime_impl Error in fail: interpreter_path must be an absolute path ERROR: /data/data/com.termux/files/home/.cache/bazel/_bazel_u0_a278/ff0f051e48e545133f94b1262bca6bed/external/local_config_python/BUILD:16:11: Analysis of target '@//:py3_runtime' failed Analyzing: target //jaxlib/tools:build_wheel (77 packages loaded, 392 targets configured) [1 / 1] checking cached actions ERROR: Analysis of target '//jaxlib/tools:build_wheel' failed; build aborted: Analysis failed INFO: Elapsed time: 55.109s, Critical Path: 0.01s INFO: 1 process: 1 internal. ERROR: Build did NOT complete successfully FAILED: ERROR: Build failed. Not running target Traceback (most recent call last):   File ""/data/data/com.termux/files/home/downloads/jax/build/build.py"", line 726, in      main()   File ""/data/data/com.termux/files/home/downloads/jax/build/build.py"", line 692, in main     shell(build_cpu_wheel_command)   File ""/data/data/com.termux/files/home/downloads/jax/build/build.py"", line 45, in shell     output = subprocess.check_output(cmd)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/data/data/com.termux/files/usr/lib/python3.11/subprocess.py"", line 466, in check_output     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/data/data/com.termux/files/usr/lib/python3.11/subprocess.py"", line 571, in run     raise CalledProcessError(retcode, process.args, subprocess.CalledProcessError: Command '['/data/data/com.termux/files/usr/bin/bazel', 'run', 'verbose_failures=true', '//jaxlib/tools:build_wheel', '', 'output_path=/data/data/com.termux/files/home/downloads/jax/dist', 'jaxlib_git_hash=11370b758f0f5200d079687f502532cb636c107c', 'cpu=aarch64']' returned nonzero exit status 1. ~/downloads/jax $ ``` on Termux.  (While the the base `jax` compiles and installs fine, OTOH:  ``` Building wheel for jax (pyproject.toml) ... done   Created wheel for jax: filename=jax0.4.30.dev20240611+11370b758py3noneany.whl size=1996610 sha256=28619c093a13292039fff05009d13871abb04278f309e88ad956d71477772e69   Stored in directory: /data/data/com.termux/files/usr/tmp/pipephemwheelcachengugwmem/wheels/10/7d/85/9a26ab6eb1d94592f10f21fda327ffa32529a3195d4f9156f6 Successfully built jax Installing collected packages: jax   Attempting uninstall: jax     Found existing installation: jax 0.4.29     Uninstalling jax0.4.29:       Removing file or directory /data/data/com.termux/files/usr/lib/python3.11/sitepackages/jax0.4.29.distinfo/       Removing file or directory /data/data/com.termux/files/usr/lib/python3.11/sitepackages/jax/       Successfully uninstalled jax0.4.29 Successfully installed jax0.4.30.dev20240611+11370b758 ~/downloads/jax $ ``` and while `bazel build` works fine from the `jaxlib` folder.  My ugly fix, pacem Ms MS Copilot: > Title: Installation of jaxlib from source fails due to setup.py issues >  > Body: >  > I encountered several issues when trying to install jaxlib from source on my system. The original error was as follows: >  > FileNotFoundError: [Errno 2] No such file or directory: '/data/data/com.termux/files/home/downloads/jax/1/jax/jaxlib/jaxlib/version.py' >  > To resolve this, I had to make a hardcoded version check in setup.py. However, this led to another error: >  > NameError: name 'find_packages' is not defined >  > I was able to resolve this by importing find_packages from setuptools in setup.py. After making these modifications, I was able to successfully install jaxlib from source. >  More precisely: ``` ~/.../jax/jaxlib $ cat setup.py  Copyright 2018 The JAX Authors.   Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at       https://www.apache.org/licenses/LICENSE2.0   Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. import importlib import os from setuptools import setup from setuptools.dist import Distribution from setuptools import setup, find_packages __version__ = '0.4.29'   Hardcoded version number __version__ = None project_name = 'jaxlib' def load_version_module(pkg_path):     class VersionModule:                  def _get_version_for_build():             return '0.4.29'   Hardcoded version number                  def _get_cmdclass(project_name):             return {}   Return an empty command class dictionary     return VersionModule() _version_module = load_version_module(project_name) __version__ = _version_module._get_version_for_build() _cmdclass = _version_module._get_cmdclass(project_name) ''' def load_version_module(pkg_path):   spec = importlib.util.spec_from_file_location(     'version', os.path.join(pkg_path, 'version.py'))   module = importlib.util.module_from_spec(spec)   spec.loader.exec_module(module)   return module _version_module = load_version_module(project_name) __version__ = _version_module._get_version_for_build() _cmdclass = _version_module._get_cmdclass(project_name) ''' ``` + ``` ~/.../jax/jaxlib $ uname a Linux localhost 4.14.186+ CC(Python 3 compatibility issues) SMP PREEMPT Thu Mar 17 16:28:22 CST 2022 aarch64 Android ~/.../jax/jaxlib $ ``` ~/.../jax/jaxlib $ pip install v . Using pip 24.0 from /data/data/com.termux/files/usr/lib/python3.11/sitepackages/pip (python 3.11) Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com Processing /data/data/com.termux/files/home/downloads/jax/jaxlib   Running command python setup.py egg_info   running egg_info   creating /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo   writing /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/PKGINFO   writing dependency_links to /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/dependency_links.txt   writing requirements to /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/requires.txt   writing toplevel names to /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/top_level.txt   writing manifest file '/data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/SOURCES.txt'   writing manifest file '/data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/SOURCES.txt'   Preparing metadata (setup.py) ... done Requirement already satisfied: scipy>=1.9 in /data/data/com.termux/files/usr/lib/python3.11/sitepackages (from jaxlib==0.4.29) (1.13.1) Requirement already satisfied: numpy>=1.22 in /data/data/com.termux/files/usr/lib/python3.11/sitepackages (from jaxlib==0.4.29) (1.26.4) Requirement already satisfied: ml_dtypes>=0.4.0 in /data/data/com.termux/files/usr/lib/python3.11/sitepackages (from jaxlib==0.4.29) (0.4.0) Building wheels for collected packages: jaxlib   Running command python setup.py bdist_wheel   running bdist_wheel   running build   running build_py   creating build   creating build/lib.linuxaarch64cpython311   creating build/lib.linuxaarch64cpython311/triton   copying triton/__init__.py > build/lib.linuxaarch64cpython311/triton   copying triton/dialect.py > build/lib.linuxaarch64cpython311/triton   running build_ext   /data/data/com.termux/files/usr/lib/python3.11/sitepackages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.   !!           ********************************************************************************           Please avoid running ``setup.py`` directly.           Instead, use pypa/build, pypa/installer or other           standardsbased tools.           See https://blog.ganssle.io/articles/2021/10/setuppydeprecated.html for details.           ********************************************************************************   !!     self.initialize_options()   installing to build/bdist.linuxaarch64/wheel   running install   running install_lib   creating build/bdist.linuxaarch64   creating build/bdist.linuxaarch64/wheel   creating build/bdist.linuxaarch64/wheel/triton   copying build/lib.linuxaarch64cpython311/triton/__init__.py > build/bdist.linuxaarch64/wheel/triton   copying build/lib.linuxaarch64cpython311/triton/dialect.py > build/bdist.linuxaarch64/wheel/triton   running install_egg_info   running egg_info   creating jaxlib.egginfo   writing jaxlib.egginfo/PKGINFO   writing dependency_links to jaxlib.egginfo/dependency_links.txt   writing requirements to jaxlib.egginfo/requires.txt   writing toplevel names to jaxlib.egginfo/top_level.txt   writing manifest file 'jaxlib.egginfo/SOURCES.txt'   writing manifest file 'jaxlib.egginfo/SOURCES.txt'   Copying jaxlib.egginfo to build/bdist.linuxaarch64/wheel/jaxlib0.4.29py3.11.egginfo   running install_scripts   creating build/bdist.linuxaarch64/wheel/jaxlib0.4.29.distinfo/WHEEL   creating '/data/data/com.termux/files/usr/tmp/pipwheelhn39ttzz/jaxlib0.4.29cp311cp311linux_aarch64.whl' and adding 'build/bdist.linuxaarch64/wheel' to it   adding 'triton/__init__.py'   adding 'triton/dialect.py'   adding 'jaxlib0.4.29.distinfo/METADATA'   adding 'jaxlib0.4.29.distinfo/WHEEL'   adding 'jaxlib0.4.29.distinfo/top_level.txt'   adding 'jaxlib0.4.29.distinfo/RECORD'   removing build/bdist.linuxaarch64/wheel   Building wheel for jaxlib (setup.py) ... done   Created wheel for jaxlib: filename=jaxlib0.4.29cp311cp311linux_aarch64.whl size=3425 sha256=52935f58e5020104d773ad385b27713ae5616fd53e6567d7af4516796a940f2f   Stored in directory: /data/data/com.termux/files/usr/tmp/pipephemwheelcacheq9nrs06h/wheels/ed/a6/ec/c71a34c5860c8c2d353799fb146d5baac62f79da6080c5b9cb Successfully built jaxlib Installing collected packages: jaxlib Successfully installed jaxlib0.4.29 ```"
4734,"以下是一个github上的jax下的一个issue, 标题是([dynamic-shapes] djax+pjit staging to jaxpr)， 内容是 (This is the first in a sequence of PRs making the dynamic shape machinery work with `pjit`, aka the new ""initialstyle"" `jit`. (Previously it only worked with the old ""finalstyle"" `jit`.) Concretely, we want to make all the tests in `tests/dynamic_api_test.py` work again (better than ever!). We're going to break this down into a sequence of steps: 1. [x] stagingtojaxpr e.g. with `jax.make_jaxpr` (this PR, CC([dynamicshapes] djax+pjit staging to jaxpr)) 3. [ ] paddedandmasked bintbased lowering and execution (e.g. with XLA, using our own paddingandmasking and/or using the HLO DynamicPadder) 4. [ ] dynamic shape compiler lowering and execution (e.g. with IREE) 5. [ ] autodiff e.g. with `jax.jvp`, `jax.linearize`, and `jax.grad` 6. [ ] batching e.g. with `jax.vmap` Once we get these things working again, we'll be able to start landing new features!  This PR tackles staging to dynamic shape jaxprs. The relevant tests, moved to their own `DynamicShapeStagingTest` class and unskipped in this PR, basically involve using `jax.make_jaxpr`, though the main work is in the underlying `partial_eval.trace_to_jaxpr_dynamic2` and the changes to `pjit`. So most of this PR is adapting preexisting stuff for the old `jit` to work with the new `jit`/`pjit`. Here's an overview of the changes made in this PR and why they were needed: * The changes in tests/dynamic_api_test.py are just code movement, plus renaming like `xla_call_p` > `pjit` and `call_jaxpr`>`jaxpr`. * In partial_eval.py, we needed to handle a previously unhandled case where constants closed over by a `pjit`decorated function had `Tracer`s in their shapes (and hence those Tracers need to be lifted into the `pjit`trace). Conceptually, because with dynamic shapes `Tracer`s can now have `Tracer`s in their shapes, whatever we did to handle closedover `Tracer`s in the static shape world, we now also have to apply to `Tracer`s in the shapes of closedover `Tracer`s. The reason this case wasn't handled in the previous implementation is that due to a detail of finalstyle tracing we would hit the `DynamicJaxprTrace.sublift` where with initialstyle we now hit `DynamicJaxprTrace.new_const`; see the preexisting comment in the body of `DynamicJaxprTrace.sublift`. The change here was just a one liner to call `DynamicJaxprTrace._lift_tracers_in_aval`. * In pjit.py, we had to plumb through the `abstracted_axes` API option, and use it in the call to the new helper `_flat_axes_specs` which resolves the `abstracted_axes` user annotations to the internal abstracted axis size specification. We then use that specification to determine the input type on which to trace the `jit`decorated Python callable (and the corresponding jaxpr's input type) by calling the existing utility `partial_eval.infer_lambda_input_type`. * Also in pjit.py, we use that input type `in_type: core.InputType` in place of the tuple of avals `global_in_avals: core.AbstractValue`. The previous `global_in_avals`, representing things like `(f32[3], f32[4, 5])`, was sufficient to represent statically shaped input types on which to trace `jit`decorated functions. But with dynamic shapes we need to generalize that to represent things more like `(n:i32[], f32[n], f32[4,5])`. That is, a tuple of avals is no longer sufficient. But it's useful to remember that the logic is staying the same: we always had to trace on an input type, it's just that dynamic shape input types are more general. * The generalization to `core.InputType` has a corresponding generalization of the calling convention for the `pjit_p` primitive: if we apply `jit(f)(x)` for some dynamicallyshaped `x` (dynamically shaped due to some outer `jit`), we need to build an input type with axis size arguments, like `(n:i32[], f32[n])`, and the convention is that the `pjit_p` primitive needs to be applied to _two_ arguments, corresponding to the pair `(x.shape[0], x)`. That is, with dynamic shapes, we need to automatically extract implicittousercode axis size arguments. That's also done in pjit.py, in the function `_extract_implicit_args`, which is like an intialstyle analogue of the preexisting `partial_eval._extract_implicit_args` utility for finalstyle. * The staging rule for `pjit_p` also needs to be generalized, mainly to be able to produce output `Tracer`s with `Tracer`s in their shapes, i.e. to be able to produce dynamicallyshaped outputs. All of the changes are guarded by the `jax.config.jax_dynamic_shapes` flag, so unless I've made a mistake we shouldn't be breaking anyone's code.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[dynamic-shapes] djax+pjit staging to jaxpr,"This is the first in a sequence of PRs making the dynamic shape machinery work with `pjit`, aka the new ""initialstyle"" `jit`. (Previously it only worked with the old ""finalstyle"" `jit`.) Concretely, we want to make all the tests in `tests/dynamic_api_test.py` work again (better than ever!). We're going to break this down into a sequence of steps: 1. [x] stagingtojaxpr e.g. with `jax.make_jaxpr` (this PR, CC([dynamicshapes] djax+pjit staging to jaxpr)) 3. [ ] paddedandmasked bintbased lowering and execution (e.g. with XLA, using our own paddingandmasking and/or using the HLO DynamicPadder) 4. [ ] dynamic shape compiler lowering and execution (e.g. with IREE) 5. [ ] autodiff e.g. with `jax.jvp`, `jax.linearize`, and `jax.grad` 6. [ ] batching e.g. with `jax.vmap` Once we get these things working again, we'll be able to start landing new features!  This PR tackles staging to dynamic shape jaxprs. The relevant tests, moved to their own `DynamicShapeStagingTest` class and unskipped in this PR, basically involve using `jax.make_jaxpr`, though the main work is in the underlying `partial_eval.trace_to_jaxpr_dynamic2` and the changes to `pjit`. So most of this PR is adapting preexisting stuff for the old `jit` to work with the new `jit`/`pjit`. Here's an overview of the changes made in this PR and why they were needed: * The changes in tests/dynamic_api_test.py are just code movement, plus renaming like `xla_call_p` > `pjit` and `call_jaxpr`>`jaxpr`. * In partial_eval.py, we needed to handle a previously unhandled case where constants closed over by a `pjit`decorated function had `Tracer`s in their shapes (and hence those Tracers need to be lifted into the `pjit`trace). Conceptually, because with dynamic shapes `Tracer`s can now have `Tracer`s in their shapes, whatever we did to handle closedover `Tracer`s in the static shape world, we now also have to apply to `Tracer`s in the shapes of closedover `Tracer`s. The reason this case wasn't handled in the previous implementation is that due to a detail of finalstyle tracing we would hit the `DynamicJaxprTrace.sublift` where with initialstyle we now hit `DynamicJaxprTrace.new_const`; see the preexisting comment in the body of `DynamicJaxprTrace.sublift`. The change here was just a one liner to call `DynamicJaxprTrace._lift_tracers_in_aval`. * In pjit.py, we had to plumb through the `abstracted_axes` API option, and use it in the call to the new helper `_flat_axes_specs` which resolves the `abstracted_axes` user annotations to the internal abstracted axis size specification. We then use that specification to determine the input type on which to trace the `jit`decorated Python callable (and the corresponding jaxpr's input type) by calling the existing utility `partial_eval.infer_lambda_input_type`. * Also in pjit.py, we use that input type `in_type: core.InputType` in place of the tuple of avals `global_in_avals: core.AbstractValue`. The previous `global_in_avals`, representing things like `(f32[3], f32[4, 5])`, was sufficient to represent statically shaped input types on which to trace `jit`decorated functions. But with dynamic shapes we need to generalize that to represent things more like `(n:i32[], f32[n], f32[4,5])`. That is, a tuple of avals is no longer sufficient. But it's useful to remember that the logic is staying the same: we always had to trace on an input type, it's just that dynamic shape input types are more general. * The generalization to `core.InputType` has a corresponding generalization of the calling convention for the `pjit_p` primitive: if we apply `jit(f)(x)` for some dynamicallyshaped `x` (dynamically shaped due to some outer `jit`), we need to build an input type with axis size arguments, like `(n:i32[], f32[n])`, and the convention is that the `pjit_p` primitive needs to be applied to _two_ arguments, corresponding to the pair `(x.shape[0], x)`. That is, with dynamic shapes, we need to automatically extract implicittousercode axis size arguments. That's also done in pjit.py, in the function `_extract_implicit_args`, which is like an intialstyle analogue of the preexisting `partial_eval._extract_implicit_args` utility for finalstyle. * The staging rule for `pjit_p` also needs to be generalized, mainly to be able to produce output `Tracer`s with `Tracer`s in their shapes, i.e. to be able to produce dynamicallyshaped outputs. All of the changes are guarded by the `jax.config.jax_dynamic_shapes` flag, so unless I've made a mistake we shouldn't be breaking anyone's code.",2023-03-24T00:48:56Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15187
276,"以下是一个github上的jax下的一个issue, 标题是(Guard ArrayImpl checks by xla_extension_version.)， 内容是 (Guard ArrayImpl checks by xla_extension_version.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Guard ArrayImpl checks by xla_extension_version.,Guard ArrayImpl checks by xla_extension_version.,2023-03-23T21:09:24Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15181
2364,"以下是一个github上的jax下的一个issue, 标题是(improve scan error messages)， 内容是 ( sniped me into this... Consider: ```python import jax import jax.numpy as jnp jax.lax.scan(lambda x, _: (({'a': x[0]['a'], 'b': x[0]['a']},) * 2, None), ({'a': jnp.array(0, 'int32')},) * 2, None, length=1) ``` Before (hardwrapped to 80 columns for github): ``` TypeError: scan carry output and input must have same type structure, got PyTreeDef(({'a': *, 'b': *}, {'a': *, 'b': *})) and PyTreeDef(({'a': *}, {'a': *})). ``` After this PR (again hardwrapped manually): ``` TypeError: Scanned function carry input and carry output must have the same pytree structure, but they differ:   * the input carry component x[0] is a  with 1 child but the   corresponding component of the carry output is a  with 2   children, so the numbers of children do not match, with the symmetric   difference of key sets: {'b'}   * the input carry component x[1] is a  with 1 child but the   corresponding component of the carry output is a  with 2   children, so the numbers of children do not match, with the symmetric   difference of key sets: {'b'} Revise the scanned function so that its output is a pair where the first element has the same pytree structure as the first argument. ``` Also consider: ```python jax.lax.scan(lambda x, _: ((x[0].astype('float32'), x[1].astype('float32').reshape(1, 1)), None), (jnp.array(0, 'int32'),) * 2, None, length=1) ``` Before: ``` TypeError: scan carry output and input must have identical types, got ('DIFFERENT ShapedArray(float32[]) vs. ShapedArray(int32[])', 'DIFFERENT ShapedArray(float32[1,1]) vs. ShapedArray(int32[])'). ``` After this PR: ``` TypeError: Scanned function carry input and carry output must have equal types (e.g. shapes and dtypes of arrays), but they differ:   * the input carry component x[0] has type int32[] but the corresponding output   carry component has type float32[], so the dtypes do not match   * the input carry component x[1] has type int32[] but the corresponding output   carry component has type float32[1,1], so the dtypes do not match and also   the shapes do not match Revise the scanned function so that all output types (e.g. shapes and dtypes) match the corresponding input types. ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,improve scan error messages," sniped me into this... Consider: ```python import jax import jax.numpy as jnp jax.lax.scan(lambda x, _: (({'a': x[0]['a'], 'b': x[0]['a']},) * 2, None), ({'a': jnp.array(0, 'int32')},) * 2, None, length=1) ``` Before (hardwrapped to 80 columns for github): ``` TypeError: scan carry output and input must have same type structure, got PyTreeDef(({'a': *, 'b': *}, {'a': *, 'b': *})) and PyTreeDef(({'a': *}, {'a': *})). ``` After this PR (again hardwrapped manually): ``` TypeError: Scanned function carry input and carry output must have the same pytree structure, but they differ:   * the input carry component x[0] is a  with 1 child but the   corresponding component of the carry output is a  with 2   children, so the numbers of children do not match, with the symmetric   difference of key sets: {'b'}   * the input carry component x[1] is a  with 1 child but the   corresponding component of the carry output is a  with 2   children, so the numbers of children do not match, with the symmetric   difference of key sets: {'b'} Revise the scanned function so that its output is a pair where the first element has the same pytree structure as the first argument. ``` Also consider: ```python jax.lax.scan(lambda x, _: ((x[0].astype('float32'), x[1].astype('float32').reshape(1, 1)), None), (jnp.array(0, 'int32'),) * 2, None, length=1) ``` Before: ``` TypeError: scan carry output and input must have identical types, got ('DIFFERENT ShapedArray(float32[]) vs. ShapedArray(int32[])', 'DIFFERENT ShapedArray(float32[1,1]) vs. ShapedArray(int32[])'). ``` After this PR: ``` TypeError: Scanned function carry input and carry output must have equal types (e.g. shapes and dtypes of arrays), but they differ:   * the input carry component x[0] has type int32[] but the corresponding output   carry component has type float32[], so the dtypes do not match   * the input carry component x[1] has type int32[] but the corresponding output   carry component has type float32[1,1], so the dtypes do not match and also   the shapes do not match Revise the scanned function so that all output types (e.g. shapes and dtypes) match the corresponding input types. ```",2023-03-23T20:43:29Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15178
3354,"以下是一个github上的jax下的一个issue, 标题是(Scan on forward fn blows TPU memory when not using unroll)， 内容是 ( Description Hi! I am using a jax.lax.scan over the inference call of a transformer model on a TPU v48. The function I am calling is: ``` def decode_greedy(     init_tokens_ids,     random_key,     params: hk.Params,     apply_fn: hk.Transformed.apply,     num_tokens_to_decode: int,     eos_token_id: int, ):     batch_size, init_seq_length = init_tokens_ids.shape[0], init_tokens_ids.shape[1]     complete_tokens_ids = jnp.full(         shape=(batch_size, num_tokens_to_decode),         fill_value=eos_token_id,     )     tokens_ids = jnp.concatenate(         [             init_tokens_ids,             complete_tokens_ids,         ],         axis=1,     )     def scan_loop(carry_tokens_ids, time_step):   type: ignore         logits = apply_fn(params, random_key, carry_tokens_ids)[""logits""]         logits = logits[:, time_step + init_seq_length  1, :]         new_token_id = jnp.argmax(logits, axis=1)         carry_tokens_ids = carry_tokens_ids.at[:, time_step + init_seq_length].set(             new_token_id         )         return carry_tokens_ids, None     tokens_ids, _ = jax.lax.scan(         f=scan_loop,          init=tokens_ids,          xs=jnp.arange(num_tokens_to_decode),          unroll=num_tokens_to_decode     )     return tokens_ids, random_key ``` that function is then called inside a jax.pmap over the TPU devices. ``` decode_greedy_fn = functools.partial(     decode_greedy,     apply_fn=apply_fn,     num_tokens_to_decode=16,     eos_token_id=tokenizer.eos_token_id ) decode_greedy_fn = jax.pmap(decode_greedy_fn, axis_name=""batch"", devices=devices) ``` The call of this function works when using the unroll argument (with a long compilation time though as expected). However, when not using the unroll argument, it blows the TPU memory ``` XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 33.44G of 30.75G hbm. Exceeded hbm capacity by 2.69G. Total hbm usage >= 34.69G:     reserved          1.25G      program          10.90G      arguments        22.54G  Output size 5.0K; shares 0B with arguments. Program hbm requirement 10.90G:     global            52.0K     HLO temp         10.90G (100.0% utilization: Unpadded (10.89G) Padded (10.89G), 0.1% fragmentation (5.60M))   Largest program allocations in hbm:   1. Size: 394.00M      Shape: bf16[4096,50400]{1,0:T(8,128)(2,1)}      Unpadded size: 393.75M      Extra memory due to padding: 256.0K (1.0x expansion)      XLA label: copy.1211 = copy(Arg_284.285)      Allocation type: HLO temp      ==========================   ```   which I think is unexpected as the function should not to take more memory because called inside a scan.   I tried to implement it with a python for loop (which is similar to using the unroll argument if I understand correctly) and it works with also a long compilation time, however, using a jax.lax.fori_loop or jax.lax.while_loop gives the same error.   What jax/jaxlib version are you using? v0.4.6 (get same error on v0.3.25)  Which accelerator(s) are you using? TPU v48  Additional system info Python 3.8  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Scan on forward fn blows TPU memory when not using unroll," Description Hi! I am using a jax.lax.scan over the inference call of a transformer model on a TPU v48. The function I am calling is: ``` def decode_greedy(     init_tokens_ids,     random_key,     params: hk.Params,     apply_fn: hk.Transformed.apply,     num_tokens_to_decode: int,     eos_token_id: int, ):     batch_size, init_seq_length = init_tokens_ids.shape[0], init_tokens_ids.shape[1]     complete_tokens_ids = jnp.full(         shape=(batch_size, num_tokens_to_decode),         fill_value=eos_token_id,     )     tokens_ids = jnp.concatenate(         [             init_tokens_ids,             complete_tokens_ids,         ],         axis=1,     )     def scan_loop(carry_tokens_ids, time_step):   type: ignore         logits = apply_fn(params, random_key, carry_tokens_ids)[""logits""]         logits = logits[:, time_step + init_seq_length  1, :]         new_token_id = jnp.argmax(logits, axis=1)         carry_tokens_ids = carry_tokens_ids.at[:, time_step + init_seq_length].set(             new_token_id         )         return carry_tokens_ids, None     tokens_ids, _ = jax.lax.scan(         f=scan_loop,          init=tokens_ids,          xs=jnp.arange(num_tokens_to_decode),          unroll=num_tokens_to_decode     )     return tokens_ids, random_key ``` that function is then called inside a jax.pmap over the TPU devices. ``` decode_greedy_fn = functools.partial(     decode_greedy,     apply_fn=apply_fn,     num_tokens_to_decode=16,     eos_token_id=tokenizer.eos_token_id ) decode_greedy_fn = jax.pmap(decode_greedy_fn, axis_name=""batch"", devices=devices) ``` The call of this function works when using the unroll argument (with a long compilation time though as expected). However, when not using the unroll argument, it blows the TPU memory ``` XlaRuntimeError: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 33.44G of 30.75G hbm. Exceeded hbm capacity by 2.69G. Total hbm usage >= 34.69G:     reserved          1.25G      program          10.90G      arguments        22.54G  Output size 5.0K; shares 0B with arguments. Program hbm requirement 10.90G:     global            52.0K     HLO temp         10.90G (100.0% utilization: Unpadded (10.89G) Padded (10.89G), 0.1% fragmentation (5.60M))   Largest program allocations in hbm:   1. Size: 394.00M      Shape: bf16[4096,50400]{1,0:T(8,128)(2,1)}      Unpadded size: 393.75M      Extra memory due to padding: 256.0K (1.0x expansion)      XLA label: copy.1211 = copy(Arg_284.285)      Allocation type: HLO temp      ==========================   ```   which I think is unexpected as the function should not to take more memory because called inside a scan.   I tried to implement it with a python for loop (which is similar to using the unroll argument if I understand correctly) and it works with also a long compilation time, however, using a jax.lax.fori_loop or jax.lax.while_loop gives the same error.   What jax/jaxlib version are you using? v0.4.6 (get same error on v0.3.25)  Which accelerator(s) are you using? TPU v48  Additional system info Python 3.8  NVIDIA GPU info _No response_",2023-03-23T19:19:26Z,bug,open,2,2,https://github.com/jax-ml/jax/issues/15176,"Thanks for the question! Are you doing any autodiff, or is this pure inference?",Thanks for the quick reply! This is pure inference.
7755,"以下是一个github上的jax下的一个issue, 标题是(Jax on Apple Silicon GPU)， 内容是 (Hi, I'm looking to get JAX to work on an Apple M2 GPU. Is this possible? I was able to configure JAX to work on the M2 CPU by configuring a conda environment with the following `.yaml` file. I then installed JAX with `pip install jax jaxlib` after running `conda activate` to activate that conda environment. **YAML file** ``` name: jax channels:    condaforge    defaults dependencies:    anyio=3.6.2=pyhd8ed1ab_0    appnope=0.1.3=pyhd8ed1ab_0    argon2cffi=21.3.0=pyhd8ed1ab_0    argon2cffibindings=21.2.0=py310h8e9501a_3    asttokens=2.2.1=pyhd8ed1ab_0    attrs=22.2.0=pyh71513ae_0    backcall=0.2.0=pyh9f0ad1d_0    backports=1.0=pyhd8ed1ab_3    backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0    beautifulsoup4=4.12.0=pyha770c72_0    bleach=6.0.0=pyhd8ed1ab_0    boto3=1.26.94=pyhd8ed1ab_0    botocore=1.29.94=pyhd8ed1ab_0    brotli=1.0.9=h1a8c8d9_8    brotlibin=1.0.9=h1a8c8d9_8    brotlipy=0.7.0=py310h8e9501a_1005    bzip2=1.0.8=h3422bc3_4    cares=1.18.1=h3422bc3_0    cacertificates=2023.01.10=hca03da5_0    cachedproperty=1.5.2=hd8ed1ab_1    cached_property=1.5.2=pyha770c72_1    certifi=2022.12.7=py310hca03da5_0    cffi=1.15.1=py310h2399d43_3    charsetnormalizer=2.1.1=pyhd8ed1ab_0    click=8.1.3=unix_pyhd8ed1ab_2    colorama=0.4.6=pyhd8ed1ab_0    comm=0.1.2=pyhd8ed1ab_0    contourpy=1.0.7=py310h2887b22_0    cryptography=39.0.2=py310hfc83b78_0    cycler=0.11.0=pyhd8ed1ab_0    debugpy=1.6.6=py310h0f1eb42_0    decorator=5.1.1=pyhd8ed1ab_0    defusedxml=0.7.1=pyhd8ed1ab_0    entrypoints=0.4=pyhd8ed1ab_0    executing=1.2.0=pyhd8ed1ab_0    flask=2.2.3=pyhd8ed1ab_0    flitcore=3.8.0=pyhd8ed1ab_0    fonttools=4.39.2=py310h8e9501a_0    freetype=2.12.1=hd633e50_1    h5py=3.8.0=nompi_py310h3c889c5_101    hdf5=1.14.0=nompi_h6b85c65_103    icu=70.1=h6b3803e_0    idna=3.4=pyhd8ed1ab_0    importlibmetadata=6.1.0=pyha770c72_0    importlib_metadata=6.1.0=hd8ed1ab_0    importlib_resources=5.12.0=pyhd8ed1ab_0    ipykernel=6.19.2=py310h33ce5c2_0    ipython=8.11.0=pyhd1c38e8_0    ipython_genutils=0.2.0=py_1    ipywidgets=8.0.4=pyhd8ed1ab_0    itsdangerous=2.1.2=pyhd8ed1ab_0    jedi=0.18.2=pyhd8ed1ab_0    jinja2=3.1.2=pyhd8ed1ab_1    jmespath=1.0.1=pyhd8ed1ab_0    joblib=1.2.0=pyhd8ed1ab_0    jsonschema=4.17.3=pyhd8ed1ab_0    jupyter=1.0.0=py310hbe9552e_8    jupyter_client=8.0.3=pyhd8ed1ab_0    jupyter_console=6.6.3=pyhd8ed1ab_0    jupyter_core=5.3.0=py310hbe9552e_0    jupyter_events=0.6.3=pyhd8ed1ab_0    jupyter_server=2.5.0=pyhd8ed1ab_0    jupyter_server_terminals=0.4.4=pyhd8ed1ab_1    jupyterlab_pygments=0.2.2=pyhd8ed1ab_0    jupyterlab_widgets=3.0.5=pyhd8ed1ab_0    kiwisolver=1.4.4=py310h2887b22_1    krb5=1.20.1=h69eda48_0    lcms2=2.15=hd835a16_1    lerc=4.0.0=h9a09cb3_0    libaec=1.0.6=hb7217d7_1    libblas=3.9.0=16_osxarm64_openblas    libbrotlicommon=1.0.9=h1a8c8d9_8    libbrotlidec=1.0.9=h1a8c8d9_8    libbrotlienc=1.0.9=h1a8c8d9_8    libcblas=3.9.0=16_osxarm64_openblas    libcurl=7.88.1=h9049daf_1    libcxx=15.0.7=h75e25f2_0    libdeflate=1.17=h1a8c8d9_0    libedit=3.1.20191231=hc8eb9b7_2    libev=4.33=h642e427_1    libffi=3.4.2=h3422bc3_5    libgfortran=5.0.0=12_2_0_hd922786_31    libgfortran5=12.2.0=h0eea778_31    libiconv=1.17=he4db4b2_0    libjpegturbo=2.1.5.1=h1a8c8d9_0    liblapack=3.9.0=16_osxarm64_openblas    libnghttp2=1.52.0=hae82a92_0    libopenblas=0.3.21=openmp_hc731615_3    libpng=1.6.39=h76d750c_0    libsodium=1.0.18=h27ca646_1    libsqlite=3.40.0=h76d750c_0    libssh2=1.10.0=h7a5bd25_3    libtiff=4.5.0=hdc14d85_5    libwebpbase=1.3.0=h1a8c8d9_0    libxcb=1.13=h9b22ae9_1004    libxml2=2.10.3=h67585b2_4    libxslt=1.1.37=h1bd8bc4_0    libzlib=1.2.13=h03a7124_4    llvmopenmp=16.0.0=h7cfbb63_0    lxml=4.9.2=py310h85b680a_0    markupsafe=2.1.2=py310h8e9501a_0    matplotlib=3.7.1=py310hb6292c7_0    matplotlibbase=3.7.1=py310h78c5c2f_0    matplotlibinline=0.1.6=pyhd8ed1ab_0    mistune=2.0.5=pyhd8ed1ab_0    munkres=1.1.4=pyh9f0ad1d_0    nbclassic=0.5.3=pyhb4ecaf3_3    nbclient=0.7.2=pyhd8ed1ab_0    nbconvert=7.2.9=pyhd8ed1ab_0    nbconvertcore=7.2.9=pyhd8ed1ab_0    nbconvertpandoc=7.2.9=pyhd8ed1ab_0    nbformat=5.8.0=pyhd8ed1ab_0    ncurses=6.3=h07bb92c_1    nestasyncio=1.5.6=pyhd8ed1ab_0    notebook=6.5.3=pyha770c72_0    notebookshim=0.2.2=pyhd8ed1ab_0    numpy=1.24.2=py310h3d2048e_0    openjpeg=2.5.0=hbc2ba62_2    openssl=3.1.0=h03a7124_0    packaging=23.0=pyhd8ed1ab_0    pandas=1.5.3=py310h2b830bf_0    pandasdatareader=0.10.0=pyh6c4a22f_0    pandoc=2.19.2=hce30654_2    pandocfilters=1.5.0=pyhd8ed1ab_0    parso=0.8.3=pyhd8ed1ab_0    pexpect=4.8.0=pyh1a96a4e_2    pickleshare=0.7.5=py_1003    pillow=9.4.0=py310h07496d3_2    pip=23.0.1=pyhd8ed1ab_0    pkgutilresolvename=1.3.10=pyhd8ed1ab_0    platformdirs=3.1.1=pyhd8ed1ab_0    pooch=1.7.0=pyhd8ed1ab_0    prometheus_client=0.16.0=pyhd8ed1ab_0    prompttoolkit=3.0.38=pyha770c72_0    prompt_toolkit=3.0.38=hd8ed1ab_0    psutil=5.9.4=py310h8e9501a_0    pthreadstubs=0.4=h27ca646_1001    ptyprocess=0.7.0=pyhd3deb0d_0    pure_eval=0.2.2=pyhd8ed1ab_0    pycparser=2.21=pyhd8ed1ab_0    pygments=2.14.0=pyhd8ed1ab_0    pyopenssl=23.0.0=pyhd8ed1ab_0    pyparsing=3.0.9=pyhd8ed1ab_0    pyrsistent=0.19.3=py310h8e9501a_0    pysocks=1.7.1=pyha2e5f31_6    python=3.10.9=h3ba56d0_0_cpython    pythondateutil=2.8.2=pyhd8ed1ab_0    pythonfastjsonschema=2.16.3=pyhd8ed1ab_0    pythonjsonlogger=2.0.7=pyhd8ed1ab_0    python_abi=3.10=3_cp310    pytz=2022.7.1=pyhd8ed1ab_0    pyyaml=6.0=py310h8e9501a_5    pyzmq=25.0.2=py310hc407298_0    readline=8.1.2=h46ed386_0    requests=2.28.2=pyhd8ed1ab_0    rfc3339validator=0.1.4=pyhd8ed1ab_0    rfc3986validator=0.1.1=pyh9f0ad1d_0    s3transfer=0.6.0=pyhd8ed1ab_0    scikitlearn=1.2.2=py310ha00a7cd_0    scipy=1.10.1=py310ha0d8a01_0    send2trash=1.8.0=pyhd8ed1ab_0    setuptools=67.6.0=pyhd8ed1ab_0    six=1.16.0=pyh6c4a22f_0    sniffio=1.3.0=pyhd8ed1ab_0    soupsieve=2.3.2.post1=pyhd8ed1ab_0    stack_data=0.6.2=pyhd8ed1ab_0    terminado=0.17.1=pyhd1c38e8_0    threadpoolctl=3.1.0=pyh8a188c0_0    tinycss2=1.2.1=pyhd8ed1ab_0    tk=8.6.12=he1e0b03_0    tornado=6.2=py310h8e9501a_1    tqdm=4.65.0=pyhd8ed1ab_1    traitlets=5.9.0=pyhd8ed1ab_0    typingextensions=4.5.0=hd8ed1ab_0    typing_extensions=4.5.0=pyha770c72_0    tzdata=2022g=h191b570_0    unicodedata2=15.0.0=py310h8e9501a_0    urllib3=1.26.15=pyhd8ed1ab_0    wcwidth=0.2.6=pyhd8ed1ab_0    webencodings=0.5.1=py_1    websocketclient=1.5.1=pyhd8ed1ab_0    werkzeug=2.2.3=pyhd8ed1ab_0    wheel=0.40.0=pyhd8ed1ab_0    widgetsnbextension=4.0.5=pyhd8ed1ab_0    xorglibxau=1.0.9=h27ca646_0    xorglibxdmcp=1.1.3=h27ca646_0    xz=5.2.6=h57fd34a_0    yaml=0.2.5=h3422bc3_2    zeromq=4.3.4=hbdafb3b_1    zipp=3.15.0=pyhd8ed1ab_0    zstd=1.5.2=hf913c23_6    pip:        abslpy==1.4.0        astunparse==1.6.3        bayesianoptimization==1.4.2        cachetools==5.3.0        cloudpickle==2.2.1        flatbuffers==23.3.3        gast==0.4.0        googleauth==2.16.2        googleauthoauthlib==0.4.6        googlepasta==0.2.0        grpcio==1.51.3        gym==0.26.2        gymnotices==0.0.8        kaggle==1.5.13        keras==2.11.0        libclang==15.0.6.1        markdown==3.4.1        oauthlib==3.2.2        opteinsum==3.3.0        protobuf==3.19.6        pyasn1==0.4.8        pyasn1modules==0.2.8        pythonslugify==8.0.1        requestsoauthlib==1.3.1        rsa==4.9        tensorboard==2.11.2        tensorboarddataserver==0.6.1        tensorboardpluginwit==1.8.1        tensorflowestimator==2.11.0        tensorflowmacos==2.11.0        tensorflowmetal==0.7.1        termcolor==2.2.0        textunidecode==1.3        wrapt==1.15.0 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax on Apple Silicon GPU,"Hi, I'm looking to get JAX to work on an Apple M2 GPU. Is this possible? I was able to configure JAX to work on the M2 CPU by configuring a conda environment with the following `.yaml` file. I then installed JAX with `pip install jax jaxlib` after running `conda activate` to activate that conda environment. **YAML file** ``` name: jax channels:    condaforge    defaults dependencies:    anyio=3.6.2=pyhd8ed1ab_0    appnope=0.1.3=pyhd8ed1ab_0    argon2cffi=21.3.0=pyhd8ed1ab_0    argon2cffibindings=21.2.0=py310h8e9501a_3    asttokens=2.2.1=pyhd8ed1ab_0    attrs=22.2.0=pyh71513ae_0    backcall=0.2.0=pyh9f0ad1d_0    backports=1.0=pyhd8ed1ab_3    backports.functools_lru_cache=1.6.4=pyhd8ed1ab_0    beautifulsoup4=4.12.0=pyha770c72_0    bleach=6.0.0=pyhd8ed1ab_0    boto3=1.26.94=pyhd8ed1ab_0    botocore=1.29.94=pyhd8ed1ab_0    brotli=1.0.9=h1a8c8d9_8    brotlibin=1.0.9=h1a8c8d9_8    brotlipy=0.7.0=py310h8e9501a_1005    bzip2=1.0.8=h3422bc3_4    cares=1.18.1=h3422bc3_0    cacertificates=2023.01.10=hca03da5_0    cachedproperty=1.5.2=hd8ed1ab_1    cached_property=1.5.2=pyha770c72_1    certifi=2022.12.7=py310hca03da5_0    cffi=1.15.1=py310h2399d43_3    charsetnormalizer=2.1.1=pyhd8ed1ab_0    click=8.1.3=unix_pyhd8ed1ab_2    colorama=0.4.6=pyhd8ed1ab_0    comm=0.1.2=pyhd8ed1ab_0    contourpy=1.0.7=py310h2887b22_0    cryptography=39.0.2=py310hfc83b78_0    cycler=0.11.0=pyhd8ed1ab_0    debugpy=1.6.6=py310h0f1eb42_0    decorator=5.1.1=pyhd8ed1ab_0    defusedxml=0.7.1=pyhd8ed1ab_0    entrypoints=0.4=pyhd8ed1ab_0    executing=1.2.0=pyhd8ed1ab_0    flask=2.2.3=pyhd8ed1ab_0    flitcore=3.8.0=pyhd8ed1ab_0    fonttools=4.39.2=py310h8e9501a_0    freetype=2.12.1=hd633e50_1    h5py=3.8.0=nompi_py310h3c889c5_101    hdf5=1.14.0=nompi_h6b85c65_103    icu=70.1=h6b3803e_0    idna=3.4=pyhd8ed1ab_0    importlibmetadata=6.1.0=pyha770c72_0    importlib_metadata=6.1.0=hd8ed1ab_0    importlib_resources=5.12.0=pyhd8ed1ab_0    ipykernel=6.19.2=py310h33ce5c2_0    ipython=8.11.0=pyhd1c38e8_0    ipython_genutils=0.2.0=py_1    ipywidgets=8.0.4=pyhd8ed1ab_0    itsdangerous=2.1.2=pyhd8ed1ab_0    jedi=0.18.2=pyhd8ed1ab_0    jinja2=3.1.2=pyhd8ed1ab_1    jmespath=1.0.1=pyhd8ed1ab_0    joblib=1.2.0=pyhd8ed1ab_0    jsonschema=4.17.3=pyhd8ed1ab_0    jupyter=1.0.0=py310hbe9552e_8    jupyter_client=8.0.3=pyhd8ed1ab_0    jupyter_console=6.6.3=pyhd8ed1ab_0    jupyter_core=5.3.0=py310hbe9552e_0    jupyter_events=0.6.3=pyhd8ed1ab_0    jupyter_server=2.5.0=pyhd8ed1ab_0    jupyter_server_terminals=0.4.4=pyhd8ed1ab_1    jupyterlab_pygments=0.2.2=pyhd8ed1ab_0    jupyterlab_widgets=3.0.5=pyhd8ed1ab_0    kiwisolver=1.4.4=py310h2887b22_1    krb5=1.20.1=h69eda48_0    lcms2=2.15=hd835a16_1    lerc=4.0.0=h9a09cb3_0    libaec=1.0.6=hb7217d7_1    libblas=3.9.0=16_osxarm64_openblas    libbrotlicommon=1.0.9=h1a8c8d9_8    libbrotlidec=1.0.9=h1a8c8d9_8    libbrotlienc=1.0.9=h1a8c8d9_8    libcblas=3.9.0=16_osxarm64_openblas    libcurl=7.88.1=h9049daf_1    libcxx=15.0.7=h75e25f2_0    libdeflate=1.17=h1a8c8d9_0    libedit=3.1.20191231=hc8eb9b7_2    libev=4.33=h642e427_1    libffi=3.4.2=h3422bc3_5    libgfortran=5.0.0=12_2_0_hd922786_31    libgfortran5=12.2.0=h0eea778_31    libiconv=1.17=he4db4b2_0    libjpegturbo=2.1.5.1=h1a8c8d9_0    liblapack=3.9.0=16_osxarm64_openblas    libnghttp2=1.52.0=hae82a92_0    libopenblas=0.3.21=openmp_hc731615_3    libpng=1.6.39=h76d750c_0    libsodium=1.0.18=h27ca646_1    libsqlite=3.40.0=h76d750c_0    libssh2=1.10.0=h7a5bd25_3    libtiff=4.5.0=hdc14d85_5    libwebpbase=1.3.0=h1a8c8d9_0    libxcb=1.13=h9b22ae9_1004    libxml2=2.10.3=h67585b2_4    libxslt=1.1.37=h1bd8bc4_0    libzlib=1.2.13=h03a7124_4    llvmopenmp=16.0.0=h7cfbb63_0    lxml=4.9.2=py310h85b680a_0    markupsafe=2.1.2=py310h8e9501a_0    matplotlib=3.7.1=py310hb6292c7_0    matplotlibbase=3.7.1=py310h78c5c2f_0    matplotlibinline=0.1.6=pyhd8ed1ab_0    mistune=2.0.5=pyhd8ed1ab_0    munkres=1.1.4=pyh9f0ad1d_0    nbclassic=0.5.3=pyhb4ecaf3_3    nbclient=0.7.2=pyhd8ed1ab_0    nbconvert=7.2.9=pyhd8ed1ab_0    nbconvertcore=7.2.9=pyhd8ed1ab_0    nbconvertpandoc=7.2.9=pyhd8ed1ab_0    nbformat=5.8.0=pyhd8ed1ab_0    ncurses=6.3=h07bb92c_1    nestasyncio=1.5.6=pyhd8ed1ab_0    notebook=6.5.3=pyha770c72_0    notebookshim=0.2.2=pyhd8ed1ab_0    numpy=1.24.2=py310h3d2048e_0    openjpeg=2.5.0=hbc2ba62_2    openssl=3.1.0=h03a7124_0    packaging=23.0=pyhd8ed1ab_0    pandas=1.5.3=py310h2b830bf_0    pandasdatareader=0.10.0=pyh6c4a22f_0    pandoc=2.19.2=hce30654_2    pandocfilters=1.5.0=pyhd8ed1ab_0    parso=0.8.3=pyhd8ed1ab_0    pexpect=4.8.0=pyh1a96a4e_2    pickleshare=0.7.5=py_1003    pillow=9.4.0=py310h07496d3_2    pip=23.0.1=pyhd8ed1ab_0    pkgutilresolvename=1.3.10=pyhd8ed1ab_0    platformdirs=3.1.1=pyhd8ed1ab_0    pooch=1.7.0=pyhd8ed1ab_0    prometheus_client=0.16.0=pyhd8ed1ab_0    prompttoolkit=3.0.38=pyha770c72_0    prompt_toolkit=3.0.38=hd8ed1ab_0    psutil=5.9.4=py310h8e9501a_0    pthreadstubs=0.4=h27ca646_1001    ptyprocess=0.7.0=pyhd3deb0d_0    pure_eval=0.2.2=pyhd8ed1ab_0    pycparser=2.21=pyhd8ed1ab_0    pygments=2.14.0=pyhd8ed1ab_0    pyopenssl=23.0.0=pyhd8ed1ab_0    pyparsing=3.0.9=pyhd8ed1ab_0    pyrsistent=0.19.3=py310h8e9501a_0    pysocks=1.7.1=pyha2e5f31_6    python=3.10.9=h3ba56d0_0_cpython    pythondateutil=2.8.2=pyhd8ed1ab_0    pythonfastjsonschema=2.16.3=pyhd8ed1ab_0    pythonjsonlogger=2.0.7=pyhd8ed1ab_0    python_abi=3.10=3_cp310    pytz=2022.7.1=pyhd8ed1ab_0    pyyaml=6.0=py310h8e9501a_5    pyzmq=25.0.2=py310hc407298_0    readline=8.1.2=h46ed386_0    requests=2.28.2=pyhd8ed1ab_0    rfc3339validator=0.1.4=pyhd8ed1ab_0    rfc3986validator=0.1.1=pyh9f0ad1d_0    s3transfer=0.6.0=pyhd8ed1ab_0    scikitlearn=1.2.2=py310ha00a7cd_0    scipy=1.10.1=py310ha0d8a01_0    send2trash=1.8.0=pyhd8ed1ab_0    setuptools=67.6.0=pyhd8ed1ab_0    six=1.16.0=pyh6c4a22f_0    sniffio=1.3.0=pyhd8ed1ab_0    soupsieve=2.3.2.post1=pyhd8ed1ab_0    stack_data=0.6.2=pyhd8ed1ab_0    terminado=0.17.1=pyhd1c38e8_0    threadpoolctl=3.1.0=pyh8a188c0_0    tinycss2=1.2.1=pyhd8ed1ab_0    tk=8.6.12=he1e0b03_0    tornado=6.2=py310h8e9501a_1    tqdm=4.65.0=pyhd8ed1ab_1    traitlets=5.9.0=pyhd8ed1ab_0    typingextensions=4.5.0=hd8ed1ab_0    typing_extensions=4.5.0=pyha770c72_0    tzdata=2022g=h191b570_0    unicodedata2=15.0.0=py310h8e9501a_0    urllib3=1.26.15=pyhd8ed1ab_0    wcwidth=0.2.6=pyhd8ed1ab_0    webencodings=0.5.1=py_1    websocketclient=1.5.1=pyhd8ed1ab_0    werkzeug=2.2.3=pyhd8ed1ab_0    wheel=0.40.0=pyhd8ed1ab_0    widgetsnbextension=4.0.5=pyhd8ed1ab_0    xorglibxau=1.0.9=h27ca646_0    xorglibxdmcp=1.1.3=h27ca646_0    xz=5.2.6=h57fd34a_0    yaml=0.2.5=h3422bc3_2    zeromq=4.3.4=hbdafb3b_1    zipp=3.15.0=pyhd8ed1ab_0    zstd=1.5.2=hf913c23_6    pip:        abslpy==1.4.0        astunparse==1.6.3        bayesianoptimization==1.4.2        cachetools==5.3.0        cloudpickle==2.2.1        flatbuffers==23.3.3        gast==0.4.0        googleauth==2.16.2        googleauthoauthlib==0.4.6        googlepasta==0.2.0        grpcio==1.51.3        gym==0.26.2        gymnotices==0.0.8        kaggle==1.5.13        keras==2.11.0        libclang==15.0.6.1        markdown==3.4.1        oauthlib==3.2.2        opteinsum==3.3.0        protobuf==3.19.6        pyasn1==0.4.8        pyasn1modules==0.2.8        pythonslugify==8.0.1        requestsoauthlib==1.3.1        rsa==4.9        tensorboard==2.11.2        tensorboarddataserver==0.6.1        tensorboardpluginwit==1.8.1        tensorflowestimator==2.11.0        tensorflowmacos==2.11.0        tensorflowmetal==0.7.1        termcolor==2.2.0        textunidecode==1.3        wrapt==1.15.0 ```",2023-03-23T11:28:32Z,enhancement,closed,1,5,https://github.com/jax-ml/jax/issues/15164,Thanks for the question. See the related discussion in CC(Hardware acceleration on Apple Silicon with Metal plugin)," I just got a pretty powerful M2 Max (w 38 GPU cores), and I really like to try JAX. But it seems the solutions on some of the “Issues” here are CPUs. "," Thanks for the response! I don't quite follow all the discussion in that thread, but am I right that as of now there is no way to do this?",">  I just got a pretty powerful M2 Max (w 38 GPU cores), and I really like to try JAX. But it seems the solutions on some of the “Issues” here are CPUs. Agreed  I haven't been able to find anything about JAX on these GPUs. However, it is possible to get Tensorflow running on M2 GPUs (the `.yaml` environment I created above works), so hopefully this will be coming to JAX in the future..",jaxmetal plugin is the package to accelerate JAX on these GPUs. 
286,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Fix tests broken by upgrade of XlaCallModule)， 内容是 ([jax2tf] Fix tests broken by upgrade of XlaCallModule)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Fix tests broken by upgrade of XlaCallModule,[jax2tf] Fix tests broken by upgrade of XlaCallModule,2023-03-23T09:08:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15161
11550,"以下是一个github上的jax下的一个issue, 标题是(Add initial implementation of a `run_state` primitive)， 内容是 ( Background  Side effects and types We often say JAX is inspired by functional programming when discussing the requirement that programs in JAX be *pure* to be transformed. However, taking more inspiration from functional programming, we could lift that requirement. Specifically, JAX could allow sideeffects, but only effects that JAX's type system can track and ones that have welldefined interactions with transformations. What does it mean to track effects in the type system? Consider how the Koka programming language incorporates effects into its type system: ```koka fun sqr    : (int) > total int       // total: mathematical total function     fun divide : (int,int) > exn int     // exn: may raise an exception (partial)   fun turing : (tape) > div int        // div: may not terminate (diverge)   fun print  : (string) > console ()   // console: may write to the console   fun rand   : () > ndet int           // ndet: nondeterministic ``` In Koka, the type of a function not only contains its input and output types, but also a list of effect types. For example, the print function has the `console` effect and the `randr` function has the `ndet` effect. Functions can have multiple effects, which often happens when effectful functions are composed. At the Jaxpr level, JAX currently similarly tracks effect types (specifically the effects field in Jaxprs is the set of side effects associated with the Jaxpr). We can add a primitive with the ndet effect pretty easily into JAX. ```python random_p = core.Primitive('random') .def_effectful_abstract_eval def _random_abstract_eval():   return core.ShapedArray((), jnp.float32), {""ndet""} def f():   return random_p.bind() jaxpr = jax.make_jaxpr(f)() print(jaxpr.effects)   ⇒ Prints {""ndet""} ``` The `ndet` effect in the previous example is unhandled, meaning that if we try evaluating this jaxpr or lowering it to MHLO, we will get a type error (something along the lines of “cannot run a primitive with an `ndet` effect” or “cannot lower a jaxpr with an `ndet` effect”). In order to transform this jaxpr into something practical (something we can run, or lower to MHLO), we “discharge” the effect with a transformation. For example, to discharge the `ndet` effect, we could convert the jaxpr into one that is deterministic but consumes a functional source of randomness, i.e. a `jax.random.PRNGKey`. We write the following “discharge” transformation: `run_ndet :: (a > {ndet} b) > (PRNGKey > a > b)` Pairing sideeffects with discharging transformations enables their usage with other parts of JAX. For example, we could now `vmap` the function returned from `run_ndet`.  The state effect from Dex JAX takes inspiration from Dex and Koka for its effect types. One of the effects from Dex that is of particular interest is the *state* effect. From the Dex prelude, we see that Dex offers a set of effectful functions (`get`, `:=` and `ask`) that each consume a special Ref type and emit a sideeffect. ```dex def Ref (r:Type) (a:Type) : Type = %Ref r a def get  {h s} (ref:Ref h s)       : {State h} s    = %get  ref def (:=) {h s} (ref:Ref h s) (x:s) : {State h} Unit = %put  ref x def ask  {h r} (ref:Ref h r)       : {Read  h} r    = %ask  ref ``` For example, when we pass a `Ref h s` into `get`, we are returned a value of type `s` and emit a `State h` effect.  Dex has already established how these effectful primitives are transposed, so we follow their lead in incorporating these stateful primitives into JAX.  Reference types and stateful primitives We add a new type to JAX and Jaxprs: `Ref`s. ``` data Ref = Ref Array ``` `Ref`s are basically wrapper around Array types that allow reading from and writing into the Array. Note that Arrays in JAX are immutable, so Refs offer mutation semantics. `Ref`s can be input to Jaxprs but not outputs. An example Jaxpr is: ``` { lambda ; a:Ref{float32[4]} b:Ref{bool[]} .   ...   in ... } ```  Why not make mutable arrays? Why not just make Arrays mutable? Well, we can index into an Array a giving us another Array b. If we mutate b does that also mutate a? Tracking these aliased arrays can be really tricky. We can circumvent this particular aliasing problem by making a single mutable type that produces immutable types when read from.  Effectful primitives Refs can only be manipulated/used with 3 new JAX primitives: `get`, `swap`, and `addupdate`. All of these primitives can be provided indices that are used to manipulate subarrays in the Ref. Like in NumPy, we can index Refs using Python integers, Python slice objects, and integer Arrays. ``` type Indices = Tuple[Int  eff} ()) > [a] > {eff} [a] ``` `run_state` doesn’t have any state effects corresponding to the created Refs, “discharging” the state effects. Note that other effects `{eff}` will remain (e.g. error effects, or state effects corresponding to outer Refs). An important point here is that we have not provided a way of creating Refs in “top level” user code. What we mean by this is that only run_state creates Refs, and there is no make_ref function.  Implementation `run_state` runs a Jaxpr interpreter (i.e. it's an initialstyle primitive) that keeps track of the values associated with Refs. When the interpreter hits an effectful primitive that operates on a Ref introduced by run_state, we run a discharge rule wherein we convert the stateful semantics into value semantics. For example, the `get` primitive can be discharged into `lax.dynamic_slice` or `lax.gather` depending on the index pattern. Similarly `swap` is discharged into a `dynamic_slice/gather` and `dynamic_update_slice/scatter`. `addupdate` is discharged into a `dynamic_slice/gather`, `add`, and `dynamic_update_slice/scatter`. `run_state` is also responsible for discharging higherorder primitives that contain state effects (for example, control flow primitives or nested jits). Note that `run_state` is only responsible for discharge effects corresponding to the Refs it introduces and no others.  Usage with control flow State primitives should *just work* with control flow primitives. You won’t, however, be able to pass Refs into them or return Refs from them. Reading from and writing into closedover Refs should be sufficient.  Examples without transformations  Implementing a simple pure function by discharging state effects Using only one readwrite Ref ```python def f(x_ref):   x = get(x_ref, ())   set(x_ref, (), x * 2) def mul2(x):   return run_state(f)(x) ``` Using readonly and writeonly Refs ```python def f(refs):   x_ref, y_ref = refs   x = get(x_ref, ())   set(y_ref, (), x * 2) def mul2(x):   return run_state(f)(x, 0)[1] ```  Array indexing ```python def f(x_ref, i_ref, v_ref):   i = get(i_ref, ())   set(x_ref, (i,), get(v_ref, ())) def set_at_index(x, i, v):   return run_state(f)(x, i, v)[0] set_at_index(jnp.arange(4), 2, 1)  ⇒ Returns [0, 1, 1, 3] ```  Nesting state effects Here’s an example where we nest two `run_state`s with nonoverlapping state effects ```python def square_ref(x_ref):   x = x_ref[()]   set(x_ref, (), jnp.square(x)) def f(x_ref, y_ref):   x = x_ref[()]   mul_x = run_state(square_ref)(x)   set(y_ref, (), mul_x) run_state(f)(2, 0)  ⇒ Returns (2, 4) ``` Here’s an example where we mutate an outer Ref from a nested run_state ```python def f(x_ref, y_ref):   def set_x(z_ref):      Mutating a closedover `Ref`     set(x_ref, (), get(z_ref, ()))   run_state(set_x)(4)   set(y_ref, (), get(x_ref, ())) run_state(f)(2, 0)  ⇒ Returns (4, 4) ```  Using state inside of control flow Here we write to a Ref inside of a `fori_loop` body. ```python def f(x_ref, y_ref):   def body(i, _):     set(y_ref, (i,), get(x_ref, (i,)) + 1)   lax.fori_loop(0, x_ref.shape[0], body, ()) run_state(f)(jnp.arange(4), jnp.zeros(4))[1]  ⇒ Returns [1., 2., 3., 4.] ``` Here we conditionally write to a Ref using a `lax.cond` ```python def f(x_ref):   def true_fun():     set(x_ref, (), 100)   def false_fun():     pass   x = get(x_ref, ())   lax.cond(x < 100, true_fun, false_fun) run_state(f)(5)    ⇒ Returns 100 run_state(f)(105)  ⇒ Returns 105 ```  Transformations of effectful functions In general, we disallow passing `Ref`s into higher order functions. They *must* be closed over, meaning we only need to take care of some cases under transformations. However, `run_state` should always be transformable by JAX functions, since it has a pure signature (it discharges away state effects).  Autodifferentiation If we are differentiating a function, we can safely assume there are no Ref inputs and therefore we are not differentiating with respect to any Refs. Functions we are differentiating that close over Refs and updating them is fair game. However, we will never need to apply transformation rules to the primitives in this case since they will only ever show up as purely primal values (no associated tangents). They will similarly not show up in the backwards pass so no transpose rule needed. In this example, we are mutating x_ref inside of a function we are taking `jax.grad` of. `x` is mutated in the forward pass and is untouched during the backward pass. ```python def f(x_ref):   def mul(x):     z = x_ref[()]     x_ref[()] += 1     return jnp.sin(x * z)   x = x_ref[()]   z = jax.grad(mul)(x)   x_ref[()] = z run_state(f)(2) ``` Differentiating through `run_state` is something we also support, but involves dealing with transforming the state primitives. When we are differentiating w.r.t. to values that are then passed into run_state to initialize Refs, we will need to differentiate through the body of run_state, which contains state sideeffects. With these rules in place, we can mechanically differentiate run_state with little modification to JAX’s core machinery.  Examples with transformations Differentiating through run_state ```python def f(x_ref):   x_ref[()] = jnp.sin(x_ref[()]) jax.grad(lambda x: run_state(f)(x))(4.) ⇒ Returns cos(4.) ``` Digging deeper into this, the function we are differentiating is this (in jaxpr representation): ``` { lambda ; a:f32[]. let     b:f32[] = run_state[       jaxpr={ lambda ; c:Ref{float32[]}. let           d:f32[] = get c ()           e:f32[] = sin d           _:f32[] = swap c () e         in () }     ] a   in (b,) } ``` When we take its gradient, we obtain the following function: ``` { lambda ; a:f32[]. let     _:f32[] b:f32[] = run_state[       jaxpr={ lambda ; c:Ref{float32[]} d:Ref{float32[]}. let           e:f32[] = get c ()           f:f32[] = sin e           g:f32[] = cos e           _:f32[] = swap c () f           _:f32[] = swap d () g         in () }     ] a     _:f32[], h:f32[] = run_state[       jaxpr={ lambda ; i:Ref{float32[]} j:Ref{float32[]}. let           k:f32[] = get i ()           l:f32[] = swap j () 0.0           m:f32[] = mul l k           addupdate j () m         in () }     ] b 1.0   in (h,) } ``` Notice that we have two `run_state`s in our gradient function. The first corresponds to the forward pass where we compute `sin(x)` and the residual term `cos(x)`. The residual term is passed into the second `run_state` and read into the variable k. We then perform the transpose where the `get` and `swap` in the first jaxpr are turned into an `addupdate` and `swap` respectively (and run in reverse). The end result is that we write `cos(x) * 1.` into `j` and return `cos(x)` from the function. Still TODO: vmap rule)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add initial implementation of a `run_state` primitive," Background  Side effects and types We often say JAX is inspired by functional programming when discussing the requirement that programs in JAX be *pure* to be transformed. However, taking more inspiration from functional programming, we could lift that requirement. Specifically, JAX could allow sideeffects, but only effects that JAX's type system can track and ones that have welldefined interactions with transformations. What does it mean to track effects in the type system? Consider how the Koka programming language incorporates effects into its type system: ```koka fun sqr    : (int) > total int       // total: mathematical total function     fun divide : (int,int) > exn int     // exn: may raise an exception (partial)   fun turing : (tape) > div int        // div: may not terminate (diverge)   fun print  : (string) > console ()   // console: may write to the console   fun rand   : () > ndet int           // ndet: nondeterministic ``` In Koka, the type of a function not only contains its input and output types, but also a list of effect types. For example, the print function has the `console` effect and the `randr` function has the `ndet` effect. Functions can have multiple effects, which often happens when effectful functions are composed. At the Jaxpr level, JAX currently similarly tracks effect types (specifically the effects field in Jaxprs is the set of side effects associated with the Jaxpr). We can add a primitive with the ndet effect pretty easily into JAX. ```python random_p = core.Primitive('random') .def_effectful_abstract_eval def _random_abstract_eval():   return core.ShapedArray((), jnp.float32), {""ndet""} def f():   return random_p.bind() jaxpr = jax.make_jaxpr(f)() print(jaxpr.effects)   ⇒ Prints {""ndet""} ``` The `ndet` effect in the previous example is unhandled, meaning that if we try evaluating this jaxpr or lowering it to MHLO, we will get a type error (something along the lines of “cannot run a primitive with an `ndet` effect” or “cannot lower a jaxpr with an `ndet` effect”). In order to transform this jaxpr into something practical (something we can run, or lower to MHLO), we “discharge” the effect with a transformation. For example, to discharge the `ndet` effect, we could convert the jaxpr into one that is deterministic but consumes a functional source of randomness, i.e. a `jax.random.PRNGKey`. We write the following “discharge” transformation: `run_ndet :: (a > {ndet} b) > (PRNGKey > a > b)` Pairing sideeffects with discharging transformations enables their usage with other parts of JAX. For example, we could now `vmap` the function returned from `run_ndet`.  The state effect from Dex JAX takes inspiration from Dex and Koka for its effect types. One of the effects from Dex that is of particular interest is the *state* effect. From the Dex prelude, we see that Dex offers a set of effectful functions (`get`, `:=` and `ask`) that each consume a special Ref type and emit a sideeffect. ```dex def Ref (r:Type) (a:Type) : Type = %Ref r a def get  {h s} (ref:Ref h s)       : {State h} s    = %get  ref def (:=) {h s} (ref:Ref h s) (x:s) : {State h} Unit = %put  ref x def ask  {h r} (ref:Ref h r)       : {Read  h} r    = %ask  ref ``` For example, when we pass a `Ref h s` into `get`, we are returned a value of type `s` and emit a `State h` effect.  Dex has already established how these effectful primitives are transposed, so we follow their lead in incorporating these stateful primitives into JAX.  Reference types and stateful primitives We add a new type to JAX and Jaxprs: `Ref`s. ``` data Ref = Ref Array ``` `Ref`s are basically wrapper around Array types that allow reading from and writing into the Array. Note that Arrays in JAX are immutable, so Refs offer mutation semantics. `Ref`s can be input to Jaxprs but not outputs. An example Jaxpr is: ``` { lambda ; a:Ref{float32[4]} b:Ref{bool[]} .   ...   in ... } ```  Why not make mutable arrays? Why not just make Arrays mutable? Well, we can index into an Array a giving us another Array b. If we mutate b does that also mutate a? Tracking these aliased arrays can be really tricky. We can circumvent this particular aliasing problem by making a single mutable type that produces immutable types when read from.  Effectful primitives Refs can only be manipulated/used with 3 new JAX primitives: `get`, `swap`, and `addupdate`. All of these primitives can be provided indices that are used to manipulate subarrays in the Ref. Like in NumPy, we can index Refs using Python integers, Python slice objects, and integer Arrays. ``` type Indices = Tuple[Int  eff} ()) > [a] > {eff} [a] ``` `run_state` doesn’t have any state effects corresponding to the created Refs, “discharging” the state effects. Note that other effects `{eff}` will remain (e.g. error effects, or state effects corresponding to outer Refs). An important point here is that we have not provided a way of creating Refs in “top level” user code. What we mean by this is that only run_state creates Refs, and there is no make_ref function.  Implementation `run_state` runs a Jaxpr interpreter (i.e. it's an initialstyle primitive) that keeps track of the values associated with Refs. When the interpreter hits an effectful primitive that operates on a Ref introduced by run_state, we run a discharge rule wherein we convert the stateful semantics into value semantics. For example, the `get` primitive can be discharged into `lax.dynamic_slice` or `lax.gather` depending on the index pattern. Similarly `swap` is discharged into a `dynamic_slice/gather` and `dynamic_update_slice/scatter`. `addupdate` is discharged into a `dynamic_slice/gather`, `add`, and `dynamic_update_slice/scatter`. `run_state` is also responsible for discharging higherorder primitives that contain state effects (for example, control flow primitives or nested jits). Note that `run_state` is only responsible for discharge effects corresponding to the Refs it introduces and no others.  Usage with control flow State primitives should *just work* with control flow primitives. You won’t, however, be able to pass Refs into them or return Refs from them. Reading from and writing into closedover Refs should be sufficient.  Examples without transformations  Implementing a simple pure function by discharging state effects Using only one readwrite Ref ```python def f(x_ref):   x = get(x_ref, ())   set(x_ref, (), x * 2) def mul2(x):   return run_state(f)(x) ``` Using readonly and writeonly Refs ```python def f(refs):   x_ref, y_ref = refs   x = get(x_ref, ())   set(y_ref, (), x * 2) def mul2(x):   return run_state(f)(x, 0)[1] ```  Array indexing ```python def f(x_ref, i_ref, v_ref):   i = get(i_ref, ())   set(x_ref, (i,), get(v_ref, ())) def set_at_index(x, i, v):   return run_state(f)(x, i, v)[0] set_at_index(jnp.arange(4), 2, 1)  ⇒ Returns [0, 1, 1, 3] ```  Nesting state effects Here’s an example where we nest two `run_state`s with nonoverlapping state effects ```python def square_ref(x_ref):   x = x_ref[()]   set(x_ref, (), jnp.square(x)) def f(x_ref, y_ref):   x = x_ref[()]   mul_x = run_state(square_ref)(x)   set(y_ref, (), mul_x) run_state(f)(2, 0)  ⇒ Returns (2, 4) ``` Here’s an example where we mutate an outer Ref from a nested run_state ```python def f(x_ref, y_ref):   def set_x(z_ref):      Mutating a closedover `Ref`     set(x_ref, (), get(z_ref, ()))   run_state(set_x)(4)   set(y_ref, (), get(x_ref, ())) run_state(f)(2, 0)  ⇒ Returns (4, 4) ```  Using state inside of control flow Here we write to a Ref inside of a `fori_loop` body. ```python def f(x_ref, y_ref):   def body(i, _):     set(y_ref, (i,), get(x_ref, (i,)) + 1)   lax.fori_loop(0, x_ref.shape[0], body, ()) run_state(f)(jnp.arange(4), jnp.zeros(4))[1]  ⇒ Returns [1., 2., 3., 4.] ``` Here we conditionally write to a Ref using a `lax.cond` ```python def f(x_ref):   def true_fun():     set(x_ref, (), 100)   def false_fun():     pass   x = get(x_ref, ())   lax.cond(x < 100, true_fun, false_fun) run_state(f)(5)    ⇒ Returns 100 run_state(f)(105)  ⇒ Returns 105 ```  Transformations of effectful functions In general, we disallow passing `Ref`s into higher order functions. They *must* be closed over, meaning we only need to take care of some cases under transformations. However, `run_state` should always be transformable by JAX functions, since it has a pure signature (it discharges away state effects).  Autodifferentiation If we are differentiating a function, we can safely assume there are no Ref inputs and therefore we are not differentiating with respect to any Refs. Functions we are differentiating that close over Refs and updating them is fair game. However, we will never need to apply transformation rules to the primitives in this case since they will only ever show up as purely primal values (no associated tangents). They will similarly not show up in the backwards pass so no transpose rule needed. In this example, we are mutating x_ref inside of a function we are taking `jax.grad` of. `x` is mutated in the forward pass and is untouched during the backward pass. ```python def f(x_ref):   def mul(x):     z = x_ref[()]     x_ref[()] += 1     return jnp.sin(x * z)   x = x_ref[()]   z = jax.grad(mul)(x)   x_ref[()] = z run_state(f)(2) ``` Differentiating through `run_state` is something we also support, but involves dealing with transforming the state primitives. When we are differentiating w.r.t. to values that are then passed into run_state to initialize Refs, we will need to differentiate through the body of run_state, which contains state sideeffects. With these rules in place, we can mechanically differentiate run_state with little modification to JAX’s core machinery.  Examples with transformations Differentiating through run_state ```python def f(x_ref):   x_ref[()] = jnp.sin(x_ref[()]) jax.grad(lambda x: run_state(f)(x))(4.) ⇒ Returns cos(4.) ``` Digging deeper into this, the function we are differentiating is this (in jaxpr representation): ``` { lambda ; a:f32[]. let     b:f32[] = run_state[       jaxpr={ lambda ; c:Ref{float32[]}. let           d:f32[] = get c ()           e:f32[] = sin d           _:f32[] = swap c () e         in () }     ] a   in (b,) } ``` When we take its gradient, we obtain the following function: ``` { lambda ; a:f32[]. let     _:f32[] b:f32[] = run_state[       jaxpr={ lambda ; c:Ref{float32[]} d:Ref{float32[]}. let           e:f32[] = get c ()           f:f32[] = sin e           g:f32[] = cos e           _:f32[] = swap c () f           _:f32[] = swap d () g         in () }     ] a     _:f32[], h:f32[] = run_state[       jaxpr={ lambda ; i:Ref{float32[]} j:Ref{float32[]}. let           k:f32[] = get i ()           l:f32[] = swap j () 0.0           m:f32[] = mul l k           addupdate j () m         in () }     ] b 1.0   in (h,) } ``` Notice that we have two `run_state`s in our gradient function. The first corresponds to the forward pass where we compute `sin(x)` and the residual term `cos(x)`. The residual term is passed into the second `run_state` and read into the variable k. We then perform the transpose where the `get` and `swap` in the first jaxpr are turned into an `addupdate` and `swap` respectively (and run in reverse). The end result is that we write `cos(x) * 1.` into `j` and return `cos(x)` from the function. Still TODO: vmap rule",2023-03-22T21:57:54Z,pull ready,closed,5,5,https://github.com/jax-ml/jax/issues/15149,This is it! We've jumped the shark!," Great work and thank you for the detailed writeup! BTW, I think the type of `run_state` should have been ```Haskell run_state :: ([Ref a] > {Read a, Write a, Accum a | eff} ()) > [a] > {eff} [a] ```",">  Great work and thank you for the detailed writeup! Thanks! > BTW, I think the type of `run_state` should have been >  > ```haskell > run_state :: ([Ref a] > {Read a, Write a, Accum a | eff} ()) > [a] > {eff} [a] > ``` Fixed, thanks for the catch.", Best PR doc I have ever seen !,The PR description should land in a JEP :) 
2022,"以下是一个github上的jax下的一个issue, 标题是(Running with more process than devices)， 内容是 ( Description I am running JAX on a Slurm cluster with `jax.distributed.initialize`. If I ask for a number of process _lower or equal_ to the number of devices available, everything runs fine. When using more process than devices available on a node (trying to have several JAX process use the same GPU concurently), initializing with `jax.distributed.initialize` leads to it asking for device numbers that do not exist (due to this line which asks for a device number equal to the process id). This fails causing JAX to fall back to `cpu` on some processes. Trying to pass a device number manually via `local_device_ids` causes a (seemingly unrelated) `RET_CHECK failure` down the line: ``` 20230322 15:34:15.653227: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 20230322 15:34:15.653280: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Error retrieving driver version: NOT_FOUND: could not find kernel module information in driver version file contents: ""NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  515.65.01  Release Build  (dvsbuilderT11052)  Wed Jul 20 13:54:56 UTC 2022 GCC version:  gcc version 7.5.0 (SUSE Linux)  "" 20230322 15:34:15.654027: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:54] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:641) dnn != nullptr  ``` Can the behavior of `jax.distributed.initialize` be fixed to allow for more processes than devices (or fail in a straightforward way, letting the user know that it is not a usecase)?  What jax/jaxlib version are you using? jax0.4.6 jaxlib0.4.6+cuda11.cudnn86  Which accelerator(s) are you using? GPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Running with more process than devices," Description I am running JAX on a Slurm cluster with `jax.distributed.initialize`. If I ask for a number of process _lower or equal_ to the number of devices available, everything runs fine. When using more process than devices available on a node (trying to have several JAX process use the same GPU concurently), initializing with `jax.distributed.initialize` leads to it asking for device numbers that do not exist (due to this line which asks for a device number equal to the process id). This fails causing JAX to fall back to `cpu` on some processes. Trying to pass a device number manually via `local_device_ids` causes a (seemingly unrelated) `RET_CHECK failure` down the line: ``` 20230322 15:34:15.653227: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 20230322 15:34:15.653280: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Error retrieving driver version: NOT_FOUND: could not find kernel module information in driver version file contents: ""NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  515.65.01  Release Build  (dvsbuilderT11052)  Wed Jul 20 13:54:56 UTC 2022 GCC version:  gcc version 7.5.0 (SUSE Linux)  "" 20230322 15:34:15.654027: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:54] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:641) dnn != nullptr  ``` Can the behavior of `jax.distributed.initialize` be fixed to allow for more processes than devices (or fail in a straightforward way, letting the user know that it is not a usecase)?  What jax/jaxlib version are you using? jax0.4.6 jaxlib0.4.6+cuda11.cudnn86  Which accelerator(s) are you using? GPU",2023-03-22T21:07:07Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15146,"After some trial and error I found the answer! To get it working you need to: * set XLA_PYTHON_CLIENT_MEM_FRACTION to avoid having processes ask for more memory than available, * do not _call_ a Jax function before calling `jax.distributed.initialize`[^0], * pass the `device_id` manually to `jax.distributed.initialize` (`jax.distributed.initialize(local_device_ids=[device_id])`). Note that the behavior when using more processes than devices could still be improved. [^0]: It took me a long time to realize that I needed to get rid of a call to `jax.device_count`...", for viz
1603,"以下是一个github上的jax下的一个issue, 标题是(Use options when creating a PJRT client)， 内容是 (Now the PJRT Client API can receive options, so JAX can pass the following information as an option.  replica_id  partition_id  num_replicas  num_partitions I believe the number of processes and process_id can be derived from the info above. (But I am okay to have them as option too). The information is needed to set up the NCCL library in the IREE runtime through the PJRT Client. These are some source code locations I got from Jieying Luo about the newly added options.  If you are calling the C API PJRT_Client_Create, you can set these two args https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/c/pjrt_c_api.hL222L223.  Note your implementation of PJRT_Client_Create needs to handle how to use the options.  If you are calling GetCApiClient, you can pass in an absl::flat_hash_map https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/pjrt_c_api_client.hL581.  If you are calling from jax, you can pass option in make_c_api_client https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/python/xla_client.pyL114.  I have a pending change to parse the options from a json file for jax. There are two helper functions to convert between C++ and C object for the options https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/c/pjrt_c_api_helpers.hL126L141)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Use options when creating a PJRT client,"Now the PJRT Client API can receive options, so JAX can pass the following information as an option.  replica_id  partition_id  num_replicas  num_partitions I believe the number of processes and process_id can be derived from the info above. (But I am okay to have them as option too). The information is needed to set up the NCCL library in the IREE runtime through the PJRT Client. These are some source code locations I got from Jieying Luo about the newly added options.  If you are calling the C API PJRT_Client_Create, you can set these two args https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/c/pjrt_c_api.hL222L223.  Note your implementation of PJRT_Client_Create needs to handle how to use the options.  If you are calling GetCApiClient, you can pass in an absl::flat_hash_map https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/pjrt_c_api_client.hL581.  If you are calling from jax, you can pass option in make_c_api_client https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/python/xla_client.pyL114.  I have a pending change to parse the options from a json file for jax. There are two helper functions to convert between C++ and C object for the options https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/c/pjrt_c_api_helpers.hL126L141",2023-03-20T22:39:26Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/15106
3339,"以下是一个github上的jax下的一个issue, 标题是(Custom CPU/GPU op with single return value results in ValueError ""Output of translation rule must be iterable"")， 内容是 ( Description I ran into this problem trying to register a custom operation (implemented in C++) that only returns a single output. Mockup code for the jax side: ```python from functools import partial import jax from jax.interpreters import xla, mlir import jax.numpy as jnp from jax.lib import xla_client from jax._src.abstract_arrays import ShapedArray from jaxlib.hlo_helpers import custom_call from jax.interpreters import mlir import jax.numpy as jnp  xla_client.register_cpu_custom_call_target(      b""single_return_fun_native"", None  here register the actual native function capsule  ) test_p = jax.core.Primitive(""single_return_fun"") test_p.multiple_results = False test_p.def_impl(partial(xla.apply_primitive, test_p)) def _abstract_eval(x):     return ShapedArray(x.shape, x.dtype) test_p.def_abstract_eval(_abstract_eval) def _lowering(ctx, x):     x_type = mlir.ir.RankedTensorType(x.type)     x_shape = x_type.shape     layout = range(len(x_shape)  1, 1, 1)     return custom_call(b""single_return_fun_native"", out_types=[x_type], operands=[x], operand_layouts=[layout], result_layouts=[layout]) mlir.register_lowering(test_p, _lowering) def single_return_fun(x):     return test_p.bind(x) x = jnp.zeros((3, 2)) single_return_fun(x) ``` The above fails with a `ValueError` with message ```Output of translation rule must be iterable: a:f32[3,2] = single_return_fun b, got output Value(%0 = ""stablehlo.custom_call""(%arg0) {api_version = 2 : i32, backend_config = """", call_target_name = ""single_return_fun_native"", called_computations = [], has_side_effect = false, operand_layouts = [dense : tensor], output_operand_aliases = [], result_layouts = [dense : tensor]} : (tensor) > tensor)``` This is caused by the implementation in hlo_helpers.custom_call , which returns a `OpResult` instance if only a single output is specified in the lowering rule, i.e., the length of `out_types` is one. (If the length is larger than one, this function instead returns `List[OpResult]`.) This then leads to a crash somewhere upstream in the translation stack, i.e., in  jaxpr_subcomp, which expects to _always_ get an iterable and will throw the above exception if this is not the case. Note that the setting of `test_p.multiple_results = False` doesn't factor in here. The behaviour of `custom_call` is only dependent on the length its `out_types` argument, which is in turn determined by the native/C side implementation. As a workaround, I tried to add an additional empty return value (i.e., an array of length 0), but that resulted in another error. Making `hlo_helpers.custom_call` return a list containing a single element if `len(out_types) == 1` fixes this error. (But in that case, the test above crashes because the custom call target `single_return_fun_native` has not actually been registered. I could not find a concise way of doing that for this bug report.)  What jax/jaxlib version are you using? jax v0.4.6, jaxlib v0.4.4  Which accelerator(s) are you using? CPU/any  Additional system info Python v3.10  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Custom CPU/GPU op with single return value results in ValueError ""Output of translation rule must be iterable"""," Description I ran into this problem trying to register a custom operation (implemented in C++) that only returns a single output. Mockup code for the jax side: ```python from functools import partial import jax from jax.interpreters import xla, mlir import jax.numpy as jnp from jax.lib import xla_client from jax._src.abstract_arrays import ShapedArray from jaxlib.hlo_helpers import custom_call from jax.interpreters import mlir import jax.numpy as jnp  xla_client.register_cpu_custom_call_target(      b""single_return_fun_native"", None  here register the actual native function capsule  ) test_p = jax.core.Primitive(""single_return_fun"") test_p.multiple_results = False test_p.def_impl(partial(xla.apply_primitive, test_p)) def _abstract_eval(x):     return ShapedArray(x.shape, x.dtype) test_p.def_abstract_eval(_abstract_eval) def _lowering(ctx, x):     x_type = mlir.ir.RankedTensorType(x.type)     x_shape = x_type.shape     layout = range(len(x_shape)  1, 1, 1)     return custom_call(b""single_return_fun_native"", out_types=[x_type], operands=[x], operand_layouts=[layout], result_layouts=[layout]) mlir.register_lowering(test_p, _lowering) def single_return_fun(x):     return test_p.bind(x) x = jnp.zeros((3, 2)) single_return_fun(x) ``` The above fails with a `ValueError` with message ```Output of translation rule must be iterable: a:f32[3,2] = single_return_fun b, got output Value(%0 = ""stablehlo.custom_call""(%arg0) {api_version = 2 : i32, backend_config = """", call_target_name = ""single_return_fun_native"", called_computations = [], has_side_effect = false, operand_layouts = [dense : tensor], output_operand_aliases = [], result_layouts = [dense : tensor]} : (tensor) > tensor)``` This is caused by the implementation in hlo_helpers.custom_call , which returns a `OpResult` instance if only a single output is specified in the lowering rule, i.e., the length of `out_types` is one. (If the length is larger than one, this function instead returns `List[OpResult]`.) This then leads to a crash somewhere upstream in the translation stack, i.e., in  jaxpr_subcomp, which expects to _always_ get an iterable and will throw the above exception if this is not the case. Note that the setting of `test_p.multiple_results = False` doesn't factor in here. The behaviour of `custom_call` is only dependent on the length its `out_types` argument, which is in turn determined by the native/C side implementation. As a workaround, I tried to add an additional empty return value (i.e., an array of length 0), but that resulted in another error. Making `hlo_helpers.custom_call` return a list containing a single element if `len(out_types) == 1` fixes this error. (But in that case, the test above crashes because the custom call target `single_return_fun_native` has not actually been registered. I could not find a concise way of doing that for this bug report.)  What jax/jaxlib version are you using? jax v0.4.6, jaxlib v0.4.4  Which accelerator(s) are you using? CPU/any  Additional system info Python v3.10  NVIDIA GPU info _No response_",2023-03-20T15:36:14Z,bug,closed,1,6,https://github.com/jax-ml/jax/issues/15095,"I think this is actually working as intended. `custom_call` is a private helper, but it is supposed to return a single IR value if there is only one, not a sequence. The fix is to change your lowering rule to return an arity1 tuple, i.e., ``` def _lowering(ctx, x):     x_type = mlir.ir.RankedTensorType(x.type)     x_shape = x_type.shape     layout = range(len(x_shape)  1, 1, 1)     out = custom_call(b""single_return_fun_native"", out_types=[x_type], operands=[x], operand_layouts=[layout], result_layouts=[layout])     return (out,) ```",Ha. Sometimes the simplest solutions are the ones you don't find :/ Many thanks! (I still find this behaviour of `custom_call` a bit counterintuitive and documentation on how to use it could be improved.),"Well, it's supposed to be a private helper, which is why it's not documented. We have plans to make it easier to plug in custom kernels without having to write lowering rules, which is I think what you actually wanted here?","I had a related issue, but this fix doesn't seem to work for `jax v0.4.3` and `jaxlib v0.4.3`. Is this expected behavior? The error I get happens on these lines in `core.py`: ```python TypeError:  > ~/.local/lib/python3.9/sitepackages/jax/_src/core.py(1774)raise_to_shaped()    1772     handler = raise_to_shaped_mappings.get(typ)    1773     if handler: return handler(aval, weak_type) > 1774   raise TypeError(type(aval)) ``` Even after updating to jax v0.4.6, jaxlib v0.4.4 I still have the issue. However, I am using `python3.9`, not sure if that has anything to do with it?","Hi   it looks like you're passing a tuple in a location where an array is expected. Can you please open a new issue for your question, and include a minimal reproducible example? Thanks!","> Hi   it looks like you're passing a tuple in a location where an array is expected. Can you please open a new issue for your question, and include a minimal reproducible example? Thanks! I think I realized what my problem was. I was able to track the issue down to my abstract evaluation function: ```python def _pylefty_ll_fwd_abstract(shat):     shat_dtype = dtypes.canonicalize_dtype(shat.dtype)     assert shat_dtype == jnp.dtype(np.float64)     return (ShapedArray((), jnp.dtype(np.float64)) , ) ``` which caused the issue I believe.  After removing the tuple from the return statement, I got rid of the error. I am wondering however, whether this abstract evaluation is correct if one is returning a pure double value from the corresponding `custom_call`? More details on the minimal example are in this issue."
1996,"以下是一个github上的jax下的一个issue, 标题是(How to replay an object_  replay_buffer become types that support jax data)， 内容是 (Please: ```python         replay_buffer = ReplayBuffer(env.observation_space, env.action_space,                                      FLAGS.max_steps) ``` I want to pass it as an incoming parameter to the function： ```python agent, update_info = agent.update(batch, FLAGS.utd_ratio,replay_buffer) ``` But the update function requires jit acceleration ```python (jax.jit, static_argnames='utd_ratio')     def update(self, batch: DatasetDict, utd_ratio: int,replay_buffer):         new_agent = self         for i in range(utd_ratio):             def slice(x):                 assert x.shape[0] % utd_ratio == 0                 batch_size = x.shape[0] // utd_ratio                 return x[batch_size * i:batch_size * (i + 1)]             mini_batch = jax.tree_util.tree_map(slice, batch)             print(replay_buffer)             new_agent, critic_info = self.update_critic(new_agent, mini_batch,replay_buffer)             pro=(critic_info['prios'])             pro1=jnp.array([pro])             indices=jnp.array(batch['indices'])             replay_buffer.update_priorities(batch_indices=indices,batch_priorities=pro1)         new_agent, actor_info = self.update_actor(new_agent, mini_batch)         new_agent, temp_info = self.update_temperature(new_agent,                                                        actor_info['entropy'])         return new_agent, {**actor_info, **critic_info, **temp_info} ``` Error will be reported when I run directly ``` TypeError: Argument '&lt;rl.data.replay_buffer.ReplayBuffer object at 0x7f0fd441ef70&gt;' of type &lt;class 'rl.data.replay_buffer.ReplayBuffer'&gt; is not a valid JAX type ``` Thank you very much for your help!Thank you very much for your help!Thank you very much for your help!！！！！！)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,How to replay an object_  replay_buffer become types that support jax data,"Please: ```python         replay_buffer = ReplayBuffer(env.observation_space, env.action_space,                                      FLAGS.max_steps) ``` I want to pass it as an incoming parameter to the function： ```python agent, update_info = agent.update(batch, FLAGS.utd_ratio,replay_buffer) ``` But the update function requires jit acceleration ```python (jax.jit, static_argnames='utd_ratio')     def update(self, batch: DatasetDict, utd_ratio: int,replay_buffer):         new_agent = self         for i in range(utd_ratio):             def slice(x):                 assert x.shape[0] % utd_ratio == 0                 batch_size = x.shape[0] // utd_ratio                 return x[batch_size * i:batch_size * (i + 1)]             mini_batch = jax.tree_util.tree_map(slice, batch)             print(replay_buffer)             new_agent, critic_info = self.update_critic(new_agent, mini_batch,replay_buffer)             pro=(critic_info['prios'])             pro1=jnp.array([pro])             indices=jnp.array(batch['indices'])             replay_buffer.update_priorities(batch_indices=indices,batch_priorities=pro1)         new_agent, actor_info = self.update_actor(new_agent, mini_batch)         new_agent, temp_info = self.update_temperature(new_agent,                                                        actor_info['entropy'])         return new_agent, {**actor_info, **critic_info, **temp_info} ``` Error will be reported when I run directly ``` TypeError: Argument '&lt;rl.data.replay_buffer.ReplayBuffer object at 0x7f0fd441ef70&gt;' of type &lt;class 'rl.data.replay_buffer.ReplayBuffer'&gt; is not a valid JAX type ``` Thank you very much for your help!Thank you very much for your help!Thank you very much for your help!！！！！！",2023-03-20T13:38:33Z,enhancement,open,0,2,https://github.com/jax-ml/jax/issues/15093,"Hi, thanks for the question! It's hard to tell how to help without more information. Could you edit your question to add a minimal reproducible example? Package imports are important: for example I have no idea what `ReplayBuffer` is or what package it might be coming from. Thanks!","The ReplayBuffer is a class used for experience replay in reinforcement learning, and an object of this class is created as replay_buffer. I want to pass this replay_buffer object as an input parameter to a jitaccelerated update function, but the replay_buffer object does not support the data format of jax. How can I convert the replay_buffer to a data format that supports jax? Thank you for your help. Here is the general content of the ReplayBuffer class. `class ReplayBuffer(Dataset):     def __init__(self,                  observation_space: gym.Space,                  action_space: gym.Space,                  capacity: int,                  alpha:float = 0.6,                  beta_start:float = 0.4,                  beta_frames: float = 1e6,                  next_observation_space: Optional[gym.Space] = None):         if next_observation_space is None:             next_observation_space = observation_space         observation_data = _init_replay_dict(observation_space, capacity)         next_observation_data = _init_replay_dict(next_observation_space,                                                   capacity)         dataset_dict = dict(             observations=observation_data,             next_observations=next_observation_data,             actions=np.empty((capacity, *action_space.shape),                              dtype=action_space.dtype),             rewards=np.empty((capacity, ), dtype=np.float32),             dones=np.empty((capacity, ), dtype=bool),         )         super().__init__(dataset_dict)         self._size = 0         self._capacity = capacity         self._insert_index = 0         self.alpha = alpha         self.beta_start = beta_start         self.beta_frames = beta_frames         self.frame = 1         self.capacity = capacity         self.buffer = []         self.pos = 0         self.priorities = np.zeros((capacity,),dtype = np.float32)     def beta_by_frames(self, frame_idx):         return min(1.0, self.beta_start + frame_idx * (1.0  self.beta_start) / self.beta_frames)     def insert(self, data_dict: DatasetDict):         state=data_dict['observations']         next_state=data_dict['next_observations']         assert state.ndim == next_state.ndim         state = np.expand_dims(state, 0)         next_state = np.expand_dims(next_state, 0)         max_prio = self.priorities.max() if self.buffer else 1.0   gives max priority if buffer is not empty else 1         if len(self.buffer)  cap > new posi = 0     def __len__(self) > int:         return self.buffer     def update_priorities(self, batch_indices, batch_priorities):          for idx, prio in zip(batch_indices, batch_priorities):             self.priorities[idx] = abs(prio)     def sample(self, batch_size)> frozen_dict.FrozenDict:         N = len(self.buffer)         if N == self.capacity:             prios = self.priorities         else:             prios = self.priorities[:self.pos]          calc P = p^a/sum(p^a)         probs = prios ** self.alpha         P = probs / probs.sum()          gets the indices depending on the probability p         indices = np.random.choice(N, batch_size, p=P)         samples = [self.buffer[idx] for idx in indices]         beta = self.beta_by_frames(self.frame)         self.frame += 1          Compute importancesampling weight         weights = (N * P[indices]) ** (beta)          normalize weights         weights /= weights.max()         weights = np.array(weights, dtype=np.float32)         observations, actions, rewards, next_observations, dones = zip(*samples)         batch = {'observations': observations,                          'actions': actions,                             'rewards': rewards,                             'next_observations': next_observations,                             'dones': dones,                             'indices': indices,                             'weights': weights}         print(batch['weights'])         for k,v in batch.items():             batch[k] = np.array(v)         return frozen_dict.freeze(batch)`"
927,"以下是一个github上的jax下的一个issue, 标题是(JAX RNG clashes with PyTorch on GPU)， 内容是 ( Description If you import pytorch code (that uses dataloaders) before trying to create a jax PRNG key, you get an obscure error ``` XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:641) dnn != nullptr  ``` If you execute ` key = jax.random.PRNGKey(0)` before importing pytorch, it works fine. (The problem only occurs on GPU, not CPU or TPU.) Code to reproduce this: https://colab.sandbox.google.com/drive/1gGDAGotSA0aJ9Nd1ilV9_xWBy6uFvVkU?usp=sharing   What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` Sun Mar 19 20:25:58 2023        ++  ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JAX RNG clashes with PyTorch on GPU," Description If you import pytorch code (that uses dataloaders) before trying to create a jax PRNG key, you get an obscure error ``` XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:641) dnn != nullptr  ``` If you execute ` key = jax.random.PRNGKey(0)` before importing pytorch, it works fine. (The problem only occurs on GPU, not CPU or TPU.) Code to reproduce this: https://colab.sandbox.google.com/drive/1gGDAGotSA0aJ9Nd1ilV9_xWBy6uFvVkU?usp=sharing   What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ``` Sun Mar 19 20:25:58 2023        ++  ```",2023-03-19T20:30:56Z,bug NVIDIA GPU,open,1,6,https://github.com/jax-ml/jax/issues/15084,"I suspect this means ""JAX ran out of memory"". See https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html We clearly need to improve the error message at least.","But if I create the JAX PRNG key first, it all works fine. So I don't think it's just a memory issue.",", my understanding of PyTorch memory allocation is not great; , when Pytorch is run/imported, how much does it take of the GPU memory by default?","> when Pytorch is run/imported, how much does it take of the GPU memory by default? No GPU memory will be used by default. The first CUDA operation triggered by PyTorch will create the CUDA context, which will allocate memory. The size of the context depends on the used GPU architecture, the CUDA version, if lazy module loading is enabled etc. PyTorch then uses the internal caching allocator to allocate new device memory when needed and to reuse it when possible. It has thus a growing memory usage. The cache can manually be freed and the current allocations can be checked via `print(torch.cuda.memory_summary())`.", are you able to share the PyTorch allocation when tryin to call `jax.random.PRNGKey()`? ,"Hi   I tried to run the shared colab notebook with latest JAX version 0.4.23, PyTorch 2.1.0+cu121 and T4 GPU. It executed without any error. Kindly find the gist for reference. Thank you."
1033,"以下是一个github上的jax下的一个issue, 标题是(random.* docs are not specific enough about probability density function)， 内容是 ( Description jax.random.normal and other jax.random.* should specify the actual probability density function (see numpy.random.normal for an example).  Otherwise there is the potential for confusion (see e.g. https://math.stackexchange.com/questions/1013575/continuousprobabilityaveragepowerofagaussianrandomvariablemathcaln) For the doc fix, one possibility would be to put the PDF equation inline in the docs, as numpy.random.normal does.  Another possibility would be to state that the PDF is the same as some numpy routine.   For jax.random.normal, that would be numpy.random.normal(loc=0.0, scale=1.0).  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,random.* docs are not specific enough about probability density function," Description jax.random.normal and other jax.random.* should specify the actual probability density function (see numpy.random.normal for an example).  Otherwise there is the potential for confusion (see e.g. https://math.stackexchange.com/questions/1013575/continuousprobabilityaveragepowerofagaussianrandomvariablemathcaln) For the doc fix, one possibility would be to put the PDF equation inline in the docs, as numpy.random.normal does.  Another possibility would be to state that the PDF is the same as some numpy routine.   For jax.random.normal, that would be numpy.random.normal(loc=0.0, scale=1.0).  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-03-19T19:22:41Z,documentation,closed,0,1,https://github.com/jax-ml/jax/issues/15083,"Thanks, this is a great suggestion."
5322,"以下是一个github上的jax下的一个issue, 标题是(make mlir arg and result names work with static_argnums/argnames)， 内容是 (This is the first step in a revision to how we handle the debug info pertaining to staged functions' parameter names and result pytree paths. To limit complexity, this first step adds machinery required to make our MLIR lowerings' parameter and result names work, but it does *not* yet unify it with existing argname machinery used at tracing time (in partial_eval.py, e.g. `partial_eval.DebugInfo` etc). That unification will come in a follow up commits. (I wrote the unified version first, then broke it down into this sequence of commits.) Another thing that will arrive in followup commits is `pmap` support (handling `static_broadcasted_argnums`). This PR doesn't include support for `pmap` because `pmap`'s final style implementation requires slightly different machinery than jit/pjit's initial style implementation. Indeed this PR removes the previous support for `pmap` arg/result info, and skips the corresponding tests, because the previous support didn't handle `pmap`'s `static_broadcasted_argnums` (and I think it could even lead to silently incorrect annotations when `pmap` was not at the toplevel, though I didn't work out an example case to be sure that was possible). ~This commit includes the changes from PR CC([pytrees] fix function underlying treeflattening with keys), so that PR should be merged first.~ Here's the _why_ of this change: * The preexisting solution (from PRs CC(attach debug info to jaxpr, pass to mlir/mhlo), CC(attach debug info to jaxpr, pass to mlir/mhlo), and CC(add result info to mhlo)) did not handle static_argnums or static_argnames correctly. Instead it would fail, resulting in debug info being dropped from the jaxpr and ultimately the MLIR computation (but no Exception raised). We need to handle static_argnums/argnames because while the corresponding parameters remain on the Python callable signature, they are excluded from the args/kwargs pytrees; the previous solution didn't account for that divergence. * The best way to handle static_argnums/argnames is to work out this debug info when we still have the original args/kwargs in hand, i.e. much earlier than the previous mechanism. We then just have to pass this debug info to the right places. Indeed we often already had to work out some debugrelated information at these call sites (e.g. whether the function is being staged out for jit, or scan, or whatever), so after this change we're working out all the debug info at the same time. * A side benefit is that now to get this debug info we no longer need to unflatten user pytree defs with dummy objects (to reconstruct dummy args/kwargs trees so that we can call inspect.signature(fun).bind), since we just use the original args/kwargs instead. Since some user pytree node types are not fully polymorphic in their element types (e.g. their __init__ methods sometimes contained assertions about their elements' shapes, expecting them to be arrays), that means the new mechanism is fundamentally more compatible with custom pytree node types. More concretely, effecting those highlevel changes led to: * replacing the previous `core.DebugInfo` with a class `core.JaxprDebugInfo`, which in addition to the more precise name has fields like `arg_names: Tuple[Optional[str], ...]` and `result_paths: Tuple[Optional[str], ...]`, rather than `in_tree: Optional[PyTreeDef]`, reflecting the fact that we work out the actual debug info more eagerly than before and we don't need pytrees for dummyunflattening; * introducing the new `api_util.TracingDebugInfo` class representing the debug info about inputs which we have available at tracing time; in a followup PR, we'll adapt partial_eval.py to use this new class and we'll delete `partial_eval.DebugInfo` and its corresponding helper methods (not done in this commit just to reduce complexity of each change); * moving the old `core.DebugInfo`, which before CC(attach debug info to jaxpr, pass to mlir/mhlo) lived in partial_eval.py, back to partial_eval.py pending cleanup (deletion) of that partial_eval.py debug info code; * making specific jaxprprocessing functions produce an appropriately updated `core.JaxprDebugInfo` object for their output (e.g. `pe.dce_jaxpr` prunes elements from the `arg_names` field), maintaining nowchecked invariants like a Jaxpr's `debug_info` should have the same number of argument names as the jaxpr has invars (the jaxprprocessing functions updated here are enough for toplevel jit jaxprs to have debug info attached, handling the original intended use case of jit(f).lower, but not e.g. gradofjit cases, which can be handled later by updating `ad.jvp_jaxpr` and the like to produce updated debug info on their outputs); * add some tests for static_argnums/static_argnames. Followups: * [x] restore pmap support for arg/result info CC(make mlir arg and result names work with pmap) * [ ] replace partial_eval.py's tracetime `pe.DebugInfo` machinery with the new `api_util.TracingDebugInfo` stuff * [ ] produce updated jaxpr debug info on more jaxprtojaxpr functions (needed when the jaxprstaging function is not at the top level))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,make mlir arg and result names work with static_argnums/argnames,"This is the first step in a revision to how we handle the debug info pertaining to staged functions' parameter names and result pytree paths. To limit complexity, this first step adds machinery required to make our MLIR lowerings' parameter and result names work, but it does *not* yet unify it with existing argname machinery used at tracing time (in partial_eval.py, e.g. `partial_eval.DebugInfo` etc). That unification will come in a follow up commits. (I wrote the unified version first, then broke it down into this sequence of commits.) Another thing that will arrive in followup commits is `pmap` support (handling `static_broadcasted_argnums`). This PR doesn't include support for `pmap` because `pmap`'s final style implementation requires slightly different machinery than jit/pjit's initial style implementation. Indeed this PR removes the previous support for `pmap` arg/result info, and skips the corresponding tests, because the previous support didn't handle `pmap`'s `static_broadcasted_argnums` (and I think it could even lead to silently incorrect annotations when `pmap` was not at the toplevel, though I didn't work out an example case to be sure that was possible). ~This commit includes the changes from PR CC([pytrees] fix function underlying treeflattening with keys), so that PR should be merged first.~ Here's the _why_ of this change: * The preexisting solution (from PRs CC(attach debug info to jaxpr, pass to mlir/mhlo), CC(attach debug info to jaxpr, pass to mlir/mhlo), and CC(add result info to mhlo)) did not handle static_argnums or static_argnames correctly. Instead it would fail, resulting in debug info being dropped from the jaxpr and ultimately the MLIR computation (but no Exception raised). We need to handle static_argnums/argnames because while the corresponding parameters remain on the Python callable signature, they are excluded from the args/kwargs pytrees; the previous solution didn't account for that divergence. * The best way to handle static_argnums/argnames is to work out this debug info when we still have the original args/kwargs in hand, i.e. much earlier than the previous mechanism. We then just have to pass this debug info to the right places. Indeed we often already had to work out some debugrelated information at these call sites (e.g. whether the function is being staged out for jit, or scan, or whatever), so after this change we're working out all the debug info at the same time. * A side benefit is that now to get this debug info we no longer need to unflatten user pytree defs with dummy objects (to reconstruct dummy args/kwargs trees so that we can call inspect.signature(fun).bind), since we just use the original args/kwargs instead. Since some user pytree node types are not fully polymorphic in their element types (e.g. their __init__ methods sometimes contained assertions about their elements' shapes, expecting them to be arrays), that means the new mechanism is fundamentally more compatible with custom pytree node types. More concretely, effecting those highlevel changes led to: * replacing the previous `core.DebugInfo` with a class `core.JaxprDebugInfo`, which in addition to the more precise name has fields like `arg_names: Tuple[Optional[str], ...]` and `result_paths: Tuple[Optional[str], ...]`, rather than `in_tree: Optional[PyTreeDef]`, reflecting the fact that we work out the actual debug info more eagerly than before and we don't need pytrees for dummyunflattening; * introducing the new `api_util.TracingDebugInfo` class representing the debug info about inputs which we have available at tracing time; in a followup PR, we'll adapt partial_eval.py to use this new class and we'll delete `partial_eval.DebugInfo` and its corresponding helper methods (not done in this commit just to reduce complexity of each change); * moving the old `core.DebugInfo`, which before CC(attach debug info to jaxpr, pass to mlir/mhlo) lived in partial_eval.py, back to partial_eval.py pending cleanup (deletion) of that partial_eval.py debug info code; * making specific jaxprprocessing functions produce an appropriately updated `core.JaxprDebugInfo` object for their output (e.g. `pe.dce_jaxpr` prunes elements from the `arg_names` field), maintaining nowchecked invariants like a Jaxpr's `debug_info` should have the same number of argument names as the jaxpr has invars (the jaxprprocessing functions updated here are enough for toplevel jit jaxprs to have debug info attached, handling the original intended use case of jit(f).lower, but not e.g. gradofjit cases, which can be handled later by updating `ad.jvp_jaxpr` and the like to produce updated debug info on their outputs); * add some tests for static_argnums/static_argnames. Followups: * [x] restore pmap support for arg/result info CC(make mlir arg and result names work with pmap) * [ ] replace partial_eval.py's tracetime `pe.DebugInfo` machinery with the new `api_util.TracingDebugInfo` stuff * [ ] produce updated jaxpr debug info on more jaxprtojaxpr functions (needed when the jaxprstaging function is not at the top level)",2023-03-18T22:19:45Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15080
1104,"以下是一个github上的jax下的一个issue, 标题是(separate register_pytree_node and register_pytree_with_keys tests)， 内容是 (In CC(Add JAX pytree key path API to tree_utils.) we added tests for `register_pytree_with_keys` etc, but because those replaced some `register_pytree_node` tests (e.g. replacing `register_pytree_node(AnObject, ...)` with `register_pytree_with_keys(AnObject, ...)`), it meant we had less test coverage of the preexisting `register_pytree_node` path. The aim of this PR is to restore the `register_pytree_node` tests that existed before CC(Add JAX pytree key path API to tree_utils.), but also to keep all the new test coverage as well. The basic approach is just to: 1. put back `register_pytree_node(AnObject, ...)` as it was before CC(Add JAX pytree key path API to tree_utils.); 2. introduce the `AnObject2` trivial subclass and to apply `register_pytree_with_keys(AnObject2, ...)` to it; 3. include `AnObject2` in both pathful and nonpathful pytree tests.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,separate register_pytree_node and register_pytree_with_keys tests,"In CC(Add JAX pytree key path API to tree_utils.) we added tests for `register_pytree_with_keys` etc, but because those replaced some `register_pytree_node` tests (e.g. replacing `register_pytree_node(AnObject, ...)` with `register_pytree_with_keys(AnObject, ...)`), it meant we had less test coverage of the preexisting `register_pytree_node` path. The aim of this PR is to restore the `register_pytree_node` tests that existed before CC(Add JAX pytree key path API to tree_utils.), but also to keep all the new test coverage as well. The basic approach is just to: 1. put back `register_pytree_node(AnObject, ...)` as it was before CC(Add JAX pytree key path API to tree_utils.); 2. introduce the `AnObject2` trivial subclass and to apply `register_pytree_with_keys(AnObject2, ...)` to it; 3. include `AnObject2` in both pathful and nonpathful pytree tests.",2023-03-18T21:38:51Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15079
936,"以下是一个github上的jax下的一个issue, 标题是([pytrees] fix function underlying tree-flattening with keys)， 内容是 (There were two bugs in the `_generate_key_paths` function underlying `tree_flatten_with_path`, leading to disagreement between `len(tree_flatten(x)[0])` and `len(tree_flatten_with_path(x)[0])` for some `x` 1. pytree nodes that weren't registered as pytreenodeswithkeys were treated as leaves 2. namedtuples that were registered as pytree nodes were being flattened as generic namedtuples rather than using the explicitly registered flattener It'd be nice to have some kind of propertybased test to ensure that for all `x` we have: ```python leaves1, meta1 = tree_flatten(x) pairs2, meta2 = tree_flatten_with_path(x) leaves2 = [l for _, l in pairs2] assert leaves1 == leaves2 and meta1 == meta2 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[pytrees] fix function underlying tree-flattening with keys,"There were two bugs in the `_generate_key_paths` function underlying `tree_flatten_with_path`, leading to disagreement between `len(tree_flatten(x)[0])` and `len(tree_flatten_with_path(x)[0])` for some `x` 1. pytree nodes that weren't registered as pytreenodeswithkeys were treated as leaves 2. namedtuples that were registered as pytree nodes were being flattened as generic namedtuples rather than using the explicitly registered flattener It'd be nice to have some kind of propertybased test to ensure that for all `x` we have: ```python leaves1, meta1 = tree_flatten(x) pairs2, meta2 = tree_flatten_with_path(x) leaves2 = [l for _, l in pairs2] assert leaves1 == leaves2 and meta1 == meta2 ```",2023-03-18T02:13:04Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15071
1235,"以下是一个github上的jax下的一个issue, 标题是(Why does `lax.reshape_p` carry `dimensions`(info. about transposition)?)， 内容是 (Currently, the `lax.reshape_p` primitive carries two pieces of information:  `new_sizes`: the shape of the output  `dimensions`: a permutation representing the transposition to the input before actually applying the reshape. Historical, XLA's `xla.ReshapeOp` takes the `dimensions`, but now `stablehlo.ReshapeOp` only takes `new_sizes`. As consequence, currently, logic for `transpose_p` is inlined in `_reshape_{batch...}_rule` just to handle the case where `dimensions is not None`. I think we can do better by simply the entrypoint for `lax.reshape_p` which is `lax.reshape`, and in turn simplify all `lax.reshape_p`'s rules: ```python def reshape(operand, new_sizes, dimensions):   operand = transpose(operand, dimensions)   return reshape_p.bind(operand, new_size=new_size) ``` As a data point, `_reshape_typecheck_rule` already generate the decomposed form in Jaxpr. My question is: is there a good reason why we want to carry the transposition information with `lax.reshape_p`?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Why does `lax.reshape_p` carry `dimensions`(info. about transposition)?,"Currently, the `lax.reshape_p` primitive carries two pieces of information:  `new_sizes`: the shape of the output  `dimensions`: a permutation representing the transposition to the input before actually applying the reshape. Historical, XLA's `xla.ReshapeOp` takes the `dimensions`, but now `stablehlo.ReshapeOp` only takes `new_sizes`. As consequence, currently, logic for `transpose_p` is inlined in `_reshape_{batch...}_rule` just to handle the case where `dimensions is not None`. I think we can do better by simply the entrypoint for `lax.reshape_p` which is `lax.reshape`, and in turn simplify all `lax.reshape_p`'s rules: ```python def reshape(operand, new_sizes, dimensions):   operand = transpose(operand, dimensions)   return reshape_p.bind(operand, new_size=new_size) ``` As a data point, `_reshape_typecheck_rule` already generate the decomposed form in Jaxpr. My question is: is there a good reason why we want to carry the transposition information with `lax.reshape_p`?",2023-03-17T11:18:50Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/15053,"I tagged Peter because he might have more context on this... I suspect the reason this parameter still exists is because it used to be a requirement, and now removing it would require fixes to a number of downstream users. Peter, should we think about deprecating this in favor of explicit transposition?"
539,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Refactor the backwards compatibility tests.)， 内容是 ([jax2tf] Refactor the backwards compatibility tests. Move the test data into a separate directory, otherwise the test file will get too big. Keep only `dict` in the test data. Also found and fixed a bug where the xla_call_module was running on CPU even if there was a TPU available. Fix with tf.device(...).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Refactor the backwards compatibility tests.,"[jax2tf] Refactor the backwards compatibility tests. Move the test data into a separate directory, otherwise the test file will get too big. Keep only `dict` in the test data. Also found and fixed a bug where the xla_call_module was running on CPU even if there was a TPU available. Fix with tf.device(...).",2023-03-17T08:26:27Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15050
1405,"以下是一个github上的jax下的一个issue, 标题是(Regression in JAX while_loop)， 内容是 ( Description The following gist leads to an infinite loop with jaxlib==0.4.6 even though it is guaranteed to terminate and does exactly that with jaxlib==0.4.4: https://gist.github.com/Edenhofer/ece9a2e3e8c67721dbdd706b3966f04c . The code enters the infinite loop on the following CPUs AMD Ryzen 7 4800H, AMD Ryzen 5 PRO 3400G, AMD Opteron Processor 6376 . On an Intel Xeon Gold 6150 CPU the example runs fine. However, a more elaborate version of the example also leads to an infinite loop on Intel hardware. The gist reimplements the subproblem solver for a trust region Newton conjugate gradient scheme. It is conceptually similar to the scipy implementation. The error only arises for the eggholder potential and is not reproducible for any other of the simple cost function shown in the repro. Apologies for not being able to trim down the reproducing example even further. The error seems to be very brittle and slight modifications make it work again although oftentimes yielding wrong results.  What jax/jaxlib version are you using? jax==0.4.6, jaxlib==0.4.6  Which accelerator(s) are you using? CPU  Additional system info python 3.9 and python 3.10, Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Regression in JAX while_loop," Description The following gist leads to an infinite loop with jaxlib==0.4.6 even though it is guaranteed to terminate and does exactly that with jaxlib==0.4.4: https://gist.github.com/Edenhofer/ece9a2e3e8c67721dbdd706b3966f04c . The code enters the infinite loop on the following CPUs AMD Ryzen 7 4800H, AMD Ryzen 5 PRO 3400G, AMD Opteron Processor 6376 . On an Intel Xeon Gold 6150 CPU the example runs fine. However, a more elaborate version of the example also leads to an infinite loop on Intel hardware. The gist reimplements the subproblem solver for a trust region Newton conjugate gradient scheme. It is conceptually similar to the scipy implementation. The error only arises for the eggholder potential and is not reproducible for any other of the simple cost function shown in the repro. Apologies for not being able to trim down the reproducing example even further. The error seems to be very brittle and slight modifications make it work again although oftentimes yielding wrong results.  What jax/jaxlib version are you using? jax==0.4.6, jaxlib==0.4.6  Which accelerator(s) are you using? CPU  Additional system info python 3.9 and python 3.10, Linux  NVIDIA GPU info _No response_",2023-03-16T18:49:32Z,bug,closed,1,1,https://github.com/jax-ml/jax/issues/15035,Fixed in jax (and jaxlib) version 0.4.28 or earlier.
406,"以下是一个github上的jax下的一个issue, 标题是(Remove the check for `if not isinstance(old_token, array.ArrayImpl)` since py_executable always return jax.Arrays)， 内容是 (Remove the check for `if not isinstance(old_token, array.ArrayImpl)` since py_executable always return jax.Arrays)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Remove the check for `if not isinstance(old_token, array.ArrayImpl)` since py_executable always return jax.Arrays","Remove the check for `if not isinstance(old_token, array.ArrayImpl)` since py_executable always return jax.Arrays",2023-03-16T00:07:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15022
434,"以下是一个github上的jax下的一个issue, 标题是([sparse] add BCOO lowering for div)， 内容是 (We had avoiding this previously because dividing by zero is a densifying operation, but we already support mul which has similar issues if the operand contains infinities. Fixes CC(Sparse rule for div is not implemented))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] add BCOO lowering for div,"We had avoiding this previously because dividing by zero is a densifying operation, but we already support mul which has similar issues if the operand contains infinities. Fixes CC(Sparse rule for div is not implemented)",2023-03-14T18:59:23Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/14987
647,"以下是一个github上的jax下的一个issue, 标题是([dynamic-shapes] don't require buf objects have dtype attribute)， 内容是 (Fixes ireeorg/ireejax CC(v0.2 tasks) An alternative fix would've been just to add the dtype attribute to IreeBuffer. But it seems better not to make demands on the underlying runtime objects when we don't need to. I had to run the test with: ``` JAX_PLATFORM_NAME=iree JAX_ARRAY=0 JAX_JIT_PJIT_API_MERGE=0 python tests/dynamic_api_test.py DynamicShapeTest.test_iree_buffer_doesnt_need_dtype_attribute ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[dynamic-shapes] don't require buf objects have dtype attribute,Fixes ireeorg/ireejax CC(v0.2 tasks) An alternative fix would've been just to add the dtype attribute to IreeBuffer. But it seems better not to make demands on the underlying runtime objects when we don't need to. I had to run the test with: ``` JAX_PLATFORM_NAME=iree JAX_ARRAY=0 JAX_JIT_PJIT_API_MERGE=0 python tests/dynamic_api_test.py DynamicShapeTest.test_iree_buffer_doesnt_need_dtype_attribute ```,2023-03-14T18:19:19Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14986
4028,"以下是一个github上的jax下的一个issue, 标题是([FR] support `jax.distributed` for a subset of TPU cores)， 内容是 (Hello Jax team, I have noticed that `jax.distributed` currently only supports a subset of GPU cores, and I was wondering if it would be possible to extend this support to a subset of TPU cores as well. I have tried running a code snippet with jax.pmap on TPUs using the suggested snippet from  in https://github.com/google/jax/issues/8809issuecomment988495501, but it only recognizes the local TPU device and not other devices, resulting in a failed psum operation. For comparison, I have also included the output of the same code snippet when run on GPUs for reference. ```python import jax import jax.numpy as jnp jax.distributed.initialize() print(""process_count"", jax.process_count()) print(""process_index"", jax.process_index()) print(""local_devices"", jax.local_devices()) print(""local_device_count"", jax.local_device_count()) print(""devices"", jax.devices()) def my_function(x):     return jax.lax.psum(x, 'i') x = jnp.ones((1, 2)) print(x) parallel_fn = jax.pmap(my_function, devices=jax.devices(), axis_name='i') result = parallel_fn(x) print(result) ``` On GPUs, when running with  ``` CUDA_VISIBLE_DEVICES=""0"" SLURM_JOB_ID=26017 SLURM_STEP_NODELIST=localhost SLURM_NTASKS=2 SLURM_PROCID=0 SLURM_LOCALID=0 SLURM_STEP_NUM_NODES=2  python t.py CUDA_VISIBLE_DEVICES=""1"" SLURM_JOB_ID=26017 SLURM_STEP_NODELIST=localhost SLURM_NTASKS=2 SLURM_PROCID=1 SLURM_LOCALID=0 SLURM_STEP_NUM_NODES=2  python t.py ``` produces ``` process_count 2 process_index 1 local_devices [StreamExecutorGpuDevice(id=1, process_index=1, slice_index=0)] local_device_count 1 devices [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0), StreamExecutorGpuDevice(id=1, process_index=1, slice_index=0)] [[1. 1.]] [[2. 2.]] ``` on TPUs, when running with the snippet suggested by  at https://github.com/google/jax/issues/8809issuecomment988495501 ``` TPU_CHIPS_PER_HOST_BOUNDS=1,2,1 TPU_HOST_BOUNDS=1,1,1 TPU_VISIBLE_DEVICES=0 TPU_MESH_CONTROLLER_ADDRESS=localhost:8476 TPU_MESH_CONTROLLER_PORT=8476 SLURM_JOB_ID=26017 SLURM_STEP_NODELIST=localhost SLURM_NTASKS=2 SLURM_PROCID=0 SLURM_LOCALID=0 SLURM_STEP_NUM_NODES=2 python t.py TPU_CHIPS_PER_HOST_BOUNDS=1,2,1 TPU_HOST_BOUNDS=1,1,1 TPU_VISIBLE_DEVICES=1 TPU_MESH_CONTROLLER_ADDRESS=localhost:8476 TPU_MESH_CONTROLLER_PORT=8476 SLURM_JOB_ID=26017 SLURM_STEP_NODELIST=localhost SLURM_NTASKS=2 SLURM_PROCID=1 SLURM_LOCALID=0 SLURM_STEP_NUM_NODES=2 python t.py ``` we get the following. ``` process_count 1 process_index 0 local_devices [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] local_device_count 1 devices [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] [[1. 1.]] [[1. 1.]] ``` Note that the TPU example does not recognize other devices, thus making the `psum` fails.   Use cases We are using `jax.distributed` this way in our new distributed RL platform Cleanba, which supports different ways to leverage accelerators. Specifically, the users can only use 1 accelerator core for the actor, multiple accelerator cores for the learner, and replicate the training process through `jax.distributed`. Below is a figure showing our results  The format `a0_l1 2 3 4_d1` means the actor runs on GPU 0 (`a0`), the learner runs on GPU 1 2 3 4 (`l1 2 3 4`), and the computation is replicated/distributed one time (`d1`). The blue baseline curve corresponds to `a0_l1 2 3_d2`. TPUv4 is fast, and the results on `a0_l1 2_d1` is pretty great. However, because of the problem listed in this issue, I couldn't test out the `a0_l1_d2` to fully utilize 4 chips on TPUv4. If `a0_l1_d2` were to work, I can expect it to be 1020% faster than our current fastest 8 A100 setting, which would be very exciting 😬  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[FR] support `jax.distributed` for a subset of TPU cores,"Hello Jax team, I have noticed that `jax.distributed` currently only supports a subset of GPU cores, and I was wondering if it would be possible to extend this support to a subset of TPU cores as well. I have tried running a code snippet with jax.pmap on TPUs using the suggested snippet from  in https://github.com/google/jax/issues/8809issuecomment988495501, but it only recognizes the local TPU device and not other devices, resulting in a failed psum operation. For comparison, I have also included the output of the same code snippet when run on GPUs for reference. ```python import jax import jax.numpy as jnp jax.distributed.initialize() print(""process_count"", jax.process_count()) print(""process_index"", jax.process_index()) print(""local_devices"", jax.local_devices()) print(""local_device_count"", jax.local_device_count()) print(""devices"", jax.devices()) def my_function(x):     return jax.lax.psum(x, 'i') x = jnp.ones((1, 2)) print(x) parallel_fn = jax.pmap(my_function, devices=jax.devices(), axis_name='i') result = parallel_fn(x) print(result) ``` On GPUs, when running with  ``` CUDA_VISIBLE_DEVICES=""0"" SLURM_JOB_ID=26017 SLURM_STEP_NODELIST=localhost SLURM_NTASKS=2 SLURM_PROCID=0 SLURM_LOCALID=0 SLURM_STEP_NUM_NODES=2  python t.py CUDA_VISIBLE_DEVICES=""1"" SLURM_JOB_ID=26017 SLURM_STEP_NODELIST=localhost SLURM_NTASKS=2 SLURM_PROCID=1 SLURM_LOCALID=0 SLURM_STEP_NUM_NODES=2  python t.py ``` produces ``` process_count 2 process_index 1 local_devices [StreamExecutorGpuDevice(id=1, process_index=1, slice_index=0)] local_device_count 1 devices [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0), StreamExecutorGpuDevice(id=1, process_index=1, slice_index=0)] [[1. 1.]] [[2. 2.]] ``` on TPUs, when running with the snippet suggested by  at https://github.com/google/jax/issues/8809issuecomment988495501 ``` TPU_CHIPS_PER_HOST_BOUNDS=1,2,1 TPU_HOST_BOUNDS=1,1,1 TPU_VISIBLE_DEVICES=0 TPU_MESH_CONTROLLER_ADDRESS=localhost:8476 TPU_MESH_CONTROLLER_PORT=8476 SLURM_JOB_ID=26017 SLURM_STEP_NODELIST=localhost SLURM_NTASKS=2 SLURM_PROCID=0 SLURM_LOCALID=0 SLURM_STEP_NUM_NODES=2 python t.py TPU_CHIPS_PER_HOST_BOUNDS=1,2,1 TPU_HOST_BOUNDS=1,1,1 TPU_VISIBLE_DEVICES=1 TPU_MESH_CONTROLLER_ADDRESS=localhost:8476 TPU_MESH_CONTROLLER_PORT=8476 SLURM_JOB_ID=26017 SLURM_STEP_NODELIST=localhost SLURM_NTASKS=2 SLURM_PROCID=1 SLURM_LOCALID=0 SLURM_STEP_NUM_NODES=2 python t.py ``` we get the following. ``` process_count 1 process_index 0 local_devices [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] local_device_count 1 devices [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] [[1. 1.]] [[1. 1.]] ``` Note that the TPU example does not recognize other devices, thus making the `psum` fails.   Use cases We are using `jax.distributed` this way in our new distributed RL platform Cleanba, which supports different ways to leverage accelerators. Specifically, the users can only use 1 accelerator core for the actor, multiple accelerator cores for the learner, and replicate the training process through `jax.distributed`. Below is a figure showing our results  The format `a0_l1 2 3 4_d1` means the actor runs on GPU 0 (`a0`), the learner runs on GPU 1 2 3 4 (`l1 2 3 4`), and the computation is replicated/distributed one time (`d1`). The blue baseline curve corresponds to `a0_l1 2 3_d2`. TPUv4 is fast, and the results on `a0_l1 2_d1` is pretty great. However, because of the problem listed in this issue, I couldn't test out the `a0_l1_d2` to fully utilize 4 chips on TPUv4. If `a0_l1_d2` were to work, I can expect it to be 1020% faster than our current fastest 8 A100 setting, which would be very exciting 😬  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2023-03-14T14:49:35Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/14977,"Hi, sorry for the delay on this! I'm not 100% clear on the exact configuration you're trying to run. I updated https://gist.github.com/skye/f82ba45d2445bb19d53545538754f9a3 with a few more examples of communicating processes. For example, to run 2 communicating processes each with 2 chips: ``` TPU_CHIPS_PER_PROCESS_BOUNDS=1,2,1 \   TPU_PROCESS_BOUNDS=2,1,1 \   TPU_PROCESS_ADDRESSES=localhost:8476,localhost:8477 \   TPU_VISIBLE_DEVICES=0,1 \   TPU_PROCESS_PORT=8476 \   CLOUD_TPU_TASK_ID=0 \   python3 c 'import jax print(""device count"", jax.device_count()) print(""local device count"", jax.local_device_count()) print(""process idx"", jax.process_index()) print(""local devices"", jax.local_devices())' TPU_CHIPS_PER_PROCESS_BOUNDS=1,2,1 \   TPU_PROCESS_BOUNDS=2,1,1 \   TPU_PROCESS_ADDRESSES=localhost:8476,localhost:8477 \   TPU_VISIBLE_DEVICES=2,3 \   TPU_PROCESS_PORT=8477 \   CLOUD_TPU_TASK_ID=1 \   python3 c 'import jax print(""device count"", jax.device_count()) print(""local device count"", jax.local_device_count()) print(""process idx"", jax.process_index()) print(""local devices"", jax.local_devices())' ```  yields: ``` device count 4 local device count 2 process idx 0 local devices [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)] device count 4 local device count 2 process idx 1 local devices [TpuDevice(id=1, process_index=1, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=1, coords=(1,1,0), core_on_chip=0)] ``` Note that for communicating processes, you must have an equal number of chips in each process. Please let me know if this answers your question, or if you're trying to do something else!","That you so much ! This is precisely what I needed! My experiments show that in my particular setting, 8 TPUv4 cores perform similarly to 8 A100 cores. Closing this issue now. "
577,"以下是一个github上的jax下的一个issue, 标题是([Rollforward] Move PyBuffer methods used by PyArray to c++.)， 内容是 ([Rollforward] Move PyBuffer methods used by PyArray to c++. ```   def delete(self): ...   def unsafe_buffer_pointer(self) > Any: ...   def clone(self) > ArrayImpl: ...   def _copy_single_device_array_to_host_async(self): ...   def _single_device_array_to_np_array(self) > np.ndarray: ...   def on_device_size_in_bytes(self) > int: ... ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Rollforward] Move PyBuffer methods used by PyArray to c++.,[Rollforward] Move PyBuffer methods used by PyArray to c++. ```   def delete(self): ...   def unsafe_buffer_pointer(self) > Any: ...   def clone(self) > ArrayImpl: ...   def _copy_single_device_array_to_host_async(self): ...   def _single_device_array_to_np_array(self) > np.ndarray: ...   def on_device_size_in_bytes(self) > int: ... ```,2023-03-13T23:30:11Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14963
41039,"以下是一个github上的jax下的一个issue, 标题是(Command is longer than CreateProcessW's limit error)， 内容是 ( Description When building attempting to build JAX on Windows with CUDA support, I run into the following error.  ``` INFO: Analyzed target //build:build_wheel (1 packages loaded, 169 targets configured). INFO: Found 1 target... ERROR: C:/_bzl/z5jgcps7/external/org_tensorflow/tensorflow/compiler/xla/python/BUILD:862:21: Compiling tensorflow/compiler/xla/python/xla.: (Exit 1): python.exe failed: error executing command   cd /d C:/_bzl/z5jgcps7/execroot/__main__   SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows  Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt     SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program  Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64     SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files  (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files  (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual  Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files  (x86)\Microsoft Visual Studio\2019\Enterprise\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual  Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja     SET PWD=/proc/self/cwd     SET RUNFILES_MANIFEST_ONLY=1     SET TEMP=C:\Users\Adam\AppData\Local\Temp     SET TF_CUDA_COMPUTE_CAPABILITIES=7.5     SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET TF_CUDA_VERSION=11.7     SET TF_CUDNN_VERSION=8.4.0     SET TMP=C:\Users\Adam\AppData\Local\Temp   C:\Users\Adam\anaconda3\envs\jax_latest\python.exe B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996  /Iexternal/org_tensorflow /Ibazelout/x64_windowsopt/bin/external/org_tensorflow /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazelout/x64_windowsopt/bin/external/com_google_absl /Iexternal/nsync /Ibazelout/x64_windowsopt/bin/external/nsync /Iexternal/double_conversion  /Ibazelout/x64_windowsopt/bin/external/double_conversion /Iexternal/com_google_protobuf /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf /Iexternal/local_config_python /Ibazelout/x64_windowsopt/bin/external/local_config_python /Iexternal/pybind11 /Ibazelout/x64_windowsopt/bin/external/pybind11 /Iexternal/snappy  /Ibazelout/x64_windowsopt/bin/external/snappy /Iexternal/com_googlesource_code_re2 /Ibazelout/x64_windowsopt/bin/external/com_googlesource_code_re2 /Iexternal/pybind11_abseil /Ibazelout/x64_windowsopt/bin/external/pybind11_abseil /Iexternal/local_config_cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda  /Iexternal/local_config_rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt /Iexternal/farmhash_archive /Ibazelout/x64_windowsopt/bin/external/farmhash_archive /Iexternal/tf_runtime /Ibazelout/x64_windowsopt/bin/external/tf_runtime  /Iexternal/llvmproject /Ibazelout/x64_windowsopt/bin/external/llvmproject /Iexternal/llvm_terminfo /Ibazelout/x64_windowsopt/bin/external/llvm_terminfo /Iexternal/llvm_zlib /Ibazelout/x64_windowsopt/bin/external/llvm_zlib /Iexternal/stablehlo /Ibazelout/x64_windowsopt/bin/external/stablehlo /Iexternal/com_github_grpc_grpc  /Ibazelout/x64_windowsopt/bin/external/com_github_grpc_grpc /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/upb /Ibazelout/x64_windowsopt/bin/external/upb /Iexternal/boringssl /Ibazelout/x64_windowsopt/bin/external/boringssl /Iexternal/mkl_dnn_v1 /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1 /Iexternal/dlpack  /Ibazelout/x64_windowsopt/bin/external/dlpack /Iexternal/jsoncpp_git /Ibazelout/x64_windowsopt/bin/external/jsoncpp_git /Iexternal/triton /Ibazelout/x64_windowsopt/bin/external/triton /Iexternal/cudnn_frontend_archive /Ibazelout/x64_windowsopt/bin/external/cudnn_frontend_archive /Iexternal/bazel_tools  /Ibazelout/x64_windowsopt/bin/external/bazel_tools /Ibazelout/x64_windowsopt/bin/external/pybind11/_virtual_includes/pybind11 /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinDialectIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinLocationAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CastOpInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FunctionInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpAsmInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SymbolInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorEncodingIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mlir_hlo /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/canonicalize_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/convert_op_folder /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_attrs_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_common /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_enums_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_pattern_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_typedefs_inc_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithCanonicalizationIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithOpsInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/InferIntRangeInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LoopLikeInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AsmParserTokenKinds /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/DialectUtilsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ViewLikeInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ControlFlowOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FuncIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMDialectInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMIntrinsicOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMTypesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CopyOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapedOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/QuantOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLTypesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ConversionPassIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformsPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapeOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AffineOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/DestinationStyleOpInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ParallelCombiningOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TilingInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorAttrDefsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorTypesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/RuntimeVerifiableOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/base /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/base_attr_interfaces_inc_gen  /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/broadcast_utils /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/chlo_ops /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/chlo_attrs_inc_gen /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/chlo_enums_inc_gen  /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/chlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_assembly_format /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_type_inference  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_passes /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/chlo_legalize_to_hlo  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/chlo_legalize_to_hlo_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_chlo_to_hlo_op  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AllocationOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizableOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationBaseIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationEnumsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SCFDeviceMappingInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SCFIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SCFPassIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_legalize_to_stablehlo  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_stablehlo_to_hlo_op /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_ops /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_attrs_inc_gen  /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_enums_inc_gen /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/legalize_to_linalg_utils  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_mhlo_to_scalar_op /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MathBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MathOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MaskableOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MaskingOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgEnumsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgStructuredOpsIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/legalize_to_standard_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_ops_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_ops_structs_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_structured_interface  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_structured_interface_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lower_complex_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_hlo_to_lhlo_op /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_pass_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_scatter_gather_utils /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/shape_component_analysis  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/stablehlo_legalize_to_hlo /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_ops_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_bufferizable_op_interface  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/type_conversion /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FuncTransformsPassIncGen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/unfuse_batch_norm /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/DLTIBaseIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgPassIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorEnumsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/X86VectorIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapeTransformsPassIncGen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_attrdefs_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_dialect_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_enums_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_ops  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/all_passes /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_passes  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_ops_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_utils /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_passes_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_passes /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_passes_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_transforms /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lmhlo_pass_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lmhlo_passes /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_lmhlo_to_scalar_op  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_lhlo_to_hlo_op /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_passes  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_passes_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/transforms_passes  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/transforms_passes_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/userange_analysis /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AMXIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArmNeonIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArmSVEIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMConversionIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/NVVMOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMIntrinsicConversionIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpenMPInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpenMPOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpenMPTypeInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/InstCombineTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformDialectEnumsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformDialectIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformDialectInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformTypesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformDialectTransformsIncGen /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/JITLinkTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AsyncOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86CodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86CommonTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86Info /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86UtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapeToStandardGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AMXConversionIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArmNeonConversionIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArmSVEConversionIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AsyncPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/X86VectorConversionIncGen  /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ROCDLConversionIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ROCDLOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/NVVMConversionIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/transforms_gpu_passes  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gpu_transforms_passes_inc_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUToNVVMGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUPassIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AMDGPUIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUToROCDLTGen /Ibazelout/x64_windowsopt/bin/external/triton/_virtual_includes/triton_combine_inc_gen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVAttrUtilsGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVAvailabilityIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVCanonicalizationIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVSerializationGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpenACCOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/IndexEnumsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/IndexOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/NVGPUIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/NVGPUPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TosaDialectIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TosaInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TosaPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AArch64CodeGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AArch64CommonTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AArch64Info /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AArch64UtilsAndDesc  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/ARMCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/ARMCommonTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/ARMInfo  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/ARMUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AMDGPUCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AMDGPUCommonTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/amdgpu_isel_target_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/r600_target_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AMDGPUInfo  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AMDGPUUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/NVPTXCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/NVPTXCommonTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/NVPTXInfo /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/NVPTXUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/PowerPCCodeGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/PowerPCCommonTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/PowerPCInfo /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/PowerPCUtilsAndDesc  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/SystemZCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/SystemZCommonTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/SystemZInfo  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/SystemZUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/RISCVCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/RISCVCommonTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/RISCVInfo /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/RISCVUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86DisassemblerInternalHeaders  /Ibazelout/x64_windowsopt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Iexternal/org_tensorflow/third_party/eigen3/mkl_include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/nsync/public  /Ibazelout/x64_windowsopt/bin/external/nsync/public /Iexternal/com_google_protobuf/src /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf/src /Iexternal/local_config_python/numpy_include /Ibazelout/x64_windowsopt/bin/external/local_config_python/numpy_include /Iexternal/local_config_python/python_include  /Ibazelout/x64_windowsopt/bin/external/local_config_python/python_include /Iexternal/pybind11/include /Ibazelout/x64_windowsopt/bin/external/pybind11/include /Iexternal/local_config_cuda/cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include  /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include  /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/farmhash_archive/src  /Ibazelout/x64_windowsopt/bin/external/farmhash_archive/src /Iexternal/tf_runtime/include /Ibazelout/x64_windowsopt/bin/external/tf_runtime/include /Iexternal/llvmproject/llvm/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/include /Iexternal/tf_runtime/third_party/llvm_derived/include  /Ibazelout/x64_windowsopt/bin/external/tf_runtime/third_party/llvm_derived/include /Iexternal/llvmproject/mlir/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/include /Iexternal/com_github_grpc_grpc/include /Ibazelout/x64_windowsopt/bin/external/com_github_grpc_grpc/include  /Iexternal/com_github_grpc_grpc/src/core/ext/upbgenerated /Ibazelout/x64_windowsopt/bin/external/com_github_grpc_grpc/src/core/ext/upbgenerated /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/com_github_grpc_grpc/third_party/address_sorting/include  /Ibazelout/x64_windowsopt/bin/external/com_github_grpc_grpc/third_party/address_sorting/include /Iexternal/boringssl/src/include /Ibazelout/x64_windowsopt/bin/external/boringssl/src/include /Iexternal/mkl_dnn_v1/include /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/include /Iexternal/mkl_dnn_v1/src  /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_dnn_v1/src/common /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/common /Iexternal/mkl_dnn_v1/src/common/ittnotify /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/common/ittnotify /Iexternal/mkl_dnn_v1/src/cpu  /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/cpu /Iexternal/mkl_dnn_v1/src/cpu/gemm /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/cpu/gemm /Iexternal/mkl_dnn_v1/src/cpu/x64/xbyak /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak  /Iexternal/org_tensorflow/tensorflow/compiler/xla/translate/hlo_to_mhlo/include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/translate/hlo_to_mhlo/include /Iexternal/local_config_cuda/cuda/cuda/extras/CUPTI/include /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include  /Iexternal/jsoncpp_git/include /Ibazelout/x64_windowsopt/bin/external/jsoncpp_git/include /Iexternal/llvmproject/llvm/lib/Target/X86 /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/X86 /Iexternal/llvmproject/mlir/lib/Conversion/TensorToLinalg  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TensorToLinalg /Iexternal/triton/include /Ibazelout/x64_windowsopt/bin/external/triton/include /Iexternal/llvmproject/mlir/lib/Conversion/FuncToSPIRV /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/FuncToSPIRV  /Iexternal/llvmproject/mlir/lib/Conversion/MathToSPIRV /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/MathToSPIRV /Iexternal/llvmproject/mlir/lib/Conversions/GPUToSPIRV /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversions/GPUToSPIRV /Iexternal/llvmproject/mlir/lib/Conversion/MemRefToSPIRV  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/MemRefToSPIRV /Iexternal/llvmproject/mlir/lib/Conversion/TensorToSPIRV /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TensorToSPIRV /Iexternal/llvmproject/mlir/lib/Conversion/TosaToArith  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TosaToArith /Iexternal/llvmproject/mlir/lib/Conversion/TosaToLinalg /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TosaToLinalg /Iexternal/llvmproject/mlir/lib/Conversion/TosaToSCF  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TosaToSCF /Iexternal/llvmproject/mlir/lib/Conversion/TosaToTensor /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TosaToTensor /Iexternal/llvmproject/llvm/lib/Target/AArch64  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/AArch64 /Iexternal/llvmproject/llvm/lib/Target/ARM /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/ARM /Iexternal/llvmproject/llvm/lib/Target/AMDGPU /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/AMDGPU  /Iexternal/llvmproject/llvm/lib/Target/NVPTX /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/NVPTX /Iexternal/llvmproject/llvm/lib/Target/PowerPC /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/PowerPC /Iexternal/llvmproject/llvm/lib/Target/SystemZ  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/SystemZ /Iexternal/llvmproject/llvm/lib/Target/RISCV /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/RISCV /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE  /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLTDL_SHLIB_EXT="".dll"" /DLLVM_PLUGIN_EXT="".dll"" /DLLVM_NATIVE_ARCH=""X86"" /DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser /DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter /DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler  /DLLVM_NATIVE_TARGET=LLVMInitializeX86Target /DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo /DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC /DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA /DLLVM_HOST_TRIPLE=""x86_64pcwin32"" /DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64pcwin32"" /DLLVM_VERSION_MAJOR=17 /DLLVM_VERSION_MINOR=0  /DLLVM_VERSION_PATCH=0 /DLLVM_VERSION_STRING=""17.0.0git"" /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DBLAKE3_USE_NEON=0 /DBLAKE3_NO_AVX2 /DBLAKE3_NO_AVX512 /DBLAKE3_NO_SSE2 /DBLAKE3_NO_SSE41 /DGRPC_ARES=0 /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DGEMM_KERNEL_H  /DGOOGLE_CUDA=1 /DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0 /DEIGEN_NEON_GEBP_NR=4 /DXLA_PYTHON_ENABLE_GPU=1 /showIncludes /MD /O2 /DNDEBUG /D_USE_MATH_DEFINES DWIN32_LEAN_AND_MEAN DNOGDI /Zc:preprocessor DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. /std:c++17 fnostrictaliasing fexceptions  /Fobazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/_objs/xla_extension.so/xla.obj /c external/org_tensorflow/tensorflow/compiler/xla/python/xla.cc  Configuration: 886c28b412be79c1c14ab803e30d0000e9a83aa6ffafe995f37bb4722b03ea3f  Execution platform: //:platform Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(165): CreateProcessWithExplicitHandles(""C:\Users\Adam\anaconda3\envs\jax_latest\python.exe"" B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE  /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/org_tensorflow /Ibazelout/x64_windowsopt/bin/external/org_tensorflow /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eig(...)): command is longer than CreateProcessW's limit (32767  characters) Target //build:build_wheel failed to build ```  What jax/jaxlib version are you using? jaxlib v0.4.6, jax 0.4.6  Which accelerator(s) are you using? GPU  Additional system info Windows 10, Python 3.9, Cuda 11.7, Cudnn 8.4.0  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Command is longer than CreateProcessW's limit error," Description When building attempting to build JAX on Windows with CUDA support, I run into the following error.  ``` INFO: Analyzed target //build:build_wheel (1 packages loaded, 169 targets configured). INFO: Found 1 target... ERROR: C:/_bzl/z5jgcps7/external/org_tensorflow/tensorflow/compiler/xla/python/BUILD:862:21: Compiling tensorflow/compiler/xla/python/xla.: (Exit 1): python.exe failed: error executing command   cd /d C:/_bzl/z5jgcps7/execroot/__main__   SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows  Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt     SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program  Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64     SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.29.30133\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files  (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files  (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual  Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files  (x86)\Microsoft Visual Studio\2019\Enterprise\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual  Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja     SET PWD=/proc/self/cwd     SET RUNFILES_MANIFEST_ONLY=1     SET TEMP=C:\Users\Adam\AppData\Local\Temp     SET TF_CUDA_COMPUTE_CAPABILITIES=7.5     SET TF_CUDA_PATHS=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7     SET TF_CUDA_VERSION=11.7     SET TF_CUDNN_VERSION=8.4.0     SET TMP=C:\Users\Adam\AppData\Local\Temp   C:\Users\Adam\anaconda3\envs\jax_latest\python.exe B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996  /Iexternal/org_tensorflow /Ibazelout/x64_windowsopt/bin/external/org_tensorflow /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazelout/x64_windowsopt/bin/external/com_google_absl /Iexternal/nsync /Ibazelout/x64_windowsopt/bin/external/nsync /Iexternal/double_conversion  /Ibazelout/x64_windowsopt/bin/external/double_conversion /Iexternal/com_google_protobuf /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf /Iexternal/local_config_python /Ibazelout/x64_windowsopt/bin/external/local_config_python /Iexternal/pybind11 /Ibazelout/x64_windowsopt/bin/external/pybind11 /Iexternal/snappy  /Ibazelout/x64_windowsopt/bin/external/snappy /Iexternal/com_googlesource_code_re2 /Ibazelout/x64_windowsopt/bin/external/com_googlesource_code_re2 /Iexternal/pybind11_abseil /Ibazelout/x64_windowsopt/bin/external/pybind11_abseil /Iexternal/local_config_cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda  /Iexternal/local_config_rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt /Iexternal/farmhash_archive /Ibazelout/x64_windowsopt/bin/external/farmhash_archive /Iexternal/tf_runtime /Ibazelout/x64_windowsopt/bin/external/tf_runtime  /Iexternal/llvmproject /Ibazelout/x64_windowsopt/bin/external/llvmproject /Iexternal/llvm_terminfo /Ibazelout/x64_windowsopt/bin/external/llvm_terminfo /Iexternal/llvm_zlib /Ibazelout/x64_windowsopt/bin/external/llvm_zlib /Iexternal/stablehlo /Ibazelout/x64_windowsopt/bin/external/stablehlo /Iexternal/com_github_grpc_grpc  /Ibazelout/x64_windowsopt/bin/external/com_github_grpc_grpc /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/upb /Ibazelout/x64_windowsopt/bin/external/upb /Iexternal/boringssl /Ibazelout/x64_windowsopt/bin/external/boringssl /Iexternal/mkl_dnn_v1 /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1 /Iexternal/dlpack  /Ibazelout/x64_windowsopt/bin/external/dlpack /Iexternal/jsoncpp_git /Ibazelout/x64_windowsopt/bin/external/jsoncpp_git /Iexternal/triton /Ibazelout/x64_windowsopt/bin/external/triton /Iexternal/cudnn_frontend_archive /Ibazelout/x64_windowsopt/bin/external/cudnn_frontend_archive /Iexternal/bazel_tools  /Ibazelout/x64_windowsopt/bin/external/bazel_tools /Ibazelout/x64_windowsopt/bin/external/pybind11/_virtual_includes/pybind11 /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazelout/x64_windowsopt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinDialectIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinLocationAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CastOpInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FunctionInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpAsmInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SymbolInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorEncodingIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mlir_hlo /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/canonicalize_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/convert_op_folder /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_attrs_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_common /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_enums_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_pattern_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_typedefs_inc_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithCanonicalizationIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithOpsInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/InferIntRangeInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LoopLikeInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AsmParserTokenKinds /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/DialectUtilsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ViewLikeInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ComplexOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ControlFlowOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FuncIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMDialectInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMIntrinsicOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMTypesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/CopyOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapedOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/QuantOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLTypesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ConversionPassIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformsPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapeOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AffineOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/DestinationStyleOpInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ParallelCombiningOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TilingInterfaceIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorAttrDefsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorTypesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/RuntimeVerifiableOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/base /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/base_attr_interfaces_inc_gen  /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/broadcast_utils /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/chlo_ops /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/chlo_attrs_inc_gen /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/chlo_enums_inc_gen  /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/chlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_assembly_format /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_type_inference  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_passes /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/chlo_legalize_to_hlo  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/chlo_legalize_to_hlo_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_chlo_to_hlo_op  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AllocationOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizableOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationBaseIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationEnumsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SCFDeviceMappingInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SCFIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SCFPassIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_legalize_to_stablehlo  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_stablehlo_to_hlo_op /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_ops /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_attrs_inc_gen  /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_enums_inc_gen /Ibazelout/x64_windowsopt/bin/external/stablehlo/_virtual_includes/stablehlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/legalize_to_linalg_utils  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_mhlo_to_scalar_op /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MathBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MathOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MaskableOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MaskingOpInterfaceIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgEnumsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgStructuredOpsIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/legalize_to_standard_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_ops_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_ops_structs_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_structured_interface  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_structured_interface_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lower_complex_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_hlo_to_lhlo_op /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_pass_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_scatter_gather_utils /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/shape_component_analysis  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/stablehlo_legalize_to_hlo /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_ops_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_bufferizable_op_interface  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/type_conversion /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/BufferizationPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/FuncTransformsPassIncGen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/unfuse_batch_norm /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArithPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/DLTIBaseIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUBaseIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LinalgPassIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/MemRefPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TensorPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorEnumsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/VectorPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/X86VectorIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapeTransformsPassIncGen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_attrdefs_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_dialect_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_enums_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_ops  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/all_passes /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_passes  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_ops_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_utils /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_passes_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_passes /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_passes_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_transforms /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lmhlo_pass_inc_gen  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lmhlo_passes /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_lmhlo_to_scalar_op  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_lhlo_to_hlo_op /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_passes  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_passes_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/transforms_passes  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/transforms_passes_inc_gen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/userange_analysis /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AMXIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArmNeonIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArmSVEIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMConversionIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/NVVMOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/LLVMIntrinsicConversionIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpenMPInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpenMPOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpenMPTypeInterfacesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/InstCombineTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformDialectEnumsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformDialectIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformDialectInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformTypesIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TransformDialectTransformsIncGen /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/JITLinkTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AsyncOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86CodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86CommonTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86Info /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86UtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ShapeToStandardGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SparseTensorPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AMXConversionIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArmNeonConversionIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ArmSVEConversionIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AsyncPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/X86VectorConversionIncGen  /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ROCDLConversionIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/ROCDLOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/NVVMConversionIncGen /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/transforms_gpu_passes  /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gpu_transforms_passes_inc_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUToNVVMGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUPassIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/AMDGPUIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/GPUToROCDLTGen /Ibazelout/x64_windowsopt/bin/external/triton/_virtual_includes/triton_combine_inc_gen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVAttrUtilsGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVAttributesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVAvailabilityIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVCanonicalizationIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/SPIRVSerializationGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/OpenACCOpsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/IndexEnumsIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/IndexOpsIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/NVGPUIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/NVGPUPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TosaDialectIncGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TosaInterfacesIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/_virtual_includes/TosaPassIncGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AArch64CodeGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AArch64CommonTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AArch64Info /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AArch64UtilsAndDesc  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/ARMCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/ARMCommonTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/ARMInfo  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/ARMUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AMDGPUCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AMDGPUCommonTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/amdgpu_isel_target_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/r600_target_gen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AMDGPUInfo  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/AMDGPUUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/NVPTXCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/NVPTXCommonTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/NVPTXInfo /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/NVPTXUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/PowerPCCodeGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/PowerPCCommonTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/PowerPCInfo /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/PowerPCUtilsAndDesc  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/SystemZCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/SystemZCommonTableGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/SystemZInfo  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/SystemZUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/RISCVCodeGen /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/RISCVCommonTableGen  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/RISCVInfo /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/RISCVUtilsAndDesc /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/_virtual_includes/X86DisassemblerInternalHeaders  /Ibazelout/x64_windowsopt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Iexternal/org_tensorflow/third_party/eigen3/mkl_include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eigen_archive /Iexternal/nsync/public  /Ibazelout/x64_windowsopt/bin/external/nsync/public /Iexternal/com_google_protobuf/src /Ibazelout/x64_windowsopt/bin/external/com_google_protobuf/src /Iexternal/local_config_python/numpy_include /Ibazelout/x64_windowsopt/bin/external/local_config_python/numpy_include /Iexternal/local_config_python/python_include  /Ibazelout/x64_windowsopt/bin/external/local_config_python/python_include /Iexternal/pybind11/include /Ibazelout/x64_windowsopt/bin/external/pybind11/include /Iexternal/local_config_cuda/cuda /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include  /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include  /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazelout/x64_windowsopt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/farmhash_archive/src  /Ibazelout/x64_windowsopt/bin/external/farmhash_archive/src /Iexternal/tf_runtime/include /Ibazelout/x64_windowsopt/bin/external/tf_runtime/include /Iexternal/llvmproject/llvm/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/include /Iexternal/tf_runtime/third_party/llvm_derived/include  /Ibazelout/x64_windowsopt/bin/external/tf_runtime/third_party/llvm_derived/include /Iexternal/llvmproject/mlir/include /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/include /Iexternal/com_github_grpc_grpc/include /Ibazelout/x64_windowsopt/bin/external/com_github_grpc_grpc/include  /Iexternal/com_github_grpc_grpc/src/core/ext/upbgenerated /Ibazelout/x64_windowsopt/bin/external/com_github_grpc_grpc/src/core/ext/upbgenerated /Iexternal/zlib /Ibazelout/x64_windowsopt/bin/external/zlib /Iexternal/com_github_grpc_grpc/third_party/address_sorting/include  /Ibazelout/x64_windowsopt/bin/external/com_github_grpc_grpc/third_party/address_sorting/include /Iexternal/boringssl/src/include /Ibazelout/x64_windowsopt/bin/external/boringssl/src/include /Iexternal/mkl_dnn_v1/include /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/include /Iexternal/mkl_dnn_v1/src  /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_dnn_v1/src/common /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/common /Iexternal/mkl_dnn_v1/src/common/ittnotify /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/common/ittnotify /Iexternal/mkl_dnn_v1/src/cpu  /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/cpu /Iexternal/mkl_dnn_v1/src/cpu/gemm /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/cpu/gemm /Iexternal/mkl_dnn_v1/src/cpu/x64/xbyak /Ibazelout/x64_windowsopt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak  /Iexternal/org_tensorflow/tensorflow/compiler/xla/translate/hlo_to_mhlo/include /Ibazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/translate/hlo_to_mhlo/include /Iexternal/local_config_cuda/cuda/cuda/extras/CUPTI/include /Ibazelout/x64_windowsopt/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include  /Iexternal/jsoncpp_git/include /Ibazelout/x64_windowsopt/bin/external/jsoncpp_git/include /Iexternal/llvmproject/llvm/lib/Target/X86 /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/X86 /Iexternal/llvmproject/mlir/lib/Conversion/TensorToLinalg  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TensorToLinalg /Iexternal/triton/include /Ibazelout/x64_windowsopt/bin/external/triton/include /Iexternal/llvmproject/mlir/lib/Conversion/FuncToSPIRV /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/FuncToSPIRV  /Iexternal/llvmproject/mlir/lib/Conversion/MathToSPIRV /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/MathToSPIRV /Iexternal/llvmproject/mlir/lib/Conversions/GPUToSPIRV /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversions/GPUToSPIRV /Iexternal/llvmproject/mlir/lib/Conversion/MemRefToSPIRV  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/MemRefToSPIRV /Iexternal/llvmproject/mlir/lib/Conversion/TensorToSPIRV /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TensorToSPIRV /Iexternal/llvmproject/mlir/lib/Conversion/TosaToArith  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TosaToArith /Iexternal/llvmproject/mlir/lib/Conversion/TosaToLinalg /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TosaToLinalg /Iexternal/llvmproject/mlir/lib/Conversion/TosaToSCF  /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TosaToSCF /Iexternal/llvmproject/mlir/lib/Conversion/TosaToTensor /Ibazelout/x64_windowsopt/bin/external/llvmproject/mlir/lib/Conversion/TosaToTensor /Iexternal/llvmproject/llvm/lib/Target/AArch64  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/AArch64 /Iexternal/llvmproject/llvm/lib/Target/ARM /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/ARM /Iexternal/llvmproject/llvm/lib/Target/AMDGPU /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/AMDGPU  /Iexternal/llvmproject/llvm/lib/Target/NVPTX /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/NVPTX /Iexternal/llvmproject/llvm/lib/Target/PowerPC /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/PowerPC /Iexternal/llvmproject/llvm/lib/Target/SystemZ  /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/SystemZ /Iexternal/llvmproject/llvm/lib/Target/RISCV /Ibazelout/x64_windowsopt/bin/external/llvmproject/llvm/lib/Target/RISCV /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DTF_USE_SNAPPY /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE  /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLTDL_SHLIB_EXT="".dll"" /DLLVM_PLUGIN_EXT="".dll"" /DLLVM_NATIVE_ARCH=""X86"" /DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser /DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter /DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler  /DLLVM_NATIVE_TARGET=LLVMInitializeX86Target /DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo /DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC /DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA /DLLVM_HOST_TRIPLE=""x86_64pcwin32"" /DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64pcwin32"" /DLLVM_VERSION_MAJOR=17 /DLLVM_VERSION_MINOR=0  /DLLVM_VERSION_PATCH=0 /DLLVM_VERSION_STRING=""17.0.0git"" /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DBLAKE3_USE_NEON=0 /DBLAKE3_NO_AVX2 /DBLAKE3_NO_AVX512 /DBLAKE3_NO_SSE2 /DBLAKE3_NO_SSE41 /DGRPC_ARES=0 /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DGEMM_KERNEL_H  /DGOOGLE_CUDA=1 /DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0 /DEIGEN_NEON_GEBP_NR=4 /DXLA_PYTHON_ENABLE_GPU=1 /showIncludes /MD /O2 /DNDEBUG /D_USE_MATH_DEFINES DWIN32_LEAN_AND_MEAN DNOGDI /Zc:preprocessor DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. /std:c++17 fnostrictaliasing fexceptions  /Fobazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/_objs/xla_extension.so/xla.obj /c external/org_tensorflow/tensorflow/compiler/xla/python/xla.cc  Configuration: 886c28b412be79c1c14ab803e30d0000e9a83aa6ffafe995f37bb4722b03ea3f  Execution platform: //:platform Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(165): CreateProcessWithExplicitHandles(""C:\Users\Adam\anaconda3\envs\jax_latest\python.exe"" B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE  /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/org_tensorflow /Ibazelout/x64_windowsopt/bin/external/org_tensorflow /Iexternal/eigen_archive /Ibazelout/x64_windowsopt/bin/external/eig(...)): command is longer than CreateProcessW's limit (32767  characters) Target //build:build_wheel failed to build ```  What jax/jaxlib version are you using? jaxlib v0.4.6, jax 0.4.6  Which accelerator(s) are you using? GPU  Additional system info Windows 10, Python 3.9, Cuda 11.7, Cudnn 8.4.0  NVIDIA GPU info _No response_",2023-03-13T16:09:15Z,bug build contributions welcome NVIDIA GPU Windows,closed,0,2,https://github.com/jax-ml/jax/issues/14950,This is still an issue in jaxlib v0.4.7,I think its related to this.... https://github.com/bazelbuild/bazel/issues/5163
605,"以下是一个github上的jax下的一个issue, 标题是(Improve pytype inference for Sharding type.)， 内容是 (Improve pytype inference for Sharding type. * Define use_cpp_class and use_cpp_method decorators as noops for type checking. * Remove the use of abc.ABC when defining the Sharding type. This triggers a pytype bug: the easiest fix seems to be to skip the use of the ABC. * Write use_cpp_class decorator differently on ArrayImpl to work around pytype bug. * Fix a few new type errors.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve pytype inference for Sharding type.,Improve pytype inference for Sharding type. * Define use_cpp_class and use_cpp_method decorators as noops for type checking. * Remove the use of abc.ABC when defining the Sharding type. This triggers a pytype bug: the easiest fix seems to be to skip the use of the ABC. * Write use_cpp_class decorator differently on ArrayImpl to work around pytype bug. * Fix a few new type errors.,2023-03-13T15:55:41Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14948
571,"以下是一个github上的jax下的一个issue, 标题是([Rollback] Move PyBuffer methods used by PyArray to c++.)， 内容是 ([Rollback] Move PyBuffer methods used by PyArray to c++. ```   def delete(self): ...   def unsafe_buffer_pointer(self) > Any: ...   def clone(self) > ArrayImpl: ...   def _copy_single_device_array_to_host_async(self): ...   def _single_device_array_to_np_array(self) > np.ndarray: ...   def on_device_size_in_bytes(self) > int: ... ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Rollback] Move PyBuffer methods used by PyArray to c++.,[Rollback] Move PyBuffer methods used by PyArray to c++. ```   def delete(self): ...   def unsafe_buffer_pointer(self) > Any: ...   def clone(self) > ArrayImpl: ...   def _copy_single_device_array_to_host_async(self): ...   def _single_device_array_to_np_array(self) > np.ndarray: ...   def on_device_size_in_bytes(self) > int: ... ```,2023-03-11T17:40:10Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14932
889,"以下是一个github上的jax下的一个issue, 标题是(Profiling JAX with Nsight compute not working)， 内容是 ( Description Hi, I am trying to profile JAX code with Nsight compute. However, I am unable to do it. Here's a simple example: ```python from jax import random key = random.PRNGKey(1234) arr = random.uniform(key, shape=(10,)) jax.numpy.sin(arr) ``` ` ncu targetprocesses all python test.py` shows:  ```  ==PROF== Connected to process 4093916 (/home/user/anaconda3/bin/python3.9) ==ERROR== The application returned an error code (11). ==WARNING== No kernels were profiled.  ```  What jax/jaxlib version are you using? jax v0.4.6, jaxlib v0.4.6  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.13. OS Linux  NVIDIA GPU info ``` ++  ++++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Profiling JAX with Nsight compute not working," Description Hi, I am trying to profile JAX code with Nsight compute. However, I am unable to do it. Here's a simple example: ```python from jax import random key = random.PRNGKey(1234) arr = random.uniform(key, shape=(10,)) jax.numpy.sin(arr) ``` ` ncu targetprocesses all python test.py` shows:  ```  ==PROF== Connected to process 4093916 (/home/user/anaconda3/bin/python3.9) ==ERROR== The application returned an error code (11). ==WARNING== No kernels were profiled.  ```  What jax/jaxlib version are you using? jax v0.4.6, jaxlib v0.4.6  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.13. OS Linux  NVIDIA GPU info ``` ++  ++++ ```",2023-03-11T03:45:00Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/14930
329,"以下是一个github上的jax下的一个issue, 标题是([typing] better annotations for jnp.ndarray.at)， 内容是 (Fixes CC(Type hint  `at` property in `basearray.pyi`); part of CC(Tracking Issue: JAX Type Annotations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[typing] better annotations for jnp.ndarray.at,Fixes CC(Type hint  `at` property in `basearray.pyi`); part of CC(Tracking Issue: JAX Type Annotations),2023-03-10T22:59:26Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14927
749,"以下是一个github上的jax下的一个issue, 标题是(Type hint  `at` property in `basearray.pyi`)， 内容是 (Type hint `jax.Array.at` to aid IDEs. Didn't submit a PR directly because 1. Not sure if there is any circular import issue 2. `_IndexUpdateHelper` is in internal, do we change the name or `__name__` to make it look better? General question for JAX developers: is there a 'canonical' name for the `at`properties? `jax.Array.at` is not convenient for verbal communication. Maybe, functional array index accessor? I really want a name that is searchable, and I could refer to, say when answering questions in Github discussions.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Type hint  `at` property in `basearray.pyi`,"Type hint `jax.Array.at` to aid IDEs. Didn't submit a PR directly because 1. Not sure if there is any circular import issue 2. `_IndexUpdateHelper` is in internal, do we change the name or `__name__` to make it look better? General question for JAX developers: is there a 'canonical' name for the `at`properties? `jax.Array.at` is not convenient for verbal communication. Maybe, functional array index accessor? I really want a name that is searchable, and I could refer to, say when answering questions in Github discussions.",2023-03-10T17:58:19Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/14909,"Thanks for raising this! I think I left this out because circular imports make it tricky, but we should be able to figure it out. The name we use now is ""index update helper"", but that's a bit of a mouthful. If you search for ""jnp.ndarray.at"" you find this page: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html We'd be open to suggestions for making that more discoverable.","> The name we use now is ""index update helper"" I don't think it's a bad name. As for discoverability, I'd suggest inserting the exact phrase in ndarray.at document, Array class, jax.ops document, and maybe also in the shape bits. Something like ``` Index update helper  or the `.at`property ``` in CS paper title fashion. I was trying to sell the arrayapi people this api, now I can tell them the name of the product.","Unfortunately, the header on the `ndarray.at` doc page is generated by sphinx autodoc, so it's not easy to change it. We can certainly add more words in the dosctring though.","> We can certainly add more words in the dosctring though. Yes, this is what I meant."
630,"以下是一个github上的jax下的一个issue, 标题是([JAX] Split _src/xla_bridge.py into a separate Bazel target.)， 内容是 ([JAX] Split _src/xla_bridge.py into a separate Bazel target. Include _src/distributed.py and _src/clusters/*.py in the same target because they are in a stronglyconnected component. [XLA:Python] Set type of ArrayImpl to Any, since the JAX change now allows pytype to see that some values are ArrayImpls but ArrayImpls are not instances of jax.Array to Pytype. Fix type of buffer_from_pyval.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[JAX] Split _src/xla_bridge.py into a separate Bazel target.,"[JAX] Split _src/xla_bridge.py into a separate Bazel target. Include _src/distributed.py and _src/clusters/*.py in the same target because they are in a stronglyconnected component. [XLA:Python] Set type of ArrayImpl to Any, since the JAX change now allows pytype to see that some values are ArrayImpls but ArrayImpls are not instances of jax.Array to Pytype. Fix type of buffer_from_pyval.",2023-03-10T13:49:00Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14903
549,"以下是一个github上的jax下的一个issue, 标题是(Move PyBuffer methods used by PyArray to c++.)， 内容是 (Move PyBuffer methods used by PyArray to c++. ```   def delete(self): ...   def unsafe_buffer_pointer(self) > Any: ...   def clone(self) > ArrayImpl: ...   def _copy_single_device_array_to_host_async(self): ...   def _single_device_array_to_np_array(self) > np.ndarray: ...   def on_device_size_in_bytes(self) > int: ... ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Move PyBuffer methods used by PyArray to c++.,Move PyBuffer methods used by PyArray to c++. ```   def delete(self): ...   def unsafe_buffer_pointer(self) > Any: ...   def clone(self) > ArrayImpl: ...   def _copy_single_device_array_to_host_async(self): ...   def _single_device_array_to_np_array(self) > np.ndarray: ...   def on_device_size_in_bytes(self) > int: ... ```,2023-03-10T05:08:59Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14895
5856,"以下是一个github上的jax下的一个issue, 标题是([Colab] jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure)， 内容是 ( Description Running a Colab notebook that was working on Colab GPU last month, getting `INTERNAL: RET_CHECK failure` all of sudden. Tried: https://github.com/google/jax/issues/13504, which is `!export XLA_PYTHON_CLIENT_MEM_FRACTION=0.7` Did not work. GPU: Tesla T4 on Colab Not sure if this is a Jax issue, any help? ``` Cores: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] 20230310 00:53:00.478741: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 20230310 00:53:00.479021: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 525.85.12 20230310 00:53:00.480495: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:627) dnn != nullptr  *** Begin stack trace *** 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyObject_FastCallDictTstate 	_PyObject_Call_Prepend 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	PyEval_EvalCode 	PyRun_SimpleFileExFlags 	Py_RunMain 	Py_BytesMain 	__libc_start_main 	_start *** End stack trace *** ...   File ""/usr/local/lib/python3.9/distpackages/jax/_src/numpy/lax_numpy.py"", line 2088, in copy     return array(a, copy=True, order=order)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/numpy/lax_numpy.py"", line 2047, in array     out = _array_copy(object) if copy else object   File ""/usr/local/lib/python3.9/distpackages/jax/_src/lax/lax.py"", line 4359, in _array_copy     return copy_p.bind(arr)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/core.py"", line 343, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/core.py"", line 346, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/core.py"", line 789, in process_primitive     return primitive.impl(*tracers, **params)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/lax/lax.py"", line 4400, in _copy_impl     return xla.apply_primitive(prim, *args, **kwargs)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 123, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/usr/local/lib/python3.9/distpackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 202, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), device, None,   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 355, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/usr/local/lib/python3.9/distpackages/jax/_src/interpreters/pxla.py"", line 3254, in compile     executable = self._compile_unloaded(   File ""/usr/local/lib/python3.9/distpackages/jax/_src/interpreters/pxla.py"", line 3225, in _compile_unloaded     return UnloadedMeshExecutable.from_hlo(   File ""/usr/local/lib/python3.9/distpackages/jax/_src/interpreters/pxla.py"", line 3512, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 1095, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/usr/local/lib/python3.9/distpackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 1040, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:627) dnn != nullptr  ```  What jax/jaxlib version are you using? jax 0.3.25  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16  NVIDIA GPU info GPU: Tesla T4 on Colab)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Colab] jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure," Description Running a Colab notebook that was working on Colab GPU last month, getting `INTERNAL: RET_CHECK failure` all of sudden. Tried: https://github.com/google/jax/issues/13504, which is `!export XLA_PYTHON_CLIENT_MEM_FRACTION=0.7` Did not work. GPU: Tesla T4 on Colab Not sure if this is a Jax issue, any help? ``` Cores: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] 20230310 00:53:00.478741: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED 20230310 00:53:00.479021: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 525.85.12 20230310 00:53:00.480495: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:627) dnn != nullptr  *** Begin stack trace *** 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	PyObject_Call 	_PyEval_EvalFrameDefault 	_PyFunction_Vectorcall 	_PyObject_FastCallDictTstate 	_PyObject_Call_Prepend 	_PyObject_MakeTpCall 	_PyEval_EvalFrameDefault 	_PyEval_EvalCodeWithName 	PyEval_EvalCode 	PyRun_SimpleFileExFlags 	Py_RunMain 	Py_BytesMain 	__libc_start_main 	_start *** End stack trace *** ...   File ""/usr/local/lib/python3.9/distpackages/jax/_src/numpy/lax_numpy.py"", line 2088, in copy     return array(a, copy=True, order=order)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/numpy/lax_numpy.py"", line 2047, in array     out = _array_copy(object) if copy else object   File ""/usr/local/lib/python3.9/distpackages/jax/_src/lax/lax.py"", line 4359, in _array_copy     return copy_p.bind(arr)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/core.py"", line 343, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/core.py"", line 346, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/core.py"", line 789, in process_primitive     return primitive.impl(*tracers, **params)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/lax/lax.py"", line 4400, in _copy_impl     return xla.apply_primitive(prim, *args, **kwargs)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 123, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/usr/local/lib/python3.9/distpackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 202, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), device, None,   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 355, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/usr/local/lib/python3.9/distpackages/jax/_src/interpreters/pxla.py"", line 3254, in compile     executable = self._compile_unloaded(   File ""/usr/local/lib/python3.9/distpackages/jax/_src/interpreters/pxla.py"", line 3225, in _compile_unloaded     return UnloadedMeshExecutable.from_hlo(   File ""/usr/local/lib/python3.9/distpackages/jax/_src/interpreters/pxla.py"", line 3512, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 1095, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/usr/local/lib/python3.9/distpackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/usr/local/lib/python3.9/distpackages/jax/_src/dispatch.py"", line 1040, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:627) dnn != nullptr  ```  What jax/jaxlib version are you using? jax 0.3.25  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16  NVIDIA GPU info GPU: Tesla T4 on Colab",2023-03-10T00:56:53Z,bug NVIDIA GPU,open,0,5,https://github.com/jax-ml/jax/issues/14893,"This typically means you've installed the `jaxlib` GPU wheel for the wrong CUDA version. Colab comes with a compatible JAX version preinstalled, so if you reset your runtime JAX should work outofthebox. That said, if you want to update you should be sure to use the `cuda11_cudnn82` version of jaxlib (see https://github.com/google/jaxpipinstallationgpucuda for installation instructions).","I am also having the exact same issue with any jax version newer than 0.3.13, using a 187Gb GPU, and having tried all CUDA compatible subversions of JAX and JAXLIB with my CUDA version. Would help if there is any solution to this issue. (note I have this error whenever I try to create a jax object, jax numpy array even of dimension 1x1)"," do you still experience this issue with the latest jax on Colab ?  If yes, can you please provide some repro code or link to your failing colab ? Thanks!",how solve this problem finally？,"Hey, any solution to this?"
1957,"以下是一个github上的jax下的一个issue, 标题是(xmap with SerialLoop and FLAX (same for equinox))， 内容是 ( Description When trying to xmap an axis and to use a SerialLoop (in order to reduce memory usage) on a function that evaluates a neural network the following error is raised:  ``` NameError: unbound axis name: t. The following axis names (e.g. defined by pmap) are available to collective operations: [] ``` This does not happen if Mesh is used, or no resource mapping is used at all. Same happens for Equinox, but not for very simple functions like jnp.sin...  How to reproduce with FLAX: ```python  this is just the hello world from FLAX from typing import Sequence import jax import jax.numpy as jnp import flax.linen as nn from jax.experimental.maps import xmap, SerialLoop, serial_loop class MLP(nn.Module):   features: Sequence[int]   .compact   def __call__(self, x):     for feat in self.features[:1]:       x = nn.relu(nn.Dense(feat)(x))     x = nn.Dense(self.features[1])(x)     return x model = MLP([6, 3, 2]) batch = jnp.ones((2, 4)) variables = model.init(jax.random.PRNGKey(0), batch)  output = model.apply(variables, batch) x_apply = xmap(     model.apply,     in_axes=(         [...],         ['x_batch', ...],     ),     out_axes=['x_batch',...],     axis_resources={'x_batch': 't'},      axis_resources={'x_batch': SerialLoop(2)}, ) b_batch = jnp.ones((2, 4, 4)) with serial_loop('t', 2):   x_apply(variables, b_batch)  Fails with:  NameError: unbound axis name: t. The following axis names (e.g. defined by pmap) are available to collective operations: []  Does not fail if serial_loop is replaced with Mesh ```  What jax/jaxlib version are you using? jax v0.4.5, jaxlib 0.4.4+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.9, Linux  NVIDIA GPU info ``` ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,xmap with SerialLoop and FLAX (same for equinox)," Description When trying to xmap an axis and to use a SerialLoop (in order to reduce memory usage) on a function that evaluates a neural network the following error is raised:  ``` NameError: unbound axis name: t. The following axis names (e.g. defined by pmap) are available to collective operations: [] ``` This does not happen if Mesh is used, or no resource mapping is used at all. Same happens for Equinox, but not for very simple functions like jnp.sin...  How to reproduce with FLAX: ```python  this is just the hello world from FLAX from typing import Sequence import jax import jax.numpy as jnp import flax.linen as nn from jax.experimental.maps import xmap, SerialLoop, serial_loop class MLP(nn.Module):   features: Sequence[int]   .compact   def __call__(self, x):     for feat in self.features[:1]:       x = nn.relu(nn.Dense(feat)(x))     x = nn.Dense(self.features[1])(x)     return x model = MLP([6, 3, 2]) batch = jnp.ones((2, 4)) variables = model.init(jax.random.PRNGKey(0), batch)  output = model.apply(variables, batch) x_apply = xmap(     model.apply,     in_axes=(         [...],         ['x_batch', ...],     ),     out_axes=['x_batch',...],     axis_resources={'x_batch': 't'},      axis_resources={'x_batch': SerialLoop(2)}, ) b_batch = jnp.ones((2, 4, 4)) with serial_loop('t', 2):   x_apply(variables, b_batch)  Fails with:  NameError: unbound axis name: t. The following axis names (e.g. defined by pmap) are available to collective operations: []  Does not fail if serial_loop is replaced with Mesh ```  What jax/jaxlib version are you using? jax v0.4.5, jaxlib 0.4.4+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.9, Linux  NVIDIA GPU info ``` ++  ++ ```",2023-03-09T17:00:23Z,bug NVIDIA GPU,open,0,1,https://github.com/jax-ml/jax/issues/14877,"Hi   It appears that this issue has been resolved in later versions of JAX. I tested the issue on colab with GPU Tesla T4 and on WSL2 with GPUs RTX A5000 and GeForce RTX 2060 and it work fine without any failures. ```python from typing import Sequence import jax import jax.numpy as jnp import flax.linen as nn from jax.experimental.maps import xmap, SerialLoop, serial_loop class MLP(nn.Module):   features: Sequence[int]   .compact   def __call__(self, x):     for feat in self.features[:1]:       x = nn.relu(nn.Dense(feat)(x))     x = nn.Dense(self.features[1])(x)     return x model = MLP([6, 3, 2]) batch = jnp.ones((2, 4)) variables = model.init(jax.random.PRNGKey(0), batch) x_apply = xmap(     model.apply,     in_axes=(         [...],         ['x_batch', ...],     ),     out_axes=['x_batch',...],     axis_resources={'x_batch': 't'}, ) b_batch = jnp.ones((2, 4, 4)) with serial_loop('t', 2):   output = x_apply(variables, b_batch) output ``` Output: ``` Array([[[1.7634937, 1.2312332],         [1.7634937, 1.2312332],         [1.7634937, 1.2312332],         [1.7634937, 1.2312332]],        [[1.7634937, 1.2312332],         [1.7634937, 1.2312332],         [1.7634937, 1.2312332],         [1.7634937, 1.2312332]]], dtype=float32) ``` Attaching the colab gist for reference. Also please find the below screenshot on WSL2 with GPU GeForce RTX 2060. !image !image Note: `xmap` is deprecated in JAX 0.4.26 and will be removed in future release. It recommended to use `jax.experimental.shard_map` or `jax.vmap` with `spmd_axes_name` argument to express SPMD deviceparallel computations. Thank you."
1475,"以下是一个github上的jax下的一个issue, 标题是(Support linear extrapolation in jnp.interp?)， 内容是 (`numpy.interp` does not support extrapolation, instead leaving this functionality to the much more complex scipy.interpolate.interp1d, which uses a class based interface. So I propose to interpret the string values `left='extrapolate'` and `right='extrapolate'` as indicating that extrapolation should be done instead of only using a constant value:  This value would be consistent with the keyword argument used by the `fill_value` argument to scipy.interpolate.interp1d.  There is no potential confusion about strings indicating interpolation modes versus values because `left` and `right` currently only support numeric arguments. This would be useful and extremely easy to implement in JAX. Basically all that needs to be done to obtain linear extrapolation is to conditionally remove these calls to `where` at the end of `jnp.interp` https://github.com/google/jax/blob/6634600c46c99e3f0861e9258aace3fdfe250baf/jax/_src/numpy/lax_numpy.pyL1042L1043 Any thoughts? I can see a case for asking to do this in NumPy first, but I'm not sure there would be large enough benefits to the NumPy users given that they can use SciPy instead. Also it would be slightly more annoying to implement in NumPy since the implementation of `np.interp` is written in C.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Support linear extrapolation in jnp.interp?,"`numpy.interp` does not support extrapolation, instead leaving this functionality to the much more complex scipy.interpolate.interp1d, which uses a class based interface. So I propose to interpret the string values `left='extrapolate'` and `right='extrapolate'` as indicating that extrapolation should be done instead of only using a constant value:  This value would be consistent with the keyword argument used by the `fill_value` argument to scipy.interpolate.interp1d.  There is no potential confusion about strings indicating interpolation modes versus values because `left` and `right` currently only support numeric arguments. This would be useful and extremely easy to implement in JAX. Basically all that needs to be done to obtain linear extrapolation is to conditionally remove these calls to `where` at the end of `jnp.interp` https://github.com/google/jax/blob/6634600c46c99e3f0861e9258aace3fdfe250baf/jax/_src/numpy/lax_numpy.pyL1042L1043 Any thoughts? I can see a case for asking to do this in NumPy first, but I'm not sure there would be large enough benefits to the NumPy users given that they can use SciPy instead. Also it would be slightly more annoying to implement in NumPy since the implementation of `np.interp` is written in C.",2023-03-08T20:26:42Z,enhancement,closed,4,1,https://github.com/jax-ml/jax/issues/14858,Deviating from the NumPy API in ways  suggests SGTM!
736,"以下是一个github上的jax下的一个issue, 标题是(bad error on VJP of functions returning typed key arrays)， 内容是 (Found by  in CC(expose `random_wrap` and `random_unwrap` (plus discussion of early typed key surprises))! ```python from jax import enable_custom_prng, vjp from jax.random import PRNGKey with enable_custom_prng():     def f(i):         return PRNGKey(i)     out, f_vjp = vjp(f, 1)   Fails! ``` Error is: ``` TypeError: Value 'PRNGKeyArray[fry] { [0 1] }' with dtype key is not a valid JAX array type. Only arrays of numeric types are supported by JAX. ``` (RNGs: key types and custom implementations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,bad error on VJP of functions returning typed key arrays,"Found by  in CC(expose `random_wrap` and `random_unwrap` (plus discussion of early typed key surprises))! ```python from jax import enable_custom_prng, vjp from jax.random import PRNGKey with enable_custom_prng():     def f(i):         return PRNGKey(i)     out, f_vjp = vjp(f, 1)   Fails! ``` Error is: ``` TypeError: Value 'PRNGKeyArray[fry] { [0 1] }' with dtype key is not a valid JAX array type. Only arrays of numeric types are supported by JAX. ``` (RNGs: key types and custom implementations)",2023-03-08T17:41:21Z,bug,closed,1,0,https://github.com/jax-ml/jax/issues/14856
311,"以下是一个github上的jax下的一个issue, 标题是(Stop using version 1 of XlaCallModuleOp)， 内容是 (Stop using version 1 of XlaCallModuleOp Also remove configuration flag jax2tf_use_stablehlo.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Stop using version 1 of XlaCallModuleOp,Stop using version 1 of XlaCallModuleOp Also remove configuration flag jax2tf_use_stablehlo.,2023-03-08T08:13:03Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14847
1466,"以下是一个github上的jax下的一个issue, 标题是([shape_poly, call_tf] Some improvements for call_tf in a shape polymorphic program)， 内容是 ([shape_poly, call_tf] Some improvements for call_tf in a shape polymorphic program This is another attempt to land a rolledback change https://github.com/google/jax/pull/14734 (cl/514070997). See b/272154366 for more details. The use case for call_tf with shape polymorphism is when we have a JAX program that calls into TF function, and we want to serialize the JAX program with some shapes unknown. Previously this use case did not work, except in the special case when the output shape of the called TF function returns statically known shapes. The idea is that we allow the user of call_tf to specify the output shape. This can be done even in presence of shape polymorphism, by writing the output shape as an expression in terms of the input shapes. This is what other JAX primitives do, e.g., concat, so we are simply enabling call_tf to get the same behavior. This change should be enough for oldstyle jax2tf, but will require more work for native serialization. We also removed some old code that was trying to workaround some limitations in shape inference in TF. I think that those workarounds are ugly, and I am prepared to give error messages rather than keep that code. So far no tests fail.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[shape_poly, call_tf] Some improvements for call_tf in a shape polymorphic program","[shape_poly, call_tf] Some improvements for call_tf in a shape polymorphic program This is another attempt to land a rolledback change https://github.com/google/jax/pull/14734 (cl/514070997). See b/272154366 for more details. The use case for call_tf with shape polymorphism is when we have a JAX program that calls into TF function, and we want to serialize the JAX program with some shapes unknown. Previously this use case did not work, except in the special case when the output shape of the called TF function returns statically known shapes. The idea is that we allow the user of call_tf to specify the output shape. This can be done even in presence of shape polymorphism, by writing the output shape as an expression in terms of the input shapes. This is what other JAX primitives do, e.g., concat, so we are simply enabling call_tf to get the same behavior. This change should be enough for oldstyle jax2tf, but will require more work for native serialization. We also removed some old code that was trying to workaround some limitations in shape inference in TF. I think that those workarounds are ugly, and I am prepared to give error messages rather than keep that code. So far no tests fail.",2023-03-08T07:48:15Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14846
299,"以下是一个github上的jax下的一个issue, 标题是(Set ArrayImpl.__name__ to ArrayImpl)， 内容是 (Set ArrayImpl.__name__ to ArrayImpl Fixes https://github.com/google/jax/issues/14768)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Set ArrayImpl.__name__ to ArrayImpl,Set ArrayImpl.__name__ to ArrayImpl Fixes https://github.com/google/jax/issues/14768,2023-03-07T21:10:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14831
2043,"以下是一个github上的jax下的一个issue, 标题是(Cannot build from source on Ubuntu 22.04.1 LTS (Google Cloud))， 内容是 ( Description I am trying to build JAX from source, and I'm getting stuck on this `//:python_include: Genrules without outputs don't make sense` error. ``` ERROR: /home/silvasean/.cache/bazel/_bazel_silvasean/2be0f603d22f00664ced77380d946e12/external/local_config_python/BUILD:78:8: in outs attribute of genrule rule //:python_include: Genrules without outputs don't make sense ERROR: /home/silvasean/.cache/bazel/_bazel_silvasean/2be0f603d22f00664ced77380d946e12/external/local_config_python/BUILD:78:8: Analysis of target '//:python_include' failed ``` It is talking about this genrule, which seems fishy (locally for me it is in /home/silvasean/.cache/bazel/_bazel_silvasean/2be0f603d22f00664ced77380d946e12/external/local_config_python/BUILD) ``` genrule(     name = ""python_include"",     outs = [     ],     cmd = """"""    """""", ) ``` My bazelfu is not strong enough to really debug this. It seems to be possibly coming from tensorflow/third_party/py/python_configure.bzl but I can't figure out reproduce the build step that generates the bogus genrule to debug the issue. Any help would be appreciated. Reproduction steps (following https://jax.readthedocs.io/en/latest/developer.html exactly as far as I can tell): https://gist.github.com/silvasean/5ea1c43be9e2dc1d2317d985eb696a76  What jax/jaxlib version are you using? 88d5a4110b9f902dc57ac35878c04c1eb3bf54c1  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22.04.1 LTS  NVIDIA GPU info ``` (jax_repro.venv) silvaseana100cuda117:~/jax_repro/jax$ python version Python 3.10.6 (jax_repro.venv) silvaseana100cuda117:~/jax_repro$ cat /etc/lsbrelease  DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION=""Ubuntu 22.04.1 LTS"" (jax_repro.venv) silvaseana100cuda117:~/jax_repro$ nvidiasmi ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot build from source on Ubuntu 22.04.1 LTS (Google Cloud)," Description I am trying to build JAX from source, and I'm getting stuck on this `//:python_include: Genrules without outputs don't make sense` error. ``` ERROR: /home/silvasean/.cache/bazel/_bazel_silvasean/2be0f603d22f00664ced77380d946e12/external/local_config_python/BUILD:78:8: in outs attribute of genrule rule //:python_include: Genrules without outputs don't make sense ERROR: /home/silvasean/.cache/bazel/_bazel_silvasean/2be0f603d22f00664ced77380d946e12/external/local_config_python/BUILD:78:8: Analysis of target '//:python_include' failed ``` It is talking about this genrule, which seems fishy (locally for me it is in /home/silvasean/.cache/bazel/_bazel_silvasean/2be0f603d22f00664ced77380d946e12/external/local_config_python/BUILD) ``` genrule(     name = ""python_include"",     outs = [     ],     cmd = """"""    """""", ) ``` My bazelfu is not strong enough to really debug this. It seems to be possibly coming from tensorflow/third_party/py/python_configure.bzl but I can't figure out reproduce the build step that generates the bogus genrule to debug the issue. Any help would be appreciated. Reproduction steps (following https://jax.readthedocs.io/en/latest/developer.html exactly as far as I can tell): https://gist.github.com/silvasean/5ea1c43be9e2dc1d2317d985eb696a76  What jax/jaxlib version are you using? 88d5a4110b9f902dc57ac35878c04c1eb3bf54c1  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22.04.1 LTS  NVIDIA GPU info ``` (jax_repro.venv) silvaseana100cuda117:~/jax_repro/jax$ python version Python 3.10.6 (jax_repro.venv) silvaseana100cuda117:~/jax_repro$ cat /etc/lsbrelease  DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION=""Ubuntu 22.04.1 LTS"" (jax_repro.venv) silvaseana100cuda117:~/jax_repro$ nvidiasmi ++  ++ ```",2023-03-07T15:19:48Z,bug needs info,closed,0,5,https://github.com/jax-ml/jax/issues/14822,Did you also try https://github.com/openxla/openxlapjrtpluginbuildingjaxfromsource ?,"> Did you also try https://github.com/openxla/openxlapjrtpluginbuildingjaxfromsource ? Yes, that's actually where I started. I then found that the vanilla jax build doesn't work.",I think it's coming from here: https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl I can help debug further later today if you need it,"I can't reproduce this. I took a fresh Ubuntu 22.04.2 VM, ran: ``` sudo apt install python3pip pip3 install numpy git clone https://github.com/google/jax.git cd jax python3 build/build.py ``` and everything built fine. You'll have to share more detailed reproduction instructions.","Thanks , that narrowed this down. The issue is related to having `python3dev` installed, and my system got into a ""bad state"" where bazel had the ""not python3dev installed"" state cached. It appears that cache invalidation retains its position in the pantheon of most difficult problems in computer science :rofl:  To explain how I got here: 1. I followed the steps in openxlapjrtplugin for building JAX from source: link 2. The steps there don't include installing python3dev 3. I did the bazel build, and it cached the ""not having python3dev installed"" state, which created the bogus genrule 4. Later when I installed python3dev (per the JAX docs), it was not seen and the issue persisted. The solution was ensuring python3dev was installed and removing `~/.cache/bazel`. Also, an interesting side note: In your instructions you install `apt install python3pip` which installs `python3dev` incidentally. In my flow I got pip via `apt install python3venv` which installs the `python3pipwhl` Ubuntu package as a byproduct. But `python3pipwhl` does not install `python3dev`, unlike `python3pip`. Phew"
4019,"以下是一个github上的jax下的一个issue, 标题是(Excessive memory allocation with `jax.vmap`)， 内容是 ( Description The below reproducing example defines a function `refine` that if `vmap`ed allocates memory excessively and eventually leads to an OOM even though the naivenonbatched pass is well behaved and consumes little memory. ```python  %% import os  Both allocation strategies yield the below described OOM  os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""] = "".98"" os.environ[""XLA_PYTHON_CLIENT_ALLOCATOR""] = ""platform""  %% Let's first define the problematic function import numpy as np import jax from jax import numpy as jnp from jax.lax import dynamic_slice_in_dim def refine(     coarse_values,     excitations,     olf,     fks,     index_map,     *,     chart,     precision=None, ):     def refine(coarse_values, exc, idx_hp, idx_r, olf, fks, im):         c = dynamic_slice_in_dim(             coarse_values[chart[idx_hp]],             idx_r,             slice_size=3,             axis=1         )         f_shp = (2**2, 2)         o = olf[im] if im is not None else olf         refined = jnp.tensordot(o, c, axes=2, precision=precision)         f = fks[im] if im is not None else fks         refined += jnp.matmul(f, exc, precision=precision).reshape(f_shp)         return refined     pix_hp_idx = jnp.arange(coarse_values.shape[0])     pix_r_off = jnp.arange(coarse_values.shape[1]  3 + 1)      TODO: benchmark swapping these two     off = index_map is not None     vrefine = jax.vmap(refine, in_axes=(None, 0, None, 0, 0 + off, 0 + off, None))     in_axes = (None, 0, 0, None)     in_axes += (0, 0, None) if index_map is None else (None, None, 0)     vrefine = jax.vmap(vrefine, in_axes=in_axes)     refined = vrefine(         coarse_values, excitations, pix_hp_idx, pix_r_off, olf, fks, index_map     )     refined = jnp.transpose(refined, (0, 2, 1, 3))     n_hp = refined.shape[0] * refined.shape[1]     n_r = refined.shape[2] * refined.shape[3]     return refined.reshape(n_hp, n_r) n_pix = 196608 chart = jax.device_put(np.random.choice(np.arange(n_pix), size=(n_pix, 9))) coarse_values = jax.device_put(np.random.normal(size=(n_pix, 260))) excitations = jax.device_put(np.random.normal(size=(n_pix, 258, 8))) olf = jax.device_put(np.random.normal(size=(28, 258, 4, 2, 9, 3))) fks = jax.device_put(np.random.normal(size=(28, 258, 8, 8))) index_map = jax.device_put(     np.random.choice(np.arange(olf.shape[0]), size=coarse_values.shape[:1]) )  %% refine_jit = jax.jit(lambda x, y: refine(x, y, olf, fks, index_map, chart=chart)) %timeit _ = jax.block_until_ready(refine_jit(coarse_values, excitations))  5 GB of HBM2 (70 ms)  %% refine_p_jit = jax.pmap(     lambda x, y: refine(x, y, olf, fks, index_map, chart=chart), in_axes=(0, 0) ) %timeit _ = jax.block_until_ready(refine_p_jit(coarse_values_bc, excitations_bc))  78 GB of HBM2 (100 ms)  %% coarse_values_bc = jnp.tile(coarse_values, (2, ) + (1, ) * coarse_values.ndim) excitations_bc = jnp.tile(excitations, (2, ) + (1, ) * excitations.ndim) refine_bc_jit = jax.jit(     jax.vmap(         lambda x, y: refine(x, y, olf, fks, index_map, chart=chart),         in_axes=(0, 0)     ) ) %timeit _ = jax.block_until_ready(refine_bc_jit(coarse_values_bc, excitations_bc))  58 GB of HBM2 (700 ms)  %% coarse_values_bc = jnp.tile(coarse_values, (4, ) + (1, ) * coarse_values.ndim) excitations_bc = jnp.tile(excitations, (4, ) + (1, ) * excitations.ndim) refine_bc_jit = jax.jit(     jax.vmap(         lambda x, y: refine(x, y, olf, fks, index_map, chart=chart),         in_axes=(0, 0)     ) ) %timeit _ = jax.block_until_ready(refine_bc_jit(coarse_values_bc, excitations_bc))  > 70 GB of memory > OOM ```  What jax/jaxlib version are you using? jaxlib==0.4.4, jax==0.4.4  Which accelerator(s) are you using? GPU  Additional system info python 3.9.12, Linux  NVIDIA GPU info ``` ++                                                ++  ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Excessive memory allocation with `jax.vmap`," Description The below reproducing example defines a function `refine` that if `vmap`ed allocates memory excessively and eventually leads to an OOM even though the naivenonbatched pass is well behaved and consumes little memory. ```python  %% import os  Both allocation strategies yield the below described OOM  os.environ[""XLA_PYTHON_CLIENT_MEM_FRACTION""] = "".98"" os.environ[""XLA_PYTHON_CLIENT_ALLOCATOR""] = ""platform""  %% Let's first define the problematic function import numpy as np import jax from jax import numpy as jnp from jax.lax import dynamic_slice_in_dim def refine(     coarse_values,     excitations,     olf,     fks,     index_map,     *,     chart,     precision=None, ):     def refine(coarse_values, exc, idx_hp, idx_r, olf, fks, im):         c = dynamic_slice_in_dim(             coarse_values[chart[idx_hp]],             idx_r,             slice_size=3,             axis=1         )         f_shp = (2**2, 2)         o = olf[im] if im is not None else olf         refined = jnp.tensordot(o, c, axes=2, precision=precision)         f = fks[im] if im is not None else fks         refined += jnp.matmul(f, exc, precision=precision).reshape(f_shp)         return refined     pix_hp_idx = jnp.arange(coarse_values.shape[0])     pix_r_off = jnp.arange(coarse_values.shape[1]  3 + 1)      TODO: benchmark swapping these two     off = index_map is not None     vrefine = jax.vmap(refine, in_axes=(None, 0, None, 0, 0 + off, 0 + off, None))     in_axes = (None, 0, 0, None)     in_axes += (0, 0, None) if index_map is None else (None, None, 0)     vrefine = jax.vmap(vrefine, in_axes=in_axes)     refined = vrefine(         coarse_values, excitations, pix_hp_idx, pix_r_off, olf, fks, index_map     )     refined = jnp.transpose(refined, (0, 2, 1, 3))     n_hp = refined.shape[0] * refined.shape[1]     n_r = refined.shape[2] * refined.shape[3]     return refined.reshape(n_hp, n_r) n_pix = 196608 chart = jax.device_put(np.random.choice(np.arange(n_pix), size=(n_pix, 9))) coarse_values = jax.device_put(np.random.normal(size=(n_pix, 260))) excitations = jax.device_put(np.random.normal(size=(n_pix, 258, 8))) olf = jax.device_put(np.random.normal(size=(28, 258, 4, 2, 9, 3))) fks = jax.device_put(np.random.normal(size=(28, 258, 8, 8))) index_map = jax.device_put(     np.random.choice(np.arange(olf.shape[0]), size=coarse_values.shape[:1]) )  %% refine_jit = jax.jit(lambda x, y: refine(x, y, olf, fks, index_map, chart=chart)) %timeit _ = jax.block_until_ready(refine_jit(coarse_values, excitations))  5 GB of HBM2 (70 ms)  %% refine_p_jit = jax.pmap(     lambda x, y: refine(x, y, olf, fks, index_map, chart=chart), in_axes=(0, 0) ) %timeit _ = jax.block_until_ready(refine_p_jit(coarse_values_bc, excitations_bc))  78 GB of HBM2 (100 ms)  %% coarse_values_bc = jnp.tile(coarse_values, (2, ) + (1, ) * coarse_values.ndim) excitations_bc = jnp.tile(excitations, (2, ) + (1, ) * excitations.ndim) refine_bc_jit = jax.jit(     jax.vmap(         lambda x, y: refine(x, y, olf, fks, index_map, chart=chart),         in_axes=(0, 0)     ) ) %timeit _ = jax.block_until_ready(refine_bc_jit(coarse_values_bc, excitations_bc))  58 GB of HBM2 (700 ms)  %% coarse_values_bc = jnp.tile(coarse_values, (4, ) + (1, ) * coarse_values.ndim) excitations_bc = jnp.tile(excitations, (4, ) + (1, ) * excitations.ndim) refine_bc_jit = jax.jit(     jax.vmap(         lambda x, y: refine(x, y, olf, fks, index_map, chart=chart),         in_axes=(0, 0)     ) ) %timeit _ = jax.block_until_ready(refine_bc_jit(coarse_values_bc, excitations_bc))  > 70 GB of memory > OOM ```  What jax/jaxlib version are you using? jaxlib==0.4.4, jax==0.4.4  Which accelerator(s) are you using? GPU  Additional system info python 3.9.12, Linux  NVIDIA GPU info ``` ++                                                ++  ```",2023-03-07T00:22:47Z,bug NVIDIA GPU,open,0,0,https://github.com/jax-ml/jax/issues/14808
26649,"以下是一个github上的jax下的一个issue, 标题是(Jaxlib build fails. Tensorflow link 404 error)， 内容是 ( Description Building jaxlib (cuda support) fails. Looks like the storage.googleapis.com/mirror.tensorflow.org links are invalid (404). ``` Building XLA and installing it in the jaxlib source tree... ./bazel5.1.1linuxx86_64 run verbose_failures=true :build_wheel  output_path=/home/mehdi/jax/dist cpu=x86_64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /home/mehdi/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /home/mehdi/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //tensorflow/compiler/xla/python:enable_gpu=false //tensorflow/compiler/xla/python:enable_tpu=false //tensorflow/compiler/xla/python:enable_plugin_device=false INFO: Reading rc options for 'run' from /home/mehdi/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/usr/bin/python3 action_env=PYENV_ROOT python_path=/usr/bin/python3 distinct_host_configuration=false config=avx_posix config=mkl_open_source_only config=cuda INFO: Found applicable config definition build:short_logs in file /home/mehdi/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /home/mehdi/jax/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /home/mehdi/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /home/mehdi/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda //tensorflow/compiler/xla/python:enable_gpu=true define=xla_python_enable_gpu=true INFO: Found applicable config definition build:linux in file /home/mehdi/jax/.bazelrc: config=posix copt=Wnounknownwarningoption copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /home/mehdi/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/37b7a60cd74b7a1754583b7eb63a6339158fd398.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/fdc469f88ebfc15109710da9e9fb2d6898dbdcd6.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found DEBUG: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:  AutoConfiguration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default Loading:  Loading: 0 packages loaded Analyzing: target //build:build_wheel (0 packages loaded, 0 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/66e871bc1aeb0c95bc64696ab3098127a31a7dfe.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/nvidia/nccl/archive/v2.16.51.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/triton/archive/c3f7b6e297eac767bf07295410cf959b01fe954f.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVIDIA/cudnnfrontend/archive/refs/tags/v0.7.3.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found INFO: Analyzed target //build:build_wheel (0 packages loaded, 0 targets configured). INFO: Found 1 target... [0 / 7] [Prepa] BazelWorkspaceStatusAction stablestatus.txt ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/MicrosoftDemangleNodes.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangleNodes.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangleNodes.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/MicrosoftDemangleNodes.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangleNodes.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/ItaniumDemangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/ItaniumDemangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/ItaniumDemangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/ItaniumDemangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/ItaniumDemangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/RustDemangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/RustDemangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/RustDemangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/RustDemangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/RustDemangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/Demangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/Demangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/Demangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/Demangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/Demangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/MicrosoftDemangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/MicrosoftDemangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/DLangDemangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/DLangDemangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/DLangDemangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/DLangDemangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/DLangDemangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory Target //build:build_wheel failed to build ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Jaxlib build fails. Tensorflow link 404 error," Description Building jaxlib (cuda support) fails. Looks like the storage.googleapis.com/mirror.tensorflow.org links are invalid (404). ``` Building XLA and installing it in the jaxlib source tree... ./bazel5.1.1linuxx86_64 run verbose_failures=true :build_wheel  output_path=/home/mehdi/jax/dist cpu=x86_64 INFO: Options provided by the client:   Inherited 'common' options: isatty=0 terminal_columns=80 INFO: Reading rc options for 'run' from /home/mehdi/jax/.bazelrc:   Inherited 'common' options: experimental_repo_remote_exec INFO: Reading rc options for 'run' from /home/mehdi/jax/.bazelrc:   Inherited 'build' options: apple_platform_type=macos macos_minimum_os=10.14 announce_rc define open_source_build=true spawn_strategy=standalone enable_platform_specific_config experimental_cc_shared_library define=no_aws_support=true define=no_gcp_support=true define=no_hdfs_support=true define=no_kafka_support=true define=no_ignite_support=true define=grpc_no_ares=true define=tsl_link_protobuf=true c opt config=short_logs copt=DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir. //tensorflow/compiler/xla/python:enable_gpu=false //tensorflow/compiler/xla/python:enable_tpu=false //tensorflow/compiler/xla/python:enable_plugin_device=false INFO: Reading rc options for 'run' from /home/mehdi/jax/.jax_configure.bazelrc:   Inherited 'build' options: strategy=Genrule=standalone repo_env PYTHON_BIN_PATH=/usr/bin/python3 action_env=PYENV_ROOT python_path=/usr/bin/python3 distinct_host_configuration=false config=avx_posix config=mkl_open_source_only config=cuda INFO: Found applicable config definition build:short_logs in file /home/mehdi/jax/.bazelrc: output_filter=DONT_MATCH_ANYTHING INFO: Found applicable config definition build:avx_posix in file /home/mehdi/jax/.bazelrc: copt=mavx host_copt=mavx INFO: Found applicable config definition build:mkl_open_source_only in file /home/mehdi/jax/.bazelrc: define=tensorflow_mkldnn_contraction_kernel=1 INFO: Found applicable config definition build:cuda in file /home/mehdi/jax/.bazelrc: repo_env TF_NEED_CUDA=1 action_env TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 crosstool_top=//crosstool:toolchain //:enable_cuda //tensorflow/compiler/xla/python:enable_gpu=true define=xla_python_enable_gpu=true INFO: Found applicable config definition build:linux in file /home/mehdi/jax/.bazelrc: config=posix copt=Wnounknownwarningoption copt=Wnostringoptruncation copt=Wnoarrayparameter INFO: Found applicable config definition build:posix in file /home/mehdi/jax/.bazelrc: copt=fvisibility=hidden copt=Wnosigncompare cxxopt=std=c++17 host_cxxopt=std=c++17 WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvmproject/archive/37b7a60cd74b7a1754583b7eb63a6339158fd398.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/fdc469f88ebfc15109710da9e9fb2d6898dbdcd6.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found DEBUG: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:  AutoConfiguration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default Loading:  Loading: 0 packages loaded Analyzing: target //build:build_wheel (0 packages loaded, 0 targets configured) WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/66e871bc1aeb0c95bc64696ab3098127a31a7dfe.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pybind/pybind11_abseil/archive/2c4932ed6f6204f1656e245838f4f5eae69d2e29.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/boringssl/archive/c00d7ca810e93780bd0c8ee4eea28f4f2ea4bcdc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/nvidia/nccl/archive/v2.16.51.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/triton/archive/c3f7b6e297eac767bf07295410cf959b01fe954f.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVIDIA/cudnnfrontend/archive/refs/tags/v0.7.3.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found INFO: Analyzed target //build:build_wheel (0 packages loaded, 0 targets configured). INFO: Found 1 target... [0 / 7] [Prepa] BazelWorkspaceStatusAction stablestatus.txt ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/MicrosoftDemangleNodes.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangleNodes.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangleNodes.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/MicrosoftDemangleNodes.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangleNodes.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/ItaniumDemangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/ItaniumDemangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/ItaniumDemangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/ItaniumDemangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/ItaniumDemangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/RustDemangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/RustDemangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/RustDemangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/RustDemangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/RustDemangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/Demangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/Demangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/Demangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/Demangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/Demangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/MicrosoftDemangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/MicrosoftDemangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/MicrosoftDemangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory ERROR: /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/external/llvmproject/llvm/BUILD.bazel:179:11: Compiling llvm/lib/Demangle/DLangDemangle.cpp failed: (Exit 127): crosstool_wrapper_driver_is_not_gcc failed: error executing command    (cd /home/mehdi/.cache/bazel/_bazel_mehdi/c1f3d4f2f71bea3989b01c12adc3cf8c/execroot/__main__ && \   exec env  \     LD_LIBRARY_PATH=/usr/local/cuda12.1/lib64::/usr/local/cuda12.1/lib64::/usr/local/cuda/lib64::/usr/local/cuda/lib64::/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda11.8/lib64:/usr/local/cuda/lib64 \     PATH=/usr/local/cuda12.1/bin:/usr/local/cuda12.1/bin:/usr/local/cuda/bin:/usr/local/cuda/bin:/usr/local/cuda11.8/bin:/home/mehdi/.vscodeserver/bin/92da9481c0904c6adfe372c12da3b7748d74bdcb/bin/remotecli:/home/mehdi/.local/bin:/usr/local/cuda11.8/bin:/home/mehdi/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/mehdi/.fzf/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin:/usr/local/go/bin \     PWD=/proc/self/cwd \     TF_CUDA_COMPUTE_CAPABILITIES=sm_52,sm_60,sm_70,compute_80 \   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc MD MF bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/DLangDemangle.d 'frandomseed=bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/DLangDemangle.o' 'DLLVM_ON_UNIX=1' 'DHAVE_BACKTRACE=1' 'DBACKTRACE_HEADER=' 'DLTDL_SHLIB_EXT="".so""' 'DLLVM_PLUGIN_EXT="".so""' 'DLLVM_ENABLE_THREADS=1' 'DHAVE_DEREGISTER_FRAME=1' 'DHAVE_LIBPTHREAD=1' 'DHAVE_PTHREAD_GETNAME_NP=1' 'DHAVE_PTHREAD_H=1' 'DHAVE_PTHREAD_SETNAME_NP=1' 'DHAVE_REGISTER_FRAME=1' 'DHAVE_SETENV_R=1' 'DHAVE_STRERROR_R=1' 'DHAVE_SYSEXITS_H=1' 'DHAVE_UNISTD_H=1' D_GNU_SOURCE 'DHAVE_LINK_H=1' 'DHAVE_LSEEK64=1' 'DHAVE_MALLINFO=1' 'DHAVE_SBRK=1' 'DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' 'DLLVM_NATIVE_ARCH=""X86""' 'DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' 'DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' 'DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' 'DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' 'DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' 'DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' 'DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' 'DLLVM_HOST_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64unknownlinuxgnu""' 'DLLVM_VERSION_MAJOR=17' 'DLLVM_VERSION_MINOR=0' 'DLLVM_VERSION_PATCH=0' 'DLLVM_VERSION_STRING=""17.0.0git""' D__STDC_LIMIT_MACROS D__STDC_CONSTANT_MACROS D__STDC_FORMAT_MACROS iquote external/llvmproject iquote bazelout/k8opt/bin/external/llvmproject isystem external/llvmproject/llvm/include isystem bazelout/k8opt/bin/external/llvmproject/llvm/include Wnobuiltinmacroredefined 'D__DATE__=""redacted""' 'D__TIMESTAMP__=""redacted""' 'D__TIME__=""redacted""' fPIE U_FORTIFY_SOURCE 'D_FORTIFY_SOURCE=1' fstackprotector Wall fnoomitframepointer nocanonicalprefixes fnocanonicalsystemheaders DNDEBUG g0 O2 ffunctionsections fdatasections 'fvisibility=hidden' Wnosigncompare Wnounknownwarningoption Wnostringoptruncation Wnoarrayparameter 'DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.' mavx 'std=c++17' c external/llvmproject/llvm/lib/Demangle/DLangDemangle.cpp o bazelout/k8opt/bin/external/llvmproject/llvm/_objs/Demangle/DLangDemangle.o)  Configuration: a5821c5e90227ec2c54b6359d5cd21d746812b5fe7c79f9a27169673702cbca8  Execution platform: //:platform /usr/bin/env: 'python': No such file or directory Target //build:build_wheel failed to build ```  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22  NVIDIA GPU info _No response_",2023-03-06T15:34:06Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/14797,"The 404 errors are benign: they simply mean one of the possible mirrors doesn't exist. The real problem is you don't have `python` in your path, which is currently required. (Your `python` might be an alias to `python3`, for example.)"
1402,"以下是一个github上的jax下的一个issue, 标题是(Test failures on aarch64-darwin)， 内容是 ( Description I'm seeing the following test failures when testing on aarch64darwin: ``` =========================== short test summary info ============================ FAILED tests/lax_control_flow_test.py::LaxControlFlowTest::testScanGrad_jit_scan=False_jit_f=False_impl=unroll2  AssertionError:  FAILED tests/for_loop_test.py::ForLoopTransformationTest::test_for_loop_fixpoint_correctly_identifies_loop_varying_residuals_unrolled_for_loop  AssertionError:  FAILED tests/qdwh_test.py::QdwhTest::testQdwhWithRandomMatrix3  AssertionError:  ========== 3 failed, 11328 passed, 1205 skipped in 513.09s (0:08:33) =========== ``` In particular, I'm running on an Apple M1 Max 2021 chip. Complete test logs can be seen here. All the test failures appear to be numerical precision issues. It looks like tolerances just need to be made slightly more generous. I'm running with the following pytest flags: ```nix   pytestFlagsArray = [     ""numprocesses=4""     ""W ignore::DeprecationWarning""     ""tests/""   ]; ```  What jax/jaxlib version are you using? jax v0.4.5, jaxlib v0.4.4  Which accelerator(s) are you using? n/a  Additional system info Python python33.10.10 from Nixpkgs  NVIDIA GPU info n/a)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Test failures on aarch64-darwin," Description I'm seeing the following test failures when testing on aarch64darwin: ``` =========================== short test summary info ============================ FAILED tests/lax_control_flow_test.py::LaxControlFlowTest::testScanGrad_jit_scan=False_jit_f=False_impl=unroll2  AssertionError:  FAILED tests/for_loop_test.py::ForLoopTransformationTest::test_for_loop_fixpoint_correctly_identifies_loop_varying_residuals_unrolled_for_loop  AssertionError:  FAILED tests/qdwh_test.py::QdwhTest::testQdwhWithRandomMatrix3  AssertionError:  ========== 3 failed, 11328 passed, 1205 skipped in 513.09s (0:08:33) =========== ``` In particular, I'm running on an Apple M1 Max 2021 chip. Complete test logs can be seen here. All the test failures appear to be numerical precision issues. It looks like tolerances just need to be made slightly more generous. I'm running with the following pytest flags: ```nix   pytestFlagsArray = [     ""numprocesses=4""     ""W ignore::DeprecationWarning""     ""tests/""   ]; ```  What jax/jaxlib version are you using? jax v0.4.5, jaxlib v0.4.4  Which accelerator(s) are you using? n/a  Additional system info Python python33.10.10 from Nixpkgs  NVIDIA GPU info n/a",2023-03-06T00:37:16Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/14793
1860,"以下是一个github上的jax下的一个issue, 标题是(`jax.ensure_compile_time_eval` puts the trace stack in a bad state)， 内容是 (Found this one with , who was trying to test out whether we always maintain a correct trace stack state. It turns out that we can violate our invariants by at least one route: the current implementation of `jax.ensure_compile_time_eval`. This overrides the trace stack's dynamic and base traces with a fresh main level0 `EvalTrace`, without regard for what's already in the trace stack. We can elicit an incorrect error with a slight adaptation of `RematTest.test_remat_grad_python_control_flow_static_argnums`: ```python from functools import partial import jax def test():   (jax.remat, static_argnums=(0,))   def g(x):     with jax.ensure_compile_time_eval():       x_pos = float(x) > 0     if x_pos:       return jax.lax.sin(x), 3.     else:       return jax.lax.cos(x), 4.   def f(x):     x, _ = g(x)     return x   jax.grad(f)(2.) ``` ``` >>> test() [...] The stack trace below excludes JAXinternal frames. [...] Traceback (most recent call last): [...]   File ""..."", line 8, in g     x_pos = float(x) > 0 jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Tracedwith with   primal = 2.0   tangent = Tracedwith with     pval = (ShapedArray(float32[], weak_type=True), None)     recipe = LambdaBinding() The problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.ConcretizationTypeError ``` Maybe we should also `assert` some trace stack invariants in `core.new_main` and `core.new_base_main`?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.ensure_compile_time_eval` puts the trace stack in a bad state,"Found this one with , who was trying to test out whether we always maintain a correct trace stack state. It turns out that we can violate our invariants by at least one route: the current implementation of `jax.ensure_compile_time_eval`. This overrides the trace stack's dynamic and base traces with a fresh main level0 `EvalTrace`, without regard for what's already in the trace stack. We can elicit an incorrect error with a slight adaptation of `RematTest.test_remat_grad_python_control_flow_static_argnums`: ```python from functools import partial import jax def test():   (jax.remat, static_argnums=(0,))   def g(x):     with jax.ensure_compile_time_eval():       x_pos = float(x) > 0     if x_pos:       return jax.lax.sin(x), 3.     else:       return jax.lax.cos(x), 4.   def f(x):     x, _ = g(x)     return x   jax.grad(f)(2.) ``` ``` >>> test() [...] The stack trace below excludes JAXinternal frames. [...] Traceback (most recent call last): [...]   File ""..."", line 8, in g     x_pos = float(x) > 0 jax._src.errors.ConcretizationTypeError: Abstract tracer value encountered where concrete value is expected: Tracedwith with   primal = 2.0   tangent = Tracedwith with     pval = (ShapedArray(float32[], weak_type=True), None)     recipe = LambdaBinding() The problem arose with the `float` function. If trying to convert the data type of a value, try using `x.astype(float)` or `jnp.array(x, float)` instead. See https://jax.readthedocs.io/en/latest/errors.htmljax.errors.ConcretizationTypeError ``` Maybe we should also `assert` some trace stack invariants in `core.new_main` and `core.new_base_main`?",2023-03-04T03:50:46Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/14776,Thanks for putting this together!
298,"以下是一个github上的jax下的一个issue, 标题是(Make jaxlib type check with pytype.)， 内容是 (Make jaxlib type check with pytype. Add a missing type signature to xla_client.pyi.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Make jaxlib type check with pytype.,Make jaxlib type check with pytype. Add a missing type signature to xla_client.pyi.,2023-03-03T21:43:57Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14771
1373,"以下是一个github上的jax下的一个issue, 标题是([shape_poly, call_tf] Add support for call_tf in a shape polymorphic program)， 内容是 (The use case for call_tf with shape polymorphism is when we have a JAX program that calls into TF function, and we want to serialize the JAX program with some shapes unknown. Today this use case does not work, except in the special case when the output shape of the called TF function returns statically known shapes. The idea is that we allow the user of call_tf to specify the output shape. This can be done even in presence of shape polymorphism, by writing the output shape as an expression in terms of the input shapes. This is what other JAX primitives do, e.g., concat, so we are simply enabling call_tf to get the same behavior. This change should be enough for oldstyle jax2tf, but will require more work for native serialization. We also removed some old code that was trying to workaround some limitations in shape inference in TF. I think that those workarounds are ugly, and I am prepared to give error messages rather than keep that code. So far no tests fail. Still to do:  * ~~more testing~~  * ~~check that the TF function returns values of the declared shapes~~  * ~~documentation~~  * ~~CHANGELOG~~)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[shape_poly, call_tf] Add support for call_tf in a shape polymorphic program","The use case for call_tf with shape polymorphism is when we have a JAX program that calls into TF function, and we want to serialize the JAX program with some shapes unknown. Today this use case does not work, except in the special case when the output shape of the called TF function returns statically known shapes. The idea is that we allow the user of call_tf to specify the output shape. This can be done even in presence of shape polymorphism, by writing the output shape as an expression in terms of the input shapes. This is what other JAX primitives do, e.g., concat, so we are simply enabling call_tf to get the same behavior. This change should be enough for oldstyle jax2tf, but will require more work for native serialization. We also removed some old code that was trying to workaround some limitations in shape inference in TF. I think that those workarounds are ugly, and I am prepared to give error messages rather than keep that code. So far no tests fail. Still to do:  * ~~more testing~~  * ~~check that the TF function returns values of the declared shapes~~  * ~~documentation~~  * ~~CHANGELOG~~",2023-03-01T14:31:20Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14734,  
2235,"以下是一个github上的jax下的一个issue, 标题是([custom_vjp] bwd function should not be WrappedFun, may run multiple times)， 内容是 (The `custom_jvp_p` primitive is kind of like the `xla_call_p` primitive underlying finalstyle (old) `jit`. With `xla_call_p`, we have one Python callable (wrapped in an `lu.WrappedFun`) which is called exactly once. With `custom_jvp_p` we have two functions (each wrapped in an `lu.WrappedFun`), representing either the undifferentiated primal function or its JVP rule, and exactly one is called exactly once (determined not at runtime but rather by whether an AD pass hits at tracing time). The calledexactlyonce property is useful for reasoning about values plumbed through mutable cells. Indeed one of the main reasons `lu.WrappedFun` exists is to handle populating `Store`s for such plumbing. While for `xla_call_p` and `custom_jvp_p` we have one and two functions, respectively, wrapped as `lu.WrappedFun`s corresponding to how they're called exactly once, the `custom_vjp_p` primitive is parameterized by _three_ callables. The three callables represent the primal function, the forward pass under autodiff, and the backward pass under autodiff. And each is wrapped as an `lu.WrappedFun`. At least, they were until this PR. But that was a bug: after differentiation, the backward pass rule need not be called exactly once. Unlike the primal and forward pass functions, that's just not a property it has. So it never should have been an `lu.WrappedFun`! Tellingly, we never _needed_ it to be a `WrappedFun` either, in that we never used any `Store`s with it. Even though the backwardpass rule could be run more than once, for some reason we never noticed until now. Perhaps that's because we only indirectly check whether `WrappedFun`s are run more than once, by checking whether any `Store`s are populated more than once. But `Store`s only became associated with the backward pass rule when `vmap` got involved. So I guess we just hadn't hit this combination until now! The fix is simple: just don't make the backward pass rule a `WrappedFun`. Adapt callees not to expect it.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[custom_vjp] bwd function should not be WrappedFun, may run multiple times","The `custom_jvp_p` primitive is kind of like the `xla_call_p` primitive underlying finalstyle (old) `jit`. With `xla_call_p`, we have one Python callable (wrapped in an `lu.WrappedFun`) which is called exactly once. With `custom_jvp_p` we have two functions (each wrapped in an `lu.WrappedFun`), representing either the undifferentiated primal function or its JVP rule, and exactly one is called exactly once (determined not at runtime but rather by whether an AD pass hits at tracing time). The calledexactlyonce property is useful for reasoning about values plumbed through mutable cells. Indeed one of the main reasons `lu.WrappedFun` exists is to handle populating `Store`s for such plumbing. While for `xla_call_p` and `custom_jvp_p` we have one and two functions, respectively, wrapped as `lu.WrappedFun`s corresponding to how they're called exactly once, the `custom_vjp_p` primitive is parameterized by _three_ callables. The three callables represent the primal function, the forward pass under autodiff, and the backward pass under autodiff. And each is wrapped as an `lu.WrappedFun`. At least, they were until this PR. But that was a bug: after differentiation, the backward pass rule need not be called exactly once. Unlike the primal and forward pass functions, that's just not a property it has. So it never should have been an `lu.WrappedFun`! Tellingly, we never _needed_ it to be a `WrappedFun` either, in that we never used any `Store`s with it. Even though the backwardpass rule could be run more than once, for some reason we never noticed until now. Perhaps that's because we only indirectly check whether `WrappedFun`s are run more than once, by checking whether any `Store`s are populated more than once. But `Store`s only became associated with the backward pass rule when `vmap` got involved. So I guess we just hadn't hit this combination until now! The fix is simple: just don't make the backward pass rule a `WrappedFun`. Adapt callees not to expect it.",2023-03-01T01:19:25Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14728
998,"以下是一个github上的jax下的一个issue, 标题是(Improved documentation on `jax.random.gamma`)， 内容是 (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I think the documentation for the `jax.random.gamma` random variable (sampler) could possibly be clearer. In particular, letting the user know that the standard form is implemented and how to incorporate the rate (or alternatively scale) parameter in their samples. Maybe an addendum along the lines of  *""we implement the standard gamma density. Dividing the sample by the rate is equivalent to sampling from* $\text{Gamma}(\alpha, \text{rate})$*. And multiplying the sample by the scale is equivalent to sampling from* $\text{Gamma}(\alpha, \text{scale})$."" The discussion for this started here and I'm mentioning  for visibility. Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improved documentation on `jax.random.gamma`,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I think the documentation for the `jax.random.gamma` random variable (sampler) could possibly be clearer. In particular, letting the user know that the standard form is implemented and how to incorporate the rate (or alternatively scale) parameter in their samples. Maybe an addendum along the lines of  *""we implement the standard gamma density. Dividing the sample by the rate is equivalent to sampling from* $\text{Gamma}(\alpha, \text{rate})$*. And multiplying the sample by the scale is equivalent to sampling from* $\text{Gamma}(\alpha, \text{scale})$."" The discussion for this started here and I'm mentioning  for visibility. Thanks!",2023-02-26T18:51:40Z,enhancement,closed,0,0,https://github.com/jax-ml/jax/issues/14691
1018,"以下是一个github上的jax下的一个issue, 标题是(np.linalg.norm should use numerically stable p-norms.)， 内容是 (I think that it would make to implement https://timvieira.github.io/blog/post/2014/11/10/numericallystablepnorms/ similar to to how it's implemented for the logsumexp here. This would allow for larger norms to work.  While ```python >>> jax.numpy.linalg.norm(jax.numpy.array([1000, 1000.0]), 30) Array(inf, dtype=float32) ``` while ```python >>> def snorm(x, p): ...     a = jax.numpy.max(jax.numpy.abs(x)) ...     return a * jax.numpy.linalg.norm(x / a, p) ...  >>> snorm(jax.numpy.array([1000, 1000.0]), 30) Array(1023.37384, dtype=float32) ``` maybe this should be a special argument as it increases the memory read and most people should be fine w/o it?  [x ] Check for duplicate requests.  [x ] Describe your goal, and if possible provide a code snippet with a motivating example.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,np.linalg.norm should use numerically stable p-norms.,"I think that it would make to implement https://timvieira.github.io/blog/post/2014/11/10/numericallystablepnorms/ similar to to how it's implemented for the logsumexp here. This would allow for larger norms to work.  While ```python >>> jax.numpy.linalg.norm(jax.numpy.array([1000, 1000.0]), 30) Array(inf, dtype=float32) ``` while ```python >>> def snorm(x, p): ...     a = jax.numpy.max(jax.numpy.abs(x)) ...     return a * jax.numpy.linalg.norm(x / a, p) ...  >>> snorm(jax.numpy.array([1000, 1000.0]), 30) Array(1023.37384, dtype=float32) ``` maybe this should be a special argument as it increases the memory read and most people should be fine w/o it?  [x ] Check for duplicate requests.  [x ] Describe your goal, and if possible provide a code snippet with a motivating example.",2023-02-25T04:32:16Z,enhancement,open,2,0,https://github.com/jax-ml/jax/issues/14679
555,"以下是一个github上的jax下的一个issue, 标题是(Add PyArrayResultHandler which behaves like)， 内容是 (Add PyArrayResultHandler which behaves like functools.partial(jax.arrays.ArrayImpl) with the added benefit that the new PyExecuteResults type can explode directly into ArrayImpls if passed to explode_with_handlers(). Note that this also helps with deprecating PyBuffer as the fastpath does not need to call the PyBuffer constructor.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add PyArrayResultHandler which behaves like,Add PyArrayResultHandler which behaves like functools.partial(jax.arrays.ArrayImpl) with the added benefit that the new PyExecuteResults type can explode directly into ArrayImpls if passed to explode_with_handlers(). Note that this also helps with deprecating PyBuffer as the fastpath does not need to call the PyBuffer constructor.,2023-02-25T02:56:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14678
2509,"以下是一个github上的jax下的一个issue, 标题是(Very slow JIT compilation due to constant-folding of very large BCOO sparse matrix nonzero entry array)， 内容是 ( Description I've got a use case where I'd like to store the nonzero entries of a very large sparse matrix, and then access them later during a machine learning training loop. Unfortunately, using JIT compilation results in constantfolding of this array, making it extremely slow on large problems. Here's an MWE that runs on my laptop and captures the typical behavior: ```python import jax import jax.numpy as jnp import jax.experimental.sparse as sparse from jax.experimental.sparse import BCOO n = 10000000 def build_sparse_linear_operator():     nonzeroes = sparse.eye(n).indices  shape (n,2)     def product(other):         matrix = BCOO((jnp.ones(n),nonzeroes), shape=(n,n), indices_sorted=True, unique_indices=True)         return matrix @ other     return product operator = build_sparse_linear_operator() def fn(x):     return operator(jnp.ones(n) / x).sum() fn(1.0)  executes in 0.1s jax.jit(fn)(1.0)  executes in almost one minute ``` Calling the function without JIT executes in about a tenth of a second, but calling it with JIT takes almost a minute. On larger problems in the codebase which prompted this MWE, I have had it crash due to running out of memory after about an hour. This produces warnings similar to the following: ``` Constant folding an instruction is taking > 8s:   slice.22 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above). This isn't necessarily a bug; constantfolding is inherently a tradeoff between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time. ``` The problem seems to be that the stored array, `nonzeroes` has shape `(n,2)`, which in this case is very large, yet the JIT compiler tries to constantfold it. This seems like a bug, unless there are good reasons why arrays with millions of elements should be constantfolded, in which case it would be very helpful to have some way of telling the compiler not to do so in this case.  What jax/jaxlib version are you using? v0.4.4  Which accelerator(s) are you using? N/A  Additional system info N/A  NVIDIA GPU info N/A)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Very slow JIT compilation due to constant-folding of very large BCOO sparse matrix nonzero entry array," Description I've got a use case where I'd like to store the nonzero entries of a very large sparse matrix, and then access them later during a machine learning training loop. Unfortunately, using JIT compilation results in constantfolding of this array, making it extremely slow on large problems. Here's an MWE that runs on my laptop and captures the typical behavior: ```python import jax import jax.numpy as jnp import jax.experimental.sparse as sparse from jax.experimental.sparse import BCOO n = 10000000 def build_sparse_linear_operator():     nonzeroes = sparse.eye(n).indices  shape (n,2)     def product(other):         matrix = BCOO((jnp.ones(n),nonzeroes), shape=(n,n), indices_sorted=True, unique_indices=True)         return matrix @ other     return product operator = build_sparse_linear_operator() def fn(x):     return operator(jnp.ones(n) / x).sum() fn(1.0)  executes in 0.1s jax.jit(fn)(1.0)  executes in almost one minute ``` Calling the function without JIT executes in about a tenth of a second, but calling it with JIT takes almost a minute. On larger problems in the codebase which prompted this MWE, I have had it crash due to running out of memory after about an hour. This produces warnings similar to the following: ``` Constant folding an instruction is taking > 8s:   slice.22 (displaying the full instruction incurs a runtime overhead. Raise your logging level to 4 or above). This isn't necessarily a bug; constantfolding is inherently a tradeoff between compilation time and speed at runtime.  XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time. ``` The problem seems to be that the stored array, `nonzeroes` has shape `(n,2)`, which in this case is very large, yet the JIT compiler tries to constantfold it. This seems like a bug, unless there are good reasons why arrays with millions of elements should be constantfolded, in which case it would be very helpful to have some way of telling the compiler not to do so in this case.  What jax/jaxlib version are you using? v0.4.4  Which accelerator(s) are you using? N/A  Additional system info N/A  NVIDIA GPU info N/A",2023-02-24T01:42:29Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/14655,"It seems like the compiler isn't making a great choice here with respect to the runtime/compiletime tradeoffs involved in constant folding. If you change your code slightly though, the problematic array will be computed at runtime: ```python def build_sparse_linear_operator():     def product(other):         nonzeroes = sparse.eye(n).indices  shape (n,2)         matrix = BCOO((jnp.ones(n),nonzeroes), shape=(n,n), indices_sorted=True, unique_indices=True)         return matrix @ other     return product ```","> It seems like the compiler isn't making a great choice here with respect to the runtime/compiletime tradeoffs involved in constant folding. If you change your code slightly though, the problematic array will be computed at runtime: >  > ```python > def build_sparse_linear_operator(): >     def product(other): >         nonzeroes = sparse.eye(n).indices  shape (n,2) >         matrix = BCOO((jnp.ones(n),nonzeroes), shape=(n,n), indices_sorted=True, unique_indices=True) >         return matrix @ other >     return product > ``` Thanks! Unfortunately, while this would avoid the problem here, I can't fix things upstream in that way  the array `nonzeroes` in my codebase is computed using what is effectively a blackbox, CPUonly algorithm outside of JAX. Perhaps a better way to write the MWE would have been to use `scipy.sparse.eye` or similar instead. Do you know if there are any workarounds I can implement to prevent the compiler from constantfolding the array?","I don't know... it's really an XLA bug, and I'm not sure of a way to change what XLA does here. Maybe you could rewrite your code so that `nonzeros` is explicitly passed to the jitcompiled outer function? I know that's probably not the answer you're looking for, but I think it would work...","Thanks! Unfortunately, passing around `nonzeroes` won't work, since this is something computed by the package which should not be exposed to the user, and the functions it would need to be passed into are called by the user. Should I file a bug report upstream? Would the TensorFlow repository be the right place?","It looks like one possible workaround for now is to use an optimization barrier: ```python from jax._src.ad_checkpoint import _optimization_barrier def build_sparse_linear_operator():     nonzeroes = sparse.eye(n).indices  shape (n,2)     def product(other):         nz = _optimization_barrier(nonzeroes)         matrix = BCOO((jnp.ones(n), nz), shape=(n,n), indices_sorted=True, unique_indices=True)         return matrix @ other     return product ``` This is still somewhat experimental, so unfortunately there is no public API for this.","That said, it's probably worth filing an XLA bug for this. It should be something that the compiler handles automatically: https://github.com/openxla/xla","Thanks, that works! Two comments: 1. The function `_optimization_barrier` **must** be called inside `product`, and not outside of it, so for instance `nonzeroes = _optimization_barrier(sparse.eye(n).indices)` will not work. 2. This XLA bug might be specific to integer arrays: in my upstream codebase, I have other arrays which are also precomputed, but wrapping BCOO nonzero index arrays in `_optimization_barrier` is sufficient to get JIT to not freeze. Very much appreciate your help with this!",", even though it seems like an XLA bug, it would be helpful to mention which hardware you are using (compilers have different backends, so code paths are different).","> , even though it seems like an XLA bug, it would be helpful to mention which hardware you are using (compilers have different backends, so code paths are different). Sure! Have reproduced this issue on both Nvidia GPU and Apple M1.","I know this thread is about a year old, but I thought I would note that I've run into similar issues with JITing large sparse arrays (in my case, for moderate sized finite element simulations  ~2M elements). Beyond slow constant folding, I've also run into the limit where XLA seg faults if the array is too large (generally somewhere around > 400,000,000 nonzero elements in the array). In this case, _optimization_barrier has no effect for me and the only solution is to provide the sparse array indices as an input to my function. I've run into this issue on both Linux and Windows. Due to the size of the arrays, I've only been able to try this on the CPU (as my GPUs don't have enough memory for problems this large)."
4461,"以下是一个github上的jax下的一个issue, 标题是(Add `random.chisquare` and `random.f` )， 内容是 (For my works related to statistics part, chisquare or F distribution are often used, and there're `random.chisquare` and `random.f` in `numpy`, I'd implement these for my convenience in `JAX`, I'd like to know, would you like to add `chisquare` or `f` in future? Actually I'veimplement these for my convenience in `JAX` and pass the tests Here's my code ```python def chisquare(key: KeyArray,               df: RealArray,               shape: Shape = (),               dtype: DTypeLikeFloat = dtypes.float_) > Array:   """"""Sample Chisquare random values with given shape and float dtype.   Args:     key: a PRNG key used as the random key.     df: a float or array of floats broadcastcompatible with ``shape``       representing the parameter of the distribution.     shape: optional, a tuple of nonnegative integers specifying the result       shape. Must be broadcastcompatible with ``df``. The default (None)       produces a result shape equal to ``df.shape``.     dtype: optional, a float dtype for the returned values (default float64 if       jax_enable_x64 is true, otherwise float32).   Returns:     A random array with the specified dtype and with shape given by ``shape`` if     ``shape`` is not None, or else by ``df.shape``.   """"""   key, _ = _check_prng_key(key)   if not dtypes.issubdtype(dtype, np.floating):     raise ValueError(f""dtype argument to `chisquare` must be a float ""                     f""dtype, got {dtype}"")   dtype = dtypes.canonicalize_dtype(dtype)   shape = core.canonicalize_shape(shape)   return _chisquare(key, df, shape, dtype) (jit, static_argnums=(2, 3)) def _chisquare(key, df, shape, dtype) > Array:   if shape is None:     shape = np.shape(df)   else:     _check_shape(""chisquare"", shape, np.shape(df))   df = lax.convert_element_type(df, dtype)   two = _lax_const(df, 2)   half_df = lax.div(df, two)   log_g = loggamma(key, a=half_df, shape=shape, dtype=dtype)   chi2 = lax.mul(jnp.exp(log_g), two)   return chi2 def f(key: KeyArray,       dfnum: RealArray,       dfden: RealArray,       shape: Shape = (),       dtype: DTypeLikeFloat = dtypes.float_) > Array:   """"""Sample Fdistribution random values with given shape and float dtype.   Args:     key: a PRNG key used as the random key.     dfnum: a float or array of floats broadcastcompatible with ``shape``       representing the numerator's ``df`` of the distribution.     dfden: a float or array of floats broadcastcompatible with ``shape``       representing the denominator's ``df`` of the distribution.     shape: optional, a tuple of nonnegative integers specifying the result       shape. Must be broadcastcompatible with ``dfnum`` and ``dfden``.       The default (None) produces a result shape equal to ``dfnum.shape``,       and ``dfden.shape``.     dtype: optional, a float dtype for the returned values (default float64 if       jax_enable_x64 is true, otherwise float32).   Returns:     A random array with the specified dtype and with shape given by ``shape`` if     ``shape`` is not None, or else by ``df.shape``.   """"""   key, _ = _check_prng_key(key)   if not dtypes.issubdtype(dtype, np.floating):     raise ValueError(f""dtype argument to `f` must be a float ""                     f""dtype, got {dtype}"")   if not np.shape(dfden) == np.shape(dfnum):     raise ValueError(f""shape of `dfnum` and `dfden` must be equal ""                     f""got {np.shape(dfnum)} != {np.shape(dfden)}"")   dtype = dtypes.canonicalize_dtype(dtype)   shape = core.canonicalize_shape(shape)   return _f(key, dfnum, dfden, shape, dtype) (jit, static_argnums=(3, 4)) def _f(key, dfnum, dfden, shape, dtype) > Array:   if shape is None:     shape = np.shape(dfnum)   else:     _check_shape(""f"", shape, np.shape(dfnum))   dfden = lax.convert_element_type(dfden, dtype)   dfnum = lax.convert_element_type(dfnum, dtype)   key_dfd, key_dfn = _split(key)   chisq_dfn = lax.div(chisquare(key_dfn, dfnum, shape, dtype) ,dfnum)   chisq_dfd = lax.div(chisquare(key_dfd, dfden, shape, dtype) ,dfden)   f = lax.div(chisq_dfn, chisq_dfd)   return f ``` I use gamma to produce chisquare, then chisquare to F, but when  comes to small `dfden` and `dfnum` situation, I tried to change prng key to pass the tests. As above, I wonder if you guys like to add theses features?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add `random.chisquare` and `random.f` ,"For my works related to statistics part, chisquare or F distribution are often used, and there're `random.chisquare` and `random.f` in `numpy`, I'd implement these for my convenience in `JAX`, I'd like to know, would you like to add `chisquare` or `f` in future? Actually I'veimplement these for my convenience in `JAX` and pass the tests Here's my code ```python def chisquare(key: KeyArray,               df: RealArray,               shape: Shape = (),               dtype: DTypeLikeFloat = dtypes.float_) > Array:   """"""Sample Chisquare random values with given shape and float dtype.   Args:     key: a PRNG key used as the random key.     df: a float or array of floats broadcastcompatible with ``shape``       representing the parameter of the distribution.     shape: optional, a tuple of nonnegative integers specifying the result       shape. Must be broadcastcompatible with ``df``. The default (None)       produces a result shape equal to ``df.shape``.     dtype: optional, a float dtype for the returned values (default float64 if       jax_enable_x64 is true, otherwise float32).   Returns:     A random array with the specified dtype and with shape given by ``shape`` if     ``shape`` is not None, or else by ``df.shape``.   """"""   key, _ = _check_prng_key(key)   if not dtypes.issubdtype(dtype, np.floating):     raise ValueError(f""dtype argument to `chisquare` must be a float ""                     f""dtype, got {dtype}"")   dtype = dtypes.canonicalize_dtype(dtype)   shape = core.canonicalize_shape(shape)   return _chisquare(key, df, shape, dtype) (jit, static_argnums=(2, 3)) def _chisquare(key, df, shape, dtype) > Array:   if shape is None:     shape = np.shape(df)   else:     _check_shape(""chisquare"", shape, np.shape(df))   df = lax.convert_element_type(df, dtype)   two = _lax_const(df, 2)   half_df = lax.div(df, two)   log_g = loggamma(key, a=half_df, shape=shape, dtype=dtype)   chi2 = lax.mul(jnp.exp(log_g), two)   return chi2 def f(key: KeyArray,       dfnum: RealArray,       dfden: RealArray,       shape: Shape = (),       dtype: DTypeLikeFloat = dtypes.float_) > Array:   """"""Sample Fdistribution random values with given shape and float dtype.   Args:     key: a PRNG key used as the random key.     dfnum: a float or array of floats broadcastcompatible with ``shape``       representing the numerator's ``df`` of the distribution.     dfden: a float or array of floats broadcastcompatible with ``shape``       representing the denominator's ``df`` of the distribution.     shape: optional, a tuple of nonnegative integers specifying the result       shape. Must be broadcastcompatible with ``dfnum`` and ``dfden``.       The default (None) produces a result shape equal to ``dfnum.shape``,       and ``dfden.shape``.     dtype: optional, a float dtype for the returned values (default float64 if       jax_enable_x64 is true, otherwise float32).   Returns:     A random array with the specified dtype and with shape given by ``shape`` if     ``shape`` is not None, or else by ``df.shape``.   """"""   key, _ = _check_prng_key(key)   if not dtypes.issubdtype(dtype, np.floating):     raise ValueError(f""dtype argument to `f` must be a float ""                     f""dtype, got {dtype}"")   if not np.shape(dfden) == np.shape(dfnum):     raise ValueError(f""shape of `dfnum` and `dfden` must be equal ""                     f""got {np.shape(dfnum)} != {np.shape(dfden)}"")   dtype = dtypes.canonicalize_dtype(dtype)   shape = core.canonicalize_shape(shape)   return _f(key, dfnum, dfden, shape, dtype) (jit, static_argnums=(3, 4)) def _f(key, dfnum, dfden, shape, dtype) > Array:   if shape is None:     shape = np.shape(dfnum)   else:     _check_shape(""f"", shape, np.shape(dfnum))   dfden = lax.convert_element_type(dfden, dtype)   dfnum = lax.convert_element_type(dfnum, dtype)   key_dfd, key_dfn = _split(key)   chisq_dfn = lax.div(chisquare(key_dfn, dfnum, shape, dtype) ,dfnum)   chisq_dfd = lax.div(chisquare(key_dfd, dfden, shape, dtype) ,dfden)   f = lax.div(chisq_dfn, chisq_dfd)   return f ``` I use gamma to produce chisquare, then chisquare to F, but when  comes to small `dfden` and `dfnum` situation, I tried to change prng key to pass the tests. As above, I wonder if you guys like to add theses features?",2023-02-23T14:37:45Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/14640,Hi  thanks for bringing this up! I think a welltested `chisquare` and `f` functionality would be a welcome contribution to `jax.random`. Would you like to prepare a pull request?,Sure，I think I’ll open a PR soon  ,"Since random.chi2 and random.f has been merged, the issue is closed"
924,"以下是一个github上的jax下的一个issue, 标题是(Optimize canonicalize_shape)， 内容是 (Optimize canonicalize_shape I was looking at some profiles and noticed canonicalize_shape showing up as a noticeable overhead in certain cases. Which makes sense, given that we carefully check all possible cases before trying to consider integers as plausible elements (which are the most popular _by far_). And this function is pretty hot, because it gets called any time we create a new `ShapedArray`. I wrote a small benchmark that repeatedly calls canonicalize_shape on a 4sized tuple of integers. Before: 7.62µs ± 8% After: 1.42µs ± 2% So a pretty easy 5x improvement overall. And in more real cases, when resharding an array onto 8 TPUs, 50% of the time was spent on creating shapes for avals of device buffers.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Optimize canonicalize_shape,"Optimize canonicalize_shape I was looking at some profiles and noticed canonicalize_shape showing up as a noticeable overhead in certain cases. Which makes sense, given that we carefully check all possible cases before trying to consider integers as plausible elements (which are the most popular _by far_). And this function is pretty hot, because it gets called any time we create a new `ShapedArray`. I wrote a small benchmark that repeatedly calls canonicalize_shape on a 4sized tuple of integers. Before: 7.62µs ± 8% After: 1.42µs ± 2% So a pretty easy 5x improvement overall. And in more real cases, when resharding an array onto 8 TPUs, 50% of the time was spent on creating shapes for avals of device buffers.",2023-02-23T14:19:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14639
643,"以下是一个github上的jax下的一个issue, 标题是([sparse] remove handling of padded indices from COO/CSR)， 内容是 (While trying to fix failures that arose in CC([sparse] modify impl rules to always use lowering path), it became clear that the `COO` and `CSR` representation is fundamentally flawed in the case that `nse` is set to larger than the true nse. Fixing this would take a lot of effort that would be better spent on the longterm plans with BCOO and BCSR. Eventually we should deprecate and remove `COO` and `CSR`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] remove handling of padded indices from COO/CSR,"While trying to fix failures that arose in CC([sparse] modify impl rules to always use lowering path), it became clear that the `COO` and `CSR` representation is fundamentally flawed in the case that `nse` is set to larger than the true nse. Fixing this would take a lot of effort that would be better spent on the longterm plans with BCOO and BCSR. Eventually we should deprecate and remove `COO` and `CSR`.",2023-02-22T23:01:13Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14631
570,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Use CUDA and ROCM instead of GPU for XlaCallModuleOp platforms)， 内容是 ([jax2tf] Use CUDA and ROCM instead of GPU for XlaCallModuleOp platforms JAX is moving to using ROCM and CUDA instead of the generic GPU platform type and it is already supporting separate lowerings for ROCM and CUDA. To keep up with this functionality, we move the XlaCallModuleOp to supporting ROCM and CUDA platforms.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Use CUDA and ROCM instead of GPU for XlaCallModuleOp platforms,"[jax2tf] Use CUDA and ROCM instead of GPU for XlaCallModuleOp platforms JAX is moving to using ROCM and CUDA instead of the generic GPU platform type and it is already supporting separate lowerings for ROCM and CUDA. To keep up with this functionality, we move the XlaCallModuleOp to supporting ROCM and CUDA platforms.",2023-02-22T09:03:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14621
2702,"以下是一个github上的jax下的一个issue, 标题是(Deal with NaN errors in jax.experimental.ode.optimal_step_size())， 内容是 ( Description The following is related to my discussion about diffrax with kidger here: https://github.com/patrickkidger/diffrax/issues/223  It can happen when solving certain ODE systems that the error estimate for adaptive stepping will become NaN (e.g., if a trial timestep is too large). Currently this means jax.experimental.ode.odeint will fail. If we use jax.debug.print to print the values of the state variables, derivatives, and other intermediary quantities inside the integrator function while odeint is running, we would see NaN's in all timesteps except perhaps the first few (after an initial NaN appears, odeint cannot recover). The fix is simple  change this line in jax.experimental.ode.optimal_step_size() from  ``` factor = jnp.minimum(ifactor,                       jnp.maximum(mean_error_ratio**(1.0 / order) * safety, dfactor)) ``` to something like ``` factor = jnp.nanmin(jnp.array([ifactor,                                  jnp.nanmax(jnp.array([mean_error_ratio**(1.0 / order) * safety, dfactor]))]))   ``` I don't have a minimal working example because the code I was running into this problem with is a bit complicated (it involves a nonautonomous ODE system). However, you can plug in mean_error_ratio = jnp.nan in the above two examples to see what I mean. I wrote my own adaptive RK23 (BogackiShampine) integrator in JAX to diagnose the above problem. My simple integrator had the same problem as jax's odeint until I made that switch to use the jnp.nanmin and jnp.nanmax functions when computing the multiplicative factor for the new optimal stepsize (following eqn 4.13 of Hairer section II.4). Now both jaxodeint and my manual JAXbased RK23 integrator give the correct, expected solution compared to my original pure Python implementation of my ODE system using scipy.integrate.solve_ivp and manual nonadaptive Euler integration with extremely small timesteps. For what it's worth, scipy.integrate.solve_ivp did not run into this issue because it uses Python's builtin min and max functions, which can weirdly handle NaN's as long as they are not the first argument (see lines 152168 of scipy.integrate._ivp.rk)... Thoughts? Could this spell trouble for taking the gradient? (I assume not since steps involving NaN would now explicitly be rejected?)  What jax/jaxlib version are you using? jax 0.4.2, jaxlib 0.4.2  Which accelerator(s) are you using? CPU  Additional system info Macbook M1 Pro  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Deal with NaN errors in jax.experimental.ode.optimal_step_size()," Description The following is related to my discussion about diffrax with kidger here: https://github.com/patrickkidger/diffrax/issues/223  It can happen when solving certain ODE systems that the error estimate for adaptive stepping will become NaN (e.g., if a trial timestep is too large). Currently this means jax.experimental.ode.odeint will fail. If we use jax.debug.print to print the values of the state variables, derivatives, and other intermediary quantities inside the integrator function while odeint is running, we would see NaN's in all timesteps except perhaps the first few (after an initial NaN appears, odeint cannot recover). The fix is simple  change this line in jax.experimental.ode.optimal_step_size() from  ``` factor = jnp.minimum(ifactor,                       jnp.maximum(mean_error_ratio**(1.0 / order) * safety, dfactor)) ``` to something like ``` factor = jnp.nanmin(jnp.array([ifactor,                                  jnp.nanmax(jnp.array([mean_error_ratio**(1.0 / order) * safety, dfactor]))]))   ``` I don't have a minimal working example because the code I was running into this problem with is a bit complicated (it involves a nonautonomous ODE system). However, you can plug in mean_error_ratio = jnp.nan in the above two examples to see what I mean. I wrote my own adaptive RK23 (BogackiShampine) integrator in JAX to diagnose the above problem. My simple integrator had the same problem as jax's odeint until I made that switch to use the jnp.nanmin and jnp.nanmax functions when computing the multiplicative factor for the new optimal stepsize (following eqn 4.13 of Hairer section II.4). Now both jaxodeint and my manual JAXbased RK23 integrator give the correct, expected solution compared to my original pure Python implementation of my ODE system using scipy.integrate.solve_ivp and manual nonadaptive Euler integration with extremely small timesteps. For what it's worth, scipy.integrate.solve_ivp did not run into this issue because it uses Python's builtin min and max functions, which can weirdly handle NaN's as long as they are not the first argument (see lines 152168 of scipy.integrate._ivp.rk)... Thoughts? Could this spell trouble for taking the gradient? (I assume not since steps involving NaN would now explicitly be rejected?)  What jax/jaxlib version are you using? jax 0.4.2, jaxlib 0.4.2  Which accelerator(s) are you using? CPU  Additional system info Macbook M1 Pro  NVIDIA GPU info _No response_",2023-02-21T23:22:58Z,bug contributions welcome,open,0,0,https://github.com/jax-ml/jax/issues/14612
350,"以下是一个github上的jax下的一个issue, 标题是(Add support for StableHLO Serialized Portable Artifacts in XlaCallModuleOp and JAX2TF)， 内容是 (Add support for StableHLO Serialized Portable Artifacts in XlaCallModuleOp and JAX2TF)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add support for StableHLO Serialized Portable Artifacts in XlaCallModuleOp and JAX2TF,Add support for StableHLO Serialized Portable Artifacts in XlaCallModuleOp and JAX2TF,2023-02-21T22:05:19Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14609
1832,"以下是一个github上的jax下的一个issue, 标题是(Kaggle TPU notebook wastes around a minute during initialization)， 内容是 ( With jax/jaxlib 0.4.4 installed: ``` > jax.devices() WARNING: Logging before InitGoogle() is written to STDERR I0000 00:00:1676990884.185389      14 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1676990894.686695      14 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1676990905.186952      14 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1676990915.688738      14 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1676990926.189269      14 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached /usr/local/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py:180: UserWarning: TPU backend initialization is taking more than 60.0 seconds. Did you run your code on all TPU hosts? See https://jax.readthedocs.io/en/latest/multi_process.html for more information.   warnings.warn( I0000 00:00:1676990936.690875      14 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached Failed to get agentworkernumber with error: Timeout was reachedE0221 14:49:23.280024242     375 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:Cares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:""20230221T14:49:23.280006679+00:00"", grpc_status:2} ``` Each timeout takes several seconds. Everything appears to work fine after the 6th attempt and failure, but I'm wondering why the error message is happening at all.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,Kaggle TPU notebook wastes around a minute during initialization," With jax/jaxlib 0.4.4 installed: ``` > jax.devices() WARNING: Logging before InitGoogle() is written to STDERR I0000 00:00:1676990884.185389      14 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1676990894.686695      14 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1676990905.186952      14 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1676990915.688738      14 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1676990926.189269      14 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached /usr/local/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py:180: UserWarning: TPU backend initialization is taking more than 60.0 seconds. Did you run your code on all TPU hosts? See https://jax.readthedocs.io/en/latest/multi_process.html for more information.   warnings.warn( I0000 00:00:1676990936.690875      14 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached Failed to get agentworkernumber with error: Timeout was reachedE0221 14:49:23.280024242     375 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:Cares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:""20230221T14:49:23.280006679+00:00"", grpc_status:2} ``` Each timeout takes several seconds. Everything appears to work fine after the 6th attempt and failure, but I'm wondering why the error message is happening at all.",2023-02-21T15:16:56Z,TPU,closed,0,2,https://github.com/jax-ml/jax/issues/14600,"The latest jax 0.4.6 release fixes this. Leaving this issue open until Kaggle updates their default jax version, but you can manually upgrade in the meantime as a workaround: `!pip install U jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html`","The default jax version on Kaggle notebooks is now 0.4.6, woohoo! Create a new notebook to try it out; old notebooks will still have the old image. I verified that running `import jax` + `jax.devices()` work smoothly."
528,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Enable strict platform checking for native serialized modules.)， 内容是 (This takes advantage of recent changes to XlaCallModule that allow us to use a `platforms` attribute to specify what are the allowable platforms for a serialized module. Also add a `experimental_native_lowering_strict_checks` parameter to `jax2tf.convert` to disable the check.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Enable strict platform checking for native serialized modules.,This takes advantage of recent changes to XlaCallModule that allow us to use a `platforms` attribute to specify what are the allowable platforms for a serialized module. Also add a `experimental_native_lowering_strict_checks` parameter to `jax2tf.convert` to disable the check.,2023-02-21T08:23:26Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14594, PTAL
526,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Add support for cross-platform lowering in native serialization)， 内容是 (Allow the user of native serialization to specify the platform for which the serialization to be done. This relies on newly added support for platform checking in XlaCallModule op (version 3). The implementation here is temporary, pending having a proper crosslowering API. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Add support for cross-platform lowering in native serialization,"Allow the user of native serialization to specify the platform for which the serialization to be done. This relies on newly added support for platform checking in XlaCallModule op (version 3). The implementation here is temporary, pending having a proper crosslowering API. ",2023-02-20T11:32:15Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14587
1298,"以下是一个github上的jax下的一个issue, 标题是(Integer convolution with int8 inputs and int32 output)， 内容是 (I'm trying to do a convolution with an int8 input image and kernel tensors, then get back an int32 output tensor, but I don't see any way to do that: ``` kernel = jax.numpy.arange(3*3*3*64).reshape(3,3,3,64).astype(jax.numpy.int8) img = jax.numpy.arange(32*1024*1024*3).reshape(32,1024,1024,3).astype(jax.numpy.int8) out = jax.lax.conv(lhs=jax.numpy.transpose(img,[0,3,1,2]), rhs=jax.numpy.transpose(kernel,[3,2,0,1]), window_strides=(1, 1), padding='SAME') ``` gives back an int8 result. But there is an option to give preferred_element_type to get the output from a preferred type. But it doesn't support int32 or int16 types. It only supports float32 and int8 types. `out = jax.lax.conv(lhs=jax.numpy.transpose(img,[0,3,1,2]), rhs=jax.numpy.transpose(kernel,[3,2,0,1]), window_strides=(1, 1), padding='SAME', preferred_element_type ='jax.numpy.float32')` I could cast the output tensor to jax.numpy.int32 after the convolution, but I want to use a hardwareaccelerated int8 convolution, if available. Are there any available solutions in order to do that? Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Integer convolution with int8 inputs and int32 output,"I'm trying to do a convolution with an int8 input image and kernel tensors, then get back an int32 output tensor, but I don't see any way to do that: ``` kernel = jax.numpy.arange(3*3*3*64).reshape(3,3,3,64).astype(jax.numpy.int8) img = jax.numpy.arange(32*1024*1024*3).reshape(32,1024,1024,3).astype(jax.numpy.int8) out = jax.lax.conv(lhs=jax.numpy.transpose(img,[0,3,1,2]), rhs=jax.numpy.transpose(kernel,[3,2,0,1]), window_strides=(1, 1), padding='SAME') ``` gives back an int8 result. But there is an option to give preferred_element_type to get the output from a preferred type. But it doesn't support int32 or int16 types. It only supports float32 and int8 types. `out = jax.lax.conv(lhs=jax.numpy.transpose(img,[0,3,1,2]), rhs=jax.numpy.transpose(kernel,[3,2,0,1]), window_strides=(1, 1), padding='SAME', preferred_element_type ='jax.numpy.float32')` I could cast the output tensor to jax.numpy.int32 after the convolution, but I want to use a hardwareaccelerated int8 convolution, if available. Are there any available solutions in order to do that? Thanks!",2023-02-20T09:04:08Z,enhancement needs info NVIDIA GPU,open,0,2,https://github.com/jax-ml/jax/issues/14585,Which platform are you using? GPU/TPU?,I'm using Nvidia RTX 6000 GPU. My platform details are also here. Ubuntu 20.04 Cuda 12.0 Cudnn 8.5 Jax 0.4.4 Jaxlib 0.4.4
665,"以下是一个github上的jax下的一个issue, 标题是(Document jax.Array methods & attributes)， 内容是 (Currently there is no documentation for jax Array attributes and methods. CC(DOC: improve documentation for jax.Array methods) was an attempt at this, but it caused issues and had to be rolledback in CC(BUG: avoid passing functions directly to abstractmethod) We can't document `ArrayImpl` directly, because its import path does not match its `__module__` and `__name__`, and so sphinx treats it as an alias. I'm not sure the best way to proceed.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Document jax.Array methods & attributes,"Currently there is no documentation for jax Array attributes and methods. CC(DOC: improve documentation for jax.Array methods) was an attempt at this, but it caused issues and had to be rolledback in CC(BUG: avoid passing functions directly to abstractmethod) We can't document `ArrayImpl` directly, because its import path does not match its `__module__` and `__name__`, and so sphinx treats it as an alias. I'm not sure the best way to proceed.",2023-02-17T18:37:02Z,documentation,closed,1,2,https://github.com/jax-ml/jax/issues/14563,"Once https://github.com/google/jax/commit/44082be10376805c2f80cda0dfbb917f1e916853 is part of the `minimum_jaxlib`, we can add `ArrayImpl` to the HTML docs and fix this issue.","Chatting with  and  offline, I think we want to add true abstract method definitions to `jax.Array` and document the methods that way. This will require some reworking of how tracer methods are dispatched, but this shouldn't be too problematic."
856,"以下是一个github上的jax下的一个issue, 标题是(XlaRuntimeError: UNIMPLEMENTED: Only 1 computation per replica supported, 8 requested.   In call to configurable 'train' (<function train at 0x7f364f9c11f0>))， 内容是 ( Description I meet this error . Anyone can slove it ? Thank you very much ........  What jax/jaxlib version are you using? jax                           0.4.3 jaxlib                        0.4.2  Which accelerator(s) are you using? TPU on google colab pro plus.TPU v2  Additional system info I am trying to training t5x(https://github.com/googleresearch/t5x) on TPU .  This is my code  https://drive.google.com/file/d/1yo3eHCc9T4uTSpRyzW4vNQkB76kEwLlB/view?usp=sharing  NVIDIA GPU info I don't use GPU . Just using TPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"XlaRuntimeError: UNIMPLEMENTED: Only 1 computation per replica supported, 8 requested.   In call to configurable 'train' (<function train at 0x7f364f9c11f0>)", Description I meet this error . Anyone can slove it ? Thank you very much ........  What jax/jaxlib version are you using? jax                           0.4.3 jaxlib                        0.4.2  Which accelerator(s) are you using? TPU on google colab pro plus.TPU v2  Additional system info I am trying to training t5x(https://github.com/googleresearch/t5x) on TPU .  This is my code  https://drive.google.com/file/d/1yo3eHCc9T4uTSpRyzW4vNQkB76kEwLlB/view?usp=sharing  NVIDIA GPU info I don't use GPU . Just using TPU,2023-02-17T09:55:48Z,bug needs info,open,0,4,https://github.com/jax-ml/jax/issues/14552,is it possible to get a repro?,"If you could answer the questions in place of *No response*, it would be quite helpful for diagnosing your problem – thanks!",I just update more information   ,"Thanks  this is a version compatibility issue. For Colab TPU, please use jax & jaxlib version 0.3.25"
2944,"以下是一个github上的jax下的一个issue, 标题是(Colab INTERNAL: RET_CHECK failure)， 内容是 ( Description Dear All, I am facing a weird error message when running my code on Colab Nvidia GPU. It is very strange because the same code worked previously (2 weeks ago), and it also works seamlessly when run on Colab CPU. Here is a minimal working example.  Here, I install JAX & associated libraries. `'''(1) Import JAX ''' USE_CPU = False USE_GPU = True if USE_CPU:  !pip install upgrade q pip jaxlib jax if USE_GPU:   !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install upgrade q git+https://github.com/deepmind/dmhaiku.git !pip install upgrade q git+https://github.com/deepmind/optax.git   import jax` Outputing: `Looking in indexes: https://pypi.org/simple, https://uspython.pkg.dev/colabwheels/public/simple/ Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Requirement already satisfied: jax[cuda] in /usr/local/lib/python3.8/distpackages (0.4.3) Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/distpackages (from jax[cuda]) (1.7.3) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/distpackages (from jax[cuda]) (1.21.6) Requirement already satisfied: opteinsum in /usr/local/lib/python3.8/distpackages (from jax[cuda]) (3.3.0) Requirement already satisfied: jaxlib==0.4.3+cuda11.cudnn86 in /usr/local/lib/python3.8/distpackages (from jax[cuda]) (0.4.3+cuda11.cudnn86)   Preparing metadata (setup.py) ... done   Preparing metadata (setup.py) ... done` It looks JAX sees the GPU without any problems. `'''(2) Check devices ''' jax.devices()` `[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] ` However, if I want to try something, for example just to sample random numbers `jax.random.PRNGKey(0)` I get some weird XLA error. ` XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in  > 1 jax.random.PRNGKey(0) 23 frames /usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options, host_callbacks)    1024    TODO(sharadmv): remove this fallback when all backends allow `compile`    1025    to take in `host_callbacks` > 1026   return backend.compile(built_c, compile_options=options)    1027     1028 _ir_dump_counter = itertools.count() XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr` Can somebody please give me some advice, on what to do? The same codes which worked previously are now broken...  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info Colab  NVIDIA GPU info Thu Feb 16 16:13:21 2023        ++ )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Colab INTERNAL: RET_CHECK failure," Description Dear All, I am facing a weird error message when running my code on Colab Nvidia GPU. It is very strange because the same code worked previously (2 weeks ago), and it also works seamlessly when run on Colab CPU. Here is a minimal working example.  Here, I install JAX & associated libraries. `'''(1) Import JAX ''' USE_CPU = False USE_GPU = True if USE_CPU:  !pip install upgrade q pip jaxlib jax if USE_GPU:   !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install upgrade q git+https://github.com/deepmind/dmhaiku.git !pip install upgrade q git+https://github.com/deepmind/optax.git   import jax` Outputing: `Looking in indexes: https://pypi.org/simple, https://uspython.pkg.dev/colabwheels/public/simple/ Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Requirement already satisfied: jax[cuda] in /usr/local/lib/python3.8/distpackages (0.4.3) Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/distpackages (from jax[cuda]) (1.7.3) Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/distpackages (from jax[cuda]) (1.21.6) Requirement already satisfied: opteinsum in /usr/local/lib/python3.8/distpackages (from jax[cuda]) (3.3.0) Requirement already satisfied: jaxlib==0.4.3+cuda11.cudnn86 in /usr/local/lib/python3.8/distpackages (from jax[cuda]) (0.4.3+cuda11.cudnn86)   Preparing metadata (setup.py) ... done   Preparing metadata (setup.py) ... done` It looks JAX sees the GPU without any problems. `'''(2) Check devices ''' jax.devices()` `[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] ` However, if I want to try something, for example just to sample random numbers `jax.random.PRNGKey(0)` I get some weird XLA error. ` XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in  > 1 jax.random.PRNGKey(0) 23 frames /usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options, host_callbacks)    1024    TODO(sharadmv): remove this fallback when all backends allow `compile`    1025    to take in `host_callbacks` > 1026   return backend.compile(built_c, compile_options=options)    1027     1028 _ir_dump_counter = itertools.count() XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr` Can somebody please give me some advice, on what to do? The same codes which worked previously are now broken...  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info Colab  NVIDIA GPU info Thu Feb 16 16:13:21 2023        ++ ",2023-02-16T16:27:10Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/14521,You're using the wrong version of jaxlib: it requires a newer cudnn than colab has installed. See https://github.com/google/jax/issues/14464 Hope that helps!,Thank you!
917,"以下是一个github上的jax下的一个issue, 标题是(Raise a user-friendly error message if in/outfeed-based host_callback…)， 内容是 (… stuff is used with PJRT C API. Prior to this change, it would crash horribly instead. I manually tested by running the following on a Cloud TPU v48: ``` JAX_USE_PJRT_C_API_ON_TPU=1 python3 m pytest tests/host_callback_test.py tb=no ``` And verifying that all errors were the new error message. The new error message is: `host_callback functionality isn't supported with the new Cloud TPU runtime. See https://jax.readthedocs.io/en/latest/debugging/index.html and https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html for alternatives. Please file a feature request at https://github.com/google/jax/issues if none of the alternatives are sufficent.`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Raise a user-friendly error message if in/outfeed-based host_callback…,"… stuff is used with PJRT C API. Prior to this change, it would crash horribly instead. I manually tested by running the following on a Cloud TPU v48: ``` JAX_USE_PJRT_C_API_ON_TPU=1 python3 m pytest tests/host_callback_test.py tb=no ``` And verifying that all errors were the new error message. The new error message is: `host_callback functionality isn't supported with the new Cloud TPU runtime. See https://jax.readthedocs.io/en/latest/debugging/index.html and https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html for alternatives. Please file a feature request at https://github.com/google/jax/issues if none of the alternatives are sufficent.`",2023-02-16T00:02:27Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14504
645,"以下是一个github上的jax下的一个issue, 标题是(bitcast_convert_type does not support cross-size casts)， 内容是 ( Description The current definition of `jax.lax.bitcast_convert_type` requires that the source and destination types have the same size. The underlying XLA op does not have this requirement and has welldefined semantics for differentlysized types.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,bitcast_convert_type does not support cross-size casts, Description The current definition of `jax.lax.bitcast_convert_type` requires that the source and destination types have the same size. The underlying XLA op does not have this requirement and has welldefined semantics for differentlysized types.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_,2023-02-15T21:41:17Z,bug P1 (soon),closed,0,0,https://github.com/jax-ml/jax/issues/14499
406,"以下是一个github上的jax下的一个issue, 标题是(Modify JaxArrayTest.test_defragment to work on any numbers of devices)， 内容是 (Modify JaxArrayTest.test_defragment to work on any numbers of devices Also skip it when the PJRT C API is enabled, since the C API only supports auto defrag.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Modify JaxArrayTest.test_defragment to work on any numbers of devices,"Modify JaxArrayTest.test_defragment to work on any numbers of devices Also skip it when the PJRT C API is enabled, since the C API only supports auto defrag.",2023-02-15T18:45:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14492
3134,"以下是一个github上的jax下的一个issue, 标题是(Jnp.concatenate does work right)， 内容是 ( Description Hi, I'm writing a deep neural network training code with Jax, Flax, and Optax. I observed a wired thing when I use jnp. concatenate() to aggregate two matrices into a certain dimension. The operation does not yield the right results as seen in jax.debug.print(). I conducted the operation inside the loss function, which is contained in another training function decorated with jax.pmap(). Part of the code is shown below: ```python def compute_losses_non_parallel(video_init_fr, video_init_adj_fr, warped_video_fr, disp_pred_fr):    video_init_fr shape:n,fr,h,w,3    video_init_adj_fr  shape:n,fr, 2,h,w,3    warpped_video_fr  shape:n,fr, adj2,disp4,h,w,3    disp_pred_fr  shape: n,fr, h,w,4   loss_total=0   losses = {}   total_scales = 4   fr_num = video_init_fr.shape[1]   for ii in range(fr_num):     video_init = video_init_fr[:,ii]     video_init_adj = video_init_adj_fr[:,ii]     warped_video = warped_video_fr[:,ii]     disp_pred = disp_pred_fr[:,ii]     for tt in range(total_scales):       loss = 0       reproj_losses=[]       for kk in range(2):           b,h,w,1         reproj_losses.append(compute_photometric_loss(warped_video[:,kk,tt],video_init))         if tt==3 and ii==1:           jax.debug.print(""===scale4fr1adj:{b} loss:{a}"", a=compute_photometric_loss(warped_video[:,kk,tt],video_init), b=kk)        b,h,w,2       reproj_losses = jnp.concatenate(reproj_losses, axis=3)       if tt==3 and ii==1:         jax.debug.print(""===reproj_losses:{a}"", a=reproj_losses) ``` During running, part of the `scale4fr1adj:0` loss and `scale4fr1adj:1` loss are: ```python ===scale4fr1adj:0 loss:[[[[0.8525491 ]    [0.00745098]    [0.01235293]    ...    [0.01306295]    [0.00316211]    [0.0361502 ]]   [[0.02039214]    [0.01627451]    [0.01862744]    ...    [0.01470877]    [0.01998307]    [0.01764817]] ``` ```python ===scale4fr1adj:1 loss:[[[[0.4105921 ]    [0.04968597]    [0.0486871 ]    ...    [0.02002719]    [0.04886725]    [0.01558158]]   [[0.01098038]    [0.5680393 ]    [0.06133314]    ...    [0.0539965 ]    [0.0697121 ]    [0.02950328]] ``` When conducting `reproj_losses = jnp.concatenate(reproj_losses, axis=3)`, we got ```python ===reproj_losses:[[[[0.8525491  0.00745098]    [0.01235293 0.57176477]    [0.63534826 0.23562063]    ...    [0.01507517 0.01095143]    [0.02490095 0.01470877]    [0.01998307 0.01764817]]   [[0.0262745  0.0298039 ]    [0.02882352 0.02431372]    [0.01588232 0.01411766] ``` It is not a normal operation from jnp.concatenate(), but more like a reshape() operation of the loss maps. The output shapes are shown in the source code. Can you help to provide some hints on what caused this? Thank you very much!  What jax/jaxlib version are you using? jax 0.3.24; jaxlib 0.3.24+cuda11.cudnn82; flax0.4.2; optax 0.1.4  Which accelerator(s) are you using? GPU  Additional system info Linux, python 3.8.5  NVIDIA GPU info ``` Wed Feb 15 18:44:09 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jnp.concatenate does work right," Description Hi, I'm writing a deep neural network training code with Jax, Flax, and Optax. I observed a wired thing when I use jnp. concatenate() to aggregate two matrices into a certain dimension. The operation does not yield the right results as seen in jax.debug.print(). I conducted the operation inside the loss function, which is contained in another training function decorated with jax.pmap(). Part of the code is shown below: ```python def compute_losses_non_parallel(video_init_fr, video_init_adj_fr, warped_video_fr, disp_pred_fr):    video_init_fr shape:n,fr,h,w,3    video_init_adj_fr  shape:n,fr, 2,h,w,3    warpped_video_fr  shape:n,fr, adj2,disp4,h,w,3    disp_pred_fr  shape: n,fr, h,w,4   loss_total=0   losses = {}   total_scales = 4   fr_num = video_init_fr.shape[1]   for ii in range(fr_num):     video_init = video_init_fr[:,ii]     video_init_adj = video_init_adj_fr[:,ii]     warped_video = warped_video_fr[:,ii]     disp_pred = disp_pred_fr[:,ii]     for tt in range(total_scales):       loss = 0       reproj_losses=[]       for kk in range(2):           b,h,w,1         reproj_losses.append(compute_photometric_loss(warped_video[:,kk,tt],video_init))         if tt==3 and ii==1:           jax.debug.print(""===scale4fr1adj:{b} loss:{a}"", a=compute_photometric_loss(warped_video[:,kk,tt],video_init), b=kk)        b,h,w,2       reproj_losses = jnp.concatenate(reproj_losses, axis=3)       if tt==3 and ii==1:         jax.debug.print(""===reproj_losses:{a}"", a=reproj_losses) ``` During running, part of the `scale4fr1adj:0` loss and `scale4fr1adj:1` loss are: ```python ===scale4fr1adj:0 loss:[[[[0.8525491 ]    [0.00745098]    [0.01235293]    ...    [0.01306295]    [0.00316211]    [0.0361502 ]]   [[0.02039214]    [0.01627451]    [0.01862744]    ...    [0.01470877]    [0.01998307]    [0.01764817]] ``` ```python ===scale4fr1adj:1 loss:[[[[0.4105921 ]    [0.04968597]    [0.0486871 ]    ...    [0.02002719]    [0.04886725]    [0.01558158]]   [[0.01098038]    [0.5680393 ]    [0.06133314]    ...    [0.0539965 ]    [0.0697121 ]    [0.02950328]] ``` When conducting `reproj_losses = jnp.concatenate(reproj_losses, axis=3)`, we got ```python ===reproj_losses:[[[[0.8525491  0.00745098]    [0.01235293 0.57176477]    [0.63534826 0.23562063]    ...    [0.01507517 0.01095143]    [0.02490095 0.01470877]    [0.01998307 0.01764817]]   [[0.0262745  0.0298039 ]    [0.02882352 0.02431372]    [0.01588232 0.01411766] ``` It is not a normal operation from jnp.concatenate(), but more like a reshape() operation of the loss maps. The output shapes are shown in the source code. Can you help to provide some hints on what caused this? Thank you very much!  What jax/jaxlib version are you using? jax 0.3.24; jaxlib 0.3.24+cuda11.cudnn82; flax0.4.2; optax 0.1.4  Which accelerator(s) are you using? GPU  Additional system info Linux, python 3.8.5  NVIDIA GPU info ``` Wed Feb 15 18:44:09 2023        ++  ++ ```",2023-02-15T17:44:58Z,needs info,closed,0,2,https://github.com/jax-ml/jax/issues/14490,Hi  thanks for the report. I'm having trouble understanding what your expected output is. Can you provide a minimal reproducible example showing the inputs and output of `jnp.concatenate` and how it differs from what you expect?,Closing for lack of information. Please let us know if you're still having this issue!
445,"以下是一个github上的jax下的一个issue, 标题是([PJRT:C] Add PJRT_Client_Defragment)， 内容是 ([PJRT:C] Add PJRT_Client_Defragment Also adds some helper functions: * PJRT_RETURN_STATUS_IF_ERROR (macro) * AwaitEvent * DefragmentClient * GetHostBuffer [JAX] Modify JaxArrayTest.test_defragment to work on any numbers of devices)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[PJRT:C] Add PJRT_Client_Defragment,[PJRT:C] Add PJRT_Client_Defragment Also adds some helper functions: * PJRT_RETURN_STATUS_IF_ERROR (macro) * AwaitEvent * DefragmentClient * GetHostBuffer [JAX] Modify JaxArrayTest.test_defragment to work on any numbers of devices,2023-02-15T01:16:01Z,,closed,0,1,https://github.com/jax-ml/jax/issues/14478,Closing Copybara created PR due to inactivity
1280,"以下是一个github上的jax下的一个issue, 标题是(arange XlaRuntimeError v0.4.3 colab gpu)， 内容是 ( Description jnp.arange causes XlaRuntimeError. ``` !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` ``` !nvidiasmi ``` ``` import jax.numpy as jnp ``` ``` x = jnp.arange(33) ```  XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in  > 1 x = jnp.arange(33) 16 frames /usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options, host_callbacks)    1024    TODO(sharadmv): remove this fallback when all backends allow `compile`    1025    to take in `host_callbacks` > 1026   return backend.compile(built_c, compile_options=options)    1027     1028 _ir_dump_counter = itertools.count() XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr  What jax/jaxlib version are you using? v0.4.3  Which accelerator(s) are you using? GPU  Additional system info Google Colab  NVIDIA GPU info Tue Feb 14 16:45:09 2023        ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,arange XlaRuntimeError v0.4.3 colab gpu," Description jnp.arange causes XlaRuntimeError. ``` !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` ``` !nvidiasmi ``` ``` import jax.numpy as jnp ``` ``` x = jnp.arange(33) ```  XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in  > 1 x = jnp.arange(33) 16 frames /usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options, host_callbacks)    1024    TODO(sharadmv): remove this fallback when all backends allow `compile`    1025    to take in `host_callbacks` > 1026   return backend.compile(built_c, compile_options=options)    1027     1028 _ir_dump_counter = itertools.count() XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr  What jax/jaxlib version are you using? v0.4.3  Which accelerator(s) are you using? GPU  Additional system info Google Colab  NVIDIA GPU info Tue Feb 14 16:45:09 2023        ++  ++",2023-02-14T16:51:21Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/14470,Thanks for the question! This looks like an issue of mismatched CUDA versions; see https://github.com/google/jax/issues/14464 for a similar answer.,Closing as a duplicate of CC(XLA Error when running `rng = jax.random.PRNGKey(0)` in google colab) ; the fix is to install the correct version of jaxlib.,"Hi, I'm seeing the exact same error, on WSL with cuda 11.8. I tried both jaxlib versions. I also tried updating my cuda to 12.0, but see the same error. Attempting with fresh install of https://github.com/NTT123/a0jax if it helps to repro ``` (venv) charlesf:~/workspace/a0jax$ python train_agent.py weightdecay=1e2 numiterations=3 Cores: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)] 20230214 22:00:43.393817: E external/org_tensorflow/tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20230214 22:00:43.394357: E external/org_tensorflow/tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr  *** Begin stack trace ***         _PyObject_MakeTpCall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyObject_FastCallDictTstate         _PyObject_Call_Prepend         _PyObject_MakeTpCall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         PyObject_Call         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         _PyFunction_Vectorcall         _PyEval_EvalFrameDefault         PyEval_EvalCode         _PyRun_SimpleFileObject         _PyRun_AnyFileObject         Py_RunMain         Py_BytesMain         __libc_start_main         _start *** End stack trace *** Traceback (most recent call last):   File ""/home/charlesf/workspace/a0jax/train_agent.py"", line 330, in      fire.Fire(train)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/fire/core.py"", line 141, in Fire     component_trace = _Fire(component, args, parsed_flag_args, context, name)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/fire/core.py"", line 475, in _Fire     component, remaining_args = _CallAndUpdateTrace(   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/fire/core.py"", line 691, in _CallAndUpdateTrace     component = fn(*varargs, **kwargs)   File ""/home/charlesf/workspace/a0jax/train_agent.py"", line 223, in train     env = import_class(game_class)()   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/pax/_src/core/safe_module.py"", line 23, in __call__     cls.__init__(module, *args, **kwargs)   File ""/home/charlesf/workspace/a0jax/games/connect_two_game.py"", line 52, in __init__     self.reset()   File ""/home/charlesf/workspace/a0jax/games/connect_two_game.py"", line 67, in reset     self.board = jnp.zeros((4,), dtype=jnp.int32)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2126, in zeros     return lax.full(shape, 0, _jnp_dtype(dtype))   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 1211, in full     return broadcast(fill_value, shape)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 790, in broadcast     return broadcast_in_dim(operand, tuple(sizes) + np.shape(operand), dims)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/lax/lax.py"", line 818, in broadcast_in_dim     return broadcast_in_dim_p.bind(   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/core.py"", line 343, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/core.py"", line 346, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/core.py"", line 728, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 122, in apply_primitive     compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/util.py"", line 253, in wrapper     return cached(config._trace_context(), *args, **kwargs)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/util.py"", line 246, in cached     return f(*args, **kwargs)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 201, in xla_primitive_callable     compiled = _xla_callable_uncached(lu.wrap_init(prim_fun), device, None,   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 354, in _xla_callable_uncached     return computation.compile(_allow_propagation_to_outputs=allow_prop).unsafe_call   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 3203, in compile     executable = self._compile_unloaded(   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 3174, in _compile_unloaded     return UnloadedMeshExecutable.from_hlo(   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 3456, in from_hlo     xla_executable = dispatch.compile_or_get_cached(   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1081, in compile_or_get_cached     return backend_compile(backend, serialized_computation, compile_options,   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 1026, in backend_compile     return backend.compile(built_c, compile_options=options) jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr  (venv) charlesf:~/workspace/a0jax$ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052022 NVIDIA Corporation Built on Mon_Oct_24_19:12:58_PDT_2022 Cuda compilation tools, release 12.0, V12.0.76 Build cuda_12.0.r12.0/compiler.31968024_0 (venv) charlesf:~/workspace/a0jax$ pip show jaxlib Name: jaxlib Version: 0.4.3+cuda11.cudnn86 Summary: XLA library for JAX Homepage: https://github.com/google/jax Author: JAX team Authoremail: jaxdev.com License: Apache2.0 Location: /home/charlesf/workspace/a0jax/venv/lib/python3.10/sitepackages Requires: numpy, scipy Requiredby: chex, mctx, optax (venv) charlesf:~/workspace/a0jax$  ```","I don't get this error if i install 0.4.2 of jax[cuda] with: ``` pip install upgrade ""jax[cuda]==0.4.2"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ```",> Thanks for the question! This looks like an issue of mismatched CUDA versions; see CC(XLA Error when running `rng = jax.random.PRNGKey(0)` in google colab) for a similar answer. Thank you!, I suspect you don't have the correct version of CuDNN installed.
8068,"以下是一个github上的jax下的一个issue, 标题是(Linking errors when building on windows)， 内容是 ( Description  Follow the fix in https://github.com/google/jax/issues/14369issuecomment1429839230 There will still be linking error as in https://github.com/google/jax/issues/14369issuecomment1428121867 > If all those fixed, you will get another link error as follows: >  > ``` > LINK : warning LNK4044: unrecognized option \'/lm\'; ignored > ffi.lib(ffi.obj) : error LNK2005: ""struct XLA_FFI_Stream * __cdecl xla::runtime::ffi::GetXlaFfiStream(class xla::runtime::PtrMapByType const *,class xla::runtime::DiagnosticEngine const *)"" (?GetXlaFfiStream@@?$PtrMapByType@@$0BA@@) already defined in executable.lib(executable.obj) >    Creating library bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.lib and object bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.exp > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'send_recv.lib(send_recv.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'llvm_gpu_backend.lib(gpu_backend_lib.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'transpose.lib(transpose.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'bfc_allocator.lib(bfc_allocator.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'nvptx_compiler_impl.lib(nvptx_compiler.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_compiler.lib(gpu_compiler.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'cpu_runtime.lib(cpu_runtime.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_helpers.lib(gpu_helpers.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjrt_stream_executor_client.lib(pjrt_stream_executor_client.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'local_device_state.lib(local_device_state.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'profiler.lib(profiler.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'outfeed_receiver.lib(outfeed_receiver.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'py_client.lib(py_values.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'tfrt_cpu_pjrt_client.lib(tfrt_cpu_pjrt_client.obj)\' > LINK : warning LNK4217: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'allocator_registry_impl.lo.lib(cpu_allocator_impl.obj)\' in function \'""public: static void __cdecl tsl::profiler::TraceMe::InstantActivity,1>(class  &&,int)"" (??$InstantActivity@@$00@$$QEAV@)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pmap_lib.lib(pmap_lib.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjit.lib(pjit.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'jax_jit.lib(jax_jit.obj)\' > LINK : warning LNK4217: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\' in function \'""public: __cdecl tsl::profiler::ScopedAnnotationT::ScopedAnnotationT >(class )"" (??$?0V@@@?$ScopedAnnotationT@$0A@@@@)\' > LINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(sequential_thunk.obj)\' > LINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'tracing.lib(tracing.obj)\' > bazelout\\x64_windowsopt\\bin\\external\\org_tensorflow\\tensorflow\\compiler\\xla\\python\\xla_extension.so : fatal error LNK1169: one or more multiply defined symbols found > ```  Cause of multiply defined `tsl::profiler::internal::g_annotation_enabled` `TF_COMPILE_LIBRARY` is not properly defined https://github.com/tensorflow/tensorflow/blob/959d1b144dc03fbda586f8a60dd4c117025e6c18/tensorflow/tsl/platform/macros.hL61L69 which if further caused by  parameter `is_external` of macro `get_win_copts` is not properly propagated in, in https://github.com/tensorflow/tensorflow/blob/959d1b144dc03fbda586f8a60dd4c117025e6c18/tensorflow/tensorflow.bzl  Fortunately, we can `copts=/DTF_COMPILE_LIBRARY` to workaround this problem.  Cause of already defined `xla::runtime::ffi::GetXlaFfiStream` `xla::runtime::ffi::GetXlaFfiStream` is first defined as weak symbol in `ffi.cc` then defined as normal symbol in `executable.cc` and msvc does not support weak symbol. As for dirty fix, removing the definition in `ffi.cc` and replacing it with decl will allow the linking to pass. After this, we can build a usable jaxlib whl  What jax/jaxlib version are you using? jaxlib v0.4.3  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Linking errors when building on windows," Description  Follow the fix in https://github.com/google/jax/issues/14369issuecomment1429839230 There will still be linking error as in https://github.com/google/jax/issues/14369issuecomment1428121867 > If all those fixed, you will get another link error as follows: >  > ``` > LINK : warning LNK4044: unrecognized option \'/lm\'; ignored > ffi.lib(ffi.obj) : error LNK2005: ""struct XLA_FFI_Stream * __cdecl xla::runtime::ffi::GetXlaFfiStream(class xla::runtime::PtrMapByType const *,class xla::runtime::DiagnosticEngine const *)"" (?GetXlaFfiStream@@?$PtrMapByType@@$0BA@@) already defined in executable.lib(executable.obj) >    Creating library bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.lib and object bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.exp > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'send_recv.lib(send_recv.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'llvm_gpu_backend.lib(gpu_backend_lib.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'transpose.lib(transpose.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'bfc_allocator.lib(bfc_allocator.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'nvptx_compiler_impl.lib(nvptx_compiler.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_compiler.lib(gpu_compiler.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'cpu_runtime.lib(cpu_runtime.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_helpers.lib(gpu_helpers.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjrt_stream_executor_client.lib(pjrt_stream_executor_client.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'local_device_state.lib(local_device_state.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'profiler.lib(profiler.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'outfeed_receiver.lib(outfeed_receiver.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'py_client.lib(py_values.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'tfrt_cpu_pjrt_client.lib(tfrt_cpu_pjrt_client.obj)\' > LINK : warning LNK4217: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'allocator_registry_impl.lo.lib(cpu_allocator_impl.obj)\' in function \'""public: static void __cdecl tsl::profiler::TraceMe::InstantActivity,1>(class  &&,int)"" (??$InstantActivity@@$00@$$QEAV@)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pmap_lib.lib(pmap_lib.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjit.lib(pjit.obj)\' > LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'jax_jit.lib(jax_jit.obj)\' > LINK : warning LNK4217: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\' in function \'""public: __cdecl tsl::profiler::ScopedAnnotationT::ScopedAnnotationT >(class )"" (??$?0V@@@?$ScopedAnnotationT@$0A@@@@)\' > LINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(sequential_thunk.obj)\' > LINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'tracing.lib(tracing.obj)\' > bazelout\\x64_windowsopt\\bin\\external\\org_tensorflow\\tensorflow\\compiler\\xla\\python\\xla_extension.so : fatal error LNK1169: one or more multiply defined symbols found > ```  Cause of multiply defined `tsl::profiler::internal::g_annotation_enabled` `TF_COMPILE_LIBRARY` is not properly defined https://github.com/tensorflow/tensorflow/blob/959d1b144dc03fbda586f8a60dd4c117025e6c18/tensorflow/tsl/platform/macros.hL61L69 which if further caused by  parameter `is_external` of macro `get_win_copts` is not properly propagated in, in https://github.com/tensorflow/tensorflow/blob/959d1b144dc03fbda586f8a60dd4c117025e6c18/tensorflow/tensorflow.bzl  Fortunately, we can `copts=/DTF_COMPILE_LIBRARY` to workaround this problem.  Cause of already defined `xla::runtime::ffi::GetXlaFfiStream` `xla::runtime::ffi::GetXlaFfiStream` is first defined as weak symbol in `ffi.cc` then defined as normal symbol in `executable.cc` and msvc does not support weak symbol. As for dirty fix, removing the definition in `ffi.cc` and replacing it with decl will allow the linking to pass. After this, we can build a usable jaxlib whl  What jax/jaxlib version are you using? jaxlib v0.4.3  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-02-14T14:54:06Z,bug Windows,closed,0,7,https://github.com/jax-ml/jax/issues/14466,Seems like Triton has been changed such that the advice above is no longer valid (build will still fail for similar reasons).,"Are both of these issues still current, or just the `GetXlaFfiStream` issue?", do you have suggestions on how we might avoid the weak symbol here? MSVC apparently does not support weak symbols.,Still current. But the first one is mainly configuration issue. So only the second one need to be address ATM.,The weak symbol problem has been resolved on openxla/xla latest main.,"Is this issue still a problem? (I know it isn't an CPU, because the new Windows CPU CI build is mostly happy: https://github.com/google/jax/actions/workflows/windows_ci.yml )","> The weak symbol problem has been resolved on openxla/xla latest main. That is, the second is fixed in https://github.com/openxla/xla/commit/e634d4ab1067445c0f89f4b2bbe0bafaf0400051 The first one is still there, but as it can be workaround from outside, feel free to close this issue."
1661,"以下是一个github上的jax下的一个issue, 标题是(XLA Error when running `rng = jax.random.PRNGKey(0)` in google colab)， 内容是 ( Description I am getting an XLA weird error when running  ```python rng = jax.random.PRNGKey(0) ``` in Google colab. You can check the error here. The full code is ```python !pip install upgrade pip !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install flax !pip install optax import jax rng = jax.random.PRNGKey(0) ``` and returns ```  XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in  > 1 rng = jax.random.PRNGKey(0)       2 rng, init_rng = jax.random.split(rng) 23 frames /usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options, host_callbacks)    1024    TODO(sharadmv): remove this fallback when all backends allow `compile`    1025    to take in `host_callbacks` > 1026   return backend.compile(built_c, compile_options=options)    1027     1028 _ir_dump_counter = itertools.count() XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr  ``` Also reported here https://github.com/google/jax/discussions/14463discussion4856887  What jax/jaxlib version are you using? jax == 0.4.3; jaxlib == 0.4.3+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Google colab  NVIDIA GPU info ``` Tue Feb 14 11:19:59 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,XLA Error when running `rng = jax.random.PRNGKey(0)` in google colab," Description I am getting an XLA weird error when running  ```python rng = jax.random.PRNGKey(0) ``` in Google colab. You can check the error here. The full code is ```python !pip install upgrade pip !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install flax !pip install optax import jax rng = jax.random.PRNGKey(0) ``` and returns ```  XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in  > 1 rng = jax.random.PRNGKey(0)       2 rng, init_rng = jax.random.split(rng) 23 frames /usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options, host_callbacks)    1024    TODO(sharadmv): remove this fallback when all backends allow `compile`    1025    to take in `host_callbacks` > 1026   return backend.compile(built_c, compile_options=options)    1027     1028 _ir_dump_counter = itertools.count() XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr  ``` Also reported here https://github.com/google/jax/discussions/14463discussion4856887  What jax/jaxlib version are you using? jax == 0.4.3; jaxlib == 0.4.3+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Google colab  NVIDIA GPU info ``` Tue Feb 14 11:19:59 2023        ++  ++ ```",2023-02-14T11:22:45Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/14464,"That's admittedly a bad error message, but it's happening because of a version incompatibility. Colab has CuDNN 8.4, but the version of jaxlib you installed requires CuDNN 8.6 or newer. Try: ``` !pip install upgrade ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ``` See: https://github.com/google/jaxpipinstallationgpucuda"
1423,"以下是一个github上的jax下的一个issue, 标题是(Track back from jaxlib version to source code control)， 内容是 (I'm trying to set up a CI flow that involves installing jax/jaxlib nightly and then also being able to checkout a corresponding commit in the source control system (i.e. to run more tests or build against the same XLA backend). As an example, we do the following with IREE: ``` $ python c ""from iree.compiler import version; print(version.VERSION, version.REVISIONS)"" 20220930.282 {'IREE': 'a80a1a9a07a4b16f58f3889fb836d8ba02dcd281'} ``` This lets us track back (even arbitrary nightlies without tags, etc) to the source control system, and it is used by various integrators to anchor their flows. I couldn't find a corresponding linkage in jaxlib's version.py. I'm not familiar with how Jax assembles its release artifacts. If you have pointers, I could try to add something. In IREE, our setup.py files probe for a special nonchecked in version_info.json (https://github.com/ireeorg/iree/blob/main/runtime/setup.pyL110) and use this to generate the version.py. When running from a git repo, the revisions are populated on the fly, but source tarballs can just include it all in the version_info.json. There are a lot of ways to do it. Just mentioning that by reference in case if helpful.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Track back from jaxlib version to source code control,"I'm trying to set up a CI flow that involves installing jax/jaxlib nightly and then also being able to checkout a corresponding commit in the source control system (i.e. to run more tests or build against the same XLA backend). As an example, we do the following with IREE: ``` $ python c ""from iree.compiler import version; print(version.VERSION, version.REVISIONS)"" 20220930.282 {'IREE': 'a80a1a9a07a4b16f58f3889fb836d8ba02dcd281'} ``` This lets us track back (even arbitrary nightlies without tags, etc) to the source control system, and it is used by various integrators to anchor their flows. I couldn't find a corresponding linkage in jaxlib's version.py. I'm not familiar with how Jax assembles its release artifacts. If you have pointers, I could try to add something. In IREE, our setup.py files probe for a special nonchecked in version_info.json (https://github.com/ireeorg/iree/blob/main/runtime/setup.pyL110) and use this to generate the version.py. When running from a git repo, the revisions are populated on the fly, but source tarballs can just include it all in the version_info.json. There are a lot of ways to do it. Just mentioning that by reference in case if helpful.",2023-02-14T04:59:06Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/14461,"To give a little bit of background about the flow I am using, my project has a `sync.py` that you can use to sync to different development binaries in order to limit how much needs to be built. If you run `sync.py nightly`, it installs the ireecompiler nightly and then uses the embedded revision to clone/check out the matching revision of the runtime code, which it has a source dependency on (there is no source dependency on the compiler). I wanted to do the same thing with jaxlib nightlies: Install the latest jaxlib nightly, get the jax commit and derive the corresponding tensorflow (or xla) commit to clone/checkout those repos as well. Then everything is set up to take additional source deps suitable for building a runtime, run tests, etc. Doing it this way reduces what needs to be built to construct derived projects to ~hundreds of files, reducing development iteration time and CI burden. It works pretty well for daisy chaining against IREE in this way and I figured could also help for the jax/xla side of the deps."
761,"以下是一个github上的jax下的一个issue, 标题是(possible memory leak)， 内容是 ( Description I' m experiencing  very large memory usage in forward mode . I'm studying AD and wrote a small script to compare memory usage in forward mode vs reverse mode. I was expecting less usage in forward mode, but I found the opposite and in forward mode my script reach very soon memory exaustion. Here is the script https://gist.github.com/frhack/2436e2daf6fbc9d30bbcac62ca35ee9a thanks  What jax/jaxlib version are you using? 0.4.3/0.4.3  Which accelerator(s) are you using? CPU  Additional system info debian 11/ core i7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,possible memory leak," Description I' m experiencing  very large memory usage in forward mode . I'm studying AD and wrote a small script to compare memory usage in forward mode vs reverse mode. I was expecting less usage in forward mode, but I found the opposite and in forward mode my script reach very soon memory exaustion. Here is the script https://gist.github.com/frhack/2436e2daf6fbc9d30bbcac62ca35ee9a thanks  What jax/jaxlib version are you using? 0.4.3/0.4.3  Which accelerator(s) are you using? CPU  Additional system info debian 11/ core i7  NVIDIA GPU info _No response_",2023-02-13T19:56:45Z,question,open,0,1,https://github.com/jax-ml/jax/issues/14447,"Thanks for the question! The script compares `jacfwd` with `grad`, but that comparison is a bit applestooranges: `jacfwd` has to push forward an entire batch of vectors, whereas grad only pulls back one. Based on your description here, I'm guessing you want to compare `jax.jvp` with `jax.grad`. For more on `jvp` vs `jacfwd`, see the autodiff cookbook. The memory difference is most pronounced with deeper computations than the one in your script, as `grad` requires memory which scales with the computation depth while `jvp` does not."
785,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Allow functions with unused dimension variables in native lowering)， 内容是 (In the jax2tf shape polymorphism with native lowering we raise an error if we try to convert a function whose arguments have dimension variables if those arguments are not used in the computation. This is because the unused arguments are dropped during lowering, and then there are no arguments from which to derive the value of the dimension variables at invocation time. These errors are especially annoying for gradient functions that actually do not use the dimension variables. We should not give an error in that case.  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Allow functions with unused dimension variables in native lowering,"In the jax2tf shape polymorphism with native lowering we raise an error if we try to convert a function whose arguments have dimension variables if those arguments are not used in the computation. This is because the unused arguments are dropped during lowering, and then there are no arguments from which to derive the value of the dimension variables at invocation time. These errors are especially annoying for gradient functions that actually do not use the dimension variables. We should not give an error in that case.  ",2023-02-13T09:36:33Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/14437,Fixed in CC([shape_poly] Fixed bug with dimension variables in unused args) 
786,"以下是一个github上的jax下的一个issue, 标题是(A few developer workflow enhancements for working with jaxlib.)， 内容是 (It seems to me that jaxlib development must be mostly happening on CI, because some basics are pretty essential. Here are a few things I've been typing/carrying for a while in my flow: * Add .bazelrc.user to .gitignore so it doesn't accidentally get checked in. * Add configs for 'debug_symbols' and 'debug' that make some things minimally workable under a debugger (or to get backtraces, etc). * Add `forcereinstall` to the copy/paste command to update a built jaxlib wheel (without this, if you are iterating, it fairly quietly does nothing).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,A few developer workflow enhancements for working with jaxlib.,"It seems to me that jaxlib development must be mostly happening on CI, because some basics are pretty essential. Here are a few things I've been typing/carrying for a while in my flow: * Add .bazelrc.user to .gitignore so it doesn't accidentally get checked in. * Add configs for 'debug_symbols' and 'debug' that make some things minimally workable under a debugger (or to get backtraces, etc). * Add `forcereinstall` to the copy/paste command to update a built jaxlib wheel (without this, if you are iterating, it fairly quietly does nothing).",2023-02-11T05:07:01Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14424
1897,"以下是一个github上的jax下的一个issue, 标题是(Jax at head infinite loops printing an array)， 内容是 ( Description Tensorflow checked out commit: d06d16ef29767129aeaaefd007481aeb759e5400 Jax: dc6bf9b725fcdd96f8152d6d3a1ce3f0c14d9ced This simple test program: ``` import jax  Do once and print. a = jax.numpy.asarray([1, 2, 3, 4, 5, 6, 7, 8, 9]) b = a for i in range(100):   b = jax.numpy.asarray([i]) * a + b print(b) ``` Fails with: ``` ...   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 351, in __array__     return np.asarray(self._value, dtype=dtype)   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 497, in _value     self._npy_value = np.asarray(self._arrays[0])   type: ignore   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 351, in __array__     return np.asarray(self._value, dtype=dtype)   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 496, in _value     if self.is_fully_replicated:   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 325, in is_fully_replicated     return self.shape == self._arrays[0].shape   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 196, in shape     return self.aval.shape RecursionError: maximum recursion depth exceeded while calling a Python object ``` I can reproduce this on all backends, including CPU.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax at head infinite loops printing an array," Description Tensorflow checked out commit: d06d16ef29767129aeaaefd007481aeb759e5400 Jax: dc6bf9b725fcdd96f8152d6d3a1ce3f0c14d9ced This simple test program: ``` import jax  Do once and print. a = jax.numpy.asarray([1, 2, 3, 4, 5, 6, 7, 8, 9]) b = a for i in range(100):   b = jax.numpy.asarray([i]) * a + b print(b) ``` Fails with: ``` ...   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 351, in __array__     return np.asarray(self._value, dtype=dtype)   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 497, in _value     self._npy_value = np.asarray(self._arrays[0])   type: ignore   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 351, in __array__     return np.asarray(self._value, dtype=dtype)   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 496, in _value     if self.is_fully_replicated:   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 325, in is_fully_replicated     return self.shape == self._arrays[0].shape   File ""/usr/local/google/home/laurenzo/src/openxlapjrtplugin/external/.clones/jax/jax/_src/array.py"", line 196, in shape     return self.aval.shape RecursionError: maximum recursion depth exceeded while calling a Python object ``` I can reproduce this on all backends, including CPU.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-02-11T04:24:00Z,bug,open,0,15,https://github.com/jax-ml/jax/issues/14422,"Hey  ! It works on my machine. That is, both at https://github.com/google/jax/commit/dc6bf9b725fcdd96f8152d6d3a1ce3f0c14d9ced and at current github HEAD 1bdcd5e13, using pypi's latest jaxlib 0.4.3 (but not a local build), that script doesn't fail. Did you build jaxlib yourself, or install it? Can you try it with pypi's latest? (I think: `pip install upgrade jaxlib`.) I wonder if it's a jax/jaxlib version incompatibility issue...","> I wonder if it's a jax/jaxlib version incompatibility issue... I suspect it is. I'm building with a jaxlib built from head. Working on some features that need the latest, and thought it was something I did, so I reproduced it with a clean build of at head jax and tensorflow repos and a custom build jaxlib. There isn't a problem with what is on pypi and I haven't fully bisected but this just started recently.", found that cl/508789203 is a rollback in progress!,I think this PR rollback should fix it: https://github.com/google/jax/pull/14421,"Not so sure: Just rebased to head (61da7811742d69b3d0e33c211418d1d131cf475b) and it appears to be working. But if I cherrypick that rollback, the issue returns. So something fixed it in the last few hours but it looks like the rollback might rebreak it :) In any case, going to let it be for the weekend and hope it stabilizes...",I think it is a combination of jax at head and TF at head (jaxlib). We need both changes to be reverted to get the previous behavior. We want to rollback that change anyways because of other failures in google. Let's wait for the rollback to get in and then try again? (Or i can by building jaxlib from scratch),(I'm not blocked now and assume that you all can get it past the oscillation... the repro is pretty straightforward),Rollback is submitted and is OSS'd JAX commit: https://github.com/google/jax/commit/9316188b3af802c136e78bc1908a46992633e1f0 TF commit: https://github.com/tensorflow/tensorflow/commit/1be8ca3a774653e69831fe82cf4b140062dcfcf6 I am building and reproing now,"The rollback fixes it Command I used to build jaxlib: `python3 build/build.py bazel_options=override_repository=org_tensorflow=/home/yashkatariya/tensorflow`. The tensorflow directory was cloned from master branch of TF. Same for JAX (main branch of JAX) ``` >>> import jax >>> jax.devices() No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) [CpuDevice(id=0)] >>> jax.__version__ '0.4.4' >>> import jaxlib >>> jaxlib.__version__ '0.4.4' >>> a = jax.numpy.asarray([1, 2, 3, 4, 5, 6, 7, 8, 9]) >>> b = a >>> for i in range(100): ...     b = jax.numpy.asarray([i]) * a + b ... >>> print(b) [ 4951  9902 14853 19804 24755 29706 34657 39608 44559] >>> ```", feel free to try it out at HEAD of both jax and TF and see if it is resolved in your env too :),"Thanks! I'll check tomorrow. FWIW, here is the environment I am verifying: https://github.com/openxla/openxlapjrtplugin That should work tomorrow (after a nightly dep roll) with just a local patch to work around: https://github.com/openxla/xla/issues/1237 (shameless plug to get some attention on that too :) ) I'll need to rebase my patch train, but I expect what you've done has it working.","Hi Stella, can you expound more on what is going on here? Does this mean that you're using the newest python but an old build of the jaxlib extention? Are you using a pjrt version of iree?","Sure. I checked out the jax repo from head then synced the backing tensorflow to nearhead and built a jaxlib per https://jax.readthedocs.io/en/latest/developer.htmlbuildingjaxlibfromsourcewithamodifiedtensorflowrepository. This was because I am working with some folks who have landed changes to jaxlib/tensorflow that I need in order to make progress. Due to this, I'm often developing with a nearhead jaxlib. I am doing this to support the PJRT plugin, which is where I noticed the issue. However, this bug report is just the result of running the script I included against the default XLA CPU backend, so it is not related to anything I am doing.","Right, but we have a tensorflow/compiler/xla/python/xla_client.py with a _version number which allows us to synchronize changes between jax and jaxlib. If that file gets out of sync with the compiled blob, then you could get problems like this.","Is there a written procedure for upgrading jaxlib? When developing nearhead on the OSS side, it seems like step 1 might be to do that locally and then work further. All I can see is this: https://github.com/google/jax/blob/c49af18b9b6a560c1d6cbc516e5c800ad4fb2ca1/WORKSPACEL3 Which implies that we always have our version numbers and such in a state so that just bumping the tf hash should be sufficient (modulo bugs and other stuff, I assume). Let's say that instead of encountering this issue while trying to accomplish my own objective, my objective was to upgrade the tensorflow hash. I would have updated the pin as at the top of the file and then encountered this error, which presumably would have needed to be fixed prior to actually bumping the pin."
437,"以下是一个github上的jax下的一个issue, 标题是([XLA:Python] Fix overly pessimistic handling of singleton dimensions in dlpack code.)， 内容是 ([XLA:Python] Fix overly pessimistic handling of singleton dimensions in dlpack code. Requires an accompanying jaxlib change. Fixes https://github.com/google/jax/issues/14399)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[XLA:Python] Fix overly pessimistic handling of singleton dimensions in dlpack code.,[XLA:Python] Fix overly pessimistic handling of singleton dimensions in dlpack code. Requires an accompanying jaxlib change. Fixes https://github.com/google/jax/issues/14399,2023-02-10T22:06:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14414
4544,"以下是一个github上的jax下的一个issue, 标题是(Google Cloud ""Run a calculation on a Cloud TPU VM by using JAX"" issue)， 内容是 ( Description I am executing the commands outline in Run a calculation on a Cloud TPU VM by using JAX in the Google Cloud Docs. I'm following it verbatim, only difference is I have to run.  ``` pip install tensorflow pip install tensorflow_datasets ``` in order to get  ``` cd flax/examples/mnist python3 main.py workdir=/tmp/mnist \ config=configs/default.py \ config.learning_rate=0.05 \ config.num_epochs=5 ``` to work. **The issue**: When I execute the flax example training script I get the following warnings along with an error message. ``` ~/flax/examples/mnist$ python3 main.py workdir=/tmp/mnist config=configs/default.py config.learning_rate=0.05 config.num_epochs=5 20230210 17:09:24.848077: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib 20230210 17:09:25.454669: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib 20230210 17:09:25.454771: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib 20230210 17:09:25.454786: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 20230210 17:09:27.062680: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib 20230210 17:09:27.062728: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303) ``` My guess is that jax TPU does not use CUDA, but I want to know what is throwing the error. Does tensorflow think I'm trying to use GPU?  Note that the training does proceed as expected just much slower when compared to the timestamps in the documentation ```  theres I0726 00:57:51.274136 139632684678208 train.py:146] epoch:  1, train_loss: 0.2423, train_accuracy: 92.96, test_loss: 0.0629, test_accuracy: 97.98 I0726 00:57:52.741929 139632684678208 train.py:146] epoch:  2, train_loss: 0.0594, train_accuracy: 98.15, test_loss: 0.0434, test_accuracy: 98.61 I0726 00:57:54.149238 139632684678208 train.py:146] epoch:  3, train_loss: 0.0417, train_accuracy: 98.73, test_loss: 0.0307, test_accuracy: 98.98 I0726 00:57:55.570881 139632684678208 train.py:146] epoch:  4, train_loss: 0.0309, train_accuracy: 99.03, test_loss: 0.0273, test_accuracy: 99.13 I0726 00:57:56.937045 139632684678208 train.py:146] epoch:  5, train_loss: 0.0251, train_accuracy: 99.21, test_loss: 0.0270, test_accuracy: 99.16  mine  I0210 17:09:32.381908 139853706177600 logging_logger.py:49] Constructing tf.data.Dataset mnist for split test, from /home/will/tensorflow_datasets/mnist/3.0.1 I0210 17:09:47.176413 139853706177600 train.py:146] epoch:  1, train_loss: 0.2423, train_accuracy: 92.96, test_loss: 0.0629, test_accuracy: 97.98 I0210 17:10:30.636702 139853706177600 train.py:146] epoch:  2, train_loss: 0.0594, train_accuracy: 98.15, test_loss: 0.0434, test_accuracy: 98.61 I0210 17:11:16.056597 139853706177600 train.py:146] epoch:  3, train_loss: 0.0417, train_accuracy: 98.73, test_loss: 0.0307, test_accuracy: 98.98 I0210 17:12:01.455775 139853706177600 train.py:146] epoch:  4, train_loss: 0.0309, train_accuracy: 99.03, test_loss: 0.0273, test_accuracy: 99.13 I0210 17:12:43.841351 139853706177600 train.py:146] epoch:  5, train_loss: 0.0251, train_accuracy: 99.21, test_loss: 0.0270, test_accuracy: 99.16 ``` ~1 second vs. ~45 seconds per epoch Any help would be greatly appreciated!  What jax/jaxlib version are you using? 0.4.3  Which accelerator(s) are you using? TPU  Additional system info v3  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Google Cloud ""Run a calculation on a Cloud TPU VM by using JAX"" issue"," Description I am executing the commands outline in Run a calculation on a Cloud TPU VM by using JAX in the Google Cloud Docs. I'm following it verbatim, only difference is I have to run.  ``` pip install tensorflow pip install tensorflow_datasets ``` in order to get  ``` cd flax/examples/mnist python3 main.py workdir=/tmp/mnist \ config=configs/default.py \ config.learning_rate=0.05 \ config.num_epochs=5 ``` to work. **The issue**: When I execute the flax example training script I get the following warnings along with an error message. ``` ~/flax/examples/mnist$ python3 main.py workdir=/tmp/mnist config=configs/default.py config.learning_rate=0.05 config.num_epochs=5 20230210 17:09:24.848077: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib 20230210 17:09:25.454669: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib 20230210 17:09:25.454771: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib 20230210 17:09:25.454786: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. 20230210 17:09:27.062680: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib 20230210 17:09:27.062728: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303) ``` My guess is that jax TPU does not use CUDA, but I want to know what is throwing the error. Does tensorflow think I'm trying to use GPU?  Note that the training does proceed as expected just much slower when compared to the timestamps in the documentation ```  theres I0726 00:57:51.274136 139632684678208 train.py:146] epoch:  1, train_loss: 0.2423, train_accuracy: 92.96, test_loss: 0.0629, test_accuracy: 97.98 I0726 00:57:52.741929 139632684678208 train.py:146] epoch:  2, train_loss: 0.0594, train_accuracy: 98.15, test_loss: 0.0434, test_accuracy: 98.61 I0726 00:57:54.149238 139632684678208 train.py:146] epoch:  3, train_loss: 0.0417, train_accuracy: 98.73, test_loss: 0.0307, test_accuracy: 98.98 I0726 00:57:55.570881 139632684678208 train.py:146] epoch:  4, train_loss: 0.0309, train_accuracy: 99.03, test_loss: 0.0273, test_accuracy: 99.13 I0726 00:57:56.937045 139632684678208 train.py:146] epoch:  5, train_loss: 0.0251, train_accuracy: 99.21, test_loss: 0.0270, test_accuracy: 99.16  mine  I0210 17:09:32.381908 139853706177600 logging_logger.py:49] Constructing tf.data.Dataset mnist for split test, from /home/will/tensorflow_datasets/mnist/3.0.1 I0210 17:09:47.176413 139853706177600 train.py:146] epoch:  1, train_loss: 0.2423, train_accuracy: 92.96, test_loss: 0.0629, test_accuracy: 97.98 I0210 17:10:30.636702 139853706177600 train.py:146] epoch:  2, train_loss: 0.0594, train_accuracy: 98.15, test_loss: 0.0434, test_accuracy: 98.61 I0210 17:11:16.056597 139853706177600 train.py:146] epoch:  3, train_loss: 0.0417, train_accuracy: 98.73, test_loss: 0.0307, test_accuracy: 98.98 I0210 17:12:01.455775 139853706177600 train.py:146] epoch:  4, train_loss: 0.0309, train_accuracy: 99.03, test_loss: 0.0273, test_accuracy: 99.13 I0210 17:12:43.841351 139853706177600 train.py:146] epoch:  5, train_loss: 0.0251, train_accuracy: 99.21, test_loss: 0.0270, test_accuracy: 99.16 ``` ~1 second vs. ~45 seconds per epoch Any help would be greatly appreciated!  What jax/jaxlib version are you using? 0.4.3  Which accelerator(s) are you using? TPU  Additional system info v3  NVIDIA GPU info _No response_",2023-02-10T17:24:35Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/14407
649,"以下是一个github上的jax下的一个issue, 标题是(Improve error message when passing a mismatched Sharding to device_put.)， 内容是 (Improve error message when passing a mismatched Sharding to device_put. Before: ``` ...     assert not ragged, (dim, n_shards) AssertionError: (1, 2) ``` After: ``` ValueError: One of device_put args was given the sharding of NamedSharding(mesh={'x': 2}, spec=PartitionSpec('x',)), which implies that the size of its dimension 0 should be divisible by 2, but it is equal to 1 (full shape: (1,)) ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Improve error message when passing a mismatched Sharding to device_put.,"Improve error message when passing a mismatched Sharding to device_put. Before: ``` ...     assert not ragged, (dim, n_shards) AssertionError: (1, 2) ``` After: ``` ValueError: One of device_put args was given the sharding of NamedSharding(mesh={'x': 2}, spec=PartitionSpec('x',)), which implies that the size of its dimension 0 should be divisible by 2, but it is equal to 1 (full shape: (1,)) ```",2023-02-08T17:08:13Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14354
481,"以下是一个github上的jax下的一个issue, 标题是(Correctly hash auto_spmd fields in compilation cache key.)， 内容是 (Correctly hash auto_spmd fields in compilation cache key. I'm in the process of adding test coverage for this (https://github.com/google/jax/pull/14314), which is how I found this! I manually verified with the new test coverage that it's fixed.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Correctly hash auto_spmd fields in compilation cache key.,"Correctly hash auto_spmd fields in compilation cache key. I'm in the process of adding test coverage for this (https://github.com/google/jax/pull/14314), which is how I found this! I manually verified with the new test coverage that it's fixed.",2023-02-07T00:50:27Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14320
476,"以下是一个github上的jax下的一个issue, 标题是(Add option to run tests with persistent compilation cache enabled.)， 内容是 (Add option to run tests with persistent compilation cache enabled. This can help us get a lot more coverage of the compilation cache, since all compiles will trigger it, instead of having to write explicit compilation cache tests.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add option to run tests with persistent compilation cache enabled.,"Add option to run tests with persistent compilation cache enabled. This can help us get a lot more coverage of the compilation cache, since all compiles will trigger it, instead of having to write explicit compilation cache tests.",2023-02-06T19:01:06Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14314
6659,"以下是一个github上的jax下的一个issue, 标题是(Checkpoint and fft don't play well togethet after jax > 0.3.20)， 内容是 ( Description Hello Jax Team, I have noticed that after upgrading to Jax greater than 0.3.20, the output of my acoustic simulator (based on Jax) had some edge cases where the gradient calculations were incorrect. After some investigation, I believe the issue is related to the combination of the `fftn` functions and `jax.checkpoint` and it happened somewhere between the release of `jax 0.3.21` and `jax 0.3.24`. I have provided a piece of code that reproduces the issue below. I apologize for its length, but I wasn't able to make it shorter.  I have also added comments in the code that begin with ` ERROR SOURCE` to highlight some possible changes that make the issue disappear. ```python from typing import Iterable, NamedTuple import jax import numpy as np from jax import numpy as jnp from jax.scipy.sparse.linalg import gmres from scipy.io import savemat class Domain(NamedTuple):     N: Iterable[int] = (32, 32)     dx: Iterable[float] = (1.0, 1.0) def helmholtz_solver(medium, omega, source):     helm_func = lambda u: helmholtz(u, medium, omega=omega)      ERROR SOURCE 1      Without the checkpoint decorator, the result is the same across jax versions      comment out the next line to make the problem go away     helm_func = jax.checkpoint(helm_func)     out = gmres(         helm_func,         source,         source * 0,         tol=1e3,         restart=10,         maxiter=1000,         solve_method=""batched"",     )[0]     return 1j * omega * out def helmholtz(u, medium, *, omega=1.0):     L = laplacian_term(u, medium)     k = ((omega / medium[""sound_speed""]) ** 2) * u     return L + k def _get_ffts(x, domain):     if jnp.iscomplexobj(x):         ffts = [jnp.fft.fft, jnp.fft.ifft]         _f = lambda N, dx: jnp.fft.fftfreq(N, dx) * 2 * jnp.pi     else:         ffts = [jnp.fft.rfft, jnp.fft.irfft]         _f = lambda N, dx: jnp.fft.rfftfreq(N, dx) * 2 * jnp.pi     k_axis = [_f(n, delta) for n, delta in zip(domain.N, domain.dx)]     return ffts, k_axis def fourier_filter(u, domain, kind=""gradient""):      Evaluates both the gradient of a scalar function, or the diagonal of the      jacobian of a vector function.      Make kvector     ffts, k_vec = _get_ffts(u, domain)      Builds filters     if kind == ""gradient"":          Filter for directional gradients         k_vec = [1j * k for k in k_vec]     elif kind == ""identity"":          A constant filter, equivalent to the identity operator         k_vec = [k * 0.0 + 1.0 + 0j for k in k_vec]     def single_grad(axis, u):         u = jnp.moveaxis(u, axis, 1)         Fx = ffts0         iku = Fx * k_vec[axis]         du = ffts1         return jnp.moveaxis(du, 1, axis)     if u.shape[1] == 1:          Gradient of scalar function         new_params = jnp.stack([single_grad(i, u[..., 0]) for i in range(len(domain.N))], axis=1)     else:          Diagonal of jacobian         new_params = jnp.stack([single_grad(i, u[..., i]) for i in range(len(domain.N))], axis=1)     return new_params def laplacian_term(u, medium):      Making laplacian     grad_u = fourier_filter(u, medium[""domain""], ""gradient"")     mod_diag_jacobian = fourier_filter(grad_u, medium[""domain""], ""gradient"")     nabla_u = jnp.sum(mod_diag_jacobian, axis=1, keepdims=True)      Making the density term     grad_rho0 = fourier_filter(medium[""density""], medium[""domain""], ""gradient"")      ERROR SOURCE 2      If I don't apply the identity filter, I get the same results for different jax versions     _ru = fourier_filter(grad_u * grad_rho0, medium[""domain""], ""identity"")      _ru = grad_u * grad_rho0     rho_u = jnp.sum(_ru, axis=1, keepdims=True) / medium[""density""]     return nabla_u  rho_u if __name__ == ""__main__"":     from matplotlib import pyplot as plt      Setup simulation     domain = Domain((59, 59, 29), (1.0, 1.0, 1.0))     sound_speed = jnp.zeros(domain.N + (1,), dtype=jnp.float32) + 1.0     source = jnp.zeros(domain.N + (1,), dtype=jnp.complex64).at[15:15, 15:15, 10, 0].set(1.0)     .jit     .grad     def loss_func(sound_speed, source):         density = jnp.zeros(domain.N + (1,), dtype=jnp.float32) + 1000.0         medium = {""domain"": domain, ""sound_speed"": sound_speed, ""density"": density}         total_field = helmholtz_solver(medium, 1.0, source)         lossval = jnp.sum(jnp.abs(total_field[29, 29, 21, 0]))         return lossval     gradient = loss_func(sound_speed, source)     plt.imshow(gradient[29,:,:,0])     plt.colorbar()     plt.savefig(""repro_error.png"") ``` I have run the code with two different Jax versions, keeping all the rest the same. Following are the relevant outputs of pip list for each environment. ```bash (jax_old) astanziola/folder/$ pip list Package             Version               Editable project location    ... jax                 0.3.20 jaxlib              0.3.20+cuda11.cudnn82 (jax_new) astanziola:~/repos/holab$ pip list Package             Version              Editable project location    .... jax                 0.4.1 jaxlib              0.4.1+cuda11.cudnn86 ``` When look at the variable `gradient`, by taking the slice `gradient[29,:,:,0]`, there are two very different results for the two Jax versions. I have included images of the results for reference. !image Some insights that may be helpful in resolving this issue:  If the checkpoint function is removed or the calculation of `_ru` is changed, the results for the new Jax version change to match the ones from the old code. However, this does not happen with the old Jax version, where the results are always the same.  The provided code may not make physical sense, but it is stripped from a larger and more complex acoustic simulator that has been experimentally tested. The old Jax version gives the correct results, while the new Jax version gives physically incorrect results.  The issue only seems to occur for 3D inputs and works fine for 2D inputs. It may have something to do with the 3D version of `fftn`.  What jax/jaxlib version are you using? jax 0.3.20 and jax 0.4.1 (but the problem happened since at least ~jax 0.3.24)  Which accelerator(s) are you using? GPU  Additional system info Ubuntu, python 3.10  NVIDIA GPU info ```bash $ nvidiasmi Sun Feb  5 19:02:05 2023        ++  ++++ ``` ```bash $ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Checkpoint and fft don't play well togethet after jax > 0.3.20," Description Hello Jax Team, I have noticed that after upgrading to Jax greater than 0.3.20, the output of my acoustic simulator (based on Jax) had some edge cases where the gradient calculations were incorrect. After some investigation, I believe the issue is related to the combination of the `fftn` functions and `jax.checkpoint` and it happened somewhere between the release of `jax 0.3.21` and `jax 0.3.24`. I have provided a piece of code that reproduces the issue below. I apologize for its length, but I wasn't able to make it shorter.  I have also added comments in the code that begin with ` ERROR SOURCE` to highlight some possible changes that make the issue disappear. ```python from typing import Iterable, NamedTuple import jax import numpy as np from jax import numpy as jnp from jax.scipy.sparse.linalg import gmres from scipy.io import savemat class Domain(NamedTuple):     N: Iterable[int] = (32, 32)     dx: Iterable[float] = (1.0, 1.0) def helmholtz_solver(medium, omega, source):     helm_func = lambda u: helmholtz(u, medium, omega=omega)      ERROR SOURCE 1      Without the checkpoint decorator, the result is the same across jax versions      comment out the next line to make the problem go away     helm_func = jax.checkpoint(helm_func)     out = gmres(         helm_func,         source,         source * 0,         tol=1e3,         restart=10,         maxiter=1000,         solve_method=""batched"",     )[0]     return 1j * omega * out def helmholtz(u, medium, *, omega=1.0):     L = laplacian_term(u, medium)     k = ((omega / medium[""sound_speed""]) ** 2) * u     return L + k def _get_ffts(x, domain):     if jnp.iscomplexobj(x):         ffts = [jnp.fft.fft, jnp.fft.ifft]         _f = lambda N, dx: jnp.fft.fftfreq(N, dx) * 2 * jnp.pi     else:         ffts = [jnp.fft.rfft, jnp.fft.irfft]         _f = lambda N, dx: jnp.fft.rfftfreq(N, dx) * 2 * jnp.pi     k_axis = [_f(n, delta) for n, delta in zip(domain.N, domain.dx)]     return ffts, k_axis def fourier_filter(u, domain, kind=""gradient""):      Evaluates both the gradient of a scalar function, or the diagonal of the      jacobian of a vector function.      Make kvector     ffts, k_vec = _get_ffts(u, domain)      Builds filters     if kind == ""gradient"":          Filter for directional gradients         k_vec = [1j * k for k in k_vec]     elif kind == ""identity"":          A constant filter, equivalent to the identity operator         k_vec = [k * 0.0 + 1.0 + 0j for k in k_vec]     def single_grad(axis, u):         u = jnp.moveaxis(u, axis, 1)         Fx = ffts0         iku = Fx * k_vec[axis]         du = ffts1         return jnp.moveaxis(du, 1, axis)     if u.shape[1] == 1:          Gradient of scalar function         new_params = jnp.stack([single_grad(i, u[..., 0]) for i in range(len(domain.N))], axis=1)     else:          Diagonal of jacobian         new_params = jnp.stack([single_grad(i, u[..., i]) for i in range(len(domain.N))], axis=1)     return new_params def laplacian_term(u, medium):      Making laplacian     grad_u = fourier_filter(u, medium[""domain""], ""gradient"")     mod_diag_jacobian = fourier_filter(grad_u, medium[""domain""], ""gradient"")     nabla_u = jnp.sum(mod_diag_jacobian, axis=1, keepdims=True)      Making the density term     grad_rho0 = fourier_filter(medium[""density""], medium[""domain""], ""gradient"")      ERROR SOURCE 2      If I don't apply the identity filter, I get the same results for different jax versions     _ru = fourier_filter(grad_u * grad_rho0, medium[""domain""], ""identity"")      _ru = grad_u * grad_rho0     rho_u = jnp.sum(_ru, axis=1, keepdims=True) / medium[""density""]     return nabla_u  rho_u if __name__ == ""__main__"":     from matplotlib import pyplot as plt      Setup simulation     domain = Domain((59, 59, 29), (1.0, 1.0, 1.0))     sound_speed = jnp.zeros(domain.N + (1,), dtype=jnp.float32) + 1.0     source = jnp.zeros(domain.N + (1,), dtype=jnp.complex64).at[15:15, 15:15, 10, 0].set(1.0)     .jit     .grad     def loss_func(sound_speed, source):         density = jnp.zeros(domain.N + (1,), dtype=jnp.float32) + 1000.0         medium = {""domain"": domain, ""sound_speed"": sound_speed, ""density"": density}         total_field = helmholtz_solver(medium, 1.0, source)         lossval = jnp.sum(jnp.abs(total_field[29, 29, 21, 0]))         return lossval     gradient = loss_func(sound_speed, source)     plt.imshow(gradient[29,:,:,0])     plt.colorbar()     plt.savefig(""repro_error.png"") ``` I have run the code with two different Jax versions, keeping all the rest the same. Following are the relevant outputs of pip list for each environment. ```bash (jax_old) astanziola/folder/$ pip list Package             Version               Editable project location    ... jax                 0.3.20 jaxlib              0.3.20+cuda11.cudnn82 (jax_new) astanziola:~/repos/holab$ pip list Package             Version              Editable project location    .... jax                 0.4.1 jaxlib              0.4.1+cuda11.cudnn86 ``` When look at the variable `gradient`, by taking the slice `gradient[29,:,:,0]`, there are two very different results for the two Jax versions. I have included images of the results for reference. !image Some insights that may be helpful in resolving this issue:  If the checkpoint function is removed or the calculation of `_ru` is changed, the results for the new Jax version change to match the ones from the old code. However, this does not happen with the old Jax version, where the results are always the same.  The provided code may not make physical sense, but it is stripped from a larger and more complex acoustic simulator that has been experimentally tested. The old Jax version gives the correct results, while the new Jax version gives physically incorrect results.  The issue only seems to occur for 3D inputs and works fine for 2D inputs. It may have something to do with the 3D version of `fftn`.  What jax/jaxlib version are you using? jax 0.3.20 and jax 0.4.1 (but the problem happened since at least ~jax 0.3.24)  Which accelerator(s) are you using? GPU  Additional system info Ubuntu, python 3.10  NVIDIA GPU info ```bash $ nvidiasmi Sun Feb  5 19:02:05 2023        ++  ++++ ``` ```bash $ nvcc version nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 20052022 NVIDIA Corporation Built on Tue_May__3_18:49:52_PDT_2022 Cuda compilation tools, release 11.7, V11.7.64 Build cuda_11.7.r11.7/compiler.31294372_0 ```",2023-02-05T19:11:45Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/14302,"It looks like this is not an issue anymore with jax 0.4.11, I guess this can be closed.",Closing!
6867,"以下是一个github上的jax下的一个issue, 标题是('Failed to fetch URL on try 1 out of 6: Timeout was reached' on Kaggle TPU)， 内容是 ( Description When updating to new Jax version (0.4.1/0.4.2) on Kaggle TPU VM, jax.local_devices() take a few minutes to run. Full error as below: ``` WARNING: Logging before InitGoogle() is written to STDERR I0000 00:00:1671728351.222598      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728361.724467      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728372.225771      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728382.727167      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728393.229029      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached /usr/local/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py:180: UserWarning: TPU backend initialization is taking more than 60.0 seconds. Did you run your code on all TPU hosts? See https://jax.readthedocs.io/en/latest/multi_process.html for more information.   warnings.warn( I0000 00:00:1671728403.730579      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached Failed to get agentworkernumber with error: Timeout was reachedI0000 00:00:1671728414.232041      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728424.733934      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728435.235387      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728445.736270      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728456.237589      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728466.739266      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728477.240960      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728487.742516      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728498.244542      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728508.746012      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728519.247738      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728529.749421      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728540.251204      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728550.752789      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728561.254142      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728571.755886      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728582.257456      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728592.759560      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728603.261649      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728613.763265      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728624.265004      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728634.766641      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728645.268420      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728655.770117      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728666.272072      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728676.773551      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728687.275059      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728697.776765      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728708.277604      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728718.779530      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728729.281428      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728739.783152      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728750.284565      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728760.786476      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728771.288169      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728781.789975      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached E1222 17:06:48.385107324     319 oauth2_credentials.cc:237]            oauth_fetch: UNKNOWN:Cares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:""20221222T17:06:48.385086665+00:00"", grpc_status:2} [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] ``` Here's the public kaggle notebook for reproduction https://www.kaggle.com/code/liminchen1/updatejaxtpubug Is there any setting I can set? Or point me towards the code responsive for it? Or is it an XLA issue?  What jax/jaxlib version are you using? JAX 0.4.1  Which accelerator(s) are you using? TPU  Additional system info On Kaggle VM, newest environment  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,'Failed to fetch URL on try 1 out of 6: Timeout was reached' on Kaggle TPU," Description When updating to new Jax version (0.4.1/0.4.2) on Kaggle TPU VM, jax.local_devices() take a few minutes to run. Full error as below: ``` WARNING: Logging before InitGoogle() is written to STDERR I0000 00:00:1671728351.222598      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728361.724467      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728372.225771      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728382.727167      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728393.229029      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached /usr/local/lib/python3.8/sitepackages/jax/_src/lib/xla_bridge.py:180: UserWarning: TPU backend initialization is taking more than 60.0 seconds. Did you run your code on all TPU hosts? See https://jax.readthedocs.io/en/latest/multi_process.html for more information.   warnings.warn( I0000 00:00:1671728403.730579      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached Failed to get agentworkernumber with error: Timeout was reachedI0000 00:00:1671728414.232041      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728424.733934      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728435.235387      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728445.736270      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728456.237589      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728466.739266      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728477.240960      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728487.742516      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728498.244542      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728508.746012      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728519.247738      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728529.749421      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728540.251204      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728550.752789      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728561.254142      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728571.755886      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728582.257456      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728592.759560      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728603.261649      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728613.763265      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728624.265004      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728634.766641      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728645.268420      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728655.770117      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728666.272072      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728676.773551      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728687.275059      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728697.776765      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728708.277604      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728718.779530      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached I0000 00:00:1671728729.281428      12 common_lib.cc:145] Failed to fetch URL on try 1 out of 6: Timeout was reached I0000 00:00:1671728739.783152      12 common_lib.cc:145] Failed to fetch URL on try 2 out of 6: Timeout was reached I0000 00:00:1671728750.284565      12 common_lib.cc:145] Failed to fetch URL on try 3 out of 6: Timeout was reached I0000 00:00:1671728760.786476      12 common_lib.cc:145] Failed to fetch URL on try 4 out of 6: Timeout was reached I0000 00:00:1671728771.288169      12 common_lib.cc:145] Failed to fetch URL on try 5 out of 6: Timeout was reached I0000 00:00:1671728781.789975      12 common_lib.cc:145] Failed to fetch URL on try 6 out of 6: Timeout was reached E1222 17:06:48.385107324     319 oauth2_credentials.cc:237]            oauth_fetch: UNKNOWN:Cares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:""20221222T17:06:48.385086665+00:00"", grpc_status:2} [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] ``` Here's the public kaggle notebook for reproduction https://www.kaggle.com/code/liminchen1/updatejaxtpubug Is there any setting I can set? Or point me towards the code responsive for it? Or is it an XLA issue?  What jax/jaxlib version are you using? JAX 0.4.1  Which accelerator(s) are you using? TPU  Additional system info On Kaggle VM, newest environment  NVIDIA GPU info _No response_",2023-02-05T10:34:03Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/14300,"I just tested the issue today and realize the error changed. Instead of taking around 470 seconds, it now takes 90 seconds. It now ends after one set of 6 retries. ",Hi Cakes  It looks like this issue appears to be resolved. The Kaggle notebook you provided (https://www.kaggle.com/code/liminchen1/updatejaxtpubug) executed successfully and printed local_devices in approximately 7.1287e05 seconds. Could you please confirm this resolution and close the issue if everything is working as expected?,"Yes, it seems fixed in newer version. Been ages and I forgot about this issue. "
682,"以下是一个github上的jax下的一个issue, 标题是(improve error message when jit tracer is passed into a shape)， 内容是 (Currently we print something like: ``` TypeError: Shapes must be 1D sequences of concrete values of integer type, got (8, Tracedwith). If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. ``` But we have better error messages if the user had typed something like `int(...)`, e.g. explaining why the value is a Tracer in the first place (what operations produced it, or what nonstatic arguments it depends on).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,improve error message when jit tracer is passed into a shape,"Currently we print something like: ``` TypeError: Shapes must be 1D sequences of concrete values of integer type, got (8, Tracedwith). If using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions. ``` But we have better error messages if the user had typed something like `int(...)`, e.g. explaining why the value is a Tracer in the first place (what operations produced it, or what nonstatic arguments it depends on).",2023-02-03T18:11:36Z,better_errors,closed,0,0,https://github.com/jax-ml/jax/issues/14279
1864,"以下是一个github上的jax下的一个issue, 标题是(shard_map (shmap) prototype and JEP)， 内容是 (Add the `shard_map` / `shmap` JEP design doc, and prototype implementation in `jax.experimental.shard_map`. `shmap` is a simple multidevice parallelism API which lets us write perdevice code with explicit collectives, where logical shapes match perdevice physical buffer shapes and collectives correspond exactly to crossdevice communication. It composes well with `pjit` (which is now just `jit`), both in terms of efficiently passing a `shmap`'s outputs to a `pjit`'s inputs, and in terms of nesting a `shmap` inside a `jit`/`pjit`. In particular, `shmap` can be used inside a `jit`/`pjit` to drop temporarily into a ""manual collectives"" mode, letting us write explicit perdevice code and collective communication (rather than relying on the compiler to automatically partition computation and insert collectives). ```python import jax import jax.numpy as jnp from jax.sharding import Mesh, PartitionSpec as P from jax.experimental import mesh_utils devices = mesh_utils.create_device_mesh((4, 2)) mesh = Mesh(devices, axis_names=('x', 'y')) x = jnp.arange( 8 * 16.).reshape(8, 16) y = jnp.arange(16 * 32.).reshape(16, 32) (shmap, mesh=mesh, in_specs=(P('x', 'y'), P('y', None)),          out_specs=P('x', None)) def matmul_basic(x_block, y_block):    x_block: f32[2, 8]    y_block: f32[8, 32]   z_partialsum = jnp.dot(x_block, y_block)   z_block = jax.lax.psum(z_partialsum, 'y')   return z_block z = matmul_basic(x, y)   z: f32[8, 32] ``` See the JEP for more! TODO: * [x] split out some notstrictlyrelated changes into separate PRs ( CC(minor tweaks to type annotations, specialize code on those types)) * [x] add JEP doc * [x] write pr message)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,shard_map (shmap) prototype and JEP,"Add the `shard_map` / `shmap` JEP design doc, and prototype implementation in `jax.experimental.shard_map`. `shmap` is a simple multidevice parallelism API which lets us write perdevice code with explicit collectives, where logical shapes match perdevice physical buffer shapes and collectives correspond exactly to crossdevice communication. It composes well with `pjit` (which is now just `jit`), both in terms of efficiently passing a `shmap`'s outputs to a `pjit`'s inputs, and in terms of nesting a `shmap` inside a `jit`/`pjit`. In particular, `shmap` can be used inside a `jit`/`pjit` to drop temporarily into a ""manual collectives"" mode, letting us write explicit perdevice code and collective communication (rather than relying on the compiler to automatically partition computation and insert collectives). ```python import jax import jax.numpy as jnp from jax.sharding import Mesh, PartitionSpec as P from jax.experimental import mesh_utils devices = mesh_utils.create_device_mesh((4, 2)) mesh = Mesh(devices, axis_names=('x', 'y')) x = jnp.arange( 8 * 16.).reshape(8, 16) y = jnp.arange(16 * 32.).reshape(16, 32) (shmap, mesh=mesh, in_specs=(P('x', 'y'), P('y', None)),          out_specs=P('x', None)) def matmul_basic(x_block, y_block):    x_block: f32[2, 8]    y_block: f32[8, 32]   z_partialsum = jnp.dot(x_block, y_block)   z_block = jax.lax.psum(z_partialsum, 'y')   return z_block z = matmul_basic(x, y)   z: f32[8, 32] ``` See the JEP for more! TODO: * [x] split out some notstrictlyrelated changes into separate PRs ( CC(minor tweaks to type annotations, specialize code on those types)) * [x] add JEP doc * [x] write pr message",2023-02-02T23:25:32Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14273,You'll need to update tests/BUILD.
399,"以下是一个github上的jax下的一个issue, 标题是(Remove placeholder functions for unimplemented NumPy functions.)， 内容是 (These don't seem necessary now JAX has fairly complete coverage of the NumPy API. Also removes the accidental export of _NOT_IMPLEMENTED in several modules.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Remove placeholder functions for unimplemented NumPy functions.,These don't seem necessary now JAX has fairly complete coverage of the NumPy API. Also removes the accidental export of _NOT_IMPLEMENTED in several modules.,2023-02-02T17:31:57Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14269
307,"以下是一个github上的jax下的一个issue, 标题是([sparse] implement __len__ on sparse objects)， 内容是 (...also improve some tests. Came across this when playing with sparse convolutions.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] implement __len__ on sparse objects,...also improve some tests. Came across this when playing with sparse convolutions.,2023-02-01T19:46:22Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14251
1392,"以下是一个github上的jax下的一个issue, 标题是(Running vmap with jit disabled on a function with python control flow gives errors)， 内容是 ( Description I have a function that takes in one value, does some nonlinear transformations to it based on some python control flow and outputs a single value.  Let's call it ```forward```  Now, I can call ```grad(forward)()``` to get the derivative wrt some parameters. Im trying to use ```vmap``` to run this on 10,000 samples. I know my function cannot be jitted (in it's current state) so I run:  ```  with jax.disable_jit:        batched_fun=vmap(grad(forward)))(0,None)        batched_fun(...some values...) ``` It gives the following error: ``` TypeError: Abstract value passed to `bool`, which requires a concrete value. ``` I know this bug was raised before but I went through the other posts and couldn't find an answer to fix this. Is there a way to get the gradient for the 10000 values  with vmap or other, without having the underlying function be jitable?  What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? CPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Running vmap with jit disabled on a function with python control flow gives errors," Description I have a function that takes in one value, does some nonlinear transformations to it based on some python control flow and outputs a single value.  Let's call it ```forward```  Now, I can call ```grad(forward)()``` to get the derivative wrt some parameters. Im trying to use ```vmap``` to run this on 10,000 samples. I know my function cannot be jitted (in it's current state) so I run:  ```  with jax.disable_jit:        batched_fun=vmap(grad(forward)))(0,None)        batched_fun(...some values...) ``` It gives the following error: ``` TypeError: Abstract value passed to `bool`, which requires a concrete value. ``` I know this bug was raised before but I went through the other posts and couldn't find an answer to fix this. Is there a way to get the gradient for the 10000 values  with vmap or other, without having the underlying function be jitable?  What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? CPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info _No response_",2023-02-01T16:02:42Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/14245,"Thanks for raising this! Actually, it's not a bug; this is expected behavior with `vmap`. The issue is that different elements of the batch might need to take different paths in the control flow (e.g. different branches of an `if` when the boolean predicate to the `if` is batched). But we can't take both sides of Python control flow, so we can't trace such a function. The error message is saying that we're tracing (i.e. specializing) on an abstract value representing the set of possible values in the batch (e.g. a value representing the set `{True, False}` for a boolean value), rather than specializing on a particular value (e.g. `True` or `False`), but some Python control flow in the function is demanding a concrete value, which we don't have. Both `vmap` and `jit` behave similarly in that they trace on abstracted values rather than concrete ones: `vmap` because it needs to represent all elements in the batch, and `jit` because it needs to represent all possible values we might apply a function to later (since we're staging the computation out of Python to be compiled). > Is there a way to get the gradient for the 10000 values with vmap or other, without having the underlying function be jitable? Technically yes, in some cases, but it sounds like no in your case. We don't require the whole function to be jittable; we just need any values used in Python control flow to be available / specialized on during trace time. So if we used `vmap` without batching over e.g. the boolean predicate values of any `if`s, we'd be fine. One way to get batchable (and jittable) control flow is to use structured control flow primitives like `jax.lax.cond`, `jax.lax.switch`, or the loop primitives. These are more restrictive than Python control flow, and more awkward to write, but the upside is that they're JAXtransformable, including with `vmap` (though `vmap`of`cond` may produce the equivalent of a `jnp.where`). WDYT?","If you think all elements in your batch may take only one side of the branch, and therefore the whole batch _could_ take just one side of the Python control flow, then you could tweak your program to use a collective, like this: ```python import jax import jax.numpy as jnp def f_single(x):   if x > 0:     return 2 * x   else:     return x xs = 1 + jnp.arange(3.)  jax.vmap(f_single)(xs)   error! def f_batchable(x):   pred = jax.lax.pmin(x > 0, 'batch')   pred2 = jax.lax.pmax(x > 0, 'batch')   assert pred == pred2   if pred:     return 2 * x   else:     return x ys = jax.vmap(f_batchable, axis_name='batch')(xs) print(ys) ``` But I'm guessing that's not what you want to do, and using structured control flow primitives is better.",", Thanks a lot for your answer. That makes a lot of sense. Regarding ""We don't require the whole function to be jittable; we just need any values used in Python control flow to be available / specialized on during trace time."" Can you elaborate a bit more on that (i.e. what does available/specialized mean) or share some examples? Thanks",Here are some references: * video (starting at about 32min in) * How to think in JAX tutorial * Autodidax: JAX core from scratch I may be forgetting some others!
331,"以下是一个github上的jax下的一个issue, 标题是(jax.numpy: remove private members from interface)， 内容是 (In particular, this removes `jnp._add_newdoc_ufunc`, `jnp._pyinstaller_hooks_dir`, and possibly others.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.numpy: remove private members from interface,"In particular, this removes `jnp._add_newdoc_ufunc`, `jnp._pyinstaller_hooks_dir`, and possibly others.",2023-02-01T00:48:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14237
1941,"以下是一个github上的jax下的一个issue, 标题是(`lax.cond` excessive caching behavior, how to avoid?)， 内容是 ( Description I noticed that when using `lax.cond`, the conditional functions are converted to jaxprs and are cached.   This causes a memoryleak like behavior when the conditionals are changing across compilation runs. Here's a MWE: ```python import jax def f(x):   return x**2 identity = lambda x: x def g(x):   return jax.lax.cond(x > 0.0, identity, jax.grad(f), operand=x) while True:   g1 = jax.jit(g)   r = f1(1.)   g1.clear_cache() ``` In this example, I expect `f1.clear_cache` to discard anything related to compiling `g`. However, I'm observing that memory keeps growing across iterations. The memory footprint only reaches a plateau after a few thousand iterations of the loop. Here's a plot of the memory growth of the above snippet: !mem Notice the linear memory growth from 1 ~ 80 seconds.  My use case is that I have an `g` with varying shapes across the outer loop.  Therefore, recompilation is required. Moreover, `clear_cache` is required, to avoid excessively caching `g1` that are used only once. My understanding is: Each new reference produced by `jax.grad(f)` confuses `lax.cond`, causing it to cache all the jaxprs of `jax.grad(f)`.  This problem is really unfortunate for me, since my `f` is quite complicated, and a few iterations of the loop produces more than 4 GBs of cached jaxprs (that are never used again, due to the varying shapes).  Is there a good way to prevent this behavior? Or, perhaps, this could be addressed together with CC(Memory leak when closing over large constants in compiled jax.lax.cond statement)?  What jax/jaxlib version are you using? jax v0.4.2  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"`lax.cond` excessive caching behavior, how to avoid?"," Description I noticed that when using `lax.cond`, the conditional functions are converted to jaxprs and are cached.   This causes a memoryleak like behavior when the conditionals are changing across compilation runs. Here's a MWE: ```python import jax def f(x):   return x**2 identity = lambda x: x def g(x):   return jax.lax.cond(x > 0.0, identity, jax.grad(f), operand=x) while True:   g1 = jax.jit(g)   r = f1(1.)   g1.clear_cache() ``` In this example, I expect `f1.clear_cache` to discard anything related to compiling `g`. However, I'm observing that memory keeps growing across iterations. The memory footprint only reaches a plateau after a few thousand iterations of the loop. Here's a plot of the memory growth of the above snippet: !mem Notice the linear memory growth from 1 ~ 80 seconds.  My use case is that I have an `g` with varying shapes across the outer loop.  Therefore, recompilation is required. Moreover, `clear_cache` is required, to avoid excessively caching `g1` that are used only once. My understanding is: Each new reference produced by `jax.grad(f)` confuses `lax.cond`, causing it to cache all the jaxprs of `jax.grad(f)`.  This problem is really unfortunate for me, since my `f` is quite complicated, and a few iterations of the loop produces more than 4 GBs of cached jaxprs (that are never used again, due to the varying shapes).  Is there a good way to prevent this behavior? Or, perhaps, this could be addressed together with CC(Memory leak when closing over large constants in compiled jax.lax.cond statement)?  What jax/jaxlib version are you using? jax v0.4.2  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-01-31T08:56:08Z,bug,closed,0,11,https://github.com/jax-ml/jax/issues/14220,"I don't think this has anything to do with the jit cache; we can directly query the size of the jit cache this way, and see that it is not growing: ```python import jax from jax._src.api import _cpp_jit_cache def f(x):   return x**2 identity = lambda x: x def g(x):   return jax.lax.cond(x > 0.0, identity, jax.grad(f), operand=x) for i in range(10):   print(_cpp_jit_cache.size(), end=' ')   g1 = jax.jit(g)   r = g1(1.) print()  0 1 2 2 2 2 2 2 2 2  ``` I suspect the memory growth you're seeing has to do with other cacheing that JAX is using internally; for example JAX uses `functools.lru_cache` in many places to tradeoff between memory and runtime cost. An example is here: https://github.com/google/jax/blob/574c0e704722ca6a336458521d4bd7597c268cd6/jax/_src/util.pyL243L259 Unfortunately, there's currently no way to globally clear all those caches during the course of running a JAX program. A related request that you might follow is here: https://github.com/google/jax/issues/10828","Thanks for the prompt response! Yes I agree with you that it has less to do with jit cache. The line of `lax.cond` that creates jaxpr is: https://github.com/google/jax/blob/main/jax/_src/lax/control_flow/conditionals.pyL238 But then, this gets cached by https://github.com/google/jax/blob/b374080e855deb2af6d7c5f9decfefd547c94e95/jax/_src/lax/control_flow/common.pyL65 This cache uses a default size of 4096 entries. My problem is that each entry (a jaxpr) in my case takes a few to maybe 100 megabytes of RAM. This quickly become out of control and get me OOMs, since I'm running on a cluster with memory constraints. Further, `jax.clear_backends` doesn't seem to clear this cache (I'm still seeing OOMs)","Thanks for raising this. Here are a few things we could do to improve this (and we should probably do all of them): 1. provide an easy way to inspect cache sizes and clear them 2. adapt our caches to have size limits rather than numberofentries limits For 2, we may or may not need a C++ implementation for performance. I'm guessing for this application in particular we can make do with a Python implementation, so the main challenge may be just computing the total size of the entries (keys and values) in the cache. For 1, we started brainstorming in CC(brainstorming clearing all caches), but we need to actually land it (and any other pieces we need)..."," Thanks! These options looks quite promising! One curious question, though, is why `lax.while_loop` does not have this problem?  Could we use the same caching mechanism for `while_loop` and `cond`? For now, I will workaround the issue using `select`. ","I think the reason `cond` and `while_loop` behave differently is because `cond` traces with this function, which has a strong reference cache: https://github.com/google/jax/blob/518bb56c6eb1b94b82c457607fa4217fbf769d8d/jax/_src/lax/control_flow/common.pyL65L67 And `while_loop` traces with this function, which has a weak reference cache: https://github.com/google/jax/blob/518bb56c6eb1b94b82c457607fa4217fbf769d8d/jax/_src/lax/control_flow/common.pyL57L58 I'm not sure what the reason is for the different cacheing mechanisms between these two functions."," good find. I suspect we just haven't gotten to adding `weakref_lru_cache` everywhere it should be, since it was a later development. One simple issue here is that `weakref_lru_cache` creates a weakref to its first argument, and cond/switch uses `_initial_style_jaxprs_with_common_consts` which has a sequence of funs as its first argument. Builtin sequence types don't work with weakrefs, and I vaguely recall some complexity with making our own weakrefable sequence type... like, if we drop a reference to one element in the sequence, we need to drop the whole cache entry. Yeah now that I think about this, I recall talking about this with . Parker, if you remember anything else / different, let us know! Otherwise the todo item may be to work out a scheme for applying something like `weakref_lru_cache` to functions which have a `Sequence[Callable]` first argument.",We could change the call signature of the utility to accept `*args` if the sequence argument is the only problem.,Giving it a try in CC(Use weakref_lru_cache for initial_style_jaxprs_with_common_consts),"Looking at this closer, I'm not sure that cache is the issue. For example: ```python from jax._src.lax import control_flow for i in range(10):   g1 = jax.jit(g)   r = g1(1.)   print(control_flow._initial_style_jaxprs_with_common_consts.cache_info()) ``` ``` CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) ``` The cache for this function is only hit once in your sample code. I'm having trouble replicating the memory growth you're reporting. Is it possible that you're seeing memory growth ondevice? Each of your loops creates an ondevice buffer which is then immediately dereferenced, but it won't actually be deleted from device until python garbage collection cleans up the reference and triggers an asynchronous buffer deletion on your device."," Try this code: ```python import jax def f(x):   return x**2 identity = lambda x: x def g(x):   return jax.lax.cond(x > 0.0, identity, jax.grad(f), operand=x) from jax._src.lax import control_flow while True:   g1 = jax.jit(g)   r = g1(1.)   g1.clear_cache()   print(control_flow._initial_style_jaxprs_with_common_consts.cache_info()) ``` I got the following output for the two seconds: ``` CacheInfo(hits=0, misses=1, maxsize=4096, currsize=1) CacheInfo(hits=0, misses=2, maxsize=4096, currsize=2) CacheInfo(hits=0, misses=3, maxsize=4096, currsize=3) CacheInfo(hits=0, misses=4, maxsize=4096, currsize=4) CacheInfo(hits=0, misses=5, maxsize=4096, currsize=5) CacheInfo(hits=0, misses=6, maxsize=4096, currsize=6) CacheInfo(hits=0, misses=7, maxsize=4096, currsize=7) CacheInfo(hits=0, misses=8, maxsize=4096, currsize=8) CacheInfo(hits=0, misses=9, maxsize=4096, currsize=9) CacheInfo(hits=0, misses=10, maxsize=4096, currsize=10) CacheInfo(hits=0, misses=11, maxsize=4096, currsize=11) CacheInfo(hits=0, misses=12, maxsize=4096, currsize=12) CacheInfo(hits=0, misses=13, maxsize=4096, currsize=13) CacheInfo(hits=0, misses=14, maxsize=4096, currsize=14) CacheInfo(hits=0, misses=15, maxsize=4096, currsize=15) CacheInfo(hits=0, misses=16, maxsize=4096, currsize=16) CacheInfo(hits=0, misses=17, maxsize=4096, currsize=17) CacheInfo(hits=0, misses=18, maxsize=4096, currsize=18) CacheInfo(hits=0, misses=19, maxsize=4096, currsize=19) CacheInfo(hits=0, misses=20, maxsize=4096, currsize=20) CacheInfo(hits=0, misses=21, maxsize=4096, currsize=21) CacheInfo(hits=0, misses=22, maxsize=4096, currsize=22) CacheInfo(hits=0, misses=23, maxsize=4096, currsize=23) CacheInfo(hits=0, misses=24, maxsize=4096, currsize=24) CacheInfo(hits=0, misses=25, maxsize=4096, currsize=25) CacheInfo(hits=0, misses=26, maxsize=4096, currsize=26) CacheInfo(hits=0, misses=27, maxsize=4096, currsize=27) CacheInfo(hits=0, misses=28, maxsize=4096, currsize=28) CacheInfo(hits=0, misses=29, maxsize=4096, currsize=29) CacheInfo(hits=0, misses=30, maxsize=4096, currsize=30) .... ``` In your snippet, `jit` cache is catching all the recompilations. But, in my use case, `jit` cannot cache since the shapes are varying. "," CC(fix memory leak in cond jaxpr tracing) fixed this! To verify, I ran this variant of the repro code: ```python import jax def f(x):   return x**2 identity = lambda x: x def g(x):   return jax.lax.cond(x > 0.0, identity, jax.grad(f), operand=x) from jax._src.lax import control_flow while True:   g1 = jax.jit(g)   r = g1(1.)   g1.clear_cache()   print(control_flow.common._pad_jaxpr_constvars.cache_info()) ``` Woohoo!"
322,"以下是一个github上的jax下的一个issue, 标题是(Fix test failures under SciPy 1.10.0.)， 内容是 (Using `jax.numpy.power` inside a callback means that the callback yields JAX arrays, which confuse SciPy.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix test failures under SciPy 1.10.0.,"Using `jax.numpy.power` inside a callback means that the callback yields JAX arrays, which confuse SciPy.",2023-01-30T20:34:59Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14207
1405,"以下是一个github上的jax下的一个issue, 标题是(pure_callback for eigendecomposition)， 内容是 ( Description Hi, eigendecomposition with host_callback was successful with jit and pmap. But while I was trying to do vmap, host_callback was not the option so tried pure_callback option.     import numpy as np     import jax.numpy as jnp     X = jnp.arange(1000**2).reshape((1000,1000))     def eig(X):         type_complex=jnp.complex64         _eig = lambda x: np.linalg.eig(x)           eigenvalues_shape = jax.ShapeDtypeStruct(X.shape[:1], type_complex)         eigenvectors_shape = jax.ShapeDtypeStruct(X.shape, type_complex)         result_shape_dtype = jax.ShapeDtypeStruct(         shape=(eigenvalues_shape.shape, eigenvectors_shape.shape),         dtype=type_complex         )         return jax.pure_callback(_eig, result_shape_dtype, X)     print(eig(X)) > TypeError: Shapes must be 1D sequences of concrete values of integer type, got ((1000,), (1000, 1000)). It seems that pure_callback doesn't accept multiple return values with different size (like eigendecomposition). Is it by design? Thanks!  What jax/jaxlib version are you using? jax 0.4.1,  jaxlib 0.4.1+cuda11.cudnn86  Which accelerator(s) are you using? CPU  Additional system info M1  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,pure_callback for eigendecomposition," Description Hi, eigendecomposition with host_callback was successful with jit and pmap. But while I was trying to do vmap, host_callback was not the option so tried pure_callback option.     import numpy as np     import jax.numpy as jnp     X = jnp.arange(1000**2).reshape((1000,1000))     def eig(X):         type_complex=jnp.complex64         _eig = lambda x: np.linalg.eig(x)           eigenvalues_shape = jax.ShapeDtypeStruct(X.shape[:1], type_complex)         eigenvectors_shape = jax.ShapeDtypeStruct(X.shape, type_complex)         result_shape_dtype = jax.ShapeDtypeStruct(         shape=(eigenvalues_shape.shape, eigenvectors_shape.shape),         dtype=type_complex         )         return jax.pure_callback(_eig, result_shape_dtype, X)     print(eig(X)) > TypeError: Shapes must be 1D sequences of concrete values of integer type, got ((1000,), (1000, 1000)). It seems that pure_callback doesn't accept multiple return values with different size (like eigendecomposition). Is it by design? Thanks!  What jax/jaxlib version are you using? jax 0.4.1,  jaxlib 0.4.1+cuda11.cudnn86  Which accelerator(s) are you using? CPU  Additional system info M1  NVIDIA GPU info _No response_",2023-01-29T11:20:53Z,question,closed,0,10,https://github.com/jax-ml/jax/issues/14199,"A couple issues with your first implementation:  dtypes must match. numpy routines return `complex128`, so we must specify that  your funciton returns two values, so you must specify this with two `ShapeDtypeStruct` objects rather than one With those changes, the code works: ```python import jax.numpy as jnp import jax import numpy as np jax.config.update('jax_enable_x64', True) X = jnp.arange(1000**2).reshape((1000,1000)) def eig(X):     type_complex=jnp.complex128     _eig = lambda x: np.linalg.eig(x)       eigenvalues_shape = jax.ShapeDtypeStruct(X.shape[:1], type_complex)     eigenvectors_shape = jax.ShapeDtypeStruct(X.shape, type_complex)     result_shape_dtype = (eigenvalues_shape, eigenvectors_shape)     return jax.pure_callback(_eig, result_shape_dtype, X) print(eig(X)) ```","Aha, No need to bundle in one ShapeDtypeStruct. Yes, I was switching a lot between 64 and 128 so wasn't careful with that. Thanks a lot!","Why do you call `np.linalg.eig(x) ` instead of `jnp.linalg.eig(x)`? What if one wants to take gradients through `jnp.eig` (or its equivalent, instead of `jnp.eigh`) on a GPU? Including  as we have the same question","If you want `pure_callback` to be compatible with autodiff, you have to define the autodiff rules explicitly. There's an example here: https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.htmlexamplepurecallbackwithcustomjvp","Thank you, Jake!",", Hope this can help. This enables backprop of eigendecomposition and enforces running on CPU while the whole process running on GPU. Mathematics was adopted from other papers. https://github.com/kcml2/meent/blob/main/meent/on_jax/primitives.py  ```python import jax import jax.numpy as jnp from functools import partial (jax.custom_vjp, nondiff_argnums=(1, 2)) def eig(x, type_complex=jnp.complex128, perturbation=1E10):     _eig = jax.jit(jnp.linalg.eig, device=jax.devices('cpu')[0])     eigenvalues_shape = jax.ShapeDtypeStruct(x.shape[:1], type_complex)     eigenvectors_shape = jax.ShapeDtypeStruct(x.shape, type_complex)     result_shape_dtype = (eigenvalues_shape, eigenvectors_shape)     return jax.pure_callback(_eig, result_shape_dtype, x) def eig_fwd(x, type_complex, perturbation):     return eig(x, type_complex, perturbation), eig(x, type_complex, perturbation) def eig_bwd(type_complex, perturbation, res, g):     """"""     Gradient of a general square (complex valued) matrix     Reference: https://github.com/kch3782/torcwa and https://github.com/weiliangjinca/grcwa     """"""     eigval, eigvec = res     grad_eigval, grad_eigvec = g     grad_eigval = jnp.diag(grad_eigval)     s = eigval.reshape((1, 1))  eigval.reshape((1, 1))     F = jnp.conj(s) / (jnp.abs(s) ** 2 + perturbation)     F = F.at[jnp.diag_indices_from(s)].set(0)     XH = jnp.conj(eigvec).T     tmp = jnp.conj(F) * (XH @ grad_eigvec)     XH_i = jnp.linalg.inv(XH)     grad = (XH_i @ (grad_eigval + tmp)) @ XH     if not jnp.iscomplexobj(eigval):         grad = grad.real     return grad, eig.defvjp(eig_fwd, eig_bwd) ```","Above code has bug and below is fixed. ```python import jax import jax.numpy as jnp from functools import partial (jax.custom_vjp, nondiff_argnums=(1, 2, 3)) def eig(x, type_complex=jnp.complex128, perturbation=1E10, device='cpu'):     _eig = jax.jit(jnp.linalg.eig, device=jax.devices('cpu')[0])     eigenvalues_shape = jax.ShapeDtypeStruct(x.shape[:1], type_complex)     eigenvectors_shape = jax.ShapeDtypeStruct(x.shape, type_complex)     result_shape_dtype = (eigenvalues_shape, eigenvectors_shape)     if device == 'cpu':         res = _eig(x)     else:         res = jax.pure_callback(_eig, result_shape_dtype, x)     return res def eig_fwd(x, type_complex, perturbation, device):     return eig(x, type_complex, perturbation), eig(x, type_complex, perturbation) def eig_bwd(type_complex, perturbation, device, res, g):     """"""     Gradient of a general square (complex valued) matrix     Reference: https://github.com/kch3782/torcwa and https://github.com/weiliangjinca/grcwa     """"""     eigval, eigvec = res     grad_eigval, grad_eigvec = g     grad_eigval = jnp.diag(grad_eigval)     s = eigval.reshape((1, 1))  eigval.reshape((1, 1))   original     F = s / (jnp.abs(s) ** 2 + perturbation)     F = F.at[jnp.diag_indices_from(s)].set(0)     XH = eigvec.T     tmp = F * (XH @ grad_eigvec)     XH_i = jnp.linalg.inv(XH)     grad = (XH_i @ (grad_eigval + tmp)) @ XH     if not jnp.iscomplexobj(eigval):         grad = grad.real     return grad, eig.defvjp(eig_fwd, eig_bwd) ```",Thank you so much!," Hi, I found another bug in the code. Below is the fixed. FYI, the previous code showed ~3E5 difference between backproped gradient and numerical gradient. ```python import jax import jax.numpy as jnp from functools import partial (jax.custom_vjp, nondiff_argnums=(1, 2, 3)) def eig(x, type_complex=jnp.complex128, perturbation=1E10, device='cpu'):     _eig = jax.jit(jnp.linalg.eig, device=jax.devices('cpu')[0])     eigenvalues_shape = jax.ShapeDtypeStruct(x.shape[:1], type_complex)     eigenvectors_shape = jax.ShapeDtypeStruct(x.shape, type_complex)     result_shape_dtype = (eigenvalues_shape, eigenvectors_shape)     if device == 'cpu':         res = _eig(x)     else:         res = jax.pure_callback(_eig, result_shape_dtype, x)     return res def eig_fwd(x, type_complex, perturbation, device):     return eig(x, type_complex, perturbation), (eig(x, type_complex, perturbation), x) def eig_bwd(type_complex, perturbation, device, res, g):     """"""     Gradient of a general square (complex valued) matrix     Eq. 30~32 in https://www.sciencedirect.com/science/article/abs/pii/S0010465522002715     Eq 4.77 in https://arxiv.org/pdf/1701.00392.pdf     https://github.com/kch3782/torcwa     https://github.com/weiliangjinca/grcwa     """"""     (eig_val, eig_vector), x = res     grad_eigval, grad_eigvec = g     grad_eigval = jnp.diag(grad_eigval)     X_h = eig_vector.T.conj()     Fij = eig_val.conj().reshape((1, 1))  eig_val.conj().reshape((1, 1))     Fij = Fij / (jnp.abs(Fij) ** 2 + perturbation)     Fij = Fij.at[jnp.diag_indices_from(Fij)].set(0)     grad = jnp.linalg.inv(X_h) @ (grad_eigval.conj() + Fij.conj() * (X_h @ grad_eigvec.conj())) @ X_h     grad = grad.conj()     if not jnp.iscomplexobj(x):         grad = grad.real     return grad, eig.defvjp(eig_fwd, eig_bwd) ``` > /Users/yongha/miniforge3_m1/envs/rcwa/bin/python /Users/yongha/project/rcwa/meent/QA/backprop.py  > JAX grad_ad: >  [[[0.31936418 0.21070505 0.08193991 0.56743577 0.22805356] >   [ 0.09779809 0.05806508  0.1413275   0.26680362 0.04509193]] >  [[0.07998023 0.06814561 0.0119665  0.16095988 0.10340981] >   [0.02519205 0.07270359  0.01071986 0.00646068 0.02300493]] >  [[0.01214202 0.06787044 0.0075716  0.02270837 0.08079771] >   [0.003161   0.09658575 0.02822204 0.0118468  0.03896227]]] > > JAX grad_numeric: >  [[[0.31936418 0.21070505 0.08193991 0.56743577 0.22805356] >   [ 0.09779809 0.05806508  0.1413275   0.26680362 0.04509193]] >  [[0.07998023 0.06814561 0.0119665  0.16095988 0.10340981] >   [0.02519205 0.07270359  0.01071986 0.00646068 0.02300493]] >  [[0.01214202 0.06787044 0.0075716  0.02270837 0.08079771] >   [0.003161   0.09658575 0.02822204 0.0118468  0.03896227]]] > JAX norm:  5.965338263715071e09 >  > Torch grad_ad: >  tensor([[[0.3194, 0.2107, 0.0819, 0.5674, 0.2281], >          [ 0.0978, 0.0581,  0.1413,  0.2668, 0.0451]], >         [[0.0800, 0.0681, 0.0120, 0.1610, 0.1034], >          [0.0252, 0.0727,  0.0107, 0.0065, 0.0230]], >         [[0.0121, 0.0679, 0.0076, 0.0227, 0.0808], >          [0.0032, 0.0966, 0.0282, 0.0118, 0.0390]]], dtype=torch.float64) > > Torch grad_numeric: >  tensor([[[0.3194, 0.2107, 0.0819, 0.5674, 0.2281], >          [ 0.0978, 0.0581,  0.1413,  0.2668, 0.0451]],  >         [[0.0800, 0.0681, 0.0120, 0.1610, 0.1034], >          [0.0252, 0.0727,  0.0107, 0.0065, 0.0230]], >         [[0.0121, 0.0679, 0.0076, 0.0227, 0.0808], >          [0.0032, 0.0966, 0.0282, 0.0118, 0.0390]]]) > > torch.norm:  tensor(3.3405e08, dtype=torch.float64) >  > Process finished with exit code 0 >","Basically same but unnecessary `.conj()` were removed. ```python import jax import jax.numpy as jnp from functools import partial (jax.custom_vjp, nondiff_argnums=(1, 2, 3)) def eig(x, type_complex=jnp.complex128, perturbation=1E10, device='cpu'):     _eig = jax.jit(jnp.linalg.eig, device=jax.devices('cpu')[0])     eigenvalues_shape = jax.ShapeDtypeStruct(x.shape[:1], type_complex)     eigenvectors_shape = jax.ShapeDtypeStruct(x.shape, type_complex)     result_shape_dtype = (eigenvalues_shape, eigenvectors_shape)     if device == 'cpu':         res = _eig(x)     else:         res = jax.pure_callback(_eig, result_shape_dtype, x)     return res def eig_fwd(x, type_complex, perturbation, device):     return eig(x, type_complex, perturbation), (eig(x, type_complex, perturbation), x) def eig_bwd(type_complex, perturbation, device, res, g):     """"""     Gradient of a general square (complex valued) matrix     Eq. 30~32 in https://www.sciencedirect.com/science/article/abs/pii/S0010465522002715     Eq 4.77 in https://arxiv.org/pdf/1701.00392.pdf     https://github.com/kch3782/torcwa     https://github.com/weiliangjinca/grcwa     https://github.com/pytorch/pytorch/issues/41857     https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.htmlComplexnumbersanddifferentiation     https://discuss.pytorch.org/t/autogradoncomplexnumbers/144687/3     """"""     (eig_val, eig_vector), x = res     grad_eigval, grad_eigvec = g     grad_eigval = jnp.diag(grad_eigval)     W_H = eig_vector.T.conj()     Fij = eig_val.reshape((1, 1))  eig_val.reshape((1, 1))     Fij = Fij / (jnp.abs(Fij) ** 2 + perturbation)     Fij = Fij.at[jnp.diag_indices_from(Fij)].set(0)      diag_indices = jnp.arange(len(eig_val))      Eij = eig_val.reshape((1, 1))  eig_val.reshape((1, 1))      Eij = Eij.at[diag_indices, diag_indices].set(1)      Fij = 1 / Eij      Fij = Fij.at[diag_indices, diag_indices].set(0)     grad = jnp.linalg.inv(W_H) @ (grad_eigval.conj() + Fij * (W_H @ grad_eigvec.conj())) @ W_H     grad = grad.conj()     if not jnp.iscomplexobj(x):         grad = grad.real     return grad, eig.defvjp(eig_fwd, eig_bwd) ```"
7324,"以下是一个github上的jax下的一个issue, 标题是(Build Fail with ""fatal error LNK1169: one or more multiply defined symbols found"")， 内容是 ( Description Using the following build command `python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cuda_compute_capabilities=""7.5"" cuda_version=""11.7"" cudnn_version=""8.4.0"" noenable_rocm noenable_tpu` Build fails at the **linking** stage with the following error message. `b'LINK : warning LNK4044: unrecognized option \'/lm\'; ignored\r\nffi.lib(ffi.obj) : error LNK2005: ""struct XLA_FFI_Stream * __cdecl xla::runtime::ffi::GetXlaFfiStream(class xla::runtime::PtrMapByType const *,class xla::runtime::DiagnosticEngine const *)"" (?GetXlaFfiStream@@?$PtrMapByType@@$0BA@@) already defined in executable.lib(executable.obj)\r\n   Creating library bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.lib and object bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.exp\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'send_recv.lib(send_recv.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'llvm_gpu_backend.lib(gpu_backend_lib.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'transpose.lib(transpose.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'bfc_allocator.lib(bfc_allocator.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'nvptx_compiler_impl.lib(nvptx_compiler.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_compiler.lib(gpu_compiler.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'cpu_runtime.lib(cpu_runtime.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_helpers.lib(gpu_helpers.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjrt_stream_executor_client.lib(pjrt_stream_executor_client.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'local_device_state.lib(local_device_state.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'profiler.lib(profiler.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'outfeed_receiver.lib(outfeed_receiver.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'py_client.lib(py_values.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'tfrt_cpu_pjrt_client.lib(tfrt_cpu_pjrt_client.obj)\'\r\nLINK : warning LNK4217: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'allocator_registry_impl.lo.lib(cpu_allocator_impl.obj)\' in function \'""public: static void __cdecl tsl::profiler::TraceMe::InstantActivity,1>(class  &&,int)"" (??$InstantActivity@@$00@$$QEAV@)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pmap_lib.lib(pmap_lib.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjit.lib(pjit.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'jax_jit.lib(jax_jit.obj)\'\r\nLINK : warning LNK4217: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\' in function \'""public: __cdecl tsl::profiler::ScopedAnnotation::ScopedAnnotation >(class )"" (??$?0V@@@@@)\'\r\nLINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(sequential_thunk.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'tracing.lib(tracing.obj)\'\r\nbazelout\\x64_windowsopt\\bin\\external\\org_tensorflow\\tensorflow\\compiler\\xla\\python\\xla_extension.so : fatal error LNK1169: one or more multiply defined symbols found\r\n'`  What jax/jaxlib version are you using? jaxlib v0.4.2, jax 0.4.2  Which accelerator(s) are you using? GPU  Additional system info Windows 10, Python 3.9, Cuda 11.7, Cudnn 8.4.0  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Build Fail with ""fatal error LNK1169: one or more multiply defined symbols found"""," Description Using the following build command `python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cuda_compute_capabilities=""7.5"" cuda_version=""11.7"" cudnn_version=""8.4.0"" noenable_rocm noenable_tpu` Build fails at the **linking** stage with the following error message. `b'LINK : warning LNK4044: unrecognized option \'/lm\'; ignored\r\nffi.lib(ffi.obj) : error LNK2005: ""struct XLA_FFI_Stream * __cdecl xla::runtime::ffi::GetXlaFfiStream(class xla::runtime::PtrMapByType const *,class xla::runtime::DiagnosticEngine const *)"" (?GetXlaFfiStream@@?$PtrMapByType@@$0BA@@) already defined in executable.lib(executable.obj)\r\n   Creating library bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.lib and object bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.exp\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'send_recv.lib(send_recv.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'llvm_gpu_backend.lib(gpu_backend_lib.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'transpose.lib(transpose.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'bfc_allocator.lib(bfc_allocator.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'nvptx_compiler_impl.lib(nvptx_compiler.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_compiler.lib(gpu_compiler.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'cpu_runtime.lib(cpu_runtime.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_helpers.lib(gpu_helpers.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjrt_stream_executor_client.lib(pjrt_stream_executor_client.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'local_device_state.lib(local_device_state.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'profiler.lib(profiler.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'outfeed_receiver.lib(outfeed_receiver.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'py_client.lib(py_values.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'tfrt_cpu_pjrt_client.lib(tfrt_cpu_pjrt_client.obj)\'\r\nLINK : warning LNK4217: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'allocator_registry_impl.lo.lib(cpu_allocator_impl.obj)\' in function \'""public: static void __cdecl tsl::profiler::TraceMe::InstantActivity,1>(class  &&,int)"" (??$InstantActivity@@$00@$$QEAV@)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pmap_lib.lib(pmap_lib.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjit.lib(pjit.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'jax_jit.lib(jax_jit.obj)\'\r\nLINK : warning LNK4217: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\' in function \'""public: __cdecl tsl::profiler::ScopedAnnotation::ScopedAnnotation >(class )"" (??$?0V@@@@@)\'\r\nLINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(sequential_thunk.obj)\'\r\nLINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'tracing.lib(tracing.obj)\'\r\nbazelout\\x64_windowsopt\\bin\\external\\org_tensorflow\\tensorflow\\compiler\\xla\\python\\xla_extension.so : fatal error LNK1169: one or more multiply defined symbols found\r\n'`  What jax/jaxlib version are you using? jaxlib v0.4.2, jax 0.4.2  Which accelerator(s) are you using? GPU  Additional system info Windows 10, Python 3.9, Cuda 11.7, Cudnn 8.4.0  NVIDIA GPU info _No response_",2023-01-26T08:21:38Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/14165," could this be due to the new XLA runtime for CPU or GPU. i see ""xla::runtime::ffi::GetXlaFfiStream"" symbol. Is there some c++ language feature we used that broke Windows build?",hartshorne could you make a more readable error report? something with proper newlines?,"``` LINK : warning LNK4044: unrecognized option \'/lm\'; ignored ffi.lib(ffi.obj) : error LNK2005: ""struct XLA_FFI_Stream * __cdecl xla::runtime::ffi::GetXlaFfiStream(class xla::runtime::PtrMapByType const *,class xla::runtime::DiagnosticEngine const *)"" (?GetXlaFfiStream@@?$PtrMapByType@@$0BA@@) already defined in executable.lib(executable.obj)    Creating library bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.lib and object bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.exp LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'send_recv.lib(send_recv.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'llvm_gpu_backend.lib(gpu_backend_lib.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'transpose.lib(transpose.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'bfc_allocator.lib(bfc_allocator.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'nvptx_compiler_impl.lib(nvptx_compiler.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_compiler.lib(gpu_compiler.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'cpu_runtime.lib(cpu_runtime.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'gpu_helpers.lib(gpu_helpers.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjrt_stream_executor_client.lib(pjrt_stream_executor_client.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'local_device_state.lib(local_device_state.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'profiler.lib(profiler.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'outfeed_receiver.lib(outfeed_receiver.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'py_client.lib(py_values.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'tfrt_cpu_pjrt_client.lib(tfrt_cpu_pjrt_client.obj)\' LINK : warning LNK4217: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'allocator_registry_impl.lo.lib(cpu_allocator_impl.obj)\' in function \'""public: static void __cdecl tsl::profiler::TraceMe::InstantActivity,1>(class  &&,int)"" (??$InstantActivity@@$00@$$QEAV@)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pmap_lib.lib(pmap_lib.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'pjit.lib(pjit.obj)\' LINK : warning LNK4286: symbol \'?g_trace_level@?$atomic@ (struct std::atomic tsl::profiler::internal::g_trace_level)\' defined in \'traceme_recorder_impl.lo.lib(traceme_recorder.obj)\' is imported by \'jax_jit.lib(jax_jit.obj)\' LINK : warning LNK4217: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(gpu_executable.obj)\' in function \'""public: __cdecl tsl::profiler::ScopedAnnotation::ScopedAnnotation >(class )"" (??$?0V@@@@@)\' LINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'gpu_executable.lib(sequential_thunk.obj)\' LINK : warning LNK4286: symbol \'?g_annotation_enabled@?$atomic@ (struct std::atomic tsl::profiler::internal::g_annotation_enabled)\' defined in \'annotation_stack_impl.lo.lib(annotation_stack.obj)\' is imported by \'tracing.lib(tracing.obj)\' bazelout\\x64_windowsopt\\bin\\external\\org_tensorflow\\tensorflow\\compiler\\xla\\python\\xla_extension.so : fatal error LNK1169: one or more multiply defined symbols found ```","We intentionally define this symbol twice, the default implementation uses weak linking here: https://github.com/openxla/xla/blob/f95fde0a6bfe3e7f92f26294586be978434f3e79/xla/runtime/ffi.ccL123L128. The ""real"" implementation in `executable.cc` is supposed to overwrite it at link time. But looks like MSVC doesn't like it? `ABSL_ATTRIBUTE_WEAK` is incorrectly defined?",Skimming through https://stackoverflow.com/questions/2290587/gccstyleweaklinkinginvisualstudio/11529277 CC(未找到相关数据)77 it looks like GCC/LLVM style weak linking doesn't work in MSVC. If there are no flags/attributes to make it work with MSVC I'll take a look at changing the registration mechanism to something that does not rely on weak symbols.,It seems quite a few build issues come from the fact that MSVC always has its own (often off standard) quirks and the various teams involved with JAX / TF / XLA never use MSVC. Is there any reason not to just move to a situation where the Windows build of JAX is by default set to compile with LLVM (as it is available for Windows)?,"I'm guessing this issue is stale, given we have a Windows CPU CI build that is not showing this problem at head. And yes, I think we would like to build our Windows wheels with clang, because it would be one fewer difference between platforms. We will probably wait for TensorFlow to make that switch, though, first (I gather it is in progress), simply to share the work."
3968,"以下是一个github上的jax下的一个issue, 标题是(Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.21.7)， 内容是 (Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.21.7 What's Changed  Disable GoogleDocstring._lookup_annotation by @​hoodmane in toxdev/sphinxautodoctypehints CC(What should JAX do about topK?) Fix napoleon handling of numpy docstrings with no explicitly provided return type by @​hoodmane in toxdev/sphinxautodoctypehints CC(dev timeline for more batched ops)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.21.6...1.21.7    Changelog Sourced from sphinxautodoctypehints's changelog.  1.21.7   Fixed a bug where if a class has an attribute and a constructor argument with the same name, the constructor argument type would be rendered incorrectly (issue 308)   Fixed napoleon handling of numpy docstrings with no specified return type.   1.21.6  Fix a Field list ends without a blank line warning (issue 305).  1.21.5  More robust determination of rtype location / fix issue 302  1.21.4  Improvements to the location of the return type  1.21.3  Use format_annotation to render class attribute type annotations  1.21.2  Fix overloads support  1.21.1  Fix spacing between :rtype: and directives  1.21  Handle types from types module If module is _io, use io instead Put rtype before examples or usage section Remove redundant return type for attributes Handle collections.abc.Callable as well as typing.Callable Put Literal args in code blocks  1.20.2  Fix Optional role to be data.  1.20.1  Fixed default options not displaying for parameters without type hints.  1.20   ... (truncated)   Commits  3aca67b Fix napoleon handling of numpy docstrings with no explicitly provided return ... 53f7f00 Disable GoogleDocstring._lookup_annotation ( CC(What should JAX do about topK?)) 15c6456 Update CHANGELOG.md d823f99 Resolve issue 305 ( CC(index_take in terms of gather, delete index_untake)) fb30e40 Refactor tests to make it easier to add new example functions ( CC(Implement np.take (70).)) 90ec5a7 Update CHANGELOG.md 8c931fe More robust determination of rtype location / fix issue 302 ( CC(On entry to GEMM_EX  parameter number {9,12} had an illegal value)) 7b2f213 Improvements to the location of the return type ( CC(Rev batching)) 4b6897e Update CHANGELOG.md a261a08 Use format_annotation to render class attribute type annotations ( CC(未找到相关数据)) Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.21.7,"Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.21.7 What's Changed  Disable GoogleDocstring._lookup_annotation by @​hoodmane in toxdev/sphinxautodoctypehints CC(What should JAX do about topK?) Fix napoleon handling of numpy docstrings with no explicitly provided return type by @​hoodmane in toxdev/sphinxautodoctypehints CC(dev timeline for more batched ops)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.21.6...1.21.7    Changelog Sourced from sphinxautodoctypehints's changelog.  1.21.7   Fixed a bug where if a class has an attribute and a constructor argument with the same name, the constructor argument type would be rendered incorrectly (issue 308)   Fixed napoleon handling of numpy docstrings with no specified return type.   1.21.6  Fix a Field list ends without a blank line warning (issue 305).  1.21.5  More robust determination of rtype location / fix issue 302  1.21.4  Improvements to the location of the return type  1.21.3  Use format_annotation to render class attribute type annotations  1.21.2  Fix overloads support  1.21.1  Fix spacing between :rtype: and directives  1.21  Handle types from types module If module is _io, use io instead Put rtype before examples or usage section Remove redundant return type for attributes Handle collections.abc.Callable as well as typing.Callable Put Literal args in code blocks  1.20.2  Fix Optional role to be data.  1.20.1  Fixed default options not displaying for parameters without type hints.  1.20   ... (truncated)   Commits  3aca67b Fix napoleon handling of numpy docstrings with no explicitly provided return ... 53f7f00 Disable GoogleDocstring._lookup_annotation ( CC(What should JAX do about topK?)) 15c6456 Update CHANGELOG.md d823f99 Resolve issue 305 ( CC(index_take in terms of gather, delete index_untake)) fb30e40 Refactor tests to make it easier to add new example functions ( CC(Implement np.take (70).)) 90ec5a7 Update CHANGELOG.md 8c931fe More robust determination of rtype location / fix issue 302 ( CC(On entry to GEMM_EX  parameter number {9,12} had an illegal value)) 7b2f213 Improvements to the location of the return type ( CC(Rev batching)) 4b6897e Update CHANGELOG.md a261a08 Use format_annotation to render class attribute type annotations ( CC(未找到相关数据)) Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2023-01-23T17:03:01Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/14119,Still waiting on sphinxbooktheme 0.4.0,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1341,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Generalize binary operations with symbolic dimensions.)， 内容是 (Previously binary operations involving symbolic dimensions would work only when the other operand is convertible to a symbolic dimension, e.g., an integer. This resulted in errors when trying ""x.shape[0] * 3.5"" and the recourse was to ask the user to add an explicit ""jnp.array(x.shape[0])"". Now we allow binary operations with any operand and the ""jnp.array"" is added automatically if the other operand is not an integer or a symbolic dimension. This means that instead of an error they may be an error downstream if one tries to use the result as a dimension. There is one known case where JAX works with static shapes and with the previous behavior, but will fail now. When you operate on `np.ndarray` and symbolic dimension, previously this was kept as a `np.ndarray` but not it is turned into a JAX array. The following program will now fail if `x.shape[0]` is a symbolic dimension.: `jnp.ones(np.arange(5) * x.shape[0])` Instead you should write `jnp.ones([i * x.shape[0] for i in range(5)])` We also took the opportunity to make some improvements to the shape polymorphism documentation.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Generalize binary operations with symbolic dimensions.,"Previously binary operations involving symbolic dimensions would work only when the other operand is convertible to a symbolic dimension, e.g., an integer. This resulted in errors when trying ""x.shape[0] * 3.5"" and the recourse was to ask the user to add an explicit ""jnp.array(x.shape[0])"". Now we allow binary operations with any operand and the ""jnp.array"" is added automatically if the other operand is not an integer or a symbolic dimension. This means that instead of an error they may be an error downstream if one tries to use the result as a dimension. There is one known case where JAX works with static shapes and with the previous behavior, but will fail now. When you operate on `np.ndarray` and symbolic dimension, previously this was kept as a `np.ndarray` but not it is turned into a JAX array. The following program will now fail if `x.shape[0]` is a symbolic dimension.: `jnp.ones(np.arange(5) * x.shape[0])` Instead you should write `jnp.ones([i * x.shape[0] for i in range(5)])` We also took the opportunity to make some improvements to the shape polymorphism documentation.",2023-01-21T12:15:01Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14106
681,"以下是一个github上的jax下的一个issue, 标题是([sparse] refactor tests to improve runtime)， 内容是 (This effectively combines the `_CheckAgainstDense` and `_CompileAndCheckSparse` test utilities. Previously each test would do two jit and two nonjit calls of each tested function; with this change it's down to one jit and one nonjit call. We lose some coverage in the area of recompilation checks (this is why `_CompileAndCheckSparse` would call the jitcompiled function twice) but given that test timeouts have become an issue, I think that's an OK tradeoff.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[sparse] refactor tests to improve runtime,"This effectively combines the `_CheckAgainstDense` and `_CompileAndCheckSparse` test utilities. Previously each test would do two jit and two nonjit calls of each tested function; with this change it's down to one jit and one nonjit call. We lose some coverage in the area of recompilation checks (this is why `_CompileAndCheckSparse` would call the jitcompiled function twice) but given that test timeouts have become an issue, I think that's an OK tradeoff.",2023-01-20T19:18:58Z,kokoro:force-run pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14096,"So, it turns out that this doesn't have an appreciable affect on the runtime of `sparse_test.py`. Perhaps the slow tests are dominated by JIT compile time, so that doing repeated operations doesn't make them appreciably slower? Still, I like this change because it makes the test utilities cleaner, so I think we should merge it."
2049,"以下是一个github上的jax下的一个issue, 标题是(jnp.linalg.inv gave incorrect results)， 内容是 ( Description I was trying to use ```jax.numpy.linalg.inv``` as a replacement for ```numpy.linalg.inv``` but found that ```jax.numpy.linalg.inv``` behaved differently from ```numpy.linalg.inv``` and gave incorrect results for matrix inversion. See below for a reproducible example: ```python from jax import numpy as jnp import numpy as np from jax.numpy.linalg import inv as jnp_inv from numpy.linalg import inv as np_inv def kernel_np(X1, X2, l, sigma_f):     X1 = np.expand_dims(X1, axis=1)     X2 = np.expand_dims(X2, axis=1)     sqdist = np.sum(X1 ** 2, 1).reshape(1, 1) + np.sum(X2 ** 2, 1)  2 * np.dot(X1, X2.T)     return sigma_f ** 2 * np.exp(0.5 / l ** 2 * sqdist) def kernel_jnp(X1, X2, l, sigma_f):     X1 = jnp.expand_dims(X1, axis=1)     X2 = jnp.expand_dims(X2, axis=1)     sqdist = jnp.sum(X1 ** 2, 1).reshape(1, 1) + jnp.sum(X2 ** 2, 1)  2 * jnp.dot(X1, X2.T)     return sigma_f ** 2 * jnp.exp(0.5 / l ** 2 * sqdist)  np matrix inverse X_train = np.linspace(0, 5, num=11) Y_train = np.tanh(X_train) K_np = kernel_np(X_train, X_train, 1.3773659885652723, 1.0147686098215838) K_np_inv = np_inv(K_np)  jnp matrix inverse X_train_jnp = jnp.asarray(X_train) Y_train_jnp = jnp.asarray(Y_train) K_jnp = kernel_jnp(X_train_jnp, X_train_jnp, 1.3773659885652723, 1.0147686098215838) K_jnp_inv = jnp_inv(K_jnp) ``` Multiplying the original matrix with its jnp inverse matrix does not return an identity matrix: ``` (K_jnp.dot(K_jnp_inv)  jnp.eye(K_jnp.shape[0])).max() Out[19]: Array(1.5, dtype=float32) ``` but ```numpy.linalg.inv``` gave the correct results: ``` (K_np.dot(K_np_inv)  np.eye(K_np.shape[0])).max() Out[21]: 1.6896939953329645e09 ```  What jax/jaxlib version are you using? jax0.4.1, jaxlib0.4.1  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.5  NVIDIA GPU info ```       ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jnp.linalg.inv gave incorrect results," Description I was trying to use ```jax.numpy.linalg.inv``` as a replacement for ```numpy.linalg.inv``` but found that ```jax.numpy.linalg.inv``` behaved differently from ```numpy.linalg.inv``` and gave incorrect results for matrix inversion. See below for a reproducible example: ```python from jax import numpy as jnp import numpy as np from jax.numpy.linalg import inv as jnp_inv from numpy.linalg import inv as np_inv def kernel_np(X1, X2, l, sigma_f):     X1 = np.expand_dims(X1, axis=1)     X2 = np.expand_dims(X2, axis=1)     sqdist = np.sum(X1 ** 2, 1).reshape(1, 1) + np.sum(X2 ** 2, 1)  2 * np.dot(X1, X2.T)     return sigma_f ** 2 * np.exp(0.5 / l ** 2 * sqdist) def kernel_jnp(X1, X2, l, sigma_f):     X1 = jnp.expand_dims(X1, axis=1)     X2 = jnp.expand_dims(X2, axis=1)     sqdist = jnp.sum(X1 ** 2, 1).reshape(1, 1) + jnp.sum(X2 ** 2, 1)  2 * jnp.dot(X1, X2.T)     return sigma_f ** 2 * jnp.exp(0.5 / l ** 2 * sqdist)  np matrix inverse X_train = np.linspace(0, 5, num=11) Y_train = np.tanh(X_train) K_np = kernel_np(X_train, X_train, 1.3773659885652723, 1.0147686098215838) K_np_inv = np_inv(K_np)  jnp matrix inverse X_train_jnp = jnp.asarray(X_train) Y_train_jnp = jnp.asarray(Y_train) K_jnp = kernel_jnp(X_train_jnp, X_train_jnp, 1.3773659885652723, 1.0147686098215838) K_jnp_inv = jnp_inv(K_jnp) ``` Multiplying the original matrix with its jnp inverse matrix does not return an identity matrix: ``` (K_jnp.dot(K_jnp_inv)  jnp.eye(K_jnp.shape[0])).max() Out[19]: Array(1.5, dtype=float32) ``` but ```numpy.linalg.inv``` gave the correct results: ``` (K_np.dot(K_np_inv)  np.eye(K_np.shape[0])).max() Out[21]: 1.6896939953329645e09 ```  What jax/jaxlib version are you using? jax0.4.1, jaxlib0.4.1  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.5  NVIDIA GPU info ```       ++  ++ ```",2023-01-20T01:46:09Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/14086,"Thanks for the question – this comes from the fact that JAX defaults to 32bit computations, which are more performant on accelerators. If you enable X64 mode before running your computation, you should see that the JAX accuracy is consistent with that of NumPy. For example, add this to the top of your script: ```python import jax jax.config.update(""jax_enable_x64"", True) ``` Then **restart your runtime** and reexecute, and the output should look like this: ```python >>> (K_jnp.dot(K_jnp_inv)  jnp.eye(K_jnp.shape[0])).max() DeviceArray(1.14232535e09, dtype=float64) >>> (K_np.dot(K_np_inv)  np.eye(K_np.shape[0])).max() 1.6896939953329645e09 ```",Thanks! That worked.
486,"以下是一个github上的jax下的一个issue, 标题是(CI: adjust permissions for upstream-nightly build)， 内容是 (The report job failed with a permissions error: https://github.com/google/jax/actions/runs/3958254462/jobs/6779789502 This should fix the issue. For what it's worth, the root cause is a failure in numpy nightly, and is tracked here: numpy/numpy/issues/23033)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,CI: adjust permissions for upstream-nightly build,"The report job failed with a permissions error: https://github.com/google/jax/actions/runs/3958254462/jobs/6779789502 This should fix the issue. For what it's worth, the root cause is a failure in numpy nightly, and is tracked here: numpy/numpy/issues/23033",2023-01-20T00:35:02Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14085,Successfully created issues (see links above) I need to revert some of the temporary changes.
1352,"以下是一个github上的jax下的一个issue, 标题是(jit compilation with large inlined arrays crashes JAX.)， 内容是 ( Description The following code snippet would result in kernel crashing on Colab (due to OOM). ```python import jax import jax.numpy as jnp import numpy as np batch_size = 256 dataset_size = 5000 def sample(data, key):     key1, key2 = jax.random.split(key)     indices = jax.random.randint(         key1, (batch_size,),         minval=0,         maxval=dataset_size)     data_sample = jax.tree_map(lambda d: jnp.take(d, indices, axis=0), data)     return data_sample, key2 dataset = np.random.randint(0, 10, (dataset_size, 84, 84, 9), dtype=jnp.uint8) dataset = jax.device_put(dataset) sample_fn = jax.jit(lambda key: sample(dataset, key)) sample_fn(jax.random.PRNGKey(0)) ``` Not inlining the `dataset` array works. This is a simplified usage from the code here https://github.com/deepmind/acme/blob/master/acme/datasets/tfds.pyL199L200. My temporary workaround is to fork and change the code to not inline the jax array.  What jax/jaxlib version are you using? jax v0.4.1, jax 0.3.5  Which accelerator(s) are you using? CPU/GPU  Additional system info Colab or Local machine with 3080  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,jit compilation with large inlined arrays crashes JAX.," Description The following code snippet would result in kernel crashing on Colab (due to OOM). ```python import jax import jax.numpy as jnp import numpy as np batch_size = 256 dataset_size = 5000 def sample(data, key):     key1, key2 = jax.random.split(key)     indices = jax.random.randint(         key1, (batch_size,),         minval=0,         maxval=dataset_size)     data_sample = jax.tree_map(lambda d: jnp.take(d, indices, axis=0), data)     return data_sample, key2 dataset = np.random.randint(0, 10, (dataset_size, 84, 84, 9), dtype=jnp.uint8) dataset = jax.device_put(dataset) sample_fn = jax.jit(lambda key: sample(dataset, key)) sample_fn(jax.random.PRNGKey(0)) ``` Not inlining the `dataset` array works. This is a simplified usage from the code here https://github.com/deepmind/acme/blob/master/acme/datasets/tfds.pyL199L200. My temporary workaround is to fork and change the code to not inline the jax array.  What jax/jaxlib version are you using? jax v0.4.1, jax 0.3.5  Which accelerator(s) are you using? CPU/GPU  Additional system info Colab or Local machine with 3080  NVIDIA GPU info _No response_",2023-01-19T21:51:31Z,bug needs info,closed,0,11,https://github.com/jax-ml/jax/issues/14080,What is happening is that it is recompiling it every time and baking the dataset into the graph. This is causing the oom because dataset is large enough that we run out of memory before the caching logic kicks in and starts evicting programs. Seems like a bug in the acme repository.,I am not sure if I understand what you mean by recompiling in this case though. Wouldn't the JIT happen only once in this case?,"My mistake, I thought that this was getting called in a loop. Regardless, by capturing dataset you are 'baking' that constant into the XLA graph itself which will copy it to the CPU for the compilation process (which might be what causes the OOM) and generally isn't a good idea. Additionally, I cannot reproduce the oom.",I can reproduce this when running on colab.. not sure what's different... It's still quite weird to me that I either get a hang locally or run OOM on Colab. The constant array is at most a few hundred Mb large and I don't expect that to cause any issue for XLA.,Can you post some of the details of the oom?,"No problem, here is the colab that I use. https://colab.research.google.com/drive/1mSaw6wxlSCGjiCmXWRo2_1RtbAye1s9?usp=sharing","Looks like the constant is getting copied many times (At least when I run this locally). There is possibly something slightly different in colab (I'm seeing an 8GB alloc in the oom logs). This leads to a significant blowup. I also noticed in your logs the following message which implies that you're not even using GPU: ``` WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ``` This problem might not be present in the GPU/TPU backends which is why the it isn't a problem for other users of this deepmind code.","For GPU, I do not have OOM but instead the process hangs and after some time I get the slow JIT compile error. On Tue, 24 Jan 2023 at 19:21, Parker Schuh ***@***.***> wrote: > Looks like the constant is getting copied many times (At least when I run > this locally). There is possibly something slightly different in colab (I'm > seeing an 8GB alloc in the oom logs). This leads to a significant blowup. > > I also noticed in your logs the following message which implies that > you're not even using GPU: > > WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) > > This problem might not be present in the GPU/TPU backends which is why the > it isn't a problem for other users of this deepmind code. > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","I have not encountered this issue previously working with other datasets. I only found that this becomes a problem (when I use GPU, it hangs) when my dataset consists of images (so they are significantly larger than other datasets that I used). Still, it is a little surprising for me since the dataset is at most a few hundred megabytes large which should still fit nicely in memory.","For me this is a 300MB constant copied ~9 times for some reason which roughly works out to be multiple GB. These allocations happen inside the XLA compiler, so we would need to make a bug against XLA. This is somewhat surprising to me, but the use case is not very convincing.  Maybe the constant is getting converted to a text constant in the colab backend which is causing additional blowups?","Agree that the use case is a bit artificial, as I also don't see particular reasons that I would want to inline the array. I will file an issue on the acme side suggesting not to inline the array as well."
228,"以下是一个github上的jax下的一个issue, 标题是([sparse] update primitive coverage in module doc)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[sparse] update primitive coverage in module doc,,2023-01-19T17:07:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14077
5541,"以下是一个github上的jax下的一个issue, 标题是(Sharding unable to utilize all RAM/HBM)， 内容是 ( Description When using sharding to distribute a large array over several devices, each shard is unable to utilize the entire device's RAM/HBM. In fact, it seems to be limited to about half that size. I've recreated this both on Cloud TPU (v28, having 8 devices of 8GB each) and on the host of the TPU with 8 CPU devices. _I'm using the nifty jax_smi to monitor live memory utilization_  Single device allocation ``` import jax import jax.numpy as jnp from jax.experimental import mesh_utils from jax.sharding import PositionalSharding from jax_smi import initialise_tracking initialise_tracking(interval=0.1) a = jnp.zeros([2_000_000_000]) jax.debug.visualize_array_sharding(a, use_color=False) ``` Output: ``` ┌───────┐ │ TPU 0 │ └───────┘ ``` Memory usage: ```  device: Total 7.5GB          7.5GB (  100%): TPU_0(process=0,(0,0,0,0)) ``` Here I've allocated almost the entire HBM for a single TPU v2 device.  Sharding over 8 devices  success When sharding, I'm able to allocate half of the above size (combined over all shards). The following will succeed, with the array size being 1_000_000_000, exactly half of before: ``` del a   clear previous allocation devices = mesh_utils.create_device_mesh((jax.device_count())) a = jax.device_put(     jnp.zeros([1_000_000_000]),     PositionalSharding(devices) ) jax.debug.visualize_array_sharding(a, use_color=False) ``` Output: ``` ┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐ │ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 6 │ TPU 7 │ TPU 4 │ TPU 5 │ └───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘ ``` Memory usage: ```  device: Total 3.7GB          476.8MB (12.50%): TPU_0(process=0,(0,0,0,0))          476.8MB (12.50%): TPU_1(process=0,(0,0,0,1))          476.8MB (12.50%): TPU_2(process=0,(1,0,0,0))          476.8MB (12.50%): TPU_3(process=0,(1,0,0,1))          476.8MB (12.50%): TPU_4(process=0,(0,1,0,0))          476.8MB (12.50%): TPU_5(process=0,(0,1,0,1))          476.8MB (12.50%): TPU_6(process=0,(1,1,0,0))          476.8MB (12.50%): TPU_7(process=0,(1,1,0,1)) ```  Sharding over 8 devices  fail When slightly increasing the array size (to 1_100_000_000), we fail with RESOURCE_EXHAUSTED: ``` del a   clear previous allocation a = jax.device_put(     jnp.zeros([1_100_000_000]),     PositionalSharding(devices) ) jax.debug.visualize_array_sharding(a, use_color=False) ``` Output: ``` 20230119 13:17:02.322393: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2163] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Failed to allocate request for 524.52MiB (550002688B) on device ordinal 0 Output exceeds the size limit. Open the full output data in a text editor  XlaRuntimeError                           Traceback (most recent call last) /tmp/ipykernel_2457251/453561208.py in  > 1 a = jax.device_put(       2     jnp.zeros([1_100_000_000]),       3     PositionalSharding(devices)       4 )       5 jax.debug.visualize_array_sharding(a, use_color=False) ... > 2136       out_bufs = self.xla_executable.execute_sharded_on_local_devices(    2137           input_bufs)    2138     if dispatch.needs_check_special(): XlaRuntimeError: RESOURCE_EXHAUSTED: Failed to allocate request for 524.52MiB (550002688B) on device ordinal 0 ``` (_I've truncated the entire error message, having a long stack_)  CPU devices To verify this is not specific to TPU, I've recreated this on several CPUs on the TPU host, using ``` import os os.environ[""XLA_FLAGS""] = 'xla_force_host_platform_device_count=8'  Use 8 CPU devices import jax jax.config.update('jax_platform_name', 'cpu') ``` Having what I believe is 300GB of RAM on the host, I'm able to allocate a single [80_000_000_000] array: ```  device: Total 298.0GB          298.0GB (  100%): TFRT_CPU_0 ``` When allocating a sharded [40_000_000_000] array, I can see an intermedia allocation of the entire array on a single device: ```  device: Total 149.0GB          149.0GB (  100%): TFRT_CPU_0 ``` and when finalized, it is distributed across all devices: ```  device: Total 149.0GB          18.6GB (12.50%): TFRT_CPU_0          18.6GB (12.50%): TFRT_CPU_1          18.6GB (12.50%): TFRT_CPU_2          18.6GB (12.50%): TFRT_CPU_3          18.6GB (12.50%): TFRT_CPU_4          18.6GB (12.50%): TFRT_CPU_5          18.6GB (12.50%): TFRT_CPU_6          18.6GB (12.50%): TFRT_CPU_7 ``` However, trying to allocate [45_000_000_000] will fail; this time, with a kernel crash: ``` Canceled future for execute_request message before replies were done The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click here for more info. View Jupyter log for further details. ``` The intermediate allocation is once again on a single device; it seems that there is enough memory for both the entire array allocation plus a single shard: ```  device: Total 167.6GB          167.6GB (  100%): TFRT_CPU_0 ```  Summary Is that the intended behavior?  If so  what is the source of this allocation limit? P.S.  This is part of a larger effort I'm working on.  What jax/jaxlib version are you using? jax 0.4.1, jaxlib 0.4.1  Which accelerator(s) are you using? CPU & TPU (v28)  Additional system info Cloud TPU, Ubuntu 20.04.4 LTS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Sharding unable to utilize all RAM/HBM," Description When using sharding to distribute a large array over several devices, each shard is unable to utilize the entire device's RAM/HBM. In fact, it seems to be limited to about half that size. I've recreated this both on Cloud TPU (v28, having 8 devices of 8GB each) and on the host of the TPU with 8 CPU devices. _I'm using the nifty jax_smi to monitor live memory utilization_  Single device allocation ``` import jax import jax.numpy as jnp from jax.experimental import mesh_utils from jax.sharding import PositionalSharding from jax_smi import initialise_tracking initialise_tracking(interval=0.1) a = jnp.zeros([2_000_000_000]) jax.debug.visualize_array_sharding(a, use_color=False) ``` Output: ``` ┌───────┐ │ TPU 0 │ └───────┘ ``` Memory usage: ```  device: Total 7.5GB          7.5GB (  100%): TPU_0(process=0,(0,0,0,0)) ``` Here I've allocated almost the entire HBM for a single TPU v2 device.  Sharding over 8 devices  success When sharding, I'm able to allocate half of the above size (combined over all shards). The following will succeed, with the array size being 1_000_000_000, exactly half of before: ``` del a   clear previous allocation devices = mesh_utils.create_device_mesh((jax.device_count())) a = jax.device_put(     jnp.zeros([1_000_000_000]),     PositionalSharding(devices) ) jax.debug.visualize_array_sharding(a, use_color=False) ``` Output: ``` ┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐ │ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 6 │ TPU 7 │ TPU 4 │ TPU 5 │ └───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘ ``` Memory usage: ```  device: Total 3.7GB          476.8MB (12.50%): TPU_0(process=0,(0,0,0,0))          476.8MB (12.50%): TPU_1(process=0,(0,0,0,1))          476.8MB (12.50%): TPU_2(process=0,(1,0,0,0))          476.8MB (12.50%): TPU_3(process=0,(1,0,0,1))          476.8MB (12.50%): TPU_4(process=0,(0,1,0,0))          476.8MB (12.50%): TPU_5(process=0,(0,1,0,1))          476.8MB (12.50%): TPU_6(process=0,(1,1,0,0))          476.8MB (12.50%): TPU_7(process=0,(1,1,0,1)) ```  Sharding over 8 devices  fail When slightly increasing the array size (to 1_100_000_000), we fail with RESOURCE_EXHAUSTED: ``` del a   clear previous allocation a = jax.device_put(     jnp.zeros([1_100_000_000]),     PositionalSharding(devices) ) jax.debug.visualize_array_sharding(a, use_color=False) ``` Output: ``` 20230119 13:17:02.322393: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2163] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Failed to allocate request for 524.52MiB (550002688B) on device ordinal 0 Output exceeds the size limit. Open the full output data in a text editor  XlaRuntimeError                           Traceback (most recent call last) /tmp/ipykernel_2457251/453561208.py in  > 1 a = jax.device_put(       2     jnp.zeros([1_100_000_000]),       3     PositionalSharding(devices)       4 )       5 jax.debug.visualize_array_sharding(a, use_color=False) ... > 2136       out_bufs = self.xla_executable.execute_sharded_on_local_devices(    2137           input_bufs)    2138     if dispatch.needs_check_special(): XlaRuntimeError: RESOURCE_EXHAUSTED: Failed to allocate request for 524.52MiB (550002688B) on device ordinal 0 ``` (_I've truncated the entire error message, having a long stack_)  CPU devices To verify this is not specific to TPU, I've recreated this on several CPUs on the TPU host, using ``` import os os.environ[""XLA_FLAGS""] = 'xla_force_host_platform_device_count=8'  Use 8 CPU devices import jax jax.config.update('jax_platform_name', 'cpu') ``` Having what I believe is 300GB of RAM on the host, I'm able to allocate a single [80_000_000_000] array: ```  device: Total 298.0GB          298.0GB (  100%): TFRT_CPU_0 ``` When allocating a sharded [40_000_000_000] array, I can see an intermedia allocation of the entire array on a single device: ```  device: Total 149.0GB          149.0GB (  100%): TFRT_CPU_0 ``` and when finalized, it is distributed across all devices: ```  device: Total 149.0GB          18.6GB (12.50%): TFRT_CPU_0          18.6GB (12.50%): TFRT_CPU_1          18.6GB (12.50%): TFRT_CPU_2          18.6GB (12.50%): TFRT_CPU_3          18.6GB (12.50%): TFRT_CPU_4          18.6GB (12.50%): TFRT_CPU_5          18.6GB (12.50%): TFRT_CPU_6          18.6GB (12.50%): TFRT_CPU_7 ``` However, trying to allocate [45_000_000_000] will fail; this time, with a kernel crash: ``` Canceled future for execute_request message before replies were done The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click here for more info. View Jupyter log for further details. ``` The intermediate allocation is once again on a single device; it seems that there is enough memory for both the entire array allocation plus a single shard: ```  device: Total 167.6GB          167.6GB (  100%): TFRT_CPU_0 ```  Summary Is that the intended behavior?  If so  what is the source of this allocation limit? P.S.  This is part of a larger effort I'm working on.  What jax/jaxlib version are you using? jax 0.4.1, jaxlib 0.4.1  Which accelerator(s) are you using? CPU & TPU (v28)  Additional system info Cloud TPU, Ubuntu 20.04.4 LTS  NVIDIA GPU info _No response_",2023-01-19T15:27:18Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/14075,"Your question contains the answer. You need to avoid the intermediate allocation somehow. If you actually are trying to create jnp.zeros(), then consider running jnp.zeros() as part of an initialization pjit or pmap instead.","I was thinking that might be the reason, yet the numbers do not add up: In the 3rd example (""fail""), the full intermediate allocation + the shard would add up to about 5.1GB, whereas the limit is 7.5GB. Is there an additional (somewhat large) overhead in that process? (in any case, I would make use of pjit & pmap)","This is the XLA computation that the slice lowers to: ``` def fun(x):   n = x.shape[0] // 8   return [x[k*n:(k+1)*n] for k in range(8)] fn = jax.jit(fun) lowered = fn.lower(jax.core.ShapedArray((1_100_000_000,), dtype=np.float32)) print(lowered.compile().as_text()) ``` Looks like it doubles the memory, with the original array + all 8 slices on the same devices (limiting you to 1/16th of the total memory available). Normally this isn't a problem because each shard happens independently and the duplicated memory is temporary.",Got it. Thank you.
474,"以下是一个github上的jax下的一个issue, 标题是(Reflow three boxes on docs main page for mobile)， 内容是 (On mobile the boxes for key jax features don't reflow. Trying to figure out how to do so, though this admittedly is not something I'm familiar with. Creating PR for discussion and collaboration. ://sphinxdesign.readthedocs.io/en/latest/grids.html )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Reflow three boxes on docs main page for mobile,"On mobile the boxes for key jax features don't reflow. Trying to figure out how to do so, though this admittedly is not something I'm familiar with. Creating PR for discussion and collaboration. ://sphinxdesign.readthedocs.io/en/latest/grids.html ",2023-01-18T23:41:15Z,,closed,0,2,https://github.com/jax-ml/jax/issues/14070,I did roughly the same thing here: https://github.com/google/jax/pull/14049 Just waiting for a review (cc. ),Ah way ahead of me. Closing
1046,"以下是一个github上的jax下的一个issue, 标题是(Fix test failures under NumPy 1.24.)， 内容是 (Fix test failures under NumPy 1.24. NumPy 1.24 release notes: https://numpy.org/devdocs/release/1.24.0notes.html The fixes vary, but there are three particularly common changes: * NumPy 1.24 removes a number of deprecated NumPy type aliases references (np.bool, np.int, np.float, np.complex, np.object, np.str, np.unicode, np.long). This change replaces them with their recommended replacements (bool, int, float, complex, object, str, str, int). * Under NumPy 1.24 no longer automatically infers dtype=object when ragged sequences are passed to np.array(). See https://numpy.org/neps/nep0034inferdtypeisobject.html . In most cases the fix is to pass dtype=object explicitly, but in some cases where the raggedness seems accidental other fixes were used. * NumPy 1.24 is pickier about the dtype= option passed to comparison ufuncs.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Fix test failures under NumPy 1.24.,"Fix test failures under NumPy 1.24. NumPy 1.24 release notes: https://numpy.org/devdocs/release/1.24.0notes.html The fixes vary, but there are three particularly common changes: * NumPy 1.24 removes a number of deprecated NumPy type aliases references (np.bool, np.int, np.float, np.complex, np.object, np.str, np.unicode, np.long). This change replaces them with their recommended replacements (bool, int, float, complex, object, str, str, int). * Under NumPy 1.24 no longer automatically infers dtype=object when ragged sequences are passed to np.array(). See https://numpy.org/neps/nep0034inferdtypeisobject.html . In most cases the fix is to pass dtype=object explicitly, but in some cases where the raggedness seems accidental other fixes were used. * NumPy 1.24 is pickier about the dtype= option passed to comparison ufuncs.",2023-01-18T21:17:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14067
376,"以下是一个github上的jax下的一个issue, 标题是(Don't run FP8 dtype test on TPU.)， 内容是 (Don't run FP8 dtype test on TPU. This change makes dtypes_test.py pass even when not using Bazel (e.g. with pytest). It also improves TPU coverage when using Bazel.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Don't run FP8 dtype test on TPU.,Don't run FP8 dtype test on TPU. This change makes dtypes_test.py pass even when not using Bazel (e.g. with pytest). It also improves TPU coverage when using Bazel.,2023-01-18T18:47:59Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14060
5460,"以下是一个github上的jax下的一个issue, 标题是(lax.cond leads to excessive compilation in JAX 0.4.1)， 内容是 (Reported in https://github.com/google/jax/discussions/14032discussioncomment4718316. Minimal reproduction: ```python import jax import jaxlib from jax import lax print(jaxlib.__version__) print(jax.__version__) jax.config.update('jax_log_compiles', True) def f(x):   return x + 1 def g(x):   return x + 2 for x in range(10):   lax.cond(x, f, g, x) ``` Output in JAX v0.3.25: ``` 0.3.25 0.3.25 WARNING:jax._src.dispatch:Finished tracing + transforming prim_fun for jit in 0.0018656253814697266 sec WARNING:jax._src.dispatch:Compiling prim_fun (139732125277056 for args (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). WARNING:jax._src.dispatch:Finished XLA compilation of cond in 0.013795614242553711 sec ``` Output in JAX v0.4.1: ``` 0.4.1 0.4.1 WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0008955001831054688 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230252032) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.02367258071899414 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0006630420684814453 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230252112) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.023768901824951172 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0007023811340332031 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230322960) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.018657445907592773 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.000583648681640625 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230251872) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.0319209098815918 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.01588129997253418 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230325360) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.020395517349243164 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0006628036499023438 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230323680) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.018640756607055664 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0006759166717529297 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230322400) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.017685890197753906 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0006914138793945312 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230369536) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.018553495407104492 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0007042884826660156 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230370576) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.03286623954772949 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0027599334716796875 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230367856) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.02003955841064453 sec ``` Assigning to  because it's possible this is somehow related to the jax.Array change)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,lax.cond leads to excessive compilation in JAX 0.4.1,"Reported in https://github.com/google/jax/discussions/14032discussioncomment4718316. Minimal reproduction: ```python import jax import jaxlib from jax import lax print(jaxlib.__version__) print(jax.__version__) jax.config.update('jax_log_compiles', True) def f(x):   return x + 1 def g(x):   return x + 2 for x in range(10):   lax.cond(x, f, g, x) ``` Output in JAX v0.3.25: ``` 0.3.25 0.3.25 WARNING:jax._src.dispatch:Finished tracing + transforming prim_fun for jit in 0.0018656253814697266 sec WARNING:jax._src.dispatch:Compiling prim_fun (139732125277056 for args (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). WARNING:jax._src.dispatch:Finished XLA compilation of cond in 0.013795614242553711 sec ``` Output in JAX v0.4.1: ``` 0.4.1 0.4.1 WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0008955001831054688 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230252032) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.02367258071899414 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0006630420684814453 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230252112) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.023768901824951172 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0007023811340332031 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230322960) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.018657445907592773 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.000583648681640625 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230251872) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.0319209098815918 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.01588129997253418 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230325360) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.020395517349243164 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0006628036499023438 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230323680) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.018640756607055664 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0006759166717529297 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230322400) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.017685890197753906 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0006914138793945312 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230369536) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.018553495407104492 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0007042884826660156 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230370576) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.03286623954772949 sec WARNING:jax._src.dispatch:Finished tracing + transforming jit(cond) in 0.0027599334716796875 sec WARNING:jax.interpreters.pxla:Compiling prim_fun (139864230367856) for with global shapes and types (ShapedArray(int32[]), ShapedArray(int32[], weak_type=True)). Argument mapping: (OpShardingSharding({replicated}), OpShardingSharding({replicated})). WARNING:jax._src.dispatch:Finished XLA compilation of jit(cond) in 0.02003955841064453 sec ``` Assigning to  because it's possible this is somehow related to the jax.Array change",2023-01-18T16:56:33Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/14058,"Actually, I just checked and `config.jax_array` has no effect on this.",Maybe try setting `jax_experimental_subjaxpr_lowering_cache` config to True and see what happens?,`jax_experimental_subjaxpr_lowering_cache` doesn't seem to affect it either way.,"The root cause seems to be CC(checkify: error types), which added these lines https://github.com/google/jax/blob/a37121e19512ea5ee0ad523eda39fa5bbd8c5442/jax/_src/lax/control_flow/conditionals.pyL258L261 Commenting them out fixes the issue."
1004,"以下是一个github上的jax下的一个issue, 标题是(Increased memory requirement without nn.scan)， 内容是 ( Description When using NVIDIA GPUs, we are noticing significant memory overheads when we turn we set nn.scan = False.  With nn.scan = True: As we increase the number of transformer layers, the number of assigned buffers remains constant. The total memory allocation increases mainly due to the increased parameter allocation. This seems like the expected behavior. With nn.scan = False: As we increase the number of transformer layers, the number of assigned buffers increases, and the preallocated temp allocation increases signficantly. In this case, the increased preallocated temp allocation becomes the biggest contributor to the total memory allocation. It seems like temporary buffers are not being reused when nn.scan = False. I have attached some of our results:  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Increased memory requirement without nn.scan," Description When using NVIDIA GPUs, we are noticing significant memory overheads when we turn we set nn.scan = False.  With nn.scan = True: As we increase the number of transformer layers, the number of assigned buffers remains constant. The total memory allocation increases mainly due to the increased parameter allocation. This seems like the expected behavior. With nn.scan = False: As we increase the number of transformer layers, the number of assigned buffers increases, and the preallocated temp allocation increases signficantly. In this case, the increased preallocated temp allocation becomes the biggest contributor to the total memory allocation. It seems like temporary buffers are not being reused when nn.scan = False. I have attached some of our results:  ++ ```",2023-01-18T14:24:03Z,performance XLA NVIDIA GPU GPU,open,0,5,https://github.com/jax-ml/jax/issues/14056,Can you also open a bug on openxla to have the Google XLA team see it? Link the new bug here.," I am unable to open an issue on openxla. It says: ""An owner of this repository has limited the ability to open an issue to users that have contributed to this repository in the past.""", Can you try again? When I try to open an issue here: https://github.com/openxla/xla/issues/new I seem to be able to do it.,"Hi  , is this an issue in XLA or JAX or both? Is this tracked internally to Google already?",I've created a Google internal bug to track this
2991,"以下是一个github上的jax下的一个issue, 标题是(expose `random_wrap` and `random_unwrap` (plus discussion of early typed key surprises))， 内容是 (I've been working on producing a MWE for this issue for the last month or so.  Part of this is setting everything up identically right before the freeze. What I want to do is run the program that fails and print out the key array, and then produce a MWE where I reconstruct that key array.  The problem is that I'm using the new custom PRNG and these don't seem to have any interface for construction from an array. I don't want to step on any toes, but it seems to me that the design complicates this kind of use pattern.  There are some excellent aspects to the current design.  Jax does an excellent job of having small interfaces, and the new PRNG is no exception.  However, it's just one function too small for my needs. Trying to shoehorn this feature request to fit the pimpl idiom looks like it's going to be awkward.   It would end up being something like `PRNGKeyArray(impl, key_array)`, which means exposing the pimpl objects (`threefry_prng_impl`, etc.), and `PRNGKeyArray`, and providing access to the pimpl objects—all things that go against the whole pimpl idea in the first place.  Or perhaps adding methods to construct from a base array for each implementation? I want to humbly suggest removing the pimpl pattern, and switching to using ordinary classes and inheritance.  Pimpl was popularized in C++ as a _compilation firewall_ (for example, this excellent description by C++ standards chair Herb Sutter).  The idea is that it allows you to completely hide private members (methods and data) in compiled source, and expose a minimal header with only public methods.  That's why it rarely turns up in Python since there is no distinction between source and header. The ordinary Python way to do this is: ```python class PRNGKeyArray(metaclass=PRNGKeyArrayMeta):    def __init__(self, key_data: Array):     assert not isinstance(key_data, core.Tracer)     self._base_array = key_data      def _key_shape(cls) > core.Shape:     raise NotImplementedError   def _split(self, num: int) > Self:     raise NotImplementedError   def _random_bits(self, bit_width: int, shape: core.Shape) > Array:     raise NotImplementedError   def _fold_in(self, data: Array) > Self:     raise NotImplementedError class Threefry2x32Key(PRNGKeyArray):     ... class RBGKey(PRNGKeyArray):     ... class UnsafeRBGKey(PRNGKeyArray):     ... ``` Then, you can reproduce training examples by dumping: ```python print(type(key_array))   This is a function I want. print(key_array.unsafe_raw_array()) ``` and then restoring: ```python SomeKeyType(raw_array)   This is the other function I want. ``` I'm happy to code it up, and there would be no userfacing interface changes (except for exposing the key classes), so nothing would break.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,expose `random_wrap` and `random_unwrap` (plus discussion of early typed key surprises),"I've been working on producing a MWE for this issue for the last month or so.  Part of this is setting everything up identically right before the freeze. What I want to do is run the program that fails and print out the key array, and then produce a MWE where I reconstruct that key array.  The problem is that I'm using the new custom PRNG and these don't seem to have any interface for construction from an array. I don't want to step on any toes, but it seems to me that the design complicates this kind of use pattern.  There are some excellent aspects to the current design.  Jax does an excellent job of having small interfaces, and the new PRNG is no exception.  However, it's just one function too small for my needs. Trying to shoehorn this feature request to fit the pimpl idiom looks like it's going to be awkward.   It would end up being something like `PRNGKeyArray(impl, key_array)`, which means exposing the pimpl objects (`threefry_prng_impl`, etc.), and `PRNGKeyArray`, and providing access to the pimpl objects—all things that go against the whole pimpl idea in the first place.  Or perhaps adding methods to construct from a base array for each implementation? I want to humbly suggest removing the pimpl pattern, and switching to using ordinary classes and inheritance.  Pimpl was popularized in C++ as a _compilation firewall_ (for example, this excellent description by C++ standards chair Herb Sutter).  The idea is that it allows you to completely hide private members (methods and data) in compiled source, and expose a minimal header with only public methods.  That's why it rarely turns up in Python since there is no distinction between source and header. The ordinary Python way to do this is: ```python class PRNGKeyArray(metaclass=PRNGKeyArrayMeta):    def __init__(self, key_data: Array):     assert not isinstance(key_data, core.Tracer)     self._base_array = key_data      def _key_shape(cls) > core.Shape:     raise NotImplementedError   def _split(self, num: int) > Self:     raise NotImplementedError   def _random_bits(self, bit_width: int, shape: core.Shape) > Array:     raise NotImplementedError   def _fold_in(self, data: Array) > Self:     raise NotImplementedError class Threefry2x32Key(PRNGKeyArray):     ... class RBGKey(PRNGKeyArray):     ... class UnsafeRBGKey(PRNGKeyArray):     ... ``` Then, you can reproduce training examples by dumping: ```python print(type(key_array))   This is a function I want. print(key_array.unsafe_raw_array()) ``` and then restoring: ```python SomeKeyType(raw_array)   This is the other function I want. ``` I'm happy to code it up, and there would be no userfacing interface changes (except for exposing the key classes), so nothing would break.",2023-01-17T23:52:19Z,enhancement,closed,0,19,https://github.com/jax-ml/jax/issues/14046,"Current workaround: ```python     print(key_array.impl)   Get type.     print(key_array.unsafe_raw_array())      and then...     raw_array = jnp.array((2634740717, 3214329440), dtype=jnp.uint32)     from jax._src.prng import PRNGKeyArray, threefry_prng_impl     key_array = PRNGKeyArray(threefry_prng_impl, raw_array)   Create using a type and value. ```","If you're importing `_src.prng` for workarounds, a better workaround may be: ```python jax._src.prng.random_wrap(raw_array, impl=impl) ``` In particular, this works under `jit`. I'll try to write back soon about the broader questions up top. Just wanted to get this to you in the meantime. As always, thanks for putting thought into these things and for the discussion/feedback, Neil!","Coming back to this, the ""workaround"" might actually be an answer to part of the question.  – regardless of the idiom by which we set up PRNG implementations, my original thought was to expose `random_wrap` and `random_unwrap` (perhaps by a different name) as public API functions. Would that suffice for your needs as more than a workaround? I'm also happy to separately review improvements to the internal idiom we use for PRNG implementations (e.g. a switch to inheritance), assuming it's a separate concern, and especially if it cleans things up or makes them easier! The reason that I propose to have `random_{,un}wrap` be the way only in/out of key arrays follows my previous comment: these functions bind corresponding arraycasting primitives, and in particular calling them is invariant under `jit` (and staging more generally). Constructing or unwrapping a Python `PRNGKeyArray` (or derivative thereof) is merely the corresponding impl rule for these primitives. Does this seem right?","Sorry for the delay.  I wanted to invest some time in understanding `random_wrap`.  I'm afraid I don't understand what JIT invariance means.  My understanding of Jax internals still has a long way to go.  I don't want to use up your valuable time, but why is it that simple construction doesn't work with the JIT?  In other words, why do you need to do `random_wrap(array, impl)` instead of `PRNGKeyArray(impl, array)`? > Would that suffice for your needs as more than a workaround? That means you'll be exposing the `PRNGImpl` classes like `threefry_prng_impl`?  Yes, that definitely works. > I'm also happy to separately review improvements to the internal idiom we use for PRNG implementations (e.g. a switch to inheritance), assuming it's a separate concern, and especially if it cleans things up or makes them easier! Yes, it's a separate concern!  And I love your commitment to finding the best design.","This seems to be related: ```python from jax import enable_custom_prng, vjp from jax.random import PRNGKey with enable_custom_prng():     def f(i):         return PRNGKey(i)     out, f_vjp = vjp(f, 1)   Fails! ``` Would it be possible to make newstyle `KeyArray`s pytrees?  And then arm `PRNGKey` with a custom derivative that sends a zero cotangent back? Also, how do I produce a zero cotangent for a new style key array?","> In other words, why do you need to do `random_wrap(array, impl)` instead of `PRNGKeyArray(impl, array)`? Calling `random_wrap(array, impl)` binds the `random_wrap_p` primitive. When we're staging, we can capture it as such, and so we can later lower it, as well as lower operations on its output (such as slicing; see the `gather` below): ```python >>> import jax >>> import jax.numpy as jnp >>> def f(x): ...   k = jax._src.prng.random_wrap(x, impl=jax._src.prng.threefry_prng_impl) ...   k = k[:2, :3] ...   return jax.vmap(jax.vmap(jax.random.bernoulli))(k) ...  >>> with jax.enable_custom_prng(): ...     print(jax.make_jaxpr(f)(jnp.ones((5, 6, 2), jnp.uint32))) ...  { lambda ; a:u32[5,6,2]. let     b:key[5,6] = random_wrap[impl=fry] a     c:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     d:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0     e:i32[2] = concatenate[dimension=0] c d     f:key[2,3] = gather[       dimension_numbers=GatherDimensionNumbers(offset_dims=(0, 1), collapsed_slice_dims=(), start_index_map=(0, 1))       fill_value=None       indices_are_sorted=True       mode=GatherScatterMode.PROMISE_IN_BOUNDS       slice_sizes=(2, 3)       unique_indices=True     ] b e     g:u32[2,3] = random_bits[bit_width=32 shape=()] f     h:u32[2,3] = shift_right_logical g 9     i:u32[2,3] = or h 1065353216     j:f32[2,3] = bitcast_convert_type[new_dtype=float32] i     k:f32[2,3] = sub j 1.0     l:f32[] = sub 1.0 0.0     m:f32[2,3] = mul k l     n:f32[2,3] = add m 0.0     o:f32[2,3] = max 0.0 n     p:bool[2,3] = lt o 0.5   in (p,) } >>>  ``` When evaluating eagerly, the primitive is indeed implemented as the expression `PRNGKeyArray(impl, array)`. > Would it be possible to make newstyle `KeyArray`s pytrees? Not quite, because they aren't equivalent to (some tuple of) basic arrays. It's maybe easiest to consider a few examples of what would go wrong if you made them pytrees (naturally, ones that flatten to the underlying `uint32` array). One example is that we can vmap a function over them down to the last dimension, then operate on the key bits—something we want to disallow. Another example is that if we `tree_map` over them, we'd accidentally ""unwrap"" them back down to `uint32` arrays. > Also, how do I produce a zero cotangent for a new style key array? What do you do for current `uint32`style raw key arrays?",> This seems to be related: [...] That's a bug! Thanks for catching it. Filed CC(bad error on VJP of functions returning typed key arrays),We should tag the tracking issue too: CC(RNGs: key types and custom implementations) This is a great thread Neil! Thanks for kicking the tires and asking useful questions.,"> It's maybe easiest to consider a few examples of what would go wrong if you made them pytrees (naturally, ones that flatten to the underlying `uint32` array). One example is that we can vmap a function over them down to the last dimension, then operate on the key bits—something we want to disallow. Another example is that if we `tree_map` over them, we'd accidentally ""unwrap"" them back down to `uint32` arrays. Just a thought, but does Jax support structured arrays?  If so, you could make all of the key information a single record.  Then, it would be essentially atomic in the eyes of vmap and treemap. A related, but less elegant solution would be to register new dtypes corresponding to the key types of your PRNGs. > What do you do for current `uint32`style raw key arrays? I don't do anything yet.  I'm investigating storing the key array that I used on various components of my model in the inference output.  In order for that to work, key arrays need to be pytrees, and I need to be able to produce zero cotangents for them. It does seem a bit odd that I can JIT a function that accepts a seed dynamically, but I can't jit a function that accepts a key array dynamically.","> Just a thought, but does Jax support structured arrays? If so, you could make all of the key information a single record. It doesn't, but we have a semiinternal notion of ""opaque dtypes"" on which the key arrays (and one or two other such things) are built. See CC(prototype unfettered element types in jaxpr arrays), renamed in CC(internal rename: swap mentions of ""custom eltypes"" for ""opaque dtypes""). The two are slightly different. Structured arrays allow for a standard numpy array of structs, whereas opaque dtypes allow for the data an array to be more arbitrary in structure, and allows for the array to customize what operations it supports. With opaque dtypes, our underlying data doesn't have to be an arrayofrecords, slicing doesn't have to be slicing into such an arrayofrecords, various array operations like might not be allowed, etc. > Then, it would be essentially atomic in the eyes of vmap and treemap. Yeah, there might be several ways to achieve this type of ""atomicity in the eyes of vmap."" The opaque dtypes mechanism is sufficient but probably not necessary. Another approach we've kicked around is a generalization of pytrees that involves instantiating typeclasses for different transformations (e.g. vmappable means roughly that you have a pair of rules `from_elt` and `to_elt`). This isn't a complete exploration though, especially not for all transformations. > It does seem a bit odd that I can JIT a function that accepts a seed dynamically, but I can't jit a function that accepts a key array dynamically. How do you mean? This sounds like another potential bug. I can do this: ```python >>> with jax.enable_custom_prng(): ...     print(jax.jit(jax.random.bernoulli)(jax.random.PRNGKey(3))) ...  False ```","Thanks for the very informative explanation! > How do you mean? This sounds like another potential bug. I can do this: I guess I misunderstood the consequences of key arrays not being pytrees.  If they're not a pytree, how are they dynamically passed to a jitted function? And an unrelated question, after CC(bad error on VJP of functions returning typed key arrays) is fixed, how should I produce a zero cotangent for a key array?  For a normal pytree, you would do `tree_map(lambda x: jnp.zeros_like(x), some_tree)`.","> If they're not a pytree, how are they dynamically passed to a jitted function? They're indeed not a pytree, but they do correspond to a ""JAX type"" (that error you were getting with the VJP bug is spurious). This is part of what the internal ""opaque dtype"" mechanism sets up. In other words: we made it work! > And an unrelated question, after CC(bad error on VJP of functions returning typed key arrays) is fixed, how should I produce a zero cotangent for a key array? I'm not sure that it makes sense to make one? Key arrays correspond to uintdtyped arrays under the hood, but either way neither of those forms a vector space. If a function returns a key array, what does it means to differentiate it? Int dtypes present a special case that we (perhaps questionably) decided to handle, and so `zeros_like` works there. Pragmatically: if you're taking VJP of a function whose output has some key arrays and some numeric ones, you could wrap it in one that only outputs numeric ones (and drops the keys) and take VJP of that? There's also that `has_aux` option to `jax.vjp`. Does that get you what you need? I do appreciate your point that the interaction between key arrays and our AD API changes as we upgrade to typed key arrays, and some userside code might need rewriting. We'll have to think on whether we can make the upgrade easier...","> I do appreciate your point that the interaction between key arrays and our AD API changes as we upgrade to typed key arrays, and some userside code might need rewriting. We'll have to think on whether we can make the upgrade easier... Right.  I may be able to work around this, but it's going to be complicated. One pattern that's pretty common in my code is to have classes like this: ```python  class GeometrySamplerLoss:     attention_prediction_error: RealArray     rivals_prediction_error: RealArray     attention_curvature: RealArray     rivals_curvature: RealArray          def zeros(cls) > GeometrySamplerLoss:         z = jnp.asarray(0.0)         return GeometrySamplerLoss(z, z, z, z)          def cotangent(cls) > GeometrySamplerLoss:         o = jnp.asarray(1.0)         z = jnp.asarray(0.0)         return GeometrySamplerLoss(o, o, z, z) ``` When inferring a trajectory using scan, I use the `zeros` method to initialize the state, and the `contangent` method to produce a custom cotangent to pushed into a VJP to train these model components. I understand your idea about not adding key arrays to model component outputs, but instead to create auxilliary outputs. This will complicate my design.  I'll have to have a pair of outputs—one for regular things, and one for auxilliary things.  (I guess while I'm at it, I can put all of the nondiferrentiated things in the auxxilliary output, which is a benefit.) Alternatively, we could consider something like `jax.custom_derivatives.zero_cotangent`: ```python def zero_cotangent(x: T) > T:   """"""Return a zero cotangent like x."""""" ``` Is that what `interpreters.ad.instantiate_zeros` does? > They're indeed not a pytree, but they do correspond to a ""JAX type"" Maybe I don't understand the definition of ""pytree"".  I thought anything that supports `tree_util.tree_flatten` is a pytree?  Or does pytree imply an aggregate type? So `Array` is not a pytree for that reason?","> Maybe I don't understand the definition of ""pytree"". [...] I rather meant more narrowly that the type is not registered with `tree_util.register_pytree_node[_class]`. By your more basic definition, key arrays are indeed pytrees (specifically, leaves, just like other arrays are). > Right. I may be able to work around this, but it's going to be complicated. Yeah, I can understand that this is more complex to express, at least relative to what you could write earlier. We'll have to think about this, but perhaps slightly orthogonally. For example, maybe we should allow `None` or a `SymbolicZero(shape, dtype)` as a generalized convenient shorthand for (co)tangents that indicate ""this is not involved in differentiation."" The shorthand here is really just to avoid the `has_aux` and closure rewriting, but it would have essentially equivalent meaning. And it could be nice for more than only key arrays, too. > Alternatively, we could consider something like `jax.custom_derivatives.zero_cotangent` This sounds like it'd be along the lines of the above, in that it could be how you'd obtain such a hypothetical `SymbolicZero(shape, dtype)`. We might also want it for tangents. Maybe we could call it `symbolic_zero_perturbation_like` or something that's less verbose but along the same lines. I'm thinking out loud here. > Is that what `interpreters.ad.instantiate_zeros` does? Kind of, but not exactly in that it doesn't know about primaltangent correspondences. Anyway, that's an internal helper, which will be rendered only available via `jax._src` at the next opportunity. I wouldn't depend on it.","> This sounds like it'd be along the lines of the above, in that it could be how you'd obtain such a hypothetical `SymbolicZero(shape, dtype)`. We might also want it for tangents. Maybe we could call it `symbolic_zero_perturbation_like` or something that's less verbose but along the same lines. >  > I'm thinking out loud here. Awesome. I think there's a lot of beauty to the way that you can return `None` _out of a custom VJP_ to represent a zero cotangents for a nondifferentiated primal _input_.  What I'm looking for is a way to pass something _into a custom VJP_ to represent a zero cotangent of nondifferentiated primal _output_. So, I really like your idea of `SymbolicZero`.  Like you say, it would be nice for it to work for more than only key arrays—ideally,  all ""Jax types""? > Anyway, that's an internal helper, which will be rendered only available via `jax._src` at the next opportunity. I wouldn't depend on it. I'm definitely not using that!  :smile:   I only found it because I guessed that you might be producing zero cotangents somewhere in Jax code.  If you already have a zero cotangent function inside Jax, I figured it would strengthen the argument for exposing such a function.","I brought up both `None` and `SymbolicZero` because both come up in custom JVPs as of CC(custom_jvp symbolic zeros support). Symbolic zeros enter the rule, but the rule can return `None`s if preferred. Plain `None`s are fine if information like the shape and dtype aren't needed or can be inferred. I haven't entirely thought through whether a `None` is enough when invoking a VJP. > If you already have a zero cotangent function inside Jax, I figured it would strengthen the argument for exposing such a function. To be sure, I think we've determined that this isn't quite the function you're looking for anyway – is that right? That is, your request is not ""let me obtain a proper zero cotangent for this arbitrary primal array."" Instead it is ""let me indicate, by passing a sentinel value in place of a cotangent, an intent not to perturb at this output position."" (In fact, there isn't a cotangent space for your primal array, and in particular no zero contangent.)","> > Is that what interpreters.ad.instantiate_zeros does? > Kind of, but not exactly in that it doesn't know about primaltangent correspondences. Anyway, that's an internal helper, which will be rendered only available via jax._src at the next opportunity. I wouldn't depend on it. I do depend on this... :D Please keep in available somewhere! > Plain `None`s are fine if information like the shape and dtype aren't needed or can be inferred. I haven't entirely thought through whether a `None` is enough when invoking a VJP. I'm pretty sure plain `None`s are totally fine. That's the API offered by `equinox.filter_vjp` and it works smoothly. (After all you can always just reconstruct the symbolic zero with a `Zero(core.get_aval(primal).at_least_vspace())`.)","> That is, your request is not ""let me obtain a proper zero cotangent for this arbitrary primal array."" Instead it is ""let me indicate, by passing a sentinel value in place of a cotangent, an intent not to perturb at this output position."" Yes, exactly! Ideally, the system should work on components: ```python primals_out, f_vjp = vjp(f, *primals_in) cotangents_out = f_vjp(*cotangents_in) ``` Where `cotangents_in` is a tuple of pytrees of cotangent inputs, some of which may _contain_ symbolic zero _components_.  For example, `cotangents_in[3]` may be equal to `(jnp.ones(3), {'foo': SymbolicZero(PRNGKey(2))})`. This goes beyond what's supported by `None` in a custom VJP backwards pass (which can only send `None` for an entire cotangent output—not for a component.  My ideal interface is therefore: `SymbolicZero(any_pytree)` where `any_pytree` could be an array, a key array, or any aggregate pytree type. > I'm pretty sure plain `None`s are totally fine. That's the API offered by `equinox.filter_vjp` and it works smoothly So you support `None` even in place subelements of cotangents (as in my example above)?","Closing because at this point we have `jax.random.{key_data,key_impl}` for unwrapping and `jax.random.wrap_key_data` for wrapping."
4729,"以下是一个github上的jax下的一个issue, 标题是(Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.21.0)， 内容是 (Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.21.0 What's Changed  Put Literal args in code blocks by @​hoodmane in toxdev/sphinxautodoctypehints CC(Added batching rules for convolutions + pooling.) Handle collections.abc.Callable as well as typing.Callable by @​hoodmane in toxdev/sphinxautodoctypehints CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.) Remove redundant return type for attributes by @​hoodmane in toxdev/sphinxautodoctypehints CC(Fix some TODOs that were previously blocked on a Jaxlib release.) Put rtype before examples or usage section by @​hoodmane in toxdev/sphinxautodoctypehints CC(Update XLA version to include fix for XLA reducewindow on CPU) If module is _io, use io instead by @​hoodmane in toxdev/sphinxautodoctypehints CC(fixed TypeError caused by body_fun of foreach loop) Handle types from types module by @​hoodmane in toxdev/sphinxautodoctypehints CC(vmap into dynamic_slice  TypeError: unhashable type: 'BatchTracer')  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.20.2...1.21.0    Changelog Sourced from sphinxautodoctypehints's changelog.  Changelog 1.21  Handle types from types module If module is _io, use io instead Put rtype before examples or usage section Remove redundant return type for attributes Handle collections.abc.Callable as well as typing.Callable Put Literal args in code blocks  1.20.2  Fix Optional role to be data.  1.20.1  Fixed default options not displaying for parameters without type hints.  1.20  Use hatchling instead of setuptools Add support for typing.ParamSpec Allow star prefixes for parameter names in docstring  1.19.2  Fix incorrect domain used for collections.abc.Callable.  1.19.1  Fix bug for recursive type alias.  1.19.0  Support for CPython 3.11, no longer adds Optional when the argument is default per recommendation from PEP484.  1.18.3  Support and require nptyping&gt;=2.1.2  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked module import not working when used as guarded import    ... (truncated)   Commits  0f42705 Add changelog bf68ffc Handle types from types module ( CC(vmap into dynamic_slice  TypeError: unhashable type: 'BatchTracer')) 3ecdb42 If module is _io, use io instead ( CC(fixed TypeError caused by body_fun of foreach loop)) 4ab116d Put rtype before examples or usage section ( CC(Update XLA version to include fix for XLA reducewindow on CPU)) a599552 Remove redundant return type for attributes ( CC(Fix some TODOs that were previously blocked on a Jaxlib release.)) abc497f Handle collections.abc.Callable as well as typing.Callable ( CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.)) e8cdce1 Put Literal args in code blocks ( CC(Added batching rules for convolutions + pooling.)) 2d559cf Update CHANGELOG.md 27dd8e7 Set role to data when creating an Optional from | None ( CC(Fix average pooling to align the window element counts with the spatial dimensions.)) 2747ae2 Update CHANGELOG.md Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.21.0,"Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.21.0 What's Changed  Put Literal args in code blocks by @​hoodmane in toxdev/sphinxautodoctypehints CC(Added batching rules for convolutions + pooling.) Handle collections.abc.Callable as well as typing.Callable by @​hoodmane in toxdev/sphinxautodoctypehints CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.) Remove redundant return type for attributes by @​hoodmane in toxdev/sphinxautodoctypehints CC(Fix some TODOs that were previously blocked on a Jaxlib release.) Put rtype before examples or usage section by @​hoodmane in toxdev/sphinxautodoctypehints CC(Update XLA version to include fix for XLA reducewindow on CPU) If module is _io, use io instead by @​hoodmane in toxdev/sphinxautodoctypehints CC(fixed TypeError caused by body_fun of foreach loop) Handle types from types module by @​hoodmane in toxdev/sphinxautodoctypehints CC(vmap into dynamic_slice  TypeError: unhashable type: 'BatchTracer')  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.20.2...1.21.0    Changelog Sourced from sphinxautodoctypehints's changelog.  Changelog 1.21  Handle types from types module If module is _io, use io instead Put rtype before examples or usage section Remove redundant return type for attributes Handle collections.abc.Callable as well as typing.Callable Put Literal args in code blocks  1.20.2  Fix Optional role to be data.  1.20.1  Fixed default options not displaying for parameters without type hints.  1.20  Use hatchling instead of setuptools Add support for typing.ParamSpec Allow star prefixes for parameter names in docstring  1.19.2  Fix incorrect domain used for collections.abc.Callable.  1.19.1  Fix bug for recursive type alias.  1.19.0  Support for CPython 3.11, no longer adds Optional when the argument is default per recommendation from PEP484.  1.18.3  Support and require nptyping&gt;=2.1.2  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked module import not working when used as guarded import    ... (truncated)   Commits  0f42705 Add changelog bf68ffc Handle types from types module ( CC(vmap into dynamic_slice  TypeError: unhashable type: 'BatchTracer')) 3ecdb42 If module is _io, use io instead ( CC(fixed TypeError caused by body_fun of foreach loop)) 4ab116d Put rtype before examples or usage section ( CC(Update XLA version to include fix for XLA reducewindow on CPU)) a599552 Remove redundant return type for attributes ( CC(Fix some TODOs that were previously blocked on a Jaxlib release.)) abc497f Handle collections.abc.Callable as well as typing.Callable ( CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.)) e8cdce1 Put Literal args in code blocks ( CC(Added batching rules for convolutions + pooling.)) 2d559cf Update CHANGELOG.md 27dd8e7 Set role to data when creating an Optional from | None ( CC(Fix average pooling to align the window element counts with the spatial dimensions.)) 2747ae2 Update CHANGELOG.md Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2023-01-16T17:03:15Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/14024,We can’t update this yet,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
740,"以下是一个github上的jax下的一个issue, 标题是(`jax.random.uniform` returns `nan` if `minval` or `maxval` has `inf` or an extreme value)， 内容是 ( Description ```python import jax import jax.numpy as jnp inf = float(""inf"") minval = jnp.array(inf) maxval = jnp.array(0) k = jax.random.PRNGKey(0) jax.random.uniform(k, (1,), minval=minval, maxval=maxval)  Outputs:  DeviceArray([nan], dtype=float32) ``` The same happens also if I set `minval=jnp.iinfo(jnp.float32).min`.  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.random.uniform` returns `nan` if `minval` or `maxval` has `inf` or an extreme value," Description ```python import jax import jax.numpy as jnp inf = float(""inf"") minval = jnp.array(inf) maxval = jnp.array(0) k = jax.random.PRNGKey(0) jax.random.uniform(k, (1,), minval=minval, maxval=maxval)  Outputs:  DeviceArray([nan], dtype=float32) ``` The same happens also if I set `minval=jnp.iinfo(jnp.float32).min`.  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_",2023-01-13T16:00:59Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/14003,"Thanks for the report – it's not obvious to me that a uniform distribution with infinite range is welldefined. Cosider that the uniform distribution is defined such that the probability density function satisfies $$ \int_{x_{min}}^{x_{max}} u(x) dx = 1 $$ and when $x_{min}$ or $x_{max}$ approach infinity, the pdf is everywhere zero, meaning you have zero probability of choosing any finite range of values. What do you think?","Thanks for the quick answer , I very much agree! I raised the issue because I was expecting `inf` to translate to the max representable `jnp.float32`, with the range being finite.","Thanks  that makes sense. I think though the current behavior is more consistent than clipping infinite values to the float range. For example, numpy's uniform distribution also treats infinite bounds as invalid: ```python In [1]: import numpy as np    ...: np.random.uniform(low=0, high=np.inf)  OverflowError                             Traceback (most recent call last) Cell In[1], line 2       1 import numpy as np > 2 np.random.uniform(low=0, high=np.inf) File mtrand.pyx:1139, in numpy.random.mtrand.RandomState.uniform() OverflowError: Range exceeds valid bounds ``` JAX cannot raise an error in this case for the same reasons that it cannot error on outofbound indices (exceptions/warnings depending on runtime values is not supported), so JAX returns NaN.","Got it  I'll close the issue as there seems to be no action here in jax, but thanks very much for the exchange!"
1209,"以下是一个github上的jax下的一个issue, 标题是(`jax[cuda]=0.4.1` version is slower on A100 GPU)， 内容是 ( Description I first encountered this issue in a larger code I was trying to run. Then, I was able to create a bare minimum version as follows. I think the `jnp.linalg.cholesky` is making it slower because matrix multiplication alone did not show a performance degradation. ```py import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""0"" import jax import jax.numpy as jnp from time import time key = jax.random.PRNGKey(0) x1 = jax.random.normal(key, shape=(1000, 1000, 11)) key = jax.random.PRNGKey(1) x2 = jax.random.normal(key, shape=(1000, 1000, 11)) def f(x1, x2, i):     a = x1[:, : ,i] @ x2[:, : ,i]     a = a.T     b = jnp.linalg.cholesky(a)     return b.mean() f(x1, x2, 10).block_until_ready()  Warmup times = jnp.ones(10) for i in range(10):     init = time()     f(x1, x2, i).block_until_ready()     end = time()  init     times = times.at[i].set(end) print(f""Mean: {times.mean()}, Std: {times.std()}"") ``` Outputs on different devices and JAX versions were the following:  ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax[cuda]=0.4.1` version is slower on A100 GPU," Description I first encountered this issue in a larger code I was trying to run. Then, I was able to create a bare minimum version as follows. I think the `jnp.linalg.cholesky` is making it slower because matrix multiplication alone did not show a performance degradation. ```py import os os.environ[""CUDA_VISIBLE_DEVICES""] = ""0"" import jax import jax.numpy as jnp from time import time key = jax.random.PRNGKey(0) x1 = jax.random.normal(key, shape=(1000, 1000, 11)) key = jax.random.PRNGKey(1) x2 = jax.random.normal(key, shape=(1000, 1000, 11)) def f(x1, x2, i):     a = x1[:, : ,i] @ x2[:, : ,i]     a = a.T     b = jnp.linalg.cholesky(a)     return b.mean() f(x1, x2, 10).block_until_ready()  Warmup times = jnp.ones(10) for i in range(10):     init = time()     f(x1, x2, i).block_until_ready()     end = time()  init     times = times.at[i].set(end) print(f""Mean: {times.mean()}, Std: {times.std()}"") ``` Outputs on different devices and JAX versions were the following:  ```",2023-01-13T14:05:13Z,bug NVIDIA GPU,closed,0,6,https://github.com/jax-ml/jax/issues/14002,"I am also seeing this.  Here are my timing measurements on a v100: !image and here they are on a a100: !image I've also measured this on a p100, and the timings are similar to the p100. Interestingly, when I measured jax.linalg.cholesky on a v100 back in September 2023, I got a curve that looked very similar to the a100 curve I get now.  So my theory is that the cholesky implementation improved sometime between now and then, but only for v100 and p100's.","Hi   I have tested the provided repro on GCP VM instances having GPUs A100, V100 and P100, individually, with JAX version 0.5.0 and the time taken by JAX is as follows:    0.000226063552 Could you please verify in your system and check if the issue is resolved? Also, please let us if the issue still persists. Thank you.",Verified!,Hi   Could you please close the issue if the resolved? Thank you.,"I didn't create the issue, so GitHub is telling me I don't have permissions to close it.",Great to know that this issue is resolved. Thanks to the people who worked on it and verified the solution. I am closing the issue now.
598,"以下是一个github上的jax下的一个issue, 标题是(Zero-length indices in `jax.numpy.mgrid`)， 内容是 ( Description ```python np.mgrid[()]   yields an integer type array of zero length jnp.mgrid[()]   raises a ValueError because it tries to stack a tuple of zero length ```  What jax/jaxlib version are you using? jax commit f729da4a368e9, jaxlib 0.4.1  Which accelerator(s) are you using? CPU/GPU  Additional system info python 3.10.9, Linux (Arch Linux)  NVIDIA GPU info ```  ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Zero-length indices in `jax.numpy.mgrid`," Description ```python np.mgrid[()]   yields an integer type array of zero length jnp.mgrid[()]   raises a ValueError because it tries to stack a tuple of zero length ```  What jax/jaxlib version are you using? jax commit f729da4a368e9, jaxlib 0.4.1  Which accelerator(s) are you using? CPU/GPU  Additional system info python 3.10.9, Linux (Arch Linux)  NVIDIA GPU info ```  ```",2023-01-12T12:04:14Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/13981
2258,"以下是一个github上的jax下的一个issue, 标题是(Unexpected result when solving a spring-mass system)， 内容是 ( Description Would like to report a strange result when trying to implement a springmass solver using `jax.experimental.ode.odeint`. ``` import matplotlib.pyplot as plt import jax.numpy as jnp import jax.experimental.ode def mass_spring_damper(arr, F=5.0, k=2.0, m=20.0, c=4.0):     """"""     arr is in the form of [x, \dot{x}]     """"""     return jnp.matmul(jnp.array([[0, 1.0], [k / m, c / m]]), arr.T) + jnp.array(         [[0], [F / m]]     ) time_array = jnp.linspace(0, 60.0, 1000, dtype=jnp.float64) result = jax.experimental.ode.odeint(     mass_spring_damper,     jnp.array([[0.0, 0.0]]),     t=time_array, ).squeeze(axis=1) fig, ax = plt.subplots(1, 1) plt.plot(time_array, result[:, 0], label=""displacement"", alpha=0.5) plt.plot(time_array, result[:, 1], label=""velocity"", alpha=0.5, color=""k"") plt.tight_layout() ``` !0a5d05772a724f26829b7c7be5afbb95 The result above shows the displacement going to a large positive number which I thought would have been controlled by both the spring constant k and damper c. Here is some working code using `scipy.integrate.odeint`:  ``` import numpy as np from scipy.integrate import odeint import matplotlib.pyplot as plt  Initialization tstart = 0 tstop = 60 increment = 0.1  Initial condition x_init = [0, 0] t = np.arange(tstart, tstop + 1, increment)  Function that returns dx/dt def mass_spring_damper(x, t):     c = 4   Damping constant     k = 2   Stiffness of the spring     m = 20   Mass     F = 5     dx1dt = x[1]     dx2dt = (F  c * x[1]  k * x[0]) / m     dxdt = [dx1dt, dx2dt]     return dxdt  Solve ODE x = odeint(mass_spring_damper, x_init, t)  Plot the Results fig, ax = plt.subplots(1, 1) ax.plot(t, x[:, 0]) ax.plot(t, x[:, 1]) ax.set(     title=""Simulation of MassSpringDamper System"",     xlabel=""time (s)"",     ylabel=""displacement x(t)"", ) fig.tight_layout() ``` !0de086c804ce44cf90ba9038027deb04  What jax/jaxlib version are you using? jax v0.4.1  Which accelerator(s) are you using? CPU  Additional system info Apple silicone  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unexpected result when solving a spring-mass system," Description Would like to report a strange result when trying to implement a springmass solver using `jax.experimental.ode.odeint`. ``` import matplotlib.pyplot as plt import jax.numpy as jnp import jax.experimental.ode def mass_spring_damper(arr, F=5.0, k=2.0, m=20.0, c=4.0):     """"""     arr is in the form of [x, \dot{x}]     """"""     return jnp.matmul(jnp.array([[0, 1.0], [k / m, c / m]]), arr.T) + jnp.array(         [[0], [F / m]]     ) time_array = jnp.linspace(0, 60.0, 1000, dtype=jnp.float64) result = jax.experimental.ode.odeint(     mass_spring_damper,     jnp.array([[0.0, 0.0]]),     t=time_array, ).squeeze(axis=1) fig, ax = plt.subplots(1, 1) plt.plot(time_array, result[:, 0], label=""displacement"", alpha=0.5) plt.plot(time_array, result[:, 1], label=""velocity"", alpha=0.5, color=""k"") plt.tight_layout() ``` !0a5d05772a724f26829b7c7be5afbb95 The result above shows the displacement going to a large positive number which I thought would have been controlled by both the spring constant k and damper c. Here is some working code using `scipy.integrate.odeint`:  ``` import numpy as np from scipy.integrate import odeint import matplotlib.pyplot as plt  Initialization tstart = 0 tstop = 60 increment = 0.1  Initial condition x_init = [0, 0] t = np.arange(tstart, tstop + 1, increment)  Function that returns dx/dt def mass_spring_damper(x, t):     c = 4   Damping constant     k = 2   Stiffness of the spring     m = 20   Mass     F = 5     dx1dt = x[1]     dx2dt = (F  c * x[1]  k * x[0]) / m     dxdt = [dx1dt, dx2dt]     return dxdt  Solve ODE x = odeint(mass_spring_damper, x_init, t)  Plot the Results fig, ax = plt.subplots(1, 1) ax.plot(t, x[:, 0]) ax.plot(t, x[:, 1]) ax.set(     title=""Simulation of MassSpringDamper System"",     xlabel=""time (s)"",     ylabel=""displacement x(t)"", ) fig.tight_layout() ``` !0de086c804ce44cf90ba9038027deb04  What jax/jaxlib version are you using? jax v0.4.1  Which accelerator(s) are you using? CPU  Additional system info Apple silicone  NVIDIA GPU info _No response_",2023-01-11T23:38:02Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/13975,"It looks like `F` is always applied as a positive force  shouldn't it multiply the velocity in order to be a damping term? In other words, if I'm reading it correctly, at every step the velocity is increased by `F/m`, which seems like it would cause the position to run away toward positive infinity if the spring forces can't overcome that.","The reason why I wrote it that way is because I thought the above would evaluate as the following : !image The intention would be that the force balance is evaluated as $\ddot{x} = \frac{k}{m}x  \frac{c}{m}\dot{x} + \frac{F}{m} $ At some point, $kx$ would grow large enough that it counteracts the constant $F$. Is there some nuance in my implementation that is forcing $\frac{F}{m}$ to be additive at each time step?","Oh, thanks for the details. It's easier to see when it's written out like that","Is it a problem that your input has shape `(1, 2)` and you output has shape `(2, 1)`?","Yeah that's my bad. Have now corrected this so that inputs and outputs are (2,1). However the result appears to be the same. ``` import matplotlib.pyplot as plt import jax.numpy as jnp import jax.experimental.ode def mass_spring_damper(arr, F=5.0, k=2.0, m=20.0, c=4.0):     """"""     arr is in the form of [x, \dot{x}]     """"""     return jnp.matmul(jnp.array([[0, 1.0], [k / m, c / m]]), arr) + jnp.array(         [[0], [F / m]]     ) time_array = jnp.linspace(0, 60.0, 1000, dtype=jnp.float64) result = jax.experimental.ode.odeint(     mass_spring_damper,     jnp.array([[0.0], [0.0]]),     t=time_array, ).squeeze(axis=1) fig, ax = plt.subplots(1, 1) plt.plot(time_array, result[:, 0], label=""displacement"", alpha=0.5) plt.plot(time_array, result[:, 1], label=""velocity"", alpha=0.5) ax.set(xlabel=""time (s)"", title=""MassSpringDamper System"") plt.legend() plt.tight_layout() ``` !c3e9ff888c6745abb7430a486b341b55","I see the issue: the call signature for the function passed to odeint is `func(y, t, *args)`: https://github.com/google/jax/blob/f7c915e6a239b87b1e4f35710adb35014cdc940f/jax/experimental/ode.pyL153L154 Your function signature is `func(y, F, *args)`. So you are effectively integrating a function where the external force is increasing monotonically with time, which is enough to overcome the force of the spring trying to pull it back. If you define your function like this instead, I think it should work as expected: ```python def mass_spring_damper(arr, t, F=5.0, k=2.0, m=20.0, c=4.0):   ... ```","I see my mistake now, that's worked perfectly. I should read the docstrings in more detail, thanks so much for looking into this. ``` def mass_spring_damper(arr, t, F=5.0, k=2.0, m=20.0, c=4.0):     """"""     arr is in the form of [x, \dot{x}]     """"""     return jnp.matmul(jnp.array([[0, 1.0], [k / m, c / m]]), arr) + jnp.array(         [[0], [F / m]]     ) ``` !6a934b51ee654207bbe3108789c1f5cb","Great, glad it's working!"
1668,"以下是一个github上的jax下的一个issue, 标题是(Can't initialize cloud TPU on GKE TPU Nodes)， 内容是 ( Description We are trying to run JAX on GKE Nodes. https://cloud.google.com/tpu/docs/systemarchitecturetpuvmtpu_nodes_4 The docker image works fine on the TPU VM instances but the same one never works when we try to run it on the Google Kubernetes Engine TPU Nodes. Following is a part of the error log from GKE. _ > Traceback (most recent call last):\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 335, in backends\n    backend = _init_backend(platform)\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 387, in _init_backend\n    backend = factory()\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 191, in tpu_client_timer_callback\n    client = xla_client.make_tpu_client()\n  File \""/usr/local/lib/python3.9/distpackages/jaxlib/xla_client.py\"", line 126, in make_tpu_client\n    return _xla.get_tpu_client(\njaxlib.xla_extension.XlaRuntimeError: NOT_FOUND: No ba16c7433 device found."" _ As we were able to run the TensorFlow base image and that was able to find the cloud TPU devices following this link https://cloud.google.com/tpu/docs/kubernetesenginesetupjobspec  So the issue is that JAX is not able to find the cloud TPU devices properly for some reason.  What jax/jaxlib version are you using? 0.2.16  Which accelerator(s) are you using? TPU Node supported by GKE  Additional system info Python 3.7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Can't initialize cloud TPU on GKE TPU Nodes," Description We are trying to run JAX on GKE Nodes. https://cloud.google.com/tpu/docs/systemarchitecturetpuvmtpu_nodes_4 The docker image works fine on the TPU VM instances but the same one never works when we try to run it on the Google Kubernetes Engine TPU Nodes. Following is a part of the error log from GKE. _ > Traceback (most recent call last):\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 335, in backends\n    backend = _init_backend(platform)\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 387, in _init_backend\n    backend = factory()\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 191, in tpu_client_timer_callback\n    client = xla_client.make_tpu_client()\n  File \""/usr/local/lib/python3.9/distpackages/jaxlib/xla_client.py\"", line 126, in make_tpu_client\n    return _xla.get_tpu_client(\njaxlib.xla_extension.XlaRuntimeError: NOT_FOUND: No ba16c7433 device found."" _ As we were able to run the TensorFlow base image and that was able to find the cloud TPU devices following this link https://cloud.google.com/tpu/docs/kubernetesenginesetupjobspec  So the issue is that JAX is not able to find the cloud TPU devices properly for some reason.  What jax/jaxlib version are you using? 0.2.16  Which accelerator(s) are you using? TPU Node supported by GKE  Additional system info Python 3.7  NVIDIA GPU info _No response_",2023-01-11T19:09:14Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/13969,https://github.com/google/jax/issues/12917,"GKE is still on the legacy TPU Node architecture, which are different from TPU VMs (see this blog post for a short summary of the difference).  I think you're getting that particular error because you're trying to use a TPU VM setup on a TPU Node. The `jax[tpu]` install doesn't work on TPU Nodes, only TPU VMs. I think a regular `pip install jax jaxlib` would get around that error. However, as discussed in https://github.com/google/jax/issues/12917, JAX doesn't work very well on TPU Nodes, and at this point has only besteffort support. (Thanks  for digging that thread up!) See that thread for options on how to proceed. Feel free to ask about any of them here."
1539,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Behavior of dimension polynomials as hashables)， 内容是 (jax2tf supports shape polymorphism by using dimension polynomials that duck type as integers inside shapes. See documentation. The design of dimension polynomials prioritised soundness vs. completeness. In particular, the comparison `b == 2` raises `InconclusiveDimensionOperation` because neither `True` nor `False` matches all the possible outcomes of the operation should `b` replaced with an arbitrary nonnegative integer. This choice gives us visibility into cases where the user program or the JAX internals would take different paths based on the shapes. However, it creates quite a bit of drag. E.g., we introduced `core.symbolic_equal_dim` to return `False` in those cases where `==` would return `InconclusiveDimensionOperation`. There are plenty of uses of `core.symbolic_equal_dim` in JAX core, but we hope that users would never need to use it. Recently, I learned about another cost of this implementation: dimension polynomials are not wellbehaved hashables. They do implement `__hash__` and `__eq__` but in some rare cases when there are hash collisions and Python needs to invoke `__eq__` you may get an exception instead, for example in code like this: `set(d for d in x.shape)`.  Is there a different implementation choice for dimension polynomials that does not have such a high cost?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Behavior of dimension polynomials as hashables,"jax2tf supports shape polymorphism by using dimension polynomials that duck type as integers inside shapes. See documentation. The design of dimension polynomials prioritised soundness vs. completeness. In particular, the comparison `b == 2` raises `InconclusiveDimensionOperation` because neither `True` nor `False` matches all the possible outcomes of the operation should `b` replaced with an arbitrary nonnegative integer. This choice gives us visibility into cases where the user program or the JAX internals would take different paths based on the shapes. However, it creates quite a bit of drag. E.g., we introduced `core.symbolic_equal_dim` to return `False` in those cases where `==` would return `InconclusiveDimensionOperation`. There are plenty of uses of `core.symbolic_equal_dim` in JAX core, but we hope that users would never need to use it. Recently, I learned about another cost of this implementation: dimension polynomials are not wellbehaved hashables. They do implement `__hash__` and `__eq__` but in some rare cases when there are hash collisions and Python needs to invoke `__eq__` you may get an exception instead, for example in code like this: `set(d for d in x.shape)`.  Is there a different implementation choice for dimension polynomials that does not have such a high cost?",2023-01-11T12:57:38Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/13964,"The hashability became a real problem, with many large tests starting to fail randomly. The first attempt was CC([shape_poly] Fix the hashing and equality of symbolic dimensions) (having a `DimExprHashable` to be used internally and `DimExpr` to be used outside JAX core). This turned out to be hard to land. Not surprisingly, some advanced users needed the same powers that we need internally.  In the end I chose to land CC([shape_poly] Fix the hashing and equality of symbolic dimensions), just make equality return `False` if the symbolic expressions are not evidently equal. This introduces some unsoundness, but turned out to be the simpler solution and hopefully a good compromise. "
2467,"以下是一个github上的jax下的一个issue, 标题是(enforcing eigendecomposition on CPU in TPU process)， 内容是 ( Description Hi, I'm trying to do eigendecomposition of nonhermitian matrix on TPU in Colab. As we know, it's available only on CPU so I enforced eig run on CPU with jit option. I tried 2 options. 1. w/o host_callback 2. w/  host_callback ``` from functools import partial import jax import jax.numpy as jnp import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() from jax.experimental import host_callback (jax.jit, backend='cpu') def eig(mat):     print('eig')     return jnp.linalg.eig(mat) (jax.jit, static_argnums=(1, )) def eig2(matrix, type_complex=jnp.complex128):  TODO: use type_complex arg     """"""Wraps jnp.linalg.eig so that it can be jited on a machine with GPUs.""""""     eigenvalues_shape = jax.ShapeDtypeStruct(matrix.shape[:1], type_complex)     eigenvectors_shape = jax.ShapeDtypeStruct(matrix.shape, type_complex)     return host_callback.call(          We force this computation to be performed on the cpu by jiting and          explicitly specifying the device.         jax.jit(jnp.linalg.eig, device=jax.devices('cpu')[0]),         matrix.astype(type_complex),         result_shape=(eigenvalues_shape, eigenvectors_shape),     ) aa = jnp.arange(9).reshape((3,3)) try:     eig(aa) except Exception as e:     print(1, e) try:     eig(aa) except Exception as e:     print(2, e) try:     eig2(aa) except Exception as e:     print(3, e) ``` Result ``` eig 2 Unable to cast Python instance to C++ type (define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details) 3 'NoneType' object has no attribute 'add_outfeed' :25: UserWarning: Explicitly requested dtype  requested in astype is not available, and will be truncated to dtype complex64. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jaxcurrentgotchas for more.   matrix.astype(type_complex), ``` In the first option, initial run was successful but the second outputs Error about casting. Second option returned 'add_outfeed' error. are they bug or misused?  What jax/jaxlib version are you using? jax 0.3.25,  jaxlib 0.3.25+cuda11.cudnn805  Which accelerator(s) are you using? TPU  Additional system info google colab  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,enforcing eigendecomposition on CPU in TPU process," Description Hi, I'm trying to do eigendecomposition of nonhermitian matrix on TPU in Colab. As we know, it's available only on CPU so I enforced eig run on CPU with jit option. I tried 2 options. 1. w/o host_callback 2. w/  host_callback ``` from functools import partial import jax import jax.numpy as jnp import jax.tools.colab_tpu jax.tools.colab_tpu.setup_tpu() from jax.experimental import host_callback (jax.jit, backend='cpu') def eig(mat):     print('eig')     return jnp.linalg.eig(mat) (jax.jit, static_argnums=(1, )) def eig2(matrix, type_complex=jnp.complex128):  TODO: use type_complex arg     """"""Wraps jnp.linalg.eig so that it can be jited on a machine with GPUs.""""""     eigenvalues_shape = jax.ShapeDtypeStruct(matrix.shape[:1], type_complex)     eigenvectors_shape = jax.ShapeDtypeStruct(matrix.shape, type_complex)     return host_callback.call(          We force this computation to be performed on the cpu by jiting and          explicitly specifying the device.         jax.jit(jnp.linalg.eig, device=jax.devices('cpu')[0]),         matrix.astype(type_complex),         result_shape=(eigenvalues_shape, eigenvectors_shape),     ) aa = jnp.arange(9).reshape((3,3)) try:     eig(aa) except Exception as e:     print(1, e) try:     eig(aa) except Exception as e:     print(2, e) try:     eig2(aa) except Exception as e:     print(3, e) ``` Result ``` eig 2 Unable to cast Python instance to C++ type (define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details) 3 'NoneType' object has no attribute 'add_outfeed' :25: UserWarning: Explicitly requested dtype  requested in astype is not available, and will be truncated to dtype complex64. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jaxcurrentgotchas for more.   matrix.astype(type_complex), ``` In the first option, initial run was successful but the second outputs Error about casting. Second option returned 'add_outfeed' error. are they bug or misused?  What jax/jaxlib version are you using? jax 0.3.25,  jaxlib 0.3.25+cuda11.cudnn805  Which accelerator(s) are you using? TPU  Additional system info google colab  NVIDIA GPU info _No response_",2023-01-11T07:14:10Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/13959,"I suspect this an artifact of TPU colab, which is less well supported than TPU VMs.  ?","Hi , It appears the issue has been resolved on Colab TPU v28. I tested the provided repro by modifying the code according to the updated JAX code base. Replaced the `host_calback.call` with `io_callback`, since `host_callback` is deprecated in JAX version 0.4.26 and removed in JAX 0.4.35. Also changed the `type_complex` argument of `eig2` to `jnp.complex64` instead of `jnp.complex128`, since TPU does not support double precision. The modified code successfully runs on Colab TPU without any exceptions. Please find the Colab gist for reference. Thank you.",This is exciting news. Thanks for the update! 
440,"以下是一个github上的jax下的一个issue, 标题是(DOC: add FAQ entry on converting a tracer to an array)， 内容是 (I've answered versions of this question half a dozen times in recent months. Rendered preview of the new section: https://jax13946.org.readthedocs.build/en/13946/faq.htmlhowcaniconvertajaxtracertoanumpyarray)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,DOC: add FAQ entry on converting a tracer to an array,I've answered versions of this question half a dozen times in recent months. Rendered preview of the new section: https://jax13946.org.readthedocs.build/en/13946/faq.htmlhowcaniconvertajaxtracertoanumpyarray,2023-01-10T19:23:56Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13946
335,"以下是一个github上的jax下的一个issue, 标题是(compute jit number of replicas using least-common-multiple, not max)， 内容是 (fix CC(pmap inside jit causes assert not ragged assertion) (see discussion thread there))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"compute jit number of replicas using least-common-multiple, not max",fix CC(pmap inside jit causes assert not ragged assertion) (see discussion thread there),2023-01-10T05:30:08Z,kokoro:force-run pull ready,open,0,0,https://github.com/jax-ml/jax/issues/13938
5421,"以下是一个github上的jax下的一个issue, 标题是(pmap inside jit causes assert not ragged assertion)， 内容是 ( Description I am aware that it may not be a good idea to use `pmap` inside `jit` https://github.com/google/jax/issues/2926 but I want to use `pmap` inside the cost function passed to `jax.numpy.linalg.cg()` and which jit compiles it for me, so I have no choice... Running the following minimal example on 16 Tesla V100 GPUs demonstrates the problem (here I jit manually): ``` import jax from jax import numpy as np from jax import lax for batches in [         [np.ones((2, 2)), np.ones((14, 1))],  works         [np.ones((3, 2))],                    works         [np.ones((13, 1))],                   works         [np.ones((13, 1)), np.ones((3, 2))],  pmap fails ]:     def compute_batch(batch):         return np.ones((7))     .jit     def pmap_over_batches(batches):         ret = [             jax.pmap(                 compute_batch,                 devices=jax.devices()[:len(batch)],                 )(batch).flatten()             for batch in batches         ]         return ret     pmap_ret = pmap_over_batches(batches)     print(""pmap done"", [batch.shape for batch in batches], pmap_ret) ``` The third value in the for loop triggers a `assert not ragged` assertion  ``` Traceback (most recent call last):   File ""test_pmap.py"", line 39, in      pmap_ret = pmap_over_batches(batches)   File ""test_pmap.py"", line 28, in pmap_over_batches     ret = [   File ""test_pmap.py"", line 29, in      jax.pmap( jax._src.source_info_util.JaxStackTraceBeforeTransformation: AssertionError The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test_pmap.py"", line 39, in      pmap_ret = pmap_over_batches(batches)   File ""/.../lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/.../lib/python3.8/sitepackages/jax/_src/api.py"", line 473, in cache_miss     out_flat = xla.xla_call(   File ""/.../lib/python3.8/sitepackages/jax/core.py"", line 1767, in bind     return call_bind(self, fun, *args, **params)   File ""/.../lib/python3.8/sitepackages/jax/core.py"", line 1783, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/.../lib/python3.8/sitepackages/jax/core.py"", line 680, in process_call     return primitive.impl(f, *tracers, **params)   File ""/.../lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 182, in _xla_call_impl     compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,   File ""/.../lib/python3.8/sitepackages/jax/linear_util.py"", line 285, in memoized_fun     ans = call(fun, *args)   File ""/.../lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 230, in _xla_callable_uncached     return lower_xla_callable(fun, device, backend, name, donated_invars, False,   File ""/.../lib/python3.8/sitepackages/jax/_src/profiler.py"", line 206, in wrapper     return func(*args, **kwargs)   File ""/.../lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 340, in lower_xla_callable     module, keepalive = mlir.lower_jaxpr_to_module(   File ""/.../lib/python3.8/sitepackages/jax/interpreters/mlir.py"", line 546, in lower_jaxpr_to_module     lower_jaxpr_to_fun(   File ""/.../lib/python3.8/sitepackages/jax/interpreters/mlir.py"", line 800, in lower_jaxpr_to_fun     out_vals, tokens_out = jaxpr_subcomp(ctx.replace(name_stack=callee_name_stack),   File ""/.../lib/python3.8/sitepackages/jax/interpreters/mlir.py"", line 918, in jaxpr_subcomp     ans = rule(rule_ctx, *map(_unwrap_singleton_ir_values, in_nodes),   File ""/.../lib/python3.8/sitepackages/jax/interpreters/pxla.py"", line 1700, in _pmap_lowering     outs = [_mhlo_unshard(aval, new_env, out_axis, shard,   File ""/.../lib/python3.8/sitepackages/jax/interpreters/pxla.py"", line 1700, in      outs = [_mhlo_unshard(aval, new_env, out_axis, shard,   File ""/.../lib/python3.8/sitepackages/jax/interpreters/pxla.py"", line 1649, in _mhlo_unshard     xla.axis_groups(axis_env, axis_env.names[1]))   File ""/.../lib/python3.8/sitepackages/jax/interpreters/xla.py"", line 360, in axis_groups     assert not ragged jax._src.traceback_util.UnfilteredStackTrace: AssertionError The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test_pmap.py"", line 39, in      pmap_ret = pmap_over_batches(batches) AssertionError ``` very similar to the one reported here for `scan` https://github.com/google/jax/issues/2018 . Other batch shapes also trigger the error. Some work, some don't. I wasn't able to find any pattern. The same error is triggered when spoofing 16 devices in a CPU only environment. Because of the advice here https://github.com/google/jax/issues/13858 I have also tried v0.3.10 but got the same result as on v0.4.1.  What jax/jaxlib version are you using? jax 0.4.1, jaxlib 0.4.1+cuda11.cudnn86  Which accelerator(s) are you using? CPU, GPU  Additional system info Python 3.8.12, Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,pmap inside jit causes assert not ragged assertion," Description I am aware that it may not be a good idea to use `pmap` inside `jit` https://github.com/google/jax/issues/2926 but I want to use `pmap` inside the cost function passed to `jax.numpy.linalg.cg()` and which jit compiles it for me, so I have no choice... Running the following minimal example on 16 Tesla V100 GPUs demonstrates the problem (here I jit manually): ``` import jax from jax import numpy as np from jax import lax for batches in [         [np.ones((2, 2)), np.ones((14, 1))],  works         [np.ones((3, 2))],                    works         [np.ones((13, 1))],                   works         [np.ones((13, 1)), np.ones((3, 2))],  pmap fails ]:     def compute_batch(batch):         return np.ones((7))     .jit     def pmap_over_batches(batches):         ret = [             jax.pmap(                 compute_batch,                 devices=jax.devices()[:len(batch)],                 )(batch).flatten()             for batch in batches         ]         return ret     pmap_ret = pmap_over_batches(batches)     print(""pmap done"", [batch.shape for batch in batches], pmap_ret) ``` The third value in the for loop triggers a `assert not ragged` assertion  ``` Traceback (most recent call last):   File ""test_pmap.py"", line 39, in      pmap_ret = pmap_over_batches(batches)   File ""test_pmap.py"", line 28, in pmap_over_batches     ret = [   File ""test_pmap.py"", line 29, in      jax.pmap( jax._src.source_info_util.JaxStackTraceBeforeTransformation: AssertionError The preceding stack trace is the source of the JAX operation that, once transformed by JAX, triggered the following exception.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test_pmap.py"", line 39, in      pmap_ret = pmap_over_batches(batches)   File ""/.../lib/python3.8/sitepackages/jax/_src/traceback_util.py"", line 162, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/.../lib/python3.8/sitepackages/jax/_src/api.py"", line 473, in cache_miss     out_flat = xla.xla_call(   File ""/.../lib/python3.8/sitepackages/jax/core.py"", line 1767, in bind     return call_bind(self, fun, *args, **params)   File ""/.../lib/python3.8/sitepackages/jax/core.py"", line 1783, in call_bind     outs = top_trace.process_call(primitive, fun_, tracers, params)   File ""/.../lib/python3.8/sitepackages/jax/core.py"", line 680, in process_call     return primitive.impl(f, *tracers, **params)   File ""/.../lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 182, in _xla_call_impl     compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,   File ""/.../lib/python3.8/sitepackages/jax/linear_util.py"", line 285, in memoized_fun     ans = call(fun, *args)   File ""/.../lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 230, in _xla_callable_uncached     return lower_xla_callable(fun, device, backend, name, donated_invars, False,   File ""/.../lib/python3.8/sitepackages/jax/_src/profiler.py"", line 206, in wrapper     return func(*args, **kwargs)   File ""/.../lib/python3.8/sitepackages/jax/_src/dispatch.py"", line 340, in lower_xla_callable     module, keepalive = mlir.lower_jaxpr_to_module(   File ""/.../lib/python3.8/sitepackages/jax/interpreters/mlir.py"", line 546, in lower_jaxpr_to_module     lower_jaxpr_to_fun(   File ""/.../lib/python3.8/sitepackages/jax/interpreters/mlir.py"", line 800, in lower_jaxpr_to_fun     out_vals, tokens_out = jaxpr_subcomp(ctx.replace(name_stack=callee_name_stack),   File ""/.../lib/python3.8/sitepackages/jax/interpreters/mlir.py"", line 918, in jaxpr_subcomp     ans = rule(rule_ctx, *map(_unwrap_singleton_ir_values, in_nodes),   File ""/.../lib/python3.8/sitepackages/jax/interpreters/pxla.py"", line 1700, in _pmap_lowering     outs = [_mhlo_unshard(aval, new_env, out_axis, shard,   File ""/.../lib/python3.8/sitepackages/jax/interpreters/pxla.py"", line 1700, in      outs = [_mhlo_unshard(aval, new_env, out_axis, shard,   File ""/.../lib/python3.8/sitepackages/jax/interpreters/pxla.py"", line 1649, in _mhlo_unshard     xla.axis_groups(axis_env, axis_env.names[1]))   File ""/.../lib/python3.8/sitepackages/jax/interpreters/xla.py"", line 360, in axis_groups     assert not ragged jax._src.traceback_util.UnfilteredStackTrace: AssertionError The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""test_pmap.py"", line 39, in      pmap_ret = pmap_over_batches(batches) AssertionError ``` very similar to the one reported here for `scan` https://github.com/google/jax/issues/2018 . Other batch shapes also trigger the error. Some work, some don't. I wasn't able to find any pattern. The same error is triggered when spoofing 16 devices in a CPU only environment. Because of the advice here https://github.com/google/jax/issues/13858 I have also tried v0.3.10 but got the same result as on v0.4.1.  What jax/jaxlib version are you using? jax 0.4.1, jaxlib 0.4.1+cuda11.cudnn86  Which accelerator(s) are you using? CPU, GPU  Additional system info Python 3.8.12, Linux  NVIDIA GPU info _No response_",2023-01-09T19:09:30Z,bug,open,1,5,https://github.com/jax-ml/jax/issues/13931,"Thanks for raising this, with the clear reproducer! The issue is that we compute the number of replicas needed by the `jit`ted computation with a `max` over all the numbers of replicas needed by each pmap inside it, but actually we need a least common multiple. Here's another reproducer: ```python import jax import jax.numpy as jnp .jit def f():   a = jax.pmap(lambda x: x)(jnp.arange(2))   b = jax.pmap(lambda x: x)(jnp.arange(3))   return a, b f() ```","Actually, we don't really need a leastcommonmultiple here. We just need that for nested pmaps; having a max across separate pmaps is right. So the CC(compute jit number of replicas using leastcommonmultiple, not max) fix isn't ideal yet: it'll pass these tests, but require too many devices. For example, for the code in the OP to run would require lcm(3, 13) = 39 devices. But the 'axis_groups' calculations we perform while lowering to HLO _do_ currently require this leastcommonmultiple logic. So to require fewer devices, we'll need to adapt that logic to create unevenly sized axis groups. (That's basically what the assertion is checking will never happen.)","> but I want to use pmap inside the cost function passed to jax.numpy.linalg.cg() and which jit compiles it for me, so I have no choice... You meant `jax.scipy.sparse.linalg.cg` right?","Awesome! Thanks for looking into this so quickly! Yes I mean `jax.scipy.sparse.linalg.cg`. I was under the impression that the number of devices must match the 0th axis of `batch`, so for the case `[np.ones((13, 1)), np.ones((3, 2))]` the frst `pmap` in the list comprehension would run on 13 devices and the second on 3. Is that no longer true after the function has been jit compiled?  In the actual use case I have a function that needs to compute a number of things that are not a multiple of the number of devices with `pmap` inside the cost function passed to `jax.scipy.sparse.linalg.cg`. I want to do that by calling `pmap` twice. Once with a batch whose 0th axis is equal to the number of devices and then a second call to spread the computation of the remained over a subset of devices.","Is there any chance of getting a full fix for this, which also solves the problem of requiring an excessive amount of devices? Or maybe a workaround how `pmap` could be called from a `jit` compiled function that avoids the problem in the first place? Thanks very much for your time and effort!"
3862,"以下是一个github上的jax下的一个issue, 标题是(Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.20.1)， 内容是 (Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.20.1 What's Changed  Fixed default options not displaying for parameters without type hints. by @​BhavyeMathur in toxdev/sphinxautodoctypehints CC(added support for np.newaxis to jax.numpy)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.20.0...1.20.1    Changelog Sourced from sphinxautodoctypehints's changelog.  1.20.1  Fixed default options not displaying for parameters without type hints.  1.20  Use hatchling instead of setuptools Add support for typing.ParamSpec Allow star prefixes for parameter names in docstring  1.19.2  Fix incorrect domain used for collections.abc.Callable.  1.19.1  Fix bug for recursive type alias.  1.19.0  Support for CPython 3.11, no longer adds Optional when the argument is default per recommendation from PEP484.  1.18.3  Support and require nptyping&gt;=2.1.2  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked module import not working when used as guarded import  1.18.0  Support and require nptyping&gt;=2 Handle UnionType  1.17.1  Mark it as requiring nptyping&lt;2  1.17.0  Add typehints_use_rtype option Handles TypeError when getting source code via inspect  1.16.0   ... (truncated)   Commits  2747ae2 Update CHANGELOG.md 4e6bc55 Fixed default options not displaying for parameters without type hints. ( CC(added support for np.newaxis to jax.numpy)) 32dc422 Added options to retain original typehints in signatures ( CC(jax.numpy array indexing has different outofbounds behavior to numpy)) 477c438 Bump pypa/ghactionpypipublish from 1.6.1 to 1.6.4 ( CC(AvgPool does only work as global average pooling)) 7ec1a6c Bump pypa/ghactionpypipublish from 1.5.2 to 1.6.1 ( CC(enable valueandgrad api test)) c61094f Bump pypa/ghactionpypipublish from 1.5.1 to 1.5.2 ( CC(Undefined name: 'value_and_grad' in ./tests/api_test.py)) 7e467a1 [precommit.ci] precommit autoupdate ( CC(Add multivariate_normal and pdf to jax.scipy.stats)) 9d67010 Add link to pyprojectapi docs as example of this extension ( CC(Update Jaxlib references to 0.1.6.)) cec0441 [precommit.ci] precommit autoupdate ( CC(Update XLA release.)) 65777cb Bump tools and deps Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.20.1,"Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.20.1 What's Changed  Fixed default options not displaying for parameters without type hints. by @​BhavyeMathur in toxdev/sphinxautodoctypehints CC(added support for np.newaxis to jax.numpy)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.20.0...1.20.1    Changelog Sourced from sphinxautodoctypehints's changelog.  1.20.1  Fixed default options not displaying for parameters without type hints.  1.20  Use hatchling instead of setuptools Add support for typing.ParamSpec Allow star prefixes for parameter names in docstring  1.19.2  Fix incorrect domain used for collections.abc.Callable.  1.19.1  Fix bug for recursive type alias.  1.19.0  Support for CPython 3.11, no longer adds Optional when the argument is default per recommendation from PEP484.  1.18.3  Support and require nptyping&gt;=2.1.2  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked module import not working when used as guarded import  1.18.0  Support and require nptyping&gt;=2 Handle UnionType  1.17.1  Mark it as requiring nptyping&lt;2  1.17.0  Add typehints_use_rtype option Handles TypeError when getting source code via inspect  1.16.0   ... (truncated)   Commits  2747ae2 Update CHANGELOG.md 4e6bc55 Fixed default options not displaying for parameters without type hints. ( CC(added support for np.newaxis to jax.numpy)) 32dc422 Added options to retain original typehints in signatures ( CC(jax.numpy array indexing has different outofbounds behavior to numpy)) 477c438 Bump pypa/ghactionpypipublish from 1.6.1 to 1.6.4 ( CC(AvgPool does only work as global average pooling)) 7ec1a6c Bump pypa/ghactionpypipublish from 1.5.2 to 1.6.1 ( CC(enable valueandgrad api test)) c61094f Bump pypa/ghactionpypipublish from 1.5.1 to 1.5.2 ( CC(Undefined name: 'value_and_grad' in ./tests/api_test.py)) 7e467a1 [precommit.ci] precommit autoupdate ( CC(Add multivariate_normal and pdf to jax.scipy.stats)) 9d67010 Add link to pyprojectapi docs as example of this extension ( CC(Update Jaxlib references to 0.1.6.)) cec0441 [precommit.ci] precommit autoupdate ( CC(Update XLA release.)) 65777cb Bump tools and deps Additional commits viewable in compare view    Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting ` rebase`. [//]:  (dependabotautomergestart) [//]:  (dependabotautomergeend)   Dependabot commands and options  You can trigger Dependabot actions by commenting on this PR:  ` rebase` will rebase this PR  ` recreate` will recreate this PR, overwriting any edits that have been made to it  ` merge` will merge this PR after your CI passes on it  ` squash and merge` will squash and merge this PR after your CI passes on it  ` cancel merge` will cancel a previously requested merge and block automerging  ` reopen` will reopen this PR if it is closed  ` close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually  ` ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)  ` ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)  ` ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) ",2023-01-09T17:02:35Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/13928,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it.",Not possible for the reason given in the comment above the line in question.
343,"以下是一个github上的jax下的一个issue, 标题是(Enable jax2tf strided_slice test requiring dynamism calculation)， 内容是 (Enable jax2tf strided_slice test requiring dynamism calculation The underlying issue has been fixed.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Enable jax2tf strided_slice test requiring dynamism calculation,Enable jax2tf strided_slice test requiring dynamism calculation The underlying issue has been fixed.,2023-01-09T06:48:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13920
2603,"以下是一个github上的jax下的一个issue, 标题是(When lowering with repeated inputs the compiled function fails only after a second call.)， 内容是 ( Description I was incorrectly setting my training loop and compiling ahead of time, I used by mistake a reference to the same pytree twice and got errors in an unexpected way when calling the compiled function. I managed to reduce to the following: ```python In [1]: import jax In [2]: import jaxlib In [3]: import jax.numpy as jnp In [4]: jax.__version__ Out[4]: '0.4.1' In [5]: jaxlib.__version__ Out[5]: '0.4.1' In [6]: x = jnp.array(0) In [7]: y = x In [8]: def f(x, y):    ...:     return x    ...:  In [9]: f_ = jax.jit(f).lower(x, y).compile() In [10]: f_(x, y) Out[10]: Array(0, dtype=int32, weak_type=True) In [11]: f_(x, y) 20230109 06:52:44.191879: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2163] Execution of replica 0 failed: INVALID_ARGUMENT: Executable expected 1 arguments but got 2  ValueError                                Traceback (most recent call last) Input In [11], in () > 1 f_(x, y) File ~/miniconda3/lib/python3.9/sitepackages/jax/_src/stages.py:504, in Compiled.__call__(self, *args, **kwargs)     502 def __call__(self, *args, **kwargs):     503   if self._cpp_call is not None: > 504     return self._cpp_call(*args, **kwargs)     506   outs, _ = Compiled.call(self._params, *args, **kwargs)     507   return outs ValueError: INVALID_ARGUMENT: Executable expected 1 arguments but got 2 ``` I was originally getting a similar error message but saying `buffers` instead of `arguments`.  I solved my problem before detecting the repeated reference by using `keep_unused=True`, but nothing was telling me that the function was removing an argument, and the function failed only at the second call, so, it was particularly hard to catch. Here is an example in Colab: https://colab.research.google.com/gist/jorgeecardona/6c2c32a26e0176317b4ec204aa7bb07f/bugexample.ipynbscrollTo=bC0Ceqwetddh I would expect either a clear message of which variables are removed when lowering, and to fail on the first call to the compiled function (unless I am missing an extra step here, I don't see a clear reason why the function fails only at the second call)  What jax/jaxlib version are you using? 0.4.1  Which accelerator(s) are you using? GPU  Additional system info python 3.9.12 in conda, linux 6, debian sid  NVIDIA GPU info ``` Mon Jan  9 07:05:52 2023        ++  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,When lowering with repeated inputs the compiled function fails only after a second call.," Description I was incorrectly setting my training loop and compiling ahead of time, I used by mistake a reference to the same pytree twice and got errors in an unexpected way when calling the compiled function. I managed to reduce to the following: ```python In [1]: import jax In [2]: import jaxlib In [3]: import jax.numpy as jnp In [4]: jax.__version__ Out[4]: '0.4.1' In [5]: jaxlib.__version__ Out[5]: '0.4.1' In [6]: x = jnp.array(0) In [7]: y = x In [8]: def f(x, y):    ...:     return x    ...:  In [9]: f_ = jax.jit(f).lower(x, y).compile() In [10]: f_(x, y) Out[10]: Array(0, dtype=int32, weak_type=True) In [11]: f_(x, y) 20230109 06:52:44.191879: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2163] Execution of replica 0 failed: INVALID_ARGUMENT: Executable expected 1 arguments but got 2  ValueError                                Traceback (most recent call last) Input In [11], in () > 1 f_(x, y) File ~/miniconda3/lib/python3.9/sitepackages/jax/_src/stages.py:504, in Compiled.__call__(self, *args, **kwargs)     502 def __call__(self, *args, **kwargs):     503   if self._cpp_call is not None: > 504     return self._cpp_call(*args, **kwargs)     506   outs, _ = Compiled.call(self._params, *args, **kwargs)     507   return outs ValueError: INVALID_ARGUMENT: Executable expected 1 arguments but got 2 ``` I was originally getting a similar error message but saying `buffers` instead of `arguments`.  I solved my problem before detecting the repeated reference by using `keep_unused=True`, but nothing was telling me that the function was removing an argument, and the function failed only at the second call, so, it was particularly hard to catch. Here is an example in Colab: https://colab.research.google.com/gist/jorgeecardona/6c2c32a26e0176317b4ec204aa7bb07f/bugexample.ipynbscrollTo=bC0Ceqwetddh I would expect either a clear message of which variables are removed when lowering, and to fail on the first call to the compiled function (unless I am missing an extra step here, I don't see a clear reason why the function fails only at the second call)  What jax/jaxlib version are you using? 0.4.1  Which accelerator(s) are you using? GPU  Additional system info python 3.9.12 in conda, linux 6, debian sid  NVIDIA GPU info ``` Mon Jan  9 07:05:52 2023        ++  ++ ```",2023-01-09T06:15:08Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/13918,"Hi   It looks like this issue has been resolved in the latest JAX versions. I executed the mentioned code in JAX version 0.4.24 without any errors: ```Python >>> import jax >>> import jaxlib >>> import jax.numpy as jnp >>>  >>> print(f""jax version: {jax.__version__}"") jax version: 0.4.24 >>> print(f""jaxlib version: {jaxlib.__version__}"") jaxlib version: 0.4.24 >>> x = jnp.array(0) >>> y = x >>> def f(x, y): ...     return x ...  >>> f_ = jax.jit(f).lower(x, y).compile() >>> f_(x, y) Array(0, dtype=int32, weak_type=True) >>> f_(x, y) Array(0, dtype=int32, weak_type=True) >>>  ``` This Gist also demonstrates successful execution of the code in Colab using JAX version 0.4.23.",Thanks for following up   it looks like this issue can be closed!
650,"以下是一个github上的jax下的一个issue, 标题是(Can't import stax from jax)， 内容是 ( Description I am bit new in jax and I was trying to import the **stax** library from **Jax** library. But, it keeps giving me this error: ```Python ImportError: cannot import name 'stax' from 'jax' ``` Please, help me to solve this error :) **Systems requirements:** Jax latest version  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Windows  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Can't import stax from jax," Description I am bit new in jax and I was trying to import the **stax** library from **Jax** library. But, it keeps giving me this error: ```Python ImportError: cannot import name 'stax' from 'jax' ``` Please, help me to solve this error :) **Systems requirements:** Jax latest version  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Windows  NVIDIA GPU info _No response_",2023-01-06T13:10:10Z,bug,closed,1,2,https://github.com/jax-ml/jax/issues/13896,"`jax.experimental.stax` was moved to `jax.example_libraries.stax` in in JAX v0.2.25 and the deprecated alias was removed in JAX v0.3.16. In current JAX versions, you can import it like this: ```python from jax.example_libraries import stax ```","That answer my question, thank!"
2334,"以下是一个github上的jax下的一个issue, 标题是(Hand Vectorisation Gives Unexpected Poor Performance)， 内容是 ( Description Hi there The JAX Team, thanks for your patience and support! I am finding inconsistent performance when I hand vectorize code. Here is a simple example writing alternatives to `np.hypot` for a very specific input shape of coordinates.  ```python import jax  def coords(n: int, rad: float) > float:     arange: float = jax.lax.iota(float, n)     max_: float = np.array(n  1, dtype=float)     axes: float = arange * 2. * rad / max_  rad     s: int = axes.size     shape: tuple = (1, s, s)      x: float = jax.lax.broadcast_in_dim(axes, shape, (2,))     y: float = jax.lax.broadcast_in_dim(axes, shape, (1,))     return jax.lax.concatenate([x, y], 0) .jit def hypotenuse_v0(ccoords: float) > float:     return jax.lax.sqrt(jax.lax.integer_pow(ccoords, 2).sum(axis = 0)) .jit def hypotenuse_v1(ccoords: float) > float:     x: float = ccoords[0]     y: float = ccoords[1]     return np.hypot(x, y) .jit def hypotenuse_v2(ccoords: float) > float:     x: float = jax.lax.index_in_dim(ccoords, 0)     y: float = jax.lax.index_in_dim(ccoords, 1)     x_sq: float = jax.lax.integer_pow(x, 2)     y_sq: float = jax.lax.integer_pow(y, 2)     return jax.lax.sqrt(x_sq + y_sq) ccoords: float = coords(100, 1.) %%timeit hypotenuse_v0(ccoords) %%timeit hypotenuse_v1(ccoords) %%timeit hypotenuse_v2(ccoords) ``` On my device (:sigh: not those keywords) I get the following times  !image What's more, I can't seem to wrap my head around how the timing relates to the `jaxpr` representation. I was not expecting to be able to predict times perfectly, but I was expecting more complex `jaxpr`, with more visited instructions of similar size, to take longer, but this does not seem to be the case. Is there any merit in trying to ""optimise"" the `jaxpr` i.e. reduce the number of instructions. Obviously some instructions take more time than others, is there any way of knowing which instructions to avoid? Regards Jordan  What jax/jaxlib version are you using? jax 0.4.1 and jaxlib 0.4.1  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8 Ubuntu 22.04  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Hand Vectorisation Gives Unexpected Poor Performance," Description Hi there The JAX Team, thanks for your patience and support! I am finding inconsistent performance when I hand vectorize code. Here is a simple example writing alternatives to `np.hypot` for a very specific input shape of coordinates.  ```python import jax  def coords(n: int, rad: float) > float:     arange: float = jax.lax.iota(float, n)     max_: float = np.array(n  1, dtype=float)     axes: float = arange * 2. * rad / max_  rad     s: int = axes.size     shape: tuple = (1, s, s)      x: float = jax.lax.broadcast_in_dim(axes, shape, (2,))     y: float = jax.lax.broadcast_in_dim(axes, shape, (1,))     return jax.lax.concatenate([x, y], 0) .jit def hypotenuse_v0(ccoords: float) > float:     return jax.lax.sqrt(jax.lax.integer_pow(ccoords, 2).sum(axis = 0)) .jit def hypotenuse_v1(ccoords: float) > float:     x: float = ccoords[0]     y: float = ccoords[1]     return np.hypot(x, y) .jit def hypotenuse_v2(ccoords: float) > float:     x: float = jax.lax.index_in_dim(ccoords, 0)     y: float = jax.lax.index_in_dim(ccoords, 1)     x_sq: float = jax.lax.integer_pow(x, 2)     y_sq: float = jax.lax.integer_pow(y, 2)     return jax.lax.sqrt(x_sq + y_sq) ccoords: float = coords(100, 1.) %%timeit hypotenuse_v0(ccoords) %%timeit hypotenuse_v1(ccoords) %%timeit hypotenuse_v2(ccoords) ``` On my device (:sigh: not those keywords) I get the following times  !image What's more, I can't seem to wrap my head around how the timing relates to the `jaxpr` representation. I was not expecting to be able to predict times perfectly, but I was expecting more complex `jaxpr`, with more visited instructions of similar size, to take longer, but this does not seem to be the case. Is there any merit in trying to ""optimise"" the `jaxpr` i.e. reduce the number of instructions. Obviously some instructions take more time than others, is there any way of knowing which instructions to avoid? Regards Jordan  What jax/jaxlib version are you using? jax 0.4.1 and jaxlib 0.4.1  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8 Ubuntu 22.04  NVIDIA GPU info _No response_",2023-01-05T23:36:08Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/13891,"Sorry, the times I gave were for `1024` not `100`.","In general I wouldn't expect the length of a jaxpr to always be directly correlated with the runtime, for a couple reasons:  individual primitives require widely varying numbers of flops. As an extreme example, `lax.linalg.svd` and `lax.neg` both lower to a single primitive, but will have vastly different runtimes as the size of the input increases.  even if you're looking at operations with comparable runtimes, the jaxpr doesn't directly map onto the eventual operations done in the compiled code. In general the compiler is able to fuse, elide, and otherwise optimize sequences of operations, and this is not reflected in the jaxpr. That said, it's hard to guess by looking at code (or jaxprs) exactly how fast it will execute, because it has to do with the details of the compiler and the execution engine. For example, when I run your benchmark on a GPU, I find the three approaches to be essentially indistinguishable in performance ```python %timeit hypotenuse_v0(ccoords).block_until_ready()  161 µs ± 993 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each) %timeit hypotenuse_v1(ccoords).block_until_ready()  161 µs ± 1.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) %timeit hypotenuse_v2(ccoords).block_until_ready()  161 µs ± 1.19 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) ``` This is likely due to the fact that the XLA:GPU compiler has had seen a lot more development effort that the XLA:CPU at this point, and so can better target the hardware for each particular sequence of operations. So overall, my advice would be to write code in a way that is as clear as possible, don't worry about the jaxpr, and let the compiler worry about the optimization.","Thanks for the feedback on the `jaxpr`. I thought it translated a little more closely to the machine, but that matches the actual results I have been getting. I'll chalk this up as a machine problem and avoid vectorising code that I am only going to run locally.  Thanks again, Jordan","I should have added one thing: if you want to get a better idea of what the compiler is doing with your code, there are APIs for that. for example: ```python print(jax.jit(hypotenuse_v2).lower(ccoords).compile().as_text()) ``` ``` HloModule jit_hypotenuse_v2, entry_computation_layout={(f32[2,1024,1024]{2,1,0})>f32[1,1024,1024]{2,1,0}} %fused_computation (param_0.3: f32[2,1024,1024]) > f32[1,1024,1024] {   %param_0.3 = f32[2,1024,1024]{2,1,0} parameter(0)   %slice.7 = f32[1,1024,1024]{2,1,0} slice(f32[2,1024,1024]{2,1,0} %param_0.3), slice={[0:1], [0:1024], [0:1024]}, metadata={op_name=""jit(hypotenuse_v2)/jit(main)/jit(hypotenuse_v2)/slice[start_indices=(0, 0, 0) limit_indices=(1, 1024, 1024) strides=(1, 1, 1)]"" source_file="""" source_line=28}   %bitcast.16 = f32[1024,1024]{1,0} bitcast(f32[1,1024,1024]{2,1,0} %slice.7)   %multiply.5 = f32[1024,1024]{1,0} multiply(f32[1024,1024]{1,0} %bitcast.16, f32[1024,1024]{1,0} %bitcast.16), metadata={op_name=""jit(hypotenuse_v2)/jit(main)/jit(hypotenuse_v2)/mul"" source_file="""" source_line=30}   %slice.6 = f32[1,1024,1024]{2,1,0} slice(f32[2,1024,1024]{2,1,0} %param_0.3), slice={[1:2], [0:1024], [0:1024]}, metadata={op_name=""jit(hypotenuse_v2)/jit(main)/jit(hypotenuse_v2)/slice[start_indices=(1, 0, 0) limit_indices=(2, 1024, 1024) strides=(1, 1, 1)]"" source_file="""" source_line=29}   %bitcast.15 = f32[1024,1024]{1,0} bitcast(f32[1,1024,1024]{2,1,0} %slice.6)   %multiply.4 = f32[1024,1024]{1,0} multiply(f32[1024,1024]{1,0} %bitcast.15, f32[1024,1024]{1,0} %bitcast.15), metadata={op_name=""jit(hypotenuse_v2)/jit(main)/jit(hypotenuse_v2)/mul"" source_file="""" source_line=31}   %add.2 = f32[1024,1024]{1,0} add(f32[1024,1024]{1,0} %multiply.5, f32[1024,1024]{1,0} %multiply.4), metadata={op_name=""jit(hypotenuse_v2)/jit(main)/jit(hypotenuse_v2)/add"" source_file="""" source_line=32}   %sqrt.2 = f32[1024,1024]{1,0} sqrt(f32[1024,1024]{1,0} %add.2), metadata={op_name=""jit(hypotenuse_v2)/jit(main)/jit(hypotenuse_v2)/sqrt"" source_file="""" source_line=32}   ROOT %bitcast.14 = f32[1,1024,1024]{2,1,0} bitcast(f32[1024,1024]{1,0} %sqrt.2), metadata={op_name=""jit(hypotenuse_v2)/jit(main)/jit(hypotenuse_v2)/sqrt"" source_file="""" source_line=32} } ENTRY %main.11 (Arg_0.1: f32[2,1024,1024]) > f32[1,1024,1024] {   %Arg_0.1 = f32[2,1024,1024]{2,1,0} parameter(0)   ROOT %fusion = f32[1,1024,1024]{2,1,0} fusion(f32[2,1024,1024]{2,1,0} %Arg_0.1), kind=kLoop, calls=%fused_computation, metadata={op_name=""jit(hypotenuse_v2)/jit(main)/jit(hypotenuse_v2)/sqrt"" source_file="""" source_line=32} } ``` Raw HLO is definitely not the easiest thing to read, but this can help you get a better idea of the low level execution of your code.","Awesome thanks,  I nearly asked how to view the assembly but this is probably better. Is HLO separate from `mlir` or is this `mlir` HLO (possibly a dumb question)? Regards Jordan","I believe that in the current version of JAX, this is MHLO, which is also used by MLIR, but  would know the details of that."
370,"以下是一个github上的jax下的一个issue, 标题是(Make Shard.device and Shard.data read-only properties.)， 内容是 (A user was confused when modifying `x.addressable_shards.data`, since it doesn't modify the underlying data buffers (and it shouldn't!).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Make Shard.device and Shard.data read-only properties.,"A user was confused when modifying `x.addressable_shards.data`, since it doesn't modify the underlying data buffers (and it shouldn't!).",2023-01-05T14:42:02Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13881
545,"以下是一个github上的jax下的一个issue, 标题是(GPU tests: disable cusparse_lowering to avoid segfaults.)， 内容是 (We've started seeing failures in the GPU CI jobs: https://source.cloud.google.com/results/invocations/8a57fcfe439c4fea9434ae71d017625e/targets/jax%2Ftesting%2Fcpu%2Fpresubmit_github/log I suspect this is due to increased test coverage, and similar to the other cusparserelated GPU segfaults we've been seeing.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,GPU tests: disable cusparse_lowering to avoid segfaults.,"We've started seeing failures in the GPU CI jobs: https://source.cloud.google.com/results/invocations/8a57fcfe439c4fea9434ae71d017625e/targets/jax%2Ftesting%2Fcpu%2Fpresubmit_github/log I suspect this is due to increased test coverage, and similar to the other cusparserelated GPU segfaults we've been seeing.",2023-01-04T19:10:57Z,,closed,0,1,https://github.com/jax-ml/jax/issues/13863,"Hmm, looks like this wasn't the issue. Now that I look more closely, it's the CPU test that's failing, not the GPU test. I suspect it's probably a timeout rather than a cusparse issue."
848,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: _src when importing jax)， 内容是 ( Description I'm trying to use `jax` in a brand new virtual environment. I have `jax 0.4.1` and `jaxlib 0.4.1` . When I try to import jax.  ``` File ~/git_supply/monopsonistic_nk/code/quantitative_model/venv/lib/python3.9/sitepackages/jax/__init__.py:162     158 from jax import util as util     160 import jax.lib   TODO(phawkins): remove this export. > 162 del jax._src AttributeError: _src ``` If I comment this line in the `__init__` file then everything works fine.   What jax/jaxlib version are you using? 0.4.1  Which accelerator(s) are you using? _No response_  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AttributeError: _src when importing jax, Description I'm trying to use `jax` in a brand new virtual environment. I have `jax 0.4.1` and `jaxlib 0.4.1` . When I try to import jax.  ``` File ~/git_supply/monopsonistic_nk/code/quantitative_model/venv/lib/python3.9/sitepackages/jax/__init__.py:162     158 from jax import util as util     160 import jax.lib   TODO(phawkins): remove this export. > 162 del jax._src AttributeError: _src ``` If I comment this line in the `__init__` file then everything works fine.   What jax/jaxlib version are you using? 0.4.1  Which accelerator(s) are you using? _No response_  Additional system info Linux  NVIDIA GPU info _No response_,2023-01-04T11:04:15Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/13857,"Thanks for the report – this is a very strange error and I'm not sure what might be causing it. Can you say more about what exactly you are executing, and how you created your environment?","I faced this issue too, when I tried to install the package from within IPython console.. but went off, when restarted the pykernel.","Thanks for the info. With that as a clue, I've been able to reproduce the issue this way: ```python In [1]: import jax In [2]: import importlib In [3]: importlib.reload(jax)  AttributeError                            Traceback (most recent call last) File ~/github/google/jax/jax/__init__.py:166     162 from jax import util as util     164 import jax.lib   TODO(phawkins): remove this export. > 166 del jax._src AttributeError: _src ``` also like this: ```python In [1]: import jax imp In [2]: import sys In [3]: del sys.modules['jax'] In [4]: import jax  AttributeError                            Traceback (most recent call last) Cell In [4], line 1 > 1 import jax File ~/github/google/jax/jax/__init__.py:166     162 from jax import util as util     164 import jax.lib   TODO(phawkins): remove this export. > 166 del jax._src AttributeError: _src ``` I suspect you were somehow reloading / reimporting JAX, which led to this error. We should probably avoid this by checking `hasattr(jax, '_src')` before deleting it, although I suspect reloading the library in an active runtime could probably lead to other issues as well (for example, objects that are assumed to be singletons may no longer evaluate as equal using an `is` check, pytrees may be doubly registered, etc.)","Yeah, maybe it would be a good idea to handle that situation by raising a more informative error message? It might be safer to pass on this given that it's not a standard use case and would require some careful thinking from us","I would propose for now we do something like this: ```python if hasattr(jax, '_src'):   del jax._src else:   import warnings   warnings.warn(""The jax module appears to have been reloaded within the python process, either with reload_module() ""                 ""or the %autoreload extension. Be aware that this can cause unintended sideeffects."") ```",Looks very reasonable to me!,Sounds great  thanks! I'll send you a PR to review.
458,"以下是一个github上的jax下的一个issue, 标题是(removing repeated average function implementation and docs rendering)， 内容是 (Closes https://github.com/google/jax/issues/13853 The function `jax.numpy.average` is repeated many times on the documentation page jax.numpy.average This PR removes duplicated code that caused the issue Thanks)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,removing repeated average function implementation and docs rendering,Closes https://github.com/google/jax/issues/13853 The function `jax.numpy.average` is repeated many times on the documentation page jax.numpy.average This PR removes duplicated code that caused the issue Thanks,2023-01-04T04:28:21Z,,closed,0,4,https://github.com/jax-ml/jax/issues/13855,"Thanks  I don't think this is the right fix. These typing overloads do serve a purpose: they ensure that when the function is used in typechecked code, the static type checker can infer the correct number of outputs. If your goal is fixing the docs, I'd suggest modifying the docs rather than the source code. This documentation is generated via sphinx autodoc: perhaps that extension has settings that control how overloaded function definitions are displayed?",Yeah sure it is the issue with sphinx you're right there might be support for this as if it is many overloads it may seem very weird, CC([typing] improve sphinx rendering of type aliases) improves the situation somewhat by preserving the `ArrayLike` alias rather than printing out the full union it represents.,I think as these function overloading are needed we can close this PR
456,"以下是一个github上的jax下的一个issue, 标题是(remove repeated average function implementation and docs rendering)， 内容是 (Closes https://github.com/google/jax/issues/13853 The function `jax.numpy.average` is repeated many times on the documentation page jax.numpy.average This PR removes duplicated code that caused the issue Thanks)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,remove repeated average function implementation and docs rendering,Closes https://github.com/google/jax/issues/13853 The function `jax.numpy.average` is repeated many times on the documentation page jax.numpy.average This PR removes duplicated code that caused the issue Thanks,2023-01-04T04:11:41Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13854
590,"以下是一个github上的jax下的一个issue, 标题是(The `jax.numpy.average` function is rendering repeatedly in Docs)， 内容是 ( Description This is being caused by repetition of the same function at the implementation side  reduction functions I am going to open a pull request that can fix the mentioned issue !image  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,The `jax.numpy.average` function is rendering repeatedly in Docs, Description This is being caused by repetition of the same function at the implementation side  reduction functions I am going to open a pull request that can fix the mentioned issue !image  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_,2023-01-04T04:08:07Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/13853,"Hi  thanks for the report. I think this is intended: the `jnp.average` function has multiple type overloads, and this is how sphinx represents this in the documentation. The key difference in each is the annotation of `returned`, and how it affects the return value. Still, I agree that it makes the docs a bit busy. I wonder if there's a way to tell sphinx to avoid repeating the definition?",Hi   The `jax.numpy.average` function now renders only once in the documentation as follows:   Please have a look at the documentation here: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.average.html Thank you.,"Yes, it is fixed. Thanks for the update, I am closing this as completed."
