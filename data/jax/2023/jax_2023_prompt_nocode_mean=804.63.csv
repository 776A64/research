189,"以下是一个github上的jax下的一个issue, 标题是(Title)， 内容是 (Body)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",key,Title,Body,Created At,Tags,State,Reactions,Comments_count,Link,Comments
788,"以下是一个github上的jax下的一个issue, 标题是(Testing __cuda_array_interface__ attribute of a CPU array throws an exception)， 内容是 ( Description As in the title. Reproducer:  The expected behavior is   similar to PyTorch or NumPy:   What jax/jaxlib version are you using? 0.4.24.dev20231215+41531123f 0.4.24.dev20231215+41531123f  Which accelerator(s) are you using? CPU  Additional system info? 1.26.2 3.11.0  (main, Jan 14 2023, 12:27:40) [GCC 11.3.0] uname_result(system='Linux', node='ex', release='5.4.0153generic', version=' CC(Random key error in stax.Dropout layer)Ubuntu SMP Fri Jun 16 13:43:31 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Testing __cuda_array_interface__ attribute of a CPU array throws an exception," Description As in the title. Reproducer:  The expected behavior is   similar to PyTorch or NumPy:   What jax/jaxlib version are you using? 0.4.24.dev20231215+41531123f 0.4.24.dev20231215+41531123f  Which accelerator(s) are you using? CPU  Additional system info? 1.26.2 3.11.0  (main, Jan 14 2023, 12:27:40) [GCC 11.3.0] uname_result(system='Linux', node='ex', release='5.4.0153generic', version=' CC(Random key error in stax.Dropout layer)Ubuntu SMP Fri Jun 16 13:43:31 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_",2023-12-27T21:04:05Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/19134,The same issue also breaks `jax.numpy.from_dlpack` on CPU arrays: ,"Hi  thanks for the report! Can you confirm what JAX version you're using? I can't reproduce your `jax.numpy.from_dlpack` error on any recent JAX release; in particular I can't find any JAX version where line 2186 of `lax_numpy.py` contains the line `    if hasattr(a, ""__cuda_array_interface__""):` (though I didn't look at versions older than 0.4.0). As for the `hasattr` function failing... I think this is working as expected. All arrays have this attribute, it's just that some of them raise an error because they are not on the appropriate device. We could have chosen to implement this differently, so that only arrays on particular devices have this attribute, but that would break the assumptions of static type checkers like `mypy` and `pytype`, in which the existence of object attributes cannot depend on runtime values. Rather than `hasattr`, you might think about doing something like this:  What do you think?","It's possible that we should throw a different exception from our pybind11 code though, so Python thinks the attribute is not present.","If we throw an `AttributeError`, then the `isinstance` check at the top of this thread would work as expected.","> Can you confirm what JAX version you're using? Sure:  Also, confirming that with recent main branch, https://github.com/google/jax/issues/19134issuecomment1873304362 is fixed.","> As for the `hasattr` function failing... I think this is working as expected. All arrays have this attribute, it's just that some of them raise an error because they are not on the appropriate device. We could have chosen to implement this differently, so that only arrays on particular devices have this attribute, but that would break the assumptions of static type checkers like `mypy` and `pytype`, in which the existence of object attributes cannot depend on runtime values. The current behavior contradicts `hasattr` semantics. It should return True or False depending on whether the object has the attribute specified by a name. At the level of `hasattr`, devicespecific behavior is irrelevant. The devicespecific behavior should be implemented in `getattr`. There are a few choices how to handle cudaspecific attributes of noncudaspecific arrays:  throw a runtime error  I think this is not correct because the devicespecific attributes are set at array construction time and that cannot be changed during the array life time.  throw an attribute error  this is exact behavior when `hasattr()` returns `False` I think throwing an attribute error from `getattr` in this situation is not going to be a problem for `mypy`. For instance, PyTorch is mypycompliant (however, it could depend on PyTorch mypy configuration, I haven't check that yet) and it implements the following behavior: ","> The current behavior contradicts `hasattr` semantics. It should return True or False depending on whether the object has the attribute specified by a name. That's not how `hasattr` is implemented, nor is it how it is documented But I think we could raise `AttributeError` here instead of `XlaRuntimeError` to make `hasattr` work as one would expect here. It would involve changing this line: https://github.com/openxla/xla/blob/768c94954fed0104a7dac2a875970227e1f71380/xla/python/py_array.ccL1112",I'll make it throw an attribute error.,https://github.com/openxla/xla/pull/8183 (primarily) fixes this.,This still is problematic for sharded arrays; tested on the most recent nightly builds on CPU: 
1259,"以下是一个github上的jax下的一个issue, 标题是(Potential memory issue on Apple Metal)， 内容是 ( Description Evaluating a simple multilayerperceptron (MLP) implemented in `flax` on the same input data and parameters potentially yields nondeterministic outputs on the **apple metal** device when the function is **NOT jitted**. When the function is **jitted** the outputs of the MLP are deterministic and the problem disappears. I was able to verify that this problem is specific to apple metal, as on a linux system with an nvidia gpu, the problem does not occur (with the current jax version). Empirically the problem frequency seems to be worse when the batch dimension is not of shape `2**n` and `n > 10`. For example for batch dimensions of 2500 and 5000 the problems occurs frequently.  Another empirical observation is that  the values are not random but repeat themself. For example `y[0, 0]` is always one of `m` different numbers (empirically `m \approx 34`) but it is random which one of the m options ones get, which kind of hints into a memory problem.   It is debatable whether this problem is a `jax`, `flax` or `apple metal)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Potential memory issue on Apple Metal," Description Evaluating a simple multilayerperceptron (MLP) implemented in `flax` on the same input data and parameters potentially yields nondeterministic outputs on the **apple metal** device when the function is **NOT jitted**. When the function is **jitted** the outputs of the MLP are deterministic and the problem disappears. I was able to verify that this problem is specific to apple metal, as on a linux system with an nvidia gpu, the problem does not occur (with the current jax version). Empirically the problem frequency seems to be worse when the batch dimension is not of shape `2**n` and `n > 10`. For example for batch dimensions of 2500 and 5000 the problems occurs frequently.  Another empirical observation is that  the values are not random but repeat themself. For example `y[0, 0]` is always one of `m` different numbers (empirically `m \approx 34`) but it is random which one of the m options ones get, which kind of hints into a memory problem.   It is debatable whether this problem is a `jax`, `flax` or `apple metal",2023-12-27T18:36:08Z,bug Apple GPU (Metal) plugin,closed,0,3,https://github.com/jax-ml/jax/issues/19132,Hi   I executed the mentioned code with jaxmetal 0.0.6 on a Macbook Pro with an M1 Pro chip to see if the reported issue persists. The code produces the same output regardless of using JustInTime (JIT) compilation.    Could you please verify with jaxmetal 0.0.6 and confirm if the issue issue still persists. Thank you.,"Thanks for following up , I'm going to close the issue","Can confirm after upgrading `jaxmetal`, `jax` as well as `flax` this error seems to be gone."
471,"以下是一个github上的jax下的一个issue, 标题是(Help with aarch64 dependencies)， 内容是 (For the past week, I've been trying to get this: https://huggingface.co/blog/sdxl_jax to work with my coral tpu usb accelerator. I've noticed that Jax depends on Jaxlib for XLA, and on pypi, there are some aarch64 wheels but pip install jaxlib returns an error.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Help with aarch64 dependencies,"For the past week, I've been trying to get this: https://huggingface.co/blog/sdxl_jax to work with my coral tpu usb accelerator. I've noticed that Jax depends on Jaxlib for XLA, and on pypi, there are some aarch64 wheels but pip install jaxlib returns an error.",2023-12-26T22:29:37Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/19128,"never mind, I just reinstalled raspbian 64 bit this time. It is still very disappointing that nobody tried to help"
1253,"以下是一个github上的jax下的一个issue, 标题是(cuBLAS and JAX version mismatch)， 内容是 ( Description I noticed that the update for JAX 0.4.23 introduced some version mismatch errors when I am running a training script:  `CUDA backend failed to initialize: Found cuBLAS version 120103, but JAX was built against version 120205, which is newer. The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` I shall note that this issue was mentioned ( CC(Unable to correct CUDA vs. JAX version mismatch)) not long ago by another fellow user, and I have attempted the following command to fix, but to no avail.  `pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` The error unfortunately still persisted, and I suspect that this could be something to do with the CUDA backend. Please correct me if I am wrong.   What I have done  Did a fresh install for CUDA and CUDA Toolkit to upgrade it to the latest version (12.3)  `cudatoolkit123 is already the newest version (12.3.11).` My inst)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,cuBLAS and JAX version mismatch," Description I noticed that the update for JAX 0.4.23 introduced some version mismatch errors when I am running a training script:  `CUDA backend failed to initialize: Found cuBLAS version 120103, but JAX was built against version 120205, which is newer. The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` I shall note that this issue was mentioned ( CC(Unable to correct CUDA vs. JAX version mismatch)) not long ago by another fellow user, and I have attempted the following command to fix, but to no avail.  `pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` The error unfortunately still persisted, and I suspect that this could be something to do with the CUDA backend. Please correct me if I am wrong.   What I have done  Did a fresh install for CUDA and CUDA Toolkit to upgrade it to the latest version (12.3)  `cudatoolkit123 is already the newest version (12.3.11).` My inst",2023-12-25T19:15:19Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/19121,"I've managed to fix my issue, simply pip installing a previous version of JAX.  `pip install jax[cuda12_pip]==0.4.21 f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`","Just in case it helps anyone coming here from Google (as I did): I got this version mismatch error in Google Colab when using _both_ Jax and Torch in one Notebook. For some to me unexplainable reason, the following reliably fixes the issue for me: Instead of  insert any call to jax _between_ the `import jax` and `import torch`, for example:  I assume this has something to do with how Jax and Torch handle initialisation of the GPU backend, with Torch messing things up for Jax upon being imported. Perhaps. See also related issue CC(JAX and TORCH).","Please check CC(Latest JAX `0.4.24` does not detect GPUs) . Installing it from pip works, provided you don't have another CUDA loaded. That downloads the correct CUDA and cuBLAS to site_packages."
556,"以下是一个github上的jax下的一个issue, 标题是(jax.lax.scan ""tuple index out of range error"" when using as described in docs)， 内容是 ( Description I'm trying to migrate a forloop to `scan` as per the docs. Here's the code I'm trying: python3 def scan(f, init, xs, length=None):   if xs is None:     xs = [None] * length   carry = init   ys = []   for x in xs:     carry, y = f(carry, x)     ys.append(y)   return carry, np.stack(ys) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"jax.lax.scan ""tuple index out of range error"" when using as described in docs"," Description I'm trying to migrate a forloop to `scan` as per the docs. Here's the code I'm trying: python3 def scan(f, init, xs, length=None):   if xs is None:     xs = [None] * length   carry = init   ys = []   for x in xs:     carry, y = f(carry, x)     ys.append(y)   return carry, np.stack(ys) ",2023-12-24T11:21:55Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/19113,Thanks for the report! I'm having trouble figuring out how to call your function in order to reproduce this error. Could you put together a minimal reproducible example that I could paste into a code interpreter to see the same output you're seeing?,Closing as not reproducible – feel free to open another issue if you're still having this problem. Thanks!
1298,"以下是一个github上的jax下的一个issue, 标题是(Underscore in setup.py for extras_require is (probably) not supported by pip)， 内容是 ( Description When I run the suggested installation command:  I get the following warning (which causes other downstream problems):  Note that `cuda12pip` has a hyphen in the warning, not an underscore. This problem occurs with `pip` versions since 23.3, but not earlier ones. In the release notes for `pip 23.3` this change seems to be considered a bug fix: > Normalize extras according to PEP 685 from package metadata in the resolver for comparison. This ensures extras are correctly compared and merged as long as the package providing the extra(s) is built with values normalized according to the standard. Note, however, that this does not solve cases where the package itself contains unnormalized extra values in the metadata. ( CC(Reland 11498 after internal fixes.)) The normalization mentioned is the following:  i.e., all consecutive hyphens, underscores, and periods are replaced with a single hyphen. Given these circumstances, does it make sense to change the `setup.py` in `jax` and `jaxlib` to not use underscores? Or at least)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Underscore in setup.py for extras_require is (probably) not supported by pip," Description When I run the suggested installation command:  I get the following warning (which causes other downstream problems):  Note that `cuda12pip` has a hyphen in the warning, not an underscore. This problem occurs with `pip` versions since 23.3, but not earlier ones. In the release notes for `pip 23.3` this change seems to be considered a bug fix: > Normalize extras according to PEP 685 from package metadata in the resolver for comparison. This ensures extras are correctly compared and merged as long as the package providing the extra(s) is built with values normalized according to the standard. Note, however, that this does not solve cases where the package itself contains unnormalized extra values in the metadata. ( CC(Reland 11498 after internal fixes.)) The normalization mentioned is the following:  i.e., all consecutive hyphens, underscores, and periods are replaced with a single hyphen. Given these circumstances, does it make sense to change the `setup.py` in `jax` and `jaxlib` to not use underscores? Or at least",2023-12-24T01:38:11Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/19111,"Never mind, I just realized my ""downstream problems"" were caused by something else. Sorry for the spam."
1251,"以下是一个github上的jax下的一个issue, 标题是(Wrappers for`scipy.linalg`quadratic control solvers (lyapunov, ARE))， 内容是 (Please:  [x ] Check for duplicate requests.  [x ] Describe your goal, and if possible provide a code snippet with a motivating example. I'm interested in implementing `solve_discrete_lyapunov`, `solve_continuous_lyapunov`, and `solve_discrete_are` from `scipy.linalg` as JAX primitives. My particular usecase is Kalman filtering  these functions are handy for computing initial and steadystate covariance matrices, but they also have wide application in linearquadratic control applications. There are gradients computed in this paper, https://arxiv.org/pdf/2011.11430.pdf and I've also done implementations in PyTensor here and here. I'm relying heavily on compiling pytensor graphs to JAX for highperformance scans, and not having these functions is a bit of a painpoint for me at the moment. I didn't see these functions from a quick search of the codebase, but I just wanted to check that 1) a contribute would be welcome, and 2) they didn't exist elsewhere in the JAX ecosystem before starting a PR.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Wrappers for`scipy.linalg`quadratic control solvers (lyapunov, ARE)","Please:  [x ] Check for duplicate requests.  [x ] Describe your goal, and if possible provide a code snippet with a motivating example. I'm interested in implementing `solve_discrete_lyapunov`, `solve_continuous_lyapunov`, and `solve_discrete_are` from `scipy.linalg` as JAX primitives. My particular usecase is Kalman filtering  these functions are handy for computing initial and steadystate covariance matrices, but they also have wide application in linearquadratic control applications. There are gradients computed in this paper, https://arxiv.org/pdf/2011.11430.pdf and I've also done implementations in PyTensor here and here. I'm relying heavily on compiling pytensor graphs to JAX for highperformance scans, and not having these functions is a bit of a painpoint for me at the moment. I didn't see these functions from a quick search of the codebase, but I just wanted to check that 1) a contribute would be welcome, and 2) they didn't exist elsewhere in the JAX ecosystem before starting a PR.",2023-12-23T22:00:22Z,enhancement,open,1,2,https://github.com/jax-ml/jax/issues/19109,How do you think these solvers match up according to our rubric for JAX scipy wrappers? https://jax.readthedocs.io/en/latest/jep/18137numpyscipyscope.html,"I think they're on the margin. They fail axis 1 spectacularly, but arguably pass the other axes, with varying levels of difficultly in making those arguments. I think they pass on 2, 3, 5, and arguably on 6  github code search finds 5001000 snippets using each of `solve_discrete_lyapunov` and `solve_discrete_are`, so it's clearly not as popular as `linalg.solve`, but more widely used than  `bessel_jn`. I guess the weakest case is on 4, ideally I'd hope to just wrap up some calls to LAPACK for forward computation together with some gradients, but this is likely to be more complicated than I realize (hardware targeting issues? introduction of additional package requirements? I have no idea if either of these would be issues, but I can imagine that they could be). On the other hand, quadratic control generally and Kalman filtering specifically aren't exactly niche topics in scientific computing, so I'm sure these functions would see some use if they were available. Plus `scipy.linalg` is in scope. But I could see them belonging in something more like jaxopt, though. "
398,"以下是一个github上的jax下的一个issue, 标题是(partial rollback of #19096 due to internal breakage (relying on jax internals))， 内容是 (partial rollback of CC(del add_any_p and zeros_like_p, replace avaldispatched traceable) due to internal breakage (relying on jax internals))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,partial rollback of #19096 due to internal breakage (relying on jax internals),"partial rollback of CC(del add_any_p and zeros_like_p, replace avaldispatched traceable) due to internal breakage (relying on jax internals)",2023-12-22T22:46:02Z,,closed,0,0,https://github.com/jax-ml/jax/issues/19105
627,"以下是一个github上的jax下的一个issue, 标题是(Pallas NotImplementedError: unsupported layout change)， 内容是 ( Description I'm trying to write a simple rnnlike for loop in pallas but get a  for some reason. If anyone can help fixing this that would be very helpful! The code is:  ` And the full error message:    What jax/jaxlib version are you using? jax v0.4.21 jaxlib v0.4.21  Which accelerator(s) are you using? TPU  Additional system info? Python 3.10, Kaggle Notebook  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",text generation,Pallas NotImplementedError: unsupported layout change," Description I'm trying to write a simple rnnlike for loop in pallas but get a  for some reason. If anyone can help fixing this that would be very helpful! The code is:  ` And the full error message:    What jax/jaxlib version are you using? jax v0.4.21 jaxlib v0.4.21  Which accelerator(s) are you using? TPU  Additional system info? Python 3.10, Kaggle Notebook  NVIDIA GPU info _No response_",2023-12-20T04:31:28Z,bug pallas,open,0,0,https://github.com/jax-ml/jax/issues/19058
605,"以下是一个github上的jax下的一个issue, 标题是(GPU memory allocation not working as expected)， 内容是 ( Description When using the GPU memory allocation environment variables described here, I get unexpected results. ``XLA_PYTHON_CLIENT_PREALLOCATE=false`` still preallocates memory and ``XLA_PYTHON_CLIENT_MEM_FRACTION=.99`` does not preallocate 99% of memory. Code to reproduce (test.py):  When run with ``XLA_PYTHON_CLIENT_PREALLOCATE=false python3.10 test.py``, nvidiasmi shows: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,GPU memory allocation not working as expected," Description When using the GPU memory allocation environment variables described here, I get unexpected results. ``XLA_PYTHON_CLIENT_PREALLOCATE=false`` still preallocates memory and ``XLA_PYTHON_CLIENT_MEM_FRACTION=.99`` does not preallocate 99% of memory. Code to reproduce (test.py):  When run with ``XLA_PYTHON_CLIENT_PREALLOCATE=false python3.10 test.py``, nvidiasmi shows: ",2023-12-18T21:41:11Z,bug,closed,1,2,https://github.com/jax-ml/jax/issues/19035,It looks like this has regressed in jax/jaxlib 0.4.23. The workaround for the moment is probably to revert to v0.4.20.,"I've submitted a fix, but it will need a new release. CC([JAX:GPU] Add a test that verifies that the XLA_PYTHON_CLIENT_PREALLOCATE environment variable is parsed correctly.) adds a test so this doesn't regress again."
1302,"以下是一个github上的jax下的一个issue, 标题是(Error reading persistent compilation cache entry: Instruction name is not unique)， 内容是 ( Description I am encountering this error while trying to run a function which I have a compiled executable for in the persistent compile cache.  It only seems to happen on larger batch sizes/resolutions (the script I am using operates on ~40 different input shapes with a fairly wide range of total area, on some batch sizes only half of my inputs will be affected).  Functions work absolutely fine regardless of size once compiled, but cannot be loaded from cache.  This happens with jit compilation and AOT compilation.  I have also experienced the same issue on both CUDA 11 and CUDA 12 (currently using 12) versions of JAX and at least as far back as 0.4.18. I suspect that it may be related to specific CUDA kernels being selected during benchmarking since it only happens on larger input sizes? I can provide HLO dumps or other diagnostic info if needed. Here is the error that shows up (empty lines removed).  The name of the constant is not always the same.   What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Error reading persistent compilation cache entry: Instruction name is not unique," Description I am encountering this error while trying to run a function which I have a compiled executable for in the persistent compile cache.  It only seems to happen on larger batch sizes/resolutions (the script I am using operates on ~40 different input shapes with a fairly wide range of total area, on some batch sizes only half of my inputs will be affected).  Functions work absolutely fine regardless of size once compiled, but cannot be loaded from cache.  This happens with jit compilation and AOT compilation.  I have also experienced the same issue on both CUDA 11 and CUDA 12 (currently using 12) versions of JAX and at least as far back as 0.4.18. I suspect that it may be related to specific CUDA kernels being selected during benchmarking since it only happens on larger input sizes? I can provide HLO dumps or other diagnostic info if needed. Here is the error that shows up (empty lines removed).  The name of the constant is not always the same.   What jax/jaxlib version are you using? 0.4.23 0.4.23  Which accelerator(s",2023-12-18T07:29:04Z,bug needs info NVIDIA GPU,open,0,3,https://github.com/jax-ml/jax/issues/19028,"An HLO dump would be helpful, please. Failing that, some other sort of reproducer would be necessary.","576x576x4_working_lowered.txt 576x576x16_bugged_lowered.txt Here are the lowered forms of the function, one on an input size where the function works as expected, and one on an input size where the function fails to save to the compile cache correctly. I have the compiled forms of the function saved, but they are over 500MB each.  They will be difficult to upload, so tell me if you do explicitly need them.  I did search for the misbehaving constant which in this instance is `constant_29335`, and did in fact find two instances of it in the compiled function: ","I am having the same issue,   did you solve it?"
918,"以下是一个github上的jax下的一个issue, 标题是(Random spaces are added when printing `Array`)， 内容是 ( Description It seems some random spaces are added when printing arrays using both the built in `print()` and  `jax.debug.print`. A minimal, reproducible code example is provided below:  The output exhibits the issue, showing extra spaces:   What jax/jaxlib version are you using? jax: 0.4.23 jaxlib: 0.4.23  Which accelerator(s) are you using? CPU  Additional system info?  numpy: 1.26.2  python: 3.11.6 (main, Oct  2 2023, 13:45:54) [Clang 15.0.0 (clang1500.0.40.1)]  platform uname:     system: 'Darwin'    release: '23.2.0'    version: 'Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu10002.61.3~2/RELEASE_ARM64_T6000'    machine: 'arm64'  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Random spaces are added when printing `Array`," Description It seems some random spaces are added when printing arrays using both the built in `print()` and  `jax.debug.print`. A minimal, reproducible code example is provided below:  The output exhibits the issue, showing extra spaces:   What jax/jaxlib version are you using? jax: 0.4.23 jaxlib: 0.4.23  Which accelerator(s) are you using? CPU  Additional system info?  numpy: 1.26.2  python: 3.11.6 (main, Oct  2 2023, 13:45:54) [Clang 15.0.0 (clang1500.0.40.1)]  platform uname:     system: 'Darwin'    release: '23.2.0'    version: 'Darwin Kernel Version 23.2.0: Wed Nov 15 21:53:18 PST 2023; root:xnu10002.61.3~2/RELEASE_ARM64_T6000'    machine: 'arm64'  NVIDIA GPU info _No response_",2023-12-16T15:22:25Z,bug needs info,closed,0,2,https://github.com/jax-ml/jax/issues/19018,"JAX delegates its array printing to NumPy, so this is how NumPy would format these arrays. Can you say more about why this is a bug? It looks like it's working as intended to me: the printing is aligning the brackets and the leading digits.","Certainly, as you mentioned, spaces were simply added to align the appearance. I simply misunderstood since I ""noticed"" this behavior when printing an array reshaped with `.reshape(1)`. I apologize for any confusion."
904,"以下是一个github上的jax下的一个issue, 标题是(Unable to load the Mac M2 version of Tensor)， 内容是 ( Description WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found  What jax/jaxlib version are you using? 0.4.10  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.13 (main, Aug 24 2023, 22:36:46) [Clang 14.0.3 (clang1403.0.22.14.1)] uname_result(system='Darwin', node='JamessMacBookAir.local', release='23.3.0', version='Darwin Kernel Version 23.3.0: Fri Dec  1 03:20:24 PST 2023; root:xnu10002.80.11~58/RELEASE_ARM64_T8112', machine='arm64')  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Unable to load the Mac M2 version of Tensor," Description WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/7d879c8b161085a4374ea481b93a52adb19c0529.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found  What jax/jaxlib version are you using? 0.4.10  Which accelerator(s) are you using? GPU  Additional system info? 1.26.2 3.10.13 (main, Aug 24 2023, 22:36:46) [Clang 14.0.3 (clang1403.0.22.14.1)] uname_result(system='Darwin', node='JamessMacBookAir.local', release='23.3.0', version='Darwin Kernel Version 23.3.0: Fri Dec  1 03:20:24 PST 2023; root:xnu10002.80.11~58/RELEASE_ARM64_T8112', machine='arm64')  NVIDIA GPU info _No response_",2023-12-16T03:51:40Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/19015,Can you include some information about what code you were executing when you saw this error? Thanks!
1289,"以下是一个github上的jax下的一个issue, 标题是(CUDA error when importing torch before creating any jax.numpy.array)， 内容是 ( Description If pytorch is imported before creating a jax.numpy.array then jax can't use cuda.  will fail with `CUDA backend failed to initialize: Found cuDNN version 8500, but JAX was built against version 8600, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` while   will work with only a mildly annoying warning `The NVIDIA driver's CUDA version is 11.7 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages.` I don't know how reproducible this is outside of my own setting as I'm working on a server of which I don't have privileges to update. We are running `NVIDIASMI 515.76       Driver Version: 515.76       CUDA Version: 11.7  ` (output of `nvidiasmi`). Jax was installed using `pip )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,CUDA error when importing torch before creating any jax.numpy.array," Description If pytorch is imported before creating a jax.numpy.array then jax can't use cuda.  will fail with `CUDA backend failed to initialize: Found cuDNN version 8500, but JAX was built against version 8600, which is newer. The copy of cuDNN that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` while   will work with only a mildly annoying warning `The NVIDIA driver's CUDA version is 11.7 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages.` I don't know how reproducible this is outside of my own setting as I'm working on a server of which I don't have privileges to update. We are running `NVIDIASMI 515.76       Driver Version: 515.76       CUDA Version: 11.7  ` (output of `nvidiasmi`). Jax was installed using `pip ",2023-12-15T21:27:19Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/19004,"There's nothing we can do about this. If you have two libraries that use cudnn, the first one to load it ""wins"". If you load torch first, you end up with the older cudnn version loaded by your torch version. JAX loads cudnn when the backend is initialized, which happens during the first op usually. Calling `jax.devices()` would work also. However, cudnn is usually backward compatible, so you can probably just work around by loading JAX first. Hope that helps!","Oh I see, thanks for the answer. Is there any way to ask jax which cuda/cudnn is it using and where is it located in the system? Something like `torch.backends.cudnn.version()`.","Hi ,  You can use alternative solution, look for CUDA and cuDNN versions in dependencies: `pip show jaxlib `  OR `pip show jaxlib | grep cudatoolkit` You can find below code snippet to point out the CUDA installation directory:  Hope this helps!","Hey , thanks for answering. I'm not sure those commands work (maybe because of how the cluster is set up or because of micromamba). The variable are empty but jax still works on the GPU so cuda must be in use. !image"
708,"以下是一个github上的jax下的一个issue, 标题是(Make jax versions 0.2.* available on Google Storage)， 内容是 (   I am looking at the Google Storage bucket linked above and I don't see anything hosted for Jax .  The LRA codebase raises an error indicating a breaking change when I used Jax , and the LRA repo provides a looselyspecified dependency of , so including Jax  on the aforementioned Google Storage bucket would be very helpful.  Can you make these versions available as well? Thank you! _Originally posted by  in https://github.com/google/jax/issues/18368issuecomment1857432763_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Make jax versions 0.2.* available on Google Storage,"   I am looking at the Google Storage bucket linked above and I don't see anything hosted for Jax .  The LRA codebase raises an error indicating a breaking change when I used Jax , and the LRA repo provides a looselyspecified dependency of , so including Jax  on the aforementioned Google Storage bucket would be very helpful.  Can you make these versions available as well? Thank you! _Originally posted by  in https://github.com/google/jax/issues/18368issuecomment1857432763_",2023-12-15T07:56:13Z,question,closed,0,3,https://github.com/jax-ml/jax/issues/18993,"Hi  all previous jaxlib versions are available at https://storage.googleapis.com/jaxreleases/jax_releases.html; you can find information on installing them at https://jax.readthedocs.io/en/latest/installation.htmlinstallingolderjaxlibwheels Note that `jaxlib` 0.2.X does not exist: prior to 0.3.0, the jax and jaxlib versioning scheme did not match, and the last jaxlib version released before 0.3.0 was was (I believe) 0.1.76. Finding matching versions from that era of jax/jaxlib is a bit difficult, but from the release dates it looks like jax v0.2.4 is likely compaible with jaxlib v0.1.56 or so. So it sounds like you want something like this:  Note however that this jaxlib wheel is only available on linux, and only for Python 3.6, 3.7, and 3.8 (which were the three newest Python versions 3+ years ago when this version of jax was released). Best of luck!","Closing, but feel free to comment if Jake's answer didn't solve the problem!","The issue is resolved, thanks!  I had been under the impression the versions were synchronized the whole time. This, combined with the numbering jump of Jaxlib versions in the GCS bucket and a Jaxspecific pip error message, led me to believe Jax 0.2.* wasn't on PyPI or GCS. In fact, while not on GCS, Jax 0.2.* is available on PyPI still.  Editing to add: Having gotten everything to work on CPU, I also got everything to work on TPU v3 just now. For this, I also had to install the correct libtpunightly version, which I located by using the timestamps in the Jax changelog, and appending "
1281,"以下是一个github上的jax下的一个issue, 标题是(ROCM build fails when the ROCM_PATH is a symlinked location)， 内容是 ( Description The jaxlib build with ROCm will fail if the ROCM_PATH is directed to a symlinked location. This issue is particularly critical for NixOS, as the ROCM_PATH is invariably a symlinked location joining each individual library together, thereby making ROCM build on NixOS unfeasible. While this may be less significant for other distributions, it is still a common issue, given that /opt/rocm is likely to be a symlink. To reproduce this behavior, start from a ROCM container `rocm/devubuntu22.04:5.7.1complete` and run the following commands to prepare the build environment.  In this environment the realpath for ROCM library is `/opt/rocm5.7.1` and there is also a symlink `/opt/rocm > /opt/rocm5.7.1` Now build jaxlib with `python build/build.py enable_rocm rocm_path=/opt/rocm` will result in a failure, but`python build/build.py enable_rocm rocm_path=/opt/rocm5.7.1` won't. Build log   It seems that the symlink location is included in the dependency, however the realpath is used during the build, resulting in a ""undeclared in)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,ROCM build fails when the ROCM_PATH is a symlinked location," Description The jaxlib build with ROCm will fail if the ROCM_PATH is directed to a symlinked location. This issue is particularly critical for NixOS, as the ROCM_PATH is invariably a symlinked location joining each individual library together, thereby making ROCM build on NixOS unfeasible. While this may be less significant for other distributions, it is still a common issue, given that /opt/rocm is likely to be a symlink. To reproduce this behavior, start from a ROCM container `rocm/devubuntu22.04:5.7.1complete` and run the following commands to prepare the build environment.  In this environment the realpath for ROCM library is `/opt/rocm5.7.1` and there is also a symlink `/opt/rocm > /opt/rocm5.7.1` Now build jaxlib with `python build/build.py enable_rocm rocm_path=/opt/rocm` will result in a failure, but`python build/build.py enable_rocm rocm_path=/opt/rocm5.7.1` won't. Build log   It seems that the symlink location is included in the dependency, however the realpath is used during the build, resulting in a ""undeclared in",2023-12-14T07:26:08Z,bug AMD GPU,open,0,2,https://github.com/jax-ml/jax/issues/18976,@ rahulbatra85, In the JAX containers we build we use real path. As you noted the symlink causes build to fail. I am guessing this is bazel(build system) thing where it ends up following symlink and creates a cache. I will need to look further to see what can be done here.
406,"以下是一个github上的jax下的一个issue, 标题是(jaxlib 0.4.22 has very spammy GPU compiler logs)， 内容是 (We messed up this release and the GPU compiler is very spammy, with lots of LOG output from C++. Workaround: set `TF_CPP_MIN_LOG_LEVEL=2`, and we're trying to cut another release.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jaxlib 0.4.22 has very spammy GPU compiler logs,"We messed up this release and the GPU compiler is very spammy, with lots of LOG output from C++. Workaround: set `TF_CPP_MIN_LOG_LEVEL=2`, and we're trying to cut another release.",2023-12-14T02:34:07Z,bug P0 (urgent) NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/18970,The immediate issue was fixed by the release of JAX v0.4.23.  https://github.com/google/jax/issues/18973 covers adding testing.,If you are like me and still see the logs after updating jax with `pip install jax upgrade`. Remember to also upgrade `jaxlib` with `pip install jaxlib upgrade`,"Yes, that's right! You need to update both `jax` and `jaxlib`. Sorry for the disruption."
344,"以下是一个github上的jax下的一个issue, 标题是(Improve shape validation when jax_dynamic_shapes=True)， 内容是 (Fixes CC([jax_dynamic_shapes] specifying shapes with floating point abstract values does not raise a TypeError))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve shape validation when jax_dynamic_shapes=True,Fixes CC([jax_dynamic_shapes] specifying shapes with floating point abstract values does not raise a TypeError),2023-12-12T20:57:06Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/18946,Probably needs a test before we can merge.
1306,"以下是一个github上的jax下的一个issue, 标题是(large groups of derived test cases are jointly sensitive to their common random seed)， 内容是 (Many of our pseudorandom tests carry out statistical tests (namely KS and chisquared), which have a failure probability `p`. Assuming there are no bugs, we might expect a that a single test case fails over a `p` fraction of random seeds. However, we sometimes generate many test cases from one fragment of test code, e.g. via test method parameterization or by testing several backends/environments. When these all share a seed (because they share the seeding code), changing that seed amounts to redrawing many pseudorandom trials at once. It is then quite likely (beyond `p`) that at least one of these cases fails across such a change. As one example, six generated testcases derived from `LaxRandomTest.testBinomialSample` fail if we change the seed here to `1234567`: https://github.com/google/jax/blob/b077483bfaaf197b79717a86bee3e626474e93f2/tests/random_lax_test.pyL1174L1175 Maybe we can try to decouple the seeds across test cases and/or decrease pvalues for commonlyseeded groups of test cases. Relatedly, if we do something like c)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,large groups of derived test cases are jointly sensitive to their common random seed,"Many of our pseudorandom tests carry out statistical tests (namely KS and chisquared), which have a failure probability `p`. Assuming there are no bugs, we might expect a that a single test case fails over a `p` fraction of random seeds. However, we sometimes generate many test cases from one fragment of test code, e.g. via test method parameterization or by testing several backends/environments. When these all share a seed (because they share the seeding code), changing that seed amounts to redrawing many pseudorandom trials at once. It is then quite likely (beyond `p`) that at least one of these cases fails across such a change. As one example, six generated testcases derived from `LaxRandomTest.testBinomialSample` fail if we change the seed here to `1234567`: https://github.com/google/jax/blob/b077483bfaaf197b79717a86bee3e626474e93f2/tests/random_lax_test.pyL1174L1175 Maybe we can try to decouple the seeds across test cases and/or decrease pvalues for commonlyseeded groups of test cases. Relatedly, if we do something like c",2023-12-12T16:47:40Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/18941,In my experience a lot of these distribution tests are sensitive to a change in seed.,"This may be the expected effect of producing many trials, across generated cases, environments (CPU, GPU, TPU), etc. One workaround is to decrease the failure threshold (i.e. pvalue) for tests like this.",Agreed  but this is something that we need to look at more comprehensively in the random distribution test suite. I don't think the Binomial test is an outlier here!,I see what you mean and I agree. I've renamed the issue and tried to update the description to describe things better now.
1023,"以下是一个github上的jax下的一个issue, 标题是([jax_dynamic_shapes] specifying shapes with floating point abstract values does not raise a TypeError)， 内容是 ( Description The following example gets to produce valid StableHLO but should fail during tracing. Note that the StableHLO is valid for alternative lowering strategies. XLA's lowering strategy will raise an error during compilation.  I've narrowed down the error to `_canonicalize_dimension` inlined below:  Happy to submit a PR perhaps on the next year.  What jax/jaxlib version are you using? 0.4.21  Which accelerator(s) are you using? CPU  Additional system info? 1.26.1 3.10.11 (main, May 13 2023, 12:07:51) [GCC 9.4.0] uname_result(system='Linux', node='DL7420GS4N1J3', release='5.15.088generic', version=' CC(make it easy to print jaxprs)~20.04.1Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax_dynamic_shapes] specifying shapes with floating point abstract values does not raise a TypeError," Description The following example gets to produce valid StableHLO but should fail during tracing. Note that the StableHLO is valid for alternative lowering strategies. XLA's lowering strategy will raise an error during compilation.  I've narrowed down the error to `_canonicalize_dimension` inlined below:  Happy to submit a PR perhaps on the next year.  What jax/jaxlib version are you using? 0.4.21  Which accelerator(s) are you using? CPU  Additional system info? 1.26.1 3.10.11 (main, May 13 2023, 12:07:51) [GCC 9.4.0] uname_result(system='Linux', node='DL7420GS4N1J3', release='5.15.088generic', version=' CC(make it easy to print jaxprs)~20.04.1Ubuntu SMP Mon Oct 9 16:43:45 UTC 2023', machine='x86_64')  NVIDIA GPU info _No response_",2023-12-12T16:01:47Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/18937
1267,"以下是一个github上的jax下的一个issue, 标题是(Bump actions/setup-python from 4.7.1 to 5.0.0)， 内容是 (Bumps actions/setuppython from 4.7.1 to 5.0.0.  Release notes Sourced from actions/setuppython's releases.  v5.0.0 What's Changed In scope of this release, we update node version runtime from node16 to node20 (actions/setuppython CC(backward pass of scan is very slow to compile in CPU)). Besides, we update dependencies to the latest versions. Full Changelog: https://github.com/actions/setuppython/compare/v4.8.0...v5.0.0 v4.8.0 What's Changed In scope of this release we added support for GraalPy (actions/setuppython CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values)). You can use this snippet to set up GraalPy: steps:  uses: actions/checkout  uses: actions/setuppython    with:     pythonversion: 'graalpy22.3'   run: python my_script.py  Besides, the release contains such changes as:  Trim python version when reading from file by @​FerranPares in actions/setuppython CC(Forwardmode differentiation rule for 'select_and_scatter_add' not implemented) Use nondeprecated versions in examples)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Bump actions/setup-python from 4.7.1 to 5.0.0,"Bumps actions/setuppython from 4.7.1 to 5.0.0.  Release notes Sourced from actions/setuppython's releases.  v5.0.0 What's Changed In scope of this release, we update node version runtime from node16 to node20 (actions/setuppython CC(backward pass of scan is very slow to compile in CPU)). Besides, we update dependencies to the latest versions. Full Changelog: https://github.com/actions/setuppython/compare/v4.8.0...v5.0.0 v4.8.0 What's Changed In scope of this release we added support for GraalPy (actions/setuppython CC(Higher order derivatives of norm.logcdf seem to have numerical problems at low input values)). You can use this snippet to set up GraalPy: steps:  uses: actions/checkout  uses: actions/setuppython    with:     pythonversion: 'graalpy22.3'   run: python my_script.py  Besides, the release contains such changes as:  Trim python version when reading from file by @​FerranPares in actions/setuppython CC(Forwardmode differentiation rule for 'select_and_scatter_add' not implemented) Use nondeprecated versions in examples",2023-12-11T17:15:29Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/18921,"Closing, I'll address this separately via ratchet","OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1327,"以下是一个github上的jax下的一个issue, 标题是(0.4.21 release: ""JAX does not support string indexing"" error, but it used to work fine (for jax2tf only!))， 内容是 ( Description I pass a dictionary into my jitted function as an argument, including several string/jax array pairs. I can run my jitted function with no issues. Something like the following:  However, when I do this, it breaks: ```python tf_init = jax2tf.convert(jax.jit(env.init), jit_compile=True, autograph=False) tf_state = tf_init(tf.constant(key)) tf_step = jax2tf.convert(step_fn, jit_compile=True, autograph=False) state = tf_step(tf_state, tf.constant(action))   Relevant stack trace   My code    File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\myEnv\core.py"", line 205, in step      is_illegal = ~state['legal_actions'][action]  Jax code    File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 741, in op     return getattr(self.aval, f""_{name}"")(self, *args)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 354, in _getitem     return lax_numpy._rewriting_take(self, item)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,"0.4.21 release: ""JAX does not support string indexing"" error, but it used to work fine (for jax2tf only!)"," Description I pass a dictionary into my jitted function as an argument, including several string/jax array pairs. I can run my jitted function with no issues. Something like the following:  However, when I do this, it breaks: ```python tf_init = jax2tf.convert(jax.jit(env.init), jit_compile=True, autograph=False) tf_state = tf_init(tf.constant(key)) tf_step = jax2tf.convert(step_fn, jit_compile=True, autograph=False) state = tf_step(tf_state, tf.constant(action))   Relevant stack trace   My code    File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\myEnv\core.py"", line 205, in step      is_illegal = ~state['legal_actions'][action]  Jax code    File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 741, in op     return getattr(self.aval, f""_{name}"")(self, *args)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib\sitepackages\jax\_src\numpy\array_methods.py"", line 354, in _getitem     return lax_numpy._rewriting_take(self, item)   File ""C:\Users\kmjab\miniconda3\envs\myEnv\lib",2023-12-10T00:21:38Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18903,"Turns out this was an unrelated usercode bug. My bad, closing."
909,"以下是一个github上的jax下的一个issue, 标题是(Pallas NotImplementedError: Unimplemented primitive in Pallas TPU lowering: dynamic_slice.)， 内容是 ( Description I'm trying to RWKV in jax using Pallas but run into problems with the for loop. In my code, I'm using jax.lax.fori_loop to iterate over the sequence dimension. When indexing into my arrays I get the error:  **JaxStackTraceBeforeTransformation: NotImplementedError: Unimplemented primitive in Pallas TPU lowering: dynamic_slice. Please file an issue on https://github.com/google/jax/issues.** Code:   Problem Arises with ""k[t] * v[t]""  What jax/jaxlib version are you using? jax v0.4.21 jaxlib v0.4.21  Which accelerator(s) are you using? TPU  Additional system info? Python 3.10, Kaggle Notebook  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Pallas NotImplementedError: Unimplemented primitive in Pallas TPU lowering: dynamic_slice.," Description I'm trying to RWKV in jax using Pallas but run into problems with the for loop. In my code, I'm using jax.lax.fori_loop to iterate over the sequence dimension. When indexing into my arrays I get the error:  **JaxStackTraceBeforeTransformation: NotImplementedError: Unimplemented primitive in Pallas TPU lowering: dynamic_slice. Please file an issue on https://github.com/google/jax/issues.** Code:   Problem Arises with ""k[t] * v[t]""  What jax/jaxlib version are you using? jax v0.4.21 jaxlib v0.4.21  Which accelerator(s) are you using? TPU  Additional system info? Python 3.10, Kaggle Notebook  NVIDIA GPU info _No response_",2023-12-09T14:54:31Z,bug pallas,open,0,0,https://github.com/jax-ml/jax/issues/18897
736,"以下是一个github上的jax下的一个issue, 标题是(15% speed regression in LLM training code on v3 TPU from JAX 0.4.14 -> 0.4.16)， 内容是 ( Description Will attempt to minimize unless something is obvious. Attached are XLA outputs. The only difference is the JAX version. Verified the regression persists through 0.4.21 Tested with Levanter main (133dbba64ff08873ed07a5661211db867a471343) levanter_train_step.tar.gz Thanks! I really appreciate it!  What jax/jaxlib version are you using? 0.4.14>0.4.21  Which accelerator(s) are you using? TPU v3128  Additional system info? _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,15% speed regression in LLM training code on v3 TPU from JAX 0.4.14 -> 0.4.16, Description Will attempt to minimize unless something is obvious. Attached are XLA outputs. The only difference is the JAX version. Verified the regression persists through 0.4.21 Tested with Levanter main (133dbba64ff08873ed07a5661211db867a471343) levanter_train_step.tar.gz Thanks! I really appreciate it!  What jax/jaxlib version are you using? 0.4.14>0.4.21  Which accelerator(s) are you using? TPU v3128  Additional system info? _No response_  NVIDIA GPU info _No response_,2023-12-08T05:23:58Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/18875
1063,"以下是一个github上的jax下的一个issue, 标题是(unexpected UnshapedArray)， 内容是 ( Description  Above code produces the below error message 1. When I change ""data = DataClass(tmp, 1.0/(1.0+tmp**2), data0.dataclass_sub)"" to ""data = DataClass(tmp, tmp, data0.dataclass_sub)"", I get below error message 2. (I have obscured part of the file paths). I am a beginner in using jax and pytree, so I would appreciate it if you could let me know if I am using it incorrectly. ===error message 1===  ===error message 2===   What jax/jaxlib version are you using? jax0.4.21 jaxlib0.4.21  Which accelerator(s) are you using? cpu  Additional system info? print(numpy.__version__):1.24.3,   print(sys.version):3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)],    print(platform.uname()):uname_result(system='Windows', node='DESKTOPTU7H8UO', release='10', version='10.0.22621', machine='AMD64')  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,unexpected UnshapedArray," Description  Above code produces the below error message 1. When I change ""data = DataClass(tmp, 1.0/(1.0+tmp**2), data0.dataclass_sub)"" to ""data = DataClass(tmp, tmp, data0.dataclass_sub)"", I get below error message 2. (I have obscured part of the file paths). I am a beginner in using jax and pytree, so I would appreciate it if you could let me know if I am using it incorrectly. ===error message 1===  ===error message 2===   What jax/jaxlib version are you using? jax0.4.21 jaxlib0.4.21  Which accelerator(s) are you using? cpu  Additional system info? print(numpy.__version__):1.24.3,   print(sys.version):3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)],    print(platform.uname()):uname_result(system='Windows', node='DESKTOPTU7H8UO', release='10', version='10.0.22621', machine='AMD64')  NVIDIA GPU info _No response_",2023-12-07T06:00:26Z,better_errors,closed,0,2,https://github.com/jax-ml/jax/issues/18859,The problem is  you're returning a scalar `0.` as the gradient for the `data` input. These are not matching pytree structures or matching array shapes. You want either:  to explicitly pass zeros for every position; or  as JAX understands a `None` to mean passing a zero gradient for the whole argument. I agree that this definitely isn't obvious though! The errors you get here don't communicate this at all. I've labelled this issue as a case where JAX needs a better error.,Thank you for pointing out the error in my code. I appreciate your guidance on the correct usage of the library!
539,"以下是一个github上的jax下的一个issue, 标题是([Pallas] Add missing shape checks in the Pallas FlashAttention kernel for TPUs)， 内容是 ([Pallas] Add missing shape checks in the Pallas FlashAttention kernel for TPUs Missing shape checks can cause hard to understand runtime errors caused by OOB checks inserted by XLA. We weren't verifying that the attention bias and the segment ids have the shapes we were expecting.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Pallas] Add missing shape checks in the Pallas FlashAttention kernel for TPUs,[Pallas] Add missing shape checks in the Pallas FlashAttention kernel for TPUs Missing shape checks can cause hard to understand runtime errors caused by OOB checks inserted by XLA. We weren't verifying that the attention bias and the segment ids have the shapes we were expecting.,2023-12-06T11:08:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18837
558,"以下是一个github上的jax下的一个issue, 标题是(Fixed failing shardmap error message builder when using functions with variable number of arguments)， 内容是 (Previously, when using `shard_map` with a function that uses a variable number of arguments (i.e. `f(*args)`), if you made a mistake in your shard_map spec, instead of the really nicely formatted error message, you would actually get this error instead.  This PR fixes the issue.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fixed failing shardmap error message builder when using functions with variable number of arguments,"Previously, when using `shard_map` with a function that uses a variable number of arguments (i.e. `f(*args)`), if you made a mistake in your shard_map spec, instead of the really nicely formatted error message, you would actually get this error instead.  This PR fixes the issue.",2023-12-05T15:02:24Z,,closed,0,2,https://github.com/jax-ml/jax/issues/18823,Any updates on this?,"I thought I had sent this comment before posting that PR, but it seems I didn't actually click 'Comment'! Oops: > Thanks for finding this issue, and suggesting a fix! >  > However I think we can still provide the parameter name: we can identify when the argument position must be bound to a varargs parameter, and thus still print the extra error info. >  > I attempted that in CC([shardmap] fix varargs error message bug), which I marked as coauthored by you. What do you think? Anyway, we basically merged this in CC([shardmap] fix varargs error message bug) so I'll close this PR."
722,"以下是一个github上的jax下的一个issue, 标题是(Allow compilation cache to be saved from process indices that are not process index 0)， 内容是 (At present, this check stops the compilation cache from being written on any process that is not process index 0. This makes sense if the compilation cache directory resides on shared storage. However, in our case, we do not wish to put this directory on shared storage and would instead prefer to save it on every process. WDYT of having an enum state `jax_persistent_cache_write` with values `['always', 'never', 'on_process_0']` to control this behavior?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Allow compilation cache to be saved from process indices that are not process index 0,"At present, this check stops the compilation cache from being written on any process that is not process index 0. This makes sense if the compilation cache directory resides on shared storage. However, in our case, we do not wish to put this directory on shared storage and would instead prefer to save it on every process. WDYT of having an enum state `jax_persistent_cache_write` with values `['always', 'never', 'on_process_0']` to control this behavior?",2023-12-05T07:47:17Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/18819,"This behaviour is supposed to be controlled by the `jax_share_binary_between_hosts` flag: https://github.com/google/jax/blob/42724ebc73e2bd06d5fc9c803c79e33377c7d93f/jax/_src/config.pyL1242L1249 However, the code is currently written in a way that this flag would be disregarded if `is_multi_process` is true.","More debugging on this. The lines https://github.com/google/jax/blob/7a3e2140fbda4674da855f611f515d271adba1a1/jax/_src/compiler.pyL387L389 may not be the root cause. The reason is that, on TPU v332, I added  before these lines, but it only prints:  which means that `distributed.global_state.process_id != 0` is always `False`.",I've tested on TPU v332 and the compiled programs are indeed saved on all hosts. Seems that the issue is fixed now.,Screenshot: ![](https://github.com/userattachments/assets/0b22d2d63f5643f4b9c5307a64b16816)
907,"以下是一个github上的jax下的一个issue, 标题是(Jax GPU is slower than in CPU)， 内容是 ( Description Hi, I am trying to run Jax on an Nvidia GPU on a Ubuntu 20.04 computer. I succeeded to install it, but when I run my program, it is slower than with CPU. I tried to put the code from https://jax.readthedocs.io/en/latest/gpu_performance_tips.htmlxlaperformanceflags and https://jax.readthedocs.io/en/latest/gpu_performance_tips.htmlncclflags, at the beginning of my python file, but there is no change of the performances. Thanks in advance for helping me.  What jax/jaxlib version are you using? jax and jaxlib 0.4.19  Which accelerator(s) are you using? Nvidia GPU  Additional system info? Python 3.11, Ubuntu 20.04, Cuda 12.3, cudnn 8.9, driver Cuda 545.23.08  NVIDIA GPU info !image)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax GPU is slower than in CPU," Description Hi, I am trying to run Jax on an Nvidia GPU on a Ubuntu 20.04 computer. I succeeded to install it, but when I run my program, it is slower than with CPU. I tried to put the code from https://jax.readthedocs.io/en/latest/gpu_performance_tips.htmlxlaperformanceflags and https://jax.readthedocs.io/en/latest/gpu_performance_tips.htmlncclflags, at the beginning of my python file, but there is no change of the performances. Thanks in advance for helping me.  What jax/jaxlib version are you using? jax and jaxlib 0.4.19  Which accelerator(s) are you using? Nvidia GPU  Additional system info? Python 3.11, Ubuntu 20.04, Cuda 12.3, cudnn 8.9, driver Cuda 545.23.08  NVIDIA GPU info !image",2023-12-04T23:12:00Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/18816,"Thanks for the report! It will be hard to help without more information. For example, if your script is I/O bound, then the computational backend will not make much difference when it comes to endtoend execution. Can you put together a minimal reproducible example? Also please take a look at FAQ: Benchmarking JAX code for general tips on getting meaningful benchmarks of JAX code.","For a reproductible example, I got one optimization problem : you need to install the package noloadj (pip install noloadj) and run the 'optimize_mono' file  (in the zip folder attached to this message). With CPU optimization converges in 45 secunds, and with GPU in 55 secunds. I runned that code one year ago with an old jax version (0.1.67) and at  that time, optimization converges in 25 secunds with GPU, which was coherent. reproductible_example.zip","Thanks  if you can provide a selfcontained example without the need to download and extract a zip file, that would be helpful.","If you want a simpler example, you can test this :  Optimization converges in 13 secunds in CPU, and in 15 secunds in GPU. noloadj is a solvingoptimization problem library I contribued to develop. I just like to know if you have same performances than me, to understand if it is a problem of installation or a problem of parallelization of my code.","For the code example above, I also tested with a jacobian computation (jacfwd) instead of an optimization, and in that case, GPU is faster than CPU, which is coherent. ","Thanks  I'm not familiar with the `noloadj` package, and it look like it contains a pretty significant amount of code. I'm not sure offhand why it would execute faster on CPU than GPU; figuring out why would involve doing some profiling to look for the most costly operations on each device. There's some information on that here if you're interested in digging in: https://jax.readthedocs.io/en/latest/profiling.html Best of luck!",It looks like there's not enough information to answer this here – I'm going to close the issue because there's been no activity in quite a while. Please feel free to open another issue if you have further questions. Thanks!
768,"以下是一个github上的jax下的一个issue, 标题是(Getting UserWarning: cloud_tpu_init failed)， 内容是 ( Description Trying to use TPU in CoLab and getting an error that asks to report a bug.  CoLab: https://colab.research.google.com/drive/1tdAuqHBPTRV1lHXnfMv3CIDLDdCMEq?usp=sharing  What jax/jaxlib version are you using? 0.3.25 0.3.25  Which accelerator(s) are you using? TPU  Additional system info? 1.23.5 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] uname_result(system='Linux', node='e2a721115727', release='5.15.120+', version=' CC(Python 3 compatibility issues) SMP Wed Aug 30 11:19:59 UTC 2023', machine='x86_64')  NVIDIA GPU info N/A)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Getting UserWarning: cloud_tpu_init failed," Description Trying to use TPU in CoLab and getting an error that asks to report a bug.  CoLab: https://colab.research.google.com/drive/1tdAuqHBPTRV1lHXnfMv3CIDLDdCMEq?usp=sharing  What jax/jaxlib version are you using? 0.3.25 0.3.25  Which accelerator(s) are you using? TPU  Additional system info? 1.23.5 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] uname_result(system='Linux', node='e2a721115727', release='5.15.120+', version=' CC(Python 3 compatibility issues) SMP Wed Aug 30 11:19:59 UTC 2023', machine='x86_64')  NVIDIA GPU info N/A",2023-12-02T05:07:05Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18780,"JAX versions 0.4 and greater do not support Colab TPUs at the moment because Colab TPUs use an older software architecture. That might change in the future, but at the moment if you want to use TPUs, your options are: a) use Colab TPU, but use an old JAX release (< 0.4). You're mostly on your own if you do this, since these JAX releases are now quite old and we aren't going to make fixes to old releases. I'd recommend against this. b) use Kaggle notebooks, which give a very similar experience to Colab, but offer TPUs supported by JAX, or c) use a Google Cloud TPU VM, on which you can run a selfhosted Colab kernel and connect to it from Colab. Closing because I don't think there's anything for us to do here. Hope that helps!",This is helpful! Thank you very much for the fast answer!
1264,"以下是一个github上的jax下的一个issue, 标题是(New random key design not numpy compatible)， 内容是 ( Description The new random key design introduces some sharpedges when interoperating with numpy.  This leads to problems in a number of dependent libraries built on jax. For example in chex https://github.com/googledeepmind/chex/issues/318:  I'm also running into issues with Orbax and serializing the random keys since the `dtype` cannot be converted to a valid numpy array. The problem is that Orbax tries to write `dtype` metadata by converting the jax `dtype` to a numpy `dtype`. But this of course leads to:  I can write a custom TypeHandler or a forward and inverse transformation for the serialization, this is fine. The problem is that the keys produced by `jax.random.key` hide their data under private attributes `_base_array` and `_impl`. These should either be public, or jax should properly serialize the keys with the numpy API. It would be better if numpy conversion would just drop the PRNG implementation metadata in favor of compatibility:   What jax/jaxlib version are you using? 0.4.20 0.4.20  Which accelerator(s) are yo)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,New random key design not numpy compatible," Description The new random key design introduces some sharpedges when interoperating with numpy.  This leads to problems in a number of dependent libraries built on jax. For example in chex https://github.com/googledeepmind/chex/issues/318:  I'm also running into issues with Orbax and serializing the random keys since the `dtype` cannot be converted to a valid numpy array. The problem is that Orbax tries to write `dtype` metadata by converting the jax `dtype` to a numpy `dtype`. But this of course leads to:  I can write a custom TypeHandler or a forward and inverse transformation for the serialization, this is fine. The problem is that the keys produced by `jax.random.key` hide their data under private attributes `_base_array` and `_impl`. These should either be public, or jax should properly serialize the keys with the numpy API. It would be better if numpy conversion would just drop the PRNG implementation metadata in favor of compatibility:   What jax/jaxlib version are you using? 0.4.20 0.4.20  Which accelerator(s) are yo",2023-12-01T14:00:34Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18767,"Thanks for the report. I think this is working as expected: the new key data type is not recognized by numpy, so you will not be able to use it with numpy functions. We understand that some downstream libraries are not yet compatible with the new key types; this is the reason we have not yet switched over to recommending the newstyle universally (e.g. all the JAX docs still recommend `jax.random.PRNGKey` at this point).","> Thanks for the report. I think this is working as expected: the new key data type is not recognized by numpy, so you will not be able to use it with numpy functions. >  > We understand that some downstream libraries are not yet compatible with the new key types; this is the reason we have not yet switched over to recommending the newstyle universally (e.g. all the JAX docs still recommend `jax.random.PRNGKey` at this point). I see, thanks for clarifying. I thought this was unintentional."
1076,"以下是一个github上的jax下的一个issue, 标题是(JAX should allow stack frames to be hidden in tracebacks)， 内容是 (**Context** When `jax.config.include_full_tracebacks_in_locations` is set to `True`, JAX propagates Python stack trace metadata to XLA. This can be useful for debugging. (If this option is set to `False`, only a single filename + line number is propagated). For models that have been constructed using frameworks layered on top of JAX, these stack traces can be quite noisy with many frames coming from implementation details of the frameworks. JAX already suppresses its own frames. An example is  which contains various ""wrapper"", ""wrapped"", etc. frames. **Suggestion** If JAX provided some relevant tools, frameworks could hint that some of their implementation details should not be shown by default in tools that present this information. JAX already does something similar internally to suppress its own implementation details. cc:   )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,JAX should allow stack frames to be hidden in tracebacks,"**Context** When `jax.config.include_full_tracebacks_in_locations` is set to `True`, JAX propagates Python stack trace metadata to XLA. This can be useful for debugging. (If this option is set to `False`, only a single filename + line number is propagated). For models that have been constructed using frameworks layered on top of JAX, these stack traces can be quite noisy with many frames coming from implementation details of the frameworks. JAX already suppresses its own frames. An example is  which contains various ""wrapper"", ""wrapped"", etc. frames. **Suggestion** If JAX provided some relevant tools, frameworks could hint that some of their implementation details should not be shown by default in tools that present this information. JAX already does something similar internally to suppress its own implementation details. cc:   ",2023-12-01T12:50:06Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/18764
476,"以下是一个github上的jax下的一个issue, 标题是(Option to sanitise MLIR dumps)， 内容是 (When JAX is exporting MLIR (via whatever incantation you like, my preferred one is `JAX_DUMP_IR_TO=""tmp/jax_logs""`) a lot of sensitive information is included, in the form of lines that look like this  It would be good to have an option to emit sanitised mlir dumps. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Option to sanitise MLIR dumps,"When JAX is exporting MLIR (via whatever incantation you like, my preferred one is `JAX_DUMP_IR_TO=""tmp/jax_logs""`) a lot of sensitive information is included, in the form of lines that look like this  It would be good to have an option to emit sanitised mlir dumps. ",2023-11-29T23:08:39Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/18734,This should be easy enough to do. We have code in the compilation cache that strips MLIR location information for caching. We just need to do the same in the IR dumping path.
493,"以下是一个github上的jax下的一个issue, 标题是(Downgrade a bunch of logging to DEBUG)， 内容是 (The logs related to compilation cache ended up being quite chatty, which is quite unlike the other logs in JAX. This downgrades a bunch of them to debug, as they can always be enabled independently using JAX config. This should also fix the recent failures in logging_test.py.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,Downgrade a bunch of logging to DEBUG,"The logs related to compilation cache ended up being quite chatty, which is quite unlike the other logs in JAX. This downgrades a bunch of them to debug, as they can always be enabled independently using JAX config. This should also fix the recent failures in logging_test.py.",2023-11-29T12:12:23Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18724
374,"以下是一个github上的jax下的一个issue, 标题是(Replace apply_primitive internals with `jax.jit`.)， 内容是 (Replace apply_primitive internals with `jax.jit`. This allows deletion of a lot of code and leads to ~40% eager performance speedup. Benchmarks: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Replace apply_primitive internals with `jax.jit`.,Replace apply_primitive internals with `jax.jit`. This allows deletion of a lot of code and leads to ~40% eager performance speedup. Benchmarks: ,2023-11-29T04:46:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18722
1289,"以下是一个github上的jax下的一个issue, 标题是([shard-map] fix transpose replication checking bug with integer_pow)， 内容是 (At HEAD this `shard_map` code crashes with a replication checking error:   The narrow issue is that the JVP of `integer_pow[y=2] x` produces a `mul x 2`, and so when `x` is devicevarying      across axis `'i'`, our checking rule is unhappy that `mul` gets two arguments with different replication types (where the replication type of `2` is just `{}`, i.e. it's not devicevarying over any axes). This is not an issue with differentiating `pow x 2`, since we would rewrite that to `pow x (pbroadcast[axes='i'] 2)`! The broader issue is that we're running checks _after_ transformations, but only performing the `pbroadcast`inserting rewrite _before_ transformations. That inconsistency means that rules, especially constants in rules, don't have `pbroadcast`s inserted. (We could try to be consistent about where we apply both: either do       `pbroadcast`insertion _after_ transformations, i.e. assuming the transformation rules themselves may require         `pbroadcast` operations in them, or else do the checking _before_ transformations,)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard-map] fix transpose replication checking bug with integer_pow,"At HEAD this `shard_map` code crashes with a replication checking error:   The narrow issue is that the JVP of `integer_pow[y=2] x` produces a `mul x 2`, and so when `x` is devicevarying      across axis `'i'`, our checking rule is unhappy that `mul` gets two arguments with different replication types (where the replication type of `2` is just `{}`, i.e. it's not devicevarying over any axes). This is not an issue with differentiating `pow x 2`, since we would rewrite that to `pow x (pbroadcast[axes='i'] 2)`! The broader issue is that we're running checks _after_ transformations, but only performing the `pbroadcast`inserting rewrite _before_ transformations. That inconsistency means that rules, especially constants in rules, don't have `pbroadcast`s inserted. (We could try to be consistent about where we apply both: either do       `pbroadcast`insertion _after_ transformations, i.e. assuming the transformation rules themselves may require         `pbroadcast` operations in them, or else do the checking _before_ transformations,",2023-11-28T21:48:50Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/18711
787,"以下是一个github上的jax下的一个issue, 标题是(building jaxlib on Windows10: An error occurred during the fetch of repository 'local_config_cuda')， 内容是 ( Description Greetings I am trying to build jaxlib on Windows 10 within a python=3.11 env using the following command:             python .\build\build.py `             >>   enable_cuda `             >>   cuda_path='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3' `             >>   cudnn_path='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3' `             >>   cuda_version='12.3' `             >>   cudnn_version='8.9.' Below is the output:              _   _  __  __                  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,building jaxlib on Windows10: An error occurred during the fetch of repository 'local_config_cuda', Description Greetings I am trying to build jaxlib on Windows 10 within a python=3.11 env using the following command:             python .\build\build.py `             >>   enable_cuda `             >>   cuda_path='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3' `             >>   cudnn_path='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3' `             >>   cuda_version='12.3' `             >>   cudnn_version='8.9.' Below is the output:              _   _  __  __                  ++,2023-11-28T20:49:10Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18707,"JAX for Windows GPU is communitysupported. i.e., we will accept PRs to fix it, but we don't support it ourselves. If you want a supported path for JAX on Windows, either use the CPU build on Windows (which is on pypi, so you can just `pip install` it), or you can use the Linux CUDA build under WSL. Hope that helps!"
270,"以下是一个github上的jax下的一个issue, 标题是(Fix indexing bug when querying _input_layouts)， 内容是 (Fix indexing bug when querying _input_layouts)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix indexing bug when querying _input_layouts,Fix indexing bug when querying _input_layouts,2023-11-28T04:51:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18693
1282,"以下是一个github上的jax下的一个issue, 标题是(Spurious data during copying on multiple GPU Jax/ROCm setup )， 内容是 ( Description jax.device_put doesn't seem to work across multiple AMD GPUs. I would expect the following to copy the same data from the Mi25 to the Mi60, but instead, I see what appears to be spurious data(sometimes random numbers, sometimes zeros). The following is executed in the `rocm/jax:rocm5.7.0jax0.4.20py3.11.0` docker container, although I get similarly spurious results running on the host against jax://github.com/ROCmSoftwarePlatform/jax/releases/download/jaxlibv0.4.20/jaxlib0.4.20+rocm570cp311cp311manylinux2014_x86_64.whl as well.  prints out: x=Array([1, 2], dtype=int32) x.device()=rocm(id=0) x.dtype=dtype('int32') a=Array([0, 0], dtype=int32) a.device()=rocm(id=1) a.dtype=dtype('int32')  What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20  Which accelerator(s) are you using? Dual GPU, AMD Mi25 + AMD Mi60  Additional system info? Python 3.11, Linux x86 in docker; 1.26.2 3.11.0 (main, Nov 16 2023, 20:45:15) [GCC 9.4.0] uname_result(system='Linux', node='fb9e20c7dcf8', release='6.2.037generic', version=' C)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Spurious data during copying on multiple GPU Jax/ROCm setup ," Description jax.device_put doesn't seem to work across multiple AMD GPUs. I would expect the following to copy the same data from the Mi25 to the Mi60, but instead, I see what appears to be spurious data(sometimes random numbers, sometimes zeros). The following is executed in the `rocm/jax:rocm5.7.0jax0.4.20py3.11.0` docker container, although I get similarly spurious results running on the host against jax://github.com/ROCmSoftwarePlatform/jax/releases/download/jaxlibv0.4.20/jaxlib0.4.20+rocm570cp311cp311manylinux2014_x86_64.whl as well.  prints out: x=Array([1, 2], dtype=int32) x.device()=rocm(id=0) x.dtype=dtype('int32') a=Array([0, 0], dtype=int32) a.device()=rocm(id=1) a.dtype=dtype('int32')  What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20  Which accelerator(s) are you using? Dual GPU, AMD Mi25 + AMD Mi60  Additional system info? Python 3.11, Linux x86 in docker; 1.26.2 3.11.0 (main, Nov 16 2023, 20:45:15) [GCC 9.4.0] uname_result(system='Linux', node='fb9e20c7dcf8', release='6.2.037generic', version=' C",2023-11-27T14:56:12Z,bug,open,1,3,https://github.com/jax-ml/jax/issues/18681,"I will note that although the Mi25 GPU is no longer officially supported by AMD, I'm able to run PyTorch models just fine on the Mi25 with the latest stable PyTorch. I want to switch to Jax or Tensorflow since taking PyTorch models into production with distributed training or jitted models is not straightforward, whilst it seems Jax and Tensorflow have out of the box support for this.","Heterogeneous GPUs is also technically not supported so perhaps the older GPU there is not playing nice. This does work as expected with 2 MI250s in that same container. $ python test.py 20231221 20:23:15.975131: E external/xla/xla/stream_executor/plugin_registry.cc:90] Invalid plugin kind specified: DNN x=Array([1, 2], dtype=int32) x.device()=rocm(id=0) x.dtype=dtype('int32') a=Array([1, 2], dtype=int32) a.device()=rocm(id=1) a.dtype=dtype('int32') $ rocminfo | grep gfx   Name:                    gfx90a       Name:                    amdgcnamdamdhsagfx90a:sramecc+:xnack   Name:                    gfx90a       Name:                    amdgcnamdamdhsagfx90a:sramecc+:xnack   Name:                    gfx90a       Name:                    amdgcnamdamdhsagfx90a:sramecc+:xnack   Name:                    gfx90a       Name:                    amdgcnamdamdhsagfx90a:sramecc+:xnack",Seems to work when using the same GPUs in a machine.
1010,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Simplify the indexing computations to be compatible with shape polymorphism)， 内容是 (Currently, we do not support shape polymorphism when we index with a slice, e.g., `x[a:b:c]`, and insted direct the user to use to `lax.dynamic_slice`. This is only because so far we have not tried to ensure that the index and bounds checking computations in gather are compatible with shape polymorphism. The problem was that there were a lot of conditionals, e.g., `if start >= stop` that cannot be handled in general in presence of symbolic shapes. Here we introduce a new helper function `_preprocess_slice` to contain all the computations for the start and the size of the slice. To test that this does not break the JAX index computations, I ran the tests with `JAX_NUM_GENERATED_CASES=1000`, especially the `lax_numpy_indexer_test.py`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Simplify the indexing computations to be compatible with shape polymorphism,"Currently, we do not support shape polymorphism when we index with a slice, e.g., `x[a:b:c]`, and insted direct the user to use to `lax.dynamic_slice`. This is only because so far we have not tried to ensure that the index and bounds checking computations in gather are compatible with shape polymorphism. The problem was that there were a lot of conditionals, e.g., `if start >= stop` that cannot be handled in general in presence of symbolic shapes. Here we introduce a new helper function `_preprocess_slice` to contain all the computations for the start and the size of the slice. To test that this does not break the JAX index computations, I ran the tests with `JAX_NUM_GENERATED_CASES=1000`, especially the `lax_numpy_indexer_test.py`.",2023-11-27T10:51:20Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18679
1209,"以下是一个github上的jax下的一个issue, 标题是(Clarify the Applicability and Efficiency of Loss Function in autodiff_cookbook)， 内容是 (In this PR, I am introducing additional comments to the loss function within the autodiff_cookbook to enhance the understanding of its scope and efficiency. The current implementation is optimized for binary targets, a detail that is now explicitly mentioned to guide beginners and prevent any potential confusion when it comes to more complex scenarios, such as label smoothing or multiclass targets. Furthermore, I've included an explanation on how this approach conserves computational resources by applying the log function selectively, which results in fewer floatingpoint operations (FLOPs). This commentary aims to provide clarity on why this method is efficient for the provided example, while also indicating that a different approach would be necessary for nonbinary cases. I trust that these annotations will enrich the educational value of the cookbook and I'm grateful for the opportunity to contribute to its precision and accessibility.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Clarify the Applicability and Efficiency of Loss Function in autodiff_cookbook,"In this PR, I am introducing additional comments to the loss function within the autodiff_cookbook to enhance the understanding of its scope and efficiency. The current implementation is optimized for binary targets, a detail that is now explicitly mentioned to guide beginners and prevent any potential confusion when it comes to more complex scenarios, such as label smoothing or multiclass targets. Furthermore, I've included an explanation on how this approach conserves computational resources by applying the log function selectively, which results in fewer floatingpoint operations (FLOPs). This commentary aims to provide clarity on why this method is efficient for the provided example, while also indicating that a different approach would be necessary for nonbinary cases. I trust that these annotations will enrich the educational value of the cookbook and I'm grateful for the opportunity to contribute to its precision and accessibility.",2023-11-27T03:56:14Z,,open,0,2,https://github.com/jax-ml/jax/issues/18677,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.","Please feel free to make any edits to my added comments to better suit the repository's style, as English is not my first language. Your expertise in clarifying these points is greatly appreciated."
545,"以下是一个github上的jax下的一个issue, 标题是(Trying to use Objax on Jax-metal)， 内容是 ( Description I am trying to use objax.nn.Conv2D and I keep getting an XlaRuntimeError :(  xlaruntimeerror.txt offendingline.txt  What jax/jaxlib version are you using? Jax v0.4.11, Jaxmetal 0.0.4, jaxlib v0.4.11  Which accelerator(s) are you using? GPU  Additional system info? Python 3.10.12, MacOS M1  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Trying to use Objax on Jax-metal," Description I am trying to use objax.nn.Conv2D and I keep getting an XlaRuntimeError :(  xlaruntimeerror.txt offendingline.txt  What jax/jaxlib version are you using? Jax v0.4.11, Jaxmetal 0.0.4, jaxlib v0.4.11  Which accelerator(s) are you using? GPU  Additional system info? Python 3.10.12, MacOS M1  NVIDIA GPU info _No response_",2023-11-26T01:48:52Z,bug Apple GPU (Metal) plugin,closed,0,1,https://github.com/jax-ml/jax/issues/18674,The issue is fixed in jaxmetal 0.0.7. Pls reopen the issue if otherwise. 
1291,"以下是一个github上的jax下的一个issue, 标题是(Resharding across MGPUs results in long series of cuMemAlloc_v2 calls)， 内容是 ( Description I'm implementing a time marching simulation across multiple GPUs. The calculation has field arrays sharded in one axis, and operators sharded in another (actual implementation involves ffts constrained using `experimental.custom_partitioning`, but I've omitted that here for simplicity). I use `lower `and `compile `for AOT compilation to make it easy for me to benchmark actual runtime. What I'm seeing is that on first execution a long time is spent executing a series of cuMemAlloc_v2 calls on each device in a series of streams of the form `Stream N(Memset)`. The time this takes appears to grow with the square of the GPUs. For the minimal example below I see the following:  2 GPU first call takes 2.2 s  4 GPU first call takes 7.8 s  8 GPU first call takes 29 s  16 GPU first call takes 118 s My questions: 1. Are these calls expected? 2. If so, is it expected that they should take so long? Below is a minimal example. In practice I'm using `donate_argnames` and specifying `in_shardings` and `out_shardings`, but have om)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Resharding across MGPUs results in long series of cuMemAlloc_v2 calls," Description I'm implementing a time marching simulation across multiple GPUs. The calculation has field arrays sharded in one axis, and operators sharded in another (actual implementation involves ffts constrained using `experimental.custom_partitioning`, but I've omitted that here for simplicity). I use `lower `and `compile `for AOT compilation to make it easy for me to benchmark actual runtime. What I'm seeing is that on first execution a long time is spent executing a series of cuMemAlloc_v2 calls on each device in a series of streams of the form `Stream N(Memset)`. The time this takes appears to grow with the square of the GPUs. For the minimal example below I see the following:  2 GPU first call takes 2.2 s  4 GPU first call takes 7.8 s  8 GPU first call takes 29 s  16 GPU first call takes 118 s My questions: 1. Are these calls expected? 2. If so, is it expected that they should take so long? Below is a minimal example. In practice I'm using `donate_argnames` and specifying `in_shardings` and `out_shardings`, but have om",2023-11-24T10:51:38Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/18666,Any thoughts on this? 
1243,"以下是一个github上的jax下的一个issue, 标题是(Solution for https://github.com/google/jax/pull/18641)， 内容是 (In tests/BUILD file, set the environment variable to 0 for lobpcg_test. When using a clean installation, such as a base Python and relying solely on the testrequirements.txt from /build, as per the instructions in the documentation, the tests that require matplotlib tend to fail. Users are required to install matplotlib independently. To prevent these failures by default, it is advisable to disable matplotlib: name = ""lobpcg_test"",     srcs = [""lobpcg_test.py""],     env = {""LOBPCG_EMIT_DEBUG_PLOTS"": ""0""}, If someone wishes to visualize results using matplotlib, they should be informed in the documentation (developer.rmd). For example in section Running the tests, we may add extra information: Moreover, if you require visualization generated by Matplotlib during the tests, it is necessary to install it separately using the command:  Additionally, set the LOBPCG_EMIT_DEBUG_PLOTS environment variable to 1 in the /tests/BUILD file or pass it via the command line. For example, for a Bazel test case: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Solution for https://github.com/google/jax/pull/18641,"In tests/BUILD file, set the environment variable to 0 for lobpcg_test. When using a clean installation, such as a base Python and relying solely on the testrequirements.txt from /build, as per the instructions in the documentation, the tests that require matplotlib tend to fail. Users are required to install matplotlib independently. To prevent these failures by default, it is advisable to disable matplotlib: name = ""lobpcg_test"",     srcs = [""lobpcg_test.py""],     env = {""LOBPCG_EMIT_DEBUG_PLOTS"": ""0""}, If someone wishes to visualize results using matplotlib, they should be informed in the documentation (developer.rmd). For example in section Running the tests, we may add extra information: Moreover, if you require visualization generated by Matplotlib during the tests, it is necessary to install it separately using the command:  Additionally, set the LOBPCG_EMIT_DEBUG_PLOTS environment variable to 1 in the /tests/BUILD file or pass it via the command line. For example, for a Bazel test case: ",2023-11-23T11:55:58Z,pull ready,closed,0,10,https://github.com/jax-ml/jax/issues/18660,One last thing  can you squash your changes into a single commit please? See https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests Thanks!,Ok I only must read https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests,Ok I only must read https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests,"> One last thing  can you squash your changes into a single commit please? See https://jax.readthedocs.io/en/latest/contributing.htmlsinglechangecommitsandpullrequests >  > Thanks! Ok, I done squashed commit. I hope it is good","Thanks! There are still three commits on the branch, so I think something went awry in the squashing process",I have a problem to create one single commit based on the two other commits before made. I am not git master ;),"Try something like this, assuming `origin` points to `https://github.com/mmarcinmichael/jax` and `upstream` points to `https://github.com/google/jax` ","Looks like it didn't work... If you followed my instructions, then it probably means that your `main` branch includes additional commits that are not in `upstream/main`, which I would recommend avoiding in general.","Try doing the `rebase` and the `reset` commands against `upstream/main` directly, i.e substitute this in the workflow above: ","Hi, I am sorry for the mess. Now, all should be ok."
708,"以下是一个github上的jax下的一个issue, 标题是(Memory profiling )， 内容是 ( Description We are trying to profile the memory usage for the DESC package, but we are getting an unreasonable memory estimation. The report by the pprof tool estimates the memory usage as about 3.4MB, but we are sure it should be ~2.9GB.  The package can be installed from PyPI: `pip install descopt`, and here is a code sample:    What jax/jaxlib version are you using? jax V0.4.14  jaxlib V0.4.14  Which accelerator(s) are you using? CPU  Additional system info? _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Memory profiling ," Description We are trying to profile the memory usage for the DESC package, but we are getting an unreasonable memory estimation. The report by the pprof tool estimates the memory usage as about 3.4MB, but we are sure it should be ~2.9GB.  The package can be installed from PyPI: `pip install descopt`, and here is a code sample:    What jax/jaxlib version are you using? jax V0.4.14  jaxlib V0.4.14  Which accelerator(s) are you using? CPU  Additional system info? _No response_  NVIDIA GPU info _No response_",2023-11-22T19:08:31Z,bug,open,1,0,https://github.com/jax-ml/jax/issues/18650
810,"以下是一个github上的jax下的一个issue, 标题是(Error in Automatic Differenciation for Functions with Powers of Zero)， 内容是 ( Description I am trying to vectorize a procedure, where I need to do the elementwise power of a large array before differentiating it. If I have only one element, I can use this code which gives the expected output:  However, when I try to vectorize this procedure, I get a different result.  Is this expected? If yes, could you please explain the convention/reason? Thank you!  What jax/jaxlib version are you using? 0.4.16 0.4.14  Which accelerator(s) are you using? _No response_  Additional system info? python 3.11.5 on macOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Error in Automatic Differenciation for Functions with Powers of Zero," Description I am trying to vectorize a procedure, where I need to do the elementwise power of a large array before differentiating it. If I have only one element, I can use this code which gives the expected output:  However, when I try to vectorize this procedure, I get a different result.  Is this expected? If yes, could you please explain the convention/reason? Thank you!  What jax/jaxlib version are you using? 0.4.16 0.4.14  Which accelerator(s) are you using? _No response_  Additional system info? python 3.11.5 on macOS  NVIDIA GPU info _No response_",2023-11-22T15:54:46Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18640,"Hi  thanks for the question! Using more recent jaxlib, I get the expected result. I think this issue was fixed by CC(fix pow jvp rule with int exponent (broken since 16419)), but you'll need to update to a more recent JAX release to get the fix – I think JAX v0.4.19 or newer should do it.",Indeed both codes snippets give the same output with JAX v0.4.19. Thank you!
588,"以下是一个github上的jax下的一个issue, 标题是(Accessing array memory to perform an operation causes a SIGSEGV (Address boundary error))， 内容是 ( Description Hi, I encounter a SIGSEGV error, when trying to use jax numpy and I don't know why.   What jax/jaxlib version are you using? jax version = '0.4.21.dev20231120', jaxlib version = '0.4.21.dev20231120'  Which accelerator(s) are you using? GPU  Additional system info python3.11 Ubuntu  NVIDIA GPU info ++  ++++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Accessing array memory to perform an operation causes a SIGSEGV (Address boundary error)," Description Hi, I encounter a SIGSEGV error, when trying to use jax numpy and I don't know why.   What jax/jaxlib version are you using? jax version = '0.4.21.dev20231120', jaxlib version = '0.4.21.dev20231120'  Which accelerator(s) are you using? GPU  Additional system info python3.11 Ubuntu  NVIDIA GPU info ++  ++++",2023-11-21T18:38:56Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/18625,Can you confirm what version of Python 3.11 you have installed? What's the ubuntu package version? If it's the rc version provided with Ubuntu 22.04 it's broken.,It is the rc version `Python 3.11.0rc1`. I will try with another version and then report back.,> It is the rc version `Python 3.11.0rc1`. I will try with another version and then report back. Do you recommend a specific version? ,"If you need a newer Python on Ubuntu, I recommend either the deadsnakes PPA (https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa) or use pyenv to build a copy from source.",I installed python3.12 from the deadsnakes repository and now my test case doesn't crash. I will test my actual use case and then close the issue if I encounter no erros. Thank you for you help :),It fixed my error! Thanks a ton!
856,"以下是一个github上的jax下的一个issue, 标题是(Gradient computations take more time for the first repetitions after compilation than de last)， 内容是 ( Description Hi, For benchmarking purposes, I need to measure the time spent to compute the gradient of some Flax model w.r.t. the model's parameters. The gradient is jitted, and a first run is performed for compilation. However, when running this code on a GPU, the five first computations are longer than the fifteen last, while computing the same thing. Is there an explanation for that?  Output:   What jax/jaxlib version are you using? jax v0.4.7, jaxlib v0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.11, Linux  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Gradient computations take more time for the first repetitions after compilation than de last," Description Hi, For benchmarking purposes, I need to measure the time spent to compute the gradient of some Flax model w.r.t. the model's parameters. The gradient is jitted, and a first run is performed for compilation. However, when running this code on a GPU, the five first computations are longer than the fifteen last, while computing the same thing. Is there an explanation for that?  Output:   What jax/jaxlib version are you using? jax v0.4.7, jaxlib v0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.11, Linux  NVIDIA GPU info ",2023-11-21T15:22:43Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/18622,"I suspect the issue is that you're not calling `block_until_ready` on the first invocation of `grad_fun(params)`, and so the subsequent calculations are being asynchronously dispatched while the device is still busy. When I change your compilation run to this:  I see more consistent timing in the first several runs of the benchmark.","Thank you for answering. I tried your solution and got the same results, unfortunately.",I'm unable to reproduce this on a Colab T4 TPU runtime with jax v0.4.20 and the `block_until_ready` that I suggested above. Can you try updating your jax and jaxlib version and see if that affects the result?
1259,"以下是一个github上的jax下的一个issue, 标题是(Bump actions/setup-python from 1 to 4)， 内容是 (Bumps actions/setuppython from 1 to 4.  Release notes Sourced from actions/setuppython's releases.  v4.0.0 What's Changed  Support for pythonversionfile input:  CC(rename ""minmax"" > ""optimizers"")  Example of usage:  uses: actions/setuppython   with:     pythonversionfile: '.pythonversion'  Read python version from a file  run: python my_script.py  There is no default python version for this setuppython major version, the action requires to specify either pythonversion input or pythonversionfile input. If the pythonversion input is not specified the action will try to read required version from file from pythonversionfile input.  Use pypyX.Y for PyPy pythonversion input:  CC(Error importing jax after certain tensorflow import)  Example of usage:  uses: actions/setuppython   with:     pythonversion: 'pypy3.9'  pypyX.Y kept for backward compatibility  run: python my_script.py    RUNNER_TOOL_CACHE environment variable is equal AGENT_TOOLSDIRECTORY:  CC(Change implementation of negative loglikelihood in README toy example)   Bugfix)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,Bump actions/setup-python from 1 to 4,"Bumps actions/setuppython from 1 to 4.  Release notes Sourced from actions/setuppython's releases.  v4.0.0 What's Changed  Support for pythonversionfile input:  CC(rename ""minmax"" > ""optimizers"")  Example of usage:  uses: actions/setuppython   with:     pythonversionfile: '.pythonversion'  Read python version from a file  run: python my_script.py  There is no default python version for this setuppython major version, the action requires to specify either pythonversion input or pythonversionfile input. If the pythonversion input is not specified the action will try to read required version from file from pythonversionfile input.  Use pypyX.Y for PyPy pythonversion input:  CC(Error importing jax after certain tensorflow import)  Example of usage:  uses: actions/setuppython   with:     pythonversion: 'pypy3.9'  pypyX.Y kept for backward compatibility  run: python my_script.py    RUNNER_TOOL_CACHE environment variable is equal AGENT_TOOLSDIRECTORY:  CC(Change implementation of negative loglikelihood in README toy example)   Bugfix",2023-11-20T17:23:12Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/18607,We need to update the comments as well; I'll do this separately.,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1260,"以下是一个github上的jax下的一个issue, 标题是(Bump actions/github-script from 6 to 7)， 内容是 (Bumps actions/githubscript from 6 to 7.  Release notes Sourced from actions/githubscript's releases.  v7.0.0 What's Changed  Add baseurl option by @​robandpdx in actions/githubscript CC(Add docstrings for lax.gather/scatter.) Expose asyncfunction argument type by @​viktorlott in actions/githubscript CC(Do JAX jit'd Python loops run faster than jit'd LAX loop constructs?), see for details https://github.com/actions/githubscriptusescriptswithjsdocsupport Update dependencies and use Node 20 by @​joshmgross in actions/githubscript CC(Implement np.float_power.)  New Contributors  @​navarroaxel made their first contribution in actions/githubscript CC(Fix average pooling to align the window element counts with the spatial dimensions.) @​robandpdx made their first contribution in actions/githubscript CC(Add docstrings for lax.gather/scatter.) @​viktorlott made their first contribution in actions/githubscript CC(Do JAX jit'd Python loops run faster than jit'd LAX loop constructs?)  Full Changelog: https://github.com/actions/githubscrip)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Bump actions/github-script from 6 to 7,"Bumps actions/githubscript from 6 to 7.  Release notes Sourced from actions/githubscript's releases.  v7.0.0 What's Changed  Add baseurl option by @​robandpdx in actions/githubscript CC(Add docstrings for lax.gather/scatter.) Expose asyncfunction argument type by @​viktorlott in actions/githubscript CC(Do JAX jit'd Python loops run faster than jit'd LAX loop constructs?), see for details https://github.com/actions/githubscriptusescriptswithjsdocsupport Update dependencies and use Node 20 by @​joshmgross in actions/githubscript CC(Implement np.float_power.)  New Contributors  @​navarroaxel made their first contribution in actions/githubscript CC(Fix average pooling to align the window element counts with the spatial dimensions.) @​robandpdx made their first contribution in actions/githubscript CC(Add docstrings for lax.gather/scatter.) @​viktorlott made their first contribution in actions/githubscript CC(Do JAX jit'd Python loops run faster than jit'd LAX loop constructs?)  Full Changelog: https://github.com/actions/githubscrip",2023-11-20T17:23:05Z,dependencies github_actions,closed,0,2,https://github.com/jax-ml/jax/issues/18606,We need to update the comments as well; I'll do this separately.,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
695,"以下是一个github上的jax下的一个issue, 标题是(Arctan2 gradient explosion for small inputs)， 内容是 ( Description Arctan2 seems to have a large gradient when the inputs are small. If training a neural network, this blows up the weights, destroying the network. I believe the gradient of `arctan2` should be clamped to 1.0. This behavior does not seem to be present in arctan  Related to https://github.com/google/jax/issues/15407  What jax/jaxlib version are you using? 0.4.19  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Arctan2 gradient explosion for small inputs," Description Arctan2 seems to have a large gradient when the inputs are small. If training a neural network, this blows up the weights, destroying the network. I believe the gradient of `arctan2` should be clamped to 1.0. This behavior does not seem to be present in arctan  Related to https://github.com/google/jax/issues/15407  What jax/jaxlib version are you using? 0.4.19  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ",2023-11-20T16:18:34Z,bug needs info,closed,0,3,https://github.com/jax-ml/jax/issues/18605,"Hi  thanks for the report. It looks to me like the gradient is returning the expected result – analytically, the gradient is $$ \frac{\partial}{\partial x}{\rm atan2}(y, x) = \frac{y}{x^2 + y^2} $$ $$ \frac{\partial}{\partial y}{\rm atan2}(y, x) = \frac{x}{x^2 + y^2} $$ It's clear here that as `x` and `y` approach zero, both partial derivatives diverge. On the other hand, the derivative of `arctan` is: $$ \frac{\rm d}{{\rm d}x} {\rm arctan}(x) = \frac{1}{1 + x^2} $$ so as `x` approaches zero, the derivative approaches 1. Both seem consistent with the output you're seeing from JAX. What do you think?","Whoops, good point. Even though `arctan` is wellbehaved, I suppose the division in a neural network is really the issue (we explicitly divide when calling `arctan(y / x)`, where as `arctan2(y, x)` divides implicitly). I am not sure how good of an idea it is, but I can clamp the gradient from `arctan2` to be in [1, 1] to prevent these explosions (at the cost of slower convergence). Is this the correct way to do so?  One final question: to get this to work over batches, I need to `vmap(clamped_atan2)`, as `jax.grad` only works for a single output. Is there a way to get it to broadcast like the original `arctan2` function?","You haven't said much about where you're using this function, but I would be hesitant to create custom autodiff rules that are deliberately producing incorrect derivatives for the function you're evaluating. I wonder if you might have better luck choosing an alternative function which has betterbehaved derivatives in the domains you're interested in?"
742,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Clean up the shape_poly_test.py)， 内容是 (When we recently moved much of shape_poly_test out of jax2tf we had to add a number of flags to avoid warnings (which are errors in GitHub CI). Here we clean the tests so that we can run them without the flags. The most common problem was that tests were relying on implicit rank promotion. We added a number of `jnp.expand_dims` to fix the rank and let the implicit broadcasting do the rest. The other problem solved here was for the `jax.random` functions, we add `jax.random.wrap_key_data` to turn arrays into keys.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Clean up the shape_poly_test.py,"When we recently moved much of shape_poly_test out of jax2tf we had to add a number of flags to avoid warnings (which are errors in GitHub CI). Here we clean the tests so that we can run them without the flags. The most common problem was that tests were relying on implicit rank promotion. We added a number of `jnp.expand_dims` to fix the rank and let the implicit broadcasting do the rest. The other problem solved here was for the `jax.random` functions, we add `jax.random.wrap_key_data` to turn arrays into keys.",2023-11-20T08:11:32Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18602
622,"以下是一个github上的jax下的一个issue, 标题是([shard_map] add eager axis_index implementation and tests)， 内容是 (This approach works because: 1. we can think of eager as only applying `jit` to individual primitives, which this implementation literally does; 2. when we run `jit(lambda: axis_index(axis_name))()`, the `axis_index` call gets staged into a jaxpr, and then we bind the `pjit_p` primitive, which works just like the noneager path (i.e. `shard_map`of`jit`of`collectives` already worked).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard_map] add eager axis_index implementation and tests,"This approach works because: 1. we can think of eager as only applying `jit` to individual primitives, which this implementation literally does; 2. when we run `jit(lambda: axis_index(axis_name))()`, the `axis_index` call gets staged into a jaxpr, and then we bind the `pjit_p` primitive, which works just like the noneager path (i.e. `shard_map`of`jit`of`collectives` already worked).",2023-11-19T19:12:37Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18599
630,"以下是一个github上的jax下的一个issue, 标题是(Pallas implementation of attention doesn't work on CloudTPU)， 内容是 ( Description  Got:   What jax/jaxlib version are you using? jax==0.4.21.dev20231117 jaxlib==0.4.21.dev20231117  Which accelerator(s) are you using? TPU  Additional system info Python 3.10.2 Uname=Linux t1vncfe84bb3w0 5.19.01022gcp CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llama,Pallas implementation of attention doesn't work on CloudTPU, Description  Got:   What jax/jaxlib version are you using? jax==0.4.21.dev20231117 jaxlib==0.4.21.dev20231117  Which accelerator(s) are you using? TPU  Additional system info Python 3.10.2 Uname=Linux t1vncfe84bb3w0 5.19.01022gcp CC(attempt to centerjustify the jax logo in readme)~22.04.1Ubuntu SMP Sun Apr 23 09:51:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux  NVIDIA GPU info _No response_,2023-11-18T01:18:22Z,bug TPU,closed,0,16,https://github.com/jax-ml/jax/issues/18590,Can you try the version in jax/experimental/ops/tpu/flash_attention.py?,"Sure, updated test script to   Got  Without the debug = True would get the same exception but without the logs of MLIR; so I pasted the one with debug = True.",Another data point: if I add `interpret=True` to `pl.pallas_call` inside of the flash_attention.py; then it passes.,What TPU version are you using?,I think you'll also need a nightly jaxlib for this to work.,"Updated today's nightlies:  jax==0.4.21.dev20231118 jaxlib==0.4.21.dev20231118 libtpunightly @ https://storage.googleapis.com/cloudtputpuvmartifacts/wheels/libtpunightly/libtpu_nightly0.1.dev20231118py3noneany.whl Issue still presists. (Note, in today's nightlies, it will first give `KeyError: 'xla_mosaic_dump_to'`; I commented that function out and then it will give back the same `RuntimeError: Expected a tiled memref` error.",One thing I see is that you need to swap the num_heads and seqlen dimension for TPU. Could you try that? Unsure of it will fix the error but worth trying.,"Tried, doesnt work. Why would it be differnt anyways? both `seqlen` and `n_heads` are just integers; and flash_attention should work bof both (seqlen=1000, n_heads=32) and (seqlen=32, n_heads=1000).  I am curious if you can repro my error or not. If not, I'd love to know your environment setup so I can use that exact one (I don't care of using the newest Jax, I just need a version that works). Thanks!","> Why would it be differnt anyways? In pallas, dimension order has physical implication. On TPUs, we need to pick some two dimensions to be layed out as vector register dimensions and the rest will be unrolled across many vector registers. In the case of flash attention, we lay the seqlen and head_dim across register dimensions and unroll over bs, heads. One last thing to try before I can do a more thorough investigation tomorrow would be to use seqlen=1024. Mosaic doesn't support arbitrary padding on all dimensions quite yet so it's worth trying powers of 2 for the last two dimensions. head_dim of 128 is already fine.","yes, the issue still persists with `seqlen=1024`. Just to understand more, you are saying the flipping variables `seqlen` and `n_heads` matter because the kernel might expect `seqlen > n_heads`? Because the name of python variables  definitely shouldn't matter.","Okay, I'll do a more thorough investigation tomorrow. Wrt to dimension order, the kernel expects a sequence length dimension (ie something usually bigger than number of heads) in the second to last position.",I was able to run it successfully on a Cloud TPU VM (v5e) with these versions:  I ran this script: ,Thanks! Confirmed that `jax @ git+https://github.com/google/jax.git` Also works on cloudtpu v48,"Reopening the issue, the flash attention still have the following 2 issues: 1. When `seqlen = 1` it gives `ValueError: block_q=128 should be smaller or equal to q_seq_len=1`. I understand that it probably it is by design at this point. However, I would like to ask for the support for the case of seqlen = 1 as this is the case during the `decode` phase for a typical LLM inference. 2. It's not faster than the reference implementation; which kinda of defeats the purpose of using a specialized kernel. Script used:  Output  The first run it's faster I guess it's faster to compile. But the subsequent runs are slower.","> When seqlen = 1 it gives ValueError: block_q=128 should be smaller or equal to q_seq_len=1. I understand that it probably it is by design at this point. However, I would like to ask for the support for the case of seqlen = 1 as this is the case during the decode phase for a typical LLM inference. You probably want a slightly different kernel for decoding. On a TPU, naively using a sequence length of 1 will result in an extremely padded matmul. We might want to do a matrix vector product or somethign else. > It's not faster than the reference implementation; which kinda of defeats the purpose of using a specialized kernel. You're seeing these results for 2 reasons: 1. You are using default block sizes, which tend to be slow. Try passing in `block_sizes` (https://github.com/google/jax/blob/c855bb0371fd7df3e2c33c0d153a23299b4f1988/jax/experimental/pallas/ops/tpu/flash_attention.pyL148) and sweep over larger ones. You should see significant performance improvements. 2. You are using a somewhat small sequence length. For sequence lengths <= 4096, XLA has some fusions that do something similar to flash attention so the expected improvement over XLA isn't that big. Once you go to 8k and above, you should see much bigger improvements.",Thanks Sharad! Yes I tried few block sizes indeed it got faster. Thanks for the pointers!
762,"以下是一个github上的jax下的一个issue, 标题是(If parameter to `get_aval` is AbstractValue return it.)， 内容是 (Hello,  While exploring the `abstracted_axes={0: ""n""}` I noticed that passing `AbstractValue`s as inputs to the `make_jaxpr` function will cause it to yield a `TypeError`. However, passing `AbstractValue`s as inputs to `make_jaxpr` is possible when `abstracted_axes=None`. To preserve this feature, this is a possible change. I believe passing `AbstractValue`s to `make_jaxpr` directly makes sense as a way to compile AOT via type signatures. EDIT: The following PR may also be relevant: https://github.com/google/jax/pull/18505)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,If parameter to `get_aval` is AbstractValue return it.,"Hello,  While exploring the `abstracted_axes={0: ""n""}` I noticed that passing `AbstractValue`s as inputs to the `make_jaxpr` function will cause it to yield a `TypeError`. However, passing `AbstractValue`s as inputs to `make_jaxpr` is possible when `abstracted_axes=None`. To preserve this feature, this is a possible change. I believe passing `AbstractValue`s to `make_jaxpr` directly makes sense as a way to compile AOT via type signatures. EDIT: The following PR may also be relevant: https://github.com/google/jax/pull/18505",2023-11-16T17:24:59Z,,closed,0,1,https://github.com/jax-ml/jax/issues/18561,"Thanks for the suggestion! But I don't think we want this change; `core.get_aval` should only be applied to objects representing JAX values, which in turn have JAX types. An `AbstractValue` is a JAX type, not a JAX value, so it doesn't fit. The right fix is probably in the calling code, not in `core.get_aval`."
1295,"以下是一个github上的jax下的一个issue, 标题是(Feature Request: A way to redefine mesh specifications within a shard_map)， 内容是 (There are situations where one way of labeling the same mesh can make certain communication patterns easier to code than others. However, there isn't really one mesh labeling strategy that is consistently easy to use across the space of all possible communication patterns. Currently, if a user or library developer wants to develop advanced communication patterns via `shard_map`, they'll be forced to either  a) Force end users to use only a predefined set of axis labels for certain libraries. If these are not consistent between libraries, then you can not use those libraries together. b) User explicitly passes axis labels for all ops. Both library developers and end users are forced to lug around passed axes names throughout their code. It's a large burden, and very fragile to change. This is what I propose instead: A new method `mesh_remap(fn: Callable, mesh: Mesh)` that can be used within any normal `shard_map`ed method. The basic use case looks like this:  Here, all we are doing when calling `gather_middle` is redefining how)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Feature Request: A way to redefine mesh specifications within a shard_map,"There are situations where one way of labeling the same mesh can make certain communication patterns easier to code than others. However, there isn't really one mesh labeling strategy that is consistently easy to use across the space of all possible communication patterns. Currently, if a user or library developer wants to develop advanced communication patterns via `shard_map`, they'll be forced to either  a) Force end users to use only a predefined set of axis labels for certain libraries. If these are not consistent between libraries, then you can not use those libraries together. b) User explicitly passes axis labels for all ops. Both library developers and end users are forced to lug around passed axes names throughout their code. It's a large burden, and very fragile to change. This is what I propose instead: A new method `mesh_remap(fn: Callable, mesh: Mesh)` that can be used within any normal `shard_map`ed method. The basic use case looks like this:  Here, all we are doing when calling `gather_middle` is redefining how",2023-11-15T03:53:52Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/18537
900,"以下是一个github上的jax下的一个issue, 标题是(Test failures: numerical error in`tests/qdwh_test.py::QdwhTest::testQdwhWithOnRankDeficientInput5`)， 内容是 ( Description A test failure was found when upgrading jaxlib in NixOS This error is hardwaredependent. I can reproduce the error on an intel machine with i513400F, but the error doesn't occur on an AMD platform with Ryzen7 6850HS. The jaxlib binary is CPU only with MKL enabled at compile time. The two machines shared the same jaxlib binary, which implies that the bug depends on the underlying hardware.  The output of the failed test    What jax/jaxlib version are you using? v0.4.20 (refs/tags/jaxv0.4.20)  Which accelerator(s) are you using? CPU  Additional system info Python 3.11 NixOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Test failures: numerical error in`tests/qdwh_test.py::QdwhTest::testQdwhWithOnRankDeficientInput5`," Description A test failure was found when upgrading jaxlib in NixOS This error is hardwaredependent. I can reproduce the error on an intel machine with i513400F, but the error doesn't occur on an AMD platform with Ryzen7 6850HS. The jaxlib binary is CPU only with MKL enabled at compile time. The two machines shared the same jaxlib binary, which implies that the bug depends on the underlying hardware.  The output of the failed test    What jax/jaxlib version are you using? v0.4.20 (refs/tags/jaxv0.4.20)  Which accelerator(s) are you using? CPU  Additional system info Python 3.11 NixOS  NVIDIA GPU info _No response_",2023-11-15T02:42:50Z,bug contributions welcome needs info CPU,open,0,8,https://github.com/jax-ml/jax/issues/18535,Does this reproduce with an upstream `pip` build of jax/jaxlib?,> Does this reproduce with an upstream pip build of jax/jaxlib?  (or others) This could be tested relatively easily by building `python3Packages.jax.override { jaxlib = jaxlibbin; }` or something thereabouts since `python3Packages.jaxlibbin` is pulled directly from PyPI.,"Yes, I can reproduce this problem using `pip` build jax/jaxlib on that same machine (the i513400F one). The test is conducted in an archlinux container with a fresh venv environment and `pip install ""jax[cpu]""`, and follow the test using pytest instruction. And it gives the exact same result:   pytest output     pip show jax jaxlib  ","Do me another favor: use upstream `scipy` in this test as well and disable MKL or similar if you're using it? Some of the linear algebra routines used by JAX come from `scipy`, and in turn they might be provided by MKL if you have MKLenabled scipy installed.","Sorry for the late reply. The second test was already run with the upstream version of scipy, jax, jaxlib, and no intel MKL involved. 1. > A test failure was found when [upgrading jaxlib in NixOS]    Here I used the NixOS's build, jax/jaxlib/scipy are built through Nix, and MKL is enabled. 3. > The test is conducted in an archlinux container with a fresh venv environment and pip install ""jax[cpu]"", and follow the test using pytest instruction.     Here everything were from upstream. I started from a clean archlinux container with only python/pip, and no MKL installed, then I created a new python venv, and `pip install U ""jax[cpu]""`, `pip install r build/testrequirements.txt`. So it should be a pretty clean environment. Here is everything in that venv, listed by `pip list`, and all from upstream. ","Hmm. I can't reproduce this. I used Ubuntu 22.04, Python 3.11 (from the deadsnakes PPA), and I tried: * a GCP c3 VM (Intel Sapphire Rapids) * a GCP n2 VM (Intel Cascade Lake) * a GCP n1 VM (Intel Skylake) At this point, I'm stuck. If I can't reproduce it, I can't debug it. I don't have access to that particular desktop Intel chip. Does it reproduce with Nix on a Cloud VM of some kind?","If you shared the output of `lscpu` on that machine, it might give me a clue.", I hope it helps.
960,"以下是一个github上的jax下的一个issue, 标题是(Avoid querying `named_shape` to support `DShapedArray`s.)， 内容是 (Hi, We are using JAX to compile Python to MLIR. We are currently exploring the implementation of dynamic tensor sizes support in JAX (i.e., `abstracted_axes`). I noticed that when using the option calling `make_jaxpr` with `abstracted_axes=something` and `return_shapes=True` an exception is raised. This is because `DShapedArray`s does not have a `named_shape` attribute which is needed in these lines of code:  I am not too familiar with Dynamic shapes in JAX yet, but implementing the changes allowed me to return a meaningful shape with the following value:  which appears to be sufficient. Happy to hear your thoughts here. I would really like to be able to use `abstracted_axes` and `return_shapes=True`. Thank you! :))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Avoid querying `named_shape` to support `DShapedArray`s.,"Hi, We are using JAX to compile Python to MLIR. We are currently exploring the implementation of dynamic tensor sizes support in JAX (i.e., `abstracted_axes`). I noticed that when using the option calling `make_jaxpr` with `abstracted_axes=something` and `return_shapes=True` an exception is raised. This is because `DShapedArray`s does not have a `named_shape` attribute which is needed in these lines of code:  I am not too familiar with Dynamic shapes in JAX yet, but implementing the changes allowed me to return a meaningful shape with the following value:  which appears to be sufficient. Happy to hear your thoughts here. I would really like to be able to use `abstracted_axes` and `return_shapes=True`. Thank you! :)",2023-11-13T21:10:56Z,,closed,0,3,https://github.com/jax-ml/jax/issues/18505,"Thanks for the suggestion! Actually, `named_shape` is kind of dead at the moment, so instead of adding it to a new place, I'd rather edit any place that expects the `named_shape` attribute so as not to expect it. Does that make sense?","Hi , Thanks for your feedback. There are several ways to fix this, I took the least invasive approach here, just modifying the `make_jaxpr_f` function to not assume that `out_avals` have the `shaped_name` attribute. Would you like me to go through all the code and change `${var}.named_shape` to a function that checks if `named_shape` is present or not? For the time being, the only change that is impeding our project is this line (and another PR I'll open to address the `get_aval` CC(If parameter to `get_aval` is AbstractValue return it.) from the caller of `get_aval`). Thanks!",I'm closing as this has already been implemented.
1296,"以下是一个github上的jax下的一个issue, 标题是(Use a Jacobi SVD solver for unbatched SVDs up to 1024x1024 on NVIDIA GPUs.)， 内容是 (Use a Jacobi SVD solver for unbatched SVDs up to 1024x1024 on NVIDIA GPUs. The unbatched Jacobi solver is faster for smallmoderate matrices, and the unbatched kernel doesn't have size restrictions. Timings on T4 GPU: Before:  Benchmark                  Time             CPU   Iterations  svd/m:1/n:1           263587 ns       242274 ns         2780 svd/m:2/n:1           335561 ns       298238 ns         2303 svd/m:5/n:1           337784 ns       299841 ns         2304 svd/m:10/n:1          339184 ns       300703 ns         2311 svd/m:100/n:1         359826 ns       320088 ns         2159 svd/m:500/n:1         376124 ns       338660 ns         2076 svd/m:800/n:1         375779 ns       335590 ns         2060 svd/m:1000/n:1        419171 ns       341487 ns         2072 svd/m:1/n:2           307564 ns       270663 ns         2544 svd/m:2/n:2           320928 ns       283601 ns         2487 svd/m:5/n:2           377373 ns       344228 ns         2035 svd/m:10/n:2          380557 ns       349412 ns         1953 svd/m:100/n:2         )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Use a Jacobi SVD solver for unbatched SVDs up to 1024x1024 on NVIDIA GPUs.,"Use a Jacobi SVD solver for unbatched SVDs up to 1024x1024 on NVIDIA GPUs. The unbatched Jacobi solver is faster for smallmoderate matrices, and the unbatched kernel doesn't have size restrictions. Timings on T4 GPU: Before:  Benchmark                  Time             CPU   Iterations  svd/m:1/n:1           263587 ns       242274 ns         2780 svd/m:2/n:1           335561 ns       298238 ns         2303 svd/m:5/n:1           337784 ns       299841 ns         2304 svd/m:10/n:1          339184 ns       300703 ns         2311 svd/m:100/n:1         359826 ns       320088 ns         2159 svd/m:500/n:1         376124 ns       338660 ns         2076 svd/m:800/n:1         375779 ns       335590 ns         2060 svd/m:1000/n:1        419171 ns       341487 ns         2072 svd/m:1/n:2           307564 ns       270663 ns         2544 svd/m:2/n:2           320928 ns       283601 ns         2487 svd/m:5/n:2           377373 ns       344228 ns         2035 svd/m:10/n:2          380557 ns       349412 ns         1953 svd/m:100/n:2         ",2023-11-13T19:01:16Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18503
520,"以下是一个github上的jax下的一个issue, 标题是(jax.eval_shape silence the typing annotations)， 内容是 (`jax.eval_shape` is badly annotated, so users loose typechecking and autocomplete on returned input. Current anntations:  Should be instead:  The `ParamSpec` is less important, but loosing the type checking on the function output is quite annoying as there's no more autocomplete on the output: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.eval_shape silence the typing annotations,"`jax.eval_shape` is badly annotated, so users loose typechecking and autocomplete on returned input. Current anntations:  Should be instead:  The `ParamSpec` is less important, but loosing the type checking on the function output is quite annoying as there's no more autocomplete on the output: ",2023-11-13T18:43:02Z,enhancement,closed,0,7,https://github.com/jax-ml/jax/issues/18502,"Thanks! I think this annotation predates the availability of `typing.ParamSpec`. However, there's a bit of a problem here: `eval_shape` does not return the same type as the input function, rather it returns the same pytree structure, with all array & scalar entries replaced by `ShapeDtypeStruct`. I'm not sure that Python type annotations are expressive enough to actually annotate that behavior. What do you think?","Thank you for the answer. I agree that python annotations are not expressive enough, however this seems a case where practicality should beat purity.  I think loosing autocomplete has quite a big impact on productivity. In our codebase, we reduce friction to the minimum and forcing users to inspect the original function and having to navigate multiple levels in the codebase to understand the structure.","Sure, we can give it a try, but fundamentally the proposed annotation is incorrect, so if it breaks too many downstream packages we won't be able to add it.","I expect this will break code. Not because of the `jax.Array` vs `ShapeDtypeStruct`, but because it will uncover actual pytype error that were previously hidden. I recently sent cl/582235119 to fix annotations for `.named_call` (which should be a noop), but this exposed many bad calls that were ignored by pytype because decorated function were loosing their type checking. It should also be possible to support the simple cases of simple nested structures with overload:  But this might add too much complexity","That works for the simplest cases, but arbitrary types can be registered as pytrees at runtime so fundamentally there's no way to correctly annotate PyTrees in Python's type system.","I don't think this will help JAX itself, but if it helps any thirdparty projects: the next jaxtyping release will have enough to make annotating this possible.  where the `""T""` is bound to the PyTree structure, so both input and output must have matching structures. (And the `ArrayLike`  / `ShapeDtypeStruct` indicate the leaf types as usual.) (Actually, the next release will even make it possible to annotate that the array shapes match too:  but that one is probably unreadable magic unless you really like your type annotations.)","I'm going to close this; I don't think it's really possible to do better than the current annotation, given the fact that pytree types are not statically resolveable."
760,"以下是一个github上的jax下的一个issue, 标题是(libtpu.so present but jax fails with "".local/lib/python3.9/site-packages/libtpu/libtpu.so: cannot open shared object file: No such file or directory"")， 内容是 ( Description i just got my hands on an usb coral tpu and wanted to try JAX, however i can't seem to get it to work. I get:  however libtpu.so is present at that location:  of note, i'm just trying things out and did not setup a venv  What jax/jaxlib version are you using? jax0.4.20  Which accelerator(s) are you using? TPU  Additional system info Raspberry Pi 4, Debian GNU/Linux 11 (bullseye) 64bit  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"libtpu.so present but jax fails with "".local/lib/python3.9/site-packages/libtpu/libtpu.so: cannot open shared object file: No such file or directory"""," Description i just got my hands on an usb coral tpu and wanted to try JAX, however i can't seem to get it to work. I get:  however libtpu.so is present at that location:  of note, i'm just trying things out and did not setup a venv  What jax/jaxlib version are you using? jax0.4.20  Which accelerator(s) are you using? TPU  Additional system info Raspberry Pi 4, Debian GNU/Linux 11 (bullseye) 64bit  NVIDIA GPU info _No response_",2023-11-10T19:08:18Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18486,"Ah! `libtpu` is for Google's Cloud TPU VM product (https://cloud.google.com/tpu), which is a datacenter accelerator.  Cloud TPUs are a different thing to ""Edge TPU""s, which are a very different kind of TPU for lowpower edge devices. JAX doesn't support edge TPU at the moment, I'm afraid! The error loading the `.so` file is because it is the wrong architecture: it's an x8664 library, but the Raspberry Pi is an ARM machine. Hope that helps! I'm sorry I don't have better news. ",oops. *facepalm* i was hoping to accelerate numpy operations on large datasets. I guess i didn't read enough beforehand. Another search tells me the tensorflow package might offer what i need.  Thanks!
967,"以下是一个github上的jax下的一个issue, 标题是(One context manager with all of the flags?)， 内容是 (With the recent announcement on RNGs, I just found out about `jax.threefry_partitionable`.  There are now quite a few flags, each with its own context manager.  I love that they're set with context managers, but they're hard to discover unless you pay really close attention to all of the announcements.  Have you considered adding one context manager for all of the flags?  Something like:  This would make it: * easy to discover flags, * encourage users to set them all at the same time, and * ensure uniformity (`cache_dir` is not a context manager and instead raises if you call it with different values, `threefry_partitionable` takes a Boolean flag whereas `enable_custom_prng` doesn't even though they could have been coded the same way).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,One context manager with all of the flags?,"With the recent announcement on RNGs, I just found out about `jax.threefry_partitionable`.  There are now quite a few flags, each with its own context manager.  I love that they're set with context managers, but they're hard to discover unless you pay really close attention to all of the announcements.  Have you considered adding one context manager for all of the flags?  Something like:  This would make it: * easy to discover flags, * encourage users to set them all at the same time, and * ensure uniformity (`cache_dir` is not a context manager and instead raises if you call it with different values, `threefry_partitionable` takes a Boolean flag whereas `enable_custom_prng` doesn't even though they could have been coded the same way).",2023-11-10T16:53:31Z,enhancement,open,0,5,https://github.com/jax-ml/jax/issues/18483,"Related to https://github.com/google/jax/issues/17571, another advantage would be to have an environment variable that when set makes it an error to use Jax outside this context manager.  This would prevent Jax from being initialized outside threads, and make it easy to work with threads.  Otherwise, if Jax gets initialized, then it breaks when the thread is spawned and Jax is used within the thread. What I did previously was put an assert in Jax's internal initialization code and find any initializations that happened before my thread started.  This flag would make that unnecessary.","I agree the current configuration story is not ideal. If the main problem is discoverability, do you think improving the docs would help? On a related note, I started an internal refactoring a few weeks ago, which should eventually translate to meaningful improvements in the public API. The current thinking is * JAX has two distinct kinds of configuration options: _flags_, set at most once and never changed, and _state objects_, which can be modified any number of times both globally (via `config.update()`) and threadlocally via a context manager. * We want to clearly separate these on the type level and make the corresponding lookup and update APIs statically typed as well. That should help with catching typos (e.g. `jax.config.enable_custom_rng`) statically and also improve discoverability in editors/IDEs.","I'm not opposed to the idea of having a `config.update()`like API which updates state objects jointly. I would probably prefer to have just one way of doing threadlocal updates, though. So, we would potentially need to deprecate the current context managers in favor of the new API. I also like that we can get static type errors whenever flags are used for threadlocal updates (assuming the changes from my previous message materialize). E..g ","> do you think improving the docs would help? My mistake, I didn't see that they were all collected into section.  That said, what's going on here?  Is that `config.update`? > I'm not opposed to the idea of having a `config.update()`like API which updates state objects jointly.  Yeah, that would probably be good.  But, I'm going to use the context manager interface since I do use threads in some invocations, so this wouldn't help me.  I also find context managers more elegant. > I would probably prefer to have just one way of doing threadlocal updates, though. So, we would potentially need to deprecate the current context managers in favor of the new API. Fair enough.  I understand that there might be advantages of the current API. A couple other reasons to favor a single context manager: * it would allow you to have grouped flags.  E.g., `future=True` could turn on `enable_custom_vjp_by_custom_transpose` and `enable_custom_prng`; or `check_all=True` could enable `check_nans`, `check_infs`, `check_tracer_leaks`, etc. * it keeps the API smaller, which is one of the benefits of Jax. > I also like that we can get static type errors Yeah, I agree!","Also, `enable_x64` and `enable_debug_logging` (only exposed from the command line?) seem to be missing from the configuration section?"
901,"以下是一个github上的jax下的一个issue, 标题是(Implement more efficient `jax.block_until_ready(x)` in C++)， 内容是 (Implement more efficient `jax.block_until_ready(x)` in C++ The current implementation synchronously calls `ArrayImpl.block_until_ready()` one by one. This is suboptimal when it's not cheap to query the readiness of an array. Also, calling `x.block_until_ready()` causes GIL to be acquired/released repeatedly. To address this issue, this CL introduces a C++ implementation of `jax.block_until_ready(x)` that uses IFRT's `Array::GetReadyFuture()` to asynchronously query the readiness of all arrays and wait for them once. To preserve the previous behavior, the C++ implementation also has a slow path for any nonPyArray objects that implement `block_until_ready`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Implement more efficient `jax.block_until_ready(x)` in C++,"Implement more efficient `jax.block_until_ready(x)` in C++ The current implementation synchronously calls `ArrayImpl.block_until_ready()` one by one. This is suboptimal when it's not cheap to query the readiness of an array. Also, calling `x.block_until_ready()` causes GIL to be acquired/released repeatedly. To address this issue, this CL introduces a C++ implementation of `jax.block_until_ready(x)` that uses IFRT's `Array::GetReadyFuture()` to asynchronously query the readiness of all arrays and wait for them once. To preserve the previous behavior, the C++ implementation also has a slow path for any nonPyArray objects that implement `block_until_ready`.",2023-11-10T03:19:28Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18474
519,"以下是一个github上的jax下的一个issue, 标题是(operation gpusolverDnCreate(&handle) failed: cuSolver has not been initialized)， 内容是 ( Description After installing jax as follows:  I get the following error:   What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.10, CentOS Linux 7 (Core)  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,operation gpusolverDnCreate(&handle) failed: cuSolver has not been initialized," Description After installing jax as follows:  I get the following error:   What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.10, CentOS Linux 7 (Core)  NVIDIA GPU info ",2023-11-09T22:28:45Z,bug needs info NVIDIA GPU,closed,0,10,https://github.com/jax-ml/jax/issues/18471,"That return code from `cusolverDnCreate` means ""cusolver initialization failed"". My best guess: is it possible you have multiple copies of cusolver installed and JAX is finding the wrong one? Is there another copy in `/usr/local/cuda` perhaps?", Here are the directory contents around that path. Let me what other commands you'd like me to run for diagnosis. ," can you try updating your CUDA version to `>=11.8` and see if the issue persists? Currently it looks like it is using `cusolver` from the `/usr/local/cuda11.4` install, and that may be causing the conflict. Note that JAX expects `cusolver>=11.4.1` while the `cuda11.4` toolkit provides `cusolver=11.2.0`. It can be a bit confusing since `cusolver` is versioned **separately** from that of the toolkit as a whole. I wonder if this is another instance where we ought to try and prefer pipinstalled CUDA tools if possible Edit: Actually, looking again, it should already prefer the pipinstalled `cusolver`, which is guaranteed compatible. For some extra details, would you be able to share the output of `pip list | grep nvidia`?",  ,One possible problem is you have both `cu11` and `cu12` packages installed. These cannot coexist! They will stomp over each other's files. Can you try in a fresh virtualenv?, What's the recommended way to pip uninstall one of the two `cu` versions? Just manually pip uninstalling the packages one by one?," I'd recommend a fresh virtualenv, but if you want to try removing manually, I'd remove *all* of the packages listed above and then reinstall `jax[cuda11_pip]` (or cuda 12, if you prefer, but only one). LMK if that works!", Looks like the error stopped occurring.,Great! Closing.,">  I'd recommend a fresh virtualenv, but if you want to try removing manually, I'd remove _all_ of the packages listed above and then reinstall `jax[cuda11_pip]` (or cuda 12, if you prefer, but only one). LMK if that works! how can i remove all packages  ??"
603,"以下是一个github上的jax下的一个issue, 标题是(Using CPU array raise Disallowed host-to-device transfer)， 内容是 ( Description When the array is on CPU backend, simple ops trigger hosttodevice transfer. Reproduction:  `x + 0` raise:  It looks like `0` gets converted to REPLICATED array before, so `x + 0` trigger a transfer. Might be related to: * https://github.com/google/jax/issues/16002 * https://github.com/google/jax/issues/16602  What jax/jaxlib version are you using? HEAD)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Using CPU array raise Disallowed host-to-device transfer," Description When the array is on CPU backend, simple ops trigger hosttodevice transfer. Reproduction:  `x + 0` raise:  It looks like `0` gets converted to REPLICATED array before, so `x + 0` trigger a transfer. Might be related to: * https://github.com/google/jax/issues/16002 * https://github.com/google/jax/issues/16602  What jax/jaxlib version are you using? HEAD",2023-11-08T16:04:41Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/18436,The problem here is that the `0` is on host. There will still be a host > device transfer in the sense of sending `0` from host to CPU device 0. So I would say this is working as expected.
1264,"以下是一个github上的jax下的一个issue, 标题是(Inconsistent behaviour of jited and non jited array assignment function on some `PositionalSharding` topologies.)， 内容是 ( Description We are trying to implement `PositionalSharding` in a minimal case where we are are shifting values in an array and assigning new ones. We have observed that when jiting this `assign` function the outputs are different and incorrect when `array` has a `(8,1)` or `(2, 4)` shard topology. The sharding does not effect the unjited function. From inspection of the jited outputs on the topologies that fail, we have noticed the values from the right hand side of the initial  array end up at index `7` i.e. `jitted_assign(array, value)[7] == array[1]`. The correct behaviour is `assign(array, value)[7] == array[8]`.  Any help to explain what is going on would be very appreciated, whether this is a bug or me just not understanding how sharding is expected to work.  What jax/jaxlib version are you using? jax==0.4.16 jaxlib==0.4.16  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, Ubuntu 22.04.3 LTS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Inconsistent behaviour of jited and non jited array assignment function on some `PositionalSharding` topologies.," Description We are trying to implement `PositionalSharding` in a minimal case where we are are shifting values in an array and assigning new ones. We have observed that when jiting this `assign` function the outputs are different and incorrect when `array` has a `(8,1)` or `(2, 4)` shard topology. The sharding does not effect the unjited function. From inspection of the jited outputs on the topologies that fail, we have noticed the values from the right hand side of the initial  array end up at index `7` i.e. `jitted_assign(array, value)[7] == array[1]`. The correct behaviour is `assign(array, value)[7] == array[8]`.  Any help to explain what is going on would be very appreciated, whether this is a bug or me just not understanding how sharding is expected to work.  What jax/jaxlib version are you using? jax==0.4.16 jaxlib==0.4.16  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, Ubuntu 22.04.3 LTS  NVIDIA GPU info _No response_",2023-11-08T14:54:52Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/18434,Can you try on the latest jax and jaxlib version?,I get the same result after upgrading to:  Stack trace: ,"Any ideas  ? Is there someway I can shard without having to use anything experimental, if there is a stability issue there?"
1131,"以下是一个github上的jax下的一个issue, 标题是(Performance regression for reduction on GPU)， 内容是 ( Description I've observed a performance regression moving from jax 0.4.14 to 0.4.16 in a rejection sampling while loop. Here's a simplified version of the code that exhibits the same behavior:  Running this under several different environments yields  I.e. the newer version is ~100x slower (in fact, there seems to be a consistent increase in runtime from version to version). I see similar behavior on two different types NVIDIA GPUs (A100 and RTX 2080 Ti). If I force everything to run on CPU, the timings become more inconsistent, but generally consistent with each other, so this likely a GPUspecific issue:  Potentially related issues: https://github.com/google/jax/issues/16661 https://github.com/google/jax/issues/16663  What jax/jaxlib version are you using? Several (see above)  Which accelerator(s) are you using? CPU, GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Performance regression for reduction on GPU," Description I've observed a performance regression moving from jax 0.4.14 to 0.4.16 in a rejection sampling while loop. Here's a simplified version of the code that exhibits the same behavior:  Running this under several different environments yields  I.e. the newer version is ~100x slower (in fact, there seems to be a consistent increase in runtime from version to version). I see similar behavior on two different types NVIDIA GPUs (A100 and RTX 2080 Ti). If I force everything to run on CPU, the timings become more inconsistent, but generally consistent with each other, so this likely a GPUspecific issue:  Potentially related issues: https://github.com/google/jax/issues/16661 https://github.com/google/jax/issues/16663  What jax/jaxlib version are you using? Several (see above)  Which accelerator(s) are you using? CPU, GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info _No response_",2023-11-07T19:02:35Z,bug performance XLA NVIDIA GPU,closed,0,8,https://github.com/jax-ml/jax/issues/18424,Can you try updating to the latest version of JAX? I believe jaxlib==0.4.16 has a known performance issue that may be related: https://github.com/google/jax/pull/17724,"The latest version I've tried is 0.4.20, which is the most recent release as far as I can tell. The issue is still present there. The issue you linked inspired me to dig a bit deeper, and after some further investigation it appears the issue is not actually in the while loop, but rather in the condition. Specifically the combination `jnp.logical_not(jnp.all(...))` seems to be the slow part. This simplifies things a bit:  Running this I get  I'm not sure if this is the same as CC(Added `serial_dot_products` benchmark), but it does sound plausible.",I can't reproduce this on my (CPU) machine:  On what platform does this issue occur?,"The issue is only present on GPU  in particular I've seen this with 2 different Nvidia GPUs (A100 and RTX 2080 Ti), and also two different CUDA versions (11.8 and 12.0).","Thanks, it looks like XLA's code generation regressed for this reduction. Filed https://github.com/openxla/xla/issues/7152","We have found that the regression came from reduction epilogue fusion which was implemented in that time frame when the regression occurred. We allowed to create a fusion that is actually not supported by the reduction emitter, so the reduction got emitted via the loop emitter as elementwise reduction, which is quite a bit slower in this case. I am about to submit a fix where we do not allow this fusion, so the reduce will still be the root of the fusion.",https://github.com/openxla/xla/commit/cc5307ec4f968a80aecaeb0febed2a65789ab713 was the fix for this,Closing because the XLA issue was fixed! Please try it out.
863,"以下是一个github上的jax下的一个issue, 标题是(Setting values in a matrix by broadcasting slower than using a loop after jit compilation)， 内容是 ( Description When inserting an array of values into a matrix using a for loop seems to run faster than setting the values by broadcasting after compilation. The compilation of the for loop is much slower as expected. This issue only seems to show up when using ""jax_enable_x64"" as when I remove that and use the default dtype for the matrices the broadcasting solution is faster than the loop as expected.    What jax/jaxlib version are you using? jax v0.4.13, jaxlib v0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Linux  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Setting values in a matrix by broadcasting slower than using a loop after jit compilation," Description When inserting an array of values into a matrix using a for loop seems to run faster than setting the values by broadcasting after compilation. The compilation of the for loop is much slower as expected. This issue only seems to show up when using ""jax_enable_x64"" as when I remove that and use the default dtype for the matrices the broadcasting solution is faster than the loop as expected.    What jax/jaxlib version are you using? jax v0.4.13, jaxlib v0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Linux  NVIDIA GPU info ",2023-11-07T16:19:53Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/18420,"Hi  thanks for the report! When I run your code on a Colab T4 GPU I see this:  This is not all that different than what I'd expect for code like this: the compilation of the `for`loop version takes a long time becuase JAX tracing unrolls python loops. In general, if you find yourself looping over array values, you should expect  compilation time to grow approximately quadratically with the number of iterations: this is becuse the compiler must consider each statement in sequence and use heuristics to figure out how to execute it efficiently within the program. On the other hand, a single broadcastbased approach will have relatively quick compilation, because there are many fewer statements for the compiler to consider. The flipside of this is that loopbased implementations give the compiler a lot of freedom to select an efficient implementation, so the resulting function can be a bit faster than if the computation is expressed in terms of single broadcasted operations, which lower to predefined kernels. In your case, the fact that the predefined 64bit kernel is slower, while the predefined 32bit kernel is faster, probably speaks to the kind of operations your hardware was built for (64bit operations are often much slower than 32bit on modern GPUs). All of that seems consistent with the results you're seeing in your experiment.","I didn't realize the loopbased implementation would give the compiler more freedom. In my case I see the ""loop_set run time"" is quite a bit faster than the ""broadcast_set run time""   From your explanation it seems this difference just comes down to what the hardware is built for? Thank you for the thorough and quick response!","Yeah, it's a combination of what operations the hardware can do efficiently, and what sequences of operations the XLA compiler has heuristics for. If you're curious about the details, you can dig deeper by printing out the optimized HLO using the Ahead of time compilation APIs, but the HLO for your loopbased approach is going to be very long!",I'm going to close this as I think the question is addressed – thanks!
661,"以下是一个github上的jax下的一个issue, 标题是(TPU V3-32 jax profiling example failing)， 内容是 ( Description Hello, I tried running the https://jax.readthedocs.io/en/latest/profiling.html example:  and received  the following error:  Here is the command I used to create the tpu nodes:  Any advice on what is going wrong and how to fix this? Best Regards, AI  What jax/jaxlib version are you using? jax 0.4.19, jaxlib 0.4.19  Which accelerator(s) are you using? TPU  Additional system info tpuubuntu2204base  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,TPU V3-32 jax profiling example failing," Description Hello, I tried running the https://jax.readthedocs.io/en/latest/profiling.html example:  and received  the following error:  Here is the command I used to create the tpu nodes:  Any advice on what is going wrong and how to fix this? Best Regards, AI  What jax/jaxlib version are you using? jax 0.4.19, jaxlib 0.4.19  Which accelerator(s) are you using? TPU  Additional system info tpuubuntu2204base  NVIDIA GPU info _No response_",2023-11-06T17:13:21Z,bug needs info TPU,open,0,8,https://github.com/jax-ml/jax/issues/18408,These errors usually occur when there's a conflict with multiple registrations of the same plugin.  Try checking if there are any duplicate registrations or conflicts in your code or environment.,"Hi  , Can you please help understand you suggestion? For more context here is the code I ran    And then after you suggested multiple registrations of the same plugin, I ran the below code    any idea what is going wrong? Best regards, AI","The errors you are getting are related to TPU driver and TPU runtime not being installed. You can install them using the following commands:  ` gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install tensorflowtpu"" gcloud compute tpus tpuvm ssh nodev332euw4a zone=europewest4a worker=all command=""pip install tensorflowio"" `  Once you have installed the TPU driver and TPU runtime, you can try running the code again.﻿ Hope it work","Hi  , Thank you for the suggestion. I created a new node and ran the above commands, still I got the same error:  Not sure what the issue is. I am finding it hard to debugfrom the error messages as well. Any idea what can be tried next? Best regards, AI","Your TensorRT installation is wrong I guess. To install TensorRT manually, you can follow the instructions in the TensorRT documentation. Just try it but after installation you will find some version mismatch issues so it's gonna be tough stuff. Gud luck!",Moreover I'm getting this output on T4  On TPU I'm getting this error  But not the one you mentioned,"  What actually goes wrong? Is it just those errors are spat out? Those look benign to me, since they are related to `tensorflow` trying to initialize GPU things, which won't work, because you don't have a GPU on a TPU machine. You can probably work around by installing `tensorflowcpu` instead. Does the profile itself work if you leave aside those errors?",I had the same issue.
776,"以下是一个github上的jax下的一个issue, 标题是(Add comb function to jax._src.scipy.special)， 内容是 (Here, we add the comb function to jax.scipy.special. This implementation provides an approximate and efficient calculation, while also handling the exact case. As in scipy, we also handle the case where the number of repetitions is computed. Finally, we add a test in the `tests/lax_scipy_special_functions_test.py` file to maintain testing coverage. This is my first PR to jax. Although I tried to follow the contributing guidelines closely, please let me know if there's anything I missed.  Thanks! Resolves: CC([scipy] Add scipy.special.comb to JAX) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add comb function to jax._src.scipy.special,"Here, we add the comb function to jax.scipy.special. This implementation provides an approximate and efficient calculation, while also handling the exact case. As in scipy, we also handle the case where the number of repetitions is computed. Finally, we add a test in the `tests/lax_scipy_special_functions_test.py` file to maintain testing coverage. This is my first PR to jax. Although I tried to follow the contributing guidelines closely, please let me know if there's anything I missed.  Thanks! Resolves: CC([scipy] Add scipy.special.comb to JAX) ",2023-11-04T18:38:34Z,,open,0,5,https://github.com/jax-ml/jax/issues/18389,"Thanks for the review  ! Hm, unsure why this PR looks so messy now, but I added in the changes that you suggested with the exception of my question on `exact`.  Please let me know if I should create a new PR that's less messy, or if this is ok. ","You should be able to fix the commit history with something like this (assuming upstream points to the main jax repo, and origin points to your fork: ",Hi  let me know if you'd like to continue working on this!,"Hi , Thanks for the ping! I've made changes based on your most recent suggestions.  Please let me know if this looks good to you or if there are any other changes for me to add. Thanks for your review!",Can I bump this? Am looking to migrate some bezier curve code from scipy to jax to see if jax can be used to fit composite bezier curves efficiently
466,"以下是一个github上的jax下的一个issue, 标题是(Inaccurate output when converting bool array to bfloat16 )， 内容是 ( Description  Outputs:  Note the difference in the sum!  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Inaccurate output when converting bool array to bfloat16 , Description  Outputs:  Note the difference in the sum!  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_,2023-11-04T04:40:54Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18387,"You closed this so you've probably figured it out, but in case others stumble across this: the reason for this is that bfloat16 is incapable of representing the correct output in this case. For example:  Using `numpy.nextafter`, we can see that the next representable bfloat16 value after 1824 is 1832:  You should always keep this in mind when working with `bfloat16` values: they encode only an 8bit significand, so computational results will only be accurate to about 1 part in 256. This is by design: `bfloat16` was specifically created for situations where numerical accuracy can be discarded in favor of more efficient computation. "
737,"以下是一个github上的jax下的一个issue, 标题是(Segfault on multiple gpus (sharding+scan+select))， 内容是 ( Description While running some complicated code on multiple gpus I was consistently getting segmentation faults whenever I was jitting my code. I was able to condense it down to the following reproducer:  output:   side remark: actually I would like to run this inside of a shard map, which segfaults the same way:    What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20+cuda12.cudnn89  Which accelerator(s) are you using? 2 x GPU  Additional system info Python 3.9.2, Debian 11  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Segfault on multiple gpus (sharding+scan+select)," Description While running some complicated code on multiple gpus I was consistently getting segmentation faults whenever I was jitting my code. I was able to condense it down to the following reproducer:  output:   side remark: actually I would like to run this inside of a shard map, which segfaults the same way:    What jax/jaxlib version are you using? jax 0.4.20, jaxlib 0.4.20+cuda12.cudnn89  Which accelerator(s) are you using? 2 x GPU  Additional system info Python 3.9.2, Debian 11  NVIDIA GPU info ",2023-11-03T22:00:09Z,bug XLA NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/18384,Looks like an XLA bug. Filed https://github.com/openxla/xla/issues/7155,Hi   I tried to execute the mentioned code on cloud VM having 4 T4 GPUs with JAX version 0.4.26 and it executed without any error. Please find the attached screenshot for reference.  Could you please verify the same and let us know if the issue resolved or still exist. Thank you.,> Hi  >  > I tried to execute the mentioned code on cloud VM having 4 T4 GPUs with JAX version 0.4.26 and it executed without any error. Please find the attached screenshot for reference. >  >  > Could you please verify the same and let us know if the issue resolved or still exist. >  > Thank you. I can confirm that this seems to be fixed now.
466,"以下是一个github上的jax下的一个issue, 标题是([XLA:GPU] Consider Triton for all non-pure GEMM fusions)， 内容是 ([XLA:GPU] Consider Triton for all nonpure GEMM fusions This is a big step toward enabling xla_gpu_triton_gemm_any by default. It shows about 1.05x geomean speedup on internal benchmarks (comparable to xla_gpu_triton_gemm_any=true).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gemma,[XLA:GPU] Consider Triton for all non-pure GEMM fusions,[XLA:GPU] Consider Triton for all nonpure GEMM fusions This is a big step toward enabling xla_gpu_triton_gemm_any by default. It shows about 1.05x geomean speedup on internal benchmarks (comparable to xla_gpu_triton_gemm_any=true).,2023-11-03T15:28:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18371
1295,"以下是一个github上的jax下的一个issue, 标题是([Pallas] GPU: CUDA error when kernel launch dimensions >= 65536 in Y or Z)， 内容是 ( Description The following code (adapted from the pallas quickstart) results in a CUDA error, because Pallas tries to launch the resulting CUDA kernel with too large a grid in the Y dimension.  On my machine (jax & jaxlib head, jaxtriton 1f41ec2 and tritonnightly2.1.0.dev20231014192330, cuda 12), the above code prints the following:  For context, I'm hitting this issue in my populationbasedtraining codebase, where I'm training an ensemble of models simultaneously using jax.vmap over my update loop (this expands the training loop inputs from shape [N, C] to [P, N, C], where P is my population size and P = 65536. The core issue is twofold: 1. The pallas_call batching rule internally *prepends* the vmapped dimension to GridMapping 2. When mapping this to a 3D CUDA grid, `_process_grid_to_3d_grid` just assigns the GridMapping dims from left to right to X, Y, and Z in the kernel launch. The problem is that the CUDA API specifies that the max dimensions for a launch are [2^31  1, 65535, 65535] for the 3D grid. I believe the intent i)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[Pallas] GPU: CUDA error when kernel launch dimensions >= 65536 in Y or Z," Description The following code (adapted from the pallas quickstart) results in a CUDA error, because Pallas tries to launch the resulting CUDA kernel with too large a grid in the Y dimension.  On my machine (jax & jaxlib head, jaxtriton 1f41ec2 and tritonnightly2.1.0.dev20231014192330, cuda 12), the above code prints the following:  For context, I'm hitting this issue in my populationbasedtraining codebase, where I'm training an ensemble of models simultaneously using jax.vmap over my update loop (this expands the training loop inputs from shape [N, C] to [P, N, C], where P is my population size and P = 65536. The core issue is twofold: 1. The pallas_call batching rule internally *prepends* the vmapped dimension to GridMapping 2. When mapping this to a 3D CUDA grid, `_process_grid_to_3d_grid` just assigns the GridMapping dims from left to right to X, Y, and Z in the kernel launch. The problem is that the CUDA API specifies that the max dimensions for a launch are [2^31  1, 65535, 65535] for the 3D grid. I believe the intent i",2023-11-02T21:55:43Z,bug NVIDIA GPU pallas,closed,0,2,https://github.com/jax-ml/jax/issues/18361,"Thanks for the thorough investigation. The batch dimensions do need to be major dimensions on the TPU, so I don't think changing the batching rule is a good idea. Handling this issue in `_process_grid_to_3d_grid` makes sense. I'm wondering if we can end up with a solution in between yours and the current one. Rather than folding all the thread indices into a 1d grid, could we treat the `x` dimension in the grid as the 'catch all' dimension (aka use a reversed CUDA launch grid). Are there any downsides to that approach?","Thanks for the fast response: I've submitted a PR (linked above) that implements roughly your proposed solution. It has to be a bit more complicated, because you can hit easily hit this bug even with less than 3 axes. I've added code that collapses >3 dimensions into X, and then also will collapse further if the resulting Y or Z dimensions are too large."
336,"以下是一个github上的jax下的一个issue, 标题是(Add a test for double donation.)， 内容是 (Add a test for double donation. The underlying issue was fixed some time ago. Fixes https://github.com/google/jax/issues/9635)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add a test for double donation.,Add a test for double donation. The underlying issue was fixed some time ago. Fixes https://github.com/google/jax/issues/9635,2023-11-02T20:55:25Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18359
501,"以下是一个github上的jax下的一个issue, 标题是(Unimplemented ""discharged_consts"" when discharging while loop effects)， 内容是 ( Description The test below, when added into `pallas_test.py`, yields the error:  Test case:   What jax/jaxlib version are you using? Google internal  Which accelerator(s) are you using? GPU  Additional system info Google internal  NVIDIA GPU info A100)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Unimplemented ""discharged_consts"" when discharging while loop effects"," Description The test below, when added into `pallas_test.py`, yields the error:  Test case:   What jax/jaxlib version are you using? Google internal  Which accelerator(s) are you using? GPU  Additional system info Google internal  NVIDIA GPU info A100",2023-11-02T19:23:53Z,bug pallas,open,0,1,https://github.com/jax-ml/jax/issues/18357,This looks related to CC(Add discharge rules for scan/while); assigning  
697,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] InconclusiveDimensionOperation for scatter op with leading vmapped symbolic dimension)， 内容是 ( Description Running into an issue when trying to use scatter operations inside a converted batch jax function when gradient is required. Is there a recommended work around if I have an unknown batch dimension? Repro  Results in error   What jax/jaxlib version are you using? jax v0.4.17, jaxlib v0.4.17, tensorflow v2.14.0  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] InconclusiveDimensionOperation for scatter op with leading vmapped symbolic dimension," Description Running into an issue when trying to use scatter operations inside a converted batch jax function when gradient is required. Is there a recommended work around if I have an unknown batch dimension? Repro  Results in error   What jax/jaxlib version are you using? jax v0.4.17, jaxlib v0.4.17, tensorflow v2.14.0  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_",2023-11-01T20:47:41Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18348,"I will look into this, it seems that jvp(scatter) with shape polymorphism is not working properly. I looked at the code a bit, and there is a simpler code path is the update indices are unique. If you are willing to promise that, then use `set(updates, unique_indices=True)` and I think then it should work."
610,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Unable to wrap tf.function + convert + call_tf when using tf.Variable / stateful call_tf)， 内容是 ( Description Having trouble nesting a call_tf inside a converted jax function when using a stateful transform / tf.Variable.  Error is:   What jax/jaxlib version are you using? jax v0.4.17, jaxlib v0.4.17, tensorflow v2.14.0  Which accelerator(s) are you using? CPU  Additional system info Apple M1 Max  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Unable to wrap tf.function + convert + call_tf when using tf.Variable / stateful call_tf," Description Having trouble nesting a call_tf inside a converted jax function when using a stateful transform / tf.Variable.  Error is:   What jax/jaxlib version are you using? jax v0.4.17, jaxlib v0.4.17, tensorflow v2.14.0  Which accelerator(s) are you using? CPU  Additional system info Apple M1 Max  NVIDIA GPU info _No response_",2023-10-30T00:34:28Z,bug,open,0,9,https://github.com/jax-ml/jax/issues/18315,"I found that `keras_core` (I believe soon to be Keras 3) provides a workaround for my example. In particular, the `stateless_call` method of keras_core layers makes it possible.  Here is a contrived example:  This can work great for my use case! but can't use it just yet as I understand polymorphism in call_tf doesn't work with native_serialization which I currently require","I will take a look at this repro, but in general the call_tf mechanism works best when there are not tf.Variables captured in the called function. It is best to refactor the TF code to pass the variable values in and out of the TF function.","I looked at the original repro. The problem here is that when call_tf processes `func_tf` is notices that `func_tf` captures two `tf.Variables`, and it tries to read the values of those variables using `.numpy()`. This works if the code runs in eager mode, but does not work when the outer code is under `tf.function`. You can verify this if you remove `tf.function` from your repro.  Can you please take a look if there is some workaround? ",Sure. > and it tries to read the values of those variables using .numpy() Could you point me where it call `.numpy()` in keras lib ? I will try to see if we can remove it.,The call to `numpy` happens as part of `np.asarray(inp)` here. This works if we are in eager mode bug fails under `tf.function`,"  As  's suggestion, the best way is to rewrite the tf function so it has on captured tf.Variables.    Another choice is https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly but it has performance penalty. I test it on colab https://shorturl.at/bkwHJ and it works. By the way, I try to use same way on jax2tf.py but it is much complicated.   , do you know where is the best location insert this context_manager call ? Thanks","I do not think that we should change jax2tf to force running of TF functions eagerly. If the user starts with a `tf.keras.layer`, which references variables, is it possible to split that function into a set of variable values and a function that takes the variables as inputs?","It would be nice if there were some other way of resolving this. I am loading a saved model (originally JAX code) to run inside more JAX code, which I then want to export into another savedmodel, but it is tricky to use call_tf on the inner savedmodel while also passing in all variables / etc it uses (since they are tf tensors).","For resolving this when functions are not run eagerly, what about modifying _call_tf_lowering to grab the value of variables? This is what I had to do to get my use case working for https://github.com/google/jax/issues/11753 "
715,"以下是一个github上的jax下的一个issue, 标题是([XlaCallModule] Fixes for serialization version 9.)， 内容是 ([XlaCallModule] Fixes for serialization version 9. In version 9, the main function of a serialized module may contain token arguments and outputs. Those do not correspond to actual XlaCallModule op inputs and outputs. In cl/577032011 we had adjusted the input_shapes for the call to RefineDynamicShapes from xla_call_module_op. Here we move the adjustment to input_shapes inside the RefineDynamicShapes, so that it takes effect for all call sites, including those from shape_inference.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[XlaCallModule] Fixes for serialization version 9.,"[XlaCallModule] Fixes for serialization version 9. In version 9, the main function of a serialized module may contain token arguments and outputs. Those do not correspond to actual XlaCallModule op inputs and outputs. In cl/577032011 we had adjusted the input_shapes for the call to RefineDynamicShapes from xla_call_module_op. Here we move the adjustment to input_shapes inside the RefineDynamicShapes, so that it takes effect for all call sites, including those from shape_inference.",2023-10-28T14:33:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18312
845,"以下是一个github上的jax下的一个issue, 标题是(``custom_jvp`` of ``while_loop`` failes on reverse mode.)， 内容是 ( Description I'm working on a library for numerical quadrature in JAX, with derivatives defined via Leibniz rule. I've defined ``custom_jvp`` for my quadrature functions and it works fine in forward mode, but when trying reverse mode AD I get a ``NotImplementedError``  The error:  As far as I understand, reverse mode should work here, since the jvp is defined in terms of calls to the primal function. Is there something I'm missing?  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,``custom_jvp`` of ``while_loop`` failes on reverse mode.," Description I'm working on a library for numerical quadrature in JAX, with derivatives defined via Leibniz rule. I've defined ``custom_jvp`` for my quadrature functions and it works fine in forward mode, but when trying reverse mode AD I get a ``NotImplementedError``  The error:  As far as I understand, reverse mode should work here, since the jvp is defined in terms of calls to the primal function. Is there something I'm missing?  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-10-28T02:24:54Z,bug,open,0,8,https://github.com/jax-ml/jax/issues/18311,"You might be interested in this, https://github.com/patrickkidger/equinox/blob/main/equinox/internal/_loop/loop.py","I seem to get the same error when using scan, so it may not be unique to while loops.","This is interesting. I have no problems using while_loop with custom JVP rule in my case but don't know what is happening in your case (I reproduced the error). Maybe using `dummy_integrate` twice in `_dummy_integrate_jvp` caused the error. I defined `dummy_integrate2`, which is exactly the same as `dummy_integrate` and replaced the second `dummy_integrate` with `dummy_integrate2`. I got the following error: ` ValueError: Reversemode differentiation does not work for lax.while_loop or lax.fori_loop with dynamic start/stop values. Try using lax.scan, or using fori_loop with static start/stop. ` This was interesting because I thought JAX does not differentiate `_dummy_integrate_jvp` function!","  I think the issue you're seeing here is because the `df` function (defined inside the custom JVP rule) closes over `argsdot`, but is then passed as a `nondiff_argnum` to `dummy_integrate` (the one that returns `f2`). When using a `custom_jvp` or `custom_vjp` rule, you must ensure any functionvalued arguments don't close over any tracers. (Typically I'd recommend always making them global functions as a way to avoid any possibility of this.) You should rewrite things so that the `argsdot` tracers are passed as formal arguments.   Right, you're now bumping into the second issue with the original code (and which f0uriest will hit after having fixed the above issue)! Explaining this one requires knowing a bit about JAX internals. Buckle up, this gets a bit complicated. JAX performs VJPs (backpropagation) by first figuring out what the tangent part of a JVP rule looks like, and then transposing it (loosely speaking, the ""transpose"" here is the ""run it backwards"" part of backpropagation). In terms of public JAX APIs, what this corresponds to is that `jax.grad` is built by applying  `jax.linear_transpose` to the tangent inputs and outputs of `jax.jvp`. Unfortunately, `jax.lax.while_loop` does not support transposition. Backpropagating through a while loop would require saving the result of every step. As it's a while loop, the number of steps is not known at compile time. That means the amount of memory needed is not known at compile time. And the XLA compiler only performs static memory allocation. Thus, no backpropagating through `jax.lax.while_loop`. The error message you're seeing is to help catch the common case when `jax.grad(jax.lax.while_loop)`, i.e. basically `jax.linear_tranpose(jax.jvp(jax.lax.while_loop))`. In the case of this example, then as you note, we're already inside the JVP rule. Thus what we're actually doing is `jax.linear_transpose(jax.lax.while_loop)`. This is an equally impossible operation to perform, it's just that the error message is only designed to help with the common error described in the previous paragraph.  Phew! Okay, what are the possible fixes? You've got a few possible options: 1) write a custom JAX primitive. This will allow you to define both JVP and transposition rules to your heart's content. 2) fix the issue I first described, then use `jax.custom_vjp`. This won't allow you to perform forwardmode autodiff, though. FWIW, this highlights a usecase for CC(custom_vjp now supports jvps), which adds support for jvpof`custom_vjp`. If the approach there can be accepted + the PR finished off, then you'll be able to do use `custom_vjp` without having to sacrifice support for JVPs.","kidger Thank you, it was very educational! Hope your PR or something similar will be approved. I also want to have a way to define both vjp and jvp for largescale inverse problems.",Thanks for all the information.,"Thanks for all the help kidger. I finally got around to working a bit more on this and running into what might be a related problem. I've fixed `df` to not close over anything, and I'm using `scan` instead of `while_loop` so I think it should be fine to transpose. Updated code:  I'm now getting an assertion error from `_scan_transpose` about undefined primals:  any ideas?","Hmm. So 'undefined primals' actually refer to the tangents  here, `argsdot`  whose values aren't available when transposing. Somehow the JVP rule of the scan is saving such an undefined primal as one of its residual values (those values that the forward pass saves for the backward pass). If you figure this out then I'd be curious to know the answer!"
778,"以下是一个github上的jax下的一个issue, 标题是(JAX CUDA incompatible with PyTorch in Python 3.11)， 内容是 ( Description Install ipython, torch, and jax on a fresh python 3.11.3 environment:  Fire up ipython and try to run a simple command:  `which ptxas`: no results `nvidiasmi`: CUDA Version 12.2, Driver 535.113.01, NVIDIA GeForce GTX 4090 `uname a`: Linux 6.5.8_1 (Void LInux) `pip list  ++++ ``` Torch still works in this installation. But if I install JAX without torch, it updates some of the libraries and then torch fails to load because of something cuda link related. Can test this out in a separate environment if the details of this are useful.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX CUDA incompatible with PyTorch in Python 3.11," Description Install ipython, torch, and jax on a fresh python 3.11.3 environment:  Fire up ipython and try to run a simple command:  `which ptxas`: no results `nvidiasmi`: CUDA Version 12.2, Driver 535.113.01, NVIDIA GeForce GTX 4090 `uname a`: Linux 6.5.8_1 (Void LInux) `pip list  ++++ ``` Torch still works in this installation. But if I install JAX without torch, it updates some of the libraries and then torch fails to load because of something cuda link related. Can test this out in a separate environment if the details of this are useful.",2023-10-25T21:49:53Z,bug needs info NVIDIA GPU,closed,0,14,https://github.com/jax-ml/jax/issues/18281,"Torch and JAX have incompatible CUDA 12 versions. If you want to mix both, use the CUDA 11.8 packages of both.","I'd also recommend: start over in a fresh virtual environment, or at least make sure you don't have multiple CUDA major version packages installed (they will conflict).","Ok, thank you.","Even using CUDA 11.8 (fresh environment), I get the following when I try to do `jnp.linspace(1, 1, 30)`:  ","Can you clarify what packages you now have installed (`pip list`)? Can you also confirm that you started from a fresh virtual environment? (You had both cuda 12 and cuda 11 packages installed before and they will *overwrite* each other's files. I'm concerned you might have an installation that still has some cuda 12 libraries installed, and the easiest way to prevent that is to start with a fresh virtual environment.)",Here is a reproduction script ,"I'm also getting an error running this script for PyMC, which reports: ","Note that `jax.numpy.product` was removed in JAX v0.4.16; see CHANGELOG: jax v0.4.16. This was following a similar deprecation of `numpy.product`. You'll either have to change your script to use `jax.numpy.prod`, or use JAX version 0.4.15 or older.","Ok that's fine, but I still can't get JAX to load CUDA; that was just intended to show it can't initialize the backend; linspace still does not work","Something seems broken with your installation. In particular, you are missing many of the packages declared as direct dependencies of `jax[cuda11_pip]`: https://github.com/google/jax/blob/57e33dc3b553fc49095090d0389b7cbf0e694728/setup.pyL115 JAX declares a dependency on `nvidiacufftcu11>=10.9`, which appears to be missing from your installation. That would explain why JAX can't find cufft. I have no idea how this could happen: JAX clearly declares this as a dependency. Try reinstalling `jax[cuda11_pip]`? If that doesn't work, try manually installing the packages listed in the link above?","It does include it...`nvidia` wasn't in the regex, only `cuda`; here is the list filtered to include all nvidia packages, as well as the issue to confirm its the same environment: ","I've also confirmed this on two machines, both with NVIDIA driver version 535.113.01 and CUDA 12.2. I also can't seem to get logs: ","I'm not able to reproduce this. I made a *fresh* GCP VM (1 NVIDIA T4 GPU, 16 vCPUs, Ubuntu 22.04 LTS), and did exactly this:  and everything worked fine. Do you have any other CUDA installations on this machine, e.g., in `/usr/local/cuda`? Are any CUDA libraries in `LD_LIBRARY_PATH`?","There aren't, but I was able to manually uninstall and reinstall the offending packages. It appears as though the issue was with pip  it wasn't properly upgrading the packages  somehow it was glossing over packages if they satisfied the requirements (even if they weren't properly installed)."
1002,"以下是一个github上的jax下的一个issue, 标题是(MLIR translation rule for primitive 'pallas_call' not found for platform cuda)， 内容是 ( Description I am using jax_triton for blocksparse matmul kernels. With the installation instructions on jax_triton main, I face no issues. My environment from July 2023 installed    jaxlib v0.4.15.dev20230802+cuda12.cudnn89  jax v0.4.15  jaxtriton v0.1.4 Since pallas was moved to jax.experimental, I want to get rid of the jaxtriton requirement and work with jax/jaxlib only. When trying to run a kernel from jax.experimental.pallas.pallas_call, I get the above mentioned error  Tested with jax/jaxlib versions 0.4.17 to 0.4.19. Is this a system related issue or an issue with jax?  What jax/jaxlib version are you using? jax v0.4.19 jaxlib v0.4.19  Which accelerator(s) are you using? GPU  Additional system info Python 3.12  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,MLIR translation rule for primitive 'pallas_call' not found for platform cuda," Description I am using jax_triton for blocksparse matmul kernels. With the installation instructions on jax_triton main, I face no issues. My environment from July 2023 installed    jaxlib v0.4.15.dev20230802+cuda12.cudnn89  jax v0.4.15  jaxtriton v0.1.4 Since pallas was moved to jax.experimental, I want to get rid of the jaxtriton requirement and work with jax/jaxlib only. When trying to run a kernel from jax.experimental.pallas.pallas_call, I get the above mentioned error  Tested with jax/jaxlib versions 0.4.17 to 0.4.19. Is this a system related issue or an issue with jax?  What jax/jaxlib version are you using? jax v0.4.19 jaxlib v0.4.19  Which accelerator(s) are you using? GPU  Additional system info Python 3.12  NVIDIA GPU info ",2023-10-25T16:40:31Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18275,You will still need jax_triton. We are investigating some solutions on how to remove its dependency but they will not materialize for some time.,Thanks for letting me know. Looking forward to see it in the future.
1303,"以下是一个github上的jax下的一个issue, 标题是(jax.nn.initializers. Should JAX prefer user-requested dtype over scale parameter?)， 内容是 ( Description Hi,  This is both sort of a bug report or clarification on the semantics of `jax.nn.initializers`. Should JAX prefer the dtype specified by users over the parameter passed to the initializers? Consider the following snippet.  Here the scale is specified as a NumPy array and by defaults it uses float64. Due to the way the initializer is implemented, the user requested dtype may be upcasted to float64. Is this the intended behavior?  Some additional context, this is not the behavior used in some NN libraries. For example, in dmhaiku, see  https://github.com/googledeepmind/dmhaiku/blob/402a701364201dbfabd0e93faee16a201dd48a9c/haiku/_src/initializers.pyL299 the scale will be converted to have the same dtype as the user requested one. Within JAX, I think the variance_scaling initializer also works that way. I created a PR https://github.com/google/jax/pull/18266 trying to fix this, but I wasn't sure how to structure the test, where to put it and how to enable test it under the condition that x64 is enabled.  What jax/j)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.nn.initializers. Should JAX prefer user-requested dtype over scale parameter?," Description Hi,  This is both sort of a bug report or clarification on the semantics of `jax.nn.initializers`. Should JAX prefer the dtype specified by users over the parameter passed to the initializers? Consider the following snippet.  Here the scale is specified as a NumPy array and by defaults it uses float64. Due to the way the initializer is implemented, the user requested dtype may be upcasted to float64. Is this the intended behavior?  Some additional context, this is not the behavior used in some NN libraries. For example, in dmhaiku, see  https://github.com/googledeepmind/dmhaiku/blob/402a701364201dbfabd0e93faee16a201dd48a9c/haiku/_src/initializers.pyL299 the scale will be converted to have the same dtype as the user requested one. Within JAX, I think the variance_scaling initializer also works that way. I created a PR https://github.com/google/jax/pull/18266 trying to fix this, but I wasn't sure how to structure the test, where to put it and how to enable test it under the condition that x64 is enabled.  What jax/j",2023-10-24T23:47:36Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/18267,"Hi   It appears that this issue has been resolved in JAX 0.4.26. I tested the provided code with multiple JAX versions on colab CPU, GPU and TPU. From JAX 0.4.26 onwards, JAX prefers userrequested dtype when using `jax.nn.initializers`:  Output:  Attaching the gist for reference. Thank you."
1249,"以下是一个github上的jax下的一个issue, 标题是(Instantiating a very large jax.Array according to a Sharding)， 内容是 (I'd like to initialize an array according to a sharding, rather than initializing it on the default device and then moving it to the sharding. This is required when trying to instantiate arrays that are larger than a single GPU/TPU memory but smaller that many GPU/TPUs combined memory. I'm building an on device replay buffer to work with Podracer style architectures.  In the cases where an algorithm requires a large replay buffer (ApeXDQN, MuZero, Muesli) that replay buffer will need to be instantiated according to a sharding to prevent OOM errors.   There are implementations of the tooling I'm talking about brax, dejax and very recently flashbax. These implementations work well with stateless environments as you can just pmap over the training loop to shard the replay buffer across devices, effectively increasing buffer size. However this doesn't make use of the sharding tooling available through the new unified jax.Array API. Similar to https://github.com/google/jax/issues/4221issue695968528.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Instantiating a very large jax.Array according to a Sharding,"I'd like to initialize an array according to a sharding, rather than initializing it on the default device and then moving it to the sharding. This is required when trying to instantiate arrays that are larger than a single GPU/TPU memory but smaller that many GPU/TPUs combined memory. I'm building an on device replay buffer to work with Podracer style architectures.  In the cases where an algorithm requires a large replay buffer (ApeXDQN, MuZero, Muesli) that replay buffer will need to be instantiated according to a sharding to prevent OOM errors.   There are implementations of the tooling I'm talking about brax, dejax and very recently flashbax. These implementations work well with stateless environments as you can just pmap over the training loop to shard the replay buffer across devices, effectively increasing buffer size. However this doesn't make use of the sharding tooling available through the new unified jax.Array API. Similar to https://github.com/google/jax/issues/4221issue695968528.",2023-10-24T21:05:59Z,enhancement,closed,1,8,https://github.com/jax-ml/jax/issues/18263,"You can try this:  Another way is this:  This will instantiate `x` directly on the devices as a sharded Array. In other words, `x` will never be on the default device. The above 2 ways are similar but just differ in style and taste. I personally like the first way.","That worked like a charm thank you. Follow up question: Does donate_argnums in jit not play well with the sharding API? I can't seem to implement donate_argnums in a way where the donated buffer is useable. I keep getting:   The documentation still references deprecated pjit, which makes me ask.","You need to set out_shardings too so then jit will donate properly. donation works by looking at the sharded shape. If you don't specify out_shardings, we don't know what the sharding is going to be until after compilation and that's too late in the stack to set donation bits. There is a fix for this but I just need some time to get it submitted. Until then, you can set out_shardings :)",hmmm I very well could be misusing but here is a minimal example that doesn't use the buffers:  ,I think I'm conflating `jax.lax.with_sharding_constraint` with explicitly passing the argument `out_shardings`... However the output of the jitted function is a pytree. Can I pass a pytree of shards to `out_shardings`?,"Yeah, pass in the out_shardings to jit instead of `wsc`. I guess that's one advantage of using out_shardings. > Can I pass a pytree of shards to out_shardings? Yeah I'll fix this though so this never happens again.",THis works for me ,"Likewise, I should have closed on my last comment. Greatly appreciate your help!"
1097,"以下是一个github上的jax下的一个issue, 标题是(Repeatedly building JAX causes string substitution failures)， 内容是 ( Description When developing JAX locally one often wants to repeatedly compile jaxlib etc. https://github.com/NVIDIA/JAXToolbox/blob/c50839183fb69d20ab946cd7312521d796dc2c53/.github/container/buildjax.shL272L273 is a wrapper for triggering this build. Executing it twice in succession yields an error:  which can be avoided by running  before every build. These two files differ:  it's annoying to have to do this every time. It seems reasonable to expect that rebuilding jax with no changes should be a ~noop, not something that triggers an error, but perhaps this `buildjax.sh` script is doing something that it shouldn't?  What jax/jaxlib version are you using? development branch (commit a4fd1097b64448d37ddcb7bb2a2f4a488322bdf4)  Which accelerator(s) are you using? GPU (but N/A)  Additional system info Python 3.10, Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Repeatedly building JAX causes string substitution failures," Description When developing JAX locally one often wants to repeatedly compile jaxlib etc. https://github.com/NVIDIA/JAXToolbox/blob/c50839183fb69d20ab946cd7312521d796dc2c53/.github/container/buildjax.shL272L273 is a wrapper for triggering this build. Executing it twice in succession yields an error:  which can be avoided by running  before every build. These two files differ:  it's annoying to have to do this every time. It seems reasonable to expect that rebuilding jax with no changes should be a ~noop, not something that triggers an error, but perhaps this `buildjax.sh` script is doing something that it shouldn't?  What jax/jaxlib version are you using? development branch (commit a4fd1097b64448d37ddcb7bb2a2f4a488322bdf4)  Which accelerator(s) are you using? GPU (but N/A)  Additional system info Python 3.10, Linux  NVIDIA GPU info _No response_",2023-10-24T14:15:40Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/18252,"This tells me that your build process is finding the JAX sources in the output of the previous build, rather than from the actual source. This seems problematic for many reasons, and so the error is arguably a good thing! I’d recommend deleting the previous build before making a new one, or otherwise ensuring that you’re building from source rather than from the previous build artifact.","Note the repro here is, from the JAX source tree:  I would have expected that to work?","Yeah, indeed – do you know the mechanism by which the new build process reads from the previous build artifact? Is that intended behavior by `pip`? If it's intended, we should adjust our buildtime source patching script to account for that. If it's not intended, we should figure out why it's happening.","It seems like we just leave some files under`build/` (which is coincidentally our directory containing other things, sigh), which if you delete them the problem goes away. I'm not sure why `pip` doesn't remove the previous state when doing this.",Same issue here. Maybe we can add a if check on `build.py` to check and remove `build/bdist.linuxx86_64` `build/lib/` and `build/__pycache__/` ?,"The thing that's more concerning to me here is that when you run `pip install .`, the contents of the `build/` directory are used instead of the actual source you're trying to install. That seems like a bug in `pip`, and I'd like to understand it rather than work around it. The error you're seeing here is reflecting a real build issue: `pip` is building the package using the wrong set of files!","OK, I'll investigate further before doing anything else with https://github.com/NVIDIA/JAXToolbox/pull/396.","> The thing that's more concerning to me here is that when you run pip install ., the contents of the build/ directory are used instead of the actual source you're trying to install. Can you clarify what you meant by this? As far as I can see, the code that's being **executed** is from the source directory. i.e. in the traceback in the issue, I see `/opt/jaxsource/jax/version.py` (without a `build/`). It seems that by default the `build_py` step only copies files if the source is newer than the destination (or if the destination doesn't exist). In our case, the build scripts modify the destination copy `build/version.py` and leave the source copy `version.py` alone, so the destination is newer and subsequent installs do not update it. This doesn't seem unreasonable. The error then follows because the build scripts only know how to modify an unmodified copy, they cannot update an alreadymodified `version.py`. In light of all of this, I think the workaround of https://github.com/NVIDIA/JAXToolbox/pull/396 is valid. As for an actual fix, the leastmagic solution would probably be to not modify files that are notionally copies from the source directory in the pip build directory... Maybe a more pragmatic / less invasive suggestion is to explicitly remove the build tree copy during the build if it exists, i.e. something like  This seems preferable to ""fixing"" the modification logic to cope with alreadymodified files. Thoughts, ?","> It seems that by default the build_py step only copies files if the source is newer than the destination I see – that would explain the issue. > As for an actual fix, the leastmagic solution would probably be to not modify files that are notionally copies from the source directory in the pip build directory... Sure  if you have a suggestion for how to embed buildtime version strings in the package distributions without doing this, I'd be happy to hear it! For what it's worth, the approach we use here (overriding `build_py` to overwrite the version file) is similar to the one used by standard tools like versioneer, though the details differ. Regarding the proposed fix – this looks reasonable to me, assuming the file is copied from the source directory to the build directory during `super().run()`. Would you like to put together a PR?","I opened https://github.com/google/jax/pull/18746 with essentially the workaround above, please take a look  :)"
1014,"以下是一个github上的jax下的一个issue, 标题是(sparse sparse matmul exhausts memory; even when dense version works fine)， 内容是 ( Description Given two BCOO matrices: ` Sparse matmul, implemented with the generalized dot operation  ` runs out of memory:   Similarly, implementing the matmul by sparsifying (sp?) the infix operator,   crashes the interpreter. Multiplying the dense version of these matrices, with   or,   goes through just fine (the former is fast, too; kudos).  Any idea why the sparse version isn't working? Maybe I've constructed the BCOOs improperly (I'm still confused by the n_batch property, so maybe toggling it would help?).  EDIT: Tried straight up sparse sparse matmul via,   but got,    What jax/jaxlib version are you using? jax v0.4.18  Which accelerator(s) are you using? CPU   Additional system info python 3.10.13, ubuntu 20.04  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,sparse sparse matmul exhausts memory; even when dense version works fine," Description Given two BCOO matrices: ` Sparse matmul, implemented with the generalized dot operation  ` runs out of memory:   Similarly, implementing the matmul by sparsifying (sp?) the infix operator,   crashes the interpreter. Multiplying the dense version of these matrices, with   or,   goes through just fine (the former is fast, too; kudos).  Any idea why the sparse version isn't working? Maybe I've constructed the BCOOs improperly (I'm still confused by the n_batch property, so maybe toggling it would help?).  EDIT: Tried straight up sparse sparse matmul via,   but got,    What jax/jaxlib version are you using? jax v0.4.18  Which accelerator(s) are you using? CPU   Additional system info python 3.10.13, ubuntu 20.04  NVIDIA GPU info _No response_",2023-10-24T03:25:38Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18249,Hi  thanks for the report. This is a duplicate of CC(BCOO sparsedense matrixmatrix products produce high memory usage due to nonzero entry copies) – there is some discussion of the issue there.,Oops. My bad. 
652,"以下是一个github上的jax下的一个issue, 标题是(`BCOO.fromdense` is not compatible with `jax.vmap`)， 内容是 ( Description Something unrelated that I bumped into whilst investigating CC(`BCOO` is not compatible with `jax.vmap`):  I know about the `fromdense(..., n_batch=...)` argment, but I think it'd be reasonable for `fromdense` to occur within traced code.  What jax/jaxlib version are you using? JAX 0.4.19  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`BCOO.fromdense` is not compatible with `jax.vmap`," Description Something unrelated that I bumped into whilst investigating CC(`BCOO` is not compatible with `jax.vmap`):  I know about the `fromdense(..., n_batch=...)` argment, but I think it'd be reasonable for `fromdense` to occur within traced code.  What jax/jaxlib version are you using? JAX 0.4.19  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-10-23T22:35:57Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/18245,"This is working as intended. You need to pass a static value to `nse` in order to use `BCOO.fromdense` within `vmap` and other JAX transformations, because otherwise the output arrays have datadependent size.","Ah, gotcha. In that case I think treat this as a report that the error message could be improved.","Thanks  I think the intent is that it would hit this line and raise a useful error, but it's clearly not doing that: https://github.com/google/jax/blob/20e583834ee8e8dc6e4d0c43d2eb86cfe9428f58/jax/experimental/sparse/bcoo.pyL289 This is the intended error message: https://github.com/google/jax/blob/20e583834ee8e8dc6e4d0c43d2eb86cfe9428f58/jax/experimental/sparse/bcoo.pyL244L248"
676,"以下是一个github上的jax下的一个issue, 标题是(Cloudpickle and deepcopy support for Jaxprs)， 内容是 (Fixes CC(Allow Jaxprs to be cloudpickleable)  Needed to implement the methods `__reduce_ex__` and `__deepcopy__` on `Primitive` and `SourceInfo`. Added unit tests in a new file `copying_test.py`. This is generally useful in distributed environments, i.e., I can make a transform to create a `shard_map`ed jaxpr, send this jaxpr to each of my worker nodes over the network via Ray / Dask (both of which use cloudpickle), and then just execute the jaxprs.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cloudpickle and deepcopy support for Jaxprs,"Fixes CC(Allow Jaxprs to be cloudpickleable)  Needed to implement the methods `__reduce_ex__` and `__deepcopy__` on `Primitive` and `SourceInfo`. Added unit tests in a new file `copying_test.py`. This is generally useful in distributed environments, i.e., I can make a transform to create a `shard_map`ed jaxpr, send this jaxpr to each of my worker nodes over the network via Ray / Dask (both of which use cloudpickle), and then just execute the jaxprs.",2023-10-23T22:10:52Z,,closed,0,14,https://github.com/jax-ml/jax/issues/18243,Classic case of works on my machine. Not sure what's wrong with the docs build,"> Jitted functions are already pickleable. Not for my usecase. The jaxpr is still picked up in the closure, making the pickle impossible.  `lower(...)` isn't pickleable either.  ","That wasn't exactly the question, though. You can pickle `jit(f)`, no problem. Why is the jaxpr the right thing to pickle?","> Why is the jaxpr the right thing to pickle? Because my goal is to be able to transform `f` _before_ sending it off to the workers. I can not do that with just the original definition of `f` unless I do that transform instead on every single worker, which is not scalable for my goals. ","> which is not scalable for my goals. For example: if my transform is a randomized optimization, I would need to ensure all of the workers came to the exact same solution. Possible, but incredibly fragile. Solving this first on the client before dispatching prevents this issue forever. ","I want jaxprs to be able to be passed around, modified, and executed using standard distributed python tooling. This could have a wide range of applications beyond just pickling `jit(f)` You could imagine the jaxprs being used with a Ray / Dask server just for the _compiler optimization_ and not even the actual execution. Something like: ","I think we can imagine similar such things (in fact,  had a branch doing something similar for a Ray prototype a while back, though we never merged it). While we all broadly agree that this can be potentially useful, it's not free, hence the questions about alternatives. For your immediate, concrete use case, does pickling `jit(f)` suffice? Could you do anything else short of pickling jaxpr?",">  For your immediate, concrete use case, does pickling jit(f) suffice? Sadly not really, I've tried and it's usually pretty painful. > Could you do anything else short of pickling jaxpr? This is the setup I want to solve: I have a jax function, `f`, and I have a transform `trfm`, which can be expensive, and possibly non deterministic. I want to execute `trfm(f)(args...)` in an SPMD fashion on a distributed `mesh` I've come up with several ideas to do this 1. Make all of the transforms execute on the workers.       * This is what is recommended currently in JAX.      * This is reasonable for welldefined, deterministic transforms (i.e., `grad, vmap, vjp`), but can become difficult if `trfm` needs to do expensive optimization searches, or if any randomization is used.       * When developers create their own custom transform as `jax.extend` evolves, they're going to have a bad time ~if~ when they need to debug a nondeterminism bug across a cluster.  2. Make some kind of separate IR that is pickled instead of the `Jaxpr`s.      * I don't think anyone wants to support this lol.  3. Support pickling `Jaxprs`.       * This PR.       * In this setup, our `Jaxpr` can be derived locally if `trfm` is randomized or complicated. We then dispatch this jaxpr with Ray or Dask (via cloudpickle) to the entire mesh.       * All nodes have the exact same `Jaxpr`, so when you run `jit(jaxpr_as_fun(jaxpr))(arg...)` on all nodes simultaneously, the chance of bugs related to nodes running mismatching SPMD binaries drops significantly.  Honestly, these are the only solutions I could think of. I've had to both 1) and 2) at various times in the past and they always are very fragile. If instead I could have 3) JustWork™, it would significantly simplify a lot of the dispatching infrastructure for autopartitioning work.  There could be a forth even easier solution I am missing, but I haven't found it yet. Ideas are welcomed!  >  While we all broadly agree that this can be potentially useful, it's not free, hence the questions about alternatives Nothing is ever free, but what is the cost we're trying to avoid here? We have unit tests that will catch obvious problems quickly, and I am happy to be the one responsible to fix issues related to this (I'll probably be the one hitting issues the most anyway lol).  I can see the argument against adding another global dictionary to manage, but we already use similar global dictionaries for `vmap`, `jit`, `grad`, and well, basically everything! It's not a weird thing to see in the JAX codebase.  I can also see an argument against the name strings being used for infrastructure. I also don't like this either, but the inclusion of `namespace` and possibly also including the `jax.__version__` (I should add this...), should be enough to avoid conflicts/compatibility issues.  There are no name conflicts as it stands today (at least in OSS land), and again issues could likely be caught quickly with good unit tests. So the cost is:  * Manage 40 new LOC, a single extra global dictionary, and a few unit tests.  * Risk that we add new attributes that are not pickleable in the future and have to deal with them.    * Unit tests will likely catch it, and you can `None` them out in a `__reduce_ex__` method like I did with `Traceback`. Annoying but not terrible.  The value:  * Ray and Dask are automatically fully compatible with Jaxprs.  * Unlocks distributed jaxpr optimizations and dispatch. In my opinion it's super worth it. ","At one point not long ago,  made executables experimentally serializable by relying on pickle's persistent ID mechanism. See: https://github.com/google/jax/blob/cd177fd5663e1f25c94e76e6babf6d676c8f5c50/jax/experimental/serialize_executable.pyL62L91 Could something similar be useful here, in particular to decouple a bit from the jax core type definitions (especially if we want this actually decoupled at first)? https://docs.python.org/3/library/pickle.htmlpersistenceofexternalobjects https://docs.python.org/3/library/pickle.htmldispatchtables https://docs.python.org/3/library/pickle.htmlcustomreductionfortypesfunctionsandotherobjects", please take a look at the latest implementation. I think it should be a much more agreeable solution than what I had before.,"With https://github.com/google/jax/pull/18243issuecomment1781294233, why can't this live in your experimental project? In other words, pickling jaxprs doesn't have to live in JAX with the above approach I think :) I would recommend that you try out what's recommended in the above comment and see if that works?",Should we instead be serializing the stablehlo if you want it posttransform?,"> Should we instead be serializing the stablehlo if you want it posttransform? I want to stay in Jaxpr / python land. The serialization is less important than its compatibility with standard python cloud tooling.  > why can't this live in your experimental project? I can do anything I want internally, but I think this is valuable enough to the larger OSS community for it to exist and work easily.",Closing as stale. 
711,"以下是一个github上的jax下的一个issue, 标题是(jax.lax.psum hangs when using TPU v3-32 )， 内容是 ( Description Greetings, I am attempting to run allreduce (jax.lax.psum) on TPU v332.  Here is the command I used to create the tpu nodes::  I have been using this python script to do the all reduce:   and it just hangs there indefinitely.  Any advice on what is going wrong and how to fix this? Best regards, AI  What jax/jaxlib version are you using? jax 0.4.19, jaxlib 0.4.19  Which accelerator(s) are you using? TPU  Additional system info tpuubuntu2204base  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jax.lax.psum hangs when using TPU v3-32 ," Description Greetings, I am attempting to run allreduce (jax.lax.psum) on TPU v332.  Here is the command I used to create the tpu nodes::  I have been using this python script to do the all reduce:   and it just hangs there indefinitely.  Any advice on what is going wrong and how to fix this? Best regards, AI  What jax/jaxlib version are you using? jax 0.4.19, jaxlib 0.4.19  Which accelerator(s) are you using? TPU  Additional system info tpuubuntu2204base  NVIDIA GPU info _No response_",2023-10-22T15:45:10Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/18224,"Looks like one of the workers failed to even start for some reason. Your script worked for me (I only modified the print stmt to print out the worker id).  you can also try putting: print(""Initialization complete: %s"" % jax.process_index()) before anything else to see if all the processes have initialized properly. Consider looking through (or attaching) the logs for each process to see if there is some problem initializing: `gcloud compute tpus tpuvm ssh nodev332euw4a     zone=europewest4a     worker=all     command=""cat /tmp/tpu_logs/tpu_driver.INFO""`","Hi , Thank you for your response! I tried it again and it did work. However, when I increased the  to  It hangs and terminates with this message:  Here is the log I collected as you suggested: temp.log any idea why this is happening? For context, I am running the iteration 1000 times so that I get an averaged time of the collective communication execution.  Best regards, AI","Hi, another possibility is that you're early exiting from one of the tasks. Consider fetching the final pmap result on all tasks in order to ensure that all tasks have finished the pmap result. Also consider lifting the pmap like so:  I'm noticing in your log that there are many recompiles.","Hi  , Thank you so much for the suggestion! it looks like it is working! I am new this, so please forgive me if my doubt is too basic: How is this logic executing:  is the pmap executing `iterations` times or , is the result being assigned to `r` `iterations` times I ask because here is the execution times (in s): 1. iteration = 1        execution_time = 0.27577686309814453 2. iteration = 100    execution_time = 0.003425896167755127 3. iteration = 1000  execution_time = 0.000904909372329712 Best Regards, AI","Because jax uses a jit compilation where pmapped_fn is lazily compiled on the first step, we can expect that the time breakdown is approximately: `jit_time + n_steps * (max(dispatch_overhead, on_device_time))` Because your ondevice function is trivial (just a allreduce of a trivial value), this means that your code is: `jit_time + n_steps * dispatch_overhead`. From your numbers, it looks like the jit time is roughly ~270ms and the dispatch_overhead is ~600us. You can see more with: https://jax.readthedocs.io/en/latest/profiling.html (This will also give you a better estimate of the on_device_time). For performance, you want your entire model step update (fwd + bwd + reducegradients + optimizer apply) to be a single training step. I would also recommend switching to the jit API with something like this (This will trigger the SPMD partitioner and make it easier to try different shardings in the future): ","Thank you so much for the detailed solution! I will take a look into the suggested options.  Best regards, AI","Hi , I had a question regarding  the formula `jit_time + n_steps * dispatch_overhead` How are you calculating `n_steps`? I believe it depends on the type of allreduce I guess my question is what type of allreduce is performed by `jax.lax.psum`? is it Ring, Binary tree, halvingdoubling or something else?"
899,"以下是一个github上的jax下的一个issue, 标题是([export] Add jax.global_constant MLIR attributes for dimension variable arguments)， 内容是 (In presence of shape polymorphism and multiplatorm lowering we pass the global values for the dimension variables and the platform index to all inner HLO functions. At the moment, prior to compilation we run a shape refinement pass to infer which of the arguments of a function carry such global values and to constantfold those values. This inference can yield false positives, e.g., when a userdefined function is called with a constant int32 as the first argument. With this change we do not need to infer anymore the arguments that carry global constants. This is in preparation for a more reliable implementation of shape refinement.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[export] Add jax.global_constant MLIR attributes for dimension variable arguments,"In presence of shape polymorphism and multiplatorm lowering we pass the global values for the dimension variables and the platform index to all inner HLO functions. At the moment, prior to compilation we run a shape refinement pass to infer which of the arguments of a function carry such global values and to constantfold those values. This inference can yield false positives, e.g., when a userdefined function is called with a constant int32 as the first argument. With this change we do not need to infer anymore the arguments that carry global constants. This is in preparation for a more reliable implementation of shape refinement.",2023-10-20T01:54:47Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18202
562,"以下是一个github上的jax下的一个issue, 标题是(Support older TPU which does not have get_library_path.)， 内容是 (Support older TPU which does not have get_library_path. This should fix the CI failure with older TPU (the oldest supported TPU should be updated to 20230912 as well). Tested with:  FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/google/jax/pull/18168 from superbobry:noconfigimport 107930425937d9e9799d96506272eb9fb8389b30)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Support older TPU which does not have get_library_path.,Support older TPU which does not have get_library_path. This should fix the CI failure with older TPU (the oldest supported TPU should be updated to 20230912 as well). Tested with:  FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/google/jax/pull/18168 from superbobry:noconfigimport 107930425937d9e9799d96506272eb9fb8389b30,2023-10-19T16:53:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18192
316,"以下是一个github上的jax下的一个issue, 标题是([random] make PRNG impl attributes private)， 内容是 (We don't want users to rely on these – instead they should use APIs like `jax.random.key_impl`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[random] make PRNG impl attributes private,We don't want users to rely on these – instead they should use APIs like `jax.random.key_impl`,2023-10-17T21:43:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18161
356,"以下是一个github上的jax下的一个issue, 标题是(Set public module for exported jax.dtypes APIs)， 内容是 (Before:  After:  I'm beginning to look at parameterized dtype annotations, and the `_src` in the representation was annoying me 😁 )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Set public module for exported jax.dtypes APIs,"Before:  After:  I'm beginning to look at parameterized dtype annotations, and the `_src` in the representation was annoying me 😁 ",2023-10-17T20:44:31Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/18159
748,"以下是一个github上的jax下的一个issue, 标题是([XlaCallModule] Drop support for dim_args_spec attribute.)， 内容是 ([XlaCallModule] Drop support for dim_args_spec attribute. This attribute was used to support shape polymorphism in versions up to and including version 4. Starting on March 28th 2023 with JAX version 0.4.6 we stopped using this attribute. We are now beyond the 6 month backward compatibility version and we drop support for this attribute. We also increase the minimum supported serialization version to 5. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[XlaCallModule] Drop support for dim_args_spec attribute.,[XlaCallModule] Drop support for dim_args_spec attribute. This attribute was used to support shape polymorphism in versions up to and including version 4. Starting on March 28th 2023 with JAX version 0.4.6 we stopped using this attribute. We are now beyond the 6 month backward compatibility version and we drop support for this attribute. We also increase the minimum supported serialization version to 5. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions,2023-10-16T18:20:18Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18136
994,"以下是一个github上的jax下的一个issue, 标题是(Gradient of `jax.random.dirichlet` with small `alpha` element gives `nan`)， 内容是 ( Description Hi, Thanks for building this awesome library! I am still amazed that you can take derivatives of random variables like the Dirichlet distribution, while JAX takes care of the reparameterisations. 🤯 While playing around, I came across an edge case where taking the derivative of a `jax.random.dirichlet` sample gives a `nan`.  It is probably related to the fact that, in this case, `jax.random.dirichlet` samples an exact zero:  I suppose this is a bug, right? Let me know if there is anything I can do to further clarify. Kind regards, Hylke  What jax/jaxlib version are you using? jax0.4.18/jaxlib0.4.18  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.2, Ubuntu 23.04  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Gradient of `jax.random.dirichlet` with small `alpha` element gives `nan`," Description Hi, Thanks for building this awesome library! I am still amazed that you can take derivatives of random variables like the Dirichlet distribution, while JAX takes care of the reparameterisations. 🤯 While playing around, I came across an edge case where taking the derivative of a `jax.random.dirichlet` sample gives a `nan`.  It is probably related to the fact that, in this case, `jax.random.dirichlet` samples an exact zero:  I suppose this is a bug, right? Let me know if there is anything I can do to further clarify. Kind regards, Hylke  What jax/jaxlib version are you using? jax0.4.18/jaxlib0.4.18  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.2, Ubuntu 23.04  NVIDIA GPU info _No response_",2023-10-16T13:46:02Z,bug,open,0,4,https://github.com/jax-ml/jax/issues/18129,"Hi  thanks for the report! I think this doesn't necessarily have to do with `dirichlet`, but rather with the fact that the function $f(x) = \sqrt{x^2}$ doesn't have welldefined gradients as $x\to 0$:  Given this, I think the example is working as expected, and if you want different behavior near `x=0` you'll need to use a different functional form. What do you think?","Thanks for your response.  I think I don't completely follow you. Where does `sqrt` enter in the equation? Is `jax.random.dirichlet` using `jax.numpy.sqrt` under the hood?   Thanks in advance, Hylke","Oh, sorry, I thought you were computing a norm. Let me look closer at this...",It looks like you're right that there's something in the `dirichlet` implementation that leads to `NaN` gradients for very small values of `alpha`. Here's a shorter repro: 
721,"以下是一个github上的jax下的一个issue, 标题是(todense() on jax sparse COO returning only zeros)， 内容是 ( Description I was testing the jax.experimental.sparse library and found that todense() method on the COO sparse matrix is returning a vector of zeros. Code to reproduce (https://colab.research.google.com/drive/1_xxQEqEl5c9NMvYYe8zKsqOS6LJ_V_9P?usp=sharing):  Currently, I am using a workaround with jax.lax.scatter:   What jax/jaxlib version are you using? v0.4.16  Which accelerator(s) are you using? Tested on CPU and GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,todense() on jax sparse COO returning only zeros," Description I was testing the jax.experimental.sparse library and found that todense() method on the COO sparse matrix is returning a vector of zeros. Code to reproduce (https://colab.research.google.com/drive/1_xxQEqEl5c9NMvYYe8zKsqOS6LJ_V_9P?usp=sharing):  Currently, I am using a workaround with jax.lax.scatter:   What jax/jaxlib version are you using? v0.4.16  Which accelerator(s) are you using? Tested on CPU and GPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-10-15T20:34:52Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/18124,"Update: The ""bug"" that I was experience was related to my indices and the shape of the matrix i believe:  As you can see I am wrongly setting the column indices to ""1"" instead of ""0"".  Shouldn't this be impossible when I create a COO matrix? I am attempting to index column 1, but based on the shape, this seems unfeasible. Shouldn't this throw an error?","I think you meant to use this:  For an axis of size 1, an index value 1 is outofbounds (indexing in Python is generally zerobased), and BCOO uses outofbound indexing to indicate padded values that don't affect the array.","Exactly, . As mentioned above, I realized that later. However, shouldn't jax have thrown an error by validating outofbounds?","As I mentioned, JAX's `BCOO` matrix uses outofbound indices to mark padded values in its representation, so the data you passed is valid, it's just that it represents a different matrix than you intended it to represent. But in general, JAX tends not to error on outofbound indices for good reason; see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.htmloutofboundsindexing",Ty for the explanation :) Really appreciated the link.  Closing the issue
508,"以下是一个github上的jax下的一个issue, 标题是([XlaCallModule] Allow i64 platform index arguments.)， 内容是 ([XlaCallModule] Allow i64 platform index arguments. Previously, for multiplatform serialization the platform index argument was required to be an i32. Now we allow also i64, just like we do for dimension variables. This flexibility is useful for JAX when running in 64bit mode.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[XlaCallModule] Allow i64 platform index arguments.,"[XlaCallModule] Allow i64 platform index arguments. Previously, for multiplatform serialization the platform index argument was required to be an i32. Now we allow also i64, just like we do for dimension variables. This flexibility is useful for JAX when running in 64bit mode.",2023-10-13T21:26:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/18114
713,"以下是一个github上的jax下的一个issue, 标题是(Crash due to out-of-range index access)， 内容是 ( Discussed in https://github.com/google/jax/discussions/18103  Originally posted by **DanPuzzuoli** October 13, 2023 I'm trying to run a jit compiled gradient and I'm getting the following error:  This error does not get raised if I don't try to `jit` the gradient function, which makes it difficult to track down what's causing the error. I'm still trying to find a minimal example, but wanted to ask here in case anyone has any insight. On my Macbook this produces:  with this lldb backtrace: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Crash due to out-of-range index access," Discussed in https://github.com/google/jax/discussions/18103  Originally posted by **DanPuzzuoli** October 13, 2023 I'm trying to run a jit compiled gradient and I'm getting the following error:  This error does not get raised if I don't try to `jit` the gradient function, which makes it difficult to track down what's causing the error. I'm still trying to find a minimal example, but wanted to ask here in case anyone has any insight. On my Macbook this produces:  with this lldb backtrace: ",2023-10-13T17:53:27Z,,closed,0,1,https://github.com/jax-ml/jax/issues/18106,"openxla/xla CC([jax2tf] Fix bug in dot_general.) should have fixed this, and the fix should be present in the next jaxlib."
575,"以下是一个github上的jax下的一个issue, 标题是(Setup compatibility testing to support the oldest supported libtpu ve…)， 内容是 (…rsion (12 weeks)  A new matrix option was added to jaxlibversion matrix  Expected to immediately fail due to strict jaxlib version check (requires libtpu to have the same API verison as jaxlib, fails otherwise)  Due to expected failure, so chat will be sent when the jaxlibversion matrix is ""nightly+oldest_supported_libtpu"")请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",chat,Setup compatibility testing to support the oldest supported libtpu ve…,"…rsion (12 weeks)  A new matrix option was added to jaxlibversion matrix  Expected to immediately fail due to strict jaxlib version check (requires libtpu to have the same API verison as jaxlib, fails otherwise)  Due to expected failure, so chat will be sent when the jaxlibversion matrix is ""nightly+oldest_supported_libtpu""",2023-10-11T22:05:17Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/18070, cc,"Looks good! We should test this to double check it works, but we can commit and then try :)"
727,"以下是一个github上的jax下的一个issue, 标题是(Cannot call custom primitive with ordered effect twice in JVP rule)， 内容是 ( Description We are currently migrating mpi4jax from manual XLA tokens to the new ordered effects. The only remaining test failures are related to our JVP rules which call the underlying primitive twice (once on the primal and once on the tangent). Commenting out one of the `.bind` calls in the JVP rule works. Reproducer:  Error:   What jax/jaxlib version are you using? 0.4.18  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot call custom primitive with ordered effect twice in JVP rule, Description We are currently migrating mpi4jax from manual XLA tokens to the new ordered effects. The only remaining test failures are related to our JVP rules which call the underlying primitive twice (once on the primal and once on the tangent). Commenting out one of the `.bind` calls in the JVP rule works. Reproducer:  Error:   What jax/jaxlib version are you using? 0.4.18  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_,2023-10-11T20:36:17Z,bug,closed,1,4,https://github.com/jax-ml/jax/issues/18068,"Funnily enough, it works if I add an (unused) token argument to the primitive:  ","I would like to second this issue, but I think it happens any time the primitive is called twice for the same abstract values, not just for jvp rules. Taking a look into `apply_primitive` (line ~121 in jax/_src/dispatch.py), the error can also be raised at `return compiled_fun(*args)` by simply calling it twice in a row (with the exact same local `args`). The second call appears to not get some internally prepended token argument that the first one gets, and says there is one less argument than expected. So it seems to be some statefullness to the call wrapper that gets used up, and the wrapper is cached so it can't be called again. Also looking at the method `xla_primitive_callable` (line ~144 in jax/_src/dispatch.py) that creates the wrapper, there's the branching statement where it either binds the call to `compiled.create_cpp_call_for_apply_primitive` versus `compiled.unsafe_call`. Forcing it to use `compiled.unsafe_call` does *not* cause the same error. I can also confirm that adding dummy argument seems to work around it, as in it no longer raises the error, but it still makes me uneasy because that doesn't seem selfconsistent.",Hi   It looks like this issue has been resolved in the latest JAX versions. I executed the mentioned code without any errors in Colab using JAX version 0.4.23:  Output: `jax version: 0.4.23 `  Output: `ok ` Kindly find the gist for your reference.,"Wow, thanks  ! I also ran your repro and verified it works."
664,"以下是一个github上的jax下的一个issue, 标题是(Batched/Parallelised `make_array_from_callback` for array trees)， 内容是 (When using array trees (e.g. dictionary of arrays) as input to jitted functions, one can use `make_array_from_callback` within a `jax.tree_map` call to transform arrays in jax arrays. For example  This turns out to be somewhat slow and would be nice to have `make_array_from_callback` accepting array trees, pushing down the stack tree traversing and `device_put`s to amortise and possibly parallelise the underlying ops.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Batched/Parallelised `make_array_from_callback` for array trees,"When using array trees (e.g. dictionary of arrays) as input to jitted functions, one can use `make_array_from_callback` within a `jax.tree_map` call to transform arrays in jax arrays. For example  This turns out to be somewhat slow and would be nice to have `make_array_from_callback` accepting array trees, pushing down the stack tree traversing and `device_put`s to amortise and possibly parallelise the underlying ops.",2023-10-11T14:11:12Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/18058
348,"以下是一个github上的jax下的一个issue, 标题是([random] add shaped_abstractify handler for custom PRNG key)， 内容是 (I confirmed with a `breakpoint()` that with this change keyarrays no longer go via `shaped_abstractify_slow`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[random] add shaped_abstractify handler for custom PRNG key,I confirmed with a `breakpoint()` that with this change keyarrays no longer go via `shaped_abstractify_slow`.,2023-10-10T23:16:39Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/18052
805,"以下是一个github上的jax下的一个issue, 标题是(JAX and TORCH)， 内容是 ( Description When I only pip the latesd jax with cuda(pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html), I can use the jax with gpu.But when I pip the torch(pip install torch) later, Ican't use the jax with gpu,it remind me that cuda or cusolver's version is older than jax's.Why? Can Older jax version avoid it?Then how can I pip the jax[cuda] with relevant version?   What jax/jaxlib version are you using? jax0.4.18 jaxlib0.4.18+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info 3.10.9/Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX and TORCH," Description When I only pip the latesd jax with cuda(pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html), I can use the jax with gpu.But when I pip the torch(pip install torch) later, Ican't use the jax with gpu,it remind me that cuda or cusolver's version is older than jax's.Why? Can Older jax version avoid it?Then how can I pip the jax[cuda] with relevant version?   What jax/jaxlib version are you using? jax0.4.18 jaxlib0.4.18+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info 3.10.9/Linux  NVIDIA GPU info _No response_",2023-10-10T10:17:39Z,bug NVIDIA GPU,closed,9,34,https://github.com/jax-ml/jax/issues/18032,"That's correct. The current releases of PyTorch and JAX have incompatible CUDA version dependencies. I reported this issue to the PyTorch developers a while back, but there has been no interest in relaxing their CUDA version dependencies. My recommendations: * use a different virtualenv for PyTorch and JAX. This is the simplest solution and probably the best. * if for some reason you really want PyTorch and JAX in the same virtualenv, install the CPU version of one of them and the CUDA version of the other. That avoids any CUDA version conflicts. * it may work to simply install JAX after PyTorch, since JAX wants a newer CUDA version than PyTorch's current release does, and in practice NVIDIA's CUDA releases are backwards compatible. I'm not sure if PyTorch enforces a version check, but if not it's highly likely this will work. But I don't think the PyTorch developers support it. * another solution is to install the CUDA version needed for one of the two, and build the other one from source. For example, JAX will happily build from source with an older CUDA release, it's just the binary distribution that requires a CUDA version matching the version against which it was built. Does that resolve your problem? Hope that helps!","This is quite annoying (and inconvenient) now that people have written torch2jax functionality which allows GPUaccelerated interaction, https://github.com/samuela/torch2jax https://github.com/rdyro/torch2jax","Hi , I've been experimenting the simultaneous usage of Torch and JAX for a while. I'm currently working in a Docker container in which they both work on GPU. JAX was installed according to the official documentation as:  I leave here the Conda YAML of the environment, there will probably be some extra packages, but I hope this can help:  conda environment    Python 3.10.8  Ubuntu 22.04  jax==0.4.18  jaxlib==0.4.18+cuda12.cudnn89  Driver Version: 525.125.06  CUDA Version: 12.0","Thank you for your all help. For some reason I can't experience it now,but I'll try it soon and reply you.","ok people, this has been a 1 day nightmare. But finally got this to work on an H100 machine with cuda 12.2, without sudo.  First install Cuda 12.2 (this was already there for me)  Then install Cudnn 8.9 through the official website, using the tar option: https://docs.nvidia.com/deeplearning/cudnn/installguide/index.htmlinstalllinuxtar  then follow what this guy did to build magma: https://github.com/huggingface/autotrainadvanced/issues/281issuecomment1740762360 then install pytorch from source as that post says!!!! and bualaaa","No promises, but informally we're going to try to keep at least one JAX release have a version that is also released with PyTorch. Right now, that's the CUDA 11.8 release of JAX. It's not a guarantee, though; it might happen that for some JAX and Pytorch versions there's no intersecting CUDA version.","I hit a similar issue when installing pytorch and jax into the same conda environment: when torch is loaded first, `jax.devices()` will list only cpu devices. A short summary of diagnosis: It turns out that torch is built against cudnn version 8.7 while jaxlib is built against cudnn version 8.8 leading to an exception when executing `jax._src.xla_bridge._check_cuda_versions()`. Here follows a reproducer:  (note: using strict channel priority would lead to a mamba solver problem). Import torch before checking jax.devices:  Import torch after checking jax.devices:  Notices that the result of `jaxlib.cuda._versions.cudnn_get_version()` depends on whether `torch` was imported before or after calling `jaxlib.cuda._versions.cudnn_get_version`:  vs  that qualifies as an incompatible linkage issue: since libcudnn is dynamically loaded, the result of cudnnGetVersion ought to give the version of loaded library and not of the version of the library that a software was built against. The behavior above suggests that torch was linked with libcudnn statically. A possible resolution: Note that cuDNN minor releases are backward compatible with applications built against the same or earlier minor release. Hence, as long as jaxlib and torch are built against libcudnn with the same major version (8), the jax version check ought to ignore cudnn minor versions. Here is a patch: ","> No promises, but informally we're going to try to keep at least one JAX release have a version that is also released with PyTorch. Right now, that's the CUDA 11.8 release of JAX. The latest version pair I could find that were compatible with each other were `jax[cuda11pip,cuda11_pip]==0.4.10` and `torch==2.2.1+cu118`. The main conflict in later versions for jax is for cudnn, which want `>8.8`, but torch wants `==8.7`. One way to check this would be: ","A workaround that works better for us is to use CUDA 11 with Jax, but CUDA 12 with Torch. So basically `jax[cuda11_pip]` and `torch` in our requirements file works for us.","> A workaround that works better for us is to use CUDA 11 with Jax, but CUDA 12 with Torch. How did you get this to work? I'm using conda, but after installing `pytorchcuda=12.1` I get the following error from JAX: ","We did not have to do anything special. Just installed the two packages in a clean env, and both worked.",The only way I was able to solve the environment with both JAX and PyTorch on CUDA12 was to install some packages from the nvidia channel:    conda list   fyi ,"> The only way I was able to solve the environment with both JAX and PyTorch on CUDA12 was to install some packages from the nvidia channel: FYI, at the moment it is not possible to get both jax and pytorch with cuda 12 only using condaforge dependencies for this reason (I pinned several dependencies to get a clearer error): ~~~ traversaro:~$ mamba create n jaxtorchcuda pytorch==2.1.2=*cuda* jaxlib==0.4.23=*cuda* jax cudaversion=12.* python==3.11.* cudatoolkit==12.* Looking for: ['pytorch==2.1.2[build=*cuda*]', 'jaxlib==0.4.23[build=*cuda*]', 'jax', 'cudaversion=12', 'python=3.11', 'cudatoolkit=12'] condaforge/linux64                                        Using cache condaforge/noarch                                          Using cache Could not solve for environment specs The following packages are incompatible ├─ cudaversion 12**  is installable with the potential options │  ├─ cudaversion [12.012.4.* , which can be installed; ├─ cudatoolkit 12**  does not exist (perhaps a typo or a missing channel); ├─ jaxlib 0.4.23 *cuda* is installable with the potential options │  ├─ jaxlib 0.4.23 would require │  │  └─ cudatoolkit >=11.8,=1.62.1,=4.25.3,=11.8,=1.59.3,=4.24.4,=4.25.1,=4.25.1,=11.8,=4.25.1,=11.8,=4.25.1,=12.0,=4.25.1,=12.0,=4.24.4,=12.0,=4.25.1,<4.25.2.0a0 , which conflicts with any installable versions previously reported. ~~~ Once a condaforge pytorch version gets compiled with libprotobuf==4.25.3 (i.e. https://github.com/condaforge/pytorchcpufeedstock/pull/228 is ready and merged, big thanks to who the pytorch and jax condaforge mantainers) it should be possible to install both jax and pytorch with cuda enabled and using cuda 12 just with condaforge packages.","JAX 0.4.26 relaxed our CUDA version dependencies so the minimum CUDA version for JAX is 12.1. This is a version also supported by PyTorch. Try it out! We're going to try to make sure our supported version range overlaps with at least one PyTorch release. We dropped support for CUDA 11, note.","> > The only way I was able to solve the environment with both JAX and PyTorch on CUDA12 was to install some packages from the nvidia channel: >  > FYI, at the moment it is not possible to get both jax and pytorch with cuda 12 only using condaforge dependencies for this reason (I pinned several dependencies to get a clearer error): After a bunch of fixes from both jax and pytorch mantainers, now (late May 2024) it is possible to just install jax and pytorch from condaforge on Linux and out of the box they will work with GPU/CUDA support without the need to use any other conda channel: ~~~ $ conda create c condaforge n jaxpytorch pytorch jax $ conda activate jaxpytorch $ python Python 3.12.3  (main, Apr 15 2024, 18:38:13) [GCC 12.3.0] on linux Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import torch >>> import jax >>> torch.cuda.is_available() True >>> jax.devices() [cuda(id=0)] >>> ~~~ If for some reason this command does not install the cudaenabled jax, perhaps you are still using the classic conda solver, in that case you can force the installation of cudaenabled jax and pytorch with: ~~~ conda create n condaforge n jaxpytorch pytorch=*=cuda* jax jaxlib=*=cuda* ~~~ However, this is not necessary if you are using a recent conda install that defaults to use the `condalibmambasolver`, see https://www.anaconda.com/blog/afastercondaforagrowingcommunity .   conda list for reference  ~~~ (jaxpytorch) traversaro:~$ conda list  packages in environment at /home/traversaro/miniforge3/envs/jaxpytorch:   Name                    Version                   Build  Channel _libgcc_mutex             0.1                 conda_forge    condaforge _openmp_mutex             4.5                  2_kmp_llvm    condaforge _sysroot_linux64_curr_repodata_hack 3                   h69a702a_14    condaforge binutils_impl_linux64    2.40                 ha1999f0_1    condaforge binutils_linux64         2.40                 hdade7a5_3    condaforge bzip2                     1.0.8                hd590300_5    condaforge cares                    1.28.1               hd590300_0    condaforge cacertificates           2024.2.2             hbcca054_0    condaforge cudacccl_linux64        12.5.39              ha770c72_0    condaforge cudacrtdev_linux64     12.5.40              ha770c72_0    condaforge cudacrttools            12.5.40              ha770c72_0    condaforge cudacudart               12.5.39              he02047a_0    condaforge cudacudartdev           12.5.39              he02047a_0    condaforge cudacudartdev_linux64  12.5.39              h85509e4_0    condaforge cudacudartstatic        12.5.39              he02047a_0    condaforge cudacudartstatic_linux64 12.5.39              h85509e4_0    condaforge cudacudart_linux64      12.5.39              h85509e4_0    condaforge cudacupti                12.5.39              he02047a_0    condaforge cudadriverdev_linux64  12.5.39              h85509e4_0    condaforge cudanvcc                 12.5.40              hcdd1206_0    condaforge cudanvccdev_linux64    12.5.40              ha770c72_0    condaforge cudanvccimpl            12.5.40              hd3aeb46_0    condaforge cudanvcctools           12.5.40              hd3aeb46_0    condaforge cudanvcc_linux64        12.5.40              h8a487aa_0    condaforge cudanvrtc                12.5.40              he02047a_0    condaforge cudanvtx                 12.5.39              he02047a_0    condaforge cudanvvmdev_linux64    12.5.40              ha770c72_0    condaforge cudanvvmimpl            12.5.40              h59595ed_0    condaforge cudanvvmtools           12.5.40              h59595ed_0    condaforge cudaversion              12.5                 hd4f0392_3    condaforge cudnn                     8.9.7.29             h092f7fd_3    condaforge filelock                  3.14.0             pyhd8ed1ab_0    condaforge fsspec                    2024.5.0           pyhff2d567_0    condaforge gcc_impl_linux64         12.3.0               h58ffeeb_7    condaforge gcc_linux64              12.3.0               h6477408_3    condaforge gmp                       6.3.0                h59595ed_1    condaforge gmpy2                     2.1.5           py312h1d5cde6_1    condaforge gxx_impl_linux64         12.3.0               h2a574ab_7    condaforge gxx_linux64              12.3.0               h4a1b8e8_3    condaforge icu                       73.2                 h59595ed_0    condaforge importlibmetadata        7.1.0              pyha770c72_0    condaforge importlib_metadata        7.1.0                hd8ed1ab_0    condaforge jax                       0.4.27             pyhd8ed1ab_0    condaforge jaxlib                    0.4.23          cuda120py312h6027bbc_202    condaforge jinja2                    3.1.4              pyhd8ed1ab_0    condaforge kernelheaders_linux64   3.10.0              h4a8ded7_14    condaforge ld_impl_linux64          2.40                 hf3520f5_1    condaforge libabseil                 20240116.2      cxx17_h59595ed_0    condaforge libblas                   3.9.0           22_linux64_openblas    condaforge libcblas                  3.9.0           22_linux64_openblas    condaforge libcublas                 12.5.2.13            he02047a_0    condaforge libcufft                  11.2.3.18            he02047a_0    condaforge libcurand                 10.3.6.39            he02047a_0    condaforge libcusolver               11.6.2.40            he02047a_0    condaforge libcusparse               12.4.1.24            he02047a_0    condaforge libexpat                  2.6.2                h59595ed_0    condaforge libffi                    3.4.2                h7f98852_5    condaforge libgccdevel_linux64     12.3.0             h0223996_107    condaforge libgccng                 13.2.0               h77fa898_7    condaforge libgfortranng            13.2.0               h69a702a_7    condaforge libgfortran5              13.2.0               hca663fb_7    condaforge libgomp                   13.2.0               h77fa898_7    condaforge libgrpc                   1.62.2               h15f2491_0    condaforge libhwloc                  2.10.0          default_h5622ce7_1001    condaforge libiconv                  1.17                 hd590300_2    condaforge liblapack                 3.9.0           22_linux64_openblas    condaforge libmagma                  2.7.2                h173bb3b_2    condaforge libmagma_sparse           2.7.2                h173bb3b_3    condaforge libnsl                    2.0.1                hd590300_0    condaforge libnvjitlink              12.5.40              he02047a_0    condaforge libopenblas               0.3.27          pthreads_h413a1c8_0    condaforge libprotobuf               4.25.3               h08a7969_0    condaforge libre211                 2023.09.01           h5a48ba9_2    condaforge libsanitizer              12.3.0               hb8811af_7    condaforge libsqlite                 3.45.3               h2797004_0    condaforge libstdcxxdevel_linux64  12.3.0             h0223996_107    condaforge libstdcxxng              13.2.0               hc0a3c3a_7    condaforge libtorch                  2.3.0           cuda120_h2b0da52_301    condaforge libuuid                   2.38.1               h0b41bf4_0    condaforge libuv                     1.48.0               hd590300_0    condaforge libxcrypt                 4.4.36               hd590300_1    condaforge libxml2                   2.12.7               hc051c1a_0    condaforge libzlib                   1.2.13               hd590300_5    condaforge llvmopenmp               18.1.6               ha31de31_0    condaforge markupsafe                2.1.5           py312h98912ed_0    condaforge mkl                       2023.2.0         h84fe81f_50496    condaforge ml_dtypes                 0.4.0           py312h1d6d2e6_1    condaforge mpc                       1.3.1                hfe3b2da_0    condaforge mpfr                      4.2.1                h9458935_1    condaforge mpmath                    1.3.0              pyhd8ed1ab_0    condaforge nccl                      2.21.5.1             h3a97aeb_0    condaforge ncurses                   6.5                  h59595ed_0    condaforge networkx                  3.3                pyhd8ed1ab_1    condaforge numpy                     1.26.4          py312heda63a1_0    condaforge openssl                   3.3.0                h4ab18f5_3    condaforge opteinsum                3.3.0                hd8ed1ab_2    condaforge opt_einsum                3.3.0              pyhc1e730c_2    condaforge pip                       24.0               pyhd8ed1ab_0    condaforge python                    3.12.3          hab00c5b_0_cpython    condaforge python_abi                3.12                    4_cp312    condaforge pytorch                   2.3.0           cuda120_py312h26b3cf7_301    condaforge re2                       2023.09.01           h7f4b329_2    condaforge readline                  8.2                  h8228510_1    condaforge scipy                     1.13.1          py312hc2bc53b_0    condaforge setuptools                70.0.0             pyhd8ed1ab_0    condaforge sleef                     3.5.1                h9b69904_2    condaforge sympy                     1.12            pypyh9d50eac_103    condaforge sysroot_linux64          2.17                h4a8ded7_14    condaforge tbb                       2021.12.0            h297d8ca_1    condaforge tk                        8.6.13          noxft_h4845f30_101    condaforge typing_extensions         4.11.0             pyha770c72_0    condaforge tzdata                    2024a                h0c530f3_0    condaforge wheel                     0.43.0             pyhd8ed1ab_1    condaforge xz                        5.2.6                h166bdaf_0    condaforge zipp                      3.17.0             pyhd8ed1ab_0    condaforge zstd                      1.5.6                ha6fb4c9_0    condaforge ~~~ ","Can someone please point out the correct version necessary to get pytorch and jax both with GPU support on CUDA 12 as of July 2024? I would prefer it to be a standard venv rather than a conda env, but either is fine."," totally by chance I follow this issue, but in general you may have more success in finding help by using official jax help channels (see https://jax.readthedocs.io/en/latest/beginner_guide.htmlfindinghelp), rather then posting in closed issues. More on topic, I have no idea about pip/venv with cuda, but for conda the procedure posted in https://github.com/google/jax/issues/18032issuecomment2132399059 is working fine for me (when I originally posted the message I forgot to add the `c condaforge` to ensure it works fine also on `anaconda` or `miniconda` installation of conda that use `defaults` instead of `condaforge`, I just fixed that to avoid confusion).    By change I just noticed that you added a 👎🏽 reaction to my previous comment, any reason for doing so? Just fyi, authors do not get (at least by default) notifications for post reactions.", I found that running your command with `conda` will install: `jaxlib             condaforge/linux64::jaxlib0.4.27cpu_py312h17e8b90_0` whereas with `mamba` the correct version is installed: `mamba create c condaforge n jaxpytorch pytorch jax` `jaxlib                                    0.4.27  cuda120py312h4008524_200   condaforge/linux64` Perhaps this is why you got 3 thumbs down,">  I found that running your command with `conda` will install: `jaxlib condaforge/linux64::jaxlib0.4.27cpu_py312h17e8b90_0` whereas with `mamba` the correct version is installed: `mamba create c condaforge n jaxpytorch pytorch jax` `jaxlib 0.4.27 cuda120py312h4008524_200 condaforge/linux64` Perhaps this is why you got 3 thumbs down   Interestingly, in my system with: ~~~ rootT0NQNLN:~ conda info      active environment : None             shell level : 0        user config file : /root/.condarc  populated config files : /root/miniforge3/.condarc                           /root/.condarc           conda version : 24.3.0     condabuild version : not installed          python version : 3.10.14.final.0                  solver : libmamba (default)        virtual packages : __archspec=1=skylake                           __conda=24.3.0=0                           __cuda=12.0=0                           __glibc=2.39=0                           __linux=5.15.153.1=0                           __unix=0=0        base environment : /root/miniforge3  (writable)       conda av data dir : /root/miniforge3/etc/conda   conda av metadata url : None            channel URLs : https://conda.anaconda.org/condaforge/linux64                           https://conda.anaconda.org/condaforge/noarch           package cache : /root/miniforge3/pkgs                           /root/.conda/pkgs        envs directories : /root/miniforge3/envs                           /root/.conda/envs                platform : linux64              useragent : conda/24.3.0 requests/2.31.0 CPython/3.10.14 Linux/5.15.153.1microsoftstandardWSL2 ubuntu/24.04 glibc/2.39 solver/libmamba condalibmambasolver/24.1.0 libmambapy/1.5.8                 UID:GID : 0:0              netrc file : None            offline mode : False ~~~ the command ~~~ conda create n condaforge n jaxpytorch pytorch jax ~~~ installs the cuda jax, but indeed: ~~~ conda create solver=classic n condaforge n jaxpytorch pytorch jax ~~~ installs cpu jax. Perhaps you are using an old conda version that is using the classic solver by default? (You can see this if you report the `conda info` output, see https://www.anaconda.com/blog/afastercondaforagrowingcommunity). However, even with the classic solver forcing the solver to install the cuda version of jaxlib and pytorch works as expected (even if the classic solver is much slower): ~~~ conda create solver=classic n condaforge n jaxpytorch pytorch=*=cuda* jax jaxlib=*=cuda* ~~~ I edited the original comment accordingly.","You are right, I'm using the classic solver: ","> The only way I was able to solve the environment with both JAX and PyTorch on CUDA12 was to install some packages from the nvidia channel: >  >  >  >  >  > conda list > fyi  Thanks for the solution, however i have found a possible bug that the jax numpy cannot initialize an array which size is bigger than (2, 52, 10) with both jax and jaxlib version are 0.4.30, so i have to downgrade the jax version to 0.4.23 and then works just fine, so for the insurance, the command could be like  python 3.12 is too newer to some commonly used pkgs","Just a curiosity, are you actually getting any packages from the `nvidia` or `pytorch` channel? If `condaforge` channel is used and you are using strict priority, all the packages you get should come from `condaforge`, and so I guess you could drop the `c nvidia c pytorch` from your command. However, you can check this by calling `conda list` and checking from where packages are installed.","> Just a curiosity, are you actually getting any packages from the `nvidia` or `pytorch` channel? If `condaforge` channel is used and you are using strict priority, all the packages you get should come from `condaforge`, and so I guess you could drop the `c nvidia c pytorch` from your command. However, you can check this by calling `conda list` and checking from where packages are installed. I'm not sure, maybe later i can do a test,thx for the noticing","> Just a curiosity, are you actually getting any packages from the `nvidia` or `pytorch` channel? If `condaforge` channel is used and you are using strict priority, all the packages you get should come from `condaforge`, and so I guess you could drop the `c nvidia c pytorch` from your command. However, you can check this by calling `conda list` and checking from where packages are installed. sorry for the late reply, here is the outputs !image since the jax and jax cuda lib are manually reinstalled by the pypi,  i guess yes that the packages are privileged installed from condaforge :)","Not sure how you can can end up with jax/jaxlib installed via pypi if you just created the environment with `conda create n _env_name_ jaxlib=0.4.23 pytorch cudanvcc python=3.11 c condaforge c nvidia c pytorch`, but as a general comment if you are installing something with pip is a good idea not to install it via conda, to avoid conflicts.","> Not sure how you can can end up with jax/jaxlib installed via pypi if you just created the environment with `conda create n _env_name_ jaxlib=0.4.23 pytorch cudanvcc python=3.11 c condaforge c nvidia c pytorch`, but as a general comment if you are installing something with pip is a good idea not to install it via conda, to avoid conflicts. In my case, the conflicts comes from the torch and jaxlib stick to different cudnn version, formerly i didn't seek to condaforge to install the cudatoolkit compatible for both torch and jaxlib. i use the pip command from the official jax documentation btw.","Ok, but in that case it is probably a good idea not to install `jax` and `jaxlib` from conda, and only install it from pip.","> Ok, but in that case it is probably a good idea not to install `jax` and `jaxlib` from conda, and only install it from pip. i think the only reason for the `jax` and `'jaxlib` suffix is to make sure the condaforge could search and install a compatible cudnn version, i did not do the test, so for the insurance, i recommend to annoyingly reinstall `jax` and `jaxlib` from pip","> > Ok, but in that case it is probably a good idea not to install `jax` and `jaxlib` from conda, and only install it from pip. >  > i think the only reason for the `jax` and `'jaxlib` suffix is to make sure the condaforge could search and install a compatible cudnn version, i did not do the test, so for the insurance, i recommend to annoyingly reinstall `jax` and `jaxlib` from pip But conda has no idea which version of cudnn the jaxlib installed via pip requires. If you want to install cudnn (and even a specific version) with conda, just install cudnn, to avoid problems is tipically useful to avoid to install jax or jaxlib via conda if you are installing it via pip.","> > > Ok, but in that case it is probably a good idea not to install `jax` and `jaxlib` from conda, and only install it from pip. > >  > >  > > i think the only reason for the `jax` and `'jaxlib` suffix is to make sure the condaforge could search and install a compatible cudnn version, i did not do the test, so for the insurance, i recommend to annoyingly reinstall `jax` and `jaxlib` from pip >  > But conda has no idea which version of cudnn the jaxlib installed via pip requires. If you want to install cudnn (and even a specific version) with conda, just install cudnn, to avoid problems is tipically useful to avoid to install jax or jaxlib via conda if you are installing it via pip. you are right, accidentally i use the pip install, and it just found the cudnn version meets the requirement lol."
1050,"以下是一个github上的jax下的一个issue, 标题是(Bitwise negation bug/no support)， 内容是 ( Description Hey there, I've recently encountered an interesting problem when using bitwise negation () operation on booleans that are immediate results of the functions. Here is a sample code:  It looks like JAX (or XLA) incorrectly reduces the negation operation with (probably) an intermediate representation of function output, though I don't know. This bug is extremely annoying to debug when passing a function as an argument to another function, so I think it is quite important to fix this, or do something else that would mitigate the problem. Of course, if one knows about the bug, it is easy to fix it with  instead of . Cheers  What jax/jaxlib version are you using? jax v0.4.18; jaxlib v0.4.18  Which accelerator(s) are you using? CPU  Additional system info 3.10.4; Ubuntu 22.04; Kernel 5.15.81  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bitwise negation bug/no support," Description Hey there, I've recently encountered an interesting problem when using bitwise negation () operation on booleans that are immediate results of the functions. Here is a sample code:  It looks like JAX (or XLA) incorrectly reduces the negation operation with (probably) an intermediate representation of function output, though I don't know. This bug is extremely annoying to debug when passing a function as an argument to another function, so I think it is quite important to fix this, or do something else that would mitigate the problem. Of course, if one knows about the bug, it is easy to fix it with  instead of . Cheers  What jax/jaxlib version are you using? jax v0.4.18; jaxlib v0.4.18  Which accelerator(s) are you using? CPU  Additional system info 3.10.4; Ubuntu 22.04; Kernel 5.15.81  NVIDIA GPU info _No response_",2023-10-10T09:59:38Z,bug needs info,closed,0,2,https://github.com/jax-ml/jax/issues/18031,Note your reproduction doesn't use JAX at all! You're importing JAX but not using it:  Did you get the correct repro?,"Oh wow, I reduced my example so much, that it looks like it is not a bug in jax, but a 'bug' in python lambdas. Sorry! I have a piece of code that allows passing both python functions, and pjit functions, hence the confusion: jax counterpart (when doing  works as expected, unexpected was the difference in behaviours with pure Python lambdas."
1269,"以下是一个github上的jax下的一个issue, 标题是(Unable to correct CUDA vs. JAX version mismatch)， 内容是 ( Description I'm developing on a HPC cluster where I don't have the ability to modify the CUDA version and I'm getting: `CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12020, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` Is it possible to specify the CUDA version for JAX to build against? Meaning doing something like `pip install U ""jax[cuda12010_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` My intuition is that this would be done by specifying a version of JAX CUDA that was released prior to CUDA 12.2 however the documentation is unclear on how to download `pip install U ""jax[cuda12_pip]""` while specifying the version. I've tried ` pip install U ""jax[cuda12_pip]""==0.3.25` which results in `WARNING: jax 0.3.25 does not provide the extra 'cuda12_pip'` causing the CPU version to be installed in favor of the CUDA version.  What )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unable to correct CUDA vs. JAX version mismatch," Description I'm developing on a HPC cluster where I don't have the ability to modify the CUDA version and I'm getting: `CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12020, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` Is it possible to specify the CUDA version for JAX to build against? Meaning doing something like `pip install U ""jax[cuda12010_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` My intuition is that this would be done by specifying a version of JAX CUDA that was released prior to CUDA 12.2 however the documentation is unclear on how to download `pip install U ""jax[cuda12_pip]""` while specifying the version. I've tried ` pip install U ""jax[cuda12_pip]""==0.3.25` which results in `WARNING: jax 0.3.25 does not provide the extra 'cuda12_pip'` causing the CPU version to be installed in favor of the CUDA version.  What ",2023-10-09T23:02:30Z,bug NVIDIA GPU,closed,14,22,https://github.com/jax-ml/jax/issues/18027,"If you run:  the right version of CUDA should be installed by `pip`. Can you show the output of `pip list` after doing that? If you did that (it sounds like you did), my guess is that you have another copy of CUDA installed and that JAX is preferring it for some reason. Is there another copy of CUDA present in your `LD_LIBRARY_PATH` directories and/or `PATH` directories? It is not possible to ""override"" the CUDA version JAX uses, but it should be very easy to `pip install` the correct version, and I'd like to understand why that's not working in your case.","I have a similar issue.  This is after doing ...  Running with LD_DEBUG=libs shows that it's getting the right libraries, with one oddity: `/home/michael/.local/lib/python3.10/sitepackages/jaxlib/cuda/../../nvidia/cusolver/lib/../../cusparse/lib/libcusparse.so.12: error: symbol lookup error: undefined symbol: __nvJitLinkGetLinkedCubin_12_2, version libnvJitLink.so.12 (fatal)`  If I manually upgrade via   then the error message changes, but still isn't right...  which might be a different issue??","I figured it out, though, not entirely. Unfortunately I still don't understand what went wrong with  `pip install U ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` but I did the following.  1. `pip install U ""https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.18+cuda12.cudnn88cp38cp38manylinux2014_x86_64.whl` 2. `pip install U https://storage.googleapis.com/jaxreleases/cuda12/jaxlib0.4.18+cuda12.cudnn88cp38cp38manylinux2014_x86_64.whl` 3. `pip install nvidiacublascu12 nvidiacudanvcccu12 nvidiacudaruntimecu12 nvidiacudnncu12 nvidiacufftcu12 nvidiacusolvercu12 nvidiacusparsecu12` I'm getting this new error/warning   Which can be resolved with `os.environ[""XLA_FLAGS""] = ""xla_gpu_force_compilation_parallelism=1""` I've noticed ~40% regression on performance on a baseline I have run before on the same machine. Any ideas on if this is related to the warning or an error I've made in installation?   hopefully the fix I outlined can get you up and running. You'll need to change the url in steps 1 and 2 for your setup, options are @ `https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`"," Your issue should be fixed by PR CC(Add a version constraint on nvidianvjitlinkcu12.). The issue is that NVIDIA's own pip packages don't have a necessary version constraint, so `nvidianvjitlinkcu12` doesn't get upgraded. It's not a direct dependency of JAX, rather a transitive dependency. The warning `20231010 21:32:22.307087: W external/xla/xla/service/gpu/nvptx_compiler.cc:703] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.2.140). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIAprovided CUDA forward compatibility packages. ` can be ignored if you like. It does no harm, it only slows down compilation a bit. The fix for that one is to upgrade your NVIDIA *driver*, like the message says. But you don't have to; nothing will go wrong if you don't.","> If you run: >  >  >  > the right version of CUDA should be installed by `pip`. Can you show the output of `pip list` after doing that? >  > If you did that (it sounds like you did), my guess is that you have another copy of CUDA installed and that JAX is preferring it for some reason. Is there another copy of CUDA present in your `LD_LIBRARY_PATH` directories and/or `PATH` directories? >  > It is not possible to ""override"" the CUDA version JAX uses, but it should be very easy to `pip install` the correct version, and I'd like to understand why that's not working in your case. Doin this does not automatically install the right version... in my case I have CUDA version 12010,, when I use that command, it installed the latest one but its against 12030 CUda version... what version of Jax that supports CUDA version 12010?"," We don't support CUDA 12.1, only 12.3 or later. You need to upgrade, and the `pip` command above should install the newer version. If you're still having trouble, share the output of `pip list | grep nvidia`. Are the up to date CUDA packages installed in the `pip` output?",">  We don't support CUDA 12.1, only 12.3 or later. You need to upgrade, and the `pip` command above should install the newer version. If you're still having trouble, share the output of `pip list  grep nvidia nvidiacublascu12       12.1.3.1 nvidiacudacupticu12   12.1.105 nvidiacudanvcccu12    12.4.99 nvidiacudanvrtccu12   12.1.105 nvidiacudaruntimecu12 12.1.105 nvidiacudnncu12        8.9.2.26 nvidiacufftcu12        11.0.2.54 nvidiacurandcu12       10.3.2.106 nvidiacusolvercu12     11.4.5.107 nvidiacusparsecu12     12.1.0.106 nvidiancclcu12         2.19.3 nvidianvjitlinkcu12    12.3.101 nvidianvtxcu12         12.1.105 ``` let me know what I should do to make it work. thanks thanks :)","Hello! I encountered a similar issue while I tried to make a new conda environment with JAX cuda12. After  I got the error messages below: CUDA backend failed to initialize: Found cuBLAS version 120205, but JAX was built against version 120304, which is newer.  The copy of cuBLAS that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Here is my output of the command  :  Could you help me??  I found that my HPC cluster has automatically loaded a module of CUDA 12.2.  resolved the issue without downgrading JAX.","CUDA backend failed to initialize: Found cuBLAS version 120205, but JAX was built against version 120304, which is newer. I have encountered this issue and solved it by specifying an older version (`0.4.23`): ",0.4.20 is works on me which `Found cuBLAS version 120103` same solution as   ,None of these solutions work for the Grace Hopper 200. ,In my case I had to downgrade the nvidia* pip pacakges from `12.4` to `12.3`. Though I really think that was the `nvidiacudnncu12`  package downgrade from `9.0.0.312` => `8.9.7.29` which fixed the  `CUDA backend failed to initialize: Unable to load cuDNN. Is it installed?` error for me. The simplest was to just run:  Which did just that:    Before  ,> In my case I had to downgrade the nvidia* pip pacakges from `12.4` to `12.3`. Though I really think that was the `nvidiacudnncu12` package downgrade from `9.0.0.312` => `8.9.7.29` which fixed the `CUDA backend failed to initialize: Unable to load cuDNN. Is it installed?` error for me. >  > The simplest was to just run: >  >  >  > Which did just that: >  > Before > After Worked like a charm! thanks!,"> In my case I had to downgrade the nvidia* pip pacakges from `12.4` to `12.3`. Though I really think that was the `nvidiacudnncu12` package downgrade from `9.0.0.312` => `8.9.7.29` which fixed the `CUDA backend failed to initialize: Unable to load cuDNN. Is it installed?` error for me. >  > The simplest was to just run: >  >  >  > Which did just that: >  > Before > After I got the same `Unable to load cuDNN` error when installing Jax using   I was able to solve this issue just by doing this, thank you so much!","> In my case I had to downgrade the nvidia* pip pacakges from `12.4` to `12.3`. Though I really think that was the `nvidiacudnncu12` package downgrade from `9.0.0.312` => `8.9.7.29` which fixed the `CUDA backend failed to initialize: Unable to load cuDNN. Is it installed?` error for me. >  > The simplest was to just run: >  >  >  > Which did just that: > Before > After This worked like hell, after 2 hours of work!!! Thank you  !!!",`pip install nvidiacudnncu12==8.9.7.29` is a more direct/simpler fix than `python3 m pip install tensorflow[andcuda]`,"Try set `LD_LIBRARY_PATH=""$YOUR_CUDA_PATH""` in the environment variables. Sometimes it's just jax cannot find the proper version. YOUR_CUDA_PATH can be for example `/usr/local/cuda12.1/lib64`","I'm having a similar issue with the newer version of jaxlib 0.4.26 and CUDA 12.1 and running   doesn't do the trick. In this case, the error seems to be derived from `cuSPARSE` **When running an scVI model I get the following message:**  I find it confusing that the message says the CUDA backend failed to initialize, but then the trainer spits out `GPU available: True (cuda), used: True` which I think suggests cuda is still being used... Should I ignore this? Also, not sure what it means by `installed version 12002`. That doesn't track with the actual versions installed (see below). I'm developing in an HPC so I have limited options to change CUDA. I'm working with CUDA 12.1 and I need to have pytorch and jax in the same environment. The jax 0.4.26 released last month should be compatible with this version of CUDA, and I installed them as recommended by the developers using:  **NVIDIA drivers**  jax                           0.4.26 jaxlib                        0.4.26+cuda12.cudnn89 ``` Any thoughts/ideas are greatly appreciated!","> `pip install nvidiacudnncu12==8.9.7.29` is a more direct/simpler fix than `python3 m pip install tensorflow[andcuda]` Worked for me. I guess because I had `0.4.20+cuda12.cudnn89`, which didn't work with the `nvidiacudnncu129.1.0.70`. ",How can i slove this problem?  pip list  grep jax jax                      0.4.28 jaxlib                   0.4.28+cuda12.cudnn89, Please open a new discussion for this topic. I'm also not clear exactly which issue you're trying to solve.,"Used the `tensorflow[andcuda]` method from this thread to fix this problem when I originally ran into it a few months ago. Got this error again today while messing with drivers, and installing `tensorflow[andcuda]` didn't solve the problem. For a dirty fix with driver version 12.2, uninstalling `tensorflow` then running   worked for me."
390,"以下是一个github上的jax下的一个issue, 标题是([JAX] Add an option `subset_by_index` that allows computing a contiguous subset of eigenvalues from eigh.)， 内容是 ([JAX] Add an option `subset_by_index` that allows computing a contiguous subset of eigenvalues from eigh.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[JAX] Add an option `subset_by_index` that allows computing a contiguous subset of eigenvalues from eigh.,[JAX] Add an option `subset_by_index` that allows computing a contiguous subset of eigenvalues from eigh.,2023-10-09T22:22:03Z,,closed,0,1,https://github.com/jax-ml/jax/issues/18026,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
753,"以下是一个github上的jax下的一个issue, 标题是(cuSOLVER version question)， 内容是 ( Description When to use jnp.array(),it remind that""CUDA backend failed to initialize: Found cuSOLVER version 11501, but JAX was built against version 11502, which is newer. The copy of cuSOLVER that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ""  What jax/jaxlib version are you using? jax0.4.18 jaxlib0.4.18+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info 3.10.9/Linux  NVIDIA GPU info Mon Oct  9 22:20:04 2023        ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,cuSOLVER version question," Description When to use jnp.array(),it remind that""CUDA backend failed to initialize: Found cuSOLVER version 11501, but JAX was built against version 11502, which is newer. The copy of cuSOLVER that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) ""  What jax/jaxlib version are you using? jax0.4.18 jaxlib0.4.18+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info 3.10.9/Linux  NVIDIA GPU info Mon Oct  9 22:20:04 2023        ++  ++",2023-10-09T14:20:46Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/18009,"That's correct. In the latest JAX releases, we added code to verify that your installed CUDA libraries are at least as new as the libraries against which JAX was built. You can fix by updating your copy of cusolver. That said, we're probably being too strict here since we're requiring matching patch versions, so I'll relax that version test.",Hi   May I ask how to update the cusolver? Download and install a new CUDA Toolkit? Is it correct?
1259,"以下是一个github上的jax下的一个issue, 标题是( Issue with JAX's frompyfunc and at methods when compared to NumPy)， 内容是 ( Description I encountered an issue when trying to use JAX's frompyfunc and at methods for a specific use case that works fine in NumPy. Below is the code snippet that demonstrates the issue:   Expected Behavior: The code above is expected to perform an addition operation using JAX's frompyfunc and at methods, similar to the behavior in NumPy. Actual Behavior: However, running this code results in the following error:  Additional Information: This code works as expected in NumPy using the equivalent NumPy functions. The issue appears to be specific to JAX's implementation.  Numpy seems to work fine with the following results.  Environment: JAX version: 0.4.16 Python version: 0.4.16+cuda11.cudnn86 Operating System: Colab Please let me know if any further information or clarification is needed to address this issue.  What jax/jaxlib version are you using? jax: 0.4.16, jaxlib: 0.4.16+cuda11.cudnn86  Which accelerator(s) are you using? GPU T4   Additional system info On Colab  NVIDIA GPU info nvidiasmi)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi, Issue with JAX's frompyfunc and at methods when compared to NumPy," Description I encountered an issue when trying to use JAX's frompyfunc and at methods for a specific use case that works fine in NumPy. Below is the code snippet that demonstrates the issue:   Expected Behavior: The code above is expected to perform an addition operation using JAX's frompyfunc and at methods, similar to the behavior in NumPy. Actual Behavior: However, running this code results in the following error:  Additional Information: This code works as expected in NumPy using the equivalent NumPy functions. The issue appears to be specific to JAX's implementation.  Numpy seems to work fine with the following results.  Environment: JAX version: 0.4.16 Python version: 0.4.16+cuda11.cudnn86 Operating System: Colab Please let me know if any further information or clarification is needed to address this issue.  What jax/jaxlib version are you using? jax: 0.4.16, jaxlib: 0.4.16+cuda11.cudnn86  Which accelerator(s) are you using? GPU T4   Additional system info On Colab  NVIDIA GPU info nvidiasmi",2023-10-08T05:09:23Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/18004,Thanks for the report! I'm looking into it
1280,"以下是一个github上的jax下的一个issue, 标题是(Jax/Brax program Crashes When Using Jacrev, but not Jacfwd)， 内容是 ( Description I opened a parallel issue in Brax, but was looking for some general JAX knowledge. How do I troubleshoot Jax crashing in an intelligent way? I saw this issue mention throwing error messages rather than crashing, but that does not seem to help me.  Essentially I am trying to persistently cache jacobians of the step function in brax on GPU, but am finding that if I use jacrev, the program will crash anytime the code is run a second time (when it accesses the persistent cache) and will do so without any error message. However, if I compute the jacobian and cache it with jacfwd, the cache is accessed without issue. Provided below is a minimal example using brax + jax. To reproduce my issue: Inside of minimal.py set which_calc to either jacfwd or jacrev Run minimal.py  entire program will execute and ./cache will be made and populated Run minimal.py again  second print (line 63) will not execute and program will quit prematurely if using jacrev, but not jacfwd minimal.txt  What jax/jaxlib version are you using? 0.4.14 )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Jax/Brax program Crashes When Using Jacrev, but not Jacfwd"," Description I opened a parallel issue in Brax, but was looking for some general JAX knowledge. How do I troubleshoot Jax crashing in an intelligent way? I saw this issue mention throwing error messages rather than crashing, but that does not seem to help me.  Essentially I am trying to persistently cache jacobians of the step function in brax on GPU, but am finding that if I use jacrev, the program will crash anytime the code is run a second time (when it accesses the persistent cache) and will do so without any error message. However, if I compute the jacobian and cache it with jacfwd, the cache is accessed without issue. Provided below is a minimal example using brax + jax. To reproduce my issue: Inside of minimal.py set which_calc to either jacfwd or jacrev Run minimal.py  entire program will execute and ./cache will be made and populated Run minimal.py again  second print (line 63) will not execute and program will quit prematurely if using jacrev, but not jacfwd minimal.txt  What jax/jaxlib version are you using? 0.4.14 ",2023-10-06T21:24:55Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/17993
825,"以下是一个github上的jax下的一个issue, 标题是(identify PRNG schemes on key arrays; recognize them in key constructors)， 内容是 (This change covers another iteration on CC([random] wrap_key_data accepts impl=key.dtype.impl), following discussion there. Specifically: * Introduce `jax.random.key_impl`, which accepts a key array and returns a hashable identifier of its PRNG implementation. * Accept this identifier optionally as the `impl` argument to `jax.random.key` and `wrap_key_data`. This now works:  This change also set up an internal PRNG registry and register builtin implementations, to simplify various places where we essentially reconstruct such a registry from scratch (such as in tests).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,identify PRNG schemes on key arrays; recognize them in key constructors,"This change covers another iteration on CC([random] wrap_key_data accepts impl=key.dtype.impl), following discussion there. Specifically: * Introduce `jax.random.key_impl`, which accepts a key array and returns a hashable identifier of its PRNG implementation. * Accept this identifier optionally as the `impl` argument to `jax.random.key` and `wrap_key_data`. This now works:  This change also set up an internal PRNG registry and register builtin implementations, to simplify various places where we essentially reconstruct such a registry from scratch (such as in tests).",2023-10-06T15:16:10Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17983
1328,"以下是一个github上的jax下的一个issue, 标题是(jax_threefry_partitionable + rematerialization doesn't seem to be working together in distributed training)， 内容是 ( Description I have a transformer model where each transformer block is rematerialized. The model is distributed over multiple devices using jit. Each transformer block has dropout enabled. To prevent rng implementation from inserting synchronization operations I'm also enabling `jax_threefry_partitionable` as suggested in the doc. Problem is, `jax_threefry_partitionable` doesn't seem to play nicely with rematerialization. As soon as I enable dropout, I get GPU OOM because JAX decides to preserve huge arrays containing rng key per activation tensor component for each transformer block, despite them being rematerialized. It should be possible for jax to reconstruct this key array from a single key during rematerialization, but it doesn't seem to do that. I'm happy to provide a repoduction if you can confirm that this is unexpected behavior. If not, can you please suggest a workaround? Currently it doesn't seem possible to efficiently train large models with dropout. A relevant discussion with OOM error message example here: htt)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,jax_threefry_partitionable + rematerialization doesn't seem to be working together in distributed training," Description I have a transformer model where each transformer block is rematerialized. The model is distributed over multiple devices using jit. Each transformer block has dropout enabled. To prevent rng implementation from inserting synchronization operations I'm also enabling `jax_threefry_partitionable` as suggested in the doc. Problem is, `jax_threefry_partitionable` doesn't seem to play nicely with rematerialization. As soon as I enable dropout, I get GPU OOM because JAX decides to preserve huge arrays containing rng key per activation tensor component for each transformer block, despite them being rematerialized. It should be possible for jax to reconstruct this key array from a single key during rematerialization, but it doesn't seem to do that. I'm happy to provide a repoduction if you can confirm that this is unexpected behavior. If not, can you please suggest a workaround? Currently it doesn't seem possible to efficiently train large models with dropout. A relevant discussion with OOM error message example here: htt",2023-10-06T14:22:33Z,bug,open,9,16,https://github.com/jax-ml/jax/issues/17982,I strugle with the same issue.,"Also relevant for me, would be great to have it solved."," Hey, sorry for mentioning you directly, but this issue hasn't received any attention for several weeks. Can someone from the jax team please take a look? Thanks!"," Hey, sorry for mentioning you directly, but can someone take a look at this issue? It's a big blocker for me.","I've made a repro for this bug. Turns out it has nothing to do with `jax_threefry_partitionable`, perfectly repoducible without it. Repo was made for A100 80Gb, so tensor shapes might need to be adjusted for a GPU with a different amount of memory. `./repro.py` — will fail because without rematerialization it needs ~122.75 Gb of GPU RAM `./repro.py remat` — works perfectly fine with remat, because it now needs just 63Gb of GPU RAM `./repro.py remat dropoutrate 0.1` — OOMs again, requiring ~118Gb of GPU RAM. From looking at peak buffers it becomes clear that the dropout mask is not being rematerialized: tensors correponding to full dropout masks for different layers are occupying memory.  Repro code: ",Thanks for the repro!," sorry that this slipped through the cracks. Thanks for the pings, everyone. Can you check that this repros with jaxlib 0.4.20? IIRC there was one GPUspecific remat fix that happened recently, though I don't have a link to it at the moment. EDIT: https://github.com/openxla/xla/pull/6527","Thanks for the pointer! Unfortunately, it looks like the problem is still present with jaxlib==0.4.20","Thanks for checking. I think our next step is to try to repro on TPU, to see if it's GPUspecific. We can do that on our end.","Hey, any updates on this?","  Happy new year, gentlemen! Do you think 2024 is the year when this bug finally got fixed? ;) ",Ping!,"Hmm, looks like using `jax_default_prng_impl=rbg` fixes this issue.","> Hmm, looks like using `jax_default_prng_impl=rbg` fixes this issue. Thanks, this is a useful additional bit of info. This is still in our queue, but we haven't dug in yet. I understood your most recent comment to mean that you have a workaround. Is that right? At large scales, `jax_default_prng_impl=rbg` can be a good idea to try anyway, as it can drastically speed up compilation times.","> I understood your most recent comment to mean that you have a workaround. Is that right? Looks like it. Interestingly, it also seems to fix another rngrelated issue: https://github.com/google/jax/issues/19893 Btw, can you elaborate a bit on how does the `rng` implementation work when keys are sharded? E.g. does it require any additional communication?","On GPU, for a fixed key, I do not expect that sharded _number generation_ under `rbg` would require communication. E.g. I expect the following to print `False`:  (and the same if we check for other collectives in the HLO.) Meanwhile I also expect the output sharding of `f(key, x)` to be, e.g.:  when `jax.devices()` is a list of two GPUs. Your comment however asks ""when keys are sharded."" Do you mean that you are sharding a computation that vmaps a random number generation operation over a batch of keys (in the form of a sharded key array)? If so, then there's a current unrelated issue to watch specifically regarding `vmap` of `rbg` over keys, covered by CC(efficient untrue batching of `random_bit_generator`). The workaround there is not to `vmap` number generation over keys, but instead to hoist the generation step: draw the entire batch of random numbers from a single key outside of the vmapped function, and pass that in."
464,"以下是一个github上的jax下的一个issue, 标题是(Error with random.PRNGKey)， 内容是 ( Description Error occurred when generate PRNGKey with jax.random   After calling the above lines    What jax/jaxlib version are you using? jax v0.4.2  Which accelerator(s) are you using? GPU   Additional system info Python 3.10.9, OS(Linux)  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Error with random.PRNGKey," Description Error occurred when generate PRNGKey with jax.random   After calling the above lines    What jax/jaxlib version are you using? jax v0.4.2  Which accelerator(s) are you using? GPU   Additional system info Python 3.10.9, OS(Linux)  NVIDIA GPU info ",2023-10-05T17:41:38Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17960,"Thanks for the report! This looks like it is probably due to a mismatch between your jax version and your jaxlib version. On that, you report that you're using jax v0.4.2, but the traceback shows a file that did not exist until jax v0.4.15: https://github.com/google/jax/blob/jaxlibv0.4.15/jax/_src/cache_key.py. So you must actually be using a more recent version of JAX. So the place to start would be to make sure that (1) you're using the JAX version you intend to be using, and (2) the jaxlib version you have installed is compatible.","Thank you for the help, after correcting the Jaxlib version, the error is resolved. "
506,"以下是一个github上的jax下的一个issue, 标题是(Draft: Scaled Dot Product Attention API in JAX)， 内容是 (Attention mechanisms, particularly the Scaled Dot Product Attention, play a vital role in modern neural architectures, especially in transformers. This PR introduces a JAXbased Scaled Dot Product Attention API, providing users with an optimized, flexible, and easytouse interface.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Draft: Scaled Dot Product Attention API in JAX,"Attention mechanisms, particularly the Scaled Dot Product Attention, play a vital role in modern neural architectures, especially in transformers. This PR introduces a JAXbased Scaled Dot Product Attention API, providing users with an optimized, flexible, and easytouse interface.",2023-10-05T17:20:25Z,,closed,0,3,https://github.com/jax-ml/jax/issues/17957,"Thanks for this! Is there a way to test that XLA:GPU is lowering indeed this to the intended cuDNN calls? (I have some ideas for how to do that, and I can give them a shot if you'd like, but I wanted to check if you already had thoughts on this.)","> Thanks for this! >  > Is there a way to test that XLA:GPU is lowering indeed this to the intended cuDNN calls? (I have some ideas for how to do that, and I can give them a shot if you'd like, but I wanted to check if you already had thoughts on this.) I have some internal unit tests that I'm using to validate the lowering. But it'd be great if you can give it a shot :).",This project is replaced by https://github.com/google/jax/pull/18814 So closing this PR.
720,"以下是一个github上的jax下的一个issue, 标题是(Getting Array instead of DeviceArray  )， 内容是 ( Description Hi Team, I am trying to understand the `Asynchronous dispatch` link  and when executing the below program (in colab) mentioned in this page I am getting `Array` instead of `DeviceArray`.  My query is whats the difference between `Array` and `DeviceArray`? Secondly, do we need document update here?   What jax/jaxlib version are you using? jax    0.4.16 ,    jaxlib   0.4.16+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Mac  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Getting Array instead of DeviceArray  ," Description Hi Team, I am trying to understand the `Asynchronous dispatch` link  and when executing the below program (in colab) mentioned in this page I am getting `Array` instead of `DeviceArray`.  My query is whats the difference between `Array` and `DeviceArray`? Secondly, do we need document update here?   What jax/jaxlib version are you using? jax    0.4.16 ,    jaxlib   0.4.16+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Mac  NVIDIA GPU info _No response_",2023-10-04T21:23:31Z,documentation,closed,0,2,https://github.com/jax-ml/jax/issues/17935,We need to update that doc but DeviceArray was deleted in favor of Array. https://jax.readthedocs.io/en/latest/jax_array_migration.html goes into more details.,I think we can declare this fixed now CC(Asynchronous dispatch doc update regarding jax.Array migration) was merged.
448,"以下是一个github上的jax下的一个issue, 标题是(Shard_map does not have a checkify rule)， 内容是 ( Description Trying to use checkify leads to following error:   What jax/jaxlib version are you using? jax,jaxlib: HEAD  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Shard_map does not have a checkify rule," Description Trying to use checkify leads to following error:   What jax/jaxlib version are you using? jax,jaxlib: HEAD  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-10-03T19:14:18Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17903,"Hey! I want to reproduce this error. Can you share the details of your environment, testsetup (commands etc)?"
574,"以下是一个github上的jax下的一个issue, 标题是([export] Set the default export serialization version to 8.)， 内容是 ([export] Set the default export serialization version to 8. This version has been supported by XlaCallModule since July 21, 2023 and we are now past the forwardcompatibility window. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions Reverts ae81ac9cc21696a22b973b1eae6ce222c7318ba7)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[export] Set the default export serialization version to 8.,"[export] Set the default export serialization version to 8. This version has been supported by XlaCallModule since July 21, 2023 and we are now past the forwardcompatibility window. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions Reverts ae81ac9cc21696a22b973b1eae6ce222c7318ba7",2023-10-03T04:56:36Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17897
1002,"以下是一个github上的jax下的一个issue, 标题是(Getting NaN for gradients when computing the loss with jit compile)， 内容是 ( Description I am trying to get the sensitivity of the loss with respect to the model params by running the following minimum reproducible example.   When I run this with Jax jit compile I get NaN for the gradients, but when I implement config.update(""jax_disable_jit"", True) to disable the jit compile I am getting values for gradients, but it is very slow.  Can anyone help me shed some light as to why this is happening, and how I can compute gradients with the jit compiler and BRAX physics engine? I am currently raising this issue with the authors of BRAX as well.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Python Verion =3.11.5, Ubuntu=22.04  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Getting NaN for gradients when computing the loss with jit compile," Description I am trying to get the sensitivity of the loss with respect to the model params by running the following minimum reproducible example.   When I run this with Jax jit compile I get NaN for the gradients, but when I implement config.update(""jax_disable_jit"", True) to disable the jit compile I am getting values for gradients, but it is very slow.  Can anyone help me shed some light as to why this is happening, and how I can compute gradients with the jit compiler and BRAX physics engine? I am currently raising this issue with the authors of BRAX as well.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Python Verion =3.11.5, Ubuntu=22.04  NVIDIA GPU info _No response_",2023-10-02T20:27:21Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/17891,I was able to solve the issue. It seems that the problem was in how I was initializing the initial state with Brax.,"Hi mander , how did you solve this issue? I also got this error while using Brax and Jax. It bugs me for a long time. I would appreciate it very much if you could share more information.",So I rewrote my system and how I initialized my initial state and it seemed to work for me. See the example below:  ,Thanks for sharing! mander  I ended up using the spring backend to avoid the issue. 
573,"以下是一个github上的jax下的一个issue, 标题是(Test failures on aarch64-darwin: `RuntimeWarning: divide by zero encountered in equal`)， 内容是 ( Description Running the test suite on aarch64darwin gives me 3 errors.   What jax/jaxlib version are you using? jax 0.4.16 (refs/tags/jaxv0.4.16), the jaxlib wheel from PyPI v0.4.16.  Which accelerator(s) are you using? CPU  Additional system info macOS aarch64darwin, M1 Pro  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Test failures on aarch64-darwin: `RuntimeWarning: divide by zero encountered in equal`," Description Running the test suite on aarch64darwin gives me 3 errors.   What jax/jaxlib version are you using? jax 0.4.16 (refs/tags/jaxv0.4.16), the jaxlib wheel from PyPI v0.4.16.  Which accelerator(s) are you using? CPU  Additional system info macOS aarch64darwin, M1 Pro  NVIDIA GPU info _No response_",2023-09-29T22:23:36Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/17867,Just ran again and I'm seeing another failure. Perhaps they're nondeterministic? ,"""ANOTHER ONE""  DJ Khaled  EDIT: ok turns out there's a bunch of these: ","Thanks for the report. Out of curiosity, what Python version are you using? I suspect these may be versiondependent.","It looks like this is similar to https://github.com/numpy/numpy/issues/18992, but for dividebyzero warnings rather than overflow warnings.","Looking closer, it seems like it has something to do with the interaction between `np.asanyarray` and custom PRNG keys:  But I'm only seeing this on Python 3.9, which is probably why this isn't getting caught by our CI.","Interesting, I'm on Python 3.10.12... not sure why it's getting picked up only on some versions","I'm still not sure the root cause of this, but the warnings boil down to this:  That is, if you ever call `==` on an object array that contains JAX arrays, numpy raises this warning on some platforms. I have some fixes in CC(random_test: fix deprecation warnings for key tests) & CC(api_test: fix platformdependent deprecation warning)"
399,"以下是一个github上的jax下的一个issue, 标题是(Random Bernoulli allocates massive amounts of unnecessary memory  )， 内容是 (I am running jax.random.bernoulli to run 64*64 trials for 4 million different mean values. However, this consumes ~800GB of VRAM. It probably shouldn't. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Random Bernoulli allocates massive amounts of unnecessary memory  ,"I am running jax.random.bernoulli to run 64*64 trials for 4 million different mean values. However, this consumes ~800GB of VRAM. It probably shouldn't. ",2023-09-29T20:50:23Z,enhancement,closed,0,6,https://github.com/jax-ml/jax/issues/17865,"Thanks for the report! This certainly seems like a lot of memory, but you should keep in mind that just storing `64*64*4000000` float32 values will take ~60GB (64 * 64 * 4000000 * 4 bytes), and `random.bernoulli` needs several times this memory to perform the operations it uses to go from random bits to bernoullidistributed floats. You can get some sense of what's involved by looking at the jaxpr for this operation:  Since each `64,64,4000000` value occupies ~60GB, it's not unreasonable that the whole operation, with several individual computations on intermediate arrays of this size, would require several hundred gigabytes. If you need to be more memory efficient, you might try generating your bernoulli samples in batches, within `lax.scan` for example. Do you think an approach like that might work for your usecase?","Is it not just a simple boolean comparison between p and some uniforms? I understand that there are technical reasons for what is happening. However, in theory, writing this in CUDA would result in 2GB of VRAM. That means that Jax Bernoulli has a 100x memory overhead for this operation.","> Is it not just a simple boolean comparison between p and some uniforms? Those uniforms take up 60GB on their own, so I'm not sure how you could fit that into 2GB in theory without doing some sort of sequentiallybatched operation. Even the output (4000000 x 64 x 64 x 1 byte booleans) takes up ~15 GB. Maybe there's some assumption you're making that I'm not understanding?","Why do those uniforms need to be stored in VRAM? Can’t they be generated, stored in one of the warp registers, then discarded immediately? They don’t need to be accumulated, right? I’m not familiar with TPU architecture. Also, isn’t a boolean 1 bit?","It's true that there is only one bit of information per boolean value, but JAX/XLA (like many other numerical computing systems) stores booleans as bytes for reasons related to computational efficiency.","I'm going to close this, because it seems to be working as expected."
442,"以下是一个github上的jax下的一个issue, 标题是(Clean up build_wheel.py and build_gpu_plugin_wheel.py.)， 内容是 (* Use pathlib.Path objectoriented paths. * Change copy_files() helper to copy many files in one call. * Make copy_files() also make the output directory, if needed. * Format file with pyink pyinkindentation=2)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Clean up build_wheel.py and build_gpu_plugin_wheel.py.,"* Use pathlib.Path objectoriented paths. * Change copy_files() helper to copy many files in one call. * Make copy_files() also make the output directory, if needed. * Format file with pyink pyinkindentation=2",2023-09-29T20:04:23Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17864
1266,"以下是一个github上的jax下的一个issue, 标题是(Support for ragged arrays, like torch.nested)， 内容是 (One of the quite compelling new additions to the torch ecosystem is NestedTensors, see https://pytorch.org/docs/stable/nested.html. Basically, this is a new primitive in the Torch ecosystem which allows for tensors which are ""ragged"" along one dimension (aka a tensor with shape `B x ? x 3`), as well as a bunch of ops which have been optimized to support this style of batching. Ragged batches are very common in a number of domains; variablelength sequences in NLP tasks, 3D vision tasks such as Point Cloud and Mesh analysis (which often have variable numbers of nodes+faces), graph processing, etc. When batching for parallel/vectorized computation, most researchers tend to use padding operations or do some clever (manual) operations on tensors of shape (`(sum N_i) x 3` where `N_i` is the number of elements in the ith element of a batch.  Examples of such clever vectorization: * https://vladfeinberg.com/2021/01/07/vectorizingraggedarrays.html * https://github.com/googledeepmind/jraph There are a few wellknown tricks for doing thes)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",few shot,"Support for ragged arrays, like torch.nested","One of the quite compelling new additions to the torch ecosystem is NestedTensors, see https://pytorch.org/docs/stable/nested.html. Basically, this is a new primitive in the Torch ecosystem which allows for tensors which are ""ragged"" along one dimension (aka a tensor with shape `B x ? x 3`), as well as a bunch of ops which have been optimized to support this style of batching. Ragged batches are very common in a number of domains; variablelength sequences in NLP tasks, 3D vision tasks such as Point Cloud and Mesh analysis (which often have variable numbers of nodes+faces), graph processing, etc. When batching for parallel/vectorized computation, most researchers tend to use padding operations or do some clever (manual) operations on tensors of shape (`(sum N_i) x 3` where `N_i` is the number of elements in the ith element of a batch.  Examples of such clever vectorization: * https://vladfeinberg.com/2021/01/07/vectorizingraggedarrays.html * https://github.com/googledeepmind/jraph There are a few wellknown tricks for doing thes",2023-09-29T19:45:02Z,enhancement,open,3,2,https://github.com/jax-ml/jax/issues/17863,Any progress?,I didn't really test this thing so there's probably lurking bugs. But it's a high level sketch of a way to do something slightly more general than ragged arrays: https://gist.github.com/vyeevani/e10c4a92bb74edf51b03d8a05e652049. I don't necessarily think this is something that would have to be added directly into jax. Feels like it can be handled by libraries 
372,"以下是一个github上的jax下的一个issue, 标题是(Add tests to cover `PyTreeDef.flatten_up_to` error scenarios.)， 内容是 (Add tests to cover `PyTreeDef.flatten_up_to` error scenarios. Also improve coverage of `PyTreeDef.flatten_up_to` success scenarios.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add tests to cover `PyTreeDef.flatten_up_to` error scenarios.,Add tests to cover `PyTreeDef.flatten_up_to` error scenarios. Also improve coverage of `PyTreeDef.flatten_up_to` success scenarios.,2023-09-29T19:33:02Z,,closed,0,1,https://github.com/jax-ml/jax/issues/17862,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
511,"以下是一个github上的jax下的一个issue, 标题是(Divergence between `jnp.load` and `np.load`)， 内容是 ( Description  I assume this is due to some internal dtype conversion that isn't supposed to be happening.  What jax/jaxlib version are you using? JAX: 173a27017; jaxlib: 0.4.16  Which accelerator(s) are you using? _No response_  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Divergence between `jnp.load` and `np.load`, Description  I assume this is due to some internal dtype conversion that isn't supposed to be happening.  What jax/jaxlib version are you using? JAX: 173a27017; jaxlib: 0.4.16  Which accelerator(s) are you using? _No response_  Additional system info Linux  NVIDIA GPU info _No response_,2023-09-29T17:58:08Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/17858,"Unless you `ENABLE_X64`, `x` is float64 and `y` is float32, which would explain the discrepancy. Try this: ","Right, but I don't always have 64bit enabled. I understand the reason for this behaviour, it's just hard to work around! Maybe simply to target my actual usecase: given an `np.generic` to deserialise, then this behaviour means that `np.load` must be used for `np.float64`, but `jnp.load` must be used for `jax.dtypes.bfloat16`. Is there a clean way to handle both of these cases, or must a manual whitelist be constructed? I'm hoping to avoid doing something like ","I think if you want the output to be JAX arrays, use `jnp.load`. If you want the output to be numpy arrays, use `np.load`. Note that `jnp.load` is just a light wrapper around `np.load`: https://github.com/google/jax/blob/ef6fd2ebb6cb31174c63338a07bdec88886dcd8b/jax/_src/numpy/lax_numpy.pyL282L294 If the bakedin logic doesn't work for you, then you could write your own utility in a few lines.","Closing. This makes things a bit tricky for downstream libraries, but on reflection I appreciate there's not that much more JAX can do here."
1282,"以下是一个github上的jax下的一个issue, 标题是(scan + slice update leads to copy instead of in-place update)， 内容是 ( Description Hi, I am running a `scan` with a 1D `carry` array over several thousand steps, where in each step, for a given index `idx`, I need to return the value at `carry[idx]`, then update the `carry` by shifting a slice of the `carry` left oneplace, with `start_index = idx + 1` and a fixed `slice_size`. I call this operation a `pop`, and it is fairly simple to implement with a combination of `scan` + `dynamic_slice` + `dynamic_update_slice`. However, a naive implementation (called `slow_pop` below) seems to be very inefficient, and I suspect it leads to a copy of the `carry` at every step, instead of an inplace update. I came up with a workaround (called `fast_pop` below) that is a couple of orders of magnitude faster. Here is a minimal working example:  !image In the first row of the plots, we see that `fast_pop` has little dependence on `carry_size`, which is to be expected, while `slow_pop` scales linearly with increase in `carry_size`, which suggests it might be copying the `carry`. In the second row of plots, we see )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,scan + slice update leads to copy instead of in-place update," Description Hi, I am running a `scan` with a 1D `carry` array over several thousand steps, where in each step, for a given index `idx`, I need to return the value at `carry[idx]`, then update the `carry` by shifting a slice of the `carry` left oneplace, with `start_index = idx + 1` and a fixed `slice_size`. I call this operation a `pop`, and it is fairly simple to implement with a combination of `scan` + `dynamic_slice` + `dynamic_update_slice`. However, a naive implementation (called `slow_pop` below) seems to be very inefficient, and I suspect it leads to a copy of the `carry` at every step, instead of an inplace update. I came up with a workaround (called `fast_pop` below) that is a couple of orders of magnitude faster. Here is a minimal working example:  !image In the first row of the plots, we see that `fast_pop` has little dependence on `carry_size`, which is to be expected, while `slow_pop` scales linearly with increase in `carry_size`, which suggests it might be copying the `carry`. In the second row of plots, we see ",2023-09-29T09:30:40Z,bug,open,2,0,https://github.com/jax-ml/jax/issues/17845
1252,"以下是一个github上的jax下的一个issue, 标题是(jit of scan gives XlaRuntimeError: UNIMPLEMENTED: Dot with multiple contracting dimensions not implemented.)， 内容是 ( Description Getting some weird behavior and trying to diagnose.  I have something of the form  Where `Thing` is a custom pytree class, `params` is a dict of arrays im scanning over, and `Thing.compute` is some big long function. As written above, things seem to run ok. However, if I jit `compute_field` I get the following:  I'm unable to give a more complete MWE as I'm not even sure what operation is causing the issue, as far as I know there aren't any dot operations with multiple contracting dimensions, and the error message doesn't give any more detail about where it's failing. Also curious that although `scanfun` should be getting compiled by `jax.lax.scan` it seems like it recompiles on every call (from running `with jax.log_compiles()`) Any clues?  What jax/jaxlib version are you using? jax version=0.4.13, jaxlib version=0.4.13  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jit of scan gives XlaRuntimeError: UNIMPLEMENTED: Dot with multiple contracting dimensions not implemented.," Description Getting some weird behavior and trying to diagnose.  I have something of the form  Where `Thing` is a custom pytree class, `params` is a dict of arrays im scanning over, and `Thing.compute` is some big long function. As written above, things seem to run ok. However, if I jit `compute_field` I get the following:  I'm unable to give a more complete MWE as I'm not even sure what operation is causing the issue, as far as I know there aren't any dot operations with multiple contracting dimensions, and the error message doesn't give any more detail about where it's failing. Also curious that although `scanfun` should be getting compiled by `jax.lax.scan` it seems like it recompiles on every call (from running `with jax.log_compiles()`) Any clues?  What jax/jaxlib version are you using? jax version=0.4.13, jaxlib version=0.4.13  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-09-28T19:29:35Z,bug needs info,closed,0,3,https://github.com/jax-ml/jax/issues/17834,"Can you please fill in the bug sections about accelerator and GPU info? What hardware platform are you using. Can you please try with the latest JAX and jaxlib releases (0.4.18)? The issue might already be fixed. If not, we'll need some sort of reproduction to debug this. One possibility would be for you to take an XLA HLO dump, which you can get by reproducing the bug with the environment variable `XLA_FLAGS=xla_dump_to=/somewhere`, and zip up `/somewhere` and attach it to this bug.",I searched for this message in the code and it appears to be in the CPU XLAtoLLVM IR translator here.,Closing; we need a reproduction or an HLO dump. We'd be happy to look into this if you can provide either.
923,"以下是一个github上的jax下的一个issue, 标题是(Compilation error on jaxlib v0.4.16)， 内容是 ( Description Following this old tutorial for Jax on Jetson Nano I am unable to successfully build jax from source on a Jetson Orin Nano. Env setup & download JAX:  Build taken from here:  Fails with the following output:  I've been trying to get this working for the past weeks but I have not been able to find a proper solution. If anyone made it work and is willing to share what they did I would greatly appreciate it.  What jax/jaxlib version are you using? jaxlib 0.4.16  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 20.04(aarch64 NVIDIA Jetson Orin Nano), Python 3.9.18, gcc 9.4  NVIDIA GPU info No `nvidiasmi` but with jtop I can list the CUDA version and the cuDNN version. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Compilation error on jaxlib v0.4.16," Description Following this old tutorial for Jax on Jetson Nano I am unable to successfully build jax from source on a Jetson Orin Nano. Env setup & download JAX:  Build taken from here:  Fails with the following output:  I've been trying to get this working for the past weeks but I have not been able to find a proper solution. If anyone made it work and is willing to share what they did I would greatly appreciate it.  What jax/jaxlib version are you using? jaxlib 0.4.16  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 20.04(aarch64 NVIDIA Jetson Orin Nano), Python 3.9.18, gcc 9.4  NVIDIA GPU info No `nvidiasmi` but with jtop I can list the CUDA version and the cuDNN version. ",2023-09-28T14:49:18Z,bug build NVIDIA GPU,closed,0,7,https://github.com/jax-ml/jax/issues/17829,"I'm not 100% sure, but my guess is this is related to your CUDA version (11.4), which is much older than anything JAX supports (11.8 or newer). Can you update?",I'll try to update today but if it fails is there any older version that works with the CUDA version that I have?,"I'm confident that's it. This code doesn't define that value for CUDA < 11.6. https://github.com/openxla/xla/blob/bdb788c2e164c661ed8390d7825b0d792d298ac1/xla/backends/profiler/gpu/cupti_tracer.ccL63 We could fix that, but I don't know there's a lot of point: we've dropped support for that CUDA version. You should just update. Closing since the fix is to update CUDA!","I have tried building some older versions of jax for CUDA 11.4, haven't had any success yet. The NX series is flashed with CUDA 11.4 and cuDNN 0.8.6. Any leads regarding this would be helpful, thanks!",If you want to use an older version of Jax for CUDA 11.4 and cuDNN 8.6.0 you can try using this wheel. Last year when I used it I remember that it worked as intended.,"  Thanks, I've checked your wheels, really helpful! Were you able to update cuDNN on Xavier NX to build the later versions?","I was able to use an updated version of cuDNN and CUDA on an Orin Nano. I am not sure about an Xavier NX but it should be possible. There are special versions for both that work on Jetson. cuDNN CUDA (these are the latest just an example) Whatever you are trying to do remember that if it doesn't work directly you could always try to use docker with Jax, CUDA, and cuDNN but it does require some tinkering."
333,"以下是一个github上的jax下的一个issue, 标题是(Fix incorrect backend allowlist in array_interoperability_test.)， 内容是 (We intended to only enable this test on CPU and GPU, but we were missing a critical ""not"".)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix incorrect backend allowlist in array_interoperability_test.,"We intended to only enable this test on CPU and GPU, but we were missing a critical ""not"".",2023-09-28T14:31:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17828
659,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] jax.random.split fails on CUDA)， 内容是 ( Description I'm unable to use `jax.random.split` in code that gets converted with `tf2jax`. Example:  Expected result:  Actual result:  Seems like tf2jax is spitting out `cu_threefry2x32` calls that don't correspond to valid TF ops.  What jax/jaxlib version are you using? jax 0.4.16, jaxlib 0.4.16+cuda12.cudnn89, tfnightly 2.15.0.dev20230927  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22.04  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] jax.random.split fails on CUDA," Description I'm unable to use `jax.random.split` in code that gets converted with `tf2jax`. Example:  Expected result:  Actual result:  Seems like tf2jax is spitting out `cu_threefry2x32` calls that don't correspond to valid TF ops.  What jax/jaxlib version are you using? jax 0.4.16, jaxlib 0.4.16+cuda12.cudnn89, tfnightly 2.15.0.dev20230927  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22.04  NVIDIA GPU info ",2023-09-27T22:11:12Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17816,"Hi , I tried to reproduce this issue with the latest JAX version 0.5.0 on colab notebook.It works without any error now. Please find the gist for reference. Thank you."
1126,"以下是一个github上的jax下的一个issue, 标题是(Destruct objects owned by `WeakRefLRUCache::CacheEntry` out of band using `GlobalPyRefManager()`)， 内容是 (Destruct objects owned by `WeakRefLRUCache::CacheEntry` out of band using `GlobalPyRefManager()` This assumes less about whether the thread that destructs `CacheEntry` has GIL or not, which is difficult to reason about due to the `xla::LRUCache`'s use of `std::shared_ptr`. The following changes have been made in JAX to accommodate the behavior differences from direct destruction to GC: * Since `PyLoadedExecutable`s cached in `WeakRefLRUCache` are now destructed out of band, `PyClient::LiveExecutables()` calls `GlobalPyRefManager()>CollectGarbage()` to make the returned information accurate and up to date. * `test_jit_reference_dropping` has been updated to call `gc.collect()` before verifying the live executable counts since the destruction of executables owned by weak ref maps is now done out of band as part of `GlobalPyRefManager`'s GC.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Destruct objects owned by `WeakRefLRUCache::CacheEntry` out of band using `GlobalPyRefManager()`,"Destruct objects owned by `WeakRefLRUCache::CacheEntry` out of band using `GlobalPyRefManager()` This assumes less about whether the thread that destructs `CacheEntry` has GIL or not, which is difficult to reason about due to the `xla::LRUCache`'s use of `std::shared_ptr`. The following changes have been made in JAX to accommodate the behavior differences from direct destruction to GC: * Since `PyLoadedExecutable`s cached in `WeakRefLRUCache` are now destructed out of band, `PyClient::LiveExecutables()` calls `GlobalPyRefManager()>CollectGarbage()` to make the returned information accurate and up to date. * `test_jit_reference_dropping` has been updated to call `gc.collect()` before verifying the live executable counts since the destruction of executables owned by weak ref maps is now done out of band as part of `GlobalPyRefManager`'s GC.",2023-09-27T18:12:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17810
525,"以下是一个github上的jax下的一个issue, 标题是([export] Set the default export serialization version to 8.)， 内容是 ([export] Set the default export serialization version to 8. This version has been supported by XlaCallModule since July 21, 2023 and we are now past the forwardcompatibility window. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[export] Set the default export serialization version to 8.,"[export] Set the default export serialization version to 8. This version has been supported by XlaCallModule since July 21, 2023 and we are now past the forwardcompatibility window. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.mdnativeserializationversions",2023-09-26T06:59:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17782
288,"以下是一个github上的jax下的一个issue, 标题是([random] wrap_key_data accepts impl=key.dtype.impl)， 内容是 (Fixes CC(jax.random.wrap_key_data(impl=key.impl) problems))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[random] wrap_key_data accepts impl=key.dtype.impl,Fixes CC(jax.random.wrap_key_data(impl=key.impl) problems),2023-09-25T16:04:16Z,pull ready,closed,0,8,https://github.com/jax-ml/jax/issues/17768,"Could we instead expose a function, `jax.random.key_impl`, that takes a key array and returns a string that specifies the implementation (acceptable by `key` and `wrap_key_data`)?","Note that `impl` is not a member of the key array class, but rather a member of the keyspecific `dtype` class.","Oh, thanks for correcting me there. Adjusting: what do you think of introducing `key_impl` that accepts a dtype (maybe as well as a key array)? And then we hide the `.impl` attribute of the `.dtype`returned object too, as well as any other attributes that diverge from Numpy's dtypes! The key dtype could then look more like other dtypes.",Are you proposing something like this for the new API? ,"Not quite. What I'm suggesting we consider is keeping `wrap_key_data` as is, and then introducing a new function `key_impl`. The `key_impl` function takes a dtype (maybe also an array, for convenience) and returns a string specifying the implementation. The string, when passed back to `wrap_key_data`, closes the round trip:  Separately, I'm suggesting we hide `.impl` from the `k.dtype` object's public attributes. What do you think of that?","> ... returns a string specifying the implementation. The string, when passed back to wrap_key_data, closes the round trip: OK, in that case it sounds like we'll need `PRNGKeyImpl` to have some kind of registry system, whereby when it is created it will register its name with some dictionary that maps strings to impls that can then be used by `resolve_prng_impl`. We'll have to figure out what to do if someone tries to register a duplicate impl name: probably the best thing would be to error in that case, because it could cause unintended consequences. An annoying part about that though is if you're developing iteratively in a notebook, then reexecuting a cell that defines your `PRNGKeyImpl` would lead to a duplicate registration error. Another option is that rather than returning a unique string identifier, we instead return the actual `PRNGImpl` object. What do you think?","Another observation here: `jax.random.default_prng_impl()` returns a `PRNGImpl` object, not a string. I've found a case where a user is doing something like:  which seems like it should work, but it doesn't for the reasons discussed above. If we decide to go with the ""impls are only referred to by their string name"" route, we'll have to figure out what to do with `jax.random.default_prng_impl()`.",Replaced by CC(identify PRNG schemes on key arrays; recognize them in key constructors)
469,"以下是一个github上的jax下的一个issue, 标题是(jax.random.key(0).itemsize crashes)， 内容是 ( Description I would expect the following to work, as it works for numpy arrays.   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,jax.random.key(0).itemsize crashes," Description I would expect the following to work, as it works for numpy arrays.   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-09-25T14:06:00Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/17761,Thanks for the report! CC([random] add itemsize property to custom PRNG) should fix it.
566,"以下是一个github上的jax下的一个issue, 标题是(jax.random.wrap_key_data(impl=key.impl) problems)， 内容是 ( Description Problem: I would expect the following to work, in order to programmatically wrap/unwrap keys, but it does not  as it fails with error   What jax/jaxlib version are you using? jax==master (today, 25 September) jaxlib==0.4.16  Which accelerator(s) are you using? CPU  Additional system info MacOs  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.random.wrap_key_data(impl=key.impl) problems," Description Problem: I would expect the following to work, in order to programmatically wrap/unwrap keys, but it does not  as it fails with error   What jax/jaxlib version are you using? jax==master (today, 25 September) jaxlib==0.4.16  Which accelerator(s) are you using? CPU  Additional system info MacOs  NVIDIA GPU info _No response_",2023-09-25T12:08:10Z,bug,closed,0,10,https://github.com/jax-ml/jax/issues/17758,"Hi, thanks for the report! The `impl` argument to `wrap_random_key` must be a string, and `key.impl` is not a string. I suppose we could allow the `impl` argument to be a private `Impl` object, but this is an internal class that is not documented or easily userconstructable, so that might add some confusion. What do you think would be best?","My need is to be able to unwrap and rewrap arbitrary keys programmatically (to be able to communicate them using `mpi4jax`, which can only handle primitive/standard dtypes). I was thinking of doing something like  For the simple fact that they have the same name (`impl`) I would expect this to work.  I imagine it would not be very hard for you to support, adding some logic to `resolve_prng_impl` such as:  If you do not think this is a clean solution (though, it seems to me it makes sense? why am I obliged to pass a string?) I think you should ensure that a PRNG implementation knows/contains its 'string' specification. For example, right now `impl.tag` returns something that cannot be roundtripped as an implementation specification `In [3]: a.impl.tag Out[3]: 'fry'`. Simply ensuring that `impl.tag` or another field has the correct key, and properly documenting this, would make it easier to work with. However, this latter approach is IMO more convoluted, as you're asking a value in a dictionary to know the key with which it is stored in a dictionary, and therefore it seems to me that the first approach, simply accepting a `PRNGImpl` as an implementation, is much cleaner. After all, the string to `PRNGImpl` is just some sort of _input sanitisation_ that you are running because you don't expect users to construct those objects themselves, but if they do... is it a problem?","Thanks – I agree that that's a clean solution at runtime, but people love their types. I'm not sure how to annotate that meaningfully without either (1) making the type `Any`, (2) exposing the currently private `PRNGImpl` type which we'd prefer not be in the public API, or (3) keeping it asis and requiring you to write ` type: ignore` when you write the code you suggest.  What do you think?","Well, I would like anything you define to be semipublic and semidocumented such that we don't have to change our code often.  If you do not want to expose the `PRNGImpl` type, there might be an alternative. I just noticed that `tag` is used to construct the `hash` of the `PRNGImpl` object, so it already must be 'correctly' defined. Can't you instead change the (semipublic, `jax.extend`) API to register a new `PRNGImplementation` to register it under the `tag` name? Then, the fact that the implementations are stored in a dictionary is just an implementation detail. In short, I'm suggesting adding a function   instead of exposing `PRNG_IMPLS` in `jax.extend.random` you could expose this function (as well as use it internally whenever you register your implementations).","If that's not acceptable, of course a 'misannotation' of the type as you proposed in (3), if documented/tested, would also be okay.","Stepping back – we originally did not plan to expose `wrap_random_key`, because we expect usage of it to be relatively rare in the eventual steady state. Why do you need to unwrap and rewrap keys? Can you not change your package to work directly with the newstyle keys?","To get `mpi` working, the bare minimum we need is to get the pointer to the data and the 'data size' .  I could try to just treat an extended data type as a `uint` array of length `dtype.itemsize` as well...",Thanks for the feedback and discussion – I think CC([random] wrap_key_data accepts impl=key.dtype.impl) should address this.,"I commented on CC([random] wrap_key_data accepts impl=key.dtype.impl) about the possibility of exposing a function, `jax.random.key_impl` that supports the round trip. Separately, we could look into hiding the `impl` attribute, as well as any others that aren't shared with other dtypes (i.e. Numpy's). > If you do not want to expose the `PRNGImpl` type, there might be an alternative. We expose the `PRNGImpl` type via `jax.extend.random`, but not through the main jax API.",I believe CC(identify PRNG schemes on key arrays; recognize them in key constructors) (which replaced CC([random] wrap_key_data accepts impl=key.dtype.impl)) closes this. Please reopen if you disagree!
1191,"以下是一个github上的jax下的一个issue, 标题是(Class 'function' is not a valid JAX type with Custom VJP)， 内容是 ( Description Hi JAX team,  I am adapting this tutorial to define a custom vjp for the fixed point iteration below. I understand the changes you made in the past, and I believe I marked my `func` and `integrator` arguments (both functions) as nondiff as it should be; and I left `B0`, `z0`, and `rhs` as 'diff' arguments eventhough the only derivative I am interested into is the one wrt `rhs`.   I keep getting this error.   I should note that the error was a bit different before I put together this MWE. Rather than `function` not being a valid JAX type, it used to say   I believe this issue is similar to CC(New `TypeError:  is not a valid JAX type`  exception in JAX 0.2.0), although that one mostly involved `jit`, rather than `vjp`. Please, how to overcome this issue ? Thanks.  What jax/jaxlib version are you using? jax 0.4.14; jaxlib 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Typical Colab runtime  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Class 'function' is not a valid JAX type with Custom VJP," Description Hi JAX team,  I am adapting this tutorial to define a custom vjp for the fixed point iteration below. I understand the changes you made in the past, and I believe I marked my `func` and `integrator` arguments (both functions) as nondiff as it should be; and I left `B0`, `z0`, and `rhs` as 'diff' arguments eventhough the only derivative I am interested into is the one wrt `rhs`.   I keep getting this error.   I should note that the error was a bit different before I put together this MWE. Rather than `function` not being a valid JAX type, it used to say   I believe this issue is similar to CC(New `TypeError:  is not a valid JAX type`  exception in JAX 0.2.0), although that one mostly involved `jit`, rather than `vjp`. Please, how to overcome this issue ? Thanks.  What jax/jaxlib version are you using? jax 0.4.14; jaxlib 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Typical Colab runtime  NVIDIA GPU info ",2023-09-24T08:17:36Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/17753,"It looks like the problem is this line:  You are passing a tuple containing `(z0, rhs, integrator, B_star, v)` as the `B0` parameter of `fixed_point_finder`, which initializes the `carry` of the loop. Then in your function, you seem to assume here that `B0` is an array rather than a tuple of objects of mixed type:  I think you need to take another look at the code and make sure you are calling the functions in the way you expected them to be called.","Hi , Thanks for your help. There was a problem with the code where you mentioned. The rewrite below should have fixed any such issues. But I still get the same error. I guess the bigger question is how to debug something like this. Is there any way to make the error message a bit more explicit, or maybe point to the line where the error actually happens ?  ","Thanks for adding the more complete code. The issue is that your `fwd` function should not be returning nondifferentiable arguments in its result. If you update your functions this way, it should work:  You can find more information on `custom_vjp` with `nondiff_argnums` here: https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.htmljaxcustomvjpwithnondiffargnums Regarding debugging... I'm not sure what to suggest, besides doublechecking that you're passing arguments correctly in `custom_vjp`. The error made me think that you were passing a python function where an array was expected, and that turned out to be the fix.","Wow, it worked! I completely missed that part of the tutorial. Apart from suggesting a more targeted error message for less sharp and experienced eyes like mine, I don't see what can be improved here. :) Thanks again for the debugging tips."
1305,"以下是一个github上的jax下的一个issue, 标题是(XlaRuntimeError: INTERNAL: custom_partitioner: NameError: name 'fft' is not defined)， 内容是 ( Description This issue is loosely related to CC(sharded fft unnecessarily loads entire array) and CC(Sharded FFT with JIT gives incorrect results on GPU since 0.4.9) in that I am trying to run a threedimensional FFT on a sharded threedimensional grid. However, the bug(?) can be demonstrated from the code in the documentation on `custom_partitioning`. The gist of the bug is that `fft` has to be explicitly imported via `from jax.numpy.fft import fft` when using the `custom_partitioning` example from the documentation. It is not enough to do `import jax.numpy as jnp` and then calling `jnp.fft.fft`. Code that produces the error:  Running this script gives the error:  By uncommenting the line `from jax.numpy.fft import fft` the error disappears. The code above is slightly altered from the code in the documentation. The changes are:  No import of `regex`.  Adding `import jax.numpy as jnp`, and replacing `fft` > `jnp.fft.fft`.  Emulating 8 devices.  No output of the arrays nor checking of the HLO as done in the documentation. I thou)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XlaRuntimeError: INTERNAL: custom_partitioner: NameError: name 'fft' is not defined," Description This issue is loosely related to CC(sharded fft unnecessarily loads entire array) and CC(Sharded FFT with JIT gives incorrect results on GPU since 0.4.9) in that I am trying to run a threedimensional FFT on a sharded threedimensional grid. However, the bug(?) can be demonstrated from the code in the documentation on `custom_partitioning`. The gist of the bug is that `fft` has to be explicitly imported via `from jax.numpy.fft import fft` when using the `custom_partitioning` example from the documentation. It is not enough to do `import jax.numpy as jnp` and then calling `jnp.fft.fft`. Code that produces the error:  Running this script gives the error:  By uncommenting the line `from jax.numpy.fft import fft` the error disappears. The code above is slightly altered from the code in the documentation. The changes are:  No import of `regex`.  Adding `import jax.numpy as jnp`, and replacing `fft` > `jnp.fft.fft`.  Emulating 8 devices.  No output of the arrays nor checking of the HLO as done in the documentation. I thou",2023-09-22T13:11:30Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17735,"Hi, I'm not sure I understand this issue and I never came across this in my FFT tests. Isn't the issue just that in this code block:  you are specifying a `fft` function/variable that is nowhere defined? If you would instead specify `mesh, jnp.fft.fft, ...` (so the same function you call inside `my_fft`) then I think the code should work as expected.","> I am trying to run a threedimensional FFT on a sharded threedimensional grid You might already know this, but keep in mind that the example code in jax.experimental.custom_partitioning runs a 1DFFT on a threedimensional grid. For a 3dFFT (`np.fft.fftn`) you would then afterwards need to run another FFT along the other two axis.","Oh, whoopie, you are absolutely right! I was so focused on the `my_fft`part that I overlooked the rest of the example. Thanks a lot! :)"
798,"以下是一个github上的jax下的一个issue, 标题是(XLA Check failed: common_utilization <= producer_output_utilization)， 内容是 ( Description When trying to run a longer algorithm, the execution fails with an error message without a more precise indication of where in the code the issue occurred:  This error only occurs in some use cases of the algorithm, and slightly changing parameters such as iterations and batch size sometimes permit it.   What jax/jaxlib version are you using? jax v0.4.9, jaxlib v0.4.9+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info python 3.8  NVIDIA GPU info  I am running the code with disabled parallel compilation: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XLA Check failed: common_utilization <= producer_output_utilization," Description When trying to run a longer algorithm, the execution fails with an error message without a more precise indication of where in the code the issue occurred:  This error only occurs in some use cases of the algorithm, and slightly changing parameters such as iterations and batch size sometimes permit it.   What jax/jaxlib version are you using? jax v0.4.9, jaxlib v0.4.9+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info python 3.8  NVIDIA GPU info  I am running the code with disabled parallel compilation: ",2023-09-22T06:22:33Z,bug XLA needs info GPU,open,0,9,https://github.com/jax-ml/jax/issues/17730,"Well, that sounds like an XLA bug. First, can you try with the latest jaxlib release (0.4.16)? The bug may already be fixed, so this is the first thing to try. You will need to update your Python version to 3.9 or newer to do this. If that doesn't work, can you please provide instructions to reproduce? If it's hard to do that, one way is to provide an HLO dump from XLA, which you can get by setting `XLA_FLAGS=xla_dump_to=/somewhere` and `JAX_COMPILER_DETAILED_LOGGING_MIN_OPS=0`, running your script, and zip up and attach the output of `/somewhere` to this issue.",Any updates? Can you share instructions to reproduce?,"I upgraded the jax and jaxlib versions, but the error persists. Unfortunately, I could not track the error to a specific part of the code.  However, I did the steps you described and attached the dump. xla_dump_part_1.tar.gz xla_dump_part_2.tar.gz","Hi Peter Hawkins, Do you have any updates on this issue? Best Sebastien","I've run into the same error  Description  The code in question is quite heavy in integer arithmetic, which may be part of the problem. You can find it here xladump.tar.gz  Jax version Jax/Jaxlib: `jax0.4.20 jaxlib0.4.20+cuda12.cudnn89`  System info Using GPU on an RTX 4090. My code runs successfully on CPU. Python 3.11.4, installed through anaconda ",Same problem here. Running on GPU. Any solutions ? Depends on the hyper parameters. ,EDIT: Originally thought I had the same problem but it looks like I'm failing a different XLA check so opened a separate issue. https://github.com/google/jax/issues/20024,Any updates on this?  Running into a similar error on A6000 GPU: F external/xla/xla/service/gpu/model/gpu_performance_model.cc:540] Check failed: common_utilization ()     @     0x7fd84a1e88e7  xla::HloPassPipeline::Run()     @     0x7fd84560ec71  xla::HloPassInterface::Run()     @     0x7fd8456245dd  xla::gpu::GpuCompiler::OptimizeHloModule()     @     0x7fd845628e61  xla::gpu::GpuCompiler::RunHloPasses()     @     0x7fd8455473a9  xla::Service::BuildExecutable()     @     0x7fd8453012ad  xla::LocalService::CompileExecutables()     @     0x7fd8452fbf82  xla::LocalClient::Compile()     @     0x7fd8452bad7c  xla::PjRtStreamExecutorClient::Compile()     @     0x7fd845296c9f  xla::StreamExecutorGpuClient::Compile()     @     0x7fd8452ce00a  xla::PjRtStreamExecutorClient::Compile()     @     0x7fd8451e866f  xla::ifrt::PjRtLoadedExecutable::Create()     @     0x7fd8451ded14  xla::ifrt::PjRtCompiler::Compile()     @     0x7fd844763a52  xla::PyClient::Compile()     @     0x7fd8444950b3  pybind11::detail::argument_loader::call_impl()     @     0x7fd844495560  pybind11::cpp_function::initialize()::{lambda() CC(Undefined name: from ..core import JaxTuple)}::_FUN()     @     0x7fd84444a768  pybind11::cpp_function::dispatcher()     @           0x525d17  cfunction_call,"Same error with `jax` and `jaxlib` version 0.4.28. I found that replacing one line in my code  with this  fixes the error. EDIT: Actually the error is intermittent, it sometimes appears even with the ""fix""."
501,"以下是一个github上的jax下的一个issue, 标题是(If an input to `jnp.asarray` is a numpy array, then convert it to a jax.Array via device_put to avoid a copy.)， 内容是 (If an input to `jnp.asarray` is a numpy array, then convert it to a jax.Array via device_put to avoid a copy. Do a similar thing for jax.Array too if dtypes match. Fixes https://github.com/google/jax/issues/17702)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"If an input to `jnp.asarray` is a numpy array, then convert it to a jax.Array via device_put to avoid a copy.","If an input to `jnp.asarray` is a numpy array, then convert it to a jax.Array via device_put to avoid a copy. Do a similar thing for jax.Array too if dtypes match. Fixes https://github.com/google/jax/issues/17702",2023-09-21T21:42:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17721
551,"以下是一个github上的jax下的一个issue, 标题是(PyInstaller package jax error)， 内容是 ( Description My jax code like this, when I use PyInstaller package it, it will be ok.   But when I run the packaged code ,it return some error, how can I solve it?   What jax/jaxlib version are you using? jax v0.4.13 jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python=3.10.12 Ubuntu 22.04  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,PyInstaller package jax error," Description My jax code like this, when I use PyInstaller package it, it will be ok.   But when I run the packaged code ,it return some error, how can I solve it?   What jax/jaxlib version are you using? jax v0.4.13 jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python=3.10.12 Ubuntu 22.04  NVIDIA GPU info ",2023-09-21T13:21:12Z,bug contributions welcome P3 (no schedule),open,0,8,https://github.com/jax-ml/jax/issues/17705,"This sounds like a broken installation. For example, I can imagine this would happen if the MLIR initialization did not run JAX's site initialization (https://github.com/google/jax/blob/main/jaxlib/mlir/_mlir_libs/_site_initialize_0.cc), which might happen if for some reason that module could not be found. I don't think this is a bug in JAX: JAX when installed as intended works fine. This sounds like PyInstaller breaks the JAX installation when it packages up an app. PRs welcome, if there's something we can do on the JAX end to make PyInstaller work, but I don't think there's an action item for us here.","> This sounds like a broken installation. For example, I can imagine this would happen if the MLIR initialization did not run JAX's site initialization (https://github.com/google/jax/blob/main/jaxlib/mlir/_mlir_libs/_site_initialize_0.cc), which might happen if for some reason that module could not be found. >  > I don't think this is a bug in JAX: JAX when installed as intended works fine. This sounds like PyInstaller breaks the JAX installation when it packages up an app. PRs welcome, if there's something we can do on the JAX end to make PyInstaller work, but I don't think there's an action item for us here. Thank you for your reply. I installed jax through pip. I would also like to ask if there is any good solution to this problem. I look forward to your reply."," The problem is likely not your JAX installation itself, it's that pyinstaller doesn't successfully package up JAX into a package. I have no knowledge of pyinstaller and I'm not sure what to suggest. Contributions welcome!","Hey , did you find a solution?",  I was able to get around this by using `py2app`,">  I was able to get around this by using `py2app` I commented out the MLIR detection code in the error reporting part, and the program can still run normally.  Although it solves the problem, I don't think it's a good solution. https://github.com/google/jax/blob/2be6019f1c99b234b91bf736578cd3d6886a6f18/jax/_src/interpreters/mlir.pyL823C5L823C5   823839",lol fair enough :) with `py2app` I could get JAX to run but I am having issues getting `py2app` to find `jaxmetal`. Looks like neither solution is perfect so far. But thanks for sharing ! Hope you have a good week!,"Realised that `py2app` is only really viable for Mac users, I also tested pyinstaller on linux and found that commenting out that verification code in the mlir file to work as well lol (tested for CUDA backend)"
620,"以下是一个github上的jax下的一个issue, 标题是(jnp.asarray making copies in latest jax)， 内容是 ( Description  In version 0.4.16, `jnp.asarray` now seems to be making copies when called on numpy arrays. In earlier versions, it would just reuse the underlying data and no (little) extra memory would be consumed. Is this intended?  What jax/jaxlib version are you using? jax, jaxlib 0.4.16  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jnp.asarray making copies in latest jax," Description  In version 0.4.16, `jnp.asarray` now seems to be making copies when called on numpy arrays. In earlier versions, it would just reuse the underlying data and no (little) extra memory would be consumed. Is this intended?  What jax/jaxlib version are you using? jax, jaxlib 0.4.16  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-09-21T09:52:36Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/17702,"Yash, is it possible that this occurs because of the changes to the primitive dispatch path and the fact that we now copy for trivial computations?","Yeah, that sounds correct to me. We removed the trivial dispatch path from JAX because it was a preominstaging optimization which doesn't have much use postomnistaging. Also it helped speed up the dispatch path by 100x. Is the copy causing problems?",I think not copying (while it has only ever been best effort) is an important optimization.,https://github.com/google/jax/pull/17721 should fix,Thanks guys! before the next release is there another way of converting without copies?,"If you want to convert a numpy array to a jax array without copying in version 0.4.16, you can use `y = jax.device_put(x)` This will only be avoid copies if using a CPU device, and if your numpy array's byte alignment is compatible with XLA."
413,"以下是一个github上的jax下的一个issue, 标题是(Copying a pmap replicated array gives an error.)， 内容是 ( Description  gives an error   What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Copying a pmap replicated array gives an error., Description  gives an error   What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_,2023-09-20T22:35:57Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/17690
389,"以下是一个github上的jax下的一个issue, 标题是(Deprecate non-array inputs to several jax.numpy functions)， 内容是 (Some stragglers from CC(Regularize jax.numpy API) I've modified `check_arraylike` to emit warnings so we can more gracefully deprecate the old behavior.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Deprecate non-array inputs to several jax.numpy functions,Some stragglers from CC(Regularize jax.numpy API) I've modified `check_arraylike` to emit warnings so we can more gracefully deprecate the old behavior.,2023-09-20T21:02:42Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17687
490,"以下是一个github上的jax下的一个issue, 标题是(Avoid references to symbols removed in numpy 2.0)， 内容是 (NumPy 2.0 will remove `np.int_`, `np.float_`, and `np.complex_` (see https://github.com/numpy/numpy/issues/24743) Some of these have already been removed upstream, which is causing failures in our upstreamnightly tests ( CC(⚠️ Nightly upstreamdev CI failed ⚠️)).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Avoid references to symbols removed in numpy 2.0,"NumPy 2.0 will remove `np.int_`, `np.float_`, and `np.complex_` (see https://github.com/numpy/numpy/issues/24743) Some of these have already been removed upstream, which is causing failures in our upstreamnightly tests ( CC(⚠️ Nightly upstreamdev CI failed ⚠️)).",2023-09-19T18:35:02Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17668
395,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Tests are failing)， 内容是 ( Description  Output:   What jax/jaxlib version are you using? jax v0.4.16 jaxlib v0.4.16  Which accelerator(s) are you using? CPU  Additional system info OSX  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Tests are failing, Description  Output:   What jax/jaxlib version are you using? jax v0.4.16 jaxlib v0.4.16  Which accelerator(s) are you using? CPU  Additional system info OSX  NVIDIA GPU info _No response_,2023-09-19T12:51:37Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/17660,"Having a similar issue when trying to follow the README here: https://github.com/google/jax/tree/main/jax/experimental/jax2tf/examples/serving Getting this error running the python command `TypeError: call_module() got an unexpected keyword argument 'function_list'`  Full output:  Using same jax version, CPU, Macbook M2","Just downgraded jax to v0.4.12/13/14 and it is passing on all of them (I started at a way too old version oops), but issue starts from `jax[cpu]==v0.4.15.dev20230919` ","In general, `jax2tf` tests require the nightly tensorflow release (`pip install tfnightly`). They often get out of sync with the tensorflow release.","Thanks  tried that, same result. ",`call_module` accepts a `function_list` argument as of four months ago: https://github.com/tensorflow/tensorflow/blame/a90eb068e805fbe39ccfd7f4bfc2e33dd8a592a0/tensorflow/compiler/tf2xla/python/xla.pyL637 Can you double check that the Python executable you're running is picking up the nightly tensorflow listed by `pip`? For example: ,"Thanks, that was it. The problem is that `tensorflow_serving_api` depends on `tensorflow`, so that gets installed along with `tfnightly` and is found first. A workaround is to do this instead:  Maybe the jax2tf serving guide should recommend that instead?"
1311,"以下是一个github上的jax下的一个issue, 标题是(jax 0.4.6 does not provide the extra 'cuda12_pip' | No GPU/TPU found, falling back to CPU)， 内容是 ( Description I came to Jax's repository looking for a solution because I was facing an issue with another repository. Jax was one of the dependencies of that repository. No matter what I try or do, I get the following error.  And at the end, jax with cuda is not being installed. I have tried the workarounds provided in the issues section of the repository but no success.  Edit 1: I made a mistake. Without carefully reading the installation guide, I installed CUDA V12.2 but as the installation guide says:  so I removed the CUDA V12.2 and installed V12.0. I also downloaded `cudnn` files and placed them in `lib`, `include` and `bin` folder but still the same output. Edit 2: I made a change to `pip` command `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and remove `_pip` after `cuda12` and the installed completed though not successfully. Instead, the v0.4.14 was installed in one go rather than cycling though the older versions and trying to install them. Edit 3: I used t)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"jax 0.4.6 does not provide the extra 'cuda12_pip' | No GPU/TPU found, falling back to CPU"," Description I came to Jax's repository looking for a solution because I was facing an issue with another repository. Jax was one of the dependencies of that repository. No matter what I try or do, I get the following error.  And at the end, jax with cuda is not being installed. I have tried the workarounds provided in the issues section of the repository but no success.  Edit 1: I made a mistake. Without carefully reading the installation guide, I installed CUDA V12.2 but as the installation guide says:  so I removed the CUDA V12.2 and installed V12.0. I also downloaded `cudnn` files and placed them in `lib`, `include` and `bin` folder but still the same output. Edit 2: I made a change to `pip` command `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and remove `_pip` after `cuda12` and the installed completed though not successfully. Instead, the v0.4.14 was installed in one go rather than cycling though the older versions and trying to install them. Edit 3: I used t",2023-09-17T10:12:45Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17634,"JAX does not provide CUDAenabled wheels on Windows (https://github.com/google/jaxinstallation). Closing, since this is working as intended: pip will not succeed because there aren't any wheels to install.",Thanks  for clarifying. I opened another issue due to the error I'm receiving while building jax (cuda enabled) for Windows. https://github.com/google/jax/issues/17635. So does it mean that there is no official support for the Cudaenabled builds for Windows?,"Correct, there is no support from us (the JAX core team) for CUDA enabled builds on Windows. We release CPUonly wheels on Windows, and even those are best effort. But we welcome PRs to fix things!"
747,"以下是一个github上的jax下的一个issue, 标题是(`jaxlib` compilation error on 0.4.11)， 内容是 ( Description Following Apple's installation tutorial and trying to use `jaxlib` 0.4.11 on an M2 machine I am unable to build `jax` from source.  Env setup & download JAX  Then build  fails with output  This seems to contain a warnings   and a linking error:   What jax/jaxlib version are you using? jaxlib 0.4.11  Which accelerator(s) are you using? Apple M2 Pro (MacMini),   Additional system info Python 3.11.5 (homebrew), MacOS 13.5.2, Apple M2 Pro chip (MacMini), gcc (Homebrew GCC 13.1.0) 13.1.0  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jaxlib` compilation error on 0.4.11," Description Following Apple's installation tutorial and trying to use `jaxlib` 0.4.11 on an M2 machine I am unable to build `jax` from source.  Env setup & download JAX  Then build  fails with output  This seems to contain a warnings   and a linking error:   What jax/jaxlib version are you using? jaxlib 0.4.11  Which accelerator(s) are you using? Apple M2 Pro (MacMini),   Additional system info Python 3.11.5 (homebrew), MacOS 13.5.2, Apple M2 Pro chip (MacMini), gcc (Homebrew GCC 13.1.0) 13.1.0  NVIDIA GPU info _No response_",2023-09-16T09:42:07Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17630," I was struggling a bit with the same issues. However, jaxmetal 0.0.4 was just released (https://pypi.org/project/jaxmetal/0.0.4/). This seems to be installing a prebuilt version of jaxlibv0.4.11. ","I strongly suspect this is because of libraries in your homebrew setup. Note that this bug does not reproduce in our (nonhomebrew) CI. Can you try without homebrew? Closing, since I suspect you should now use a prebuilt jaxlib wheel, as  suggests, because `jaxmetal` is compatible with a released jaxlib (admittedly a slightly older one)."
1184,"以下是一个github上的jax下的一个issue, 标题是(Unexpected exception from jax.lax.fori_loop)， 内容是 ( Description There appears to be an issue with `jax.lax.fori_loop`. When I try to use this function, I get the following exception: ""the input carry component loop_carry[1][3].positions has type float32[0] but the corresponding output carry component has type float32[10,3], so the shapes do not match"" The code producing this error is the following:  A similar exception is being thrown for velocities and constraints. In this function, `controller` extends `equinox.Module`, and `BouncingBall` is a `flax.struct.dataclass` that wraps a Brax `System` with some other arrays for state information at different timesteps. When I disable jit compiling using   the function runs without issues, but when it is JIT compiled it throws these exceptions.  What jax/jaxlib version are you using? jax v0.4.14, jaxlib 0.4.14  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.12, Ubuntu 22.04, Intel Xeon E31230 V2  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unexpected exception from jax.lax.fori_loop," Description There appears to be an issue with `jax.lax.fori_loop`. When I try to use this function, I get the following exception: ""the input carry component loop_carry[1][3].positions has type float32[0] but the corresponding output carry component has type float32[10,3], so the shapes do not match"" The code producing this error is the following:  A similar exception is being thrown for velocities and constraints. In this function, `controller` extends `equinox.Module`, and `BouncingBall` is a `flax.struct.dataclass` that wraps a Brax `System` with some other arrays for state information at different timesteps. When I disable jit compiling using   the function runs without issues, but when it is JIT compiled it throws these exceptions.  What jax/jaxlib version are you using? jax v0.4.14, jaxlib 0.4.14  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.12, Ubuntu 22.04, Intel Xeon E31230 V2  NVIDIA GPU info _No response_",2023-09-15T20:16:25Z,bug question,closed,0,2,https://github.com/jax-ml/jax/issues/17629,"When running `fori_loop` under `jit`, the shapes of input arrays must match the shapes of output arrays. From the error message:  It looks like `loop_carry[1][3]` is the variable you call `ball`, and on input `ball.positions` has shape `(0,)` and on output `ball.positions` has shape `(10, 3)`. The way to fix this is to ensure that the input arrays have the same shape as the output arrays. I would look for where you're initializing `ball` in your code, and make sure it's initialized with the same shape arrays as you expect on output.",Thanks ! I hadn't thought to look at ball.positions. I changed the array in `BouncingBall` to have a preallocated size and now it works.
597,"以下是一个github上的jax下的一个issue, 标题是(No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.))， 内容是 ( Description I am trying to run this code on my college GPU with SLURM. Torch is running fine however JAX wont run.  !Screenshot 20230915 at 6 43 22 PM  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Fri Sep 15 18:44:40 2023        ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)", Description I am trying to run this code on my college GPU with SLURM. Torch is running fine however JAX wont run.  !Screenshot 20230915 at 6 43 22 PM  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Fri Sep 15 18:44:40 2023        ++  ++,2023-09-15T16:44:59Z,bug,closed,0,20,https://github.com/jax-ml/jax/issues/17624,How did you install JAX?,"I installed JAX using pip install jax. As pip install jax[cuda] does not work and says version mismatch with python, hwoever my python is on latest version already. I cannot downgrade either.","As your `nvidiasmi` shows the CUDA version is 12.0, you should use  to install JAX, according to the installation guide.",What happens when you did what  suggested?,"Thanks for the reply. Its still at the same output. I did the installation using  suggestions, it installed noramlly. But ""JAX is not running on a GPU."" !Screenshot 20230923 at 12 51 30 PM", You should first uninstall the old `jax` and `jaxlib` using: ,Hi  did you find the solution?," I did that and didn't work  Not yet, and hence moved to torch. Lot others were also struggling  https://twitter.com/s4nyam/status/1702727177144295929", Please don't move to PyTorch! JAX is much better than PyTorch. I can fix this for you!,"Based on the screenshot provided, it seems that you are utilizing a conda environment. To proceed, I recommend the following steps: 1. Create a new conda environment with Python version 3.11:  Replace `` with your desired environment name. 2. Install the necessary package with pip:  Please ensure to activate the newly created conda environment before executing the pip installation command. 3. Test JAX GPU Accessibility: Execute the following Python code to check if JAX can access the GPU:  This script prints a list of available devices. If the GPU is configured correctly, you should see `GpuDevice`."," I try your description. I give below error: ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflowcpu 2.12.0 requires keras=2.12.0, but you have keras 2.14.0 which is incompatible. tensorflowcpu 2.12.0 requires numpy=1.22, but you have numpy 1.26.1 which is incompatible. tensorflowcpu 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,=3.20.3, but you have protobuf 3.12.4 which is incompatible. tensorflowcpu 2.12.0 requires tensorboard=2.12, but you have tensorboard 2.8.0 which is incompatible. tensorflowcpu 2.12.0 requires tensorflowestimator=2.12.0, but you have tensorflowestimator 2.14.0 which is incompatible. torch 2.1.0 requires nvidiacublascu12==12.1.3.1; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacublascu12 12.3.2.9 which is incompatible. torch 2.1.0 requires nvidiacudacupticu12==12.1.105; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacudacupticu12 12.3.52 which is incompatible. torch 2.1.0 requires nvidiacudaruntimecu12==12.1.105; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacudaruntimecu12 12.3.52 which is incompatible. torch 2.1.0 requires nvidiacufftcu12==11.0.2.54; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacufftcu12 11.0.11.19 which is incompatible. torch 2.1.0 requires nvidiacusolvercu12==11.4.5.107; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacusolvercu12 11.5.3.52 which is incompatible. torch 2.1.0 requires nvidiacusparsecu12==12.1.0.106; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiacusparsecu12 12.1.3.153 which is incompatible. torch 2.1.0 requires nvidiancclcu12==2.18.1; platform_system == ""Linux"" and platform_machine == ""x86_64"", but you have nvidiancclcu12 2.19.3 which is incompatible. Successfully installed jax0.4.19 jaxlib0.4.19+cuda12.cudnn89 nvidiacublascu1212.3.2.9 nvidiacudacupticu1212.3.52 nvidiacudanvcccu1212.3.52 nvidiacudaruntimecu1212.3.52 nvidiacufftcu1211.0.11.19 nvidiacusolvercu1211.5.3.52 nvidiacusparsecu1212.1.3.153 nvidiancclcu122.19.3 jax.device() resuilt is ""[cuda(id=0)]"" Have you got any idea? Thanks your help","I apologies for the oversight. It appears that `GpuDevice` is renamed to `cuda` in recent JAX releases. Therefore, the output `[cuda(id=0)]` means JAX is working correctly on your machine. For the pip's dependency resolver error, it seems that you are trying to install JAX into an environment with Tensorflow and PyTorch installed. It appears that there are version mismatches between the dependencies of these three packages. You should install JAX into a clean environment, preferably created with venv. Remember to activate the newly create environment before executing the pip installation command."," thnaks your answer. I remove tensorflowcpu and pytorch, next i can install the jax. How to install optax with jax and gpu compatibility?","  > I remove tensorflowcpu and pytorch, next i can install the jax. You should completely delete the environment and recreate a new one to avoid any potential conflicts between the dependencies. > How to install optax with jax and gpu compatibility? You can just use:  Optax depends on JAX. If you install Optax in this way, it should work as expected, since you have already set up JAX successfully."," thanks your help. I have now other issue with jax. i created a new env, but i use cuda 12. 11/02/2023 18:42:19  WARNING  jax._src.xla_bridge    CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12020, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built.  Can I install jax 12010 version? I muss use the cuda 12010 version.","My solution in my project: Install cuda 11.8, because this cuda version compatiblity with pytorch, and jax. You can check the versions and install command: https://pytorch.org/getstarted/locally/ Create empty enviromet after install cuda 11.8. I use python 3.10 on Ubuntu `conda create n  python=3.10` Install torch: `pip3 install torch torchvision torchaudio indexurl https://download.pytorch.org/whl/cu118` Install jax with gpu. It's important, use jax with cuda 11, if you use 11.* conda version! `pip install U ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Install flax. This command install optax for me. `pip install flax` Good luck!","Hi, I also faced this problem: `No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)` when directly installing it with `pip install flax`.  My cuda version is 12.2, python version is 3.8 (this is requested). I tried different commands to install gpubased jax, like:  , and  , and   and  During installation, I would see a warning similar to `WARNING: jax 0.4.13 does not provide the extra 'cuda12local' `, but the installations would all succeed with a jax version of 0.4.13. However, when running: `python c ""import jax.numpy as jnp; jnp.ones((3,)); print('JAX installed successfully')"" `, I would always get something like:  May I have your suggestions on how to solve this problem? Thank you in advance for any help!! Look forward to your reply!"," In a clean environment (without any jax, flax, etc. installed), do: ","Thank you so much for your quick reply and help!!!   I'm really sorry that I made a very stupid mistake, that the cuda version is 10.1 based on `nvcc version`. In a clean env, I tried:  and get it installed successfully with:  But when trying to run some example codes, I got:  Thus, Jax still cannot make use of GPU. May I again have your suggestions on this problem? Thank you so much for considering this matter and your help!","Regarding the `jax[cuda10_pip]` error, see the discussion in CC(未找到相关数据)"
320,"以下是一个github上的jax下的一个issue, 标题是([Mosaic] Add support for specifying estimated costs for Mosaic kernels)， 内容是 ([Mosaic] Add support for specifying estimated costs for Mosaic kernels)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Mosaic] Add support for specifying estimated costs for Mosaic kernels,[Mosaic] Add support for specifying estimated costs for Mosaic kernels,2023-09-13T10:58:02Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17588
1228,"以下是一个github上的jax下的一个issue, 标题是(fast dispatch for functions over typed PRNG key arrays)， 内容是 (fast dispatch for functions over typed PRNG key arrays Before this change, JAX could dispatch compiled functions over newstyle (typed) RNG key arrays, but it would always do so off of the fast (C++based) dispatch path. In other words, switching from oldstyle `uint32` RNG keys to newstyle keys would regress dispatch times. With this change, dispatch happens on the fast path again and performance regressions ought to be minimal. We currently maintain only one pytree registry, for all registered pytree node types. We want RNG key arrays to also be treated as pytree leaves everywhere *except* during dispatch. In other words: we want operations on (typed) RNG key arrays to appear in Jaxpr, but we want to unravel those arrays into their underlying `uint32` arrays only during dispatch. To do this, we add a new internal pytree registry that dispatch respects uniquely. This registry includes all items in the default registry, but also the RNG key array type. Coauthoredby: Matthew Johnson )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fast dispatch for functions over typed PRNG key arrays,"fast dispatch for functions over typed PRNG key arrays Before this change, JAX could dispatch compiled functions over newstyle (typed) RNG key arrays, but it would always do so off of the fast (C++based) dispatch path. In other words, switching from oldstyle `uint32` RNG keys to newstyle keys would regress dispatch times. With this change, dispatch happens on the fast path again and performance regressions ought to be minimal. We currently maintain only one pytree registry, for all registered pytree node types. We want RNG key arrays to also be treated as pytree leaves everywhere *except* during dispatch. In other words: we want operations on (typed) RNG key arrays to appear in Jaxpr, but we want to unravel those arrays into their underlying `uint32` arrays only during dispatch. To do this, we add a new internal pytree registry that dispatch respects uniquely. This registry includes all items in the default registry, but also the RNG key array type. Coauthoredby: Matthew Johnson ",2023-09-12T21:57:05Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17570
525,"以下是一个github上的jax下的一个issue, 标题是(JAX does not load bundled CUDNN)， 内容是 ( Description Somehow JAX does not use the bundled cudnn. How to reproduce:  Error:  The nvidia driver is  510.47.3, which should support cuda 11.  What jax/jaxlib version are you using? 0.4.13  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 20.04  NVIDIA GPU info NVIDIA GeForce GTX 1080 Ti)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX does not load bundled CUDNN," Description Somehow JAX does not use the bundled cudnn. How to reproduce:  Error:  The nvidia driver is  510.47.3, which should support cuda 11.  What jax/jaxlib version are you using? 0.4.13  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 20.04  NVIDIA GPU info NVIDIA GeForce GTX 1080 Ti",2023-09-12T01:16:46Z,bug NVIDIA GPU,closed,0,6,https://github.com/jax-ml/jax/issues/17553,"I'm not 100% sure I interpret it correctly, but isn't 'Memory usage: 7995392 bytes free' implying that you only have ~8 MB of free memory on your GPU?","> I'm not 100% sure I interpret it correctly, but isn't 'Memory usage: 7995392 bytes free' implying that you only have ~8 MB of free memory on your GPU? When I try another GPU I get: `20230911 22:50:18.801369: E external/xla/xla/service/gpu/gpu_compiler.cc:1423] The CUDA linking API did not work. Please use XLA_FLAGS=xla_gpu_force_compilation_parallelism=1 to bypass it, but expect to get longer compilation time due to the lack of multithreading. Original error: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Input file '/tmp/tempfilegracee8fbd375348513060523048fe531.cubin' newer than toolkit (118 vs 116)`",This should be fixed with jax 0.4.20. JAX will prefer the pippackaged CUDA libraries if they are present.,"I just tried to reinstall 0.4.20 and I still have to set LD_LIBRARY_PATH to """" for it to work. Could there be a conflict with the changes in 0.4.17? > CUDA: JAX now verifies that the CUDA libraries it finds are at least as new as the CUDA libraries that JAX was built against. If older libraries are found, JAX raises an exception since that is preferable to mysterious failures and crashes. as without setting LD_LIBRARY_PATH I get  EDIT: apparently I also have to modify PATH to include the directory containing the 'ptxas"" executable installed by Jax as it's not picked up automatically.","> EDIT: apparently I also have to modify PATH to include the directory containing the 'ptxas"" executable installed by Jax as it's not picked up automatically This should actually be resolved by https://github.com/openxla/xla/pull/7884 for future releases!",We think this is fixed! Please comment if you're still having a problem.
1064,"以下是一个github上的jax下的一个issue, 标题是([Pallas] Subpar performance vs JAX (unexpected))， 内容是 ( Description In XLB library, I attempted to create an unrolled version of the equilibrium function using Pallas—a task that is typically not feasible in JAX. Normally, this approach results in a significant increase in computational speed (in CUDA/C++, due to reusing several computations and ignoring computatons that results in zero). However, I observed a 50% reduction in performance. Please see the repro below. While I acknowledge that my implementation may not fully leverage the capabilities of Pallas, I would be grateful if you could review it to determine whether the performance drop is due to an error on my part or if it is an expected outcome in this scenario.   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[Pallas] Subpar performance vs JAX (unexpected)," Description In XLB library, I attempted to create an unrolled version of the equilibrium function using Pallas—a task that is typically not feasible in JAX. Normally, this approach results in a significant increase in computational speed (in CUDA/C++, due to reusing several computations and ignoring computatons that results in zero). However, I observed a 50% reduction in performance. Please see the repro below. While I acknowledge that my implementation may not fully leverage the capabilities of Pallas, I would be grateful if you could review it to determine whether the performance drop is due to an error on my part or if it is an expected outcome in this scenario.   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info ",2023-09-11T20:40:17Z,bug performance NVIDIA GPU,closed,2,5,https://github.com/jax-ml/jax/issues/17545,"The problem comes from the grid_size, `(256, 256, 256)` is unfortunate. You should rather use smthg like `(4, 16, 4)`. It goes from `0.32x` to `80x` faster. CC:    Before `  After    Diff File ",Great  !! Very interesting. Now if I can install Pallas easily I should be able to implement it in the library 😆  https://github.com/google/jax/issues/18603issuecomment1885016966 Why the JAX time went up though?," JAX time didn't go up. One is in seconds (old), new is in ms. It's easier to read ;) ",I see. This is great news! For this function that does a large part of compute in our code we will be replacing it with Pallas in our ongoing major refactoring. This could bring our JAX backend's speed to within twice that of the fastest C++ codes—a substantial improvement from the previous 68 times slower rate (I should verify this though) Thanks a lot for the analysis!,> Great  !! Very interesting. Now if I can install Pallas easily I should be able to implement it in the library 😆  CC(Make Pallas/GPU easier to install) (comment) We have a public container last month that should work: docker pull ghcr.io/nvidia/jax:nightlypallas20231216
1301,"以下是一个github上的jax下的一个issue, 标题是(""None of the algorithms provided by cuDNN heuristics worked"" for Ampere NV GPUs)， 内容是 ( Description I'm training learned optimizers using Jax and a custom version of https://github.com/google/learned_optimization. I get the following warnings when training on Ampere GPUs (tested for RTX 3090 and A6000), however, no warning message appears when using an RTX2080ti or an RTX8000 GPU.  I'm listing this as an issue since the **training is much slower (2x or more) on ampere GPUs** than their predecessors, which should not be the case.   Unfortunately, I did not manage to extract a minimal reproducing example within two hours, so I have gone ahead and posted the issue anyway. Here is the complete stack trace for reference: ``` 20230907 03:07:24.380550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT                                                                                               ERROR:absl:Oryx not found! This library will still work but no summarywill be logged.                        gpu                                                                    /bth)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"""None of the algorithms provided by cuDNN heuristics worked"" for Ampere NV GPUs"," Description I'm training learned optimizers using Jax and a custom version of https://github.com/google/learned_optimization. I get the following warnings when training on Ampere GPUs (tested for RTX 3090 and A6000), however, no warning message appears when using an RTX2080ti or an RTX8000 GPU.  I'm listing this as an issue since the **training is much slower (2x or more) on ampere GPUs** than their predecessors, which should not be the case.   Unfortunately, I did not manage to extract a minimal reproducing example within two hours, so I have gone ahead and posted the issue anyway. Here is the complete stack trace for reference: ``` 20230907 03:07:24.380550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TFTRT Warning: Could not find TensorRT                                                                                               ERROR:absl:Oryx not found! This library will still work but no summarywill be logged.                        gpu                                                                    /bth",2023-09-08T19:45:23Z,bug XLA needs info NVIDIA GPU,open,0,7,https://github.com/jax-ml/jax/issues/17523,"Can you try with a `jaxlib` built from head? There was at least one recent XLA change (https://github.com/openxla/xla/commit/0ef9d092689e767a431c01b332d94b76d66866c9) that should be included in a head jaxlib that may help. (We'll probably make a new release this week, also.)","Just built jax & jaxlib from source and installed them, within the docker image mentioned above, using the following steps:   After the above steps I get  However, the same error appears again (with some small differences in the message due to batch size):   should I checkout to a particular dev branch before building or specify an XLA version (e.g., following https://jax.readthedocs.io/en/latest/developer.htmlbuildingjaxlibfromsourcewithamodifiedxlarepository)?","It looks like what you did was correct. In that case, we'd need a reproduction of the problem. One way you could give us that is to share an XLA HLO dump. Set the environment variable `XLA_FLAGS=xla_dump_to=/somewhere` and zip the contents of `/somewhere` and attach them. Note this in effect shares your model, which you may or may not want to do. Up to you.","Any chance you can grab that HLO dump, as requested above?", Here are the logs and a screenshot of the warning output when they were produced. This was run on an RTX 3090 GPU. Let me know if you need any more information. xla_logs.zip ,Hmm. I couldn't reproduce on A100. Does this still reproduce for you with `jax` and `jaxlib` 0.4.20?,I get an identical error with 0.4.20  
743,"以下是一个github上的jax下的一个issue, 标题是(Singular value decomposition JVP not implemented for full matrices/ workaround?)， 内容是 ( Description It seems that the Singular Value Decomposition JVP is not implemented for full matrices. The output of the function jax.linalg.svd is a tuple of three matrices (U, Sigma, Vh). Is there a plan to implement this feature or a work around? Appreciate any help. Reference:   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, MacOS 11.7.9, Jax 0.4.14, optax 0.1.7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Singular value decomposition JVP not implemented for full matrices/ workaround?," Description It seems that the Singular Value Decomposition JVP is not implemented for full matrices. The output of the function jax.linalg.svd is a tuple of three matrices (U, Sigma, Vh). Is there a plan to implement this feature or a work around? Appreciate any help. Reference:   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.5, MacOS 11.7.9, Jax 0.4.14, optax 0.1.7  NVIDIA GPU info _No response_",2023-09-08T16:16:28Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/17518,"Hi  thanks for the report! This is a longstanding known issue; the problem is that for full matrices, the gradients are not welldefined. You can read more in https://github.com/google/jax/issues/508 I'm going to close this as a duplicate of CC(Implement JVP for SVD when full_matrices=True). Thanks!"
1337,"以下是一个github上的jax下的一个issue, 标题是(JAX uses the first CUDA libraries found in `LD_LIBRARY_PATH` instead of the bundled CUDA libraries installed by pip)， 内容是 ( Description  Problem  Description Most CUDAenabled systems tend to have CUDA libraries' directory paths added to `LD_LIBRARY_PATH` automatically by a `~/.bashrc` script, or similar. When installed through the pip installation option with CUDA installed via pip, JAX doesn't use the CUDA libraries installed by pip. Instead, JAX uses whatever CUDA libraries it finds first when searching through the directories specified in the `LD_LIBRARY_PATH` variable. This is confusing, as when I use the pip installation option that also installs CUDA, I expect JAX to use the bundled CUDA instead of any other, possibly older and incompatible, CUDA version I have on the system. Notably, PyTorch with CUDA installation handles this case as expected, see details below.  Expected result  If JAX was installed using the pip installation option that also installs CUDA, then JAX should use the CUDA libraries that were installed by pip during the JAX installation, disregarding the value of the `LD_LIBRARY_PATH` variable.  If JAX was installed using the)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX uses the first CUDA libraries found in `LD_LIBRARY_PATH` instead of the bundled CUDA libraries installed by pip," Description  Problem  Description Most CUDAenabled systems tend to have CUDA libraries' directory paths added to `LD_LIBRARY_PATH` automatically by a `~/.bashrc` script, or similar. When installed through the pip installation option with CUDA installed via pip, JAX doesn't use the CUDA libraries installed by pip. Instead, JAX uses whatever CUDA libraries it finds first when searching through the directories specified in the `LD_LIBRARY_PATH` variable. This is confusing, as when I use the pip installation option that also installs CUDA, I expect JAX to use the bundled CUDA instead of any other, possibly older and incompatible, CUDA version I have on the system. Notably, PyTorch with CUDA installation handles this case as expected, see details below.  Expected result  If JAX was installed using the pip installation option that also installs CUDA, then JAX should use the CUDA libraries that were installed by pip during the JAX installation, disregarding the value of the `LD_LIBRARY_PATH` variable.  If JAX was installed using the",2023-09-07T16:37:40Z,bug,closed,1,6,https://github.com/jax-ml/jax/issues/17497,"I'm not sure I consider this a bug. If you're interested in why this happens: JAX sets a field called `RUNPATH` on its shared libraries, which specifies an additional search path for dependent shared libraries. Notably `RUNPATH` comes lower in the search priority order than the `LD_LIBRARY_PATH` environment variable. This is arguably a good thing: it allows users and system administrators to override the choice of libraries if they want. There is another field that can be set instead called `RPATH`, which also adds an additional search path, but does so at a *higher* priority than the `LD_LIBRARY_PATH` environment variable. I would guess (but have not personally confirmed) that is what PyTorch is setting. There is no mechanism to override an `RPATH`, even if you wanted to, bar tools like `patchelf`. JAX on CUDA can be installed in two modes: `cuda12_pip`, which installs the CUDA dependencies as `pip` packages, and `cuda12_local`, which does not and relies on the user having the right libraries installed. The `jaxlib` binaries we ship is the same in both cases. For `jaxlib` to be able to locate the `pip`installed NVIDIA libraries, `jaxlib` sets `RUNPATH` on its shared libraries to the relative location of the CUDA libraries when `pip`installed. If the `pip` packages aren't installed, this likely doesn't do much because the libraries won't be present: we'll fall back searching `LD_LIBRARY_PATH`. It would be possible to change `jax` to set `RPATH` instead of `RUNPATH`, but the catch would be that then if the CUDA `pip` packages were installed, then we would use them always and there would be no way to override that choice for development or by a system administrator. If you had the CUDA `pip` packages installed but needed to use another CUDA installation, you'd be forced to uninstall the CUDA pip packages. `RPATH` is also deprecated, according to some authorities: https://akkadia.org/drepper/dsohowto.pdf Is there a reason that it doesn't suffice simply not to set `LD_LIBRARY_PATH`? i.e., just don't override JAX's choice? That's the point of `LD_LIBRARY_PATH`  to override the app's choice!"," thank for the detailed explanation! This is insightful indeed. > Is there a reason that it doesn't suffice simply not to set LD_LIBRARY_PATH? In most cases, setting `LD_LIBRARY_PATH` to an empty string should be an acceptable solution. However, there is an important scenario where it will not work: when JAX is used together with another CUDA and `LD_LIBRARY_PATH` dependent library in the same process: 1. A single project requires both JAX and TensorFlow in a single process. 2. TensorFlow uses CUDA toolkit 11.8 that is specified in `LD_LIBRARY_PATH`. 3. JAX is supposed to use the bundled CUDA 12.2 installed by pip during the JAX's installation. 4. There is a script that uses both JAX and TensorFlow simultaneously. This is likely because 1) we may need `tensorflow_datasets` to load data for JAX models or 2) an older part of the codebase is implemented in Tensorflow and the newer in JAX, and we want to bridge them together. 5. Since TensorFlow and JAX need incompatible CUDA versions in the same process with the same `LD_LIBRARY_PATH` variable, there is no way the beforementioned script could work. JAX would be confused by `LD_LIBRARY_PATH` and would try to use an older 11.8 CUDA. There are two possible solutions: either split it into two separate processes, or find versions of TF and JAX that use the same CUDA version and point `LD_LIBRARY_PATH` to it. 6. However, had JAX always used the bundled CUDA version, this use case would've just worked. My main point is that the current dependency on `LD_LIBRARY_PATH` of the library that comes with bundled CUDA clearly causes confusion. Although, I am not sure that behavioral change is strictly necessary here. Apparently, the easiest way to resolve this confusion is by adding a warning to the pip installation: GPU (CUDA, installed via pip, easier) section that goes something like this: > **Warning**: Ensure your `LD_LIBRARY_PATH` environment variable does not include paths that contain CUDA libraries. Otherwise, JAX will use CUDA libraries from `LD_LIBRARY_PATH` instead of the CUDA libraries bundled with JAX and installed by pip. Do you think this is a feasible solution? In addition, it would be good to provide some guidance for people who want to get JAX and TensorFlow working in the same process, albeit with different CUDA versions. Especially for teams that are evaluating JAX alongside TF. Another point is convenience. When switching to a conda environment with JAX we need to remember to clear the `LD_LIBRARY_PATH` variable, which is an additional step that will frequently be forgotten. > It would be possible to change jax to set RPATH instead of RUNPATH, but the catch would be that then if the CUDA pip packages were installed, then we would use them always and there would be no way to override that choice for development or by a system administrator. If you had the CUDA pip packages installed but needed to use another CUDA installation, you'd be forced to uninstall the CUDA pip packages. The need to switch from the bundled CUDA to another CUDA from a single JAX installation seems unlikely to me. JAX is usually installed within a conda or a virtualenv environment. Thus, it's easier to create two conda environments with different setups of JAX and switch between them.","I agree that the lack of clarity is the main issue here. The fact the JAX behaves differently from PyTorch in such a common scenario is already a reason to think about what's going on. I also believe that since there are already two packages provided (i.e., pip and local) the user has already the ability to chose the behaviour of JAX with regard to LD_LIBRARY_PATH and so, maybe, the pip version shouldn't be affected by its value (if I'm a new user and I download the ""easytouse"" cuda1x_pip version I expect it to work out of the box, independently of how the system administrator configured my machine).","In my case I had to import jax first and do an initial computation before importing tensorflow, otherwise tensorflow would set the wrong cudnn version and I would get `jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed` afterwards","I'll note that *none* of these things will help if you try to import two libraries that want different CUDA/CuDNN versions into the same process. Irrespective of how we do things, whichever library you import first will ""win"". If you just want TF for a data pipieline, I recommend the `tensorflowcpu` package!","I suspect that https://github.com/google/jax/commit/9404518201c5ac8af6c85ecdf12a7cc34c102585 will help catch this case. We will now raise an exception if we detect that the versions of the CUDA libraries loaded are too old, which would happen if you override the `LD_LIBRARY_PATH` to point to an old version."
308,"以下是一个github上的jax下的一个issue, 标题是([Mosaic] apply_vector_layout C++ rewrite (13): scf.if, scf.yield)， 内容是 ([Mosaic] apply_vector_layout C++ rewrite (13): scf.if, scf.yield)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[Mosaic] apply_vector_layout C++ rewrite (13): scf.if, scf.yield","[Mosaic] apply_vector_layout C++ rewrite (13): scf.if, scf.yield",2023-09-07T07:08:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17488
332,"以下是一个github上的jax下的一个issue, 标题是(Improve the gpu lowering error message if users forget link the gpu library.)， 内容是 (Improve the gpu lowering error message if users forget link the gpu library.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve the gpu lowering error message if users forget link the gpu library.,Improve the gpu lowering error message if users forget link the gpu library.,2023-09-06T20:24:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17475
777,"以下是一个github上的jax下的一个issue, 标题是(lax.conv_transpose takes FOREVER to compile)， 内容是 ( Description I initially submitted the issue here: https://github.com/deepmind/dmhaiku/issues/724 But then realized it was a jax issue. In short,  I've been trying to use Conv2DTranspose in my model, and even for very simple case... it takes forever to compile.  output  For comparison, here is the pytorch:  Google colab notebook: https://colab.research.google.com/drive/15YkOuK0EjqZdBNaXpF2wpYexGqtjZjLr  What jax/jaxlib version are you using? Google Colab  Which accelerator(s) are you using? GPU  Additional system info Google Colab  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,lax.conv_transpose takes FOREVER to compile," Description I initially submitted the issue here: https://github.com/deepmind/dmhaiku/issues/724 But then realized it was a jax issue. In short,  I've been trying to use Conv2DTranspose in my model, and even for very simple case... it takes forever to compile.  output  For comparison, here is the pytorch:  Google colab notebook: https://colab.research.google.com/drive/15YkOuK0EjqZdBNaXpF2wpYexGqtjZjLr  What jax/jaxlib version are you using? Google Colab  Which accelerator(s) are you using? GPU  Additional system info Google Colab  NVIDIA GPU info ",2023-09-06T14:21:47Z,bug XLA NVIDIA GPU,open,0,3,https://github.com/jax-ml/jax/issues/17464,I think this in turn is an XLA bug. Opened https://github.com/openxla/xla/issues/5541.,This is apparently due to convolution autotuning: some of the algorithms in cudnn are very slow and we try them all during autotuning. Once autotuning has run we will choose a fast algorithm.,"It seems in this case the same algorithms are returned by heuristics_mode_a and heuristics_mode_b. So when we deduplicate the algorithms to try during autotuning, we can half the compile time. That still means it is slow, but it is a step in the right direction. There is an idea how to potentially speed it up more by stopping an autotuning attempt if the best known runtime is already exceeded, but that will take a bit longer to implement."
1281,"以下是一个github上的jax下的一个issue, 标题是(build: write appropriate version strings to build artifacts)， 内容是 ( Description The goal of this PR is to address CC(`pip install jax jaxlib` broken with 0.4.15), the issue that led us to yank the 0.4.15 release. The desired endstate are for `jax.__version__` and `jaxlib.__version__` to satisfy the following (keeping PEP 440 in mind):  when importing or installing jax/jaxlib from the github source tree, the version looks like `0.4.16.dev20230906+g5227f1d14` where `20230906` is the date of the most recent commit, and `g5227f1d14` is the commit hash prefix.  when building the jax/jaxlib nightly release, the resulting source distribution and wheel files have the date of build (not date of commit) encoded in the version, like `0.4.16.dev20230906`  when building jax/jaxlib releases, the resulting source distribution and wheel files have clean release versions that look like `0.4.16`. I looked into the possibility of using `versioneer` or `miniver` for this, but they seem to not be well set up for repositories with multiple packages (`jax` and `jaxlib`), and also use mechanisms that are not compati)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,build: write appropriate version strings to build artifacts," Description The goal of this PR is to address CC(`pip install jax jaxlib` broken with 0.4.15), the issue that led us to yank the 0.4.15 release. The desired endstate are for `jax.__version__` and `jaxlib.__version__` to satisfy the following (keeping PEP 440 in mind):  when importing or installing jax/jaxlib from the github source tree, the version looks like `0.4.16.dev20230906+g5227f1d14` where `20230906` is the date of the most recent commit, and `g5227f1d14` is the commit hash prefix.  when building the jax/jaxlib nightly release, the resulting source distribution and wheel files have the date of build (not date of commit) encoded in the version, like `0.4.16.dev20230906`  when building jax/jaxlib releases, the resulting source distribution and wheel files have clean release versions that look like `0.4.16`. I looked into the possibility of using `versioneer` or `miniver` for this, but they seem to not be well set up for repositories with multiple packages (`jax` and `jaxlib`), and also use mechanisms that are not compati",2023-09-05T22:01:42Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17453
660,"以下是一个github上的jax下的一个issue, 标题是(Jax leaks memory with random keys)， 内容是 ( Description I'm able to leak memory on CPU by running the following script  When run, it prints:  This also uses way more memory than I was expecting. Why does the first run require nearly 1GB? 500,000 random keys at 8 bytes each should take up less than 4MB, right?  What jax/jaxlib version are you using? jax0.4.14, jaxlib0.4.14  Which accelerator(s) are you using? CPU  Additional system info python3.11.4, MacOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax leaks memory with random keys," Description I'm able to leak memory on CPU by running the following script  When run, it prints:  This also uses way more memory than I was expecting. Why does the first run require nearly 1GB? 500,000 random keys at 8 bytes each should take up less than 4MB, right?  What jax/jaxlib version are you using? jax0.4.14, jaxlib0.4.14  Which accelerator(s) are you using? CPU  Additional system info python3.11.4, MacOS  NVIDIA GPU info _No response_",2023-09-05T10:09:12Z,bug,open,0,14,https://github.com/jax-ml/jax/issues/17432,"This looks like a garbage collection issue. In each iteration, once `inputs` goes out of scope, it doesn't immediately get deleted. Rather, its CPython reference count goes to zero, and sometime later the garbage collector will delete the variable. Once the garbage collector deletes the Python object, it triggers a call to XLA to clear the allocated buffer from the device. This delete call is also asynchronous, and will happen some time later in the program. If you're concerned about arrays/buffers from previous loops being deleted more quickly during your program execution, one thing you can do is to explicitly delete or garbage collect, though this will slow the execution of your code. For example:  You should see the memory use greatly reduced.","Interestingly, this doesn't grow:  But this does (one line added): ","Ah I think  figured it out. This also doesn't leak:  The issues are: 1. the program uses more memory than you might expect because `key, *data_keys = random.split(key, iters + 1)` is unpacking a single array into 50,001 separate arrays (e.g. the 50,000 components of the `data_keys` tuple); 2. the memory usage grows because each jax.Array has cached properties on it, and so each of the 50,000 arrays has that cached metadata populated when it's touched (i.e. when it's read by the `ranodm.normal(data_keys[i], (10,))` expression) I'm not sure if there's anything to fix here, at least in JAX itself, though I could be wrong. If you have a real program with this kind of memory issue, consider not unpacking 50,000 separate arrays and instead writing something more like ",Actually  and  have ideas for how we can reduce the cached stuff here... looking into it!," thanks for the tip, I was unaware of the array caching mechanics (hidden side effect!). I figured precomputing all the keys in one batched operation would be more efficient than doing it one at a time in the loop. The array slice seems like a good alternative.",https://github.com/google/jax/pull/17452 should fix the leak problem.,The PR has been submitted. Can you try again?,"Sorry I'm not super familiar with your PR process. https://github.com/google/jax/pull/17452 does not have any commits attached to it. I just cloned and tested on master and I'm still seeing the memory leak, even with the call to `gc.collect()` (albeit it leaks much more slowly).","The script I ran, for posterity:  ",I can't seem to repro it. Are you sure you installed jax at HEAD? Note you would need to clone jax and then `cd jax; pip install U .`. Maybe also uninstall jax before you do this? Also note that the memory usage will go up and down which is expected. ,"Weird, maybe it's something to do with my python/macos version.   ",Any updates to this issue  ? I'm on jax==0.4.23 and jaxlib==0.4.23+cuda11.cudnn86 (which should include changes from CC(Make `is_fully_addressable` an abstract method and implement it on each concrete Sharding.) ) and see the same memory leak issue.,"I think this is working as expected: when you run these lines:  you're creating a list `data_keys` with 500,000 elements, each of which is a JAX array (and each of which will have the memory overhead assocated with a concrete JAX array instance). I would not expect the memory footprint to be reduced until those 500,000 arrays are deallocated, for example by deleting `data_keys` and then calling `gc.collect()`. Iterating through `data_keys` does not change or deallocate the contents of `data_keys`. Perhaps you were aiming for something like this:  in which `data_keys` will be a single array rather than a list of arrays.",(I just realized my comment is the same solution  offered above... sorry if there's something I'm missing)
964,"以下是一个github上的jax下的一个issue, 标题是(XLARunTime Error: Cannot remove instruction %all-reduce while sharding with convolutions)， 内容是 ( Description Hi team, I have been using jax with equinox for some time, and was excited by the new AutoParallelism feature using sharding. Unfortunately, there's a bug I have encountered while using sharding using convolutions. XLA with sharding and convolutions just breaks.  With all other operations with FC layers and other, sharding works, but fails with convolution. See the below error:  !image MWE for the replication of code is below:   What jax/jaxlib version are you using? jax==0.4.13, jaxlib==0.4.13+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info NVIDIASMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XLARunTime Error: Cannot remove instruction %all-reduce while sharding with convolutions," Description Hi team, I have been using jax with equinox for some time, and was excited by the new AutoParallelism feature using sharding. Unfortunately, there's a bug I have encountered while using sharding using convolutions. XLA with sharding and convolutions just breaks.  With all other operations with FC layers and other, sharding works, but fails with convolution. See the below error:  !image MWE for the replication of code is below:   What jax/jaxlib version are you using? jax==0.4.13, jaxlib==0.4.13+cuda12.cudnn89  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info NVIDIASMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1",2023-09-05T09:19:57Z,bug XLA needs info NVIDIA GPU,closed,0,2,https://github.com/jax-ml/jax/issues/17431,"I can't reproduce this at head on 2 GPUs. Can you say more about your GPU setup? How many GPUs are you using and what kinds? If you're so inclined, if you build `jaxlib` from source at head does the problem reproduce? Another thing that might help is if you include an XLA HLO dump. You can collect one by setting the environment variable `XLA_FLAGS=xla_dump_to=/some/directory`, running your computation, and zipping up the output directory and attaching it to this bug.",Closing because no further details were provided. I'd be happy to look into this if you can still reproduce this with the latest jax and jaxlib!
1301,"以下是一个github上的jax下的一个issue, 标题是(XLA Runtime Error at jax.ppermute when setting ""XLA_PYTHON_CLIENT_MEM_FRACTION"")， 内容是 ( Description We are performing scaling runs of our JAX code (https://github.com/tumaer/JAXFLUIDS) on the JUWELS Booster (https://apps.fzjuelich.de/jsc/hps/juwels/boosteroverview.html). We experience errors using jax.lax.ppermute depending on the setting of certain environmental variables. Software in use: Python 3.10.4, jax 0.4.14, jaxlib, 0.4.14+cuda12.cudnn89, CUDA 12 installed using pip wheels provided by the jax pip install. Please find attached requirements.txt for details about the used package versions. We run the following python script on a single compute node (4xNVIDIA A100) using a single process.   And get the expected output:  However, when we set the environmental variable  and run the exact same script, we get the following error:  The program fails at compiling the pmapped function that performs a simple jax.lax.ppermute (jax.lax.psum works just fine). We noticed however, that if we deactivate P2P using  the script works again. Specifying the preallocated memory fraction enables us to run larger jobs for the sc)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"XLA Runtime Error at jax.ppermute when setting ""XLA_PYTHON_CLIENT_MEM_FRACTION"""," Description We are performing scaling runs of our JAX code (https://github.com/tumaer/JAXFLUIDS) on the JUWELS Booster (https://apps.fzjuelich.de/jsc/hps/juwels/boosteroverview.html). We experience errors using jax.lax.ppermute depending on the setting of certain environmental variables. Software in use: Python 3.10.4, jax 0.4.14, jaxlib, 0.4.14+cuda12.cudnn89, CUDA 12 installed using pip wheels provided by the jax pip install. Please find attached requirements.txt for details about the used package versions. We run the following python script on a single compute node (4xNVIDIA A100) using a single process.   And get the expected output:  However, when we set the environmental variable  and run the exact same script, we get the following error:  The program fails at compiling the pmapped function that performs a simple jax.lax.ppermute (jax.lax.psum works just fine). We noticed however, that if we deactivate P2P using  the script works again. Specifying the preallocated memory fraction enables us to run larger jobs for the sc",2023-09-04T13:23:06Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/17426,"`XLA_PYTHON_CLIENT_MEM_FRACTION` is normally expressed as a ratio. 0.75 is typical. Using ""100"" seems like a mistake.  Also you cannot use the entire GPU for XLA's heap: NVIDIA's libraries need a certain amount of GPU memory also. Hope that helps!"
1287,"以下是一个github上的jax下的一个issue, 标题是(Flexible Type Annotation for jax.lax.scan Function - Fixes #17405)， 内容是 (What does this PR do? fix : CC(Type annotation for jax.lax.scan appears incorrect) This pull request addresses issue CC(Type annotation for jax.lax.scan appears incorrect), which concerns the type annotation for the jax.lax.scan function in the JAX library. The issue pointed out that the existing type annotation was not accurate when the function f returned different types, especially when it returned Python scalar types. To resolve this issue, this pull request introduces a more flexible type annotation for the jax.lax.scan function. Instead of specifying a fixed type for the return value, we use Any for the type of Y. This change acknowledges the inherent variability in the return type of f and ensures that the type annotation is more inclusive and representative of realworld usage. By using Any for Y, we avoid overly restrictive type annotations that might lead to type errors in cases where f returns Python scalar types, while still maintaining type safety for common cases where Y represents an array with an unspecified num)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Flexible Type Annotation for jax.lax.scan Function - Fixes #17405,"What does this PR do? fix : CC(Type annotation for jax.lax.scan appears incorrect) This pull request addresses issue CC(Type annotation for jax.lax.scan appears incorrect), which concerns the type annotation for the jax.lax.scan function in the JAX library. The issue pointed out that the existing type annotation was not accurate when the function f returned different types, especially when it returned Python scalar types. To resolve this issue, this pull request introduces a more flexible type annotation for the jax.lax.scan function. Instead of specifying a fixed type for the return value, we use Any for the type of Y. This change acknowledges the inherent variability in the return type of f and ensures that the type annotation is more inclusive and representative of realworld usage. By using Any for Y, we avoid overly restrictive type annotations that might lead to type errors in cases where f returns Python scalar types, while still maintaining type safety for common cases where Y represents an array with an unspecified num",2023-09-02T12:45:25Z,,closed,0,3,https://github.com/jax-ml/jax/issues/17412,"Jakevdp, Thank you for taking the time to review this pull request and provide valuable feedback. I appreciate your insights and concerns regarding the proposed changes. It's clear that there are some challenges in finding an optimal solution for annotating the `jax.lax.scan` function, especially given the variability in the return type of `f`. I acknowledge your point about shadowing the builtin `Any` with a `TypeVar` of the same name, which could be confusing. I'll address this concern in the next iteration of the pull request. Regarding the use of `tuple` versus `Tuple`, I understand the preference for the builtin lowercase `tuple`, and I'll make sure to follow that convention in the updated code. I'll also take into consideration your comment about the limitations of Python's type system and continue to explore potential alternatives or improvements to the type annotation. Your feedback is greatly appreciated, and I'll work on addressing these concerns and refining the pull request accordingly. Thank you.",Hello  are you still interested in working on this? Thanks!,> Hello  are you still interested in working on this? Thanks! No
1287,"以下是一个github上的jax下的一个issue, 标题是(Flexible Type Annotation for jax.lax.scan Function - Fixes #17405)， 内容是 (What does this PR do? fix : CC(Type annotation for jax.lax.scan appears incorrect)  This pull request addresses issue CC(Type annotation for jax.lax.scan appears incorrect), which concerns the type annotation for the jax.lax.scan function in the JAX library. The issue pointed out that the existing type annotation was not accurate when the function f returned different types, especially when it returned Python scalar types. To resolve this issue, this pull request introduces a more flexible type annotation for the jax.lax.scan function. Instead of specifying a fixed type for the return value, we use Any for the type of Y. This change acknowledges the inherent variability in the return type of f and ensures that the type annotation is more inclusive and representative of realworld usage. By using Any for Y, we avoid overly restrictive type annotations that might lead to type errors in cases where f returns Python scalar types, while still maintaining type safety for common cases where Y represents an array with an unspecified nu)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Flexible Type Annotation for jax.lax.scan Function - Fixes #17405,"What does this PR do? fix : CC(Type annotation for jax.lax.scan appears incorrect)  This pull request addresses issue CC(Type annotation for jax.lax.scan appears incorrect), which concerns the type annotation for the jax.lax.scan function in the JAX library. The issue pointed out that the existing type annotation was not accurate when the function f returned different types, especially when it returned Python scalar types. To resolve this issue, this pull request introduces a more flexible type annotation for the jax.lax.scan function. Instead of specifying a fixed type for the return value, we use Any for the type of Y. This change acknowledges the inherent variability in the return type of f and ensures that the type annotation is more inclusive and representative of realworld usage. By using Any for Y, we avoid overly restrictive type annotations that might lead to type errors in cases where f returns Python scalar types, while still maintaining type safety for common cases where Y represents an array with an unspecified nu",2023-09-02T12:41:06Z,,closed,0,1,https://github.com/jax-ml/jax/issues/17411,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
766,"以下是一个github上的jax下的一个issue, 标题是(jax.debug.breakpoint() give numpy array and with bfloat16, it comptue)， 内容是 ( Description jax.debug.breakpoint() give strange results. Some of the inputs are tensors of 1s with shape of 2048 (bfloat16). But if we do `x.sum()` is return 256. But if I do that in the python script, I get the good results (2048):  To prevent all that category of issues, breakpoint should give ArrayImpl object and not numpy object.    What jax/jaxlib version are you using? upstream  Which accelerator(s) are you using? Independent of backend.  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"jax.debug.breakpoint() give numpy array and with bfloat16, it comptue"," Description jax.debug.breakpoint() give strange results. Some of the inputs are tensors of 1s with shape of 2048 (bfloat16). But if we do `x.sum()` is return 256. But if I do that in the python script, I get the good results (2048):  To prevent all that category of issues, breakpoint should give ArrayImpl object and not numpy object.    What jax/jaxlib version are you using? upstream  Which accelerator(s) are you using? Independent of backend.  Additional system info _No response_  NVIDIA GPU info _No response_",2023-09-01T15:47:07Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17402,We should probably return a CPU `Array` in all cases.
591,"以下是一个github上的jax下的一个issue, 标题是(Differentiation failing on real function)， 内容是 ( Description First off, thank you all for your work on this incredible project! We are trying to compute the derivative of the potential function below (found in gravitational wave parameter estimation). We have a complex valued inner product $ : \mathbb{C}^n \times \mathbb{C}^n \to \mathbb{C}$, and a probability distribution defined by $$ p(x) = e^{\frac{1}{2}  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Differentiation failing on real function," Description First off, thank you all for your work on this incredible project! We are trying to compute the derivative of the potential function below (found in gravitational wave parameter estimation). We have a complex valued inner product $ : \mathbb{C}^n \times \mathbb{C}^n \to \mathbb{C}$, and a probability distribution defined by $$ p(x) = e^{\frac{1}{2}  ++ ```",2023-09-01T15:40:24Z,bug needs info,closed,0,7,https://github.com/jax-ml/jax/issues/17401,"Thanks for the report! It's definitely possible that something in JAX's autodiff machinery is producing incorrect gradients, but my first guess seeing the code would be that your analytical expression is incorrect – it's a relatively complicated expression, and after staring at it for two minutes it's not obvious to me that the gradient is correct (not obvious that it's wrong either). Still, it wouldn't be hard to make a mistake there. If you think JAX is producing the wrong answer, I'd suggest trying to come up with a reproduction in which it is more straightforward to see and confirm that JAX is producing the wrong value.","Thank you for such a quick reply!  So the method `gradient_strain` which implements the derivative analytically is confirmed to yield the same result as `jax.jacfwd(model.strain)`. So this portion of the code doesn't have any bugs in it! Its only when we try to differentiate through the inner product that we get bugs. However, this part is very clean. Since the potential is given by $$V(x) = \frac{1}{2} Re \lt h(x)  d, h(x)  d \gt$$ and since $d$ is a constant, the gradient has the following form $$\nabla V(x) = Re \lt \nabla h(x) , h(x)  d \gt$$ And for reference, the inner product is defined as $$\lt a,b \gt = \sum_i a_i^*b_i$$ If you have any hypotheses on what may be causing this bug I can reply with a more minimal example, but I'm stumped. Update 1: I got rid of a bunch of fluff in the original code I posted, and the bug remains Update 2: Just an observation, but the last component of the gradient agrees, but all others disagree","I haven't looked into the details, but I just wanted to point out that numerical checking of gradients (to convince yourself that autodiff is doing the right thing) is pretty easy: just compare to finite differences, like $f'(x) \approx \frac{f(x + \epsilon)  f(x)}{\epsilon}$ (along a random direction). That's what the internal tool `jax._src.test_util.check_grads` does automatically. (Also I recommend using `\langle` and `\rangle` to typeset inner products in TeX!)",One thing that immediately sticks out to me is that you have a 10^22 range in the magnitude of your inputs – it wouldn't be surprising to me if floating point roundoff errors are coming into play with any computation involving numbers at such different scales.,"I believe you have the right idea Jake. It appears that changing the scales for the amplitude variable fixes the issue. Specifically, changing  to  resolves the issue. Could you explain why this would be an issue? Also, would it be possible to have JAX throw a warning in a situation like this?","> Could you explain why this would be an issue? For example:  Floating point math is only an approximation of real math. 64bit floating point only can represent about 16 decimal places in a single value; 32bit can only represent about 8. So if you're doing arithmetic operations involving numbers of very different scales, floating point expressions are likely to lose precision. > Also, would it be possible to have JAX throw a warning in a situation like this? No, this is just something you have to work with and be aware of when doing floating point operations on all modern systems. There's a good reference on this topic here: https://stackoverflow.com/q/588004",Makes sense! I appreciate the help.
1263,"以下是一个github上的jax下的一个issue, 标题是(rolling forward shard_map transpose fixes)， 内容是 (rolling forward shard_map transpose fixes The new efficienttranspose path, enabled by setting check_rep=True in the shard_map call, had kept working. But the change inadvertently broke the check_rep=False path. And because most tests set check_rep=True, we didn't notice it in the tests! The issue was that with check_rep=False, we need the shard_map transpose rule to insert psums corresponding to in_specs with fanout, and correspondingly insert division for out_specs with faninconsensus. (With the new check_rep=True path that this change adds, those extra operations aren't necessary as the body itself transposes correctly.) But the PR accidentally removed those! The fix was simple: just track whether we've applied the efficienttransposebodyrewrite (i.e. whether we're in the new bodyistransposable path or old needextraoperations path) by adding a boolean parameter `rewrite` to the shard_map primitive, and if the rewrite hasn't been applied then include the explicit psum/div operations in the transpose rule. Reverts 8a04dfd830ff8)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,rolling forward shard_map transpose fixes,"rolling forward shard_map transpose fixes The new efficienttranspose path, enabled by setting check_rep=True in the shard_map call, had kept working. But the change inadvertently broke the check_rep=False path. And because most tests set check_rep=True, we didn't notice it in the tests! The issue was that with check_rep=False, we need the shard_map transpose rule to insert psums corresponding to in_specs with fanout, and correspondingly insert division for out_specs with faninconsensus. (With the new check_rep=True path that this change adds, those extra operations aren't necessary as the body itself transposes correctly.) But the PR accidentally removed those! The fix was simple: just track whether we've applied the efficienttransposebodyrewrite (i.e. whether we're in the new bodyistransposable path or old needextraoperations path) by adding a boolean parameter `rewrite` to the shard_map primitive, and if the rewrite hasn't been applied then include the explicit psum/div operations in the transpose rule. Reverts 8a04dfd830ff8",2023-08-31T23:47:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17395
484,"以下是一个github上的jax下的一个issue, 标题是(Change jax's api_test.py for IFRT compatibility.)， 内容是 (Change jax's api_test.py for IFRT compatibility. The test relies relying on implicit crossbackend resharding, which is  not supported on arbitrary IFRT clients, see: (https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Change jax's api_test.py for IFRT compatibility.,"Change jax's api_test.py for IFRT compatibility. The test relies relying on implicit crossbackend resharding, which is  not supported on arbitrary IFRT clients, see: (https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b)",2023-08-31T21:37:51Z,,closed,0,1,https://github.com/jax-ml/jax/issues/17392,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
1056,"以下是一个github上的jax下的一个issue, 标题是(Setting `typing.TYPE_CHECKING=True` breaks Jax's API)， 内容是 ( Description I am not sure if this is intended, but I recently encountered an issue when documenting my jaxrelated library, where array would by of type `ArrayImpl`, which would break my code. This is caused by an update in `Sphinx`'s autodoc, where they import the packages with `TYPE_CHECKING=True` as of `v7.2`, see the discussion in https://github.com/sphinxdoc/sphinx/issues/11652. However, they feel it is an issue from Jax that it fails to have the expected behavior when type checking is enabled. I could not produce the same error, but we can see that importing `jax` after type checking was set results in an error:  which outputs:   What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14+cuda12.cudnn89  Which accelerator(s) are you using? CPU/GPU  Additional system info 3.10, Linux  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Setting `typing.TYPE_CHECKING=True` breaks Jax's API," Description I am not sure if this is intended, but I recently encountered an issue when documenting my jaxrelated library, where array would by of type `ArrayImpl`, which would break my code. This is caused by an update in `Sphinx`'s autodoc, where they import the packages with `TYPE_CHECKING=True` as of `v7.2`, see the discussion in https://github.com/sphinxdoc/sphinx/issues/11652. However, they feel it is an issue from Jax that it fails to have the expected behavior when type checking is enabled. I could not produce the same error, but we can see that importing `jax` after type checking was set results in an error:  which outputs:   What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14+cuda12.cudnn89  Which accelerator(s) are you using? CPU/GPU  Additional system info 3.10, Linux  NVIDIA GPU info ",2023-08-31T12:48:26Z,bug,closed,0,12,https://github.com/jax-ml/jax/issues/17385,"It looks like the error is coming from `import numpy.typing`, which comes via `import scipy`  if this is a bug, it’s a bug in numpy and scipy, not in JAX itself I think.","Looking closer at this – I don't think this would be expected to work. `typing.TYPE_CHECKING=True` implies that the code is not being executed by a Python runtime, but rather only typechecked. So for example, `numpy/typing/_ufunc.py` does not exist: you cannot import it. But `numpy/typing/_ufunc.pyi` does exist, so when a static type checker parses this code, it will be able to find the type declarations at that path. The sphinx bug looks like its real, but the code snippet above seems unrelated to the original issue.",I think this issue comes from what different Python modules assume `TYPE_CHECKING=True` means...,"Looking at the error from the sphinx issue, I also don't understand where that is coming from. `ArrayImpl` is in fact subscriptable in the current version of JAX:   What version of JAX are you using when you're seeing this error?",Versions 0.4.14 for both `jax` and `jaxlib`.,"Strange  ArrayImpl is definitiely indexable in jax v0.4.14 (see https://github.com/google/jax/blob/jaxv0.4.14/jax/_src/array.pyL297). If you can come up with a selfcontained reproducer, let me know.","I suspect the bug you're hitting has to do with this TODO, which has to do with working around a static type issue in pytype: https://github.com/google/jax/blob/88a60b808c1f91260cc9e75b9aa2508aae5bc9f9/jax/_src/array.pyL544L548 But I'd still like to understand in which situations this would come up.",Did you check the MWE I put in the Sphinx issue I me mentioned above?,"Yes I saw that, but it's not very minimal... If it's a JAX bug, we should be able to reproduce it without setting up a sphinx project directory. If we can't reproduce it without sphinx, then I'd assume it's a sphinx bug. The fact that sphinx sets `TYPE_CHECKING = True` before doing a runtime import is a bit suspicious.",I’ll try to have a better example when I can,Hi   Looks like the PR CC(Collapsed a few unnecessary ``if TYPE_CHECKING`` blocks) successfully addressed an issue that prevented JAX from importing when `typing.TYPE_CHECKING=True`. I tested the issue with the latest nightly version of JAX.  Output:  Attaching the gist for reference. Thank you.,Thanks for notifying me ! I think this issue can be closed then :)
1327,"以下是一个github上的jax下的一个issue, 标题是(Constant segfaults using latest JAX on NVIDIA Turing GPU, even on small arrays and with sufficient memory)， 内容是 ( Description With three different installations of JAX on the same hardware, I have consistently encountered segfaults on all but the most trivial operations. For example,  Install scenarios follow.  1. Installed JAX from pip (with `pip install jax[cuda11_local]`). After digging deeper into the problem, found CuDNN was missing and installed it (version 8) and upgraded CUDA itself (to version 12.2).  2. Uninstalling and reinstalling from pip again (`pip install jax[cuda12_local]`) had exactly the same results regarding segfaults.  3. Built from source (following the instructions in the docs) and eventually got a successful build (`import jax` works without error) but no change in behavior. Using my build of JAX, I ran the unittests with Bazel and see that the vast majority of them (149 of 170) fail. I am including a snippet of one arbitrarilyselected log file below.  JAX was built against the correct CUDA libraries, which I have tested separately using the sample `mnistCUDNN` program from NVIDIA. The outofmemory message has app)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Constant segfaults using latest JAX on NVIDIA Turing GPU, even on small arrays and with sufficient memory"," Description With three different installations of JAX on the same hardware, I have consistently encountered segfaults on all but the most trivial operations. For example,  Install scenarios follow.  1. Installed JAX from pip (with `pip install jax[cuda11_local]`). After digging deeper into the problem, found CuDNN was missing and installed it (version 8) and upgraded CUDA itself (to version 12.2).  2. Uninstalling and reinstalling from pip again (`pip install jax[cuda12_local]`) had exactly the same results regarding segfaults.  3. Built from source (following the instructions in the docs) and eventually got a successful build (`import jax` works without error) but no change in behavior. Using my build of JAX, I ran the unittests with Bazel and see that the vast majority of them (149 of 170) fail. I am including a snippet of one arbitrarilyselected log file below.  JAX was built against the correct CUDA libraries, which I have tested separately using the sample `mnistCUDNN` program from NVIDIA. The outofmemory message has app",2023-08-29T19:29:17Z,bug,closed,0,12,https://github.com/jax-ml/jax/issues/17349,"I'd like to update this bug with some additional information I've been able to gather. Using `faulthandler` and JAX's logging environment variables, I now know about where the segfault happens, and can share additional details of the unusual behavior I'm seeing.  Testing script   Backtrace and logs ","Hmm.. Well, we can't reproduce this, which makes it hard to debug. If you wanted to help a bit more, can you grab a C++ backtrace with `gdb`? A Python stack trace isn't really enough to speculate on the cause. You can probably do it something like this:  and attach the output. Another way would be to attach to a running process, e.g. run an `ipython` session in one window, find its process ID with `ps`, and then do:  and then share the trace.","I'm more than happy to help track this down; I've used JAX pretty extensively before and never run into a problem like this. When I examine the core dump, there are 140 threads (!) and most of them have the exact same backtrace:  The other threads' backtraces appear to contain Python API calls and nothing else (`PyEval_*`, `PyObject_*`, etc.). This is a lot to try to sift through, and I don't even see any JAXrelated code. I don't import NumPy in the script at all (just `jax.numpy`), so I'm not sure why it's appearing in the backtrace. Any thoughts?","Oops, I managed to find a thread that at least includes jaxlib in the backtrace.  No idea if it's helpful, but it may be more relevant than the above.","I'd really want to see the backtrace from the faulting thread. If you run by attaching to the process, then `gdb` will give you that thread by default. Also you could set `NPROC=1` to reduce the size of several of JAX's thread pools while debugging. (JAX starts one or two threads per CPU core, in the form of a thread pool. Threads that are sitting at a futex wait inside a ThreadPool are likely idle and waiting for work). And the other thread you have there comes from a similar threadpool from OpenBLAS, which probably comes from numpy or scipy.","Sure, here you go. This should be from the thread with the segfault, since I didn't switch threads this time. ","That's really strange! It doesn't even seem to be calling jax code in that thread. I wonder if there's a way for me to reproduce this? e.g.,: * it sounds like you're using Ubuntu 22.04? * how did you install Python? * how exactly did you install CUDA? The best idea I have is to try reproducing this in a cloud VM.","Yeah, I'm pretty stumped, too. I'm using Kubuntu 22.04 (the latest LTS). The Python I'm running is installed via APT. As for CUDA, it's from NVIDIA's repo, instructions here.","Continuing the theme of bizarre behavior, the test script works without segfaulting if I run under Python 3.10 instead of 3.11. It was something I tried on a whim after reinstalling all my CUDA software, which made no difference. In a clean Python 3.11 virtual environment, the segfault returns. I'm curious to hear if anyone was able to reproduce this.","Sorry about bumping this thread again, but I have a little more information now. The version of Python 3.11 that APT provides (and so the one I've been using in a virtual environment) is 3.11.0rc1, which is not the latest 3.11 available. So I built Python 3.11.5 from sources downloaded straight from the Python site. The segfault is gone under this version, using the same test script as before. I suppose it's possible this was a bug in Python itself that had already been fixed, exacerbated by some quirk of my particular system? As  noted, the segfault doesn't even happen in JAX code, but in Python itself. Given the new info, my problem is essentially resolved for now, so I don't know if it's worth pursuing any further.","Without looking further, I think we can assume it was a bug in a prerelease Python. If it happens again with a released version be sure to let us know! Thanks for digging into this, I'm not sure I would have ever figured this out.","> Without looking further, I think we can assume it was a bug in a prerelease Python.  In a completely different setup, I also happened to be seeing a similar error. I also was using a release candidate (which was installed on Ubuntu 22.04).  Upgrading Python fixed it in my case (for Ubuntu 22.04 this meant just adding the deadsnakes ppa described here: https://vegastack.com/tutorials/howtoinstallpython311onubuntu2204/). Anyway, just wanted to report some evidence that points to your hunch being correct."
1029,"以下是一个github上的jax下的一个issue, 标题是(Sharp edge in (probably ill-advised) `custom_vmap` usecase)， 内容是 ( Description Hey  hope all is well! I'm trying to use `custom_vmap` to do something that probably isn't advised, but if it could work it would be awesome. I have a function that I want to be defined without a batch dimension and then to add the batch dimension via `vmap`. However, I have one component of the function that I would like to act on a flattened representation of the data, even in the batched setting. I can do this with `custom_vmap` using something like this  which works great! However, this breaks with AD (either forward or backward) and  gives the error:  Is this out of scope or is this a sane thing to try to do?  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Sharp edge in (probably ill-advised) `custom_vmap` usecase," Description Hey  hope all is well! I'm trying to use `custom_vmap` to do something that probably isn't advised, but if it could work it would be awesome. I have a function that I want to be defined without a batch dimension and then to add the batch dimension via `vmap`. However, I have one component of the function that I would like to act on a flattened representation of the data, even in the batched setting. I can do this with `custom_vmap` using something like this  which works great! However, this breaks with AD (either forward or backward) and  gives the error:  Is this out of scope or is this a sane thing to try to do?  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-08-29T07:32:40Z,bug,open,2,0,https://github.com/jax-ml/jax/issues/17342
1312,"以下是一个github上的jax下的一个issue, 标题是(Make mlir.custom_call() more general and expose it as jax.interpreters.mlir.custom_call().)， 内容是 (Make mlir.custom_call() more general and expose it as jax.interpreters.mlir.custom_call(). This change is in preparation for deprecating the XlaBuilder APIs for building nonMLIR HLO. In general JAX would be best served by adding a more userfriendly ""custom kernel"" API that doesn't require the user to build IR directly, but for the moment the best we can do is migrate users to use MLIR/StableHLO utilities instead of classic HLO utilities. Since most users of custom kernels probably want to build a customcall we can get most of the benefit by providing an ergonomic helper function for building the IR for custom calls that can be called by external primitive lowering rules. This function has two benefits over just building the stablehlo directly: a) it is a JAX API, and we can be more confident the API won't change because of upstream MLIR changes b) the Python API to build stablehlo.custom_call generated by the bindings isn't that easy to use (e.g. it doesn't have sensible defaults). Next step will be to deprecate XlaBuilder and)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Make mlir.custom_call() more general and expose it as jax.interpreters.mlir.custom_call().,"Make mlir.custom_call() more general and expose it as jax.interpreters.mlir.custom_call(). This change is in preparation for deprecating the XlaBuilder APIs for building nonMLIR HLO. In general JAX would be best served by adding a more userfriendly ""custom kernel"" API that doesn't require the user to build IR directly, but for the moment the best we can do is migrate users to use MLIR/StableHLO utilities instead of classic HLO utilities. Since most users of custom kernels probably want to build a customcall we can get most of the benefit by providing an ergonomic helper function for building the IR for custom calls that can be called by external primitive lowering rules. This function has two benefits over just building the stablehlo directly: a) it is a JAX API, and we can be more confident the API won't change because of upstream MLIR changes b) the Python API to build stablehlo.custom_call generated by the bindings isn't that easy to use (e.g. it doesn't have sensible defaults). Next step will be to deprecate XlaBuilder and",2023-08-28T22:34:25Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17338
286,"以下是一个github上的jax下的一个issue, 标题是(Remove tests for jax.numpy.in1d, which is deprecated.)， 内容是 (Remove tests for jax.numpy.in1d, which is deprecated.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Remove tests for jax.numpy.in1d, which is deprecated.","Remove tests for jax.numpy.in1d, which is deprecated.",2023-08-28T20:36:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17336
1304,"以下是一个github上的jax下的一个issue, 标题是(Improve compilation speed for generated codes with automatically marginalized MCMC)， 内容是 (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Hi JAX team, this is Jinlin Lai, an author of the ICML paper Automatically Marginalized MCMC in Probabilistic Programming. Thank you for inviting me to open this issue at AABI!  When doing this work, we have observed excessive compilation time for the gradient function of the log density function of certain models, especially when a node with hundreds of children is marginalized. As you could imagine very complicated codes can be generated by Algorithm 1 and Algorithm 2 in our paper. A small model provided in Appendix G is $$ x\sim \mathcal{N}(0,1), \log\sigma\sim\mathcal{N}(0,1), y_i\sim\mathcal{N}(x,\sigma^2), $$ where $y_1,...y_N$ are observed. After marginalizing $x$, codes of $\mathcal{O}(N)$ length are generated, yet the compilation time grows superlinear with $N$. This can also be reproduced with manual marginalization. The following codes are the marginalized model by manually executing )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve compilation speed for generated codes with automatically marginalized MCMC,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Hi JAX team, this is Jinlin Lai, an author of the ICML paper Automatically Marginalized MCMC in Probabilistic Programming. Thank you for inviting me to open this issue at AABI!  When doing this work, we have observed excessive compilation time for the gradient function of the log density function of certain models, especially when a node with hundreds of children is marginalized. As you could imagine very complicated codes can be generated by Algorithm 1 and Algorithm 2 in our paper. A small model provided in Appendix G is $$ x\sim \mathcal{N}(0,1), \log\sigma\sim\mathcal{N}(0,1), y_i\sim\mathcal{N}(x,\sigma^2), $$ where $y_1,...y_N$ are observed. After marginalizing $x$, codes of $\mathcal{O}(N)$ length are generated, yet the compilation time grows superlinear with $N$. This can also be reproduced with manual marginalization. The following codes are the marginalized model by manually executing ",2023-08-28T20:09:58Z,enhancement,open,2,0,https://github.com/jax-ml/jax/issues/17335
1244,"以下是一个github上的jax下的一个issue, 标题是(Broken compilation in embarrassing parallelization with `concurrent.futures.ProcessPoolExecutor` and `jax.experimental.ode`)， 内容是 ( Description I noticed that parallelization via `concurrent.futures.ProcessPoolExecutor` does trying to jitcompile a function that is using an already jitcompiled function inside a worker. I could not reduce it down to a MWE _without_ `jax.experimental.ode`, so my suspicion is that it is specific to that. **MWE** When trying to execute the following, it simply gets stuck and checking the table of processes on my machine indicates that nothing is happening. A single execution works as expected.  I noticed is that if I dont compile `loss` before the execution (i.e. comment out `print(loss(theta))`, there is no problem and it executes as expected. So my suspicion is that the problem is stemming from multiple workers trying to simultaneously access the compiled cache.  What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? CPU  Additional system info Python 3.11, ubuntu WSL  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Broken compilation in embarrassing parallelization with `concurrent.futures.ProcessPoolExecutor` and `jax.experimental.ode`," Description I noticed that parallelization via `concurrent.futures.ProcessPoolExecutor` does trying to jitcompile a function that is using an already jitcompiled function inside a worker. I could not reduce it down to a MWE _without_ `jax.experimental.ode`, so my suspicion is that it is specific to that. **MWE** When trying to execute the following, it simply gets stuck and checking the table of processes on my machine indicates that nothing is happening. A single execution works as expected.  I noticed is that if I dont compile `loss` before the execution (i.e. comment out `print(loss(theta))`, there is no problem and it executes as expected. So my suspicion is that the problem is stemming from multiple workers trying to simultaneously access the compiled cache.  What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? CPU  Additional system info Python 3.11, ubuntu WSL  NVIDIA GPU info _No response_",2023-08-28T15:05:33Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/17329
402,"以下是一个github上的jax下的一个issue, 标题是(Update __init__.py to include dlpack module)， 内容是 (import for dlpack module was missing in `__init__.py` file. just added that. will also closes the issue CC(AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__'))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update __init__.py to include dlpack module,import for dlpack module was missing in `__init__.py` file. just added that. will also closes the issue CC(AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__'),2023-08-28T05:02:15Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/17322,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
1322,"以下是一个github上的jax下的一个issue, 标题是(jaxlib.xla_extension.XlaRuntimeError due to failed: Shared memory requested exceeds device resources)， 内容是 ( Description **Hi, there. While applying FusedAttention with jaxtriton, the following XLA error happens on NvidiaA100:** 20230828 03:06:51.319566: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.319790: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. 20230828 03:06:51.319901: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.320187: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 2)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jaxlib.xla_extension.XlaRuntimeError due to failed: Shared memory requested exceeds device resources," Description **Hi, there. While applying FusedAttention with jaxtriton, the following XLA error happens on NvidiaA100:** 20230828 03:06:51.319566: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.319790: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2593] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: Shared memory requested exceeds device resources.; current tracing scope: customcall.371; current profiling annotation: XlaModule:hlo_module=pjit__wrapped_step_fn,program_id=190. 20230828 03:06:51.319901: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 20230828 03:06:51.320187: W external/xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error: INTERNAL: Shared memory requested exceeds device resources. 2",2023-08-28T03:18:02Z,bug NVIDIA GPU,open,0,2,https://github.com/jax-ml/jax/issues/17320,"My guess is that this isn't a JAX bug, in the sense that this means that a custom Pallas/Triton kernel in PAX/Praxis is using an infeasible amount of shared memory. I think the fix would need to be to that kernel, not to JAX.","I believe that praxis is using the pallas kernel in Jax, so this is our issue."
515,"以下是一个github上的jax下的一个issue, 标题是(bug(pallas): Unimplemented primitive in Pallas GPU lowering: sign)， 内容是 ( Description  or  in a `pallas_call` yields:  Seems like the jaxpr lowering just totally fails for (`a // b`) where `a` is an expression containing a `program_id` Traceback:     What jax/jaxlib version are you using? master  Which accelerator(s) are you using? GPU CC:  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,bug(pallas): Unimplemented primitive in Pallas GPU lowering: sign, Description  or  in a `pallas_call` yields:  Seems like the jaxpr lowering just totally fails for (`a // b`) where `a` is an expression containing a `program_id` Traceback:     What jax/jaxlib version are you using? master  Which accelerator(s) are you using? GPU CC:  ,2023-08-27T15:34:49Z,bug NVIDIA GPU pallas,closed,0,1,https://github.com/jax-ml/jax/issues/17317,You can get around this for now by using lax.div. The // operator expands into more than just division.
1306,"以下是一个github上的jax下的一个issue, 标题是([export] Implement the calling convention for exporting with multi-platform lowering)， 内容是 (This is a first step towards supporting multiplatform exported JAX modules. Such modules are usable on more than one platform, and take an additional first argument that encodes the actual compilation platform as an index into the sequence of platforms for which the module was lowered. More details about the calling convention are in the docstring for jax_export.Exported in this PR. The value of the platform index is set by `jax_export.call_exported` when calling from JAX, and in the tf.XlaCallModule prior to compilation, when called from TensorFlow. This is already implemented in tf.XlaCallModule. This PR has some incomplete pieces:   * Currently we actually lower only for the first platform specified, and the platform argument is not used. There are a couple of implementation strategies for actual multiplatform lowering, both using the same calling convention. We could lower separately for each platform and put the results together with one toplevel conditional. Alternatively, we can take advantage of the fact that few primi)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[export] Implement the calling convention for exporting with multi-platform lowering,"This is a first step towards supporting multiplatform exported JAX modules. Such modules are usable on more than one platform, and take an additional first argument that encodes the actual compilation platform as an index into the sequence of platforms for which the module was lowered. More details about the calling convention are in the docstring for jax_export.Exported in this PR. The value of the platform index is set by `jax_export.call_exported` when calling from JAX, and in the tf.XlaCallModule prior to compilation, when called from TensorFlow. This is already implemented in tf.XlaCallModule. This PR has some incomplete pieces:   * Currently we actually lower only for the first platform specified, and the platform argument is not used. There are a couple of implementation strategies for actual multiplatform lowering, both using the same calling convention. We could lower separately for each platform and put the results together with one toplevel conditional. Alternatively, we can take advantage of the fact that few primi",2023-08-27T14:26:54Z,pull ready,closed,0,6,https://github.com/jax-ml/jax/issues/17316,  PTAL,"> One highlevel question (not a blocker): what are the advantages of exporting one StableHLO module with multiplatform functions like in this PR, compared to simply exporting one StableHLO module per platform? >  > I'm wondering if we will ever be in a situation where having just one StableHLO module for all platforms could be limiting, e.g., different modulelevel params depending on platforms, having to do constant folding to get rid of irrelevant branches (especially if some custom calls are not known by other platforms), etc. If so, dispatching different StableHLO modules at the `XlaCallModule` level might be simpler/modular. The main benefit I can see is that JAX has few primitives with plaformspecific lowerings and I expect that a multiplatform lowering using conditionals for those primitives will be much smaller than multiple lowerings. The lowering time should similarly be smaller. Philosophically, such a lowering seems to fit better with JAX, that is mostly platformindependent and picks different lowerings only exceptionally to workaround differences in the XLA compilers. It is likely that there differences will get smaller. However, I am not 100% certain that this scheme will work in all cases, it is somewhat of an experiment. In terms of the costs of this scheme, there is a bit of extra complexity, but not much. I have already tested that custom calls are not an issue, because the HLO lowering already eliminates the redundant branches, and the resulting HLO that gets to the compiler is the same as if we lowering just for one platform. The alternative, which I think you have in mind, is to add multiple serialized modules to the XlaCallModule op?   ","Thanks for the clarification. > The main benefit I can see is that JAX has few primitives with plaformspecific lowerings and I expect that a multiplatform lowering using conditionals for those primitives will be much smaller than multiple lowerings. The lowering time should similarly be smaller. Philosophically, such a lowering seems to fit better with JAX, that is mostly platformindependent and picks different lowerings only exceptionally to workaround differences in the XLA compilers. It is likely that there differences will get smaller. This makes sense and it does sound useful. I'm curious how this will play out with our another goal of not transforming StableHLO as much as possible for debuggability (e.g., how can we constantfold only the platform index switch and nothing else), but we can continue experimenting with this idea. > The alternative, which I think you have in mind, is to add multiple serialized modules to the XlaCallModule op?    Yeah, that's what I had in mind. But it seems reasonable to start from this idea and see if it works well, especially given that it looks like we can switch between two approaches without having to change the userfacing interface much (if any).","Good discussion.  There is another user case maybe worth to mention here. In PAX/SAX, I saw some jax function has backendconditional logic.  For example  I wonder how to do multiplatform lowering here now.   .","> Good discussion. There is another user case maybe worth to mention here. In PAX/SAX, I saw some jax function has backendconditional logic. For example >  >  >  > I wonder how to do multiplatform lowering here now. . That code cannot be handled by multiplatform lowering, or crossplatform lowering. It is in fact very suspicious because it uses global device state to decide what code to use, but JAX will make platform decisions based on what arguments are passed to the function being traced. We should look into why they are doing this and what are cleaner ways of achieving that. But this should be a separate issue.",>  I paste one example here  third_party/py/praxis/py_utils.py;l=457466
1084,"以下是一个github上的jax下的一个issue, 标题是(fix(pallas): flash attention tiled iteration upper bound too small when `block_q > block_k`)， 内容是 (https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL80  For reference: https://github.com/openai/triton/blob/5f448b2f085f554aaf291db07d18f7b16942d21f/python/triton/ops/flash_attention.pyL74  When `block_q` > `block_k`, the bound is too small. Consider `start_q = 1, block_q = 3, block_k = 1`. As implemented, this would yield 4 iterations. However, what is actually required is `cdiv((block_q * (start_q + 1)), block_k) == 6`  Originally identified: https://github.com/google/jax/issues/17267  Added regression test that fails on master: ` tests/pallas/pallas_test.py::FusedAttentionTest::test_fused_attention_fwd_batch_size=1_seq_len=384_num_heads=8_head_dim=64_causal=True_use_fwd=True_kwargs={'block_q': 128, 'block_k': 64} FAILED [ 55%] ` CC:  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fix(pallas): flash attention tiled iteration upper bound too small when `block_q > block_k`,"https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL80  For reference: https://github.com/openai/triton/blob/5f448b2f085f554aaf291db07d18f7b16942d21f/python/triton/ops/flash_attention.pyL74  When `block_q` > `block_k`, the bound is too small. Consider `start_q = 1, block_q = 3, block_k = 1`. As implemented, this would yield 4 iterations. However, what is actually required is `cdiv((block_q * (start_q + 1)), block_k) == 6`  Originally identified: https://github.com/google/jax/issues/17267  Added regression test that fails on master: ` tests/pallas/pallas_test.py::FusedAttentionTest::test_fused_attention_fwd_batch_size=1_seq_len=384_num_heads=8_head_dim=64_causal=True_use_fwd=True_kwargs={'block_q': 128, 'block_k': 64} FAILED [ 55%] ` CC:  ",2023-08-27T10:43:50Z,pull ready,closed,0,4,https://github.com/jax-ml/jax/issues/17314,"FYI, we just updated the kernel to handle segment_ids, you will probably need to rebase on top of those changes. Could you also squash your commits?",Thanks for the updates! One last thing: could you update your commit message to reflect the change? It currently looks like this: ,Hello  could this be merged now?,Yes I'll try to get it merged today
824,"以下是一个github上的jax下的一个issue, 标题是(Subsampling sparse matrices during (or after) multiplication)， 内容是 (Hi, I'm wondering if it is possible to efficiently implement a function analogous to `bcoo_dot_general_sampled` with sparse operands. I noticed that the function is actually expecting dense arrays, and also slicing a sparse array appears to return a dense array? My use case is simply trying to do online accumulation of gradients for a sparse recurrent network, with the sparsity constraint enforced on the gradients. Casting to and from a dense array in between would be way too slow and memoryintensive. Are there any workarounds I missed? How difficult would this be to implement?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Subsampling sparse matrices during (or after) multiplication,"Hi, I'm wondering if it is possible to efficiently implement a function analogous to `bcoo_dot_general_sampled` with sparse operands. I noticed that the function is actually expecting dense arrays, and also slicing a sparse array appears to return a dense array? My use case is simply trying to do online accumulation of gradients for a sparse recurrent network, with the sparsity constraint enforced on the gradients. Casting to and from a dense array in between would be way too slow and memoryintensive. Are there any workarounds I missed? How difficult would this be to implement?",2023-08-26T19:54:13Z,enhancement,open,0,3,https://github.com/jax-ml/jax/issues/17312,"No, I don't know of any implementation similar to `bcoo_dot_general_sampled` with sparse operands. >  I noticed that the function is actually expecting dense arrays Yes, this is the intent of `bcoo_dot_general_sampled`. It essentially implements the generalized SDDMM operation, which is important because it appears in the transpose of a sparsedense dot product. > also slicing a sparse array appears to return a dense array? I don't think this is true in general; for example:  > Are there any workarounds I missed? Not that I know of – with some thought, you could probably implement this logic from scratch for the particular matrix layouts you're interested in.","Thanks for your fast response as usual! > I don't think this is true in general; You are right. Taking that into account, I think the following general idea works:  As long as the operands have the same indices and shape, this should work. Although it runs a few unnecessary multiplies in the general case, I think after JIT this will be pretty efficient?","I don't totally understand what you have in mind, but it looks like you're trying to remove the batch dimension in the indexing results. If that's the case, you could use `result.update_layout(n_batch=0)` rather than manually creating the new BCOO matrix from the data and index buffers."
1112,"以下是一个github上的jax下的一个issue, 标题是(Error importing jax)， 内容是 ( Description I have created a fresh environemnt `conda create name myenv`. Then I activate the environment and the very first package I am installing is `jax` with the command: `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` That looks to install everything in this folder under my home directory: `$HOME/.local/lib/python3.10/sitepackages` However when I install the other packages that I need to work with, these go into the typical location for conda enviroments: `$HOME/miniconda3/envs/myenv/lib/python3.8/sitepackages` Hence, when I try to import jax from within my new environment, I got the `ModuleNotFoundError: No module named 'jax'` error. What am I doing wrong please?  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Error importing jax," Description I have created a fresh environemnt `conda create name myenv`. Then I activate the environment and the very first package I am installing is `jax` with the command: `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` That looks to install everything in this folder under my home directory: `$HOME/.local/lib/python3.10/sitepackages` However when I install the other packages that I need to work with, these go into the typical location for conda enviroments: `$HOME/miniconda3/envs/myenv/lib/python3.8/sitepackages` Hence, when I try to import jax from within my new environment, I got the `ModuleNotFoundError: No module named 'jax'` error. What am I doing wrong please?  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-08-26T11:42:20Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/17309,"When you install a package with `pip install`, it will be installed in the `python` environment associated with the `pip` executable you are using. From your description, it sounds like the `pip` executable you are using is not the one associated with your conda environment (you can check this by running `type pip`). Rather than digging into system paths to try to figure out how the `pip` command is being resolved, it's often easier to fix situations like this by specifying exactly which Python environment you want to run `pip` in; you can do this with  This will ensure that JAX is installed in the packages directory associated with the Python executable you are using.",Oh ok! yes you are right! I installed pip with `sudo apt install python3pip` and that messed thinks up with the conda pip. Do you know by the way how to unistall jax (and all its dependencies like nvidia cuda binaries) that sit under my  `$HOME/.local/lib/python3.10/sitepackages` I guess the whole python folder in this location`$HOME/.local/lib/python3.10/` needs removal to be honest.  Many thanks!,"This line should should get those JAX, JAXLIB, and the immediate NVIDIA dependencies. (link to list of packages) pip uninstall y jax jaxlib nvidiacublascu12 nvidiacudacupticu12 nvidiacudanvcccu12 nvidiacudaruntimecu12 nvidiacudnncu12 nvidiacufftcu12 nvidiacusolvercu12 nvidiacusparsecu12 These are other required packages, but may have already been installed: ml_dtypes numpy opt_einsum scipy  If your python is <3.10, then it should have also installed ""importlib_metadata"" I imagine some of these packages had their own dependencies, which may have fetched more from there... if you want it 100% clean, it may be easier to just delete your Python packages directory and reinstall your other packages.","Thanks so much! I am now left with these.   As you said these must be dependencies fetched from the packages that have just been removed. The timestamp of all these dependencies that I list above matches the time when I was trying to install jax. They are the only contents under `~/.local/lib/python3.10/sitepackages` Just want to double check, would you guys think it will be ok to just manually delete inside `~/.local/lib/python3.10/sitepackages`. My miniconda is in a totally different location anyway.","I think of those packages only those ending with `cu12` might have been installed as `jax` dependencies. I'm not sure if it's safe for you to delete that directory: there's no way for us to tell without knowing your setup. Sounds like this issue is resolved, so closing..."
551,"以下是一个github上的jax下的一个issue, 标题是(TinyMap: An XLA map with linear-time operations)， 内容是 (Hey JAX folks, I needed a map in XLA so I created this TinyMap class:  You can use it like this:  My questions: 1. Does this already exist somewhere? I tried to find something like this before I implemented it, but perhaps I used the wrong search terms. 1. If not, should I contribute it somewhere, and if so, where? Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,TinyMap: An XLA map with linear-time operations,"Hey JAX folks, I needed a map in XLA so I created this TinyMap class:  You can use it like this:  My questions: 1. Does this already exist somewhere? I tried to find something like this before I implemented it, but perhaps I used the wrong search terms. 1. If not, should I contribute it somewhere, and if so, where? Thanks!",2023-08-25T18:21:53Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/17298
1293,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__')， 内容是 ( Description 1  jax only consumes arrays with __dlpack__ and __dlpack_device__ denders (torch and numpy arrays). It can't consumes PyCapsule which makes it not compatible with frameworks which only produce PyCapsule (tensorflow and paddle). 2  Jax has __dlpack__ and __dlpack_device__ denders but still torch can't consume jax arrays?  Error: ```  AttributeError                            Traceback (most recent call last) Cell In[58], line 2       1 import torch > 2 torch.from_dlpack(jax_exp) File /opt/fw/torch/torch/utils/dlpack.py:100, in from_dlpack(ext_tensor)      49 """"""from_dlpack(ext_tensor) > Tensor      50       51 Converts a tensor from an external library into a ``torch.Tensor``.    (...)      97       98 """"""      99 if hasattr(ext_tensor, '__dlpack__'): > 100     device = ext_tensor.__dlpack_device__()     101      device is either CUDA or ROCm, we need to pass the current     102      stream     103     if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM): AttributeError: 'ArrayImpl' object has no attribute ')请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AttributeError: 'ArrayImpl' object has no attribute '__dlpack_device__'," Description 1  jax only consumes arrays with __dlpack__ and __dlpack_device__ denders (torch and numpy arrays). It can't consumes PyCapsule which makes it not compatible with frameworks which only produce PyCapsule (tensorflow and paddle). 2  Jax has __dlpack__ and __dlpack_device__ denders but still torch can't consume jax arrays?  Error: ```  AttributeError                            Traceback (most recent call last) Cell In[58], line 2       1 import torch > 2 torch.from_dlpack(jax_exp) File /opt/fw/torch/torch/utils/dlpack.py:100, in from_dlpack(ext_tensor)      49 """"""from_dlpack(ext_tensor) > Tensor      50       51 Converts a tensor from an external library into a ``torch.Tensor``.    (...)      97       98 """"""      99 if hasattr(ext_tensor, '__dlpack__'): > 100     device = ext_tensor.__dlpack_device__()     101      device is either CUDA or ROCm, we need to pass the current     102      stream     103     if device[0] in (DLDeviceType.kDLGPU, DLDeviceType.kDLROCM): AttributeError: 'ArrayImpl' object has no attribute '",2023-08-25T04:08:34Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17285,"For 1., that's not true: you can pass capsules to `jax.dlpack.from_dlpack`. (Note: please do *not* import things from `jax._src` since that's private and we can and will break your code.) 2. Actually, support for this just landed at head. `__dlpack_device__` was only just implemented very recently. If you build JAX and jaxlib at head, it should work. Please try?","Hi , `__dlpack_device__` is working very fine at head. Thanks for assistance :) But for 1. dlpack module exists in jax but I think you forgot to import in `__init__.py`, that's why it's not available publicly and I've to use private `to_dlpack`. If that's the case, please let me know I'm very happy to make my first contribution to jax. Thanks again!","For 1. yes it appears not to be imported by default. But irrespective of that, you can import it: just `import jax.dlpack`. No need for private APIs. I'll comment on the PR also."
349,"以下是一个github上的jax下的一个issue, 标题是(Deprecate jax.numpy.trapz.)， 内容是 (Expose the current implementation of jax.numpy.trapz as jax.scipy.integrate.trapezoid instead. Fixes https://github.com/google/jax/issues/17244)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Deprecate jax.numpy.trapz.,Expose the current implementation of jax.numpy.trapz as jax.scipy.integrate.trapezoid instead. Fixes https://github.com/google/jax/issues/17244,2023-08-24T20:04:51Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17277
1272,"以下是一个github上的jax下的一个issue, 标题是(feat(pallas): support MQA/GQA and FlashAttention-2)， 内容是 (Please:  [X] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Relevant files:  https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/attention.py Pallas provides a way of writing jaxpr/JAX DSL that get converted to tiled kernels. The original flash attention implementation exists in the above linked file. But it is only for MQA and does not have the optimizations in flash attention 2. (https://princetonnlp.github.io/flashatttention2/).  In particular, flash attention 2: 1. uses a work partitioning that was found to perform better (in A100/H100)     actually seems to have already been implemented        https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL92       https://github.com/google/jax/blob/841baabd3ffa63b27fa1c4199adc65fbf073c7c0/jax/experimental/pallas/ops/attention.pyL46     ~~or perhaps Triton does not expose how work is partitioned across warps...? Is making this deci)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,feat(pallas): support MQA/GQA and FlashAttention-2,"Please:  [X] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Relevant files:  https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/attention.py Pallas provides a way of writing jaxpr/JAX DSL that get converted to tiled kernels. The original flash attention implementation exists in the above linked file. But it is only for MQA and does not have the optimizations in flash attention 2. (https://princetonnlp.github.io/flashatttention2/).  In particular, flash attention 2: 1. uses a work partitioning that was found to perform better (in A100/H100)     actually seems to have already been implemented        https://github.com/google/jax/blob/36cdafdcf400e5f201ebffe94c9d048c5d2ae952/jax/experimental/pallas/ops/attention.pyL92       https://github.com/google/jax/blob/841baabd3ffa63b27fa1c4199adc65fbf073c7c0/jax/experimental/pallas/ops/attention.pyL46     ~~or perhaps Triton does not expose how work is partitioned across warps...? Is making this deci",2023-08-24T06:34:39Z,enhancement,open,1,0,https://github.com/jax-ml/jax/issues/17267
1222,"以下是一个github上的jax下的一个issue, 标题是(JAX_DISABLE_JIT=1 doesn't disable (xmap's) jitting)， 内容是 ( Description I'm trying to run a multihost dataparallel job using JAX. Due to working with a nested mess of xmapscangradpmapscangrad (multihost outer optimization loop, multidevice inner optimization loop), I'm trying to disable jit altogether in an attempt to avoid some suspected performance gotchas. However, means of disabling jitting don't seem to affect xmap. In fact, I suspect it is disabling jit that (ironically) yields obscene compilation times of minutes to hours in my case (depending on configuration), because the subxmap logic gets unrolled into nonjitted versions, and xmap jits the entirety of that, maybe? This issue was already mentioned in a comment on a closed issue, but seemed not to have much visibility. A minimal repro can also be found there. Opening a dedicated issue for better visibility.  What jax/jaxlib version are you using? jax==0.4.14 jaxlib==0.4.14  Which accelerator(s) are you using? CPU, TPU  Additional system info 3.11  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JAX_DISABLE_JIT=1 doesn't disable (xmap's) jitting," Description I'm trying to run a multihost dataparallel job using JAX. Due to working with a nested mess of xmapscangradpmapscangrad (multihost outer optimization loop, multidevice inner optimization loop), I'm trying to disable jit altogether in an attempt to avoid some suspected performance gotchas. However, means of disabling jitting don't seem to affect xmap. In fact, I suspect it is disabling jit that (ironically) yields obscene compilation times of minutes to hours in my case (depending on configuration), because the subxmap logic gets unrolled into nonjitted versions, and xmap jits the entirety of that, maybe? This issue was already mentioned in a comment on a closed issue, but seemed not to have much visibility. A minimal repro can also be found there. Opening a dedicated issue for better visibility.  What jax/jaxlib version are you using? jax==0.4.14 jaxlib==0.4.14  Which accelerator(s) are you using? CPU, TPU  Additional system info 3.11  NVIDIA GPU info _No response_",2023-08-24T06:28:38Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/17266,Can I suggest using shard_map instead?  https://github.com/google/jax/blob/0ddbe76cba371c5290bfb7a317f83c89c2c2641d/docs/jep/14273shardmap.md,"Thanks for the suggestion, , will definitely take a look!",Is there a way to disable jit for xmapped models (say for existing codebases)?,"xmap was removed from JAX, so this is obsolete"
557,"以下是一个github上的jax下的一个issue, 标题是(Indexing error when looping over zero length arrays)， 内容是 ( Description MWE:   Gives an index error:  Given that `modes_old.shape[0] == 0`, this shouldn't even be entering the body, but it seems like it is?   What jax/jaxlib version are you using? jax==0.4.11, jaxlib==0.4.11  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Indexing error when looping over zero length arrays," Description MWE:   Gives an index error:  Given that `modes_old.shape[0] == 0`, this shouldn't even be entering the body, but it seems like it is?   What jax/jaxlib version are you using? jax==0.4.11, jaxlib==0.4.11  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-08-24T02:56:14Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17264,"Thanks for the report! I believe this is expected behavior: JAX will always trace the body of the `fori_loop`, even in cases where the loop is executed zero times. See the similar discussion for `while_loop` in CC(lax.while_loop fails with zero iterations). The suggested fix would be to not call `fori_loop` in this kind of problematic case – there are examples in JAX's codebase where behavior similar to what you're seeing is anticipated and accounted for; for example: https://github.com/google/jax/blob/b4f7928a8154435c92f6ce9d7f904c9f99c7132b/jax/_src/lax/linalg.pyL1133L1137","Ahh I see, thanks! "
253,"以下是一个github上的jax下的一个issue, 标题是(Deprecate jax.numpy.in1d.)， 内容是 (Issue https://github.com/google/jax/issues/17244)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Deprecate jax.numpy.in1d.,Issue https://github.com/google/jax/issues/17244,2023-08-23T23:30:38Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17261
643,"以下是一个github上的jax下的一个issue, 标题是(Add `TransferToMemoryKind` as a private API to allow device_put to transfer to different memories without specifying the sharding and allowing the SPMD partitioner to choose the sharding for the intermediate.)， 内容是 (Add `TransferToMemoryKind` as a private API to allow device_put to transfer to different memories without specifying the sharding and allowing the SPMD partitioner to choose the sharding for the intermediate. Exposing it as a public API can be done later.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add `TransferToMemoryKind` as a private API to allow device_put to transfer to different memories without specifying the sharding and allowing the SPMD partitioner to choose the sharding for the intermediate.,Add `TransferToMemoryKind` as a private API to allow device_put to transfer to different memories without specifying the sharding and allowing the SPMD partitioner to choose the sharding for the intermediate. Exposing it as a public API can be done later.,2023-08-23T04:11:13Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17241
749,"以下是一个github上的jax下的一个issue, 标题是(memory leak when calling odeint)， 内容是 ( Description I managed to make the following MWE  and you can see the memory usage keep growing, while scipy.odeint has no such issue  thus I suspect there are memory leak somewhere. This happens to diffrax as well, and it seems to be an issue with jax itself. See also kidger's comments in https://github.com/patrickkidger/diffrax/issues/142   What jax/jaxlib version are you using? jax/jaxlib 0.4.14  Which accelerator(s) are you using? CUDA  Additional system info _No response_  NVIDIA GPU info this behaviour can be reproduced on CPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,memory leak when calling odeint," Description I managed to make the following MWE  and you can see the memory usage keep growing, while scipy.odeint has no such issue  thus I suspect there are memory leak somewhere. This happens to diffrax as well, and it seems to be an issue with jax itself. See also kidger's comments in https://github.com/patrickkidger/diffrax/issues/142   What jax/jaxlib version are you using? jax/jaxlib 0.4.14  Which accelerator(s) are you using? CUDA  Additional system info _No response_  NVIDIA GPU info this behaviour can be reproduced on CPU",2023-08-22T20:03:00Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17232,"I believe this is due to CC(`jax.jit(..., static_argnums=...)` keeps strong (not weak) reference in cache, causing memory leak). I think everyone agrees something should probably be done about it. ? FWIW this issue doesn't usually turn up very often in reallife programs. It's fairly rare to dynamically define functions in a loop outside of JIT. E.g. I don't think I've ever needed this. (Side note  Roger, it's great to see your interest in JAX!)","> FWIW this issue doesn't usually turn up very often in reallife programs. It's fairly rare to dynamically define functions in a loop outside of JIT. E.g. I don't think I've ever needed this. Indeed, I realize in my case I should just use the parameters instead of creating a new equation object. I think I'm gonna close this issue then given this is reported already. > (Side note  Roger, it's great to see your interest in JAX!) I've been writing JAX since Jan actually ;)"
911,"以下是一个github上的jax下的一个issue, 标题是(""cudaErrorMemoryAllocation : out of memory"" when more than one program involving JAX and cuDNN)， 内容是 ( Description Hi, The thing is, when i first create a code involving JAX and cuDNN, everything works just as fine. But if i start running a completely same code, the second will instantly crash and report the following information:  Note that the code involve JAX.JIT and i did put these two code into seperate GPUs.  May i know what happened? thanks~  What jax/jaxlib version are you using? I install via the following command: pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? GPU  Additional system info Linux Ubuntu 22.04  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"""cudaErrorMemoryAllocation : out of memory"" when more than one program involving JAX and cuDNN"," Description Hi, The thing is, when i first create a code involving JAX and cuDNN, everything works just as fine. But if i start running a completely same code, the second will instantly crash and report the following information:  Note that the code involve JAX.JIT and i did put these two code into seperate GPUs.  May i know what happened? thanks~  What jax/jaxlib version are you using? I install via the following command: pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? GPU  Additional system info Linux Ubuntu 22.04  NVIDIA GPU info ",2023-08-22T07:47:21Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17217,"are you using any special array sharding methods like pjit ,pmap or anything like that? The error message ""cudaErrorMemoryAllocation : out of memory""  can happen for a few reasons: 1. The code is trying to allocate too much memory. 2. The GPU is already running low on memory due to other processes. 3. There is a memory leak in the code. To troubleshoot this issue, you can try the following: 1. Check the code to make sure that it is not trying to allocate too much memory. 2. Close any other processes that are using the GPU. 3. Use the gdb debugger to find memory leaks in the code. You can also try setting the environment variable XLA_PYTHON_CLIENT_PREALLOCATE=false. This will disable the preallocation behavior, which can sometimes help to reduce memory usage.  and if you want to have a better look at the detail of the processing code and know exactly about the size of the buffer or executable you can use Jaxsmi or my code  ","Closing as a duplicate of https://github.com/google/jax/issues/12461 The gist of it is as mentioned above: JAX preallocates memory, and this can lead to memory starvation which renders CUDA initialization impossible. See this comment from the linked issue, as well as this section of the FAQ."
585,"以下是一个github上的jax下的一个issue, 标题是(bf16 * int8 matmul results in incorrect value)， 内容是 ( Description  I have been trying to find a DL framework that does bf16 and int8 matrix multiplication, so far only Jax supports it, but it seems to have this rounding issue at the moment.  What jax/jaxlib version are you using? 0.3.20+cuda11.cudnn82  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info A100)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,bf16 * int8 matmul results in incorrect value," Description  I have been trying to find a DL framework that does bf16 and int8 matrix multiplication, so far only Jax supports it, but it seems to have this rounding issue at the moment.  What jax/jaxlib version are you using? 0.3.20+cuda11.cudnn82  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info A100",2023-08-22T03:27:26Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/17214,"Thanks for the question! I believe this is working as expected: you're doing math at `bfloat16` precision, and `bfloat16` only has 7 bits of mantissa, meaning that you should generally expect numerical results to be good to within roughly one part in $2^7$. Doing this computation in `float32` reveals the ""true"" result:  In `bfloat16`, you got `208`, which is actually the closest bfloat16representable value to the true answer. You can see this by using the `jnp.nextafter` function to see what the next representable value is:  The next bfloat16representable value greater than `208` is `207`, so it's clear that `208` is the best possible bfloat16 representation of the answer to your computation. The reason your manual matmul returns this incorrect value is because by splitting the ops you incur bfloat16 rounding errors twice instead of once. Hope that helps!","Great, thank you for the help!","I guess this implies that matmul internally converts the bf16/int8 arrays to fp32 for both multiplication and accumulation?  but this means we cast everything to fp32 such that the acceleration from lowbit computation is lost. Thus, what I would have expected is:  I am unfamiliar with A100's internal instruction, but I would have thought the bf16/int8 matrix multiplication is performed in lowbit for mul and highbit for add, in order to reduce accumulation error whilst maintaining a performance edge.","The implementation of bfloat16 matmul is hardwarespecific, and I’m not sure of the details on A100."
405,"以下是一个github上的jax下的一个issue, 标题是(Improve type annotations for jax.numpy.)， 内容是 (Improve type annotations for jax.numpy. * Allow sequences of axes to jnp.flip, rather than mandating tuples. Users sometimes pass lists here. * Allow arraylike pad_width values to pad().)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve type annotations for jax.numpy.,"Improve type annotations for jax.numpy. * Allow sequences of axes to jnp.flip, rather than mandating tuples. Users sometimes pass lists here. * Allow arraylike pad_width values to pad().",2023-08-21T21:05:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17207
599,"以下是一个github上的jax下的一个issue, 标题是(xmap + expm bug)， 内容是 ( Description Not sure if I'm misusing `xmap`, but the following code:  produces the error:  My understanding is that the above usage of `xmap` should produce the same output as the default behaviour of `vmap` (i.e. mapping over the 0th axis).  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info python 3.11, macOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,xmap + expm bug," Description Not sure if I'm misusing `xmap`, but the following code:  produces the error:  My understanding is that the above usage of `xmap` should produce the same output as the default behaviour of `vmap` (i.e. mapping over the 0th axis).  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info python 3.11, macOS  NVIDIA GPU info _No response_",2023-08-21T15:34:13Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17201,"Hi   Looks like this issue appears to be resolved. I tested this with JAX version 0.4.26 on Colab CPU. It gives the same result as that of `vmap` `xmap`:  Output:  `vmap`:  Output:  Please find the gist for reference. I have also tested this on Macbook Pro with M1 Pro chip and it works there also. please find the screenshot for reference.  However, it is recommended transitioning away from `xmap` as it is deprecated and will be removed in future versions. Consider using `jax.vmap` with the `spmd_axis_name` argument or `jax.experimental.shard_map.shard_map` instead as suggested by warning message. You can find the documentation on `jax.experimental.shard_map.shard_map` here:https://jax.readthedocs.io/en/latest/notebooks/shard_map.html  Thank you.","Ah thanks  , I wasn't aware that `xmap` had been deprecated.",Hi   Please feel free to close the issue if resolved. Thank you.
732,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] call_module() got an unexpected keyword argument 'function_list')， 内容是 ( Description When trying to run minimal example for jax2tf.convert using native_serialization, I'm getting the following error: tensorflow==2.12.0 jax==0.4.14 **Code**  **Error**   What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14, tensorflow v2.12.0,   Which accelerator(s) are you using? CPU and/or GPU  Additional system info python==3.10, linux or macos silicon  NVIDIA GPU info  NVIDIASMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6      ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] call_module() got an unexpected keyword argument 'function_list'," Description When trying to run minimal example for jax2tf.convert using native_serialization, I'm getting the following error: tensorflow==2.12.0 jax==0.4.14 **Code**  **Error**   What jax/jaxlib version are you using? jax v0.4.14, jaxlib v0.4.14, tensorflow v2.12.0,   Which accelerator(s) are you using? CPU and/or GPU  Additional system info python==3.10, linux or macos silicon  NVIDIA GPU info  NVIDIASMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6      ++",2023-08-20T15:28:37Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17194,"Hi  thanks for the report. It looks like this part of jax2tf is incompatible with the current tensorflow release: I can reproduce this error with v2.13.0, but the error goes away with the 2.14.0 release candidate:  I think the best fix here will be to use the newer tensorflow version.","Indeed, you need to use a more recent version of tensorflow."
1036,"以下是一个github上的jax下的一个issue, 标题是(XLA_FLAGS is only parsed once under JAX, which lead to side effects in unit test)， 内容是 ( Description I was trying to setup my unit test to run with 8 virtual CPUs for distribution testing, and by following https://github.com/google/jax/blob/main/tests/pmap_test.pyL90L110, I was able to config 8 CPUs. However, the teardown logic doesn't actually reset the number of CPU device back. The root cause seems to be that the XLA_FLAGS is only parsed once by XLA in https://github.com/tensorflow/tensorflow/blob/e9391d9005ee38e0803026eeee34ecbec5995750/tensorflow/compiler/xla/debug_options_flags.ccL1230. Consider the following snippet, once the backend is initialized, the number of devices can't be modified again.   What jax/jaxlib version are you using? head  Which accelerator(s) are you using? CPU  Additional system info python 3.8  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"XLA_FLAGS is only parsed once under JAX, which lead to side effects in unit test"," Description I was trying to setup my unit test to run with 8 virtual CPUs for distribution testing, and by following https://github.com/google/jax/blob/main/tests/pmap_test.pyL90L110, I was able to config 8 CPUs. However, the teardown logic doesn't actually reset the number of CPU device back. The root cause seems to be that the XLA_FLAGS is only parsed once by XLA in https://github.com/tensorflow/tensorflow/blob/e9391d9005ee38e0803026eeee34ecbec5995750/tensorflow/compiler/xla/debug_options_flags.ccL1230. Consider the following snippet, once the backend is initialized, the number of devices can't be modified again.   What jax/jaxlib version are you using? head  Which accelerator(s) are you using? CPU  Additional system info python 3.8  NVIDIA GPU info _No response_",2023-08-18T23:27:08Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17188,"The issue you are facing is due to the way that XLA parses the XLA_FLAGS environment variable. As you mentioned, XLA only parses the XLA_FLAGS environment variable once, and it does not reparse the environment variable if it is changed after the backend has been initialized. I think you can fix this by reinitializing the backend like  "
1266,"以下是一个github上的jax下的一个issue, 标题是(Jaxlib fails to build from source on Windows)， 内容是 ( Description Here's the complete output (dljax) C:\Users\shash\jax>python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2"" cuda_version=""12.2"" cudnn_version=""8.9.4""      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: C:\bazel\bazel.EXE Bazel version: 6.3.2 Python binary path: C:/Users/shash/.conda/envs/dljax/python.exe Python version: 3.9 NumPy version: 1.25.2 MKLDNN enabled: yes Target CPU: AMD64 Target CPU features: release CUDA enabled: yes CUDA toolkit path: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 CUDNN library path: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 CUDA version: 12.2 CUDNN version: 8.9.4 NCCL enabled: yes TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... C:\bazel\bazel.EXE run verbose_failures=true //jaxlib/tools:build_wheel  output_path=C:\Users\shash\jax\dist cpu=AMD64 Extracting Bazel installation.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Jaxlib fails to build from source on Windows," Description Here's the complete output (dljax) C:\Users\shash\jax>python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2"" cuda_version=""12.2"" cudnn_version=""8.9.4""      _   _  __  __      / ___ \/  \  \___/_/   \/_/\_\ Bazel binary path: C:\bazel\bazel.EXE Bazel version: 6.3.2 Python binary path: C:/Users/shash/.conda/envs/dljax/python.exe Python version: 3.9 NumPy version: 1.25.2 MKLDNN enabled: yes Target CPU: AMD64 Target CPU features: release CUDA enabled: yes CUDA toolkit path: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 CUDNN library path: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2 CUDA version: 12.2 CUDNN version: 8.9.4 NCCL enabled: yes TPU enabled: no ROCm enabled: no Building XLA and installing it in the jaxlib source tree... C:\bazel\bazel.EXE run verbose_failures=true //jaxlib/tools:build_wheel  output_path=C:\Users\shash\jax\dist cpu=AMD64 Extracting Bazel installation.",2023-08-18T16:28:52Z,bug build contributions welcome Windows,closed,0,6,https://github.com/jax-ml/jax/issues/17174,Windows builds are community supported. I'll note that jaxlib built fine in the most recent CI run from today: https://github.com/google/jax/actions/workflows/windows_ci.yml Can you try building from a checkout at github head?,"Hey there. Thank you very much for replying. Can you please tell me what is this flatc.exe error and how to correct it. On Fri, 18 Aug 2023, 23:04 Peter Hawkins, ***@***.***> wrote: > Windows builds are community supported. > > I'll note that jaxlib built fine in the most recent CI run from today: > https://github.com/google/jax/actions/workflows/windows_ci.yml > > Can you try building from a checkout at github head? > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","`flatc` is a binary built as part of the jaxlib build process. Did you try from github head, as I already suggested?","I also strongly recommend installing a binary release of JAX, rather than building it yourself. I'm not sure that CUDA support even builds on Windows right now, for what its worth. So it's CPUonly or nothing. Another option is to use the Windows Subsystem for Linux and install the Linux GPU build.","I'm sorry but I don't understand what is meant by checking out by github head.  Can you please explain On Fri, 18 Aug 2023, 23:16 Peter Hawkins, ***@***.***> wrote: > flatc is a binary built as part of the jaxlib build process. > > Did you try from github head, as I already suggested? > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >",Please use a binary release of jaxlib (`pip install jax[cpu]`). Hope that helps!
516,"以下是一个github上的jax下的一个issue, 标题是(fail to download  got 404 code when to build JAX from source in Mac M1)， 内容是 ( Description TRy to build Jax from source in M1 with doc:  https://developer.apple.com/metal/jax/ I got warnings :   What jax/jaxlib version are you using? 0.4.10  Which accelerator(s) are you using? CPU  Additional system info Mac(m1)  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,fail to download  got 404 code when to build JAX from source in Mac M1, Description TRy to build Jax from source in M1 with doc:  https://developer.apple.com/metal/jax/ I got warnings :   What jax/jaxlib version are you using? 0.4.10  Which accelerator(s) are you using? CPU  Additional system info Mac(m1)  NVIDIA GPU info _No response_,2023-08-18T06:52:01Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/17167,"This is a warning, not an error: it simply means we didn't add some of those files to our mirror. However the mirror is only there for redundancy: if github is up, you don't need it. But sometimes github goes down :) Closing because it's fine to ignore the warnings for now. Hope that helps!"
379,"以下是一个github上的jax下的一个issue, 标题是(deprecate jax.numpy.issubsctype)， 内容是 (Part of CC(ENH: Reflect changes from numpy namespace refactor Part 3). This anticipates removal of this function in numpy 2.0: https://github.com/numpy/numpy/pull/24376)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,deprecate jax.numpy.issubsctype,Part of CC(ENH: Reflect changes from numpy namespace refactor Part 3). This anticipates removal of this function in numpy 2.0: https://github.com/numpy/numpy/pull/24376,2023-08-17T16:44:00Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17160
816,"以下是一个github上的jax下的一个issue, 标题是(BFGS gets lost in Poisson regression example)， 内容是 ( Description I'm working on a Poisson regression example. I define the objective in JAX and then optimize it in two different ways: 1. With jax.scipy.optimize 2. With scipy.optimize I tried to keep the two setups as similar as possible. The JAX implementation seems to get stuck with a line search error (maybe related https://github.com/google/jax/issues/4594) and quite a large gradient. Scipy terminates successfully.  Output   What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info 3.10.12, MacOS 13.5  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,BFGS gets lost in Poisson regression example," Description I'm working on a Poisson regression example. I define the objective in JAX and then optimize it in two different ways: 1. With jax.scipy.optimize 2. With scipy.optimize I tried to keep the two setups as similar as possible. The JAX implementation seems to get stuck with a line search error (maybe related https://github.com/google/jax/issues/4594) and quite a large gradient. Scipy terminates successfully.  Output   What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPU  Additional system info 3.10.12, MacOS 13.5  NVIDIA GPU info _No response_",2023-08-17T15:20:18Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/17159,"Thanks for the report. We're not doing much maintenance on `jax.scipy.optimize`, and longterm we're planning to remove it. For optimization in JAX, I'd suggest checking out the jaxopt  package.","Ah thanks, I hadn't heard of that package before.","jaxopt is a dead project now too, and optax doesn't have this capability... Anyway if jax.scipy.optimize.minimize is known to be broken, the fix in https://github.com/google/jax/issues/16236 will not be merged, and it will someday be removed  maybe the documentation could be updated to say so?"
1112,"以下是一个github上的jax下的一个issue, 标题是(type <class 'jaxlib.xla_extension.ArrayImpl'>is non-hashable when I use a class member function, while there is no problem when I use a general function.)， 内容是 ( Description Hi, I am confused about using . I would not like the variable neg was traced, so I added the decorator  as the docs.  There was no problem when I ran the following code:  However, when I added the function f to a class, **ValueError** appeared.  > ValueError: Nonhashable static arguments are not supported, as this can lead to unexpected cachemisses. Static argument (index 1) of type  for function f is nonhashable. I am new to JAX. I want to know why the problem appears and how can I solve it? Are there any methods that gracefully convert **ArrayImpl**  to other hashable types?  What jax/jaxlib version are you using? jax 0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python: 3.8, OS: WSL Ubuntu 22.04.2  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"type <class 'jaxlib.xla_extension.ArrayImpl'>is non-hashable when I use a class member function, while there is no problem when I use a general function."," Description Hi, I am confused about using . I would not like the variable neg was traced, so I added the decorator  as the docs.  There was no problem when I ran the following code:  However, when I added the function f to a class, **ValueError** appeared.  > ValueError: Nonhashable static arguments are not supported, as this can lead to unexpected cachemisses. Static argument (index 1) of type  for function f is nonhashable. I am new to JAX. I want to know why the problem appears and how can I solve it? Are there any methods that gracefully convert **ArrayImpl**  to other hashable types?  What jax/jaxlib version are you using? jax 0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python: 3.8, OS: WSL Ubuntu 22.04.2  NVIDIA GPU info _No response_",2023-08-17T15:17:32Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/17158,"The reason it differs is because `self` counts as an argument – so you'd have to use `static_argnums=(2,)` for the class method if you want `neg` to be static. However, that doesn't tell the whole story, because you also need to tell JAX how to appropriately handle `self` in this case. That's a frequently asked question with a notsosimple answer that we cover here: https://jax.readthedocs.io/en/latest/faq.htmlhowtousejitwithmethods","> The reason it differs is because `self` counts as an argument – so you'd have to use `static_argnums=(2,)` for the class method if you want `neg` to be static. >  > However, that doesn't tell the whole story, because you also need to tell JAX how to appropriately handle `self` in this case. That's a frequently asked question with a notsosimple answer that we cover here: https://jax.readthedocs.io/en/latest/faq.htmlhowtousejitwithmethods Thanks  !  I carefully read how to use jax in class and it helps me a lot to understand jax. But I still want to solve the problem caused by static arguments not supporting nonhashable type.  I have a program where the intermediate result computed by some arrays is **type ArrayImpl**.  I would like to use the intermediate result as the condition (if else). So I must use   to ensure the intermediate result is not traced. This creates a contradiction. After I go through other issues, I still can't come up with a appropriate solution. Do you have any ideas? ","Within JIT, you cannot use a JAX array as a static condition. I'd suggest taking a look at JAX Sharp Bits: Control Flow, and in particular the subsection Python Control Flow + JIT. If you want code to be conditioned on a dynamic value, you should use `lax.cond` rather than `if`/`else`.","> Within JIT, you cannot use a JAX array as a static condition. I'd suggest taking a look at JAX Sharp Bits: Control Flow, and in particular the subsection Python Control Flow + JIT. >  > If you want code to be conditioned on a dynamic value, you should use `lax.cond` rather than `if`/`else`. Thanks for your suggestions  .I tried another way to achieve my algorithm and it ran correctly.  It's very kind of you. I really appreciate it. "
419,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Relax the limit on the number of error inputs for shape assertions)， 内容是 (Lift the limit from 4 to 32 to follow the change in tf.XlaCallModule of this limit. Currently, the error message formatter needs at most 6 error message inputs.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Relax the limit on the number of error inputs for shape assertions,"Lift the limit from 4 to 32 to follow the change in tf.XlaCallModule of this limit. Currently, the error message formatter needs at most 6 error message inputs.",2023-08-17T09:21:03Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/17154
448,"以下是一个github上的jax下的一个issue, 标题是(Create lax.polygamma with native HLO lowering)， 内容是 (~Trying this out... not sure whether `polygamma` is implemented on all backends.~ Tests pass on CPU, GPU & TPU... let's ship it! Inspired by CC(Please consider implementing differentiation for the Hessian of gamma variates))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Create lax.polygamma with native HLO lowering,"~Trying this out... not sure whether `polygamma` is implemented on all backends.~ Tests pass on CPU, GPU & TPU... let's ship it! Inspired by CC(Please consider implementing differentiation for the Hessian of gamma variates)",2023-08-16T17:51:51Z,pull ready,closed,1,5,https://github.com/jax-ml/jax/issues/17142,"Wow, thanks for the quick solution! Will this solve CC(Please consider implementing differentiation for the Hessian of gamma variates)?  I'm trying it like this: ",I guess the same thing would need to be added for `gamma_grad` primitive?,"No, this won't solve CC(Please consider implementing differentiation for the Hessian of gamma variates). The issue there is that there is no autodiff rule for `gamma_grad`.","Okay, but it's possible to write one?  Should I invest some time in doing what you did for the `gamma_grad` primitive? ",See https://github.com/google/jax/issues/16076issuecomment1681273130 – let's keep discussions of CC(Please consider implementing differentiation for the Hessian of gamma variates) in CC(Please consider implementing differentiation for the Hessian of gamma variates)
1278,"以下是一个github上的jax下的一个issue, 标题是(Ensure that CompileOptions serializes deterministically.)， 内容是 (Ensure that CompileOptions serializes deterministically. CompileOptions has two serialization mechanisms: Py pickle and SerializeAsString. Neither mechanism serializes deterministically. Deterministic serialization (also called idempotent serialization or inorder serialization) ensures that a given structure serializes to the same string repeatedly. Both these mechanisms serialize by first generating the proto and then serializing it. There are three points to note: . Deterministic serialization will yield the same result   even if proto map fields are in a different order. Thus   map({""1"": 1, ""2"": 2}) and map({""2"": 2, ""1"": 1}) will   serialize the same. . Deterministic serialization does not yield the same   result for repeated fields that are out of order. Thus,   for message Foo { repeated string s = 1; },   Foo{s: ""1"", s: ""2""} will not result in the same   serialization as Foo{s: ""2"", s: ""1""}. . Deterministic serialization applies only in the context   of a given binary. It does not apply across releases. Testing: the orig)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Ensure that CompileOptions serializes deterministically.,"Ensure that CompileOptions serializes deterministically. CompileOptions has two serialization mechanisms: Py pickle and SerializeAsString. Neither mechanism serializes deterministically. Deterministic serialization (also called idempotent serialization or inorder serialization) ensures that a given structure serializes to the same string repeatedly. Both these mechanisms serialize by first generating the proto and then serializing it. There are three points to note: . Deterministic serialization will yield the same result   even if proto map fields are in a different order. Thus   map({""1"": 1, ""2"": 2}) and map({""2"": 2, ""1"": 1}) will   serialize the same. . Deterministic serialization does not yield the same   result for repeated fields that are out of order. Thus,   for message Foo { repeated string s = 1; },   Foo{s: ""1"", s: ""2""} will not result in the same   serialization as Foo{s: ""2"", s: ""1""}. . Deterministic serialization applies only in the context   of a given binary. It does not apply across releases. Testing: the orig",2023-08-16T00:34:28Z,,closed,0,1,https://github.com/jax-ml/jax/issues/17133,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
301,"以下是一个github上的jax下的一个issue, 标题是(Re-enable mixed-precision testDot on TPU)， 内容是 (Reenable mixedprecision testDot on TPU The underlying issue has been fixed in XLA)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Re-enable mixed-precision testDot on TPU,Reenable mixedprecision testDot on TPU The underlying issue has been fixed in XLA,2023-08-15T22:56:05Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17131
562,"以下是一个github上的jax下的一个issue, 标题是(Remove the `canonicalize_dtypes` argument from mlir.ir_constant(s).)， 内容是 (Remove the `canonicalize_dtypes` argument from mlir.ir_constant(s). Instead, force the caller to explicitly canonicalize the argument if that's what they want. The current behavior (canonicalize by default) is not the behavior we want to encourage: we want to canonicalize exactly where we need to and nowhere else.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Remove the `canonicalize_dtypes` argument from mlir.ir_constant(s).,"Remove the `canonicalize_dtypes` argument from mlir.ir_constant(s). Instead, force the caller to explicitly canonicalize the argument if that's what they want. The current behavior (canonicalize by default) is not the behavior we want to encourage: we want to canonicalize exactly where we need to and nowhere else.",2023-08-15T21:44:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17130
894,"以下是一个github上的jax下的一个issue, 标题是(Same functions but not same result)， 内容是 ( Description  Different results in Jax and torch and Numpy on CPU/TPU and GPU this is the function that I'm using to find are two arrays different or no  and this is the function that I'm trying to recreate from Torch (RoPE) and the part that my code is getting error or different results in calculating sin and cos and freq if you do the same as what iv done in jax for original numpy you wont get any error or False results    What jax/jaxlib version are you using? jax/jaxlib v0.4.13  Which accelerator(s) are you using? CPU/GPU/TPU  Additional system info Linux Ubuntu 22.04 LTS  NVIDIA GPU info GPU = RTX 3080 Cuda 11.8 driver 530 CPU = Intel® Core™ i79700K TPU = Kaggle TPUs)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Same functions but not same result, Description  Different results in Jax and torch and Numpy on CPU/TPU and GPU this is the function that I'm using to find are two arrays different or no  and this is the function that I'm trying to recreate from Torch (RoPE) and the part that my code is getting error or different results in calculating sin and cos and freq if you do the same as what iv done in jax for original numpy you wont get any error or False results    What jax/jaxlib version are you using? jax/jaxlib v0.4.13  Which accelerator(s) are you using? CPU/GPU/TPU  Additional system info Linux Ubuntu 22.04 LTS  NVIDIA GPU info GPU = RTX 3080 Cuda 11.8 driver 530 CPU = Intel® Core™ i79700K TPU = Kaggle TPUs,2023-08-15T11:17:39Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17121,"Thanks for the question! I think this is working as expected. In general, using exact equality checks for floating point computations is an antipattern: floating point operations are inexact representations of real numbers, and so will always have some degree of rounding error. This is not a JAXspecific thing: this is true of any library that uses floating point (which includes jax, torch, numpy, and almost all other modern computing frameworks) To compare equivalent floating point outputs, you need to account for this computation error. One function you could use for this is `numpy.allclose`, and indeed if you replace `is_same` with `numpy.allclose`, you'll see that the results in your code match. For a good practical read on rounding errors in floating point math, I'd check out this StackOverflow answer: Is Floating Point Math Broken?.",thanks<3
1095,"以下是一个github上的jax下的一个issue, 标题是(`where` related gradient bug in jax.nn.softmax and jax.nn.log_softmax (unprotected use of jnp.exp?))， 内容是 ( Description I see `nan`s in the gradient of `jax.nn.softmax` and `jax.nn.log_softmax`, specifically when I use `where`, and the logits that are filtered out by the `where` condition are large, but not large elsewhere in the logits tensor:  I believe there is an easy fix, and the bug is due to the fact that `jnp.exp` is being called over the values `x  x_max`, which may overflow since `x_max` is only taken over the values passing the `where` condition.  This is already warned against in the documentation for `jnp.where`: https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere  Instead, I tried this which provides proper gradients:   What jax/jaxlib version are you using? jax 0.4.2  Which accelerator(s) are you using? GPU  Additional system info Ubuntu  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`where` related gradient bug in jax.nn.softmax and jax.nn.log_softmax (unprotected use of jnp.exp?)," Description I see `nan`s in the gradient of `jax.nn.softmax` and `jax.nn.log_softmax`, specifically when I use `where`, and the logits that are filtered out by the `where` condition are large, but not large elsewhere in the logits tensor:  I believe there is an easy fix, and the bug is due to the fact that `jnp.exp` is being called over the values `x  x_max`, which may overflow since `x_max` is only taken over the values passing the `where` condition.  This is already warned against in the documentation for `jnp.where`: https://jax.readthedocs.io/en/latest/faq.htmlgradientscontainnanwhereusingwhere  Instead, I tried this which provides proper gradients:   What jax/jaxlib version are you using? jax 0.4.2  Which accelerator(s) are you using? GPU  Additional system info Ubuntu  NVIDIA GPU info _No response_",2023-08-14T23:40:28Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/17117,Hi   Looks like this issue has been resolved by the PR CC(nn.softmax: use doublewhere when where is specified). I tried to execute the mentioned code using JAX 0.4.26 and it now produces the result as expected:  Please find the gist for reference. Thank you.,Thanks for following up!
494,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Disable some graph serialization tests on GPU)， 内容是 ([jax2tf] Disable some graph serialization tests on GPU We recently increased the test coverage of testing for dot_general with different dtype for lhs and rhs. Some of the new combinations of dtypes are not supported by XLA:GPU, and we disable those tests now.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Disable some graph serialization tests on GPU,"[jax2tf] Disable some graph serialization tests on GPU We recently increased the test coverage of testing for dot_general with different dtype for lhs and rhs. Some of the new combinations of dtypes are not supported by XLA:GPU, and we disable those tests now.",2023-08-10T05:56:22Z,,closed,0,0,https://github.com/jax-ml/jax/issues/17056
575,"以下是一个github上的jax下的一个issue, 标题是(Compilation Error trying to Build jaxlib)， 内容是 ( Description I am trying to build jaxlib for metal, following these instructions. https://developer.apple.com/metal/jax/ I'm on MacOS 13.5 with an AMD Radeon Pro 5700 XT GPU   What jax/jaxlib version are you using? jaxlibv0.4.10  Which accelerator(s) are you using? MPS AMD GPU  Additional system info Python 3.8, MacOS 13.5  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Compilation Error trying to Build jaxlib," Description I am trying to build jaxlib for metal, following these instructions. https://developer.apple.com/metal/jax/ I'm on MacOS 13.5 with an AMD Radeon Pro 5700 XT GPU   What jax/jaxlib version are you using? jaxlibv0.4.10  Which accelerator(s) are you using? MPS AMD GPU  Additional system info Python 3.8, MacOS 13.5  NVIDIA GPU info _No response_",2023-08-09T16:22:30Z,bug Apple GPU (Metal) plugin,closed,0,6,https://github.com/jax-ml/jax/issues/17045,"I'm not sure what's up here, given we routinely build jaxlib on Mac. The paths like ` /usr/local/include/llvm/Support/Compiler.h` make me think that for some reason a system LLVM is involved: it should not be.","There's a llvmproject built from github which installed with files in /usr/local/include and /usr/local/bin.  There's also one from MacPorts  Then there's the Apple version. What's the correct version to build jaxlib on the Mac?  How do I specify which version to use (e.g. environment variable, build flag?","You shouldn't be using any of these. The `jaxlib` build downloads its own LLVM version. I'm not sure how or why these are getting picked up, but one thing you could try is removing the MacPorts LLVM release.","The trace shows /usr/local ... but MacPorts would be in /opt/local.  So, it's picking up the github version of llvmproject that was built and installed.  Somehow,it seems that it's using my search path to find the include files and not one's that are local to the jaxlib build.","Also, issue finding xla:  If I remove ' bazel_options=//xla/python:enable_tpu=true', then, ",Successfully built 'jaxlib' with Python 3.9.
1172,"以下是一个github上的jax下的一个issue, 标题是(Nesting `vmap` within `pmap` using `jax.pure_callback` segfaults)， 内容是 ( Description Some background, I'm trying to parallelize a CPUintensive computation using a callback to some `scipy.optimize` routines using `jax.pure_callback` across the available CPUs on my machine. Using `vmap` and `pmap` separately on `jax.pure_callback` works when `pmap`ing over available CPUs, but not nesting the two. Here's a toy example below:  * vmap works:  * pmap works:  * but not nesting the two:  Looks like the reverse composition `vmap` \circ `pmap` works. though I'm not sure if this is advisable. Something similar also happens using `shmap`, although I haven't tried that with the toy example above. Note: for just a single CPU the code runs fine on my machine.  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPUs, on a machine with a GPU. The GPU is not used.   Additional system info Ubuntu 22.04.2 LTS (GNU/Linux 5.15.072generic x86_64)  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Nesting `vmap` within `pmap` using `jax.pure_callback` segfaults," Description Some background, I'm trying to parallelize a CPUintensive computation using a callback to some `scipy.optimize` routines using `jax.pure_callback` across the available CPUs on my machine. Using `vmap` and `pmap` separately on `jax.pure_callback` works when `pmap`ing over available CPUs, but not nesting the two. Here's a toy example below:  * vmap works:  * pmap works:  * but not nesting the two:  Looks like the reverse composition `vmap` \circ `pmap` works. though I'm not sure if this is advisable. Something similar also happens using `shmap`, although I haven't tried that with the toy example above. Note: for just a single CPU the code runs fine on my machine.  What jax/jaxlib version are you using? 0.4.14  Which accelerator(s) are you using? CPUs, on a machine with a GPU. The GPU is not used.   Additional system info Ubuntu 22.04.2 LTS (GNU/Linux 5.15.072generic x86_64)  NVIDIA GPU info _No response_",2023-08-07T16:20:16Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/17001,Hi Tan  Looks like this issue has been resolved in later versions of JAX. I executed the provided repro code with JAX version 0.4.23 on Google Colab using GPU run time. It executes without any error.  Output:  `vmap`:  Output:  `pmap`:   Output:  Nesting `vmap` with `pmap`:  Output:  I have also verified the results in cloud VM with 4 CPUs and 4GPUs using the latest JAX version 0.4.24. It produces the following result.  Please find the gist on colab for reference. Thank you
668,"以下是一个github上的jax下的一个issue, 标题是(Possible bug of `jnp.tile()`)， 内容是 ( Description I ran into a possible bug when trying to make an Keras IO example to be KerasCore based and backendagnostic. https://github.com/kerasteam/kerascore/pull/623 with the following code snippets,   for `foo(a, b)`, I got  for `foo_jit(a, b)`, I got  and error messages:   What jax/jaxlib version are you using? jax/jaxlib v0.4.14  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.9/3.10, macOS  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Possible bug of `jnp.tile()`," Description I ran into a possible bug when trying to make an Keras IO example to be KerasCore based and backendagnostic. https://github.com/kerasteam/kerascore/pull/623 with the following code snippets,   for `foo(a, b)`, I got  for `foo_jit(a, b)`, I got  and error messages:   What jax/jaxlib version are you using? jax/jaxlib v0.4.14  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.9/3.10, macOS  NVIDIA GPU info _No response_",2023-08-07T01:44:26Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/16994,"For `jax.jit`, the output shapes must only depend on the input shapes, not the input values.  Your `foo_jit` violates that: the output shape depends on values in `b`.  The usual way to mitigate this is to declare such arguments as ""static"", which causes recompilation for different values and requires the argument to be hashable. Repro: ","FYI, printing from a jit'ed function does not work as usual. See the docs about `jax.debug.print`.","Thanks for answering the question! 's answer touches all the relevant pieces: this is working as expected. > FYI, printing from a jit'ed function does not work as usual. See the docs about `jax.debug.print`. Keep in mind `jax.debug.print` is only necessary for printing traced runtime values like array contents. Static values like array shapes and dtypes can be printed at tracetime with a standard numpy `print`. I'm going to close this, because it's working as expected. Feel free to comment here or open another issue if you have additional questions!"
1304,"以下是一个github上的jax下的一个issue, 标题是(jaxlib.xla_extension.XlaRuntimeError: UNIMPLEMENTED: batch in most minor dimension)， 内容是 ( Description I recently upgraded my JAX versions in a project from `jax` v0.4.10 and `jaxlib` v0.4.10+cuda11.cudnn86 to `jax` v0.4.14 and `jaxlib` v0.4.14+cuda11.cudnn86 and started seeing the following error on a piece of jitted code:  Unfortunately, I haven't been able to put together a minimal reproducible example and there are a lot of moving parts to the code, but I have noted the following behavior: 1. The code runs without error on CPU  it is only a problem on GPU. 2. When commenting out the return value of the function in question, the code runs without error:  After bisecting on JAX versions, I've found that the error starts afflicting all versions >=0.4.11. What are some possible causes or further directions to look that might help yield a minimal reproducible example or fix?  What jax/jaxlib version are you using? Working on 0.4.10/0.4.10+cuda11.cudnn86, not working on all versions >=0.4.11/>=0.4.11+cuda11.cudnn86  Which accelerator(s) are you using? CPU and GPU  Additional system info Python 3.10.12, Ubuntu 22.04  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jaxlib.xla_extension.XlaRuntimeError: UNIMPLEMENTED: batch in most minor dimension," Description I recently upgraded my JAX versions in a project from `jax` v0.4.10 and `jaxlib` v0.4.10+cuda11.cudnn86 to `jax` v0.4.14 and `jaxlib` v0.4.14+cuda11.cudnn86 and started seeing the following error on a piece of jitted code:  Unfortunately, I haven't been able to put together a minimal reproducible example and there are a lot of moving parts to the code, but I have noted the following behavior: 1. The code runs without error on CPU  it is only a problem on GPU. 2. When commenting out the return value of the function in question, the code runs without error:  After bisecting on JAX versions, I've found that the error starts afflicting all versions >=0.4.11. What are some possible causes or further directions to look that might help yield a minimal reproducible example or fix?  What jax/jaxlib version are you using? Working on 0.4.10/0.4.10+cuda11.cudnn86, not working on all versions >=0.4.11/>=0.4.11+cuda11.cudnn86  Which accelerator(s) are you using? CPU and GPU  Additional system info Python 3.10.12, Ubuntu 22.04  ",2023-08-06T06:31:55Z,bug XLA needs info NVIDIA GPU,closed,1,7,https://github.com/jax-ml/jax/issues/16991,Can you provide a minimal repro please?,"I'm getting the same issue. I've managed to reduce it down to this:  Throws   Confirmed it occurs on my local machine and on Colab. The code works with CPU, but breaks on GPU. Also works if the jit is removed.","Thanks for putting together the minimal repro! I hadn't found the time to get around to it. I can confirm that this snippet throws the error on both machines tested above under the same conditions on version 0.4.14 (works without JIT on GPU, works on CPU with JIT, breaks with JIT on GPU).","No worries, it's been on my list for a while too.  Note I'm able to workaround this by flattening the batch dimensions. I.e. something like: ",I filed an XLA bug (https://github.com/openxla/xla/issues/4833).,"I still have this issue, even though the XLA bug seems to be resolved. Downgrading to `jaxlib==0.4.10` seems to fix.","That's correct. The issue is fixed at jax head, but you'll need to either build `jaxlib` from source or wait until we make a new release. I suspect we'll make a new release this week, as it happens. Hope that helps!"
1183,"以下是一个github上的jax下的一个issue, 标题是([JAX] Introduce `DeviceList` backed by C++ `xla::ifrt::DeviceList`)， 内容是 ([JAX] Introduce `DeviceList` backed by C++ `xla::ifrt::DeviceList` This change adds `xla_client.DeviceList` that is implemented in C++ `jax::PyDeviceList`. `jax::PyDeviceList` implements the features of `pxla._DeviceAssignment` as a functional dropin replacement. `jax::PyDeviceList` internally has `xla::ifrt::DeviceList`, which will be used when using IFRT APIs without having to construct a new copy of a potentially large device list. `pxla._DeviceAssignment`'s interface is changed slightly to encourage avoiding conversion to tuple. Note that for the backward compatibility (and fast `xla_client.Device` conversion), `jax::PyDeviceList` still uses a Python tuple whose element can be any Python object matches `xla_client.Device` interface with duck typing. This duck typing support will be removed when such use case is deprecated. Eventually, we can try to avoid any type conversion to remove a shadow copy of device list in JAX.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[JAX] Introduce `DeviceList` backed by C++ `xla::ifrt::DeviceList`,"[JAX] Introduce `DeviceList` backed by C++ `xla::ifrt::DeviceList` This change adds `xla_client.DeviceList` that is implemented in C++ `jax::PyDeviceList`. `jax::PyDeviceList` implements the features of `pxla._DeviceAssignment` as a functional dropin replacement. `jax::PyDeviceList` internally has `xla::ifrt::DeviceList`, which will be used when using IFRT APIs without having to construct a new copy of a potentially large device list. `pxla._DeviceAssignment`'s interface is changed slightly to encourage avoiding conversion to tuple. Note that for the backward compatibility (and fast `xla_client.Device` conversion), `jax::PyDeviceList` still uses a Python tuple whose element can be any Python object matches `xla_client.Device` interface with duck typing. This duck typing support will be removed when such use case is deprecated. Eventually, we can try to avoid any type conversion to remove a shadow copy of device list in JAX.",2023-08-04T21:39:26Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16982
694,"以下是一个github上的jax下的一个issue, 标题是(Triton softmax cause a crash)， 内容是 ( Description There is a crash related to triton softmax. Disabling it with the `XLA_FLAGS=xla_gpu_enable_triton_softmax_fusion=false` remove the error. The error is:  Repro instruction on a 80G GPU:  My first guess would be that the grid/block passed isn't the right one.  What jax/jaxlib version are you using? upstream  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Reproduced on H100 80G. Need 80G to reproduce. Probably reproduce on A100 80G)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Triton softmax cause a crash, Description There is a crash related to triton softmax. Disabling it with the `XLA_FLAGS=xla_gpu_enable_triton_softmax_fusion=false` remove the error. The error is:  Repro instruction on a 80G GPU:  My first guess would be that the grid/block passed isn't the right one.  What jax/jaxlib version are you using? upstream  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info Reproduced on H100 80G. Need 80G to reproduce. Probably reproduce on A100 80G,2023-08-04T14:55:57Z,bug GPU,closed,1,6,https://github.com/jax-ml/jax/issues/16973,"Assigning  to follow up, but it's probably  who needs to look.","Managed to reproduce this also on A10040G by allowing input/output buffers to be aliased. After investigating and trying to figure out the dimension for which it starts failing, this seems to start failing around shape `(42692, 50304)`. The product of the shape is close to 2**31, so my money is on an integer overflow in Triton itself. I will follow up there, but our version of Triton in XLA is still behind the big initialH100support commit, so it might take a little bit of time to get the fix in XLA even if once is fixed upstream, as we need to make sure nothing breaks while integrating H100 support. In the meantime, I can probably provide a temporary fix that disallows Softmax fusion in such cases.","So as it turns out, I gave myself too much creditthe mistake was mine! This should fix it and is landing as we speak :)","With the container `ghcr.io/nvidia/pax:nightly20230803`, I get another error now: ","The numbers are not too far off, and this reproduces also with smaller shape (e.g. `(500, 50304)`), with or without Triton Softmax fusion. This seems like an issue of numerical stability because you changed the test to use `bfloat16` as the initial data typeand thus not a bug.",Good catch. Thanks. Closing.
506,"以下是一个github上的jax下的一个issue, 标题是(Add functions to unregister event duration listeners.)， 内容是 (Add functions to unregister event duration listeners. Add private functions _unregister_event_duration_listener_by_callback and _unregister_event_duration_listener_by_index to remove registered event duration listeners. The functions are supposed to be called in test only.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add functions to unregister event duration listeners.,Add functions to unregister event duration listeners. Add private functions _unregister_event_duration_listener_by_callback and _unregister_event_duration_listener_by_index to remove registered event duration listeners. The functions are supposed to be called in test only.,2023-08-04T01:12:28Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16958,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
368,"以下是一个github上的jax下的一个issue, 标题是(Create the failure test when tf.SavedModel miss the XLACallModule function_list after loading.)， 内容是 (Create the failure test when tf.SavedModel miss the XLACallModule function_list after loading.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Create the failure test when tf.SavedModel miss the XLACallModule function_list after loading.,Create the failure test when tf.SavedModel miss the XLACallModule function_list after loading.,2023-08-03T05:22:09Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16942
466,"以下是一个github上的jax下的一个issue, 标题是(Enable test for indexing with u8 indices.)， 内容是 (Enable test for indexing with u8 indices. https://github.com/openxla/xla/commit/4e4eff35bf9a5f8ed54fd290391bd0612f49533e fixed the underlying XLA problem. Fixes https://github.com/google/jax/issues/6122 https://github.com/google/jax/issues/16836)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Enable test for indexing with u8 indices.,Enable test for indexing with u8 indices. https://github.com/openxla/xla/commit/4e4eff35bf9a5f8ed54fd290391bd0612f49533e fixed the underlying XLA problem. Fixes https://github.com/google/jax/issues/6122 https://github.com/google/jax/issues/16836,2023-08-01T14:27:33Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16917
1306,"以下是一个github上的jax下的一个issue, 标题是(Simple optimization problem running faster in Tensor Flow compared to JAX with Optax)， 内容是 ( Description Hi, I've been trying to port Tensor Flow code in Google Colab (free version) to JAX but the execution time is 5 times slower: The original TF code takes 4 seconds to run: https://colab.research.google.com/drive/1HYYBr4M3hstzmPnTmYO8DEbodTuFBJb4?usp=drive_link The equivalent JAX code takes 20 seconds to run: https://colab.research.google.com/drive/1E9L_iyf7ZG0WfUgxmtbwAhTSuWdWzMei?usp=drive_link I changed the optimizer to the JAX library Adam optimizer (jax.example_libraries) and it runs in 5 seconds: https://colab.research.google.com/drive/1dM0QeRWeOoMhejnyymtCqh_EjVmCi8z?usp=sharing The target image is here: https://drive.google.com/file/d/1jsxSEv0c0SfXaDtKhYd6hz08yX_pm4g/view?usp=sharing I got the equivalent JAX code to run 5 times faster by taking the iteration step calculations, putting them in a function and jitting it but I haven't been able to accelerate the code further. The loss and field calculation functions on their own are 4 times faster in JAX than in TF. Regards, Alex  What jax/jaxlib version are yo)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Simple optimization problem running faster in Tensor Flow compared to JAX with Optax," Description Hi, I've been trying to port Tensor Flow code in Google Colab (free version) to JAX but the execution time is 5 times slower: The original TF code takes 4 seconds to run: https://colab.research.google.com/drive/1HYYBr4M3hstzmPnTmYO8DEbodTuFBJb4?usp=drive_link The equivalent JAX code takes 20 seconds to run: https://colab.research.google.com/drive/1E9L_iyf7ZG0WfUgxmtbwAhTSuWdWzMei?usp=drive_link I changed the optimizer to the JAX library Adam optimizer (jax.example_libraries) and it runs in 5 seconds: https://colab.research.google.com/drive/1dM0QeRWeOoMhejnyymtCqh_EjVmCi8z?usp=sharing The target image is here: https://drive.google.com/file/d/1jsxSEv0c0SfXaDtKhYd6hz08yX_pm4g/view?usp=sharing I got the equivalent JAX code to run 5 times faster by taking the iteration step calculations, putting them in a function and jitting it but I haven't been able to accelerate the code further. The loss and field calculation functions on their own are 4 times faster in JAX than in TF. Regards, Alex  What jax/jaxlib version are yo",2023-08-01T13:21:48Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/16916,"Hi  thanks for sharing the results of your exploration! So if I understand correctly, it sounds like something in the optax framework is leading to a slowdown. It might be worth opening a bug at https://github.com/deepmind/optax to see if one of the developers of that library has ideas on how to improve the runtime. You might also check out jaxopt, which is a jaxbased optimization library developed by a different research group within Google."
1285,"以下是一个github上的jax下的一个issue, 标题是(Sharded FFT with JIT gives incorrect results on GPU since 0.4.9)， 内容是 ( Description (this is not the same issue as CC(sharded fft unnecessarily loads entire array), but it gives helpful information on this topic) I am trying to calculate the 3DFFT of a 1024^3 matrix using 2 GPUs. This should work using code similar to https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html (but adapted to calculate a 3DFFT instead of a 1D) or the JAXonly code from https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuFFTMp/JAX_FFT (as shared in CC(sharded fft unnecessarily loads entire array)). My code seems to give the correct result when not using JIT, when only using GPU devices or when not sharding the memory. But in the combination I need (JIT with GPU on sharded memory), the results differ from the other results (and what scipy/numpy calculates). main code as a minimal example:  with `shared_fft_minimal.py`:  When I run this on a host with 2 GPUs (with CPUs all three outputs are the same), I get:  HLO       What jax/jaxlib version are you using? jax==0.4.14, jaxlib==0.4.14+cuda11)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Sharded FFT with JIT gives incorrect results on GPU since 0.4.9," Description (this is not the same issue as CC(sharded fft unnecessarily loads entire array), but it gives helpful information on this topic) I am trying to calculate the 3DFFT of a 1024^3 matrix using 2 GPUs. This should work using code similar to https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html (but adapted to calculate a 3DFFT instead of a 1D) or the JAXonly code from https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuFFTMp/JAX_FFT (as shared in CC(sharded fft unnecessarily loads entire array)). My code seems to give the correct result when not using JIT, when only using GPU devices or when not sharding the memory. But in the combination I need (JIT with GPU on sharded memory), the results differ from the other results (and what scipy/numpy calculates). main code as a minimal example:  with `shared_fft_minimal.py`:  When I run this on a host with 2 GPUs (with CPUs all three outputs are the same), I get:  HLO       What jax/jaxlib version are you using? jax==0.4.14, jaxlib==0.4.14+cuda11",2023-07-31T16:35:32Z,bug XLA NVIDIA GPU,closed,0,11,https://github.com/jax-ml/jax/issues/16909,"I also tried running the benchmark from cuFFTMp directly, but needed to change a few minor things so that imports work with my jax version and multiple GPUs on one host.  When modifying this code block https://github.com/NVIDIA/CUDALibrarySamples/blob/f299986ecc990c49dce295e345f9e3ad95ded188/cuFFTMp/JAX_FFT/tests/fft_test.pyL117L122 to   I also get different results between numpy and jax FFT:  and the same results when replacing `fwd_bwd_pjit` with `fwd_bwd`",Another test shows that the incorrect result differs strongly by datatype: With the above code updated to   I get   And with 64bit:  it becomes:  Also checking the full 4x4x4 array in the first case shows quite a few entries that are zero or repeating: Full Array     ,"Okay, I just tried out version 0.4.8 (as it is the oldest one I could easily install with `jax[cuda11_pip]`) and interestingly enough, it works correctly. I just needed to slightly change the above script to adapt to the older function arguments:  Testing all newer versions seems to indicate that already 0.4.9 is broken in the same way. So I guess something in https://github.com/google/jax/compare/jaxv0.4.8...jaxv0.4.9 or a different XLA version (maybe https://github.com/openxla/xla/compare/dad64948516e3672b3e2518945831a70b5e90b81...79ca8d03c296ede04dc9a86ce9dde79ed909dda8) causes this issue.","I finally got jax to compile from source on my system and was able to bisect the exact xlacommit that broke this code: It turns out that using jax 0.4.9 and jaxlib 0.4.7, everything works. But with jaxlib 0.4.9 the above issue occurs. And it turns out the issue is in XLA as compiling jaxlib 0.4.9 with https://github.com/openxla/xla/commit/dad64948516e3672b3e2518945831a70b5e90b81 is broken (as expected), but the same jax/jaxlib with https://github.com/openxla/xla/commit/79ca8d03c296ede04dc9a86ce9dde79ed909dda8 works. From there I could bisect all commits in https://github.com/openxla/xla/compare/79ca8d03c296ede04dc9a86ce9dde79ed909dda8...dad64948516e3672b3e2518945831a70b5e90b81 and it turns out the first broken commit is https://github.com/openxla/xla/commit/8680d1bad06d45396b171793d936ea71065c7107 or https://github.com/openxla/xla/pull/2778 (with https://github.com/openxla/xla/commit/9d244bfa91793c4c58ddf82ffaebb5c505b2da07 being the last working commit). Test Details      (sidenote: I had to cherrypick https://github.com/openxla/xla/commit/be1cee3e4aeee55ea5487d15398d92d9e21e19b2 on every commit to make it compile on my setup) Unfortunately I don't understand XLA enough to know how removing that optimization pass broke jax here. Maybe  has a good idea here, I would be really thankful. But to doublecheck this is correct, I went back to https://github.com/openxla/xla/commit/dad64948516e3672b3e2518945831a70b5e90b81 and reverted https://github.com/openxla/xla/commit/8680d1bad06d45396b171793d936ea71065c7107 there and the resulting jaxlib (that should be identical to the released 0.4.9 apart from that change) indeed works correctly. Applying it to the latest jax/xla version doesn't seem that easy as the related code changed a bit since. And the CPU code still contains this optimization pass which might explain why this issue only occurs on GPUs: https://github.com/openxla/xla/blob/dad64948516e3672b3e2518945831a70b5e90b81/xla/service/cpu/cpu_compiler.ccL625","Thanks. AllToAll in XLA has a mode where it first splits all inputs into N chunks (N =  of devices) and then concatenates all Chunk[0] across all device on device 0, Chunk[1] across all devices on device 1 etc. XLA/GPU at some point did not support this in the backend, so the pass in question was used to decompose such AllToAlls into other collectives. However, XLA/GPU later added support for such AllToAlls in the backend, but the pass remained. The changes in question were to remove these expansion that was not needed.  I suspect this code is exposing a potential correctness issue in our AllToAll implementation with spit dims."," thanks for sharing your excellent work. I'm facing the same problem, but your post has saved me a lot of time.  thanks for the update. Do you know if anyone is working on the AllToAll issue you described?","I looked into this issue again as it is frustrating having to use an outdated jax version to get correct results. Starting with jax 0.4.16 the reproduction script has to be slightly adapted to accommodate to https://github.com/google/jax/pull/17072.  I checked all jax version since I created this issue and it turns out that while jax+jaxlib 0.4.18 still shows the exact same issue, 0.4.19 seems to give correct results. I will need to do more testing, but as nothing in the changelog seems related, I guess some change in XLA fixed this. kogsys You might also want to try this out again with jax+jaxlib 0.4.19","I strongly suspect the fix for https://github.com/google/jax/issues/18122, which is in 0.4.19, also fixed this issue.","Let me know if you can still reproduce, but I'm going to assume this is fixed!","After a bit of testing it seems like the bug is indeed solved. For everyone interested in 3d FFTs stumbling over this post, I have now also updated my reference code for the latest jax version: https://gist.github.com/Findus23/eb5ecb9f65ccf13152cda7c7e521cbdd","All working for me. Thank you both for all your help, this is a real enabler for anything requiring large FFTs.   , one small comment, the code in the link above still appears to be targeted at 0.4.9, as the partitioning arguments are the old style. For example: `def partition(arg_shapes, arg_shardings, result_shape, result_sharding):     ...` instead of: `def partition(mesh, arg_shapes, result_shape):     ...` I got it working with the mods you mentioned on October 23rd though. Thanks again!"
1246,"以下是一个github上的jax下的一个issue, 标题是(Fast path for device_put)， 内容是 ( Some background I've been using JAX for several years to accelerate computation of various scale. For large transformer models, for my use case, the improvement is around 1.5x to 2x compared to PyTorch (thus not quite obvious). For smaller but complex scientific models, this often leads to orders of magnitude improvement. For example, a recent workload runs on CPU and contains a lot of small computations. The jitted JAX function avoids the huge dispatch cost that PyTorch incurs. The workload also requires auto differentiation, thus Numba or Cython is not an option. In short, I was able to speed up the workload by 1000 times, from 20ms to 30us.  Problem Then I discovered that while the computation is fast, there was nontrivial overhead at JIT boundary. I used to prefer explicit conversion between `np.ndarray` and `jax.Array` at JIT boundary with `jax.device_put/jax.device_get` or `jnp.asarray/np.asarray`. But for my use case, the time cost of explicit conversion is even larger than the actual execution time! Some quick benchma)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Fast path for device_put," Some background I've been using JAX for several years to accelerate computation of various scale. For large transformer models, for my use case, the improvement is around 1.5x to 2x compared to PyTorch (thus not quite obvious). For smaller but complex scientific models, this often leads to orders of magnitude improvement. For example, a recent workload runs on CPU and contains a lot of small computations. The jitted JAX function avoids the huge dispatch cost that PyTorch incurs. The workload also requires auto differentiation, thus Numba or Cython is not an option. In short, I was able to speed up the workload by 1000 times, from 20ms to 30us.  Problem Then I discovered that while the computation is fast, there was nontrivial overhead at JIT boundary. I used to prefer explicit conversion between `np.ndarray` and `jax.Array` at JIT boundary with `jax.device_put/jax.device_get` or `jnp.asarray/np.asarray`. But for my use case, the time cost of explicit conversion is even larger than the actual execution time! Some quick benchma",2023-07-31T12:58:01Z,enhancement,open,2,4,https://github.com/jax-ml/jax/issues/16905,"Yes, we're aware `device_put` is slower than it should be and looking at speeding it up soon. One possible workaround for now: try `jit(lambda x: x)` or `jit(lambda x: x+0)`. The latter at least will force us to use the (probably faster) `jit` dispatch path.","Yes, I was thinking of ""abusing"" jit for this purpose. But I only tried `jit(lambda x: x)`, and it turned out to be even slower! Thus I did not include that solution above. I did not realize that some ""clever"" optimization occurred (maybe by comparing Tracer identity?), and that somehow slowed down the execution. The extra `+ 0` or copy did the trick, making it the fastest explicit conversion solution (somewhat amusing). The extra benchmark: ","Yes. If the `jaxpr` of the `jit` is trivial (has no equations), then we hit a ""fast"" path. But as you note, the ""fast"" path is no longer as fast as the path that actually dispatches a computation. We put in much more work optimizing the `jit` dispatch path to date and it shows!","You""'re aware device_put is slower than it should be and looking at speeding it up soon"" do you have a work in progress design or any progress on this? Thanks."
886,"以下是一个github上的jax下的一个issue, 标题是(Possible BUG: Different behavior of 64-bit integers when used inside Sharding ops)， 内容是 ( Description Hey, I'm trying to vmap a function over a sharded integer array. The arrays are distributed over 8 TPU cores and have divisible dimensions. In the code below, when I run the function, ``jax.vmap(simple_op)`` with an int8 array, I get the following output.   However, when I use ``jnp.uint64`` instead. I get a different result  The corresponding code   This is a very strange bug. What could be the possible cause?   What jax/jaxlib version are you using? jax==0.4.6, jaxlib==0.4.6  Which accelerator(s) are you using? TPU  Additional system info Python 3.8.16 (Kaggle Environment)  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Possible BUG: Different behavior of 64-bit integers when used inside Sharding ops," Description Hey, I'm trying to vmap a function over a sharded integer array. The arrays are distributed over 8 TPU cores and have divisible dimensions. In the code below, when I run the function, ``jax.vmap(simple_op)`` with an int8 array, I get the following output.   However, when I use ``jnp.uint64`` instead. I get a different result  The corresponding code   This is a very strange bug. What could be the possible cause?   What jax/jaxlib version are you using? jax==0.4.6, jaxlib==0.4.6  Which accelerator(s) are you using? TPU  Additional system info Python 3.8.16 (Kaggle Environment)  NVIDIA GPU info _No response_",2023-07-31T08:29:00Z,bug needs info,closed,0,2,https://github.com/jax-ml/jax/issues/16903,Can you upgrade to the latest jax and jaxlib versions and see if you can still repro it?,I cannot repro with jax 0.4.14
1294,"以下是一个github上的jax下的一个issue, 标题是(XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed.)， 内容是 ( Description The OUTPUT: [1]  20230731 01:53:45.016563: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:427] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. [2] XlaRuntimeError                           Traceback (most recent call last) Cell In[4], line 29      26 model = trainer.make_model(nmask)      28 lr_fn, opt = trainer.make_optimizer(steps_per_epoch=len(train_dl)) > 29 state = trainer.create_train_state(jax.random.PRNGKey(0), model, opt)      30 state = checkpoints.restore_checkpoint(ckpt.parent, state) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/random.py:137, in PRNGKey(seed)     134 if np.ndim(seed):     135   raise TypeError(""PRNGKey accepts a scalar seed, but was giv)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed.," Description The OUTPUT: [1]  20230731 01:53:45.016563: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:427] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration. [2] XlaRuntimeError                           Traceback (most recent call last) Cell In[4], line 29      26 model = trainer.make_model(nmask)      28 lr_fn, opt = trainer.make_optimizer(steps_per_epoch=len(train_dl)) > 29 state = trainer.create_train_state(jax.random.PRNGKey(0), model, opt)      30 state = checkpoints.restore_checkpoint(ckpt.parent, state) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/random.py:137, in PRNGKey(seed)     134 if np.ndim(seed):     135   raise TypeError(""PRNGKey accepts a scalar seed, but was giv",2023-07-31T08:03:53Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16901,"The error message says what's wrong:  You installed a version of `jax` that needs CuDNN 8.6, but CuDNN 8.5 was found. I suggest reinstalling using the `cuda11_pip` or `cuda12_pip` packages, in a fresh virtual environment. Hope that helps!", Thank you so much！
785,"以下是一个github上的jax下的一个issue, 标题是([WIP] Implement `scipy.spatial.distance`)， 内容是 (Partially addresses CC(Implement scipy.spatial). I saw that there's a similar PR CC(Spatial Distances), but it looks like it's inactive. Todo:  [x] Metric functions  [ ] `scipy.spatial.distance.cdist`  [ ] `scipy.spatial.distance.pdist`  [ ] `scipy.spatial.distance.squareform`  [ ] `scipy.spatial.distance.directed_hausdorff`  [ ] Utility functions (`is_valid_dm`, `is_valid_y`, `num_obs_dm`, `num_obs_y`)  [ ] Clean up metric functions  [ ] `rogerstanimono` and `sokalmichener` are the same? https://github.com/scipy/scipy/issues/2011  [ ] Add tests  [ ] Add docs)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[WIP] Implement `scipy.spatial.distance`,"Partially addresses CC(Implement scipy.spatial). I saw that there's a similar PR CC(Spatial Distances), but it looks like it's inactive. Todo:  [x] Metric functions  [ ] `scipy.spatial.distance.cdist`  [ ] `scipy.spatial.distance.pdist`  [ ] `scipy.spatial.distance.squareform`  [ ] `scipy.spatial.distance.directed_hausdorff`  [ ] Utility functions (`is_valid_dm`, `is_valid_y`, `num_obs_dm`, `num_obs_y`)  [ ] Clean up metric functions  [ ] `rogerstanimono` and `sokalmichener` are the same? https://github.com/scipy/scipy/issues/2011  [ ] Add tests  [ ] Add docs",2023-07-28T11:55:50Z,,closed,1,3,https://github.com/jax-ml/jax/issues/16879,"Hi  thanks for working on this! Just a note: it looks like the implementations are copypasted from the scipy source code. This is problematic from a code licensing standpoint – for this sort of work I'd recommend not referencing the scipy source, but rather implementing the metrics from scratch, using unit tests to ensure that the implementations produce similar results."," Hi, thanks for the feedback! Makes sense, I'll take a second pass at this and implement the metrics from scratch.","Hi again: we've done some thinking about the scope of `jax.scipy` longterm, and the consensus was that `scipy.spatial` is out of scope. See https://jax.readthedocs.io/en/latest/jep/18137numpyscipyscope.html for details. Thanks so much for the contribution, and I'm sorry we weren't able to merge your code!"
1209,"以下是一个github上的jax下的一个issue, 标题是(Introduce version 8 of XlaCallModule.)， 内容是 (Introduce version 8 of XlaCallModule. Previously, XlaCallModule was running the shape refinement pass for all compilations, even if the module did not use shape polymorphism. Currently shape refinement changes the structure of the module, through inlining and constant folding all integer operations. This complicates debugging because the HLO dump is very different than the one from JAX native executions. Starting with version 8, we run shape refinement only if the module contains a boolean module attribute jax.uses_shape_polymorphism=true. I think it makes sense to put this flag as a module attribute, rather than as a TF op attribute, because the same processing will be needed when the module is executed from JAX. This attribute is not yet populated by the JAX exporter. As part of this change we moved the error check for the number of invocation arguments from RefineDynamicShapes to LoadAndPreprocessModule. This required adding a couple more arguments to the loader constructor.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Introduce version 8 of XlaCallModule.,"Introduce version 8 of XlaCallModule. Previously, XlaCallModule was running the shape refinement pass for all compilations, even if the module did not use shape polymorphism. Currently shape refinement changes the structure of the module, through inlining and constant folding all integer operations. This complicates debugging because the HLO dump is very different than the one from JAX native executions. Starting with version 8, we run shape refinement only if the module contains a boolean module attribute jax.uses_shape_polymorphism=true. I think it makes sense to put this flag as a module attribute, rather than as a TF op attribute, because the same processing will be needed when the module is executed from JAX. This attribute is not yet populated by the JAX exporter. As part of this change we moved the error check for the number of invocation arguments from RefineDynamicShapes to LoadAndPreprocessModule. This required adding a couple more arguments to the loader constructor.",2023-07-21T15:34:04Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16814
535,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Document the JAX serialization version numbers.)， 内容是 (Previously this was documented in code comments in TF source code: https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+path%3Axla_call_module+%22int+VERSION_MAXIMUM_SUPPORTED%22&type=code We want to establish a new source of truth for this. We will then update the code comments in TF source.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Document the JAX serialization version numbers.,Previously this was documented in code comments in TF source code: https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+path%3Axla_call_module+%22int+VERSION_MAXIMUM_SUPPORTED%22&type=code We want to establish a new source of truth for this. We will then update the code comments in TF source.,2023-07-20T08:59:29Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16802,  PTAL
430,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Bump the default JAX serialization version to 7.)， 内容是 ([jax2tf] Bump the default JAX serialization version to 7. This enables shape assertion checking, the support for which landed in XlaCallModule on July 12th, 2023. See the CHANGELOG for details.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Bump the default JAX serialization version to 7.,"[jax2tf] Bump the default JAX serialization version to 7. This enables shape assertion checking, the support for which landed in XlaCallModule on July 12th, 2023. See the CHANGELOG for details.",2023-07-20T05:23:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16800
598,"以下是一个github上的jax下的一个issue, 标题是(Refactor opaque dtype implementation.)， 内容是 (This makes it closer to numpy, with `dtypes.OpaqueDtype` analogous to `np.dtype`, and `dtypes.opaque` analogous to `np.numeric`. This will let us replace the `dtypes.is_opaque_dtype(dtype)` function with `jnp.issubdtype(dtype, dtypes.opaque)`. This underlying change will make the goal in CC(custom prng: introduce mechanism to identify key arrays by dtype) much easier to realize.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Refactor opaque dtype implementation.,"This makes it closer to numpy, with `dtypes.OpaqueDtype` analogous to `np.dtype`, and `dtypes.opaque` analogous to `np.numeric`. This will let us replace the `dtypes.is_opaque_dtype(dtype)` function with `jnp.issubdtype(dtype, dtypes.opaque)`. This underlying change will make the goal in CC(custom prng: introduce mechanism to identify key arrays by dtype) much easier to realize.",2023-07-19T23:00:05Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16795
900,"以下是一个github上的jax下的一个issue, 标题是(unsafe_rbg + vmap --> 10x slow down)， 内容是 ( Description unsafe_rbg is advertised as the solution for performance issues for rng, but it has surprising pessimization in the presence of vmap, more than 10x in this script, but more like 4x in real use. The key is `vmap(loss)`, where loss calls the RNG. This is with `LIBTPU_INIT_ARGS=xla_tpu_spmd_rng_bit_generator_unsafe=true` Times: * vmap, no dropout: 0.057 * vmap, dropout, threefry: 0.092 * vmap, dropout, unsafe: 0.736  (!!!!!) * no vmap, no dropout: 0.057 * no vmap, dropout, threefy: 0.088 * no vmap, dropout, unsafe: 0.075   What jax/jaxlib version are you using? 0.4.13  Which accelerator(s) are you using? TPU  Additional system info v332  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,unsafe_rbg + vmap --> 10x slow down," Description unsafe_rbg is advertised as the solution for performance issues for rng, but it has surprising pessimization in the presence of vmap, more than 10x in this script, but more like 4x in real use. The key is `vmap(loss)`, where loss calls the RNG. This is with `LIBTPU_INIT_ARGS=xla_tpu_spmd_rng_bit_generator_unsafe=true` Times: * vmap, no dropout: 0.057 * vmap, dropout, threefry: 0.092 * vmap, dropout, unsafe: 0.736  (!!!!!) * no vmap, no dropout: 0.057 * no vmap, dropout, threefy: 0.088 * no vmap, dropout, unsafe: 0.075   What jax/jaxlib version are you using? 0.4.13  Which accelerator(s) are you using? TPU  Additional system info v332  NVIDIA GPU info _No response_",2023-07-19T21:58:03Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16792,Our batching rule is more of a looping rule: https://github.com/google/jax/blob/cd951f491718841a806bb509c9950a9630c5a8b6/jax/_src/lax/control_flow/loops.pyL1956L1968
571,"以下是一个github上的jax下的一个issue, 标题是(tensorflow-datasets documentation)， 内容是 (I was trying to decide on a dataloader to use but after reading the documentation neither option (tensorflowdatasets or pytorch) seemed attractive because they both required installing 1+GB packages. After reading more about tensorflowdatasets it seems like they eliminated the tensorflow requirement recently. But JAX documentation still says it is needed: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,tensorflow-datasets documentation,I was trying to decide on a dataloader to use but after reading the documentation neither option (tensorflowdatasets or pytorch) seemed attractive because they both required installing 1+GB packages. After reading more about tensorflowdatasets it seems like they eliminated the tensorflow requirement recently. But JAX documentation still says it is needed: ,2023-07-19T02:50:50Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/16782,Hmm... well tried running the example and looks like it still does require a full `tensorflow` installation unfortunately...,The `tf.config` stuff isn't necessary if `tensorflow` isn't installed.
512,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Remove unnecessary test)， 内容是 ([jax2tf] Remove unnecessary test The `jax2tf_test.XlaCallModuleTest` was added in the early days of native serialization. Now we have much better testing through `xla_call_module_test.py` (near the `XlaCallModule` definition in TF) and through all the native serialization tests in jax2tf test suite.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Remove unnecessary test,[jax2tf] Remove unnecessary test The `jax2tf_test.XlaCallModuleTest` was added in the early days of native serialization. Now we have much better testing through `xla_call_module_test.py` (near the `XlaCallModule` definition in TF) and through all the native serialization tests in jax2tf test suite.,2023-07-17T14:29:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16758
962,"以下是一个github上的jax下的一个issue, 标题是(Different promotion behavior between numpy.right_shift and jax.numpy.right_shift for scalars)， 内容是 ( Description Hi, Thank you for your all efforts developing such a great library! I'm trying to use jax.numpy.left_shift and jax.numpy.right_shift method for bitwise calculation. And I found that the jax.numpy.right_shift operation does nothing if I do shift operation with jnp.array. Here is what I've tried,     And here is the code for printing bit string.  Is this a bug or did I miss something to use right_shift operator, or does my show_bits function matter? Regards,  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.6 / Ubuntu 22.04 with Docker  NVIDIA GPU info All RTX 3090 )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Different promotion behavior between numpy.right_shift and jax.numpy.right_shift for scalars," Description Hi, Thank you for your all efforts developing such a great library! I'm trying to use jax.numpy.left_shift and jax.numpy.right_shift method for bitwise calculation. And I found that the jax.numpy.right_shift operation does nothing if I do shift operation with jnp.array. Here is what I've tried,     And here is the code for printing bit string.  Is this a bug or did I miss something to use right_shift operator, or does my show_bits function matter? Regards,  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.6 / Ubuntu 22.04 with Docker  NVIDIA GPU info All RTX 3090 ",2023-07-16T01:41:59Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/16748,"You're right! Simpler, more complete repro:  The issue is that, given `(uint32 << int32)`, JAX promotes both arguments to `int64` (or `int32`) first. The right shift is then an arithmetic shift because that's how right shifts on signed values are defined in NumPy (i.e., the leading sign bit is copied into lower bits when shifting). This is a bug in JAX, since `jax.numpy` should match `numpy`. However, as a workaround, just make sure that the value on the right hand side of the shift operator is of unsigned type also.","Actually, I'm not so sure any more. NumPy's behavior seems to differ between scalars and arrays:  JAX doesn't distinguish between scalars and arrays (this is an intentional difference from NumPy).  What do you think?","This is a corner case that is poorly affected by the design of JAX's X64 flag. The type promotion result for `int32, uint32` in both JAX and NumPy is `int64`, and you can see with `int64` enabled that JAX produces the expected output:  Unfortunately, with `X64` disabled, this gets squashed to `int32` that causes problematic type casting of inputs, ultimately leading to an unexpected result. I'm not sure what to suggest, beyond perhaps using explicit casting of inputs to avoid these problematic corner cases. Beyond that, we should probably fix this by changing how the X64 flag operates, but that's a piece of technical debt that's so tightly entwined with the core uses of JAX that it's proven incredibly difficult to change, as even subtle tweaks can have unexpectedly farreaching consequences for users.","If you're finding that implicit dtype promotion is causing problems, you might consider running your code with strict type promotion semantics to detect these potentially problematic cases: ","Note this isn't just a problem about type squashing. It's certainly relevant to scalar handling, also. e.g., even with  `JAX_ENABLE_X64`.  NumPy handles a scalar RHS differently here.","Numpy scalars are akin to JAX weak types, and JAX and NumPy act similarly in these cases:  If you have a stronglytyped (nonscalar) value in NumPy, you see the same error as with a stronglytyped righthandside value in JAX: "
484,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Added a flag and environment variable to control the serialization version)， 内容是 (This allows us to control the serialization version to be compatible with the deployed version of tf.XlaCallModule. In particular, we can run most tests with the maximum available version, while keeping the default lower.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Added a flag and environment variable to control the serialization version,"This allows us to control the serialization version to be compatible with the deployed version of tf.XlaCallModule. In particular, we can run most tests with the maximum available version, while keeping the default lower.",2023-07-15T09:17:39Z,pull ready,closed,0,4,https://github.com/jax-ml/jax/issues/16746,  PTAL,"> The changes look good to me. >  > Somewhat related question: do you plan to maintain min/max supported version in jax2tf (if we ever need one when we clean up some old jax2tf logic) separately from TF2XLA, or in sync? I would guess that the former makes more sense, but not sure if it will make the testing more complex. I don't quite see how to keep the versioning perfectly in sync. My plan is to support in jax2tf the versions that have been active in google3 over the last month. This means 1 or 2 versions, the most recent. The tests in google3 will verify that we can use the most recent version and the one that is marked as default. There is no test per se that we can use the version that has been active a month ago; we rely on people not updating the default serialization version too early. Also in OSS people will have different versions of JAX and TF. I do assume that people will be willing to use tfnightly if they get an error; this has been true for jax2tf so far because it relies always on some very recent details for the TF ops.","> > The changes look good to me. >  > >  >  > > Somewhat related question: do you plan to maintain min/max supported version in jax2tf (if we ever need one when we clean up some old jax2tf logic) separately from TF2XLA, or in sync? I would guess that the former makes more sense, but not sure if it will make the testing more complex. >  >  >  > I don't quite see how to keep the versioning perfectly in sync. My plan is to support in jax2tf the versions that have been active in google3 over the last month. This means 1 or 2 versions, the most recent. The tests in google3 will verify that we can use the most recent version and the one that is marked as default. There is no test per se that we can use the version that has been active a month ago; we rely on people not updating the default serialization version too early. >  >  >  > Also in OSS people will have different versions of JAX and TF. I do assume that people will be willing to use tfnightly if they get an error; this has been true for jax2tf so far because it relies always on some very recent details for the TF ops. Thanks. Sounds good to me.","This was already merged, but copybara did not close it. See 603eeb19017d50526a85e9c6c49f76330254bd47"
828,"以下是一个github上的jax下的一个issue, 标题是(Can't find libdevice)， 内容是 ( Description Hi, I'm running Debian 12 with an nvidia GPU.  I didn't install a system CUDA.  I installed Jaxlib via `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and now when I try to run my code I get:  Based on this comment https://github.com/google/jax/issues/989issuecomment839772971 the error I'm seeing shouldn't be possible with the pipinstalled CUDA that I'm trying to use?  What jax/jaxlib version are you using? jax0.4.13, jaxlib 0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? nvidia tesla t4  Additional system info Debian 12  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Can't find libdevice," Description Hi, I'm running Debian 12 with an nvidia GPU.  I didn't install a system CUDA.  I installed Jaxlib via `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` and now when I try to run my code I get:  Based on this comment https://github.com/google/jax/issues/989issuecomment839772971 the error I'm seeing shouldn't be possible with the pipinstalled CUDA that I'm trying to use?  What jax/jaxlib version are you using? jax0.4.13, jaxlib 0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? nvidia tesla t4  Additional system info Debian 12  NVIDIA GPU info ",2023-07-13T23:58:18Z,bug XLA NVIDIA GPU,closed,0,10,https://github.com/jax-ml/jax/issues/16726,"For what its worth, `env/lib/python3.11/sitepackages/jaxlib/cuda/nvvm/libdevice/libdevice.10.bc` does in fact exist",The logic for finding `libdevice` is here: https://github.com/google/jax/blob/6c699815bc307e09ee553245ea5cdf8b6298b040/jax/_src/lib/__init__.pyL121 Is it possible you have `jaxlib` and NVIDIA's pip packages installed in different sitepackage directories or something like that?,"That function finds & returns `env/lib/python3.11/sitepackages/nvidia/cuda_nvcc`, which does in fact contain ptxas and libdevice.10.bc I don't think anything is installed anywhere other than `env/lib/python3.11/sitepackages`","(by anything I mean literally anything, this is a clean image and the only thing i've done besides installing `nvidiadriver` from debian nonfree is run `python m venv env; source env/bin/activate; pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`)","I'm not able to reproduce this. I did the following: * I made a fresh GCP instance with Debian 12 bookworm (amd64) and a T4 GPU. * `sudo apt install python3 python3pip python3.11venv`. * I installed the NVIDIA driver following https://wiki.debian.org/NvidiaGraphicsDriversDebian_12_.22Bookworm.22, which I did verbatim except I had to include the GCP kernel's headers.  I also then checked out JAX and ran a bunch of the test suite, which seemed happy. Can you say more about how your environment/test is different?","Hm that should be exactly the same I think? I didn't install ipython, but I don't see how that could matter... `python c ""import jax; print(jax.devices())""` works fine for me; the error doesn't happen until I actually try to run some code.  I'll see if I can get a minimal example, and also make a new image just to be extra certain, and update the ticket","Ok, here's my probably not quite minimal example:  The bug only happens when I'm trying to multiply an int32 array by a float32 array so I guess that's the actual problem, however just in case it's useful I went ahead and created a new instance and carefully logged every command I ran: ","Thanks, I can reproduce. It seems to be related to when XLA uses Triton to emit a mixedprecision GEMM kernel. You can work around for the moment by setting `XLA_FLAGS=xla_gpu_enable_triton_gemm=false`.","Good news! I think this was actually already fixed last week by https://github.com/openxla/xla/commit/6374373c40a918215813bb8a6dd5dadd695c5a9a That fix is not in the latest jaxlib release yet. Until then, use the workaround. Hope that helps!",Thanks!
491,"以下是一个github上的jax下的一个issue, 标题是(Include compile time along with executable in cache entry.)， 内容是 (Include compile time along with executable in cache entry. In order to measure cache savings, we add compilation time to the cache entry along with the serialized executable. The compile time can then be retrieved on a cache hit. Testing: updated tests.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Include compile time along with executable in cache entry.,"Include compile time along with executable in cache entry. In order to measure cache savings, we add compilation time to the cache entry along with the serialized executable. The compile time can then be retrieved on a cache hit. Testing: updated tests.",2023-07-13T21:35:38Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16722,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
271,"以下是一个github上的jax下的一个issue, 标题是(Ragged batching polish)， 内容是 (Add more test cases; also rename to jumbles to avoid misassociations.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Ragged batching polish,Add more test cases; also rename to jumbles to avoid misassociations.,2023-07-13T19:49:07Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16719,nit: you should squash the commits so that the github history is a bit clean! This is the advice I got from other people after which I stopped doing PRs lol 
1292,"以下是一个github上的jax下的一个issue, 标题是(hide typed key array class in favor of distinguishing key arrays by dtype)， 内容是 (From the outside perspective, a typed key array should be known as a `jax.Array` with a special element type. Currently we support—and even encourage—distinguishing it based on a special array type (`PRNGKeyArray`) instead, but let's move away from that. To achieve this, we'll want to hide the array type and expose the dtype in some way. At least two notable ways in which this will affect code that has already started to use typed key arrays: * `isinstance(keys, PRNGKeyArray)` will need an alternative based on `keys.dtype` * Type annotations with `PRNGKeyArray` will need to be switched to `jax.Array`. This is more permissive in principle, but actually the current `jax.random` module sets `PRNGKeyArray` to `Any` during typechecking. It might make sense to introduce a public convenience function to `jax.random` that determines that an array is a key array, analogous to `jnp.iscomplex` et. al. basically being dtype check conveniences as well. Maybe we call it `jax.random.isprngkey`? (RNGs: key types and custom implementations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,hide typed key array class in favor of distinguishing key arrays by dtype,"From the outside perspective, a typed key array should be known as a `jax.Array` with a special element type. Currently we support—and even encourage—distinguishing it based on a special array type (`PRNGKeyArray`) instead, but let's move away from that. To achieve this, we'll want to hide the array type and expose the dtype in some way. At least two notable ways in which this will affect code that has already started to use typed key arrays: * `isinstance(keys, PRNGKeyArray)` will need an alternative based on `keys.dtype` * Type annotations with `PRNGKeyArray` will need to be switched to `jax.Array`. This is more permissive in principle, but actually the current `jax.random` module sets `PRNGKeyArray` to `Any` during typechecking. It might make sense to introduce a public convenience function to `jax.random` that determines that an array is a key array, analogous to `jnp.iscomplex` et. al. basically being dtype check conveniences as well. Maybe we call it `jax.random.isprngkey`? (RNGs: key types and custom implementations)",2023-07-13T15:55:03Z,enhancement,closed,3,10,https://github.com/jax-ml/jax/issues/16716,"Sounds great to me! I've never used `numpy.iscomplex` so the convenience wrapper doesn't sound crucial (though it sounds fine). I usually write things more like `np.issubdtype(x.dtype, np.integer)` or whatever. (EDIT: I wrote ""isinstance"" but I meant ""issubdtype"", thanks Jake.)","Maybe `jnp.issubdtype(x.dtype, random.PRNGDtype)` or something like that?","We might want to be able to answer two questions: * Is this an array of keys of a particular implementation (e.g. `'threefry2x32'`)? * Is this an array of keys, of any RNG implementation? There may be some analogy to Numpy's `issubdtype` versus dtype equality, although we don't have to follow suit even if so.","I think I'd propose treating this like any other dtype. E.g. adding dtypes `jax.dtypes.key` and `jax.dtypes.threefry`, such that `jnp.issubdtype(threefry, key)` and `jnp.issubdtype(key, jnp.generic)`.",Initial proposal in CC(custom prng: introduce mechanism to identify key arrays by dtype),"I think I ran into some changes related to this. I like kidger's suggestion for the runtime behavior. For the static type checking, it may be worth considering making the dtype generic like NumPy does.  I proposed this earlier for common dtypes.  For RNG dtypes, it would be really nice because you could then provide a much more constrained type annotation:  This would then work as usual and give errors (at least with PyRight) if someone tries to pass another type of Jax array.  (MyPy seems to be fine with it too.)","Thanks for the comment – it may be that typed generics would be useful for annotating dtypes, but we haven't looked more deeply into that. I personally tend to be pretty pessimistic about the usefulness of overlysophisticated Python type annotations: in my experience they lead to far more noise and false positives than actual bugs caught. You end up having to write your code in a stilted way to appease the type checker, or alternatively sprinkle ` type: ignore` throughout, which entirely defeats the purpose. This is one reason why I kept `jax.Array` simple: it hits the sweet spot of enabling reasonable annotations and `isinstance` checks without introducing the diminishing returns of more complicated types. That said, I think reasonable people can disagree, and if someone wanted to explore using more complicated types, I wouldn't get in their way. Regarding the bug here: going forward, I don't think you should rely on `KeyArray` as a meaningful annotation. If anything, it should probably be aliased to `KeyArray = Array`, because key arrays are just jax arrays with a different dtype. Again, I think that would hit the sweet spot of being useful without being overly complex.","> I personally tend to be pretty pessimistic about the usefulness of overlysophisticated Python type annotations: I agree with you in general. Although in Jax, I bet 99% of arrays are either:  Just distinguishing these two would be beneficial for not accidentally passing value arrays as keys and vice versa. I understand if the Jax team has better things to do though.  Just on my wish list. > Regarding the bug here: going forward, I don't think you should rely on `KeyArray` as a meaningful annotation. I'm just using the definition provided by Jax:  Right now it seems to be `Any` in static analysis.  I guess that'll eventually change to `Array` in some future version of Jax?","> Right now it seems to be `Any` in static analysis. I guess that'll eventually change to `Array` in some future version of Jax? In some future version of JAX, `KeyArray` will no longer be a public symbol, because it's an implementation detail that we don't want to expose to the user (the main thrust of this issue is that `KeyArray` should no longer be load bearing, in favor of using `dtype` to distinguish newstyle keys).",Finished in https://github.com/google/jax/pull/16781
863,"以下是一个github上的jax下的一个issue, 标题是(nvlink fatal : Value 'sm_80' is not defined for option 'arch')， 内容是 ( Description I have a problem running jax with CUDA on a workstation with A100 GPUS. I have created a conda environment with python 3.9 and installed jax as described:  However, when using any jax function, for example:  I get the following error message: `jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Value 'sm_80' is not defined for option 'arch'`  What jax/jaxlib version are you using? 0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? A100 GPU  Additional system info Linux  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,nvlink fatal : Value 'sm_80' is not defined for option 'arch'," Description I have a problem running jax with CUDA on a workstation with A100 GPUS. I have created a conda environment with python 3.9 and installed jax as described:  However, when using any jax function, for example:  I get the following error message: `jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256, output: nvlink fatal   : Value 'sm_80' is not defined for option 'arch'`  What jax/jaxlib version are you using? 0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? A100 GPU  Additional system info Linux  NVIDIA GPU info ",2023-07-12T22:03:29Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16699,I think this is the same issue as https://github.com/google/jax/issues/16586. Can you try the workarounds in https://github.com/google/jax/issues/16586issuecomment1630817089 ?,"> I think this is the same issue as CC(jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256). Can you try the workarounds in  CC(jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with nonzero error code 256) (comment) ? Thanks, solved the issue!"
203,"以下是一个github上的jax下的一个issue, 标题是(JIT through ragged axes)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JIT through ragged axes,,2023-07-11T19:09:27Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16685
1247,"以下是一个github上的jax下的一个issue, 标题是(pjit uses too much memory)， 内容是 ( Description Suppose I have a large state stored on CPU, and a tree of shardings matching the state structure. Let's assume the partition is fairly uniform, so that about the same amount of memory should supposedly be allocated per device after sharding. On my multiGPU setup, the following code goes out of memory:  while the following does not:  Why? The result should be the same. In both cases, I should obtain a state sharded across my GPU devices. **Some context** The example above is a proof of concept. More specifically, I am trying to instantiate on GPU the state of GPTJ, an LLM with 6b parameters. The state is mainly comprised of the model parameters, as well as two more replicas corresponding to the mu and nu parameters of an optax.adam optimizer. So, 18b parameters in total. In halfprecision (float16, i.e. 2 bytes), this gives me 36b bytes, that is 36 GB of memory. My Amazon EC2 instance has 8 GPUs, each with 16 GiB of GPU memory. I have created a sharding tree that partitions the state fairly uniformly across the 8 de)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,pjit uses too much memory," Description Suppose I have a large state stored on CPU, and a tree of shardings matching the state structure. Let's assume the partition is fairly uniform, so that about the same amount of memory should supposedly be allocated per device after sharding. On my multiGPU setup, the following code goes out of memory:  while the following does not:  Why? The result should be the same. In both cases, I should obtain a state sharded across my GPU devices. **Some context** The example above is a proof of concept. More specifically, I am trying to instantiate on GPU the state of GPTJ, an LLM with 6b parameters. The state is mainly comprised of the model parameters, as well as two more replicas corresponding to the mu and nu parameters of an optax.adam optimizer. So, 18b parameters in total. In halfprecision (float16, i.e. 2 bytes), this gives me 36b bytes, that is 36 GB of memory. My Amazon EC2 instance has 8 GPUs, each with 16 GiB of GPU memory. I have created a sharding tree that partitions the state fairly uniformly across the 8 de",2023-07-11T07:33:00Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/16679,"I think you will have to provide more information regarding the `pjit` case. Can you dump the HLO please? Also if there are no arguments, then there is no need to specify `in_shardings`. It's an optional argument."," Here is something perhaps better. I managed to create an example reproducing the issue.  As described above, I'm running this across 8 GPU devices. When using `pjit`, I get the following warning,   and then an error message like `Execution of replica 0 failed: INTERNAL: Failed to allocate XXX bytes for new constant`. I don't understand why, since in theory the sharded state should fit in GPU memory. On the other end, when I comment the line with `pjit` and uncomment the line with `device_put`, it works as expected. Any ideas? This is blocking me quite a bit. Thanks!","The error message tells me that it is the `state` that you are closing over which is causing a problem. Try creating the `state` inside the pjitted function instead of materializing it outside? This way the state will be materialized as sharded directly. Also, if device_put works, then why are you using `pjit` to do this? `device_put` seems like a better solution to me instead of running a XLA computation.","The initial `state` is on CPU, not on GPU. I expect `pjit` to materialize the state as sharded directly on GPU. The code above is just a simplification, but eventually I would need to instantiate the state using `pjit`. The state will be created using `flax.train_state.create`. This will internally instantiate the parameters of an `optax` optimizer. While it might be possible to instantiate the whole state on CPU first (currently, I wouldn't know how to do it, as optax directly initializes its parameters on GPU, if possible), it seems like a hassle compared to just using `pjit`.  Independently of the best practice for this though, `pjit` seems to take far too much memory. I would love to understand why, and how to circumvent this. If `pjit` takes so much memory to materialize the state, it might also take a lot of memory when distributing a training step, making it very hard to train large models.","Hi, bringing this up again as I haven't yet managed to solve the issue. Any chance you might have time to look into this?","Here is a common solution:  Besides, from my understanding, if the `params` are sharded, the `train_state` created from the params would also be sharded.","Hi, we are observing the same issue. We have a model that runs fine with pmap but fails at initialization with pjit. We tested this with pjit batch parallelism and ZerO sharding and both fail.  We are doing exactly the same, loading pretrained numpy params and closing over them with pjit. Our function only takes the `rng_key` as input and inside the network checks whether params are present in outer pretrained params and does `jnp.asarray` otherwise it creates a new param to train from scratch. With pmap this works fine as it's somehow able to free up the memory, but with pjit simple batch parallelism fails. It compiles fine but then fails with: > XlaRuntimeError: RESOURCE_EXHAUSTED: Error loading program: Attempting to allocate 11.49G. That was not possible. There are 3.59G free.; (1x1x0_HBM0):"," thanks for the answer. Loading parameters on CPU is not really the bottleneck here  in my example script above, `params` is loaded on CPU. The problem arises later when sharding, basically the last line of your little script.  if you happen to find a solution, would you mind sharing it here? Thanks!",A workaround that we ended up going for is to create a new function for initializing parameters that takes pretrained parameters as input instead of taking them from the global scope. This way there are no values to close over so pjit should not OOM.  The fact that pjit does not efficiently handle memory of values closed over still worries me though. ,Do you have an update on this?
1253,"以下是一个github上的jax下的一个issue, 标题是(Wider support for FP8 datatypes)， 内容是 ( I am trying to use float8 datatypes as part of training quantized spiking neural networks but cannot initialise weights for a network because jax.random.\_uniform does not support jnp.float8_* dtypes. My intent is to train on an H100 GPU which supports fp8 with double the throughput of fp16. The error message for jax.random.\_uniform seems to be a little out of date as well as it only discusses fp32/64 while also accepting fp16.  I'm not sure how much work it would take to permit fp8 types but would greatly appreciate it! It seems that the uniform sampling method would need updated but I'm not sure how many other changes would be required to support fp8. Beyond language model applications, neuroevolution would largely benefit from fp8 because of the fact that you could double the population size for the same compute/memory capacity and SNN research would also benefit as one could represent both the input spikes and model weights in fp8 and eliminate having to recast int8 inputs to fp16 values for use. _Originally posted by  i)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",large language model,Wider support for FP8 datatypes," I am trying to use float8 datatypes as part of training quantized spiking neural networks but cannot initialise weights for a network because jax.random.\_uniform does not support jnp.float8_* dtypes. My intent is to train on an H100 GPU which supports fp8 with double the throughput of fp16. The error message for jax.random.\_uniform seems to be a little out of date as well as it only discusses fp32/64 while also accepting fp16.  I'm not sure how much work it would take to permit fp8 types but would greatly appreciate it! It seems that the uniform sampling method would need updated but I'm not sure how many other changes would be required to support fp8. Beyond language model applications, neuroevolution would largely benefit from fp8 because of the fact that you could double the population size for the same compute/memory capacity and SNN research would also benefit as one could represent both the input spikes and model weights in fp8 and eliminate having to recast int8 inputs to fp16 values for use. _Originally posted by  i",2023-07-10T19:27:55Z,enhancement,closed,0,15,https://github.com/jax-ml/jax/issues/16673,"Hi  thanks for the request! I think this is a fundamentally difficult question, for the same reasons as those discussed in https://github.com/google/jax/discussions/13798discussioncomment4499272. The issue is that typical algorithms for generating uniform deviates assume that the set of floating point numbers has similar behavior to the set of real numbers, but as you reduce the bit width this becomes less and less true. For example, if we were generating uniform deviates in `float8`, we will quickly run into resolution limits when trying to select values between `0.0` and `1.0`. Here's some quick code to visualize which values are actually possible to represent:  !download1 Again, this shows all the values that are *possible* to represent in `float8_e5m2`, and this makes clear that it wouldn't be entirely straightforward to generate uniformlydistributed values in this range, since it's really fundamentally a discrete distribution. What do you think? Do you know of any existing work in this area?"," , That's a good point  I'll have to do some research as I don't know a good method for uniformly sampling floats for 8 bits. I've tried looking at the Brevitas and bitsandbytes packages but haven't found anything there. Based on the other issue you linked perhaps uniformly sampling in a higher precision and casting down to FP8 might be a decent hack? My other thought would be some kind of inverse frequency weighting or using a lookup table since the 8 bit space is relatively small? In the meantime I think I might explore binary initialisation for the networks since this is a tough question.","> My other thought would be some kind of inverse frequency weighting or using a lookup table since the 8 bit space is relatively small? Inverse frequency weighting requires floatingpoint math, so you'd probably have to do it in higher precision, at which point you've given up on the goal of generating float8 directly. I think generating at high precision and then casting is a useful solution, if that's suitable for the usecase. But that's easy enough to do already with something like `random.uniform(key, size).astype('float8')`. It's not clear to me that we should default to that approach if you pass `float8` to `uniform`.","> I think generating at high precision and then casting is a useful solution, if that's suitable for the usecase. But that's easy enough to do already with something like random.uniform(key, size).astype('float8'). It's not clear to me that we should default to that approach if you pass float8 to uniform. Yeah that makes sense from a design perspective. The downside of having to rely on astype/casting at the moment is that I don't think that it dovetails/works with jmp/the jax mixed precision library which allows for setting the dtype for neural net modules in an organised fashion. I agree that it would be nice to have a way to directly generate in fp8 as while it may not be an issue for a neural network that gets initialised only once at the beginning it would definitely negate any gains for applications such as neuroevolution if it's still sampling in fp16 under the hood and then casting.","I haven't done a deep dive on the dtype internals but is there a reason why the jnp.float8 types work when casting with astype but not when specifying them as the dtype of an array from the start? Thank you so much for giving this attention! `>>> import jax.numpy as jnp >>> import jax >>> jax.random.rademacher(jax.random.PRNGKey(0), [3,3]).astype(jnp.float8_e4m3fn) Array([[1, 1, 1],        [1, 1, 1],        [1, 1, 1]], dtype=float8_e4m3fn) >>> jax.random.rademacher(jax.random.PRNGKey(0), [3,3], dtype=jnp.float8_e4m3fn) Traceback (most recent call last):   File """", line 1, in    File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/random.py"", line 1780, in rademacher     return _rademacher(key, shape, dtype)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 208, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 155, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/core.py"", line 2633, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1085, in _pjit_call_impl     compiled = _pjit_lower(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1177, in _pjit_lower     return _pjit_lower_cached(jaxpr, in_shardings, out_shardings, *args, **kwargs)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1237, in _pjit_lower_cached     return pxla.lower_sharding_computation(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 2072, in lower_sharding_computation     nreps, tuple_args) = _cached_lowering_to_hlo(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 1919, in _cached_lowering_to_hlo     lowering_result = mlir.lower_jaxpr_to_module(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 617, in lower_jaxpr_to_module     lower_jaxpr_to_fun(   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 932, in lower_jaxpr_to_fun     out_vals, tokens_out = jaxpr_subcomp(ctx.replace(name_stack=callee_name_stack),   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1044, in jaxpr_subcomp     in_nodes = map(read, eqn.invars)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 1019, in read     return ir_constants(v.val, canonicalize_types=True)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 217, in ir_constants     out = handler(val, canonicalize_types)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 295, in _ndarray_constant_handler     return _numpy_array_constant(val, canonicalize_types)   File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/interpreters/mlir.py"", line 249, in _numpy_array_constant     attr = ir.DenseElementsAttr.get(x, type=element_type, shape=shape) jax._src.traceback_util.UnfilteredStackTrace: ValueError: cannot include dtype '4' in a buffer The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File """", line 1, in    File ""/home/legion/.local/lib/python3.10/sitepackages/jax/_src/random.py"", line 1780, in rademacher     return _rademacher(key, shape, dtype) ValueError: cannot include dtype '4' in a buffer `","It's because `jax.random` doesn't know about float8 dtypes, but `jax.numpy` does. They're still relatively new and experimental, and not fully supported across the JAX package.",Gotchya  while direct initialization of float8s seems like a tough problem would at least be possible to support float8 types beyond just jax.numpy so that neural networks could at least perform inference in float8? It seems like this would require enabling them in the MLIR interpreter... I know these types are new but they're supported on the H100 so they'll be around for years and it would be nice to not have to deal with NVIDIA's transformer engine package to bolton float8 support. Thank you so much for your time!,"Sure, it’s possible to support float8 types in other places, and they’re already being used in neural network training. It seems like my recommendation of generating random numbers in float32 and then casting to float8 is what you want, and that’s currently a supported operation. Does that not address your issue?"," , Initialising/maintaining the parameters in higher precision works for the time being, the issue remains that when trying to use float8s for compute/outputs there is still a hang up that prevents things from running. I know this involves the Haiku/JMP but I believe the blockage remains within JAX here. policy I'm trying to use:  last part of error trace when attempting to initialise the neural net:  I can try to conjure up a minimal example if that would be helpful. ","I see, thanks. This is an example of what I mentioned – FP8 is still experimental and not fully supported throughout JAX (mainly because there are not supported operations on all hardware). Here's a minimal example of what you ran into: ",So just to be clear the intent is that float8 will remain in its current experimental state and be limited to jax.numpy for the time being until there is broader hardware support for it? If that's the case then there's nothing more to address at this time. Thank you for taking the time to respond and follow up on this!,"I'm actually not sure what the intent is, but it's probably safe to assume that experimental narrowwidth dtypes will have limited support at least for the near future."," CC(Make jit work with custom float inputs) fixes the float8+jit issue you saw above. There are still operations that are not supported for float8, but this gets us a step closer.","`kernel_init = lambda key,shape,dtype : jax.nn.initializers.normal(stddev=self.config.initializer_range）(key,shape,jnp.float32).astype(self.dtype)` It could fix kernel_init problem. But I found my H100 running FP8 and FP16 with same speed!",I'm going to close this – I think it's expected that for the foreseeable future float8 will only be supported for certain operations.
981,"以下是一个github上的jax下的一个issue, 标题是(Different result while using ``at[].apply`` inside jax.vmap)， 内容是 ( Description Jax's Array index helper `at` potentially has a bug when used inside `jax.vmap`. Using ``apply(lambda...)``  inside ``jax.vmap`` gives me a different result as to when run sequentially and inside jit.  Here's the following code that I'm trying to run  I get the expected result; ``[1 2 3 4 5 6]`` However, when the same function is used inside jax.vmap I get a different result  ``[[1 2 3 0 5 6]]`` Note that I had to add another dimension in order to use it inside vmap which I don't think is the root cause. Took me a day of debugging to hunt down this bug 😭.   What jax/jaxlib version are you using? jax==0.4.13 jaxlib==0.4.13  Which accelerator(s) are you using? CPU  Additional system info WSL  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Different result while using ``at[].apply`` inside jax.vmap," Description Jax's Array index helper `at` potentially has a bug when used inside `jax.vmap`. Using ``apply(lambda...)``  inside ``jax.vmap`` gives me a different result as to when run sequentially and inside jit.  Here's the following code that I'm trying to run  I get the expected result; ``[1 2 3 4 5 6]`` However, when the same function is used inside jax.vmap I get a different result  ``[[1 2 3 0 5 6]]`` Note that I had to add another dimension in order to use it inside vmap which I don't think is the root cause. Took me a day of debugging to hunt down this bug 😭.   What jax/jaxlib version are you using? jax==0.4.13 jaxlib==0.4.13  Which accelerator(s) are you using? CPU  Additional system info WSL  NVIDIA GPU info _No response_",2023-07-08T00:32:54Z,bug,closed,1,4,https://github.com/jax-ml/jax/issues/16655,"Thanks for the report, and sorry for the difficulty in tracking down the bug. We really appreciate it though, especially with the minimal repro. > I get the expected result; > [1 2 3 4 5 6] Did you mean `[1 2 3 3 5 6]`? This certainly looks like a bug to me! It reproduces on TPU too. Here are the jaxprs:     What's up with that second `update_jaxpr`!?","Yeah sorry, scatterapply just isn't batchable; there's a rule defined but it doesn't work. In particular, it ignores the `update_jaxpr` (and hence the userdefined fucntion) it's given.",Thanks for the report! The fix should make it into the next JAX release.,Note CC(Bug: scatter_apply autodiff returns incorrect result) – it looks like `scatter_apply` still doesn't work correctly with autodiff. I'm going to try to tackle this soon.
1209,"以下是一个github上的jax下的一个issue, 标题是(`random.split`: generalize to optional shape parameter)， 内容是 (Pair with ! Take two on CC(random.split: add optional shape parameter). Several PRNG implementations (notably partitionable threefry) support splitting to arbitrary shapes, rather than only to a 1D vector of keys. This change: * Upgrades `jax.random.split` to accept a general shape as an argument. * Updates the internal PRNG interface, and our various PRNG implementations, to accept and handle such a shape argument. This change keeps the argument name `num`. We can still think on whether and how we'd like to upgrade to `shape`. Note that we could have supported arbitrary shapes by reduction to the previous API (with a flat split count), using reshapes. We'd like to avoid that, so as not to hide this structure from the underlying implementation. For instance, partitionable threefry hashes a *shaped* iota in order to split keys, and we don't want to flatten and reshape around that for no reason. Fixes CC(random.split() should allow for passing in a desired shape))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`random.split`: generalize to optional shape parameter,"Pair with ! Take two on CC(random.split: add optional shape parameter). Several PRNG implementations (notably partitionable threefry) support splitting to arbitrary shapes, rather than only to a 1D vector of keys. This change: * Upgrades `jax.random.split` to accept a general shape as an argument. * Updates the internal PRNG interface, and our various PRNG implementations, to accept and handle such a shape argument. This change keeps the argument name `num`. We can still think on whether and how we'd like to upgrade to `shape`. Note that we could have supported arbitrary shapes by reduction to the previous API (with a flat split count), using reshapes. We'd like to avoid that, so as not to hide this structure from the underlying implementation. For instance, partitionable threefry hashes a *shaped* iota in order to split keys, and we don't want to flatten and reshape around that for no reason. Fixes CC(random.split() should allow for passing in a desired shape)",2023-07-06T21:10:01Z,kokoro:force-run pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/16644
802,"以下是一个github上的jax下的一个issue, 标题是(Jaxpr of a function without input argument is wrong)， 内容是 ( Description I am writing a function without any input argument and want to translate it into Jaxpr. Here is the example,  The output Jaxpr:  The Jaxpr treats the `jnp.array([0, 0, 0, 1])` as an input argument, which is a wrong behavior. But I found that the `invars` for the Jaxpr is empty. Is this a bug or feature? If it is a feature, how can I get the Jaxpr for a function without argument correctly?  What jax/jaxlib version are you using? Internal version  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Jaxpr of a function without input argument is wrong," Description I am writing a function without any input argument and want to translate it into Jaxpr. Here is the example,  The output Jaxpr:  The Jaxpr treats the `jnp.array([0, 0, 0, 1])` as an input argument, which is a wrong behavior. But I found that the `invars` for the Jaxpr is empty. Is this a bug or feature? If it is a feature, how can I get the Jaxpr for a function without argument correctly?  What jax/jaxlib version are you using? Internal version  Which accelerator(s) are you using? CPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-07-06T18:15:50Z,question,closed,0,3,https://github.com/jax-ml/jax/issues/16643,"It seems that it is not related to whether there is any argument or not. After I add one argument to the function, it stills treat the `jnp.array([0, 0, 0, 1], jnp.float32)` in the `jnp.where` as one *addtional* input argument.","Hi  thanks for the report! This is expected behavior. Essentially the only way to get array data into jaxprs is to either (1) create the array with a primitive like `iota` (i.e. `arange`) or `full`, or (2) pass the data as an argument to the jaxpr. In this case, you created an array within your function, but there's no XLA primitive for `jnp.asarray` with arbitrary Python arguments. So in the process of tracing this, JAX constructs that array and adds it as an implicit argument to the jaxpr. Does that make sense?",Thanks for your explanation! I will create the array in another way.
598,"以下是一个github上的jax下的一个issue, 标题是(`jax.lax.while_loop` with no attribute error `GSPMDSharding`)， 内容是 ( Description Hi,  I'm trying to implement a finding root algorithm `bisect` in the following function.   A similar error occurs when I've tried to use `jaxopt.Bisection`.    What jax/jaxlib version are you using? jax v0.4.13 jaxlib v0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.5, Ubuntu  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.lax.while_loop` with no attribute error `GSPMDSharding`," Description Hi,  I'm trying to implement a finding root algorithm `bisect` in the following function.   A similar error occurs when I've tried to use `jaxopt.Bisection`.    What jax/jaxlib version are you using? jax v0.4.13 jaxlib v0.4.13+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.5, Ubuntu  NVIDIA GPU info ",2023-07-06T06:54:35Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16636,"I'm sorry for mistaking this with a bug. It seems there is problem with my Jupyter notebook. Upon restart the notebook, the error is solved. I don't know why it is so though."
704,"以下是一个github上的jax下的一个issue, 标题是(TypeError: Shapes must be 1D sequences of concrete values of integer type- dynamic_slice)， 内容是 ( Description I'm using jax to implement a pca function, but I get an error. Here's the code  When I modify it with lax.dynamic_slice `array = lax.dynamic_slice(H, (row, row  1), (rows  row,))` An error occurs again  How can I fix it? Thanks a lot!  What jax/jaxlib version are you using? jax  0.4.8, jaxlib  0.4.7  Which accelerator(s) are you using? GPU  Additional system info python 3.8 , linux/mac  NVIDIA GPU info ++               )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,TypeError: Shapes must be 1D sequences of concrete values of integer type- dynamic_slice," Description I'm using jax to implement a pca function, but I get an error. Here's the code  When I modify it with lax.dynamic_slice `array = lax.dynamic_slice(H, (row, row  1), (rows  row,))` An error occurs again  How can I fix it? Thanks a lot!  What jax/jaxlib version are you using? jax  0.4.8, jaxlib  0.4.7  Which accelerator(s) are you using? GPU  Additional system info python 3.8 , linux/mac  NVIDIA GPU info ++               ",2023-07-04T05:59:25Z,question,closed,0,5,https://github.com/jax-ml/jax/issues/16622,"The issue is that `row` is a traced value, so `array = H[row:, row1]` has a dynamic shape. JAX transormations like `jit` and `vmap`, as well as higherorder functions like `scan` and `fori_loop` require all array shapes to be static. This means that the shapes of arrays cannot depend on dynamic values like `row` in your program. If you want to use JAX, you'll have to reexpress your program in terms of staticallyshaped arrays. You can read more at JAX Sharp Bits: Dynamic Shapes.","> The issue is that `row` is a traced value, so `array = H[row:, row1]` has a dynamic shape. JAX transormations like `jit` and `vmap`, as well as higherorder functions like `scan` and `fori_loop` require all array shapes to be static. This means that the shapes of arrays cannot depend on dynamic values like `row` in your program. If you want to use JAX, you'll have to reexpress your program in terms of staticallyshaped arrays. >  > You can read more at JAX Sharp Bits: Dynamic Shapes. Thanks ! How can I reexpress my program in terms of staticallyshaped arrays? I didn't find a suitable example in the tutorial yet.😭","I don’t see any easy way to express this program in terms of static shapes, but I probably don’t understand the algorithm as well as you do.","  I'm sorry if my algorithm has confused you. I rewrote a simple version of the function  The main problem I encountered was how to split the data from the first n_components. The error of the above code is:  When I use lax.dynamic_slice, I still get an error: ","Understood  it’s not possible to split out the first `n` components if `n` is dynamic. Note that `dynamic_slice` will not help you here because it is for slicing a static *number* of elements at a dynamic *location*, not slicing a dynamic number of elements. You’ll have to figure out how to reexpress your algorithm in terms of staticallysized slices if you want to use this algorithm with JAX. It’s not clear to me that there’s a straightforward approach to this given your code, but you may be able to figure one out."
1291,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Improve shape constraint checking using shape assertions)， 内容是 (This is a major improvement in the shape constraint checking. Even before this change JAX assumes certain constraints on the shapes of actual arguments in presence of shape polymorphism. For example,  given the `polymorphic_shapes=""(b, b, 2*d)""` specification for an argument `arg`, JAX assumes the following:   * `arg.shape[0] >= 1`   * `arg.shape[1] == arg.shape[0]`   * `arg.shape[2] % 2 == 0` and `arg.shape[0] // 2 >= 1` Previously, these constraints were checked only in graph serialization mode and only for eager execution, when we had access to the actual shapes of `arg` during tracing. With this change we check them for native serialization also (relying on newly added support for shape assertions to tf.XlaCallModule version 7). For graph serialization, we use `tf.debugger.assert`, which works in eager execution mode **and** graph execution mode (but not `jit_compile=True` mode, due to TF limitations) and produces the same errors as for native serialization. Note that even after submitting this change it does not become ac)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Improve shape constraint checking using shape assertions,"This is a major improvement in the shape constraint checking. Even before this change JAX assumes certain constraints on the shapes of actual arguments in presence of shape polymorphism. For example,  given the `polymorphic_shapes=""(b, b, 2*d)""` specification for an argument `arg`, JAX assumes the following:   * `arg.shape[0] >= 1`   * `arg.shape[1] == arg.shape[0]`   * `arg.shape[2] % 2 == 0` and `arg.shape[0] // 2 >= 1` Previously, these constraints were checked only in graph serialization mode and only for eager execution, when we had access to the actual shapes of `arg` during tracing. With this change we check them for native serialization also (relying on newly added support for shape assertions to tf.XlaCallModule version 7). For graph serialization, we use `tf.debugger.assert`, which works in eager execution mode **and** graph execution mode (but not `jit_compile=True` mode, due to TF limitations) and produces the same errors as for native serialization. Note that even after submitting this change it does not become ac",2023-07-03T14:35:35Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/16615,"  PTAL. I would like to submit this, even though we cannot activate it for a while as explained in the descritiption, because I have more changes coming stacked on top of this. You can also see this as cl/547484806 if it is more convenient.",This got merged as part of merging https://github.com/google/jax/pull/16710.
809,"以下是一个github上的jax下的一个issue, 标题是(increase random test coverage over RNG key constructors and representations)， 内容是 (This is an incremental change to our random tests that primarily: * Increases test coverage of both key constructors (`random.key` and `random.PRNGKey`), often by parameterizing tests over both. * Increases test coverage of both key representations (typed key arrays and `uint32` arrays). * Removes a handful of guards on `config.jax_enable_custom_prng`, either replacing them with `isinstance` checks for typed keys or removing them altogether if possible. * Makes a handful of other individual test improvements and fixes, and leaves comments for more.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,increase random test coverage over RNG key constructors and representations,"This is an incremental change to our random tests that primarily: * Increases test coverage of both key constructors (`random.key` and `random.PRNGKey`), often by parameterizing tests over both. * Increases test coverage of both key representations (typed key arrays and `uint32` arrays). * Removes a handful of guards on `config.jax_enable_custom_prng`, either replacing them with `isinstance` checks for typed keys or removing them altogether if possible. * Makes a handful of other individual test improvements and fixes, and leaves comments for more.",2023-07-01T03:31:25Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16605
923,"以下是一个github上的jax下的一个issue, 标题是(Jax' transfer guard and XLA-CPU)， 内容是 ( Description Jax' transfer guard  potentially has two problems related to the CPUplaced arrays. 1. Transfering numpy (or pure python) arrays/scalars to XLACPU device is considered to be host2device transfer:  Transfer guard says: `hosttodevice transfer: aval=ShapedArray(int32[]), dst_sharding=GSPMDSharding({replicated})` Ideally I want to be able to ignore `numpy > XLACPU` transfers and only trigger the transfer guard for transfers to the accelerators. 2. Interestingly, XLACPU > numpy is not triggering transfer guard:  Transfer guard says: 🤐.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax' transfer guard and XLA-CPU," Description Jax' transfer guard  potentially has two problems related to the CPUplaced arrays. 1. Transfering numpy (or pure python) arrays/scalars to XLACPU device is considered to be host2device transfer:  Transfer guard says: `hosttodevice transfer: aval=ShapedArray(int32[]), dst_sharding=GSPMDSharding({replicated})` Ideally I want to be able to ignore `numpy > XLACPU` transfers and only trigger the transfer guard for transfers to the accelerators. 2. Interestingly, XLACPU > numpy is not triggering transfer guard:  Transfer guard says: 🤐.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-30T17:34:51Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/16602,"Thanks for the report! I suspect the reason for this is that while XLA:CPU → NumPy transfers can always be done in a zerocopy fashion, NumPy → XLA:CPU transfers cannot always be done in this manner. The reason for this asymmetry is that XLA arrays have stricter requirements regarding byte layout than do NumPy arrays.  may be able to provide more context on the behavior you observe here","Yes, as  pointed out, the asymmetry came from the zerocopy vs. copy difference in two transfer directions.","I see, thanks for the clarification regarding the asymmetry. It was actually not my main question, sorry for obscure phrasing. The main problem is that I can not configure transfer guard to disallow implicit accelerator transfers, while allowing XLA:CPU transfers. Normally, I do not care much about host/cpu memory, but care about acceleratorrelated transfers and memory. Do you think you can add something like `jax.transfer_guard(""disallow_accelerator)` or  `jax.transfer_guard(""disallow_tpu)`?","There seem to be a few potential highlevel design options: 1. Focusing on logical operations that involve converting NumPy array and JAX array, or transferring JAX arrays to different devices: This is semantically clean/succinct though it may not necessarily be the not useful form of the API. In this direction, we will want to make XLA:CPU to NumPy to be guarded and remove the asymmetry. 2. Focusing on involved backend or platform types (e.g., ""CPU"", ""GPU"", ""TPU""): It takes into account the backend/platform types that are involved in the transfer, expecting the user to apply expert knowledge on which backend/platform types are considered costly when used for transfers. 3. Focusing on involved communication channels (e.g., ""memcpy"", ""pcie"", ""host network"", ""accelerator interconnect"": This instead makes JAX knowledgeable on which underlying communication channels of transfers can be expensive. JAX needs to determine which level it can draw a line, or it may attempt to let the user choose a certain transfer type. Since lowlevel runtimes typically do not explicitly reveal what communication channels are used, JAX often has to guess communication channel types being used by the lowlevel runtimes. The current transfer guard is close to Option 3 with a single threshold that looks at memcpy/relayouting. As for this feature request to extend the transfer guard API, potential candidates are (a) Option 2 with multiple backend/paltform type support (proposal in https://github.com/google/jax/issues/16602issuecomment1625413621). (b) Option 2 with XLA:CPU being ignored for all users as a special case. (c) Option 3 with multiple channel type support. (d) Option 3 with the single threshold that is raised to ignore local memcpy/relayouting for all users. Is this list complete? I think we should also hear from JAX core runtime devs.","Just stumbled upon this too and agree with Alex. The guard implementation (and most of Jax actually) already makes very clear difference between the concept of ""host"" and ""device"": https://github.com/google/jax/blob/7e1278c0400eed18a57b6402f9308c023493f942/jax/_src/config.pyL1218L1221 With that in mind, I think specialcasing ""host to host transfer"" (ie memcpy/XLA:CPU) as in your (b) suggestion seems to be the best tradeoff between practical usefulness, work required, and simplicity. Edit: I guess another way out is to allow us to cook up our own transfer guard, which would be possible if `transfer_guard_device_to_device` came with a flag or callback to choose which devices it covers. Edit on the edit: or maybe not, as I'd like to configure such transfer guard globally using `jax.config.update`."
1359,"以下是一个github上的jax下的一个issue, 标题是(vmap raises ambiguous exception when inconsistent sizes for array axes to be mapped occurs with user defined objects as static parameters)， 内容是 ( Description When trying to use vmap with functions, that accept other functions, an ambiguous error is raised if there is are inconsistent sizes for array axes to be mapped. A minimal example is shown below:  Running the above code produces the following stack trace:  I've tracked down the issue to how `jax._src.api._mapped_axis_size` creates the exception to inform users about inconsistent sizes for array axes to be mapped. When trying to get the data type of a function object, a TypeError is raised and not caught, resulting in an ambiguous exception as it is unclear why the exception is raised (since vmap normally works just fine with functions passed as static arguments). I've made a pull request that fixes the issue ( CC(Fixed _mapped_axis_size raising an uncaught TypeError)), but I'm not sure if my fix (making a wrapper around a problematic call to  `shaped_abstractify(x).str_short()` is compliant with jax's coding standards. With this fix we get the expected stack trace for this error:   What jax/jaxlib version are you )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,vmap raises ambiguous exception when inconsistent sizes for array axes to be mapped occurs with user defined objects as static parameters," Description When trying to use vmap with functions, that accept other functions, an ambiguous error is raised if there is are inconsistent sizes for array axes to be mapped. A minimal example is shown below:  Running the above code produces the following stack trace:  I've tracked down the issue to how `jax._src.api._mapped_axis_size` creates the exception to inform users about inconsistent sizes for array axes to be mapped. When trying to get the data type of a function object, a TypeError is raised and not caught, resulting in an ambiguous exception as it is unclear why the exception is raised (since vmap normally works just fine with functions passed as static arguments). I've made a pull request that fixes the issue ( CC(Fixed _mapped_axis_size raising an uncaught TypeError)), but I'm not sure if my fix (making a wrapper around a problematic call to  `shaped_abstractify(x).str_short()` is compliant with jax's coding standards. With this fix we get the expected stack trace for this error:   What jax/jaxlib version are you ",2023-06-29T23:05:11Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/16594,"Thanks for the report! Here's a shorter repro:  It seems the main issue is that when you pass invalid arguments to `vmap`, it can fail in constructing the error message if there are nonarray inputs.","That's a much more succinct way to put it! That's exactly the issue. The root cause seems to be an uncaught exception when trying to determine the data type of the nonarray inputs. The pull request I made tries to fix it. I just cleaned it up, but in squashing commits to one commit, there seems to be a noreply.com author () who's being blocked by the CLA check. Let me know if I need to do anything to fix that or if you'd rather fix the issue some other way.",Made a clean version of the pull request at CC(Fixed _mapped_axis_size raising an uncaught TypeError). 
1262,"以下是一个github上的jax下的一个issue, 标题是(Stricter validation of inputs to `dynamic_update_slice_in_dim`)， 内容是 (I was extremely surprised to find that `start_indices` is implicitly set to 0 for dimensions other than `axis`, rather than requiring that all other dimensions be equal or broadcastable to each other:  I think it's nonobvious that this is how the `dynamic_update_*_in_dim` methods would work, so noting it in the documentation would be nice. This led to me really struggling to track down a bug yesterday, which turned out to be in the following code:  Unfortunately, I called this code both with arrays of shape `(batch, head, time, hidden)` _and_ `(batch2, batch, head, time, hidden)`. When I was looking for the bug, I thought this couldn't be the problem since I was assuming it would error if it received ""incompatible"" shapes. Later when I did find it, I assumed it was just broadcasting since the `time` dimension was size 1. (Thanks for the tip on Twitter !) If this _is_ an unintended use of `dynamic_update_slice_in_dim`, it might be nice to do a dimension check, but maybe people are relying on this behavior.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Stricter validation of inputs to `dynamic_update_slice_in_dim`,"I was extremely surprised to find that `start_indices` is implicitly set to 0 for dimensions other than `axis`, rather than requiring that all other dimensions be equal or broadcastable to each other:  I think it's nonobvious that this is how the `dynamic_update_*_in_dim` methods would work, so noting it in the documentation would be nice. This led to me really struggling to track down a bug yesterday, which turned out to be in the following code:  Unfortunately, I called this code both with arrays of shape `(batch, head, time, hidden)` _and_ `(batch2, batch, head, time, hidden)`. When I was looking for the bug, I thought this couldn't be the problem since I was assuming it would error if it received ""incompatible"" shapes. Later when I did find it, I assumed it was just broadcasting since the `time` dimension was size 1. (Thanks for the tip on Twitter !) If this _is_ an unintended use of `dynamic_update_slice_in_dim`, it might be nice to do a dimension check, but maybe people are relying on this behavior.",2023-06-29T21:39:19Z,enhancement,open,0,4,https://github.com/jax-ml/jax/issues/16592,"Hi  thanks for the report, and sorry for the unclear semantics here. Can you say more about what your expected result would be? Were you expecting an error because `u.shape[1] != z.shape[1]`?","Yeah I think I was expecting the semantics to be something like numpy slice updates, so for axis=2:  I think another way this might trip someone up is if they were expecting broadcasting of size1 dimensions, but instead got the 0th index updated in those dimensions.","Makes sense, thanks. I believe this was the intended usecase for this function, I might add some shape validation and see if it breaks any code.",Reopening and renaming to track the possibility of adding input validation to these functions.
1239,"以下是一个github上的jax下的一个issue, 标题是(`jax.numpy.ndarray.at[].set()` much slower than numpy on GPU?)， 内容是 (I am sorry  I have been opening new issues quite frequently on indexing in the last couple of days and that might me annoying. I am just finding some limitations in the indexing operations, and, for each I wonder if there's is a solution (or if it is a fake problem). This might be a microbenchmark, but if this holds also for a longer program, when a set of functions rely heavily on this mechanism, updates becomes an issue. I see this is also the case for TF https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update, so perhaps not a limitation of jax per sé.  !image The x axis is shifted by `1`, so `0` should read`1`.  Also, I am sure I am misinterpreting because I don't know XLA, but it seems like the indexed array is copied at every update (`  %copy.1 = f32[100]{0} copy(f32[100]{0} %Arg_0.1)`), which is the opposite of what the docs%20compiled%20function%2C%20expressions%20like%20x%20%3D%20x.at%5Bidx%5D.set(y)%20are%20guaranteed%20to%20be%20applied%20in%2Dplace.) say?  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.numpy.ndarray.at[].set()` much slower than numpy on GPU?,"I am sorry  I have been opening new issues quite frequently on indexing in the last couple of days and that might me annoying. I am just finding some limitations in the indexing operations, and, for each I wonder if there's is a solution (or if it is a fake problem). This might be a microbenchmark, but if this holds also for a longer program, when a set of functions rely heavily on this mechanism, updates becomes an issue. I see this is also the case for TF https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update, so perhaps not a limitation of jax per sé.  !image The x axis is shifted by `1`, so `0` should read`1`.  Also, I am sure I am misinterpreting because I don't know XLA, but it seems like the indexed array is copied at every update (`  %copy.1 = f32[100]{0} copy(f32[100]{0} %Arg_0.1)`), which is the opposite of what the docs%20compiled%20function%2C%20expressions%20like%20x%20%3D%20x.at%5Bidx%5D.set(y)%20are%20guaranteed%20to%20be%20applied%20in%2Dplace.) say?  ",2023-06-29T13:40:03Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/16587,"Hi  this is expected, because in the JAX CPU and GPU  versions, you're not only benchmarking array updates, but array copies. The reason for copies is that, in the Python side of things, no two JAX arrays can share the same memory, and so a JAX function that returns an input array must return a copy. When you do a microbenchmark of a numpy function that returns a view vs. a JAX function that returns a copy, the relevant difference will be the cost of the copy. But how is that consistent with the docs you link to, which claim that updates do not lead to copies? Well, in the course of a real program, the update will generally not cause the array to be copied, because you generally will not be returning the result of the array update in a Python function, and so no copy needs to be made. In short, it's not the update that's causing your array to be copied, it's your `return` statement. For more on what to expect when comparing microbenchmarks of JAX and NumPy code, see FAQ: Is JAX Faster than NumPy?.","If you want a fairer comparison of JAX and NumPy, you could do something like this:  Returning `a.sum()` makes the JAX and numpy code moreorless semantically equivalent, and you can see that jaxoncpu is similar to numpy, but jaxongpu beats numpy once the arrays are large enough to overcome JAX's larger Python dispatch overhead. The main computation cost here is the `sum`, because the updates happen inplace and virtually free in all three cases: !download","> In short, it's not the update that's causing your array to be copied, it's your return statement. Golden info, thank you very much for the thorough answer, Jake.",Great! Feel free to let us know if you run into other issues
562,"以下是一个github上的jax下的一个issue, 标题是(jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with non-zero error code 256)， 内容是 ( Description I'm unable to get JAX on nvidia GPUs working  Start up script   Demonstration of error inside of virtual env   What jax/jaxlib version are you using? jax==0.4.11, jaxlib==0.4.11  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jaxlib.xla_extension.XlaRuntimeError: INTERNAL: nvlink exited with non-zero error code 256," Description I'm unable to get JAX on nvidia GPUs working  Start up script   Demonstration of error inside of virtual env   What jax/jaxlib version are you using? jax==0.4.11, jaxlib==0.4.11  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info _No response_",2023-06-29T02:00:25Z,bug XLA NVIDIA GPU,closed,0,8,https://github.com/jax-ml/jax/issues/16586,I had the same problem with CUDA 12.1., is this a known issue with 12.1 compatibility to the JAX team?, have you been able to find a work around?,"I upgraded CUDA to 12.2 when possible; otherwise, set `XLA_FLAGS=xla_gpu_force_compilation_parallelism=1` to suppress the issue.",I am getting the same error with CUDA12.0. Could anyone figure it out?,"This issue will occur if you have an older CUDA toolkit installed (containing `nvlink`), when using the CUDA `pip` install of JAX. You can either upgrade your local CUDA installation (so `nvlink` in the PATH is newer), remove `nvlink` from your PATH, or use the workaround suggested in https://github.com/google/jax/issues/16586issuecomment1626852639. We can probably make this fail more gracefully in a future JAX release.",https://github.com/openxla/xla/pull/4262 should fix this and it should be part of the next jaxlib release.,The fix is merged in XLA. Another and slightly better workaround until we make a new jaxlib release: set `TF_USE_NVLINK_FOR_PARALLEL_COMPILATION=0` Hope that helps!
1263,"以下是一个github上的jax下的一个issue, 标题是(""Minimal output batching"" option for vmap)， 内容是 ( pointed out to me that `out_axes` is superflous in a lot of cases, since the output batching structure could in theory be inferred without user input. For example: consider running a transformer with partially batched parameters, i.e. the first 5 layers of a transformer are unbatched, but there are N copies of each parameter tensor for the last 5 layers. The `out_axes` structure needed for the keyvalue cache is the something like `[None] * 5 + [0] * 5`. It's not that hard to specify, but it's a bit annoying that it's required to specify something in two different ways (both through the input and output axes). The feature I'm suggesting is an option to vmap which only batches outputs when necessary, rather than by default. This shouldn't be the default behavior, since something like `vmap(lambda x: 0.)(jnp.ones(3))`  is expected to return an array of size 3. To allow toggling the optional behavior, ~~`out_axes=None` could be defined to have this behavior~~, or it could be invoked by `out_axes=SomeModeConstant`. I'm not sure, b)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"""Minimal output batching"" option for vmap"," pointed out to me that `out_axes` is superflous in a lot of cases, since the output batching structure could in theory be inferred without user input. For example: consider running a transformer with partially batched parameters, i.e. the first 5 layers of a transformer are unbatched, but there are N copies of each parameter tensor for the last 5 layers. The `out_axes` structure needed for the keyvalue cache is the something like `[None] * 5 + [0] * 5`. It's not that hard to specify, but it's a bit annoying that it's required to specify something in two different ways (both through the input and output axes). The feature I'm suggesting is an option to vmap which only batches outputs when necessary, rather than by default. This shouldn't be the default behavior, since something like `vmap(lambda x: 0.)(jnp.ones(3))`  is expected to return an array of size 3. To allow toggling the optional behavior, ~~`out_axes=None` could be defined to have this behavior~~, or it could be invoked by `out_axes=SomeModeConstant`. I'm not sure, b",2023-06-29T00:46:02Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/16585,"Equinox has this. `equinox.filter_vmap(..., out_axes=equinox.internal.if_mapped(0))`. This is like `out_axes=0` if it is batched, and `out_axes=None` if it is not. Right now it's in the undocumanted `eqx.internal` namespace, just because this requires touching some JAX internals (specifically I do `isinstance(output, BatchTracer) and output._trace.main is main`)."
903,"以下是一个github上的jax下的一个issue, 标题是(`sparse.grad` only returns the gradient with respect to the first element of a PyTree)， 内容是 ( Description When applying `sparse.grad` from `jax.experimental.sparse` to a function which take in a Pytree as the first argument, only the gradient with respect to the first item in the Pytree is returned. This is both unexpected and inconsistent with the behaviour of `jax.grad`, which returns a gradient which has the same tree structure as the input.  Here is a small working example demonstrating this behaviour:  Here are the outputs for the different tests:   What jax/jaxlib version are you using? jax v0.4.13  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`sparse.grad` only returns the gradient with respect to the first element of a PyTree," Description When applying `sparse.grad` from `jax.experimental.sparse` to a function which take in a Pytree as the first argument, only the gradient with respect to the first item in the Pytree is returned. This is both unexpected and inconsistent with the behaviour of `jax.grad`, which returns a gradient which has the same tree structure as the input.  Here is a small working example demonstrating this behaviour:  Here are the outputs for the different tests:   What jax/jaxlib version are you using? jax v0.4.13  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-28T21:12:56Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/16582,Have there been any updates on this bug or hints about where it may originate from?,"Hey  sorry for being silent here. This is a bug in how `sparse.grad` is implemented. I don't think we have any plans to fix it at the moment: `jax.experimental.sparse` is experimental, and you should expect it to have some rough edges.","It looks like this is bug in the logic for postprocessing gradients. https://github.com/google/jax/blob/c1f234a95cb0932cd23ad63a9ddbe0a8d43333b7/jax/experimental/sparse/ad.pyL69L71 Currently, if `argnums` indexes a single pytree, such as a dictionary of parameters, this triggers only the first of the computed gradients to be returned. The logic doesn't account for pytrees being unpacked into multiple arguments when flattened. I opened a draft PR which passes the current sparse tests and should match the behavior of `jax.grad()` when argnums indexes a pytree. The current sparse testing lacks coverage of this pytree repacking behavior (clearly), so I think writing tests for that is the next step if this looks reasonable.",Added some tests and took the PR out of draft.
905,"以下是一个github上的jax下的一个issue, 标题是(Gradient of `sqrtm`)， 内容是 ( Description I require taking gradients with respect to the square root of a matrix. So, I have turned to `jax.scipy.linalg.sqrtm`. Based on previous discussions and issues on this repo, I understand that it is only implemented on CPU. I can accept this for now  I can always do a callback when necessary and just eat the computational overhead. But, I am getting an error when trying to calculate gradients w.r.t. this operation. For example, see the following minimal example:  This yields the following error:   What jax/jaxlib version are you using? jax v0.4.13, jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python 3.9.7, Ubuntu 18.04.6  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Gradient of `sqrtm`," Description I require taking gradients with respect to the square root of a matrix. So, I have turned to `jax.scipy.linalg.sqrtm`. Based on previous discussions and issues on this repo, I understand that it is only implemented on CPU. I can accept this for now  I can always do a callback when necessary and just eat the computational overhead. But, I am getting an error when trying to calculate gradients w.r.t. this operation. For example, see the following minimal example:  This yields the following error:   What jax/jaxlib version are you using? jax v0.4.13, jaxlib v0.4.13  Which accelerator(s) are you using? CPU  Additional system info Python 3.9.7, Ubuntu 18.04.6  NVIDIA GPU info _No response_",2023-06-28T15:16:15Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16579,"Thanks for the report. There is indeed a missing argument in the schur JVP implementation, but unfortunately all it's preventing is a `NotImplementedError`: https://github.com/google/jax/blob/f463437c7ee81f915d2404d302b6bc5b32ecffbe/jax/_src/lax/linalg.pyL2113L2116 I don't think this is trival to implement unfortunately; see some related discussion at https://github.com/google/jax/issues/669."
504,"以下是一个github上的jax下的一个issue, 标题是(Finish pile_map support enough for a basic transformer example)， 内容是 (The remaining operations to cover were  Slicing a nonragged axis when ragged axes are present  Einsum where both tensor and contraction dimensions are ragged Also improved the error message that happens when trying to broadcast a ragged axis to a rectangular one)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Finish pile_map support enough for a basic transformer example,The remaining operations to cover were  Slicing a nonragged axis when ragged axes are present  Einsum where both tensor and contraction dimensions are ragged Also improved the error message that happens when trying to broadcast a ragged axis to a rectangular one,2023-06-23T17:15:45Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/16541,Side note: maybe rename pile_map to something else? Just google `piles` and you'll see what I am talking about ;)," Yes, that's on the agenda."
914,"以下是一个github上的jax下的一个issue, 标题是(`experimental.odeint` custom gradient rule breaks with integer arguments)， 内容是 ( Description I know that `experimental.odeint` is in a sense discontinued, but maybe there is an easy fix for this? MWE:  This happens because odeint's custom gradient rule attempts to compute the gradient wrt all arguments, even `arg2` which is an integer and I was not trying to actually differentiate. While this might seem nonsensical, this case shows up if you try to integrate with odeint a differential equation defined by a sparse matrix, which has the structure encoded in integers.  What jax/jaxlib version are you using? jax 0.4.11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`experimental.odeint` custom gradient rule breaks with integer arguments," Description I know that `experimental.odeint` is in a sense discontinued, but maybe there is an easy fix for this? MWE:  This happens because odeint's custom gradient rule attempts to compute the gradient wrt all arguments, even `arg2` which is an integer and I was not trying to actually differentiate. While this might seem nonsensical, this case shows up if you try to integrate with odeint a differential equation defined by a sparse matrix, which has the structure encoded in integers.  What jax/jaxlib version are you using? jax 0.4.11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-22T09:36:52Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/16517
536,"以下是一个github上的jax下的一个issue, 标题是(Preserve integer dtype in broadcast_one_to_all)， 内容是 (I was trying to broadcast PRNGKeys using `jax.experimental.multihost_utils.broadcast_one_to_all` while in 64 bit mode, which resulted in invalid keys because the `uint32` arrays were promoted to `uint64`. This PR fixes this. reproducer:  output before: `uint32 uint64` output after this change: `uint32 uint32`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Preserve integer dtype in broadcast_one_to_all,"I was trying to broadcast PRNGKeys using `jax.experimental.multihost_utils.broadcast_one_to_all` while in 64 bit mode, which resulted in invalid keys because the `uint32` arrays were promoted to `uint64`. This PR fixes this. reproducer:  output before: `uint32 uint64` output after this change: `uint32 uint32`",2023-06-21T21:08:00Z,,closed,0,9,https://github.com/jax-ml/jax/issues/16511,"Hey , is this the recommended practice for other APIs in JAX too? Will this break a bunch of stuff? Is there a flag that the user can set which would give them this behavior?","Looking at this more closely, I think this is intended because you are setting the x64 flag to True so things will get promoted. I think this is WAI.","It's intended that `int32` gets cast to `int64` in summation aggregates. This was bakedin to `jax.numpy.sum` very early, because it's what `numpy.sum` does. We looked at changing that a while ago (the `promote_integers` argument was something I added as an intended backwardcompatibilty shim after we had decided to do this), but midway through the planned change there was a lot of pushback because it introduced the possibility of silent integer overflows in some cases (integer overflows are already a danger for other integer operations virtually everywhere already, but I digress) Whether it makes sense that `broadcast_one_to_all` calls `jnp.sum` directly and inherits its dtype behavior is less clear to me. I don't think that this PR should be merged asis, though: it would be confusing if `psum` had different type semantics from `sum`. Perhaps `psum` should gain a `promote_integers` argument? And perhaps `broadcast_one_to_all` should have different dtype semantics than `sum`? I'm not sure.","> Looking at this more closely, I think this is intended because you are setting the x64 flag to True so things will get promoted. I think this is WAI. Sorry, I think I my initial example was not clear enough (simplified too much), here is the reproducer for the actual bug I am trying to solve:  which fails with the following error:  Feel free to close this PR, and let me know if I should open an issue instead, if that is easier for you.",We should never be calling `jnp.sum` on PRNG Keys. Yash – is it possible to implement `broadcast_one_to_all` without invoking summation? Why is summation necessary?,"Note that in the future, adding or summing PRNG keys will result in an error, because semantically PRNG keys do not support addition. (You can set `jax_enable_custom_prng=True` to see how this will manifest). This tells me that either (1) `broadcast_one_to_all` is semantically incompatible with PRNG keys, or (2) `broadcast_one_to_all` does not require addition/summation of data.","Maybe broadcast_one_to_all is not the right solution for whatever you are trying to do. Can you please open a discussions topic and I can tag in relevant folks to help? Please explain what you are trying to do when you do open the discussions topic rather than just the error you are seeing. (maybe also close this PR?) > broadcast_one_to_all does not require addition/summation of data. No, that's not possible. We need to run a collective to allreduce across all the hosts so that the broadcasting can happen.","OK, then I think we can safely say that `broadcast_one_to_all` is not an appropriate operation for PRNG keys.","> Maybe broadcast_one_to_all is not the right solution for whatever you are trying to do. Can you please open a discussions topic and I can tag in relevant folks to help? Please explain what you are trying to do when you do open the discussions topic rather than just the error you are seeing. (maybe also close this PR?) I realized that for my usecase I am better off broadcasting the seed, instead of the PRNGkey. There it is no problem if the type is promoted, therefore no further discussion will be needed. I still think it is inconsistent if a broadcast function changes the type, but so be it. Thanks for the help so far, I will close this PR."
511,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: module 'jax.interpreters.xla' has no attribute 'DeviceArray')， 内容是 ( Description Hi, I am trying to save the model in. my network, but it appears an error in the title.  error track:   What jax/jaxlib version are you using? 0.4.12  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info 3090)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AttributeError: module 'jax.interpreters.xla' has no attribute 'DeviceArray'," Description Hi, I am trying to save the model in. my network, but it appears an error in the title.  error track:   What jax/jaxlib version are you using? 0.4.12  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info 3090",2023-06-21T13:10:43Z,bug needs info,closed,0,7,https://github.com/jax-ml/jax/issues/16505,"Well, that's correct, `DeviceArray` was removed from JAX several versions ago. The question is what other code is trying to refer to it. It's not really possible to tell based on the information you gave, but perhaps try updating other packages like `flax`?","Thanks for your reply. I am trying to save the checkpoint.  Should I discard this approach to save the checkpoint rather than turn to other solutions, like this URL? I have updated the package to `flax==0.4.2`. Could you please give me a quick example about how to save and load the model at the stateoftheart JAX version(`0.4.12`)? ","Well, what you are doing may work. I think the problem is you have an older version of some package that refers to the old JAX type. I think in particular it is likely your`flax` version is much too old: the current release is 0.6.11.","> Thanks for your reply. I am trying to save the checkpoint. Should I discard this approach to save the checkpoint rather than turn to other solutions, like this URL? I have updated the package to `flax==0.4.2`. >  > Could you please give me a quick example about how to save and load the model at the stateoftheart JAX version(`0.4.12`)? Did you solve this problem?","> > Thanks for your reply. I am trying to save the checkpoint. Should I discard this approach to save the checkpoint rather than turn to other solutions, like this URL? I have updated the package to `flax==0.4.2`. > > Could you please give me a quick example about how to save and load the model at the stateoftheart JAX version(`0.4.12`)? >  > Did you solve this problem? Yes. I uninstall the flax and reinstalled `flax==0.6.11` and `jax==0.4.12`. 3090. (do not update the `flax==0.6.11` model)",My code is using flax.optim optimizers and I am using flax==0.5.1. I am unable to checkpoint using orbax ( unexpected attribute .step) or the legacy api  (same error as mentioned in the thread) . Can't update to flax 0.6.0 or greater since there won't be any optim support. How do I save the train state in this case? ,Hi   you'll probably have more luck getting this question answered at https://github.com/google/flax/discussions. Best of luck!
1240,"以下是一个github上的jax下的一个issue, 标题是(GPU: Out of memory)， 内容是 ( Description > Using following batch size 512 > Using following batch size 512 > Number of parameters being optimized: 325764 > I0621 19:37:57.948089 140590213324928 checkpoints.py:252] Restoring checkpoint from /home/gaoa/FreeNeRFstage2/DietNeRFpytorch/dietnerf/logs/checkpoint_25000 > 20230621 19:38:15.226954: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.38GiB (rounded to 1477187584)requested by op > 20230621 19:38:15.227661: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:468] ________________________________________________________________________________________________ > 20230621 19:38:15.227864: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2036] Execution of replica 0 failed: Resource exhausted: Out of memory while trying to allocate 1477187472 bytes. > Traceback (most recent call last): > File ""train.py"", line 531, in > app.run(main) > File ""/home/gaoa/anaconda)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,GPU: Out of memory," Description > Using following batch size 512 > Using following batch size 512 > Number of parameters being optimized: 325764 > I0621 19:37:57.948089 140590213324928 checkpoints.py:252] Restoring checkpoint from /home/gaoa/FreeNeRFstage2/DietNeRFpytorch/dietnerf/logs/checkpoint_25000 > 20230621 19:38:15.226954: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.38GiB (rounded to 1477187584)requested by op > 20230621 19:38:15.227661: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:468] ________________________________________________________________________________________________ > 20230621 19:38:15.227864: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2036] Execution of replica 0 failed: Resource exhausted: Out of memory while trying to allocate 1477187472 bytes. > Traceback (most recent call last): > File ""train.py"", line 531, in > app.run(main) > File ""/home/gaoa/anaconda",2023-06-21T11:50:22Z,bug,open,2,0,https://github.com/jax-ml/jax/issues/16504
295,"以下是一个github上的jax下的一个issue, 标题是([sparse] support custom JVP in sparsify)， 内容是 (At some point this stopped working, but we didn't have test coverage for it.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[sparse] support custom JVP in sparsify,"At some point this stopped working, but we didn't have test coverage for it.",2023-06-21T09:36:41Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16503
335,"以下是一个github上的jax下的一个issue, 标题是(Fix test failure in array_interoperability_test due to 64-bit dtype squashing.)， 内容是 (Fix test failure in array_interoperability_test due to 64bit dtype squashing.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix test failure in array_interoperability_test due to 64-bit dtype squashing.,Fix test failure in array_interoperability_test due to 64bit dtype squashing.,2023-06-20T19:48:56Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16495
384,"以下是一个github上的jax下的一个issue, 标题是([XLA:Python] Fix __cuda_array_interface__.)， 内容是 ([XLA:Python] Fix __cuda_array_interface__. Adds a test for __cuda_array_interface__ that does not depend on cupy. Fixes https://github.com/google/jax/issues/16440)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[XLA:Python] Fix __cuda_array_interface__.,[XLA:Python] Fix __cuda_array_interface__. Adds a test for __cuda_array_interface__ that does not depend on cupy. Fixes https://github.com/google/jax/issues/16440,2023-06-20T15:02:52Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16490
1249,"以下是一个github上的jax下的一个issue, 标题是(Scan over parallelized xmap)， 内容是 ( Description I am performing a computation that requires an outer scan, but the inner computations can be vectorized. Previously, I was just using `vmap`, but to limit memory I have been experimenting with `pmap` and `xmap`. I am on a machine with 2 x 40 GB A100s. I quickly realized that I could not `pmap` inside of the `scan`. So, I turned to `xmap`. However, I am getting some unexpected behavior.  First, the following code *works*:  Both Options 1 and 2 work. However, when I try to place the same computation in a scan, I get an error with Option 2:  This yields the following error:  To be clear, I understand that this computation doesn't require a `scan`, but it is a minimal example of my failing use case. Practically, I need a way to distribute computation within a scan over multiple devices.  I have also seen `shmap` but haven't tried it yet. Thank you in advance for your help.  What jax/jaxlib version are you using? jax v0.4.11, jaxlib v0.4.11+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Scan over parallelized xmap," Description I am performing a computation that requires an outer scan, but the inner computations can be vectorized. Previously, I was just using `vmap`, but to limit memory I have been experimenting with `pmap` and `xmap`. I am on a machine with 2 x 40 GB A100s. I quickly realized that I could not `pmap` inside of the `scan`. So, I turned to `xmap`. However, I am getting some unexpected behavior.  First, the following code *works*:  Both Options 1 and 2 work. However, when I try to place the same computation in a scan, I get an error with Option 2:  This yields the following error:  To be clear, I understand that this computation doesn't require a `scan`, but it is a minimal example of my failing use case. Practically, I need a way to distribute computation within a scan over multiple devices.  I have also seen `shmap` but haven't tried it yet. Thank you in advance for your help.  What jax/jaxlib version are you using? jax v0.4.11, jaxlib v0.4.11+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info",2023-06-19T16:33:30Z,bug,closed,2,3,https://github.com/jax-ml/jax/issues/16476,Just checking in on this!  ,"Hi, I am encountering the same error. Have there been any updates on this ?  I think the same bug was mentioned in this issue: https://github.com/google/jax/discussions/15807","xmap was removed from JAX, so this is obsolete"
302,"以下是一个github上的jax下的一个issue, 标题是(Documentation improvements for XlaCallModule.disabled_checks.)， 内容是 (Documentation improvements for XlaCallModule.disabled_checks.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Documentation improvements for XlaCallModule.disabled_checks.,Documentation improvements for XlaCallModule.disabled_checks.,2023-06-18T12:49:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16469
512,"以下是一个github上的jax下的一个issue, 标题是(Disable a test that fails under Windows with a NumPy exception.)， 内容是 ( I think this is another one for you to look at when you get to it. I think as written this test depends on a particular integer overflow behavior that differs between platforms, but I'm not sure we should be relying on the integer overflow behavior in the first place.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Disable a test that fails under Windows with a NumPy exception.," I think this is another one for you to look at when you get to it. I think as written this test depends on a particular integer overflow behavior that differs between platforms, but I'm not sure we should be relying on the integer overflow behavior in the first place.",2023-06-16T15:03:35Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16453
1145,"以下是一个github上的jax下的一个issue, 标题是(Add `ClosedJaxpr.{const,in,out}vars` forwarding properties.)， 内容是 (`ClosedJaxpr` already defines many properties (like `in_avals` and `out_avals` and `effects`) that forward properties from its underlying `Jaxpr`. This PR adds a few more:  Ducktyped functions like `_check_call` now work with `call_jaxpr: ClosedJaxpr` values without any other changes. Previously, they would fall with an error:  Adding these properties is cleaner than adding adhoc `isinstance(call_jaxpr, ClosedJaxpr)` checks in functions like `core._check_call`. Note: `core._check_call` isn't currently called for `closed_call_p` since it has its own custom typechecking rule. This change would make typechecking work for `closed_call_p` even if `custom_typechecks[closed_call_p]` were removed.  This is helpful for a JAXbased research project that adds a custom primitive subclassing `ClosedCallPrimitive`, but that wants to avoid defining a new typechecking rule and reuse `core._check_call` instead.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Add `ClosedJaxpr.{const,in,out}vars` forwarding properties.","`ClosedJaxpr` already defines many properties (like `in_avals` and `out_avals` and `effects`) that forward properties from its underlying `Jaxpr`. This PR adds a few more:  Ducktyped functions like `_check_call` now work with `call_jaxpr: ClosedJaxpr` values without any other changes. Previously, they would fall with an error:  Adding these properties is cleaner than adding adhoc `isinstance(call_jaxpr, ClosedJaxpr)` checks in functions like `core._check_call`. Note: `core._check_call` isn't currently called for `closed_call_p` since it has its own custom typechecking rule. This change would make typechecking work for `closed_call_p` even if `custom_typechecks[closed_call_p]` were removed.  This is helpful for a JAXbased research project that adds a custom primitive subclassing `ClosedCallPrimitive`, but that wants to avoid defining a new typechecking rule and reuse `core._check_call` instead.",2023-06-16T06:16:31Z,,open,0,4,https://github.com/jax-ml/jax/issues/16446,"Two more ideas below.  1. Remove custom typechecking rule for `closed_call_p` I wonder if it would make sense to remove the custom typechecking rule for `closed_call_p` below? https://github.com/google/jax/blob/d16b8581caa4ccb5ec2dab60b876a3104ba8035f/jax/_src/core.pyL2691L2696 `_check_closed_call` seems to do strictly less than than `_check_call`: https://github.com/google/jax/blob/d16b8581caa4ccb5ec2dab60b876a3104ba8035f/jax/_src/core.pyL2900L2935 And since `custom_typechecks[closed_call_p] = _check_closed_call` is defined, `_check_call` isn't used for `closed_call_p`: https://github.com/google/jax/blob/d16b8581caa4ccb5ec2dab60b876a3104ba8035f/jax/_src/core.pyL2788L2794"," 2. Define `ClosedJaxpr.{const,in,out}vars` properties Currently, this PR makes this change to `_check_call` to support `closed_call_p` in addition to `call_p`:  This avoids the following error:  Instead, we could fix the error by defining more forwarding properties on `ClosedJaxpr`.  I confirmed that this works, and this seems preferable to me, especially since `ClosedJaxpr` already has a bunch of other forwarding properties based on its underlying `Jaxpr`. Could someone please confirm?","I went ahead and implemented + pushed idea (2) above, since I think it's a cleaner solution. Feel free to comment otherwise. >  2. Define `ClosedJaxpr.{const,in,out}vars` properties > ... >  > Instead, we could fix the error by defining more forwarding properties on `ClosedJaxpr`. >  > ",I think there is a reason why those properties don't exist on ClosedJaxpr. Maybe  remembers why? Doing the isinstance check is fine I think since that pattern exists in other places too.
814,"以下是一个github上的jax下的一个issue, 标题是(nested gmres does not work)， 内容是 ( Description I am building an iterative solver. Naturally some of my linear operators are defined iteratively using gmres. A simple example would be  On top of this linear operator I can define another linear operator  However, jax would throw an error message whenever i am trying to call `b`.  Here is the full script:   And here is the error message > TypeError: Value UndefinedPrimal(ShapedArray(float32[])) with type  is not a valid JAX type  What jax/jaxlib version are you using? jax v0.4.12  Which accelerator(s) are you using? cpu  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,nested gmres does not work," Description I am building an iterative solver. Naturally some of my linear operators are defined iteratively using gmres. A simple example would be  On top of this linear operator I can define another linear operator  However, jax would throw an error message whenever i am trying to call `b`.  Here is the full script:   And here is the error message > TypeError: Value UndefinedPrimal(ShapedArray(float32[])) with type  is not a valid JAX type  What jax/jaxlib version are you using? jax v0.4.12  Which accelerator(s) are you using? cpu  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-15T20:41:13Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/16441,Curiously it seems to work fine if you use `cg` or `bicgstab` for either `a` or `b` (or both) so it seems to be limited to `gmres` not other iterative linear solvers. Under the hood both `cg` and `bicgstab` use similar infrastructure that `gmres` doesn't.,"You may like to try Lineax, which is our new morecomprehensive solution for linear solvers. I've just checked and it appears to handle this case without issues.","Assuming this is still a bug, I did some digging and I figured out that the issue is probably caused when the solver calls linear_transpose at the end of the solve. For some reason, as opposed to every other time this function is called, when it generates the jaxpr (line 2282 in api.py), despite having, as far as I can tell, the same parameters, it produces a jaxpr with a nonzero length of equations (`len(jaxpr.eqns) > 0`) and produces invars that are not in the constvars. This produces a problem further down the line when backwards_pass is called (line 181 in ad.py). This method generates a mapping of sorts between the constvars of the provided jaxpr and some other constants passed into the function itself. Later, the method iterates over every equation in jaxpr.eqns, and as part of that loop, it tries to read the invars from the mapping it generated earlier. If the invar is not defined in the mapping, it gets an UndefinedPrimal by default. Later, when concrete_aval is called on the values, it tries to concrete_aval on the UndefinedPrimal, which fails because apparently an UndefinedPrimal does not have the `""__jax_array__"" `attribute, which causes the method to error out. I am not sure if UndefinedPrimals are supposed to have that attribute (they probably aren't) and I have no idea why the jaxpr generation method just suddenly decided to throw in a bunch of extra invars and constvars. Every other call I caught made a jaxpr with 0 constvars, 1 invar (a), and 0 equations, which didn't cause issues later in the backwards_pass method since it tried to iterate over the equations and immediately stopped since there aren't any. Once again, as far as I can tell, the inputs into the jaxpr generation method are identical each time.","Alright, the above is not true. The error happens when the function a is called on the pvals, but why is a returning a jaxpr anyway? Shouldn't it just return a single value?","I noticed this when working on some other stuff, but it seems like its a more general problem using `gmres` with `fori_loop` in the linear function:  gives the following assertion error: ``` AssertionError                            Traceback (most recent call last) Cell In[108], line 8       5         return out       6     return jax.lax.fori_loop(0, x.size, body, out) > 8 jax.scipy.sparse.linalg.gmres(bfun, jnp.arange(5).astype(float)) File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/scipy/sparse/linalg.py:704, in gmres(A, b, x0, tol, atol, restart, maxiter, M, solve_method)     702 def _solve(A, b):     703   return _gmres_solve(A, b, x0, atol, ptol, restart, maxiter, M, gmres_func) > 704 x = lax.custom_linear_solve(A, b, solve=_solve, transpose_solve=_solve)     706 failed = jnp.isnan(_norm(x))     707 info = jnp.where(failed, x=1, y=0)     [... skipping hidden 12 frame] File ~/miniconda3/envs/desc/lib/python3.10/sitepackages/jax/_src/lax/control_flow/loops.py:718, in _scan_transpose(reduce_axes, cts, reverse, length, num_consts, num_carry, jaxpr, linear, unroll, *args)     716 ires, _ = split_list(consts, [num_ires])     717 _, eres = split_list(xs, [sum(xs_lin)]) > 718 assert not any(ad.is_undefined_primal(r) for r in ires)     719 assert not any(ad.is_undefined_primal(r) for r in eres)     721 carry_avals, y_avals = split_list(jaxpr.out_avals, [num_carry]) AssertionError: "
520,"以下是一个github上的jax下的一个issue, 标题是(jax `__cuda_array_interface__` not working)， 内容是 ( Description  I get  Futher if you try to access the cuda array interface attribute another error comes up. (This may be a red herring.)   What jax/jaxlib version are you using? jax and jaxlib 0.4.12  Which accelerator(s) are you using? GPU  Additional system info linux w/ nvidia  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax `__cuda_array_interface__` not working, Description  I get  Futher if you try to access the cuda array interface attribute another error comes up. (This may be a red herring.)   What jax/jaxlib version are you using? jax and jaxlib 0.4.12  Which accelerator(s) are you using? GPU  Additional system info linux w/ nvidia  NVIDIA GPU info ,2023-06-15T19:35:56Z,bug NVIDIA GPU,closed,0,2,https://github.com/jax-ml/jax/issues/16440,I did some bisecting and this appears to have broken sometime around 0.4.3 or 0.4.4 when abseil moved from 20220623.0 to 20230125.0.,"https://github.com/google/jax/pull/16490 and https://github.com/openxla/xla/pull/3704 will fix, in the next jaxlib release."
802,"以下是一个github上的jax下的一个issue, 标题是(jax only recognized 1 out of 8 gpus)， 内容是 ( Description I'm trying to use jax with 8 nvidia a100 GPUs and jax.devices only recognizes 1 of them. In a fresh virtual environment I have executed the following command.  As you can see there are 8 A100 running CUDA Version: 12.0   pip is upgraded  installing jax[cuda] following https://github.com/google/jaxinstallation  Here you can see that jax.devices is only recognizing 1 of the gpu (or at least that's what you'd think)   What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax only recognized 1 out of 8 gpus, Description I'm trying to use jax with 8 nvidia a100 GPUs and jax.devices only recognizes 1 of them. In a fresh virtual environment I have executed the following command.  As you can see there are 8 A100 running CUDA Version: 12.0   pip is upgraded  installing jax[cuda] following https://github.com/google/jaxinstallation  Here you can see that jax.devices is only recognizing 1 of the gpu (or at least that's what you'd think)   What jax/jaxlib version are you using? 0.4.11  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_,2023-06-15T19:03:07Z,bug NVIDIA GPU,closed,0,3,https://github.com/jax-ml/jax/issues/16438,Can you confirm you don't have `CUDA_VISIBLE_DEVICES` set in your enviroment?,"Setting it resolved the problem, thank you!","Closing, since that seems like it's working as intended."
596,"以下是一个github上的jax下的一个issue, 标题是(Metal: Failed Assertion  / Crash when trying to create arrays in double-precision)， 内容是 ( Description There seems to be an issue with casting for the edge case of size zero arrays.  The last two commands will crash the kernel   What jax/jaxlib version are you using? jax 0.4.11, jaxlib 0.4.10, jaxmetal 0.0.2  Which accelerator(s) are you using? Apple GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Metal: Failed Assertion  / Crash when trying to create arrays in double-precision," Description There seems to be an issue with casting for the edge case of size zero arrays.  The last two commands will crash the kernel   What jax/jaxlib version are you using? jax 0.4.11, jaxlib 0.4.10, jaxmetal 0.0.2  Which accelerator(s) are you using? Apple GPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-15T15:40:38Z,bug Apple GPU (Metal) plugin,open,0,8,https://github.com/jax-ml/jax/issues/16435,"Just to confirm, this is on the Apple metal backend, yes? (All of this should work fine on nonexperimental backends)","I think the Metal plugin simply doesn't support f64, but it should report it more gracefully."," , that's right. Metal doesn't have FP64 support, we will add support to fail more gracefully."," , this issue is not related to Empty shapes. We don't have support for FP64 and we will add errorchecks early to exit gracefully.","Thanks for the prompt reply, I understand the issue now.",">  , that's right. Metal doesn't have FP64 support, we will add support to fail more gracefully. Are you planning on adding FP64 support? I would be really keen on that.","While we are looking into the issue, FP64 support will NOT be there for sometime. ", thank you for letting me know. 
522,"以下是一个github上的jax下的一个issue, 标题是(Custom PRNG: improve test coverage when enable_custom_prng=false)， 内容是 (We're now moving to a world where custom PRNG should exist sidebyside with the old PRNG implementation. This change improves test coverage for that, by enabling relevant custom PRNG tests even when the flag is set to False. Part of CC(RNGs: key types and custom implementations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Custom PRNG: improve test coverage when enable_custom_prng=false,"We're now moving to a world where custom PRNG should exist sidebyside with the old PRNG implementation. This change improves test coverage for that, by enabling relevant custom PRNG tests even when the flag is set to False. Part of CC(RNGs: key types and custom implementations)",2023-06-15T10:41:17Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16428
1141,"以下是一个github上的jax下的一个issue, 标题是(`jax` v0.4.12 with CUDA 12 depends on non-existent `jaxlib` version)， 内容是 ( Description **Problem:** The `jax` library `v0.4.12` with CUDA 12 locally installed depends on `jaxlib` `v0.4.12` using CUDNN 8.8: https://github.com/google/jax/blob/4c02f2c748a498a8c18e084e6c418afc5f6ec991/setup.pyL22L27 But for `jaxlib v0.4.12` there are only releases for CUDNN 8.9: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html, for example `jaxlib0.4.12+cuda12.cudnn89cp311cp311manylinux2014_x86_64.whl` Therefor, `jax` cannot be installed on Linux with a local CUDA 12 installation because neither having CUDNN 8.8 nor 8.9 will help because the dependencies of `jax` itself cannot be installed.  **Solution:** Publish a built for `jaxlib v0.4.12` for CUDNN 8.8 on the registry. Thank you in advance!  What jax/jaxlib version are you using? jax v0.4.12, jaxlib v0.4.12  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info RTX 3060 Ti)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,`jax` v0.4.12 with CUDA 12 depends on non-existent `jaxlib` version," Description **Problem:** The `jax` library `v0.4.12` with CUDA 12 locally installed depends on `jaxlib` `v0.4.12` using CUDNN 8.8: https://github.com/google/jax/blob/4c02f2c748a498a8c18e084e6c418afc5f6ec991/setup.pyL22L27 But for `jaxlib v0.4.12` there are only releases for CUDNN 8.9: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html, for example `jaxlib0.4.12+cuda12.cudnn89cp311cp311manylinux2014_x86_64.whl` Therefor, `jax` cannot be installed on Linux with a local CUDA 12 installation because neither having CUDNN 8.8 nor 8.9 will help because the dependencies of `jax` itself cannot be installed.  **Solution:** Publish a built for `jaxlib v0.4.12` for CUDNN 8.8 on the registry. Thank you in advance!  What jax/jaxlib version are you using? jax v0.4.12, jaxlib v0.4.12  Which accelerator(s) are you using? GPU  Additional system info Linux  NVIDIA GPU info RTX 3060 Ti",2023-06-15T01:40:35Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16421,"Thanks for the report! I think this is a duplicate of CC(Installation Bug: No Matching distribution found for jaxlib==0.4.12+cuda12.cudnn88; extra == ""cuda12_local"" (from jax[cuda12_local]))","> Thanks for the report! I think this is a duplicate of CC(Installation Bug: No Matching distribution found for jaxlib==0.4.12+cuda12.cudnn88; extra == ""cuda12_local"" (from jax[cuda12_local])) Thank you for the quick reply! Sorry, I was mainly looking through the open issues, not the closed ones. I could fix it by installing it from the repo directory, or downgrading CuDNN to 8.8 and using `0.4.10` (`0.4.11` had some issue with my CuDNN). Best, Moritz"
503,"以下是一个github上的jax下的一个issue, 标题是(JAX CUDA 12 wheel is outdated)， 内容是 ( Description I am unable to install the latest version of JAX 0.4.12 through the command:  Version 0.4.11 is installed instead.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX CUDA 12 wheel is outdated, Description I am unable to install the latest version of JAX 0.4.12 through the command:  Version 0.4.11 is installed instead.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info ++  ++,2023-06-14T16:56:17Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16412,"Thanks for the report! I think this is a duplicate of CC(Installation Bug: No Matching distribution found for jaxlib==0.4.12+cuda12.cudnn88; extra == ""cuda12_local"" (from jax[cuda12_local]))."
374,"以下是一个github上的jax下的一个issue, 标题是(CI nightly: update nightly wheel location)， 内容是 (It looks like this is now the recommended location for nightly numpy/scipy wheels (see https://github.com/scipy/scipy/issues/18675issuecomment1590697109))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,CI nightly: update nightly wheel location,It looks like this is now the recommended location for nightly numpy/scipy wheels (see https://github.com/scipy/scipy/issues/18675issuecomment1590697109),2023-06-14T09:32:36Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16400,"nightly CI failure is expected (it's tracked in CC(⚠️ Nightly upstreamdev CI failed ⚠️)) but the installation of the nightly libraries succeeded before the expected test failure, which is the relevant piece here."
459,"以下是一个github上的jax下的一个issue, 标题是(Fix test failure in LaxBackedNumpyTest.testFrexp4 on Windows.)， 内容是 (NumPy is inconsistent between platforms on what it returns for the exponent of an infinite input. On Linux/Mac it returns 0, and on Windows it returns 1. Normalize the test reference result to use 0 in this case.  FYI.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix test failure in LaxBackedNumpyTest.testFrexp4 on Windows.,"NumPy is inconsistent between platforms on what it returns for the exponent of an infinite input. On Linux/Mac it returns 0, and on Windows it returns 1. Normalize the test reference result to use 0 in this case.  FYI.",2023-06-14T00:44:12Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16393,I can locally verify this fixed the tests.
262,"以下是一个github上的jax下的一个issue, 标题是(Implement einsum for (at least some combinations) of arrays with ragged dimensions)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Implement einsum for (at least some combinations) of arrays with ragged dimensions,,2023-06-13T21:06:53Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/16386
1006,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Add native_lowering_disabled_checks parameter to jax2tf.convert)， 内容是 (Previously, we had a boolean parameter `native_lowering_strict_checks` that was disabling all safety checks. This mechanism had several disadvantages:   * the mechanism did not differentiate between different safety checks. E.g., in order to disable checking of the custom call targets, one had to disable checking for all custom call targets, and also the checking that the serialization and execution platforms are the same.   * the mechanism operated only at serialization time. Now, the XlaCallModule supports a `disabled_checks` attribute to control which safety checks should be disabled. Here we replace the `native_serialization_strict_checks` with `native_serialization_disabled_checks`, whose values are sequences of disabled check descriptors.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Add native_lowering_disabled_checks parameter to jax2tf.convert,"Previously, we had a boolean parameter `native_lowering_strict_checks` that was disabling all safety checks. This mechanism had several disadvantages:   * the mechanism did not differentiate between different safety checks. E.g., in order to disable checking of the custom call targets, one had to disable checking for all custom call targets, and also the checking that the serialization and execution platforms are the same.   * the mechanism operated only at serialization time. Now, the XlaCallModule supports a `disabled_checks` attribute to control which safety checks should be disabled. Here we replace the `native_serialization_strict_checks` with `native_serialization_disabled_checks`, whose values are sequences of disabled check descriptors.",2023-06-11T13:12:46Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/16347,   PTAL,"> I assume that we don't expect `native_serialization_disabled_checks` to be long. Otherwise, there are a few places where we are doing linear scan over this list, which we might want to fix to use `set` instead. Indeed, I expect that this will be used rarely and will be short. E.g., there are no uses anymore in Google of `native_serialization_strict_checks` (there were a few in the past, temporarily)."
1147,"以下是一个github上的jax下的一个issue, 标题是(Custom forward and reverse rule for function via transpose hitting python error)， 内容是 ( Description I was chatting with  about how to register both a custom vjp and jvp for use in https://github.com/EnzymeAD/EnzymeJAX (a plugin for Jax to import foreign code as a layer using Enzyme, testing out C++ to begin with). So far I have separate functions for use in a vjp/vjp since JaX isn't able to do both a custom forward and reverse registration on a single function. A conversation with  led me to trying to  making a custom transpose. I couldn't find docs on this so please forgive my ignorance here. I tried to use the custom_transpose in a variety of ways that all seemed to lead to errors that I don't quite understand. For example, one such attempt:   will hit   What jax/jaxlib version are you using? commit 21fc6e0229e0f5f1cb5f1f69d2c3daa2e5c2ca11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Custom forward and reverse rule for function via transpose hitting python error," Description I was chatting with  about how to register both a custom vjp and jvp for use in https://github.com/EnzymeAD/EnzymeJAX (a plugin for Jax to import foreign code as a layer using Enzyme, testing out C++ to begin with). So far I have separate functions for use in a vjp/vjp since JaX isn't able to do both a custom forward and reverse registration on a single function. A conversation with  led me to trying to  making a custom transpose. I couldn't find docs on this so please forgive my ignorance here. I tried to use the custom_transpose in a variety of ways that all seemed to lead to errors that I don't quite understand. For example, one such attempt:   will hit   What jax/jaxlib version are you using? commit 21fc6e0229e0f5f1cb5f1f69d2c3daa2e5c2ca11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_  ",2023-06-09T19:54:31Z,bug,closed,0,12,https://github.com/jax-ml/jax/issues/16336,"I was briefly using `custom_transpose` for the same purpose but eventually ran into issues. The issue that I filed about this shows a working example CC(`AssertionError` in `jax.custom_transpose` if linearized) . Currently, I am using something along the lines of  Unfortunately, `linear_call` does not support batching nor higherorder derivatives :( (which doesn't matter to Enzyme in this case though)",bump  any updates?,"Custom transposition is (very gradual) work in progress, and not yet functioning. It's not documented for that reason, and I think shouldn't have it exposed as a public symbol altogether; most attempts to use it today will raise errors. I should figure out how to do that so as to to avoid confusion. The remaining question is: what can we do for your use case without involving `custom_transpose`? One answer is to set up a custom primitive with a transposition rule. Custom primitives usually take more work, since they need a rule for every transformation. But, looking at your example (plus guessing from this being Enzyme), I gather you're already quite familiar with setting up custom primitives. Is that right? If so, what do you think of setting up one more for this purpose? To be a bit more precise, what I have in mind is that you use a custom primitive in place of where you were trying to use custom transpose. This primitive would be bound from within the JVP rule of another operation.","Yeah I think that would be effective here (perhaps even more so). In particular, it would be nice to also integrate Enzyme's batching/vector mode into vmap. This also aligns nicely with some other ongoing MLIR work as well (). For this C FFI test, we actually already define primitives already (see https://github.com/EnzymeAD/EnzymeJAX/blob/59207f8911100e426b9ca989983a41a047bcf388/enzyme_jax/primitives.pyL285 ), so it's more a matter of understanding how to register the transformations for those primitives. ","I followed this guide (https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html) to move AD onto the primitives themselves (https://github.com/EnzymeAD/EnzymeJAX/pull/6/files). An issue I have come across for reverse mode (forward mode works fine) is that I would need to specify a custom modified forward and reverse pass (with tape passed to it), which doesn't seem to have nice support.  Looking at how custom_vjp is handled, it looks like its manually written in? https://github.com/google/jax/blob/f4eed78e9079aece4feffe6ab31e7c76b6826e7d/jax/_src/interpreters/ad.pyL390","We don't define VJPs for primitives (see for instance this note). A primitive can have a JVP rule and it can have a transpose rule. What I had in mind here was that you make one custom primitive `P1` whose JVP rule binds another primitive `P2`. You endow `P2` with a transposition rule. Together, these define the reversemode (i.e. VJP) behavior of the original primitive `P1`.","Yeah, and that style of custom rule is now implemented in the sample PR (https://github.com/EnzymeAD/EnzymeJAX/pull/6). The sole issue with this at the moment, which I'm curious if you have thoughts on, is as follows: In both forward and reverse mode, the registration mechanism requires redundantly computing the forward pass results (as well as potentially preserving primal values further into the reverse pass). For example, in forward mode, Enzyme can compute both the primal and derivative together in the same custom op. Unfortunately, having the generated forward mode op return both the primal and tangent, breaks the custom transpose (https://github.com/EnzymeAD/EnzymeJAX/blob/fa8893b4c230e096e8acb412ebaad60cd80f49cc/enzyme_jax/primitives.pyL307). In particular the error recieved is as follows (replace that return with simply res):  Notably in forward mode (aka without the custom tangent) that's fine, presumably since it doesn't have the unknown value. Therefore, the remedy I found at the moment to allow the custom_tangent to work is to duplicate the forward pass for the return. A similar issue occurs for forward mode, wherein Enzyme has its own augmented forward pass (that generates a tape of the minimum computed values needed for reverse, as well as any desired outputs). We need that tape for the reverse pass, so we have to run the augmented forward. However, doing this during the transposition itself puts this in the reverse pass, unnecessarily keeping around all of the primal values until that point, in addition to unnecessarily recomputing the primal (wherein that would have been potentially save redundant forwardpass computation). See below for an example:  ","> For example, in forward mode, Enzyme can compute both the primal and derivative together in the same custom op. I think I see what you mean now. For jax to generate a VJP from a JVP, we need the JVP to be ""unzippable"" (i.e. support partial evaluation over primals, for unknown tangents). You'll have to tell me if this is possible with Enzyme, but one way to design this would be for Enzyme to supply to Python more than one underlying operation. Namely: * For forwardmode:   * A JVP op `O1`, mapping (primal inputs, tangent inputs) > (primal outputs, tangents outputs) efficiently. I believe this is what you currently have. * For reversemode:   * A ""forward"" op `O2`, mapping (primal inputs, tangent inputs) > (primal outputs, residuals), and   * A ""reverse"" op `O3`, mapping (residuals, cotangent outputs) > cotangent inputs Then you can set up two JAX primitives `P1` and `P2` like we said before: * `P1` has a JVP rule that calls out to `O1` * `P1` has a partial evaluation rule that calls out to `O2` and stages out a bind of `P2` * `P2` has a transposition rule that calls out to `O3` Is this reasonable?","Yeah I think that design remedies the redundant computation (with the possible exception of tape preservation which I need to think more on). But if there are two primitives, would that not force users to have one primitive for forward mode support as distinct from reverse mode? Or is it possible upon say the MLIR lowering phase to check if there is a transpose interpreter in the stack and switch between the forward and reverse mode lowered primitives? Or if you have a different alternative.","Oh never mind I missed the custom partial evaluation rule, I think that would fix the double user primitive issue. Let me take a stab at that. ","Got it fully working, thanks!","Great to hear. We'll try to improve the interfaces here with time (custom transpose, primitives, etc.)."
600,"以下是一个github上的jax下的一个issue, 标题是(profiler_test.py fixes and add coverage to Cloud TPU CI)， 内容是 (* Add deps to test requirements, including in new `collectprofilerequirements.txt` (to avoid adding tensorflow to `testrequirements.txt`). * Use correct Python executable `ProfilerTest.test_remote_profiler` (`python` sometimes defaults to python2) * Run computations for longer in `ProfilerTest.test_remote_profiler`, othewise `collect_profile` sometimes misses it.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,profiler_test.py fixes and add coverage to Cloud TPU CI,"* Add deps to test requirements, including in new `collectprofilerequirements.txt` (to avoid adding tensorflow to `testrequirements.txt`). * Use correct Python executable `ProfilerTest.test_remote_profiler` (`python` sometimes defaults to python2) * Run computations for longer in `ProfilerTest.test_remote_profiler`, othewise `collect_profile` sometimes misses it.",2023-06-09T18:03:19Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16333
391,"以下是一个github上的jax下的一个issue, 标题是(Increase precision in LaxBackedScipySpatialTransformTests.testRotationApply)， 内容是 (Increase precision in LaxBackedScipySpatialTransformTests.testRotationApply Otherwise the test fails due to small numerical differences.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Increase precision in LaxBackedScipySpatialTransformTests.testRotationApply,Increase precision in LaxBackedScipySpatialTransformTests.testRotationApply Otherwise the test fails due to small numerical differences.,2023-06-08T21:35:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16325
627,"以下是一个github上的jax下的一个issue, 标题是(Expand RaggedAxis representation to support raggedly-shaped broadcasting)， 内容是 (Main content:  Change `RaggedAxis` to allow multiple ragged axes keyed to one stacked axis, e.g., a stack of matrices with different sizes in each dimension.  Update the batching rules for `broadcast_in_dim` and the implementation of `broadcast_to` to support ragged arguments and target shapes. I flagged two particular points of uncertainty using `TODO(reviewer)` comments.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Expand RaggedAxis representation to support raggedly-shaped broadcasting,"Main content:  Change `RaggedAxis` to allow multiple ragged axes keyed to one stacked axis, e.g., a stack of matrices with different sizes in each dimension.  Update the batching rules for `broadcast_in_dim` and the implementation of `broadcast_to` to support ragged arguments and target shapes. I flagged two particular points of uncertainty using `TODO(reviewer)` comments.",2023-06-08T20:21:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16324
1291,"以下是一个github上的jax下的一个issue, 标题是([XLA:Python] Fix incorrect code source information under Python 3.11.)， 内容是 ([XLA:Python] Fix incorrect code source information under Python 3.11. Do not multiply the result of PyFrame_GetLasti() by sizeof(_Py_CODEUNIT), because the CPython implementation already does this inside PyFrame_GetLasti(). * In CPython versions 3.9 or earlier, the f_lasti value in a PyFrameObject was in bytes. * In CPython 3.10, f_lasti was changed to be in code units, which required multiplying it by sizeof(_Py_CODEUNIT) before passing it to functions like PyCode_Addr2Line(). https://docs.python.org/3/whatsnew/3.10.htmlchangesinthecapi * In CPython 3.11, direct access to the representation of the PyFrameObject was removed from the headers, requiring the use of PyFrame_GetLasti() (https://docs.python.org/3/whatsnew/3.11.htmlpyframeobject311hiding). This function multiplies by sizeof(_Py_CODEUNIT) internally (https://github.com/python/cpython/blob/deaf509e8fc6e0363bd6f26d52ad42f976ec42f2/Objects/frameobject.cL1353) so there is no need for the caller to do this multiplication. It is difficult to write a good test for this, sinc)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[XLA:Python] Fix incorrect code source information under Python 3.11.,"[XLA:Python] Fix incorrect code source information under Python 3.11. Do not multiply the result of PyFrame_GetLasti() by sizeof(_Py_CODEUNIT), because the CPython implementation already does this inside PyFrame_GetLasti(). * In CPython versions 3.9 or earlier, the f_lasti value in a PyFrameObject was in bytes. * In CPython 3.10, f_lasti was changed to be in code units, which required multiplying it by sizeof(_Py_CODEUNIT) before passing it to functions like PyCode_Addr2Line(). https://docs.python.org/3/whatsnew/3.10.htmlchangesinthecapi * In CPython 3.11, direct access to the representation of the PyFrameObject was removed from the headers, requiring the use of PyFrame_GetLasti() (https://docs.python.org/3/whatsnew/3.11.htmlpyframeobject311hiding). This function multiplies by sizeof(_Py_CODEUNIT) internally (https://github.com/python/cpython/blob/deaf509e8fc6e0363bd6f26d52ad42f976ec42f2/Objects/frameobject.cL1353) so there is no need for the caller to do this multiplication. It is difficult to write a good test for this, sinc",2023-06-08T13:42:51Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16312
413,"以下是一个github上的jax下的一个issue, 标题是(Add `jax_debug_log_modules` config option.)， 内容是 (This can be used to enable debug logging for specific files (e.g. `JAX_DEBUG_LOG_MODULES=""jax._src.xla_bridge,jax._src.dispatch""`) or all jax (`JAX_DEBUG_LOG_MODULES=""jax""`). Example output: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add `jax_debug_log_modules` config option.,"This can be used to enable debug logging for specific files (e.g. `JAX_DEBUG_LOG_MODULES=""jax._src.xla_bridge,jax._src.dispatch""`) or all jax (`JAX_DEBUG_LOG_MODULES=""jax""`). Example output: ",2023-06-07T22:17:52Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16304,"Ah good catch, added the BUILD rule."
1268,"以下是一个github上的jax下的一个issue, 标题是(Cannot bind to primitive Zero(AbstractToken()))， 内容是 ( Description In `mpi4jax` we make heavy use of tokens to prevent XLA to reorder our MPI calls, which is particularly a problem on XLA:GPU. A standard function would look something like  and we do not return the token because in general `jax.jit` functions cannot return them, but still, a strong order is enforced within the compiled function because of the tokens. When we transpose our functions with `jax.linear_transpose`, we expect also the transpose function to enforce a strong ordering in the reverse order, for example:  However, when mpi4jax attempts to bind the transposed token `Zero(AbstractToken)` an error is raised saying that it cannot be binded because XLA does not know how to represent it. I suspect that the correct way to treat the `Zero(AbstractToken)` should be exactly like a standard token, such that `tc_t` in the example above is    either a normal token that is passed around to enforce ordering in the transposed program.   either when binding a `Zero(AbstractToken)` you follow the same path as when binding a n)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot bind to primitive Zero(AbstractToken())," Description In `mpi4jax` we make heavy use of tokens to prevent XLA to reorder our MPI calls, which is particularly a problem on XLA:GPU. A standard function would look something like  and we do not return the token because in general `jax.jit` functions cannot return them, but still, a strong order is enforced within the compiled function because of the tokens. When we transpose our functions with `jax.linear_transpose`, we expect also the transpose function to enforce a strong ordering in the reverse order, for example:  However, when mpi4jax attempts to bind the transposed token `Zero(AbstractToken)` an error is raised saying that it cannot be binded because XLA does not know how to represent it. I suspect that the correct way to treat the `Zero(AbstractToken)` should be exactly like a standard token, such that `tc_t` in the example above is    either a normal token that is passed around to enforce ordering in the transposed program.   either when binding a `Zero(AbstractToken)` you follow the same path as when binding a n",2023-06-07T21:26:03Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/16303,"After diving deep into jax's internal, I managed to devise this possible fix: Every time you try to construct a `Zero(AbstractToken)` build a token instead. so simply editing this `read_cotangent` function  to  fixes my reproducer above.  Of course this might not be semantically correct but I'm sure you know more about it..."," if the Issue is not very clear please let me know, and I can try to further clarify it. This is a big blocker for us..","Thanks for the ping! I managed not to notice until just now. > because in general jax.jit functions cannot return them Can you say more, and/or share a reproducer? If these are JAX tokens then they should be returnable from jitted functions, otherwise that's a JAX bug (even if it's not the main bug you're talking about). As for the main issue, I understand the general outline, but I need to look at mpi4jax more closely, or alternatively set up a toy model, to understand better. I have two gut reactions: 1. taking a narrow pigeonholed view, anywhere you see a symbolic zero `Zero(AbstractToken)`, i.e. in a JVP or transpose rule (not in ad.py's backward_pass), you probably want to instantiate it so that it's no longer symbolic; but in the bigger picture... 2. I don't think we want to rely on tangentsoftokens to be tokenlike at all, since throughout JAX's AD system we assume tangent types are vectorspacelike, in particular in that they have zero elements which have the behavior that any linear function applied to them is zero. In particular I don't think the fix in this comment is on the right track, unfortunately. > an example code of how this impact mpi4jax can be had by installing the branch pv/fixtoken by for example running pip install git+https://github.com/mpi4jax/mpi4jax.git/fixtoken and then using the following MWE: Where is the token in this example?","Thanks for answering! In the example I shared above the token is automatically generated by mpi4jax, but let me share an example that is more clear. I hope you don't mind installing mpi4jax (unfortunately tokens are used nowhere in jax so I can't build a reproducer there. The reproducer is the following:  Let me comment on what is going on in here by inspecting the jaxpr:  You can see that I have two calls to the primitive `allreduce_mpi`, which is defined in here. This primitive takes two inputs: the array to be reduced and a token to prevent reordering. Now, what would be the correct transposition of this jaxpr?  I would assume is the execution in reverse of the operations. The transposition rule is defined `here` for master and it is essentially:  **notice that I bind the primal token instead of the tangent token. Is this correct?** It seems not, as this fails with error  Another reason suggesting me that I should bind the tangent of the token instead of the primal token here is that I would like to get in the linear transposition an execution order that is reverted, which I only get by binding the tangent token to the tangent primitive. Does this make sense? So in the branch `mpi4jax/fixtoken` I tried to modify the transposition rule to read  but this fails as well with the error I shared in the original post, namely ",">>because in general jax.jit functions cannot return them? >Can you say more, and/or share a reproducer? If these are JAX tokens then they should be returnable from jitted functions, otherwise that's a JAX bug (even if it's not the main bug you're talking about). Apparently I was not up to date, and it seems that it is now possible to return tokens (I remember about a year ago it was not possible).  However it will still error if you try to transpose a token:  that fails with   Though that's a different bug from what I originally reported and I'm not so worried about this one because tokens usually remain inside the jitted functions...",> I hope you don't mind installing mpi4jax I don't mind at all! Thanks for the detailed repro and info. I'll take a look...,I haven't had a chance yet :/ I expect I can in the next 48 hours or so.,thanks for the update! looking forwards for a reply , any luck? Can I do anything to help you nail this problem down?, pretty please 🥹?
908,"以下是一个github上的jax下的一个issue, 标题是(Metal plugin - failed to legalize operation 'mhlo.convolution')， 内容是 ( Description Encountered this on Apple M2 Pro after following instructions from https://developer.apple.com/metal/jax/, and then trying to get https://github.com/sanchitgandhi/whisperjax to run. Steps for reproducing: * Compile and install metal jax following instructions from: https://developer.apple.com/metal/jax/ * Install whisperjax with: `pip install git+https://github.com/sanchitgandhi/whisperjax.git` ` Leads to the following error:  Maybe this operation isnt implemented yet?    .   What jax/jaxlib version are you using? jaxlibv0.4.10  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Metal plugin - failed to legalize operation 'mhlo.convolution'," Description Encountered this on Apple M2 Pro after following instructions from https://developer.apple.com/metal/jax/, and then trying to get https://github.com/sanchitgandhi/whisperjax to run. Steps for reproducing: * Compile and install metal jax following instructions from: https://developer.apple.com/metal/jax/ * Install whisperjax with: `pip install git+https://github.com/sanchitgandhi/whisperjax.git` ` Leads to the following error:  Maybe this operation isnt implemented yet?    .   What jax/jaxlib version are you using? jaxlibv0.4.10  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-07T20:29:55Z,enhancement Apple GPU (Metal) plugin,closed,3,11,https://github.com/jax-ml/jax/issues/16302,,"Thanks   for filing the issue, we will take a look.  ",This is a case of the dimension_numbers we didn't support. We will look into expanding our conversion patterns for convolution op.  ,"By the way, this also happens when you try to train a very simple neural net with `flax`, e.g., the one in the tutorial: https://github.com/google/flax/tree/main/examples/mnist/", Thanks. Any rough estimates on a time frame here?,Do you know if there is any workaround? Thanks!,"Maybe I am wrong here, but for me this looks like lacking support for a standard 1d conv layers. Is this correct? If so, this is blocking a lot of different use cases. For me it blocks a very interesting use of Whisper on Mac. I have no idea of the complexity of implementing this, but is there any chance it could be prioritised?    ","Has this been abandoned?  Can I help? Quite the deal breaker that we cannot invert matrices, do 3d convolutions, eigenvector decomposition, etc. ","  Could you give an update on this? I am still unable to use jaxmetal to run whisperjax, even using v0.0.5. What of the needed operations is expected in the near future?",The issue is lacking of conv1d support. Will look into adding a conversion sequence to shape it to conv2d. ,The issue should be fixed in jaxmetal 0.0.7. Pls reopen it if otherwise. 
495,"以下是一个github上的jax下的一个issue, 标题是(Fix test failures in JAX under NumPy 1.25.0rc1.)， 内容是 (Fix test failures in JAX under NumPy 1.25.0rc1. `jnp.finfo(...)` of an Array type yields:  However, `np.finfo(...)` no longer accepts NumPy arrays as input either, so it would be consistent to require the user to pass a dtype where they are currently passing an array.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix test failures in JAX under NumPy 1.25.0rc1.,"Fix test failures in JAX under NumPy 1.25.0rc1. `jnp.finfo(...)` of an Array type yields:  However, `np.finfo(...)` no longer accepts NumPy arrays as input either, so it would be consistent to require the user to pass a dtype where they are currently passing an array.",2023-06-07T16:53:41Z,,closed,0,3,https://github.com/jax-ml/jax/issues/16297,We're going to hold off submitting this because https://github.com/numpy/numpy/issues/23867 might resolve this., https://github.com/numpy/numpy/issues/23867 has been resolved via https://github.com/numpy/numpy/pull/14847. The changes in this PR might be good anyway. `jnp.finfo(alpha_k.dtype)` is a bit faster than `jnp.finfo(alpha_k)` (although I have no idea whether that part of the code is performance critical)," Thanks for the fix! Yes, it seems like this might be a tiny bit better anyway. I'll submit it."
1256,"以下是一个github上的jax下的一个issue, 标题是(Vanilla installation fails: HASHES)， 内容是 ( Description This Dockerfile   **Occasionally** (almost every time) throws this error at the last step:  > [4/6] RUN pip3 install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html: CC(未找到相关数据) 0.479 Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html CC(未找到相关数据) 0.738 Collecting jax[cuda12_pip] CC(未找到相关数据) 0.795   Downloading jax0.4.11.tar.gz (1.3 MB) CC(未找到相关数据) 0.831      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 37.2 MB/s eta 0:00:00 CC(未找到相关数据) 0.911   Installing build dependencies: started CC(未找到相关数据) 2.243   Installing build dependencies: finished with status 'done' CC(未找到相关数据) 2.244   Getting requirements to build wheel: started CC(未找到相关数据) 2.342   Getting requirements to build wheel: finished with status 'done' CC(未找到相关数据) 2.343   Preparing metadata (pyproject.toml): started CC(未找到相关数据) 2.451   Preparing metadata (pyproject.toml): finished with status 'done' CC(未找到相关数据) 2.515 Collecting mldtypes>=0.1.0 (from jax[cuda12_pip]) CC(未找到相关数)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Vanilla installation fails: HASHES," Description This Dockerfile   **Occasionally** (almost every time) throws this error at the last step:  > [4/6] RUN pip3 install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html: CC(未找到相关数据) 0.479 Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html CC(未找到相关数据) 0.738 Collecting jax[cuda12_pip] CC(未找到相关数据) 0.795   Downloading jax0.4.11.tar.gz (1.3 MB) CC(未找到相关数据) 0.831      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 37.2 MB/s eta 0:00:00 CC(未找到相关数据) 0.911   Installing build dependencies: started CC(未找到相关数据) 2.243   Installing build dependencies: finished with status 'done' CC(未找到相关数据) 2.244   Getting requirements to build wheel: started CC(未找到相关数据) 2.342   Getting requirements to build wheel: finished with status 'done' CC(未找到相关数据) 2.343   Preparing metadata (pyproject.toml): started CC(未找到相关数据) 2.451   Preparing metadata (pyproject.toml): finished with status 'done' CC(未找到相关数据) 2.515 Collecting mldtypes>=0.1.0 (from jax[cuda12_pip]) CC(未找到相关数",2023-06-07T10:31:55Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16293,"I don't think this is a JAX issue. JAX isn't using a requirements file, does not specify hashes of its dependencies, and this is occurring with one of NVIDIA's packages. Does the same thing happen if you simply install `pip install nvidiacudnncu12` in your `Dockerfile` instead of installing JAX? From google searching the error message, it might be an issue with stale data in the pip cache in that docker image. Try rebuilding the docker image, or adding  before the `pip install` command? For what it's worth, if you are using the `cuda12_pip` installation, you don't need to use a CUDA docker image. You just need  the NVIDIA driver and to use `nvidiadocker` to run the container.","You're right, I discovered it is related to my ISP, even things like pip install torch fail. Using a vpn solves the issue for the moment. Thanks anyways and sorry for this."
638,"以下是一个github上的jax下的一个issue, 标题是(JAX Error when run on CPU without connection to TPU)， 内容是 ( Description I tried running the following very simple python code using JAX on my local device, without any connection to a GPU or TPU.  However, I received the following error, which I have not been able to resolve by myself. Any ideas on how to fix it?   What jax/jaxlib version are you using? v0.3.25  Which accelerator(s) are you using? CPU  Additional system info Ubuntu  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,JAX Error when run on CPU without connection to TPU," Description I tried running the following very simple python code using JAX on my local device, without any connection to a GPU or TPU.  However, I received the following error, which I have not been able to resolve by myself. Any ideas on how to fix it?   What jax/jaxlib version are you using? v0.3.25  Which accelerator(s) are you using? CPU  Additional system info Ubuntu  NVIDIA GPU info _No response_",2023-06-07T10:24:22Z,bug,open,0,6,https://github.com/jax-ml/jax/issues/16292,0.3.25 is fairly old at this point. Can you try with the current release (0.4.11)?,"Running it on 0.4.11 just causes the following message to repeat six times.  Then this comes up, after which the messaes from above repeat again.  This one also appears after a while. Any ideas? ",How did you install jaxlib?,"Also, can you share `pip list`'s output? Do you have `libtpu` installed?",I just used  I do have `libtpunightly` installed. ,"Ah! Uninstall `libtpunightly` if you don't have a TPU.  Can we fail more gracefully in this case? It's not necessarily a JAX bug, merely ""you don't have a TPU"" here..."
792,"以下是一个github上的jax下的一个issue, 标题是(Failed to launch CUDA kernel when multiplying bool matrices with large batch size.)， 内容是 ( Description Here is an minimum example to reproduce the bug. Tested using a single RTX 3090.  The code will result in the following error when compiling:  Here adj is an adjacency matrix of type bool and mat is just a random matrix. Setting adj to float or avoid using `@` by using a combination of `vmap` and `jnp.sum` could solve this problem.  What jax/jaxlib version are you using? jax v0.4.11, jaxlib 0.4.11+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Failed to launch CUDA kernel when multiplying bool matrices with large batch size.," Description Here is an minimum example to reproduce the bug. Tested using a single RTX 3090.  The code will result in the following error when compiling:  Here adj is an adjacency matrix of type bool and mat is just a random matrix. Setting adj to float or avoid using `@` by using a combination of `vmap` and `jnp.sum` could solve this problem.  What jax/jaxlib version are you using? jax v0.4.11, jaxlib 0.4.11+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Linux  NVIDIA GPU info ",2023-06-07T02:52:29Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16286,"Thanks for the report, I filed an XLA bug. If you need a workaround until it is fixed, try setting the environment variable `XLA_FLAGS=xla_gpu_enable_triton_gemm=false`","https://github.com/openxla/xla/pull/3530 fixed this, and should be in the next jaxlib release."
891,"以下是一个github上的jax下的一个issue, 标题是(Possible memory leak / OOM when closing over large constant in jitted loop)， 内容是 ( Description I'm getting consistent out of memory errors when jitting functions that close over large constant arrays. It seems similar to CC(Memory leak in compiled loops closing over large constants) but only occurs when compiling the closed over function.  MWE:  Also curiously, if `bigmat` is replaced with zeros or ones, it seems to work fine, suggesting there's some lower level compiler optimization going on.  What jax/jaxlib version are you using? jax=0.4.2, jaxlib=0.4.2  Which accelerator(s) are you using? tried on intel/AMD cpus, nvidia v100 and a100 gpus  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Possible memory leak / OOM when closing over large constant in jitted loop," Description I'm getting consistent out of memory errors when jitting functions that close over large constant arrays. It seems similar to CC(Memory leak in compiled loops closing over large constants) but only occurs when compiling the closed over function.  MWE:  Also curiously, if `bigmat` is replaced with zeros or ones, it seems to work fine, suggesting there's some lower level compiler optimization going on.  What jax/jaxlib version are you using? jax=0.4.2, jaxlib=0.4.2  Which accelerator(s) are you using? tried on intel/AMD cpus, nvidia v100 and a100 gpus  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-06T18:53:42Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/16278,Try upgrading your jax and jaxlib version to 0.4.11 and see if it still happens?,"I tried a few different versions of jax/jaxlib with similar behavior. The exact max size of the big array varies slightly with different versions but in all cases the array should fit into memory many times over, but runs out when compiling. Also, in the cases where the compilation does succeed in the closed over case, the compilation time is 1050x longer than in the nonclosed over case, which also seems excessive."
1246,"以下是一个github上的jax下的一个issue, 标题是(LLVM worker thread limit)， 内容是 ( Description Recently we have been seeing a lot of crashed jobs on our cluster, with the following error message:  caused by the jobs hitting the thread limit per user per node (set to 4096). Further investigation revealed that this is caused by jax spawning hundreds/thousands of llvmworker threads, and never exiting them. This happens whenever we run a script which contains many compilations, and a certain subset of jax primitives, e.g. qr decomposition. Here is a minimal working example to reproduce the aforementioned behaviour:  After a few thousand iterations,  shows ~6100 llvmworker threads which have been spawned:  (this was run locally, in an lxc container limited to 3 cpu cores) A few more remarks:  if the `jnp.linalg.qr` decomposition is replaced with some normal jax function, e.g. `jax.lax.sin` the issue does not happen  as can be seen from the thread names (repeated indices), they are not all spawned by the same `llvm::ThreadPool`, but somewhere else (I am not familiar enough with XLA to find where) Context:  possib)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,LLVM worker thread limit," Description Recently we have been seeing a lot of crashed jobs on our cluster, with the following error message:  caused by the jobs hitting the thread limit per user per node (set to 4096). Further investigation revealed that this is caused by jax spawning hundreds/thousands of llvmworker threads, and never exiting them. This happens whenever we run a script which contains many compilations, and a certain subset of jax primitives, e.g. qr decomposition. Here is a minimal working example to reproduce the aforementioned behaviour:  After a few thousand iterations,  shows ~6100 llvmworker threads which have been spawned:  (this was run locally, in an lxc container limited to 3 cpu cores) A few more remarks:  if the `jnp.linalg.qr` decomposition is replaced with some normal jax function, e.g. `jax.lax.sin` the issue does not happen  as can be seen from the thread names (repeated indices), they are not all spawned by the same `llvm::ThreadPool`, but somewhere else (I am not familiar enough with XLA to find where) Context:  possib",2023-06-06T13:43:14Z,bug,closed,1,2,https://github.com/jax-ml/jax/issues/16272,"Thanks for the simple reproduction. It made debugging easy. It appears we were keeping a small MLIRowned threadpool alive accidentally. CC(Disable threading in MLIR contexts.) disables the MLIR threading support, which we don't really care about anyway, and with that change the number of threads no longer grows with your test case. Hope that helps!",I can confirm that the fix seems to work. Thanks!
588,"以下是一个github上的jax下的一个issue, 标题是(Improve the logging mechanism for `XlaCallModule` op lowering)， 内容是 (Improve the logging mechanism for `XlaCallModule` op lowering * Use `mlir::StatusScopedDiagnosticHandler` to propagate error messages from MLIR to `absl::Status`. * Use `applyTensorflowAndCLOptions` to set up crash reproducer. * Use `DumpMlirOpToFile` to dump MLIR modules to `TF_DUMP_GRAPH_PREFIX` as files, which makes it easier to copy & paste.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Improve the logging mechanism for `XlaCallModule` op lowering,"Improve the logging mechanism for `XlaCallModule` op lowering * Use `mlir::StatusScopedDiagnosticHandler` to propagate error messages from MLIR to `absl::Status`. * Use `applyTensorflowAndCLOptions` to set up crash reproducer. * Use `DumpMlirOpToFile` to dump MLIR modules to `TF_DUMP_GRAPH_PREFIX` as files, which makes it easier to copy & paste.",2023-06-04T18:30:39Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16251,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
480,"以下是一个github上的jax下的一个issue, 标题是(shard_map replication rule for `shard_map conv_general_dilated` not implemented)， 内容是 (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Trying to use a model that uses `conv_general_dilated` raises the following exception: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,shard_map replication rule for `shard_map conv_general_dilated` not implemented,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. Trying to use a model that uses `conv_general_dilated` raises the following exception: ",2023-06-04T03:26:14Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/16249,We fixed this in CC([shardmap] add conv replication rules)!
388,"以下是一个github上的jax下的一个issue, 标题是(Update `type` output of arrays to be real)， 内容是 (Update `type` output of arrays to be real `type(a_jax_array)` does not return `jax.Array`, it returns `tensorflow.compiler.xla.python.xla_extension.ArrayImpl` instead.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update `type` output of arrays to be real,"Update `type` output of arrays to be real `type(a_jax_array)` does not return `jax.Array`, it returns `tensorflow.compiler.xla.python.xla_extension.ArrayImpl` instead.",2023-06-02T16:27:42Z,,closed,0,2,https://github.com/jax-ml/jax/issues/16237,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request.",Closing Copybara created PR due to inactivity
1065,"以下是一个github上的jax下的一个issue, 标题是(Wrong definition of interval length in zoom linesearch)， 内容是 ( Description My issue is in the definition of the interval length in the zoom linesearch in jax/_src/scipy/optimize/line_search.py on line 113.  The interpolation method aims at finding a point `a_j` lying in the interval with boundary points `a_lo`, `a_hi`, that is `[a_lo, a_hi]` or `[a_hi, a_lo]` depending on their order, as stated in Algorithm 3.6 of Wright and Nocedal, 'Numerical Optimization', 1999. A priori nothing prevents the algorithm as stated by Wright and Nocedal to have `a_hi  a + cchk) & (a_j_cubic  b`, that is we may select a point outside the desired interval.  Note that the second issue (wrong checks for cubic and quadratic) seems to be also present in Scipy's original code line 544, line 562 and line 564. This issue can simply be fixed by defining `dalpha = jnp.abs(state.a_hi  state.a_lo)` on line 113.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Wrong definition of interval length in zoom linesearch," Description My issue is in the definition of the interval length in the zoom linesearch in jax/_src/scipy/optimize/line_search.py on line 113.  The interpolation method aims at finding a point `a_j` lying in the interval with boundary points `a_lo`, `a_hi`, that is `[a_lo, a_hi]` or `[a_hi, a_lo]` depending on their order, as stated in Algorithm 3.6 of Wright and Nocedal, 'Numerical Optimization', 1999. A priori nothing prevents the algorithm as stated by Wright and Nocedal to have `a_hi  a + cchk) & (a_j_cubic  b`, that is we may select a point outside the desired interval.  Note that the second issue (wrong checks for cubic and quadratic) seems to be also present in Scipy's original code line 544, line 562 and line 564. This issue can simply be fixed by defining `dalpha = jnp.abs(state.a_hi  state.a_lo)` on line 113.",2023-06-02T16:23:31Z,bug,open,0,7,https://github.com/jax-ml/jax/issues/16236,"Brilliant and detailed report, thank you!  are you available to take a look at this, since IIUC you have some context on the code? If not, I can take a look.","Sorry, I have not had the time to dive into line search. We've discussed this a few times, but I think the best longterm solution for JAX is probably to drop `jax.scipy.optimize` entirely and recommend JAXopt instead. At this point I believe JAXopt has a strict superset of optimization algorithms in JAX, including its own line search. CC   ","That sounds like a good idea for the long term, though perhaps we can also fix this bug in the near term, since  has basically told us the oneline fix. Concretely I think we just need someone to grok and check vroulet's analysis here, then apply the prescribed fix.","I'm actually working on the zoom linesearch in JAXOpt to reformat it as the other linesearches.  Once done, I can run the associated tests in JAXOpt to check the resulting algorithm.  I believe the oneline fix is sufficient.  who adapted the code from Scipy may confirm.",Note that the zoom linesearch in JAXopt supports pytrees while the one in JAX doesn't.,"Nice find. Let me verify, though I have no doubt in your finding.  P.s. It's funny how very small yet significant things like this can persist in code. Seen it many times in ""hardened"" optimisation code. ","For the record, below is a minimal example, where the current implementation fails while the scipy implementation succeeds. For the longterm solution (using JAXOpt) I recoded the zoom_linesearch in JAXOpt and attached the corresponding PR. Once merged, you may drop jax.scipy.optimize for the long term but the oneline fix may be a simple patch for the short term.  By the way, the `a_rec` variable in the current implementation is not well set in one case (when `a_hi` is set to `a_lo`, and `a_lo` is set to `a_j`, `a_rec` should be set to `a_hi` not `a_lo` as currently implemented). But this only affects the cubic interpolation and won't create any bugs (it may just fail to select the cubic interpolation at times where it could have used it).  I corrected it in the new implementation in JAXOpt.  "
594,"以下是一个github上的jax下的一个issue, 标题是(`jax.jit(..., static_argnums=...)` keeps strong (not weak) reference in cache, causing memory leak)， 内容是 ( Description  Commenting out the `f(1, A)` line instead produces ``. This usecase turns up in tests, when creating and destroying many JIT'd functions.  What jax/jaxlib version are you using? 4.11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"`jax.jit(..., static_argnums=...)` keeps strong (not weak) reference in cache, causing memory leak"," Description  Commenting out the `f(1, A)` line instead produces ``. This usecase turns up in tests, when creating and destroying many JIT'd functions.  What jax/jaxlib version are you using? 4.11  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-06-01T21:09:51Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/16226,"It _could_ be argued that this is working as intended: caching *in general* is based on static _values_, not static object identities. That's what lets us get a cache hit on evaluating the expression `jit(f, static_argnums=0)(n())` twice where `n = lambda: 256 + (lambda: 1)()` (I had to embellish that to defeat some CPython optimizations). With caching based on values, in general whether an object with a particular value goes out of scope doesn't matter, since we can usually construct another object with the same value and thus the cache entry isn't dead. (Cache entries are dead, and hence ""leaked"", if they become unreachable.) This particular example doesn't fit neatly into that mold because `A` inherits the `type` hashable definition, where equality is based on object id. That is, after deleting `A` it's _hard_ to construct another object with the same value and thus hard to get a cache hit. It's hard, but technically not impossible: Python can reuse object ids. So, _technically_ the cache entry is not totally dead; it's just hard to reach, and perhaps impossible to reach deterministically without using a C extension. (This is probably a good reason not to put equalitybyobjectid instances into cache keys if the cache can outlive the object instances, as I think CPython really just checks pointer value and not anything else.) Anyway, pedantry aside, I agree this is probably an undesirable behavior for objects where equality is based on object id, and we may want to revise it. But (presumably) we don't want to break the behavior for other objects, like (noninterned) ints or strings or tuples or frozen dataclasses, where equal values can be reconstructed (and hence cache entries hit) even when the original object goes out of scope. How do we tell the difference? I'm not sure how to do it in a complete and sound way. One heuristic might be to check for each static argument `x` whether `x.__eq__ is object.__eq__`, and if so only hold a weak reference.  WDYT?","> One heuristic might be to check for each static argument x whether `x.__eq__ is object.__eq__`, and if so only hold a weak reference. Agreed  I think this something like this is probably a reasonable heuristic, and that a heuristic is probably as good as it gets. Two other important heuristic cases that come to mind:  If `type(x) is types.FunctionType`. In practice I *think* it's possible to create equal nonidentical functions (equal `__code__` etc.) but in practice that's weird and functions are my most common example of a static argument with this problem.  Nested examples of the above, e.g. `(,)`. (Or I suppose any pytree, with at least one heuristicmatching leaf.)"
559,"以下是一个github上的jax下的一个issue, 标题是(ImportError: cannot import name 'xla_call_p' from 'jax.interpreters.xla' )， 内容是 ( Description **When i ran the below code,it came a import error**    Is this an error caused by the latest version v0.4.11? Thanks a lot!  What jax/jaxlib version are you using? jax v0.4.11;jaxlib v0.4.11  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,ImportError: cannot import name 'xla_call_p' from 'jax.interpreters.xla' ," Description **When i ran the below code,it came a import error**    Is this an error caused by the latest version v0.4.11? Thanks a lot!  What jax/jaxlib version are you using? jax v0.4.11;jaxlib v0.4.11  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_",2023-06-01T11:59:57Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/16214,"Hi  this is expected. `xla_call_p` has been deprecated since JAX v0.4.4, and was removed in JAX version 0.4.11. See the Change Log for more information. If you have code that still uses `xla_call_p`, you can install JAX version 0.4.10 or older, but I'd suggest updating your code if possible.","By the way, it looks like numpyro has already fixed the issue here: https://github.com/pyroppl/numpyro/pull/1595"
230,"以下是一个github上的jax下的一个issue, 标题是(custom prng: add shard methods to PRNGKeyArrayImpl)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,custom prng: add shard methods to PRNGKeyArrayImpl,,2023-06-01T10:52:12Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/16213
1051,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Improve compile-time shape checking. )， 内容是 (JAX shape polymorphism relies on implicit assumptions. For example, when tracing with input specification `(a, a)`, we assume that the first two dimensions have the same size greater or equal to 1. Here we extend the checking that these assumptions hold. When we call an `Exported` module from jax, with `jax_export.call_exported` we check these assumptions statically. However, when we stage an `Exported` using `XlaCallModule` to be called from TensorFlow, or when we use TF graph serialization we need to check these assumptions when we execute and compile the op (that is when the shapes are available). To prepare for this compiletime shape checking we add `Exported.shape_check_module` to produce a serialized MLIR module containing the shape checking code. This will be added in a future change to `XlaCallModule`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Improve compile-time shape checking. ,"JAX shape polymorphism relies on implicit assumptions. For example, when tracing with input specification `(a, a)`, we assume that the first two dimensions have the same size greater or equal to 1. Here we extend the checking that these assumptions hold. When we call an `Exported` module from jax, with `jax_export.call_exported` we check these assumptions statically. However, when we stage an `Exported` using `XlaCallModule` to be called from TensorFlow, or when we use TF graph serialization we need to check these assumptions when we execute and compile the op (that is when the shapes are available). To prepare for this compiletime shape checking we add `Exported.shape_check_module` to produce a serialized MLIR module containing the shape checking code. This will be added in a future change to `XlaCallModule`.",2023-06-01T10:13:40Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16211, PTAL
593,"以下是一个github上的jax下的一个issue, 标题是(Some cleanup of logging in XlaCallModule.)， 内容是 (Some cleanup of logging in XlaCallModule. Previously, we were using XLA_VLOG_LINES to ensure that the whole module is logged even when large. This had the unpleasant sideeffect that each line of the module was logged separately, including the filename and line number. This made the log large and prevented copypasting the module. Now we use `if (VLOG_IS_ON()) LogModule`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Some cleanup of logging in XlaCallModule.,"Some cleanup of logging in XlaCallModule. Previously, we were using XLA_VLOG_LINES to ensure that the whole module is logged even when large. This had the unpleasant sideeffect that each line of the module was logged separately, including the filename and line number. This made the log large and prevented copypasting the module. Now we use `if (VLOG_IS_ON()) LogModule`.",2023-06-01T09:22:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16210
1240,"以下是一个github上的jax下的一个issue, 标题是(Calling a pjitted function repeatedly with the same inputs errors in multiprocessing execution)， 内容是 ( Description Hi jax team! This issue came up when I was trying to benchmark jax inference where I would pjit a few different model calls and call the function multiple times with the same input in a for loop, but I would get an error only when multiprocessing was enabled. I can reproduce this in this nightly container:  `ghcr.io/nvidia/t5x:nightly20230522` But I've also been able to reproduce it in another older nightly container. Everything runs fine when I run it like this:  But it fails with  When run like this:  I have noticed that if I remove these lines  the test succeeds, but I'm not sure why and it doesn't solve my original problem, which was running the pjitted function multiple times to obtain a benchmark. Here's `repro.py`  FYI:    What jax/jaxlib version are you using? jax 0.4.9 (from source: 63d87c6c3df2de514a41f6f6aff5340dad12cc35)  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Ubuntu  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Calling a pjitted function repeatedly with the same inputs errors in multiprocessing execution," Description Hi jax team! This issue came up when I was trying to benchmark jax inference where I would pjit a few different model calls and call the function multiple times with the same input in a for loop, but I would get an error only when multiprocessing was enabled. I can reproduce this in this nightly container:  `ghcr.io/nvidia/t5x:nightly20230522` But I've also been able to reproduce it in another older nightly container. Everything runs fine when I run it like this:  But it fails with  When run like this:  I have noticed that if I remove these lines  the test succeeds, but I'm not sure why and it doesn't solve my original problem, which was running the pjitted function multiple times to obtain a benchmark. Here's `repro.py`  FYI:    What jax/jaxlib version are you using? jax 0.4.9 (from source: 63d87c6c3df2de514a41f6f6aff5340dad12cc35)  Which accelerator(s) are you using? GPU  Additional system info Python 3.10, Ubuntu  NVIDIA GPU info ++  ++",2023-05-31T04:56:02Z,bug,open,0,3,https://github.com/jax-ml/jax/issues/16198,"Do you have a minimal repro? One guess I have is in multiprocess, make sure that all shapes you pass to pjit are global.","Hi  . By minimal repro, do you mean like instructions to run the above script endtoend?","Hi! can you repro this with jax and jaxlib 0.4.20? If yes, please provide a minimal repro: https://en.wikipedia.org/wiki/Minimal_reproducible_example:~:text=In%20computing%2C%20a%20minimal%20reproducible,to%20be%20demonstrated%20and%20reproduced."
1000,"以下是一个github上的jax下的一个issue, 标题是(When the inferred step is integer-valued, have `linspace()` return integer-valued results)， 内容是 (_I assume this feature request is a nogo given optimisation ramifications(?), but thought I'd open this for posterity's sake._ See the different results you get when using  `int64` in the following example:[^1]  I see now that the `jnp.linspace(1, 7, 7)` result doesn't consist of only integer values, only that nonints get repr'd as ints (which is understandable behaviour).  My question is if its possible for `linspace()` to give integervalued results when the the inferred linspace step is integervalued, like NumPy (at least seemingly more consistently) does.  I ran into this issue today when trying `jax.numpy` with code initially written for NumPy. [^1]: at least locally, `jax==HEAD`/`jaxlib==0.4.10`, Ubuntu 22.04, on CPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"When the inferred step is integer-valued, have `linspace()` return integer-valued results","_I assume this feature request is a nogo given optimisation ramifications(?), but thought I'd open this for posterity's sake._ See the different results you get when using  `int64` in the following example:[^1]  I see now that the `jnp.linspace(1, 7, 7)` result doesn't consist of only integer values, only that nonints get repr'd as ints (which is understandable behaviour).  My question is if its possible for `linspace()` to give integervalued results when the the inferred linspace step is integervalued, like NumPy (at least seemingly more consistently) does.  I ran into this issue today when trying `jax.numpy` with code initially written for NumPy. [^1]: at least locally, `jax==HEAD`/`jaxlib==0.4.10`, Ubuntu 22.04, on CPU",2023-05-30T15:48:28Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/16183,"Thanks for the request – you probably know this already, but the issue is that floating point arithmetic is only an approximation of realvalued arithmetic, and so you generally should not use exact equality checks with floatingpoint values. Regarding the specific request to return exact integers when possible – I don't think this is feasible. But I think we could make the results with `dtype='int64'` more reasonable."
1098,"以下是一个github上的jax下的一个issue, 标题是(jax.numpy.interp has poor performance on TPUs)， 内容是 ( Description Hi,  I recently discovered that jax.numpy.interp has poor performance on TPUs. More specifically, I am trying to perform 1D interpolation on a flattened tensor with an original shape of (128, 384, 384). After JIT, this operation takes 0.26 second on my laptop CPU, and 0.26 second on a Colab T4 GPU. However, the same operation would take 4.30 seconds on a TPUv3 core, which is way too slow. Since interpolation / linear look up table is a common operation for people working on computer vision, maybe we should improve the performance of this operation on TPU? I have attached a script to reproduce this problem  please let me know if I can provide anything that would be helpful.   What jax/jaxlib version are you using? jax v0.4.10, jaxlib v0.4.10  Which accelerator(s) are you using? TPU  Additional system info TPU VM tpuv38  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.numpy.interp has poor performance on TPUs," Description Hi,  I recently discovered that jax.numpy.interp has poor performance on TPUs. More specifically, I am trying to perform 1D interpolation on a flattened tensor with an original shape of (128, 384, 384). After JIT, this operation takes 0.26 second on my laptop CPU, and 0.26 second on a Colab T4 GPU. However, the same operation would take 4.30 seconds on a TPUv3 core, which is way too slow. Since interpolation / linear look up table is a common operation for people working on computer vision, maybe we should improve the performance of this operation on TPU? I have attached a script to reproduce this problem  please let me know if I can provide anything that would be helpful.   What jax/jaxlib version are you using? jax v0.4.10, jaxlib v0.4.10  Which accelerator(s) are you using? TPU  Additional system info TPU VM tpuv38  NVIDIA GPU info _No response_",2023-05-30T15:14:58Z,enhancement performance contributions welcome jax.numpy,open,1,9,https://github.com/jax-ml/jax/issues/16182,"I updated my code a bit and leveraged the fact that the range that needs to be interpolated is known, and each bin is evenly spaced. This results in significant speed up on GPUs, but the TPU implementation is still very slow.  ","We have a version of `interp` that uses a dot product rather than indexing, which we find to be much faster on TPUs... happy to share.", It would be awesome if you can share an interp implementation using dot product. Much appreciated!,You can use this for now:  ,"The root of this poor performance comes from the fact that `interp` uses `searchsorted`, and `searchsorted` is not a primitive recognized by the compiler. Because of this, we must choose an implementation for it at the JAX level, and the default implementation is one that performs poorly on TPU. I'm not sure how to address this in general without making binary search a primitive for which the compiler can choose the optimial operation given the hardware. Unfortunately, it doesn't look like there's sufficient appetite to handle `searchsorted` natively in HLO. Given that, your best bet is probably to tune the implementation to the hardware manually, as  does in his answer.","There are actually two key tricks for efficient interpretation on TPUs: 1. Faster searchsorted with `method='compare_all'` 2. Replacing array indexing with a dot product This combination is much faster, at each for interpolating along relatively small dimensions (less than 100 elements or so). The main downside is higher memory usage, which can usually be mitigated by also using `jax.remat`. My suggested fix for JAX would be to add options for selecting these algorithms to `jnp.interp`.","Hi  thank you for providing your interp implementation using dot product! It seems to work perfectly when `x` and `fp` has the same shape, but it would stop working if `x` is longer than `fp`. I think the current `jax.numpy.interp` allows for x to have different sizes from `fp`. ","If you need multiple `x` points, use `vmap` to vectorize over the first argument.",It looks like PR CC(Make searchsorted a primitive) might address the `searchsorted` issue.
738,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Add a polymorphic shape refinement MLIR pass accessible to JAX Python.)， 内容是 ([shape_poly] Add a polymorphic shape refinement MLIR pass accessible to JAX Python. At the moment we can run the StableHLO module lowered by jax2tf with polymorphic shapes only with jax2tf, because the tf.XlaCallModule op has the necessary shape refinement logic (which is necessary to legalize the StableHLO module with dynamic shapes to MHLO). Here we expose the shape refinement MLIR transformation to JAX Python. For now this is used only in a test in jax_export_test.py.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Add a polymorphic shape refinement MLIR pass accessible to JAX Python.,"[shape_poly] Add a polymorphic shape refinement MLIR pass accessible to JAX Python. At the moment we can run the StableHLO module lowered by jax2tf with polymorphic shapes only with jax2tf, because the tf.XlaCallModule op has the necessary shape refinement logic (which is necessary to legalize the StableHLO module with dynamic shapes to MHLO). Here we expose the shape refinement MLIR transformation to JAX Python. For now this is used only in a test in jax_export_test.py.",2023-05-30T13:52:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16181
1015,"以下是一个github上的jax下的一个issue, 标题是(Bad return values from `jax.random.poisson` when `lam` is `jnp.inf` )， 内容是 ( Description  Output  As above, `+inf` input leads to output 0, looks like an overflow occurs, for `lam = inf`, seems `214748364` should be a right answer here? A *large* output of  a `+inf` rate or expectation is more convincing here. BTW, for numpy `inf` will lead to a value error:  For `tensorflowprobability` `inf` produces `inf`s  Output  and maybe for compatibility with `numpy`, `jax` chooses `integer `return dtype, while `tfp` chooses `float`, as discussed by  https://github.com/google/jax/pull/16134pullrequestreview1446642709, seems a float return dtype may be more reasonable...   What jax/jaxlib version are you using? jax v0.4.10, jaxlib v0.4.10  Which accelerator(s) are you using? CPU  Additional system info Mac, Python 3.10.9  NVIDIA GPU info None)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bad return values from `jax.random.poisson` when `lam` is `jnp.inf` ," Description  Output  As above, `+inf` input leads to output 0, looks like an overflow occurs, for `lam = inf`, seems `214748364` should be a right answer here? A *large* output of  a `+inf` rate or expectation is more convincing here. BTW, for numpy `inf` will lead to a value error:  For `tensorflowprobability` `inf` produces `inf`s  Output  and maybe for compatibility with `numpy`, `jax` chooses `integer `return dtype, while `tfp` chooses `float`, as discussed by  https://github.com/google/jax/pull/16134pullrequestreview1446642709, seems a float return dtype may be more reasonable...   What jax/jaxlib version are you using? jax v0.4.10, jaxlib v0.4.10  Which accelerator(s) are you using? CPU  Additional system info Mac, Python 3.10.9  NVIDIA GPU info None",2023-05-27T14:38:09Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/16164,"Thanks for the report. JAX has some constraints here, namely:  we cannot raise a runtime error triggered by the value of an input – this is quite similar, for example, to the reason JAX cannot raise IndexErrors for outofbound indices.  we cannot change the type of the output based on the value of the input, so only integer arrays are an option. With that in mind, given that the API returns an integer array, all we can do is choose some valid integer value to return when lambda is too large. There's no good return value here, but the largest representable integer is probably the least bad option. Alternatively, we could change the API contract so that it returns a float array, in which case we could use `NaN` or `inf` for invalid outputs (though this would require a deprecation cycle for the API change, so would be somewhat complicated) – what do you think?","Hi , thanks for your reply! In my opinion, JAX may *utlimately* support a float array return, there're some reasons you mentioned, `nan` or `inf` are more intuitive and mathematical than `1`, other lib (`tfp`) and framework (`torch`) return a float array too (but seems bug exists too 🤔 https://github.com/pytorch/pytorch/issues/102811), and for my use case, log transformation is often used.  Surely it's not a *must* and depends on personal taste and may *impact* latter features, such as *what type will other discrete distributions(binomial, multinomial) return?*. Consistency between these apis seems important too."
648,"以下是一个github上的jax下的一个issue, 标题是([JAX] Use MLIR argument locations instead of a bespoke jax.arg_info attribute.)， 内容是 ([JAX] Use MLIR argument locations instead of a bespoke jax.arg_info attribute. https://github.com/llvm/llvmproject/commit/514dddbeba643e32310c508a15d7b6ff42f2c461 allowed for specifying argument Locations in the MLIR Python bindings. We should use them, in the form of a Name location, rather than making up our own attribute. Example of new output:  Note debug information must be enabled.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[JAX] Use MLIR argument locations instead of a bespoke jax.arg_info attribute.,"[JAX] Use MLIR argument locations instead of a bespoke jax.arg_info attribute. https://github.com/llvm/llvmproject/commit/514dddbeba643e32310c508a15d7b6ff42f2c461 allowed for specifying argument Locations in the MLIR Python bindings. We should use them, in the form of a Name location, rather than making up our own attribute. Example of new output:  Note debug information must be enabled.",2023-05-26T20:18:55Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16160
929,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Add partial support for call_exported with polymorphic shapes)， 内容是 (Until now the jax_export.call_exported did not allow calling functions that were exported with polymorphic shapes. We now add that support, including resolving the dimension variables of the called function in terms of the shapes at the call site (which themselves may include dimension variables), and then computing the output shape of the called function. The support is partial in that we can export a JAX function that calls an exported polymorphic function, but we cannot invoke it. This is because we do not yet have access to the shape refinement machinery that XlaCallModule uses. For now, we use XlaCallModule for invoking exported that includes shape polymorphism.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Add partial support for call_exported with polymorphic shapes,"Until now the jax_export.call_exported did not allow calling functions that were exported with polymorphic shapes. We now add that support, including resolving the dimension variables of the called function in terms of the shapes at the call site (which themselves may include dimension variables), and then computing the output shape of the called function. The support is partial in that we can export a JAX function that calls an exported polymorphic function, but we cannot invoke it. This is because we do not yet have access to the shape refinement machinery that XlaCallModule uses. For now, we use XlaCallModule for invoking exported that includes shape polymorphism.",2023-05-26T09:03:02Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16148, PTAL
430,"以下是一个github上的jax下的一个issue, 标题是(Add new `called_index` to custom_call  `tf.backend_config` DictAttr.)， 内容是 (Add new `called_index` to custom_call  `tf.backend_config` DictAttr. Here, `called_index` indicates the tf concrete function index in the `function_list` of the parent XLACallModule.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add new `called_index` to custom_call  `tf.backend_config` DictAttr.,"Add new `called_index` to custom_call  `tf.backend_config` DictAttr. Here, `called_index` indicates the tf concrete function index in the `function_list` of the parent XLACallModule.",2023-05-24T21:05:29Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16114
724,"以下是一个github上的jax下的一个issue, 标题是(`FAILED_PRECONDITION: DNN library initialization failed.` when importing tensorflow before jax, with `set_visible_devices`)， 内容是 ( Description Hello, the following code   results in   It seems like tensorflow is preallocating the memory even though it should not see the GPU. However, if I uncomment the line  before tensorflow is imported, the code will run.  What jax/jaxlib version are you using?  I installed jax with the CUDA libraries from pip, via   Which accelerator(s) are you using? GPU  Additional system info Manjaro Linux  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"`FAILED_PRECONDITION: DNN library initialization failed.` when importing tensorflow before jax, with `set_visible_devices`"," Description Hello, the following code   results in   It seems like tensorflow is preallocating the memory even though it should not see the GPU. However, if I uncomment the line  before tensorflow is imported, the code will run.  What jax/jaxlib version are you using?  I installed jax with the CUDA libraries from pip, via   Which accelerator(s) are you using? GPU  Additional system info Manjaro Linux  NVIDIA GPU info ",2023-05-24T08:08:26Z,bug,open,2,3,https://github.com/jax-ml/jax/issues/16107,Have you solved the problem? I also encountered the same problem,"Hi, I am encountering the same problem.  My details:  ","I did some tests changing the order of statements.  Each of these tests is a standalone independent launch of python.   It does not matter what order tf and jax are imported.  If the 'RT' command is run, something in the environment is changed after that so that the 'RJ' command produces the 'FAILED_PRECONDITION' error: Environment:  Mnemonic:  `I=import, R=run, T=tensorflow, J=jax` Script variations.  Each one is run inside the docker container as described above, using `python test.py` "
992,"以下是一个github上的jax下的一个issue, 标题是(JVP softmax implementation is missing a stop_gradient, leading to training instability)， 内容是 ( Description In jax/_src/nn/functions.py there's a new implementation of JVPbased softmax. Recently my team has been struggling with exploding gradients on training Transformer models, which we didn't have before. After bisecting all changes in JAX and other libraries we use, we narrowed it down to this change. The new softmax implementation is missing a `stop_gradient` operator around the x_max evaluation, which was done on the deprecated softmax implementation, and is highlighted as a canonical use case:  https://www.tensorflow.org/api_docs/python/tf/stop_gradient  What jax/jaxlib version are you using? jax v0.4.10  Which accelerator(s) are you using? TPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,"JVP softmax implementation is missing a stop_gradient, leading to training instability"," Description In jax/_src/nn/functions.py there's a new implementation of JVPbased softmax. Recently my team has been struggling with exploding gradients on training Transformer models, which we didn't have before. After bisecting all changes in JAX and other libraries we use, we narrowed it down to this change. The new softmax implementation is missing a `stop_gradient` operator around the x_max evaluation, which was done on the deprecated softmax implementation, and is highlighted as a canonical use case:  https://www.tensorflow.org/api_docs/python/tf/stop_gradient  What jax/jaxlib version are you using? jax v0.4.10  Which accelerator(s) are you using? TPU  Additional system info Linux  NVIDIA GPU info _No response_",2023-05-23T14:25:50Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/16093,"Just adding some context we discussed in chat: after adding the `custom_jvp`, the `stop_gradient` in the `softmax` implementation is always a noop, since we no longer autodiff through the `softmax` implementation. Just to be extra sure, I checked:  The assert triggers if we remove the custom_jvp rule, but with the custom_jvp rule in place it doesn't."
1273,"以下是一个github上的jax下的一个issue, 标题是(Add (optional) ordered effects for `jax2tf.call_tf`)， 内容是 (Add (optional) ordered effects for `jax2tf.call_tf` This allows users to express nested TensorFlow computation that must be ordered during execution. It leverages the existing JAX effects system to model such side effects and lower them to use XLA tokens. With this change, `jax2tf.call_tf(ordered=True)` can be used to generate ordered TF calls. This has the following behavior: * With `call_tf_graph=True`, this generates a custom call op with the following differences: (1) a `!stablehlo.token` argument/result is prepended to each custom call's argument/result list and (2) `tf.backend_config` has an additional `has_token_input_output = true` entry. * Without `call_tf_graph=True`, this raises a `NotImplementedError()`. For this, `jax_export.py` makes sure that dummy arguments/results added for ordered effects are not exposed to the public interface by passing constant values in a wrapper function. Because of this, adding ordered effects to jax2tfed computation no longer causes calling convention changes and can be safely allowed.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add (optional) ordered effects for `jax2tf.call_tf`,"Add (optional) ordered effects for `jax2tf.call_tf` This allows users to express nested TensorFlow computation that must be ordered during execution. It leverages the existing JAX effects system to model such side effects and lower them to use XLA tokens. With this change, `jax2tf.call_tf(ordered=True)` can be used to generate ordered TF calls. This has the following behavior: * With `call_tf_graph=True`, this generates a custom call op with the following differences: (1) a `!stablehlo.token` argument/result is prepended to each custom call's argument/result list and (2) `tf.backend_config` has an additional `has_token_input_output = true` entry. * Without `call_tf_graph=True`, this raises a `NotImplementedError()`. For this, `jax_export.py` makes sure that dummy arguments/results added for ordered effects are not exposed to the public interface by passing constant values in a wrapper function. Because of this, adding ordered effects to jax2tfed computation no longer causes calling convention changes and can be safely allowed.",2023-05-23T00:24:13Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16089,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
905,"以下是一个github上的jax下的一个issue, 标题是(Extend plugin discovery to also include entry-points.)， 内容是 (This effectively implements a mix of option 2 and option 3 from https://packaging.python.org/en/latest/guides/creatinganddiscoveringplugins/ as a pragmatic way to cover all packaging cases. The namespace/path based iteration works for situations where code has not been packaged and is present on the PYTHONPATH, whereas the advertised entrypoints work around setuptools/pkgutil issues that make it impossible to reliably iterate over installed modules in certain scenarios (noted for editable installs which use a custom finder that does not implement iter_modules()). A plugin entrypoint can be advertised in setup.py (or equivalent pyproject.toml) with something like: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Extend plugin discovery to also include entry-points.,"This effectively implements a mix of option 2 and option 3 from https://packaging.python.org/en/latest/guides/creatinganddiscoveringplugins/ as a pragmatic way to cover all packaging cases. The namespace/path based iteration works for situations where code has not been packaged and is present on the PYTHONPATH, whereas the advertised entrypoints work around setuptools/pkgutil issues that make it impossible to reliably iterate over installed modules in certain scenarios (noted for editable installs which use a custom finder that does not implement iter_modules()). A plugin entrypoint can be advertised in setup.py (or equivalent pyproject.toml) with something like: ",2023-05-19T21:14:56Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/16073, FYI
1278,"以下是一个github上的jax下的一个issue, 标题是(Support static arguments for any function transformation)， 内容是 (Following up on https://github.com/google/jax/issues/15504, I ended up needing a more general solution because I kept running into the same issue with other function transformations that often don't support static arguments. My solution ended up being this this snippet that adds static argument support to any transformation. A meta transformation! :D  A simplifying design choice here is that static arguments are required to be passed as keyword arguments and cannot be passed as positional arguments, which I find much easier to specify. When the transformed function is already the output of some other transformation, counting argument names can be painful. I'm wondering whether including a helper like the one above would be a useful addition, simplification, and unification for JAX? It might simplify the existing JAX transformations, provide a unified API to users, and allow users to use thirdparty transformations that may not support static arguments. Longer term, users could even be asked to apply the meta transformation them)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Support static arguments for any function transformation,"Following up on https://github.com/google/jax/issues/15504, I ended up needing a more general solution because I kept running into the same issue with other function transformations that often don't support static arguments. My solution ended up being this this snippet that adds static argument support to any transformation. A meta transformation! :D  A simplifying design choice here is that static arguments are required to be passed as keyword arguments and cannot be passed as positional arguments, which I find much easier to specify. When the transformed function is already the output of some other transformation, counting argument names can be painful. I'm wondering whether including a helper like the one above would be a useful addition, simplification, and unification for JAX? It might simplify the existing JAX transformations, provide a unified API to users, and allow users to use thirdparty transformations that may not support static arguments. Longer term, users could even be asked to apply the meta transformation them",2023-05-19T19:34:46Z,enhancement,open,1,2,https://github.com/jax-ml/jax/issues/16071,"You might find Equinox interesting, which does something similar: it has `equinox.filter_{jit, grad, ...}`, which automatically determines dynamic vs static arguments based on their type. After all, it mostly only makes sense to JIT wrt arrays, to grad wrt floatingpoint arrays, etc.","Thanks Patrick, I've heard of equinox and think it has a lot of interesting ideas. I use Ninjax."
908,"以下是一个github上的jax下的一个issue, 标题是(Add MLIR side effects to `tf.XlaCallModule`.)， 内容是 (Add MLIR side effects to `tf.XlaCallModule`. With `jax2tf` native serialization, the StableHLO module embedded in `tf.XlaCallModule` may contain `stablehlo.custom_call` calling TF host callback functions. In this case, the `stablehlo.custom_call`s will be lowered to `stablehlo.send` and `stablehlo.recv`, so `tf.XlaCallModule` has `TF_SendSideEffect` and `TF_RecvSideEffect`. This CL 1. replaces the `Pure` trait with `MemoryEffects` trait in the automatically generated `tf.XlaCallModule` op's definition. 2. sets `isStateful` in `XlaCallModule`'s op declaration. 2. updates TF side effect analysis to recursively analyze the TF host callback functions called by `tf.XlaCallModule`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add MLIR side effects to `tf.XlaCallModule`.,"Add MLIR side effects to `tf.XlaCallModule`. With `jax2tf` native serialization, the StableHLO module embedded in `tf.XlaCallModule` may contain `stablehlo.custom_call` calling TF host callback functions. In this case, the `stablehlo.custom_call`s will be lowered to `stablehlo.send` and `stablehlo.recv`, so `tf.XlaCallModule` has `TF_SendSideEffect` and `TF_RecvSideEffect`. This CL 1. replaces the `Pure` trait with `MemoryEffects` trait in the automatically generated `tf.XlaCallModule` op's definition. 2. sets `isStateful` in `XlaCallModule`'s op declaration. 2. updates TF side effect analysis to recursively analyze the TF host callback functions called by `tf.XlaCallModule`.",2023-05-18T21:36:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/16061
411,"以下是一个github上的jax下的一个issue, 标题是(PRNGKeyArray: add several missing attributes & methods)， 内容是 (These are properties ant methods of `ArrayImpl` that we need to define on `PRNGKeyArrayImpl` for full compatibility. Leaving some of the more difficult/ambiguous ones as a TODO.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,PRNGKeyArray: add several missing attributes & methods,These are properties ant methods of `ArrayImpl` that we need to define on `PRNGKeyArrayImpl` for full compatibility. Leaving some of the more difficult/ambiguous ones as a TODO.,2023-05-17T21:48:50Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/16047
876,"以下是一个github上的jax下的一个issue, 标题是(GPU profiling: cuptiGetTimestamp: error 999 )， 内容是 ( Description I have been trying to profile a program running on my A100 GPU on Google Cloud. When I wrap my computation with  as described in the docs I get out a perfetto link, but it doesn't have any GPU information. Looking in the logs, I see lines like this:  Any idea how to further debug or fix this? (I tested the workaround in https://github.com/google/jax/issues/13477 but it did not fix things)  What jax/jaxlib version are you using? jax=0.4.8, jaxlib=0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU (A100)  Additional system info 22.04 Ubuntu  NVIDIA GPU info https://gist.github.com/silvasean/2f6da6081642c59b9e77711f3a472fc1)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,GPU profiling: cuptiGetTimestamp: error 999 ," Description I have been trying to profile a program running on my A100 GPU on Google Cloud. When I wrap my computation with  as described in the docs I get out a perfetto link, but it doesn't have any GPU information. Looking in the logs, I see lines like this:  Any idea how to further debug or fix this? (I tested the workaround in https://github.com/google/jax/issues/13477 but it did not fix things)  What jax/jaxlib version are you using? jax=0.4.8, jaxlib=0.4.7+cuda11.cudnn86  Which accelerator(s) are you using? GPU (A100)  Additional system info 22.04 Ubuntu  NVIDIA GPU info https://gist.github.com/silvasean/2f6da6081642c59b9e77711f3a472fc1",2023-05-16T21:48:13Z,bug needs info,closed,0,5,https://github.com/jax-ml/jax/issues/16030,"Did you install CUDA using the `pip` packages? If so, can you try updating to the latest release (0.4.10)? I think this may have already been fixed by https://github.com/google/jax/commit/75d0f6522d3e4623e7fd5bd512645e4cbab6b10d which added a missing `cupti` dependency.","Thanks Peter. I see `nvidiacudacupticu11` already installed in my venv:  However, I also have CUDA installed locally. I suspect the local one is being picked, since `LD_LIBRARY_PATH=/usr/local/cuda11.7/extras/CUPTI/lib64/` appears to fix the issue. Is there a way to observe or control which CUDA installation is being picked?","I just want to check: did you actually update your jaxlib to the latest release (0.4.10)? You also need to pick up this commit: https://github.com/openxla/xla/commit/c2319a85f3b7d115e57f65ebe2deb67fc349f067 Fundamentally we'll look in the location given by the `RPATH`, and then look in your `LD_LIBRARY_PATH`.","No, I did not update, since I found a workaround (and I find these things to be fragile as they get updated  it sounds like the packaging situation is getting a lot more robust though so I'm excited to see that change!). When you say `RPATH`  which specific ELF file is most relevant here? I looked in xla_extension.so but see no RPATH, only RUNPATH (very similar, except the order in which they are searched). (and indeed, the cupti path is missing)  Overall, I think I have a solution for me locally here, and it looks like there is work in progress/landed to make this not an issue going forward, so feel free to close! Thanks for your help Peter!","Err, yes, it's `RUNPATH`. If you update to the current release, then you'll see the `cupti` path in the `RUNPATH` as well. Closing, since the issue is solved already in the current release."
590,"以下是一个github上的jax下的一个issue, 标题是(add a jax.nn.softmax_alt with alternative differentiation behavior)， 内容是 (Following up on CC(add custom_jvp for jax.nn.softmax), we found that at least one user's code worked better with the old differentiation approach. So we decided to keep it alive! Instead of relying on a flag, users can just apply `jax.nn.softmax_alt` to get the old behavior. (The flag is still here for now, though I'll delete it in followup.))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,add a jax.nn.softmax_alt with alternative differentiation behavior,"Following up on CC(add custom_jvp for jax.nn.softmax), we found that at least one user's code worked better with the old differentiation approach. So we decided to keep it alive! Instead of relying on a flag, users can just apply `jax.nn.softmax_alt` to get the old behavior. (The flag is still here for now, though I'll delete it in followup.)",2023-05-15T21:45:05Z,,closed,0,1,https://github.com/jax-ml/jax/issues/16015, talked me out of this...
584,"以下是一个github上的jax下的一个issue, 标题是(Slicing a CPU-placed jax array results in unnecessary host-to-device transfers)， 内容是 ( Description When one slices a CPUplaced array, jax keeps the result on a CPU, but, nevertheless, seems to send slice indices to TPU. Steps to reproduce:  Error:   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Slicing a CPU-placed jax array results in unnecessary host-to-device transfers," Description When one slices a CPUplaced array, jax keeps the result on a CPU, but, nevertheless, seems to send slice indices to TPU. Steps to reproduce:  Error:   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-14T14:58:41Z,bug,open,1,3,https://github.com/jax-ml/jax/issues/16002,I'm also affected by this. The simplest reproduction example is:  That raises: ,We are almost close to a fix. It should happen in the next 2 weeks or so.,I can still reproduce this 
639,"以下是一个github上的jax下的一个issue, 标题是(`jax.custom_jvp` promotes tangent-of-integer to integer (not float0))， 内容是 ( Description  This will fail the assert statement. I came across this when calling `jax.jvp(some_other_fn, ...)` inside the `defjvp` rule. This was complaining that an integer was given an integer tangent.  What jax/jaxlib version are you using? JAX 0.4.10, jaxlib 0.4.10  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.custom_jvp` promotes tangent-of-integer to integer (not float0)," Description  This will fail the assert statement. I came across this when calling `jax.jvp(some_other_fn, ...)` inside the `defjvp` rule. This was complaining that an integer was given an integer tangent.  What jax/jaxlib version are you using? JAX 0.4.10, jaxlib 0.4.10  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-14T08:17:33Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/16000,"Thanks for raising this. IIRC we did this intentionally at first when we introduced float0, I think because preexisting custom_jvp code didn't know about float0. But as you point out it's currently inconsistent with `jax.jvp` itself, so we should make these coherent somehow...  "
594,"以下是一个github上的jax下的一个issue, 标题是(sparse-sparse matrix multiply creates unnecessary zero entries)， 内容是 ( Description When multiplying two sparse BCOO matrices it seems the result always stores explicit zeroentries even when the corresponding row/column of `a` and `b` are all zero:  Expected output:   What jax/jaxlib version are you using? 0.4.8  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,sparse-sparse matrix multiply creates unnecessary zero entries, Description When multiplying two sparse BCOO matrices it seems the result always stores explicit zeroentries even when the corresponding row/column of `a` and `b` are all zero:  Expected output:   What jax/jaxlib version are you using? 0.4.8  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info _No response_,2023-05-13T21:02:05Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/15997,"Hi  thanks for the report! This is working as intended. You're correct that sparsesparse matmul often results in more stored elements than are strictly required, but those extra stored arguments are necessary due to the constraints of JAX's compilation model, which requires array shapes (and in this case the size of the sparse matrix buffers) to be known at compile time. The issue is that the sparse matrix indices are only known at runtime, so the output buffers must be able to handle the worst case. When multiplying two matrices with number of specified elements `a.nse` and `b.nse`, the worst case is an output with `out.nse = a.nse * b.nse` (an easy way to imagine this is if the first matrix has all entries in a single column, and the second matrices has all elements in a single row). In anything but this worst case, the result will be padded with zeros. To handle this, you have two options: 1) Call `out.sum_duplicates()` on the result of the matmul, outside JIT, in order to sum and remove duplicated entries. It might look like this:  2) If appropriate, you can use a structured sparse representation (e.g. with `n_batch=1` on the leftmost input) such that the output *nse* will be more constrained. Hope that helps!","ah I see, that makes sense! Would it somehow be possible to manually set the number of specified elements for the output? eg in this case I'm computing `Bi = S.T @ Ai @ S` for a bunch of very sparse matrices that are too large to store densely on the gpu but I know `Bi.nse == Ai.nse`.",How do you *know* that the output has the same nse as the input? Could you encode that knowledge by using structured sparsity for the `S` matrix (i.e. option 2 in my answer above)?,"The `Ai`s are nonzero only on subblocks (different for every i) and `S = [[D, b], [0, 1]]` where `D` is diagonal I ended up getting around the issue by simply rescaling the elements of `Ai` before constructing the sparse matrix, so no need for matrixmatrix multiplies :smile:  In case it's useful here's a basic example to illustrate, goes OOM on my 12GB GPU: "
620,"以下是一个github上的jax下的一个issue, 标题是(Refactor approx_top_k lowering to make it easier to understand)， 内容是 (Refactor approx_top_k lowering to make it easier to understand There have been two recent changes in this area: 1) migrating TPU lowering from XLA fallback to MLIR, 2) migrating lowering for other platforms from XLA fallback to MLIR. As I was trying to understand whether the versioning code makes sense, I had a hard time doing that. I think this refactoring makes this easier.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Refactor approx_top_k lowering to make it easier to understand,"Refactor approx_top_k lowering to make it easier to understand There have been two recent changes in this area: 1) migrating TPU lowering from XLA fallback to MLIR, 2) migrating lowering for other platforms from XLA fallback to MLIR. As I was trying to understand whether the versioning code makes sense, I had a hard time doing that. I think this refactoring makes this easier.",2023-05-12T19:02:08Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/15988, for review., for review. I'm not familiar the mhlo related lowering logic.
1242,"以下是一个github上的jax下的一个issue, 标题是(ROCM build is broken)， 内容是 (I was able to run the Hessian code using rocm/jaxbuild. I have not run the tests on that docker image.   Fixing Dockerfile.ms is probably not worth it I was in the middle of putting together a minimal pull request to fix this when I realized that most of the tests were broken and that basically, no one has tested the ROCm build in quite some time, which means that there's no planned support from Google  or at least that AMD ROCm support is not considered ""essential"" in the same way that CUDA/TPU support would be.  That's quite frustrating because screwing with ROCm while learning the python ecosystem is about all I've done for a month and I cannot afford a GPU. I haven't really written a line of python during that time. If this issue seems a little scattered, it's because I pulled the content from my notes.  Description I built JAX  And launched:   Initial Problems Seeing this in =step 17/21=. This is similar to RadeonOpenCompute/ROCm/issues CC(Relax test tolerance for core_test jvp tests.)  And then in =Step 21/21= after git )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",large language model,ROCM build is broken,"I was able to run the Hessian code using rocm/jaxbuild. I have not run the tests on that docker image.   Fixing Dockerfile.ms is probably not worth it I was in the middle of putting together a minimal pull request to fix this when I realized that most of the tests were broken and that basically, no one has tested the ROCm build in quite some time, which means that there's no planned support from Google  or at least that AMD ROCm support is not considered ""essential"" in the same way that CUDA/TPU support would be.  That's quite frustrating because screwing with ROCm while learning the python ecosystem is about all I've done for a month and I cannot afford a GPU. I haven't really written a line of python during that time. If this issue seems a little scattered, it's because I pulled the content from my notes.  Description I built JAX  And launched:   Initial Problems Seeing this in =step 17/21=. This is similar to RadeonOpenCompute/ROCm/issues CC(Relax test tolerance for core_test jvp tests.)  And then in =Step 21/21= after git ",2023-05-12T06:12:08Z,bug contributions welcome AMD GPU,closed,0,8,https://github.com/jax-ml/jax/issues/15983,"Are JAX tracers limited to optimizing python calls to XLA? Or does the trace analysis select from binding to different possible XLA functionality based on the JIT optimization process? If it does, then my comment about pmap is totally wrong. I see that XLA optimizes or interacts with PTX bytecode. And I've found the super helpful `autodidax.md`. Can code written with JAX benefit from AOT compilation?  Anyways, I'm just glad I can run the code on my GPU.","amd amd can you perhaps take a look? Note that we (Google) don't ship a ROCM build of JAX: one main reason for this is we have no way to test it. Without CI testing it is expected that it will break frequently. The ROCM build is community supported: we welcome PRs to fix it, but it's something the community maintains.",/,"This is a bit of a scattershot issue description but if you are having trouble building, there are some prebuilt options: Dockerhub:      https://hub.docker.com/r/rocm/jaxbuild whl files:      https://pypi.org/project/jaxlibrocm/     https://github.com/ROCmSoftwarePlatform/jax/releases/download/jaxlibv0.4.6rocm55/jaxlib0.4.6.550cp39cp39manylinux2014_x86_64.whl Also note the consumer boards are not strictly supported by ROCM at this point, but generally should work. ","thanks for responding. sorry, I'm just a bit frustrated. i'm finally able to use my new computer for programming and i'm running into issues.  I started to use the rocm/jaxbuild image, but the python i end up with can't run jupyter. it wasn't built with sqlite. here's the repository i'm running with: dcunited001/nbjax I think I can just add the wheels in on top of the rcom/tensorflow image. i'll try again and update the issue.  is there a custom branch with changes to XLA/JAX that the jaxlibrocm wheel and rocm/jaxbuild image are built with? ... ah ok, there are other release tags. i had only checked the main branch. i'm a bit new to python, so I just hadn't thought about releasing the wheels as release artifacts. also,  i would create a pull request, but it's a thin set of changes. i wasn't sure what version of numpy should be specified in `Dockerfile.ms`. I spec'd `1.21.6`, but I ended up with `1.24.3` after the build which is downgraded to `1.22.4` after running a few pip installs.","So I've got the docker image running with jaxlibrocm, tensorflowrocm and jupyter.  I'm starting with `JAX_PLATFORMS=cpu,rocm jupyterlab`, but when I run this:  Then I'm getting the error below I think I need to check out the ROCmSoftwarePlatform/jax.4.6 tag to figure out how to get past this error.  I'm trying to resolve by tracing through these references/modules in the jax codebase. I could definitely use help, but I should be able to figure this stuff out.  ",**Try this wheel** https://github.com/ROCmSoftwarePlatform/jax/releases/download/jaxlibv0.4.6rocm55/jaxlib0.4.6.550cp39cp39manylinux2014_x86_64.whl P.S. Building jaxlib for ROCm is a bit tricky. You also need to pull in XLA changes to build and there is ROCm fork for that as well.,"thanks   one of the biggest problems i've had in figuring out how to use the ROCm stuff is trying to learn how the builds/dependencies differ between the upstream tensorflow/etc repositories and the ROCmSoftwarePlatform repositories. i think diffing across the repositories checked out to specific repo's helps alot. the process i'm using is in orgbabel scripts here. using repo is a little heavy handed, but it just makes managing the repository clones easier. the resulting diff here jaxrocm046.diff. i fixed the link. i'm not sure if it's complete, since it gives an error code of 1."
1265,"以下是一个github上的jax下的一个issue, 标题是(grad of shard_map of lax.all_to_all crashes)， 内容是 ( Description The forward transformation works, but not the backwards pass:   `ValueError: all_to_all requires the size of the mapped axis axis_name to equal x.shape[split_axis], but they are 4 and 8 respectively.`  ``` ValueError                                Traceback (most recent call last)  in ()      32 np.testing.assert_array_equal(y, x)      33  > 34 print(jax.grad(loss)(1.0 * x))   errors third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     164     __tracebackhide__ = True     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e:     168       mode = _filtering_mode() third_party/py/jax/_src/api.py in grad_f(*args, **kwargs)     644        645   def grad_f(*args, **kwargs): > 646     _, g = value_and_grad_f(*args, **kwargs)     647     return g     648  third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     164     __tracebackhide__ = True     165     try: > 166       return fun(*args, **kwargs)    )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,grad of shard_map of lax.all_to_all crashes," Description The forward transformation works, but not the backwards pass:   `ValueError: all_to_all requires the size of the mapped axis axis_name to equal x.shape[split_axis], but they are 4 and 8 respectively.`  ``` ValueError                                Traceback (most recent call last)  in ()      32 np.testing.assert_array_equal(y, x)      33  > 34 print(jax.grad(loss)(1.0 * x))   errors third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     164     __tracebackhide__ = True     165     try: > 166       return fun(*args, **kwargs)     167     except Exception as e:     168       mode = _filtering_mode() third_party/py/jax/_src/api.py in grad_f(*args, **kwargs)     644        645   def grad_f(*args, **kwargs): > 646     _, g = value_and_grad_f(*args, **kwargs)     647     return g     648  third_party/py/jax/_src/traceback_util.py in reraise_with_filtered_traceback(*args, **kwargs)     164     __tracebackhide__ = True     165     try: > 166       return fun(*args, **kwargs)    ",2023-05-12T05:48:02Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/15982
662,"以下是一个github上的jax下的一个issue, 标题是([ANN] Update approx_{max,min}_k jvp to make it more TPU-friendly.)， 内容是 (Scatters/gathers are very expensive, so instead we take a dot product with the appropriate onehot representation of the indices on the tangent. Also: * Updated tests to consider varying reduction_dimension * Removed possibility of cyclic dependency in debugging.py by removing a large jnp import, replacing it with a numpy_lax import. Speeds up jvp of T5X XLsized dot product, followed by approx_max_k, by 50x on TPU.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[ANN] Update approx_{max,min}_k jvp to make it more TPU-friendly.","Scatters/gathers are very expensive, so instead we take a dot product with the appropriate onehot representation of the indices on the tangent. Also: * Updated tests to consider varying reduction_dimension * Removed possibility of cyclic dependency in debugging.py by removing a large jnp import, replacing it with a numpy_lax import. Speeds up jvp of T5X XLsized dot product, followed by approx_max_k, by 50x on TPU.",2023-05-11T21:31:16Z,pull ready,open,0,3,https://github.com/jax-ml/jax/issues/15977,How do I replicate this failure using pytests? ,Nevermind; I was able to replicate.  Debugging., Now passes.  Rebased my PR.
1003,"以下是一个github上的jax下的一个issue, 标题是(Results mismatch between different convolution algorithms (CuDNN 8.3))， 内容是 ( Description I've been encountering a CuDNN lossofprecision error with a JAX/Flax script (error log below). Strangely, the script worked before and then I started getting this error even though no system changes have been made. Perhaps relatedly, the same script has been running into OOM errors even though it didn't before. I'm running on 4x NVIDIA RTX A6000 GPUs from a Singularity container (.def file pasted below) that has CUDA 11.6 / cuDNN 8.3 installed. I've also tested on cuDNN 8.4 and run into the same issue. Any help would be greatly appreciated! Log:  Singularity .def:   What jax/jaxlib version are you using? jax v0.4.8  Which accelerator(s) are you using? GPU  Additional system info Python 3.8, CUDA 11.60, cuDNN 8.3.2  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Results mismatch between different convolution algorithms (CuDNN 8.3)," Description I've been encountering a CuDNN lossofprecision error with a JAX/Flax script (error log below). Strangely, the script worked before and then I started getting this error even though no system changes have been made. Perhaps relatedly, the same script has been running into OOM errors even though it didn't before. I'm running on 4x NVIDIA RTX A6000 GPUs from a Singularity container (.def file pasted below) that has CUDA 11.6 / cuDNN 8.3 installed. I've also tested on cuDNN 8.4 and run into the same issue. Any help would be greatly appreciated! Log:  Singularity .def:   What jax/jaxlib version are you using? jax v0.4.8  Which accelerator(s) are you using? GPU  Additional system info Python 3.8, CUDA 11.60, cuDNN 8.3.2  NVIDIA GPU info ",2023-05-11T21:23:24Z,bug NVIDIA GPU,closed,0,1,https://github.com/jax-ml/jax/issues/15976,"We no longer ship new wheels with CUDNN 8.3 support. (JAX 0.4.8 was the last release.) The earliest version we support is CUDNN 8.6, but I'd encourage you to upgrade to the latest version (8.9). The `pip` wheel installation should make this easy to do. Closing, because I believe this is a bug with an old/no longer supported CUDNN version. Please reopen if you see it with an up to date CUDNN!"
634,"以下是一个github上的jax下的一个issue, 标题是(Segfault with `jax.clear_caches`)， 内容是 ( Description Calling `jax.clear_caches`:  Notably I also had to exclude the weakref stuff from my previous manual equivalent:  so it's probably something to do with that. I can probably set up a repro using the Diffrax test suite if that'd be helpful.  What jax/jaxlib version are you using? JAX 0.4.9, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.9  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Segfault with `jax.clear_caches`," Description Calling `jax.clear_caches`:  Notably I also had to exclude the weakref stuff from my previous manual equivalent:  so it's probably something to do with that. I can probably set up a repro using the Diffrax test suite if that'd be helpful.  What jax/jaxlib version are you using? JAX 0.4.9, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.9  NVIDIA GPU info _No response_",2023-05-11T19:05:14Z,bug needs info,closed,0,5,https://github.com/jax-ml/jax/issues/15973,Possibly related: CC(`jax.clear_backends` causes doublefree and `SIGABRT`.),Please add a repro...,Closing because we don't have a repro. Please reopen if we have a way to recreate the problem!,kidger will reopen if/when he can tell us how to reproduce it...,"If that's useful, this can be reproduced using Diffrax, as per patrickkidger/diffrax CC(Implement np.take (70).)."
797,"以下是一个github上的jax下的一个issue, 标题是(Assertion error for TPU VM within `unsynced_user_spec`)， 内容是 ( Description Hey all, I have an issue when trying to run flaxformer with jax on a TPU VM V28 using the base software version. On CPU this runs perfectly but on TPU I have run into this issue consistently even when trying to shift `flaxformer`, `flax` and `jax` versions. I feel it could be an issue with sharding but am not well versed enough to dig into it. Thanks! Logs:   What jax/jaxlib version are you using? jax v0.4.9, jaxlib v0.4.9  Which accelerator(s) are you using? TPU  Additional system info Linux, Python version 3.9.5  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Assertion error for TPU VM within `unsynced_user_spec`," Description Hey all, I have an issue when trying to run flaxformer with jax on a TPU VM V28 using the base software version. On CPU this runs perfectly but on TPU I have run into this issue consistently even when trying to shift `flaxformer`, `flax` and `jax` versions. I feel it could be an issue with sharding but am not well versed enough to dig into it. Thanks! Logs:   What jax/jaxlib version are you using? jax v0.4.9, jaxlib v0.4.9  Which accelerator(s) are you using? TPU  Additional system info Linux, Python version 3.9.5  NVIDIA GPU info _No response_",2023-05-11T12:32:25Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/15964,Cna you please provide a minimal repro?,"Hey  , Thanks, no problem! I am just using this configuration of the main branches for each of `flax`, `flaxformer`:  Then I ran a modified wmt `gin` script to include `tokens_choose_small.gin` as the base:  The script to train is just: ",All good this was fixed with the latest flaxformer update and some modifications. Thanks!, could you please share what modifications you have done to make it work? I am facing the same issue with the latest flaxformer. 
1274,"以下是一个github上的jax下的一个issue, 标题是(A100 8 GPUs extremely slow compared to a single A100)， 内容是 ( Description TL;DR When I run a t5x script using a A1008 GPU machine it is much slower compared to running the same script on a single A100 machine. There are many available configurations for running language model training using the t5x library which is based on jax/flax. To reproduce this issue one can take any of the training scripts run it on a single A100 and then do the same on a A100 8 GPU machine. I also cached the tfds dataset and read it from local disk to minimize any overhead and to just compare performance when switching from 1 GPU to 8 GPU single node training. The model I use doesn't require model parallelism it only suppose to data parallelism, e.g. you can pick up any model size up to (m)T5 XL (3.7B) to reproduce. Here are the stats I got during my training runs:   I am not able to attach the file as zst format is not supported and zip compressed folder is too large.  What jax/jaxlib version are you using? jax==0.4.9 jaxlib==0.4.9+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",large language model,A100 8 GPUs extremely slow compared to a single A100," Description TL;DR When I run a t5x script using a A1008 GPU machine it is much slower compared to running the same script on a single A100 machine. There are many available configurations for running language model training using the t5x library which is based on jax/flax. To reproduce this issue one can take any of the training scripts run it on a single A100 and then do the same on a A100 8 GPU machine. I also cached the tfds dataset and read it from local disk to minimize any overhead and to just compare performance when switching from 1 GPU to 8 GPU single node training. The model I use doesn't require model parallelism it only suppose to data parallelism, e.g. you can pick up any model size up to (m)T5 XL (3.7B) to reproduce. Here are the stats I got during my training runs:   I am not able to attach the file as zst format is not supported and zip compressed folder is too large.  What jax/jaxlib version are you using? jax==0.4.9 jaxlib==0.4.9+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info",2023-05-11T09:17:13Z,bug NVIDIA GPU,open,0,4,https://github.com/jax-ml/jax/issues/15962,"You could have an issue(software more likely, but also hardware) with the 8 GPUs machine. Can you run the single GPU version on the 8 gpu machine? To limit the GPUs seen by the process, you can do:  Also, can you give the spec of the 8 GPU computer? the number of CPU and CPU type, RAM, ... It is SMX GPU from the post above.","Could you please share the full run log (as a gist etc.)? On the 8GPU machine, did you start a single process, or multiple processes via cluster management tools like SLURM?","  Thanks for replying quickly. Let me give more details below by trying to reproduce the issue on 2 GPU A100 machine (I couldn't get a 8 A100 this time I will try again with it once available): TL;DR It worked as intended in a 2 A100 GPU  more details below.   Software Stack Running umt5 xl model. Model implementation uses jax pjit and according to README should support TPU (most efficientbut pricey) and GPU clusters. I am not using SLURM yet, right now for initial benchmarking I am using a script to do single node training with 8 GPUs where distribution strategy is only data parallelism. Later, I plan to try a larger model on single node and also move the training to a GPU cluster based on the priceperformance compared to TPUs.  Training script:  Content of `xl_umt5_pretrain.gin`:   Machine Specs lscpu.info.txt cpu.info.txt Base Ubuntu 20.04 Dependency Installation Steps: 1) CUDA:  2) Miniconda:  3) Verify jax with GPUs:   4) T5X:  5) Download cached dataset to machine. 6) Run single node training scripts  Running with 1 device visible with `CUDA_VISIBLE_DEVICES=0`  Adding training logs: single_gpu_train.log  Running with all 2 devices (assuming data parallelism via JAX pjit)  Adding training logs: two_gpus_train.log Performance per core / gpu is similar but need to try again with 8 GPUs. 1 gpu training: `timing/seqs_per_second_per_core=5.6435` 2 gpu training: `timing/seqs_per_second_per_core=4.85872`","Is this still a problem? We've done lots of work on LLM training in JAX, and we expect it to be screamingly fast these days on A100 GPUs."
1310,"以下是一个github上的jax下的一个issue, 标题是(Jax requires driver version 520.61.5, although any >= 450.80.02 is specified for CUDA 11)， 内容是 ( Description Hello! Thanks for your hard work! Faced the following problem: I installed jax like pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html my Driver Version is 470.182.03 (>= 450.80.02 for CUDA 11 on Linux). But when I try: import jax num_devices = jax.device_count() I get: 20230511 05:14:42.079662: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected 20230511 05:14:42.079796: E external/xla/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 470.182.3 does not match DSO version 520.61.5  cannot find working devices in this configuration No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Unfortunately my graphics card does not support the latest drivers than 470  What jax/jaxlib version are you using? jax==0.4.9 jaxlib==0.4.9+cuda11.cudnn86  Which accelerator(s) are you using? Tesla K80  Additional system info docker image nv)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"Jax requires driver version 520.61.5, although any >= 450.80.02 is specified for CUDA 11"," Description Hello! Thanks for your hard work! Faced the following problem: I installed jax like pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html my Driver Version is 470.182.03 (>= 450.80.02 for CUDA 11 on Linux). But when I try: import jax num_devices = jax.device_count() I get: 20230511 05:14:42.079662: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDAcapable device is detected 20230511 05:14:42.079796: E external/xla/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 470.182.3 does not match DSO version 520.61.5  cannot find working devices in this configuration No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Unfortunately my graphics card does not support the latest drivers than 470  What jax/jaxlib version are you using? jax==0.4.9 jaxlib==0.4.9+cuda11.cudnn86  Which accelerator(s) are you using? Tesla K80  Additional system info docker image nv",2023-05-11T05:19:45Z,bug NVIDIA GPU,closed,0,2,https://github.com/jax-ml/jax/issues/15961,"Not sure what's going on here. From NVIDIA's documentation, I think the combination of driver and runtime should work. Given K80 is a datacenter GPU, you could try NVIDIA's CUDA forward compatibility packages (https://docs.nvidia.com/deploy/cudacompatibility/index.htmlforwardcompatibilitytitle) Note that K80 is pretty old: e.g., NVIDIA has dropped support for Kepler GPUs in CUDA 12.","JAX has dropped support for Kepler series GPUs, so we're not going to be able to take any actions here."
788,"以下是一个github上的jax下的一个issue, 标题是(v0.4.9 Build fails on linux arm)， 内容是 ( Description To compile tfdf for arm we need jaxlib. Due to no linux arm64 wheels being published, i am trying to build from source. This fails at `jaxlibv.0.4.9`, i have tried to use the XLA repo at head instead of the pinned one, which brpught no luck.   The actual issue seems to be with xla, but unfourtunately it is beyond my understanding and skills to fix the build, so any help is appreciated!  What jax/jaxlib version are you using? jax v.0.4.9  Which accelerator(s) are you using? _No response_  Additional system info python3.10, arm64  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,v0.4.9 Build fails on linux arm," Description To compile tfdf for arm we need jaxlib. Due to no linux arm64 wheels being published, i am trying to build from source. This fails at `jaxlibv.0.4.9`, i have tried to use the XLA repo at head instead of the pinned one, which brpught no luck.   The actual issue seems to be with xla, but unfourtunately it is beyond my understanding and skills to fix the build, so any help is appreciated!  What jax/jaxlib version are you using? jax v.0.4.9  Which accelerator(s) are you using? _No response_  Additional system info python3.10, arm64  NVIDIA GPU info _No response_",2023-05-11T04:53:33Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15960,This was fixed by https://github.com/openxla/xla/commit/be1cee3e4aeee55ea5487d15398d92d9e21e19b2
761,"以下是一个github上的jax下的一个issue, 标题是(XLA_PYTHON_CLIENT_MEM_FRACTION=.XX gives me gpu error)， 内容是 ( Description I am trying to calculate self attention values with large image such size 720x1280. it always make huge OOM error so I am trying to use XLA_PYTHON_CLIENT_MEM_FRACTION=.XX < this environment. but It makes another error that can not found its GPU even though jax can find GPU before add the code. I want to know how to escape from this situation thank you.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info python 3.7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,XLA_PYTHON_CLIENT_MEM_FRACTION=.XX gives me gpu error, Description I am trying to calculate self attention values with large image such size 720x1280. it always make huge OOM error so I am trying to use XLA_PYTHON_CLIENT_MEM_FRACTION=.XX < this environment. but It makes another error that can not found its GPU even though jax can find GPU before add the code. I want to know how to escape from this situation thank you.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info python 3.7  NVIDIA GPU info _No response_,2023-05-09T09:11:28Z,bug NVIDIA GPU,open,0,1,https://github.com/jax-ml/jax/issues/15926,Can you provide a command and/or code snippet that minimally reproduces the issue?
937,"以下是一个github上的jax下的一个issue, 标题是(Jit unnecessarily re-compiles with `jax.device_put(x, sharding)`)， 内容是 ( Description I was loading some pretrained parameters in JAX and found my `train_step` got compiled more than once unexpectedly. I was able to reproduce with the following simple snippet  finishes without error and logs to stderr the following  The fourth call `test(x_np2jax_sharded)` triggers jit recompilation, but I did not find it necessary. Two compilations have identical log and jaxpr. I did not find any difference between `x_np2jax` and `x_np2jax_sharded`. And the AOT version below could finish without error.  What jax/jaxlib version are you using? jax v0.4.8, jaxlib v0.4.7  Which accelerator(s) are you using? GPU  Additional system info Python 3.9  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Jit unnecessarily re-compiles with `jax.device_put(x, sharding)`"," Description I was loading some pretrained parameters in JAX and found my `train_step` got compiled more than once unexpectedly. I was able to reproduce with the following simple snippet  finishes without error and logs to stderr the following  The fourth call `test(x_np2jax_sharded)` triggers jit recompilation, but I did not find it necessary. Two compilations have identical log and jaxpr. I did not find any difference between `x_np2jax` and `x_np2jax_sharded`. And the AOT version below could finish without error.  What jax/jaxlib version are you using? jax v0.4.8, jaxlib v0.4.7  Which accelerator(s) are you using? GPU  Additional system info Python 3.9  NVIDIA GPU info _No response_",2023-05-09T05:28:41Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/15924,The recompilation is because `x_np2jax_sharded` is sharded which would lead to a different HLO than the nonsharded one. You can check that by printing `test_aot.as_text()` for both sharded and unsharded and they will be different.," Thanks for your reply, but they are no different. There are several concerns. 1. You mentioned that `x_np2jax_sharded` is sharded, but I set its sharding to be identical to `x_jax`'s sharding, I guess if `x_jax` is nonsharded, so should `x_np2jax_sharded` be. 2. **The whole snippet above run without error**, so `test_aot` is compatible with all four inputs. 3. The HLOs are identical, verified by `assert test.lower(x_jax).compile().as_text() == test.lower(x_np2jax_sharded).compile().as_text()` A bit of use case for more context: I would like to load some pretrained parameters in NumPy format to an already initialized and possibly sharded network parameter (by haiku), so I roughly do `params[key] = jax.device_put(new_param, params[key].sharding)`, but got jit recompiles. This is nonfatal, only slowing down a long running program for about 10 seconds, but it would be better to get things clear.",Ohh I missed that the sharding was the same. I'll take a look and report back,I don't see any recompilation: ,Maybe upgrade to jax and jaxlib 0.4.9 and see if you see something different?,"Indeed, with v0.4.9 released 2 hours ago this problem went away! So this issue is considered fixed and I will close this issue. But I am a bit interested with Jax internals, so could you point to a specific commit that fixes this issue?  ",The fix was over a lot of commits. https://github.com/google/jax/commit/5d2f4530940f247fc7a4a6bc7e0f2b41fa39f7fb was the main one that fixed these kind of problems.
1013,"以下是一个github上的jax下的一个issue, 标题是(JAX internal assertion fails with vmap(shard_map(...)) when setting axis_name and spmd_axis_name)， 内容是 (If I set both `axis_name` and `spmd_axis_name` on `vmap`, I get an opaque internal assertion error from shard_map. I'm not sure if this is user error or a genuine bug, because spmd_axis_name is undocumented. After doing some testing, it does appear that `spmd_axis_name` _also_ effectively sets `axis_name` (in most cases?), so maybe that is enough. But if you aren't supposed to set both of these arguments on `vmap`, an informative error raised much earlier would have saved me hours of debugging. To reproduce:  Raises:    The end of my traceback looks like:    What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,JAX internal assertion fails with vmap(shard_map(...)) when setting axis_name and spmd_axis_name,"If I set both `axis_name` and `spmd_axis_name` on `vmap`, I get an opaque internal assertion error from shard_map. I'm not sure if this is user error or a genuine bug, because spmd_axis_name is undocumented. After doing some testing, it does appear that `spmd_axis_name` _also_ effectively sets `axis_name` (in most cases?), so maybe that is enough. But if you aren't supposed to set both of these arguments on `vmap`, an informative error raised much earlier would have saved me hours of debugging. To reproduce:  Raises:    The end of my traceback looks like:    What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-08T03:54:13Z,bug,open,0,5,https://github.com/jax-ml/jax/issues/15905,"~~I'm seeing indications that setting `spmd_axis_name` sometimes but does not always set `axis_name` inside the vmapped functions for use with parallel lax APIs. Not sure if that's also a bug, but we definitely do need both types of axis names for some of our model variations :)~~ EDIT: I think I was mistaken here! `spmd_axis_name` does not set `axis_name`","Indeed I don't think we've figured out exactly what `spmd_axis_name` should mean :) Here are some potential options: 1. Act just like a regular axis_name, except in interactions with specific primitives, like `shmap` and `with_sharding_constraint`, do something different; in particular ""just like"" would include ""work with collectives"". Also setting `axis_name` would then be an error. 2. Only `axis_name` works with collectives, and `spmd_axis_name` does not, so if you want collectives you better set both. I think the first makes sense. I'm not sure the second is coherent. Is it valid to set both but set them to different names? If not then isn't that redundant information? If so what does that mean?  can you weigh in, since IIRC you added `spmd_axis_name`?",I think this is just a bug. spmd_axis_name is just supposed to add this axis to any with_sharding_constraint along the vmapped axis. The same thing should happen for shard_map and it should get added to any vmapped axis (but the outer vmap axis should just be available inside the collective).,"There are two things here: 1. the bug in the OP is just that the vmapped axis size changes from the caller of the shmap to the body, yet we don't update the axis env with the new size; however 2. there are general (not shmaprelated) ambiguities in what spmd_axis_name means (e.g. is it valid to set both axis_name and spmd_axis_name, etc). I can fix the former, but can you weigh in on the latter?","Ah, for 2 spmd_axis_name should not be visible inside. I think it is just a way to map the axis_name to an axis of the mesh. If it is visible, then that should be a bug. Note that spmd_axis_name could also be a tuple of names (which it does not make sense to reference)."
1121,"以下是一个github上的jax下的一个issue, 标题是(Operations between replicated arrays on different meshes fails if mesh devices have different orders )， 内容是 ( Description Operations between replicated arrays created using different meshes can succeed, but only if the different meshes happen to have the same internal device order (apparently C contiguous order, for multidimensional meshes). This surprised me because the `sharding` attributes on these arrays is a `GSPMDSharding({replicated})` object, for which the public API suggests device order doesn't matter: `.device_set` and `.addressable_devices` are `set` objects. Could the underlying device assignment checks also be made to be order invariant, e.g., by using sets instead of tuples, or by sorting devices on replicated arrays? Example code to reproduce:        What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Operations between replicated arrays on different meshes fails if mesh devices have different orders ," Description Operations between replicated arrays created using different meshes can succeed, but only if the different meshes happen to have the same internal device order (apparently C contiguous order, for multidimensional meshes). This surprised me because the `sharding` attributes on these arrays is a `GSPMDSharding({replicated})` object, for which the public API suggests device order doesn't matter: `.device_set` and `.addressable_devices` are `set` objects. Could the underlying device assignment checks also be made to be order invariant, e.g., by using sets instead of tuples, or by sorting devices on replicated arrays? Example code to reproduce:        What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-05-07T04:39:44Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15903,I am trying to lift that restriction (actively) but it is hard and also might lead to inefficiencies so it will require more testing than usual. But currently those restrictions exist. Is this blocking you? I can suggest a workaround!,"Glad you're working on this! I was able to figure out my own workaround pretty quickly once I figured out what was going on. (In my case, I have two meshes, one of which is 1D. So I can just copy the device order for the 1D mesh from the ND mesh.)"
1288,"以下是一个github上的jax下的一个issue, 标题是(triton_autotuner: Rounding modifier required for instruction 'cvt')， 内容是 ( Description Getting the following error when trying to run code on a A100 80GB Google Cloud Debian Deep Learning image (c0deeplearningcommoncu113v20230501debian10). This code is tested and works on TPU (using t5x library). I don't know if this error is related to my setup but after creating the instance before running the code these are the steps I took: 1) Created a new conda environment with py3.9 2) Install latest jax cuda `pip install jax[cuda] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`, which is 4.8.0 as of writing. 3)  Clone t5x library and install editable local version `e git+https://github.com/googleresearch/t5x.gitegg=t5x`. 4) Install extra dep:`pip install t5`. 5) Upgrade CUDNN library to 8.6.0 as jax complained it requires at least that version by manually downloading cudnnlinuxx86_648.6.0.163_cuda11archive.tar.xz and then running the following:   6) Verified GPU is usable by jax:  The following is the error I get when running a t5x pretraining script using train.py.   What jax/jaxlib ver)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,triton_autotuner: Rounding modifier required for instruction 'cvt'," Description Getting the following error when trying to run code on a A100 80GB Google Cloud Debian Deep Learning image (c0deeplearningcommoncu113v20230501debian10). This code is tested and works on TPU (using t5x library). I don't know if this error is related to my setup but after creating the instance before running the code these are the steps I took: 1) Created a new conda environment with py3.9 2) Install latest jax cuda `pip install jax[cuda] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html`, which is 4.8.0 as of writing. 3)  Clone t5x library and install editable local version `e git+https://github.com/googleresearch/t5x.gitegg=t5x`. 4) Install extra dep:`pip install t5`. 5) Upgrade CUDNN library to 8.6.0 as jax complained it requires at least that version by manually downloading cudnnlinuxx86_648.6.0.163_cuda11archive.tar.xz and then running the following:   6) Verified GPU is usable by jax:  The following is the error I get when running a t5x pretraining script using train.py.   What jax/jaxlib ver",2023-05-07T01:57:07Z,bug NVIDIA GPU,open,0,11,https://github.com/jax-ml/jax/issues/15900, How do we know that it originates in the triton_autotuner?, Is it hard to propagate C++ stack trace along with the error?,">  How do we know that it originates in the triton_autotuner? I am not 100% if that's the root cause but I should've probably pasted this as well:  I was able to successfully run the code with from scratch nvidia driver, cuda (12.1), cudnn and jax installation 1) Launched a A100 in Google Cloud with base ubuntu 18.04 image. 2) Install latest nvidia driver with cuda 12.1. 3) Install miniconda and create a conda env. 4) Install jax and cudnn.  5) Install t5x from source and install t5.   nvcc not installed.","Oh this is CUDA12, probably that would explain it  we had some bugs filed on CUDA12 before.","I think this is actually a case of too *old* a CUDA installation, not the other way around. The image is named `c0deeplearningcommoncu113v20230501debian10`: note ""cu113"". JAX is built for CUDA 11.8 (or CUDA 12), and if I recall correctly Ampere GPU support wasn't added until longer after 11.3. Can you update to CUDA 11.8 or newer? Note `nvidiasmi` reports the CUDA version of the driver, not the installed libraries. You need both to be sufficiently new.",Sorry if it was not clear but what I wanted mention was issue was fixed when I installed cuda 12 from scratch instead using the Google Cloud image.,"Recently got this error (might be related to 0.4.9 release looking into it):  **Edit**: Tried again by recreating a new instance, and I wasn't able to reproduce the error.","Getting similar when trying to run a custom model on an A6000 with `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` over Runpod's PyTorch or Tensorflow images.  Tried both cuda11 and cuda12, same issue. https://pastebin.com/raw/MUsYZje8 Update: Seems to only happen with bfloat16. Works fine with float32"," Do you have another copy of `ptxas` installed? Is there one in your `PATH`? My strong suspicion is still ""we are finding an ancient `ptxas`""."," I don't, but it should be whatever is used here https://hub.docker.com/r/runpod/pytorch/","   After some testing, it appears to be caused by me accidentally mixing `bflaot16` values with `float32` ones.  Seems a check for that is missing somewhere prior to assembly."
941,"以下是一个github上的jax下的一个issue, 标题是(Flax output mismatch for multi-dimensional batch input on GPUs)， 内容是 ( Description Hi, I am trying to run a simple MLP on A100 GPU with multidimensional batch inputs of shape *(b1, b2, input_dim)* and output shape *(b1, b2, 1)*. Flax outputs when passing the entire input *(b1, b2, input_dim)* v/s passing a single input *(1, 1, input_dim)* iteratively are not matching. When I run the same code example on CPU or run the equivalent PyTorch version, it matches exactly. Please see the minimal code example, colab link and outputs in the issue below: https://github.com/google/flax/issues/3084   What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Flax output mismatch for multi-dimensional batch input on GPUs," Description Hi, I am trying to run a simple MLP on A100 GPU with multidimensional batch inputs of shape *(b1, b2, input_dim)* and output shape *(b1, b2, 1)*. Flax outputs when passing the entire input *(b1, b2, input_dim)* v/s passing a single input *(1, 1, input_dim)* iteratively are not matching. When I run the same code example on CPU or run the equivalent PyTorch version, it matches exactly. Please see the minimal code example, colab link and outputs in the issue below: https://github.com/google/flax/issues/3084   What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ",2023-05-06T07:53:43Z,bug NVIDIA GPU,open,0,3,https://github.com/jax-ml/jax/issues/15898,"I think this is just a numerical preision problem, not a bug. eg change the print statement to  and it says True on GPU (and CPU). Similarly,  ","That's right, Dr. . However, I guess it should exactly match given that precision (float32) is not changed between batch and individual versions of code.  may let us know his views on this from the domain perspective.",Hi zeel  Looks like the precision issue on GPU with batch input has been resolved in later versions of JAX. I tested the provided code using JAX version 0.4.33 on colab GPU and JAX 0.4.35 on cloud VM having GPU A100. I could not find any output mismatch in both the cases. Please find the screenshot of output on cloud VM with A100 GPU. !image Attaching a colab gist for reference. Could you please test with latest JAX and Flax versions and check if the issue still persists? Thank you.
1259,"以下是一个github上的jax下的一个issue, 标题是(RNG slows down data parallel training)， 内容是 ( Discussed in https://github.com/google/jax/discussions/15783  Originally posted by **jjyyxx** April 27, 2023 I was working with a transformer model in jax and haiku, and found that dropout greatly slows down data parallel training, the main training step looks like  where  Sharding is created with `sharding = jax.sharding.PositionalSharding(jax.devices())`, containing GPU:0 and GPU:1  `train_key` is a `PRNGKeyArray`, not sharded  `self._train_state` is a PyTree of params and opt_states, replicated with `jax.device_put(train_state, sharding.replicate())`  `batch` is a PyTree of data and labels, sharded with `jax.device_put(batch, sharding)` Every operation in this model (except final loss reduction) is independent between each sample in batch, so this should be trivially data parallel. Without `x = hk.dropout(hk.next_rng_key(), self.dropout, x)` (boils down to a `jax.random.split` and a `jax.random.bernoulli`), every thing works well (Single device: 4.2 it/s, Two devices: 7.5 it/s). But when dropout is enabled (called 20 times)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,RNG slows down data parallel training," Discussed in https://github.com/google/jax/discussions/15783  Originally posted by **jjyyxx** April 27, 2023 I was working with a transformer model in jax and haiku, and found that dropout greatly slows down data parallel training, the main training step looks like  where  Sharding is created with `sharding = jax.sharding.PositionalSharding(jax.devices())`, containing GPU:0 and GPU:1  `train_key` is a `PRNGKeyArray`, not sharded  `self._train_state` is a PyTree of params and opt_states, replicated with `jax.device_put(train_state, sharding.replicate())`  `batch` is a PyTree of data and labels, sharded with `jax.device_put(batch, sharding)` Every operation in this model (except final loss reduction) is independent between each sample in batch, so this should be trivially data parallel. Without `x = hk.dropout(hk.next_rng_key(), self.dropout, x)` (boils down to a `jax.random.split` and a `jax.random.bernoulli`), every thing works well (Single device: 4.2 it/s, Two devices: 7.5 it/s). But when dropout is enabled (called 20 times",2023-05-05T21:01:26Z,bug performance,open,0,6,https://github.com/jax-ml/jax/issues/15895,", do you have a minimal code example that reproduces this?", I could give it a try. But can a snippet containing haiku be considered minimal? I suspect at least a mediumsized model could reveal the difference. But I will try it first anyway.,"  A hopefully not too long code example (dispatch cost for small models obscure the difference). Compare `return x` with `return hk.dropout(hk.next_rng_key(), 0.1, x)`, then run single & multi GPU training to see the difference. My local experiment on two RTX 3090 Ti shows ",Thank you!,"Try using `jax.lax.with_sharding_constraint` on dropout? Specifically try this in `def dropout`: `return jax.lax.with_sharding_constraint(kh.dropout(...), some_sharding)` "," I am afraid it's not a direct sharding issue. I further simplify the `dropout` function to this  For two GPU case, all calls to `dropout` print  It should also be noted that even a constant PRNG key `jax.random.PRNGKey(42)` for `bernoulli` could reproduce this. _But of course this slowdown will go away with_  _Offtopic: could you leave some comment about CC(Simple data parallelism via jax.Array feels unergonomic to use)? The sharding attached to an array must be in a ""broadcastable"" form, which is especially counterintuitive to use with PyTree?_"
795,"以下是一个github上的jax下的一个issue, 标题是(jax doesn't find multiple gpu's on A100 migs)， 内容是 ( Description When trying to use `pmap` on an A100 80GB that has been split into two 40GB multiinstance GPUs (migs) it doesn't seem to work, `jax.devices()` only finds a single device. Wrote an minimal example:  Here's an example output requesting 2 40GB migs  Here's the output on a 2 regular 40GB A100s  Any ideas why this could be happening?  What jax/jaxlib version are you using? jax 0.4.2  Which accelerator(s) are you using? A100 40GB and A100 40GB mig  Additional system info _No response_  NVIDIA GPU info Can't run this right now, will update when I have access.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax doesn't find multiple gpu's on A100 migs," Description When trying to use `pmap` on an A100 80GB that has been split into two 40GB multiinstance GPUs (migs) it doesn't seem to work, `jax.devices()` only finds a single device. Wrote an minimal example:  Here's an example output requesting 2 40GB migs  Here's the output on a 2 regular 40GB A100s  Any ideas why this could be happening?  What jax/jaxlib version are you using? jax 0.4.2  Which accelerator(s) are you using? A100 40GB and A100 40GB mig  Additional system info _No response_  NVIDIA GPU info Can't run this right now, will update when I have access.",2023-05-03T10:06:23Z,bug NVIDIA GPU,closed,0,2,https://github.com/jax-ml/jax/issues/15843,"a Were you able to understand this any better? I'm seeing similar, with 4 A100s split 7ways each with MiG, and Jax only seeing a single device. Here's what I have: ","This is a CUDA limitation, not a JAX issue per se. Unfortunately MIG was not designed with multiGPU in mind, limiting itself to use cases using less than a full GPU.  So for example it does not create a new CUDA device index for each MIG instance.  To solve this would require work in CUDA first, but then probably also NCCL after that, if not also finally in JAX itself. But beyond that, there are no fast MIGtoMIG communication mechanisms implemented either, so even if the application could use multiMIG in the way you're describing, it would likely see a communication bottleneck between them.  NCCL might end up needing to roundtrip the communications through system memory for example."
296,"以下是一个github上的jax下的一个issue, 标题是(Bump XLACallModule to version 5 and add the function_list.)， 内容是 (Bump XLACallModule to version 5 and add the function_list.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Bump XLACallModule to version 5 and add the function_list.,Bump XLACallModule to version 5 and add the function_list.,2023-05-02T21:08:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15829
1089,"以下是一个github上的jax下的一个issue, 标题是(Fully-Fused MLP implementation)， 内容是 (Dear all, Developers from Nvidia have recently fused MLP network in a large mega kernel and called it ""FullyFused MLP"", which is 5x10x faster than the equivalent MLP implementation from PyTorch or TensorFlow. People currently use these fullyfused MLPs to accelerate training and inference of ML applications, particularly they use them extensively in neural computer graphics to obtain high gains in speed. I would like to know do you maybe plan to implement it or do you have an equivalent implementation? (Im not sure if this can be achieved by simply applying `jax.jit` on normal MLP). I think it is a nice feature, which will enrich the jax library and make it more attractive for machine learning / computer graphics researchers and engineers.  References: [1] Realtime Neural Radiance Caching for Path Tracing [2] Technical Paper [3] Nvidia's Fully fused MLP implementation)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fully-Fused MLP implementation,"Dear all, Developers from Nvidia have recently fused MLP network in a large mega kernel and called it ""FullyFused MLP"", which is 5x10x faster than the equivalent MLP implementation from PyTorch or TensorFlow. People currently use these fullyfused MLPs to accelerate training and inference of ML applications, particularly they use them extensively in neural computer graphics to obtain high gains in speed. I would like to know do you maybe plan to implement it or do you have an equivalent implementation? (Im not sure if this can be achieved by simply applying `jax.jit` on normal MLP). I think it is a nice feature, which will enrich the jax library and make it more attractive for machine learning / computer graphics researchers and engineers.  References: [1] Realtime Neural Radiance Caching for Path Tracing [2] Technical Paper [3] Nvidia's Fully fused MLP implementation",2023-05-02T13:51:18Z,enhancement,closed,2,2,https://github.com/jax-ml/jax/issues/15823,"The advantages of this approach mainly apply to smaller networks with widths up to 128 and completely disappear with MLPs that have width 256 and higher. Initially, the training advantage was about 2x, but this gap has only diminished over time. Though it might be interesting to pinpoint the precise performance difference, developing a custom fully fused MLP in JAX appears unwarranted. The main reason is that the method is highly specialized and the benefits are not substantial enough. Furthermore, attempting such an integration would conflict with JAX's core values such as flexibility and simplicity. Introducing a highly specialized solution would undermine these fundamental principles.",  Thanks for your answer!
613,"以下是一个github上的jax下的一个issue, 标题是(Segmentation fault in _xla_gc_callback)， 内容是 ( Description I don't have a MWE, but I have a minimum change to a test that reproduces the segmentation fault. Simply cloning the repository and running the test should reproduce the segmentation fault:  gives  Switching back to main should work fine.  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.3)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Segmentation fault in _xla_gc_callback," Description I don't have a MWE, but I have a minimum change to a test that reproduces the segmentation fault. Simply cloning the repository and running the test should reproduce the segmentation fault:  gives  Switching back to main should work fine.  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.3",2023-05-01T17:53:21Z,bug,open,1,5,https://github.com/jax-ml/jax/issues/15810,"I have a similar problem with my code, and I can reproduce your problem on macOS 13.3.1, Arm64, Python 3.11.0, same jax and jaxlib versions", Thank you for confirming it!,"This is a known issue with the old Fortran 77 implementation of COBYLA, which is buggy and not maintained anymore. See https://github.com/scipy/scipy/issues/18118 and https://github.com/libprima/primabugfixes. ",Thanks for clarifying  !  I wonder if Jax must depend on COBYLA?,"Hi  , Yes if Jax involves optimization without derivatives. For such problems, Powell's methods are the most widely used ones. For instance, see Section 1 of a recent paper on Powell's solvers as well as the Google searches of COBYLA and BOBYQA. Note that, even though the old Fortran 77 implementation of the aforementioned solvers is truly a masterpiece, it contains many bugs (mostly due to the language itself), which can lead to segmentation faults (as you observed) or infinite loops. For example, see Section 4.4 of the above paper and many GitHub issues. It is strongly discouraged to use the Fortran 77 version of these solvers anymore. Instead, it is advised to switch to the modernized and improved implementation offered by PRIMA.  If you need help with these solvers, I will be very happy to assist.  Thanks and regards, Zaikun"
324,"以下是一个github上的jax下的一个issue, 标题是(Add new attribute `function_list` to XLACallModule and bump the version.)， 内容是 (Add new attribute `function_list` to XLACallModule and bump the version.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add new attribute `function_list` to XLACallModule and bump the version.,Add new attribute `function_list` to XLACallModule and bump the version.,2023-04-29T04:27:47Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15793
389,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Simplify back_compat_test.py to use jax_export mechanisms to run)， 内容是 ([jax2tf] Simplify back_compat_test.py to use jax_export mechanisms to run the serialized module, instead of relying on tf.XlaCallModule.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Simplify back_compat_test.py to use jax_export mechanisms to run,"[jax2tf] Simplify back_compat_test.py to use jax_export mechanisms to run the serialized module, instead of relying on tf.XlaCallModule.",2023-04-28T14:25:14Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15787
349,"以下是一个github上的jax下的一个issue, 标题是(Make sure jax2tf back_compat test capture non-backward compabible XlaCallModule)， 内容是 (Make sure jax2tf back_compat test capture nonbackward compabible XlaCallModule op changes.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Make sure jax2tf back_compat test capture non-backward compabible XlaCallModule,Make sure jax2tf back_compat test capture nonbackward compabible XlaCallModule op changes.,2023-04-28T07:32:54Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15784
793,"以下是一个github上的jax下的一个issue, 标题是(Notice a different between jax.image.resize and F.interpolate when using ""bicubic"")， 内容是 ( Description Hi, I am trying to convert a Pytorch model to JAX model, and I find there are some implementations different in ""bicubic"".  Here is a script I have used to confirm that the outputs are different.   from the prints I get:  I tried some other resizing methods. e.g., linear and bilinear, and they look fine. Does anyone have a workaround for it?   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Notice a different between jax.image.resize and F.interpolate when using ""bicubic"""," Description Hi, I am trying to convert a Pytorch model to JAX model, and I find there are some implementations different in ""bicubic"".  Here is a script I have used to confirm that the outputs are different.   from the prints I get:  I tried some other resizing methods. e.g., linear and bilinear, and they look fine. Does anyone have a workaround for it?   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-27T01:00:52Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/15768,"I'm not sure what's up here. The output of JAX exactly matches Pillow and TensorFlow for the same bicubic resize, so I don't think we're doing something wrong here, but PyTorch must be using a different convention for something as opposed to all the other systems."," I could use your input here. I looked into this a bit more. There are two groups of behaviors for cubic upsampling: * pillow, JAX, Tensorflow all agree (almost exactly) * PyTorch (with `align_corners=False`) and OpenCV agree (almost exactly) I think there are two different things happening here:  JAX and PyTorch use different cubic interpolation kernels JAX/pillow/TF use the Keys cubic kernel with A = 0.5 (https://en.wikipedia.org/wiki/Bicubic_interpolationBicubic_convolution_algorithm). PyTorch/OpenCV uses a cubic kernel with A = 0.75. At least some users consider this a bug (https://github.com/opencv/opencv/issues/17720 ), and that set of parameters suffers from ringing artifacts [^mitchell]. [^mitchell]: Mitchell, D.P. and Netravali, A.N., 1988. Reconstruction filters in computergraphics. ACM Siggraph Computer Graphics, 22(4), pp.221228. It's actually quite easy to get JAX to use the PyTorch/OpenCV kernel, if we use:  in place of https://github.com/google/jax/blob/e51d12cdef0fd118e6f8cc3357cd8f95a68a79fa/jax/_src/image/scale.pyL36 This solves the mismatch except for a band of 34 pixels near the edge of the image. This is because:  JAX and PyTorch use different approaches for edge padding Cubic interpolation requires sampling pixel values from outside the original input image. JAX/pillow/TF handle this by truncating the convolutional kernel and rescaling it to keep its total weight 1. I believe PyTorch handles this by edgepadding (i.e., repeating the value on the edge). This leads to slightly different values in a 34 pixel band near each edge. I'll look into this a bit further, we can probably replicate the PyTorch behavior (optionally) with a bit more work.","I'm curious if it's actually important to you that we replicate PyTorch's behavior exactly, or you merely noticed it is different and were curious about it.","Hi  Thanks for digging into these differences! I actually tried to reproduce some research works in JAX and noticed there are some performance degrades, then I started to look into some lowerlevel implementations and found out this : )"," I don't know about pytorch but your description of JAX's resize sounds correct, and yes there are different choices for the bicuibic kernel. The other thing to keep in mind is that when downsampling JAX's resize will do antialiasing by default.", I'm particularly curious about the edge padding behavior. How do the OpenCV/PyTorch behaviors compare with what JAX/TF/pillow do?,I should add: it's actually slightly annoying to mimic the PyTorch edge padding behavior *because* of the antialiasing we do on downsampling. The antialiasing means the kernel can be of an arbitrary width measured in input pixels.,"TF is the same: https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/refs/heads/master/tensorflow/core/kernels/image/scale_and_translate_op.(未找到相关数据) I think we could mimic the PyTorch edge behavior if we wanted to, no, we'd just shift the kernel weight to that last pixel and make it zero outside? That said, I'm skeptical this will cause large enough differences to break models. ","Hi  , thanks for digging into this and the great write up.  I have a similar issue but I want to replicate JAX's behaviour in Torch.  Based on your findings, is it possible to do this with an appropriate set of args to `jax.image.resize`? Thanks!",">  I could use your input here. >  > I looked into this a bit more. There are two groups of behaviors for cubic upsampling: >  > * pillow, JAX, Tensorflow all agree (almost exactly) > * PyTorch (with `align_corners=False`) and OpenCV agree (almost exactly) >  > I think there are two different things happening here: >  >  JAX and PyTorch use different cubic interpolation kernels > JAX/pillow/TF use the Keys cubic kernel with A = 0.5 (https://en.wikipedia.org/wiki/Bicubic_interpolationBicubic_convolution_algorithm). >  > PyTorch/OpenCV uses a cubic kernel with A = 0.75. At least some users consider this a bug ([opencv/opencv CC(MAINT Use a generator expression in tuple([... for ... in ...]))](https://github.com/opencv/opencv/issues/17720) ), and that set of parameters suffers from ringing artifacts 1. >  > It's actually quite easy to get JAX to use the PyTorch/OpenCV kernel, if we use: >  >  >  > in place of >  > https://github.com/google/jax/blob/e51d12cdef0fd118e6f8cc3357cd8f95a68a79fa/jax/_src/image/scale.pyL36 >  > This solves the mismatch except for a band of 34 pixels near the edge of the image. This is because: >  >  JAX and PyTorch use different approaches for edge padding > Cubic interpolation requires sampling pixel values from outside the original input image. JAX/pillow/TF handle this by truncating the convolutional kernel and rescaling it to keep its total weight 1. I believe PyTorch handles this by edgepadding (i.e., repeating the value on the edge). This leads to slightly different values in a 34 pixel band near each edge. >  > I'll look into this a bit further, we can probably replicate the PyTorch behavior (optionally) with a bit more work. >  >  Footnotes > 1. Mitchell, D.P. and Netravali, A.N., 1988. Reconstruction filters in computergraphics. ACM Siggraph Computer Graphics, 22(4), pp.221228. ↩ Hi , it would be really nice if this work is continued. I'm trying to convert a pytorch model to flax and this ""Bicubic interpolate"" layer is the only part that prevents the exact replication of the model.  Thank you."
324,"以下是一个github上的jax下的一个issue, 标题是(Add new attribute `function_list` to XLACallModule and bump the version.)， 内容是 (Add new attribute `function_list` to XLACallModule and bump the version.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add new attribute `function_list` to XLACallModule and bump the version.,Add new attribute `function_list` to XLACallModule and bump the version.,2023-04-27T00:41:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15767
1009,"以下是一个github上的jax下的一个issue, 标题是(Using `ir.F64Type()` for `hlo.CustomCallOp`'s `out_type`)， 内容是 ( Description Hi! I have been trying to bind my c++ method into a jax primitive. The C++ backend essentially takes in an array and outputs a double. I was unable to make it work with specifying the `out_type` as `ir.F64Type()` which is what I would expect the type to be if my c++ backend function returns a double. What I ended up with is the following code below:   So I ended up used the `RankedTensorType`, since if I replace the line:  with  I get that `mlir` doesn't recognize the type, i.e. the error I obtain is  I don't understand why `f64` is not accepted as a viable type?  What jax/jaxlib version are you using? jax v0.4.8,  jaxlib v0.4.7  Which accelerator(s) are you using? CPU  Additional system info python3.9, x86_64 GNU/Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Using `ir.F64Type()` for `hlo.CustomCallOp`'s `out_type`," Description Hi! I have been trying to bind my c++ method into a jax primitive. The C++ backend essentially takes in an array and outputs a double. I was unable to make it work with specifying the `out_type` as `ir.F64Type()` which is what I would expect the type to be if my c++ backend function returns a double. What I ended up with is the following code below:   So I ended up used the `RankedTensorType`, since if I replace the line:  with  I get that `mlir` doesn't recognize the type, i.e. the error I obtain is  I don't understand why `f64` is not accepted as a viable type?  What jax/jaxlib version are you using? jax v0.4.8,  jaxlib v0.4.7  Which accelerator(s) are you using? CPU  Additional system info python3.9, x86_64 GNU/Linux  NVIDIA GPU info _No response_",2023-04-26T18:13:41Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15758,"This is an stablehlo MLIR dialect issue, not a JAX issue, but I believe they simply only allow RankedTensor types as inputs or outputs to most operators. If you want a scalar, you need to wrap it a `RankedTensor`. I should add: we are hoping to add a more userfriendly FFI so you don't have to write your own stablehlo lowerings in most cases.","Thanks for answering!  > This is an stablehlo MLIR dialect issue, not a JAX issue, but I believe they simply only allow RankedTensor types as inputs or outputs to most operators. If you want a scalar, you need to wrap it a RankedTensor. Yes I guess it is more of a MLIR dialect issue than pure JAX issue, I hoped I could try to better understand what MLIR really wants through posting this issue. Where can I find more on exact objects defined within the mlir dialect and their attributes? I am not sure what is the correct material to read, so I usually play with the debugger and try to guess what objects are needed, so it would be nice to properly read up on what exactly is happening under the hood when doing the MLIR lowering. "
1305,"以下是一个github上的jax下的一个issue, 标题是(JAX 0.4.8 on CUDA 12.0, H100, Driver: 525.60.13  yields CUDNN_STATUS_INTERNAL_ERROR)， 内容是 ( Description Hi there,  On H100 GPU with CUDA 12.0, NvidiaDriver 525.60.13 Jax seems to have some issues. First a sample to reproduce and below that the error message: `import jax.numpy as jnp a = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) b = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) result = jnp.matmul(a, b) print(""Result: "", result)` 20230426 07:08:38.181946: W external/xla/xla/stream_executor/cuda/cuda_dnn.cc:397] There was an error before creating cudnn handle: cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found. 20230426 07:08:38.182026: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR Traceback (most recent call last):   File ""bug_report_script.py"", line 7, in      result = jnp.matmul(a, b) ..... ..... jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. As an additional note, I observed a similar error on the A1)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"JAX 0.4.8 on CUDA 12.0, H100, Driver: 525.60.13  yields CUDNN_STATUS_INTERNAL_ERROR"," Description Hi there,  On H100 GPU with CUDA 12.0, NvidiaDriver 525.60.13 Jax seems to have some issues. First a sample to reproduce and below that the error message: `import jax.numpy as jnp a = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) b = jnp.array([[1,2,3], [4,5,6], [1,2,3]]) result = jnp.matmul(a, b) print(""Result: "", result)` 20230426 07:08:38.181946: W external/xla/xla/stream_executor/cuda/cuda_dnn.cc:397] There was an error before creating cudnn handle: cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found. 20230426 07:08:38.182026: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR Traceback (most recent call last):   File ""bug_report_script.py"", line 7, in      result = jnp.matmul(a, b) ..... ..... jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. As an additional note, I observed a similar error on the A1",2023-04-26T07:24:29Z,bug NVIDIA GPU,closed,0,7,https://github.com/jax-ml/jax/issues/15752,Hi Sebastien! Is this on a cloud platform or your local setup?,"Hi, Thank you for your reply! A vendor setup the H100 server and we accessing it via ssh for testing purposes.","When I run the example as given in the OP on H100, JAX hangs for a long time and then gives the same error: ",Will look into that further.,The example works with CUDA 12.1: ,"Is this still an issue? Given yhtang@'s comments, I suspect everything is fine now. Support for H100 has also matured significantly since April.","There was no update since November, and we routinely run on H100, so I think this problem was fixed by either a newer CUDA or a newer JAX."
304,"以下是一个github上的jax下的一个issue, 标题是(Make shard_map_test compatible with custom_prng)， 内容是 (Tested:  There's no CI test coverage for this yet, but we're getting there...)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Make shard_map_test compatible with custom_prng,"Tested:  There's no CI test coverage for this yet, but we're getting there...",2023-04-25T18:40:36Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15744
1257,"以下是一个github上的jax下的一个issue, 标题是( DNN library initialization failed.)， 内容是 ( Description I tried run AlphaFoldMultimer on GPU, after the parameters and inputs are loaded, the running stopped by raising: `jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ` I can't find out the exact error information from the error message I have. Following the errors: > I0425 13:55:15.099713 140484539389760 af_multimer_multiseed.py:552] Running model model_1_multimer_v2_pred_0 I0425 13:55:15.100178 140484539389760 model.py:165] Running predict with shape(feat) = {'aatype': (247,), 'residue_index': (247,), 'seq_length': (), 'msa': (4096, 247), 'num_alignments': (), 'template_all_atom_positions': (4, 247, 37, 3), 'template_all_atom_mask': (4, 247, 37), 'template_aatype': (4, 247), 'asym_id': (247,), 'sym_id': (247,), 'entity_id': (247,), 'deletion_matrix': (4096, 247), 'deletion_mean': (247,), 'all_atom_mask': (247, 37), 'all_atom_positions': (247, 37, 3), 'assembly_num_chains': (), 'entity_mask': (247,), 'num_templates': (), 'cluster)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi, DNN library initialization failed.," Description I tried run AlphaFoldMultimer on GPU, after the parameters and inputs are loaded, the running stopped by raising: `jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. ` I can't find out the exact error information from the error message I have. Following the errors: > I0425 13:55:15.099713 140484539389760 af_multimer_multiseed.py:552] Running model model_1_multimer_v2_pred_0 I0425 13:55:15.100178 140484539389760 model.py:165] Running predict with shape(feat) = {'aatype': (247,), 'residue_index': (247,), 'seq_length': (), 'msa': (4096, 247), 'num_alignments': (), 'template_all_atom_positions': (4, 247, 37, 3), 'template_all_atom_mask': (4, 247, 37), 'template_aatype': (4, 247), 'asym_id': (247,), 'sym_id': (247,), 'entity_id': (247,), 'deletion_matrix': (4096, 247), 'deletion_mean': (247,), 'all_atom_mask': (247, 37), 'all_atom_positions': (247, 37, 3), 'assembly_num_chains': (), 'entity_mask': (247,), 'num_templates': (), 'cluster",2023-04-25T17:59:04Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15742,The error message explains the problem:  Upgrade your CuDNN installation?,">  Thanks! Problem solved. I upgraded the installation by running `pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Following https://github.com/google/jaxinstallation"
1261,"以下是一个github上的jax下的一个issue, 标题是(Seemingly odd behaviour with make_jaxpr)， 内容是 (Hello, I am trying to understand exactly how autodiff works with JAX. I have an example function:  Now I do `jax.make_jaxpr(fnc_jax)(1.0,1.0)`, which gives me the following output:  Question 1: Why is it that c, e, and h for example are doing the exact same computation? Wouldn't a better way be if we had something like this:  which gives the same output for a given input as `fnc_jax`, but doesnt repeat computations confirmed using `jax.make_jaxpr(fnc_jax_alt)(1.0,1.0)`. Is this a feature or a bug?  Another confusion I have is when I do `jax.make_jaxpr(jax.grad(fnc_jax))(1.0,1.0)`:  Question 2: Apart from repeated computations, what confuses me is what exactly is going on here? It seems to me like it is doing something of a forward differentiation. But if I understand correctly from the documentation, `jax.grad` performs reversemode differentiation by default. Am I wrong or misunderstanding the make_jaxpr? If it is forwardmode differentiation, then should we not have something like the following:  If it is reversemode different)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Seemingly odd behaviour with make_jaxpr,"Hello, I am trying to understand exactly how autodiff works with JAX. I have an example function:  Now I do `jax.make_jaxpr(fnc_jax)(1.0,1.0)`, which gives me the following output:  Question 1: Why is it that c, e, and h for example are doing the exact same computation? Wouldn't a better way be if we had something like this:  which gives the same output for a given input as `fnc_jax`, but doesnt repeat computations confirmed using `jax.make_jaxpr(fnc_jax_alt)(1.0,1.0)`. Is this a feature or a bug?  Another confusion I have is when I do `jax.make_jaxpr(jax.grad(fnc_jax))(1.0,1.0)`:  Question 2: Apart from repeated computations, what confuses me is what exactly is going on here? It seems to me like it is doing something of a forward differentiation. But if I understand correctly from the documentation, `jax.grad` performs reversemode differentiation by default. Am I wrong or misunderstanding the make_jaxpr? If it is forwardmode differentiation, then should we not have something like the following:  If it is reversemode different",2023-04-25T15:26:30Z,question,closed,0,6,https://github.com/jax-ml/jax/issues/15736," For your Q1: Eager mode (not jitted) JAX runs `fnc_jax` opbyop, so indeed the computations are duplicated, and `jax.make_jaxpr` only reflects this fact. However, if you jit your function, CSE will kickin, and `divide` only occurred once:   Assuming your functions in JAX will be jitted, you generally don't need to worry about duplicate computation, and you can code it the way that looks cleanest/makes most sense conceptually.","> Question 1: Why is it that c, e, and h for example are doing the exact same computation?  The reason for this is that jaxprs do not produce optimized code, they merely produce an intermediate representation of *the code you wrote*. Because you computed `jnp.divide(x, y)` three times, the computation is represented three times in the jaxpr. Never fear, though: when you JITcompile the code, the compiler recognizes these repeated operations and will deduplicate them, freeing you from having to worry about doing so manually. You can see this by printing the compiled HLO, though it's admittedly harder to read than the jaxpr:   > Question 2: Apart from repeated computations, what confuses me is what exactly is going on here?  `jax.grad` is implemented in terms of reversemode differentiation, so the generated jaxpr reflects the gradient computed in reverse mode. There's no exact forwardmode equivalent of `jax.grad`, but you can get roughly the same thing by computing the jaxpr of `jax.jacfwd`:   The `iota` and `slice` stuff in the beginning comes from the fact that `jacfwd` is more general than `grad`, but the rest reflects the unoptimized forwardmode gradient.",Thanks a lot  and  for the clear explanations! ,"Sorry to reopen, but I have one follow up question: As you suggested, I perform forwardmode autodiff using jacfwd and reverse mode using jacrev.  From what I understand, jacrev should be far more efficient when computing the gradient of a scalar valued function with a large input. However, for a toy computation like the following, I see the opposite! Toy function (really doesn't do anything useful, just made it this way so I can pass arbitrarily large arrays):  Timing it:  Output:   Output:  Is this something to do with the function itself? Or that I am using CPU, which might be struggling with memory transfers? Considering that reversemode uses more memory?","A good rule of thumb is that any time you're writing `for` loops over array elements, you're going to end up with a very inefficient implementation. I'd probably write your function this way, in which case you see the computational characteristics you'd expect.  Maybe something about autodiffing a 1000step unrolled loop makes jacrev less efficient than jacfwd? I'm not sure. In any case, that's a pattern you should avoid when possible.",Thank you for the clarification!
1285,"以下是一个github上的jax下的一个issue, 标题是(slow function/gradient evaluation for complex-valued parameters)， 内容是 ( Description Hello everybody, thank you very much for all your work on jax!  I noticed a performance issue while writing a likelihood function. The issue is possibly a version of CC(matmul slow for complex dtypes). The function looks something like this:  [I attached a selfcontained example at the end] Where data is of shape (nmbEvents, nDim) and dtype is complex128 and T is of length nDim also of dtype complex128. Typical values are nmbEvents ~ 100000; nDim ~ 1000. From my understanding the calculation should be mostly limited by memory bandwidth. I noticed a slow down (for the jitted version of the function) as compared to an implementation using autograd or original numpy (both using openBLAS). The same is true for the gradient/value_and_grad of the function. The slowdown disappears as soon as I switch to float. I found an implementation that is much faster (even than the original numpy implementation) by using complex dtypes only internally:  However, the gradient/value_and_grad of this function is then even slower than th)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,slow function/gradient evaluation for complex-valued parameters," Description Hello everybody, thank you very much for all your work on jax!  I noticed a performance issue while writing a likelihood function. The issue is possibly a version of CC(matmul slow for complex dtypes). The function looks something like this:  [I attached a selfcontained example at the end] Where data is of shape (nmbEvents, nDim) and dtype is complex128 and T is of length nDim also of dtype complex128. Typical values are nmbEvents ~ 100000; nDim ~ 1000. From my understanding the calculation should be mostly limited by memory bandwidth. I noticed a slow down (for the jitted version of the function) as compared to an implementation using autograd or original numpy (both using openBLAS). The same is true for the gradient/value_and_grad of the function. The slowdown disappears as soon as I switch to float. I found an implementation that is much faster (even than the original numpy implementation) by using complex dtypes only internally:  However, the gradient/value_and_grad of this function is then even slower than th",2023-04-25T14:23:10Z,bug,open,0,1,https://github.com/jax-ml/jax/issues/15735,"My results on  Python 3.10.10, Linux 6.2.2, Intel(R) Core(TM) i52520M CPU @ 2.50GHz. JAX is three times slower than the autograd version of value_and_gradient. "
492,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Add support for calling an exported JAX function)， 内容是 (Previously, the only way we could use an Exported object was via tf.XlaCallModule. Here we add support for calling the Exported object directly from JAX, without TF. There is support for custom gradients, and pytrees, but not for shape polymorphism (yet).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Add support for calling an exported JAX function,"Previously, the only way we could use an Exported object was via tf.XlaCallModule. Here we add support for calling the Exported object directly from JAX, without TF. There is support for custom gradients, and pytrees, but not for shape polymorphism (yet).",2023-04-25T11:32:42Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/15733,  PTAL
706,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Fix float0 representation under native serialization)， 内容是 (Previously, jax2tf used int32 zeros for float0 while the JAX native lowering uses bool. The actual type does not make any difference for graph serialization because TF will not use the values. But for native serialization this meant that we passed wrongdtype arguments to the exported modules. This error was masked by the shape refinement logic in XlaCallModule, which set the type of the module formal arguments to be the same as the type of the actual arguments.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Fix float0 representation under native serialization,"Previously, jax2tf used int32 zeros for float0 while the JAX native lowering uses bool. The actual type does not make any difference for graph serialization because TF will not use the values. But for native serialization this meant that we passed wrongdtype arguments to the exported modules. This error was masked by the shape refinement logic in XlaCallModule, which set the type of the module formal arguments to be the same as the type of the actual arguments.",2023-04-25T11:22:22Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/15732, PTAL,  PTAL since Junwhan is OOO
713,"以下是一个github上的jax下的一个issue, 标题是(shard_map gives strange ValueError when called with the wrong number of arguments)， 内容是 ( Description  Results in the error:  The right way to call this function is `f(x, y)`, of course. I expected to see an error message like what I get for supplying the wrong number of arguments to a standard Python function:  (this is what I see for jax.jit decorated functions.)  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,shard_map gives strange ValueError when called with the wrong number of arguments," Description  Results in the error:  The right way to call this function is `f(x, y)`, of course. I expected to see an error message like what I get for supplying the wrong number of arguments to a standard Python function:  (this is what I see for jax.jit decorated functions.)  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-25T03:36:13Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/15728,I hit the same issue.
702,"以下是一个github上的jax下的一个issue, 标题是(Cannot use jit for @classmethod)， 内容是 ( Description I'm trying to `jax.jit` a  but it does not work.  I got `TypeError: Cannot interpret value of type  as an abstract array; it does not have a dtype attribute` If I use `(jax.jit, static_argnums=0)` instead of `.jit`, I got `TypeError: A.zeros() takes 1 positional argument but 2 were given`.  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info python 3.11.3, Ubuntu 18.04  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot use jit for @classmethod," Description I'm trying to `jax.jit` a  but it does not work.  I got `TypeError: Cannot interpret value of type  as an abstract array; it does not have a dtype attribute` If I use `(jax.jit, static_argnums=0)` instead of `.jit`, I got `TypeError: A.zeros() takes 1 positional argument but 2 were given`.  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info python 3.11.3, Ubuntu 18.04  NVIDIA GPU info _No response_",2023-04-25T02:54:07Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/15727,try this ,"Thank you very much! I'm getting into a different but relevant problem:  The first two lines work well, but I got `TypeError: unhashable type: 'ArrayImpl'` for the last line.","For the above example, I somehow find `(jax.jit, static_argnums=0)` works for `A.concatenate`. Why do I have to use different arguments for the two similar functions?","`static_argnums` indicates which arguments to a JITcompiled function should be static. Array properties like shapes and dtypes must be static, while arrays themselves cannot be static. With that in mind, the reason that `static_argnums=(0, 1)` is appropriate for the first function is that argument `1` is a shape (which must be static). The reason that `static_argnums=(0, 1)` is not appropriate for the second function is because argument `1` is an array or list of arrays (which must not be static). For more conceptual background on these topics, you might find this doc useful: https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html"
244,"以下是一个github上的jax下的一个issue, 标题是(Prngkey asarray)， 内容是 (Builds on CC(PRNGKeyArrayImpl: add aval property))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Prngkey asarray,Builds on CC(PRNGKeyArrayImpl: add aval property),2023-04-21T23:53:03Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/15706
277,"以下是一个github上的jax下的一个issue, 标题是(PRNGKeyArrayImpl: add aval property)， 内容是 (This makes it more readily compatible with jax.numpy routines.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,PRNGKeyArrayImpl: add aval property,This makes it more readily compatible with jax.numpy routines.,2023-04-21T23:24:40Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/15705,"I think since `pytype_aval_mappings` uses the attribute now, existing test coverage is sufficient, what do you think?",Sounds good. Merge away!
752,"以下是一个github上的jax下的一个issue, 标题是(Reland: [XLA:Python] Add buffer protocol support to jax.Array)， 内容是 (Reland: [XLA:Python] Add buffer protocol support to jax.Array We supported the buffer protocol on the older DeviceArray class; port that support to jax.Array. The previous attempt was reverted because it led to a C++ CHECK failure if the buffer was deleted while an external Python reference was held. Change the CPU PJRT client to keep the underlying buffer alive as long as there are external references, which is what the contract of Delete() says it will do. Fixes https://github.com/google/jax/issues/14713)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Reland: [XLA:Python] Add buffer protocol support to jax.Array,"Reland: [XLA:Python] Add buffer protocol support to jax.Array We supported the buffer protocol on the older DeviceArray class; port that support to jax.Array. The previous attempt was reverted because it led to a C++ CHECK failure if the buffer was deleted while an external Python reference was held. Change the CPU PJRT client to keep the underlying buffer alive as long as there are external references, which is what the contract of Delete() says it will do. Fixes https://github.com/google/jax/issues/14713",2023-04-21T14:20:40Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15697
286,"以下是一个github上的jax下的一个issue, 标题是(Include the device_kind in the compilation cache key.)， 内容是 (Include the device_kind in the compilation cache key.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Include the device_kind in the compilation cache key.,Include the device_kind in the compilation cache key.,2023-04-20T02:06:21Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15678
905,"以下是一个github上的jax下的一个issue, 标题是(jacfwd fails on a function that returns BCOO)， 内容是 ( Description This code returns a sparse weight vector for linearly interpolating the input point `x` on a onedimensional regular grid:  The primal output does what I expect:  The Jacobian of above should be a sparse vector with entries `[0, 1/delta, 1/delta, 0, 0]`. Indeed, this is what I get if I cast the output to dense:  However, for some reason jacfwd chokes if the function returns a `BCOO`:  Is this just a limitation of the current sparse matrix implementation? Or is there some sort of workaround?  What jax/jaxlib version are you using? jax v0.4.8, jaxlib v0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.10  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jacfwd fails on a function that returns BCOO," Description This code returns a sparse weight vector for linearly interpolating the input point `x` on a onedimensional regular grid:  The primal output does what I expect:  The Jacobian of above should be a sparse vector with entries `[0, 1/delta, 1/delta, 0, 0]`. Indeed, this is what I get if I cast the output to dense:  However, for some reason jacfwd chokes if the function returns a `BCOO`:  Is this just a limitation of the current sparse matrix implementation? Or is there some sort of workaround?  What jax/jaxlib version are you using? jax v0.4.8, jaxlib v0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.10  NVIDIA GPU info _No response_",2023-04-18T18:34:37Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15654,"This is expected, unfortunately. It's not possible to override the semantics of `jax.jacfwd` to work correctly for sparse outputs. This should give you the result you're after:  Note that we have alternative autodiff operator definitions for sparse matrices, e.g. `jax.experimental.sparse.jacfwd`, but they are currently only implemented for sparse inputs, not sparse outputs."
415,"以下是一个github上的jax下的一个issue, 标题是(Import jax.experimental.compilation_cache.compilation_cache by default.)， 内容是 (Import jax.experimental.compilation_cache.compilation_cache by default. This is to fix users who were relying on this module being imported as part of 'import jax'.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Import jax.experimental.compilation_cache.compilation_cache by default.,Import jax.experimental.compilation_cache.compilation_cache by default. This is to fix users who were relying on this module being imported as part of 'import jax'.,2023-04-18T14:49:07Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15651
807,"以下是一个github上的jax下的一个issue, 标题是(Apparent Floating Point Error)， 内容是 ( Description Hello, I have been working on optimizing some routines for some machine learning methods we would like to apply to our work, but I seem to be getting some difference between the original numpy version of this code, and my, likely, semi optimized version in Jax. I attach an example which is reproducible on my machine below. If I am missing something somewhere, I apologize, this is my first pass at using Jax  Thanks, Alex  What jax/jaxlib version are you using? jax v0.4.8  Which accelerator(s) are you using? GPU  Additional system info Python 3.11.3, Ubuntu 22.04  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Apparent Floating Point Error," Description Hello, I have been working on optimizing some routines for some machine learning methods we would like to apply to our work, but I seem to be getting some difference between the original numpy version of this code, and my, likely, semi optimized version in Jax. I attach an example which is reproducible on my machine below. If I am missing something somewhere, I apologize, this is my first pass at using Jax  Thanks, Alex  What jax/jaxlib version are you using? jax v0.4.8  Which accelerator(s) are you using? GPU  Additional system info Python 3.11.3, Ubuntu 22.04  NVIDIA GPU info ",2023-04-18T02:34:09Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/15646,"Hi  thanks for the question. It's important here to look at not just the difference, but also the relative difference:  Your numpy and JAX results differ by about 1 part in $10^6$. You're using float32 values, and float32 has a 23bit mantissa, meaning that single operations will normally have errors on order of 1 part in $2^{23}$, which is roughly 1 part in $10^7$. Since your operation involves accumulation, it will consiste of multiple such operations, which makes it look like your output is consistent with the normal roundoff error of floating point operations. What do you think?","That makes sense. I assumed that it was something to do with precision and rounding of the values, given the very suspicious difference of 23552. Just wanted to make sure I wasn't experiencing any unintended behaviour. Thanks!","Thanks  another note: if your computation requires more accuracy, you could enable 64bit precision and then you'll have several orders of magnitude more precision to work with."
704,"以下是一个github上的jax下的一个issue, 标题是(Include the return path when reporting an overflow on a `jit` function output)， 内容是 (Returning an overflowing integer from a `jit`ed function leads to a very cryptic error:  It would be much better if there were a way we could report the return value path. There is of course one minor difficulty: we don't know the return value path until after tracing completes! Any ideas? The best idea I have would be to return some kind of error tracer, which might allow us to delay reporting the error until after the output pytree is known.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Include the return path when reporting an overflow on a `jit` function output,"Returning an overflowing integer from a `jit`ed function leads to a very cryptic error:  It would be much better if there were a way we could report the return value path. There is of course one minor difficulty: we don't know the return value path until after tracing completes! Any ideas? The best idea I have would be to return some kind of error tracer, which might allow us to delay reporting the error until after the output pytree is known.",2023-04-17T17:29:24Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/15631
1330,"以下是一个github上的jax下的一个issue, 标题是(NotImplementedError: Call to scatter_(update/add/multiply/min/max) cannot be converted with enable_xla=False)， 内容是 ( Description I've got a JAX function that I'm trying to fuse with a Keras model and convert everything to TFLite. To my understanding the best way to do that is to convert my function to a TensorFlow Concrete Function, merge it with my Keras model to another Concrete Function, and eventually convert it to TFlite using `TFLiteConverter.from_concrete_functions`. Problem is, getting a Concrete Function from my JAX function fails with the errors below. The weird thing is that converting this function to TFLite directly with `TFLiteConverter.experimental_from_jax` works! I assume it has something to do with conversion directly to HLO (reference), but still, is it possible to get the same behavior for `from_concrete_functions`?  Note: if it disable XLA (`enable_xla=True`) the conversion seemingly works but the final model I'm getting from `from_concrete_functions` has an empty graph. Thank you.  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7  Which accelerator(s) are you using? CPU  Additional system info WSL2 over )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,NotImplementedError: Call to scatter_(update/add/multiply/min/max) cannot be converted with enable_xla=False," Description I've got a JAX function that I'm trying to fuse with a Keras model and convert everything to TFLite. To my understanding the best way to do that is to convert my function to a TensorFlow Concrete Function, merge it with my Keras model to another Concrete Function, and eventually convert it to TFlite using `TFLiteConverter.from_concrete_functions`. Problem is, getting a Concrete Function from my JAX function fails with the errors below. The weird thing is that converting this function to TFLite directly with `TFLiteConverter.experimental_from_jax` works! I assume it has something to do with conversion directly to HLO (reference), but still, is it possible to get the same behavior for `from_concrete_functions`?  Note: if it disable XLA (`enable_xla=True`) the conversion seemingly works but the final model I'm getting from `from_concrete_functions` has an empty graph. Thank you.  What jax/jaxlib version are you using? jax==0.4.8 jaxlib==0.4.7  Which accelerator(s) are you using? CPU  Additional system info WSL2 over ",2023-04-16T22:16:50Z,enhancement,open,0,6,https://github.com/jax-ml/jax/issues/15627, Is this bug still current?,"Yes, still current with no apparent way overcoming it. I'll be happy to contribute a fix if you'll be able to guide me through.", to see if these is a fix.,"  Hi, I got back to this today and found out that `experimental_from_jax` is being deprecated and `jax2tf.convert` is the way to go now, which means I don't have a lot of time until I'll have to migrate. Can I help in some way with this issue?", What is the status of TFLite support for StableHLO? I would like to deprecate the `enable_xla=False` path altogether.,"Hi  ,  TFLite has migrated to use native serialization as the default.  We can simply save as as TFSavedModel via `tf.saved_model.save`  containing StableHLO ops as the intermediate format between JAX and TFLite. Then TFLite converter API will pick that up  `converter = tf.lite.TFLiteConverter.from_saved_model('/some/directory') tflite_model = converter.convert()`. We will update our public documentation pretty soon."
682,"以下是一个github上的jax下的一个issue, 标题是(``JAX`` conflicts with ``PyTorch`` at import time)， 内容是 ( Description When importing ``jax`` and ``pytorch``, namely  I get the following error  I thought it might be an error linked to the version of ``libstdc++``, but it appears to be not the case. Indeed, the problem disappears when swapping the import order of them.  What jax/jaxlib version are you using? jax v`0.3.25`, jaxlib v`0.3.22`  Which accelerator(s) are you using? GPU  Additional system info Python 3.7, OS Linux Ubuntu 64bit  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",glm,``JAX`` conflicts with ``PyTorch`` at import time," Description When importing ``jax`` and ``pytorch``, namely  I get the following error  I thought it might be an error linked to the version of ``libstdc++``, but it appears to be not the case. Indeed, the problem disappears when swapping the import order of them.  What jax/jaxlib version are you using? jax v`0.3.25`, jaxlib v`0.3.22`  Which accelerator(s) are you using? GPU  Additional system info Python 3.7, OS Linux Ubuntu 64bit  NVIDIA GPU info ",2023-04-16T10:54:21Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15624,"This sort of thing may happen if you mix packages installed by anaconda with packages installed via `pip`. One possible workaround: it might work to import JAX first, before `torch`. The other suggestion I have is: install both JAX and Torch from anaconda (condaforge for JAX, see the README), or install both from `pip`.",I would stick with inversing the order of imports as the conflict between both packages persists despite installing them from `conda`. Thanks for your response!
954,"以下是一个github上的jax下的一个issue, 标题是(Provide access to keepalives when calling jax.jit)， 内容是 (Custom Calls return a pointer of the serialized descriptor to jax.jit, and the jax.jitted function saves this pointer as an internal state, giving this pointer the same lifetime as the function. When calling jax.jit to get the lowered IR, we lose the keepalives list of pointers. Unless we store the entire jax.jitted function, the custom call pointers get released, giving a segfault when trying to access the descriptor through the (now released) pointer. Storing the entire jax.jitted function becomes a problem when the model is big, as we have to store this memory when all we really need is the keepalives list. A good solution would be to store the keepalives in the jax.stages.Lowered object, to which we have access.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Provide access to keepalives when calling jax.jit,"Custom Calls return a pointer of the serialized descriptor to jax.jit, and the jax.jitted function saves this pointer as an internal state, giving this pointer the same lifetime as the function. When calling jax.jit to get the lowered IR, we lose the keepalives list of pointers. Unless we store the entire jax.jitted function, the custom call pointers get released, giving a segfault when trying to access the descriptor through the (now released) pointer. Storing the entire jax.jitted function becomes a problem when the model is big, as we have to store this memory when all we really need is the keepalives list. A good solution would be to store the keepalives in the jax.stages.Lowered object, to which we have access.",2023-04-11T09:20:26Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/15536
222,"以下是一个github上的jax下的一个issue, 标题是(fix typo in functions.py)， 内容是 (Indicies > Indices)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fix typo in functions.py,Indicies > Indices,2023-04-11T02:34:08Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/15531,Thank you!
707,"以下是一个github上的jax下的一个issue, 标题是(pip installation: GPU (CUDA, installed via pip) not working for me with Brax)， 内容是 ( Description pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html jax installation using the command above is not working with the latest Brax: https://github.com/google/brax For some reason it complains on runtime CuDNN version:   What jax/jaxlib version are you using? jax 0.4.8  Which accelerator(s) are you using? RTX 4090  Additional system info Python 3.9 Ubuntu 22.04  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,"pip installation: GPU (CUDA, installed via pip) not working for me with Brax"," Description pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html jax installation using the command above is not working with the latest Brax: https://github.com/google/brax For some reason it complains on runtime CuDNN version:   What jax/jaxlib version are you using? jax 0.4.8  Which accelerator(s) are you using? RTX 4090  Additional system info Python 3.9 Ubuntu 22.04  NVIDIA GPU info ++  ++",2023-04-10T05:35:51Z,bug,closed,2,3,https://github.com/jax-ml/jax/issues/15508,"Well, the message `Loaded runtime CuDNN library: 8.3.2` tells you the problem: JAX was trying to load CuDNN, but found a really old version (8.3) when it tried. Usually this means that you have already loaded an older CuDNN into your process (e.g., importing PyTorch before importing JAX is one common way this can happen, since PyTorch usually bundles an older CuDNN). The other way it can happen is that an older CuDNN is first in your `LD_LIBRARY_PATH`, but I think that's unlikely to be the case here since you used the `pip` installation of JAX. Try searching for other CuDNN installations (files name `*cudnn*`) on your system? This is probably not something we can fix from the JAX end.","same issue, it seems because the current version of pytorch does not support cudnn8.8 or higher version"," You might do well to install a CPUonly version of PyTorch, if the goal is to use that in the context of a GPUusing JAX program."
533,"以下是一个github上的jax下的一个issue, 标题是(Support static argnames/argnums to checkify)， 内容是 (I tried using checkify today but it's not compatible with my code base because I'm relying on static argnames/argnums for some of the jitted functions. After wrapping the jitted functions with checkify, those inputs are misinterpreted as dynamic arguments and JAX raises an error because they aren't JAX types.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Support static argnames/argnums to checkify,"I tried using checkify today but it's not compatible with my code base because I'm relying on static argnames/argnums for some of the jitted functions. After wrapping the jitted functions with checkify, those inputs are misinterpreted as dynamic arguments and JAX raises an error because they aren't JAX types.",2023-04-10T01:08:16Z,enhancement,closed,0,7,https://github.com/jax-ml/jax/issues/15504,"Thanks for raising this! > those inputs are misinterpreted as dynamic arguments Well, `static_argnums`/`static_argnames` only affects the particular `jit` call to which those arguments are passed. In particular, they don't change the meaning of the body. So if I understand correctly, this issue isn't checkifyspecific: for any `f` where `jit(checkify(f), static_argnums=(0,))` would fail with the error you describe, I believe `jit(jit(f), static_argnums=(0,))` would fail in the same way. In both cases, the function `g` to which the `jit(g, static_argnums=(0,))` is applied only accepts pytrees of jax types (ie arrays) as arguments. So, there are (at least) two possible problem statements here, and I just want to figure out which one you have in mind: 1. make both `jit(jit(f), static_argnums=(0,))` and `jit(checkify(f), static_argnums=(0,))` (and `scan` etc) Just Work, by changing the meaning of the outer `jit`'s `static_argnums` to affect the body as well; or 2. change `checkify` only so that this issue doesn't arise (e.g. maybe `checkify` should try to avoid assuming  _any_ inputs are jaxtypes, independent of any caller's `static_argnums`). Did you have one of these in mind?", ,"I was thinking of your option 2, changing `checkify` to avoid assuming that all its inputs must be jaxtypes. More generally, there could be a solution along the lines of this? ","Actually, this line isnt quite right:  `jit`'s `static_argnums`/`static_argnames` can't be implemented in user code (i.e. on top of a `jax.jit` API which doesn't itself have those options). If you try using the solution quoted here, you'll always get retraces/recompiles between two applications of `fn` even when passing the same value for `string_arg`, because you're creating a fresh callable object (the `partial` instance) every time. That's why `jit` has `static_argnums`/`static_argnames` built in: not as a convenience but because it's the only way to get the caching behavior we want. This kind of caching approach can work better with `checkify`, though I'm a bit certain because it now operates differently from e.g. `jax.grad` (basically it has some internal caching which makes it a bit different). I'd have to think about it. I guess a third option is: 3. add `static_argnums` / `static_argnames` to `checkify`, perhaps just as a convenience (though it's not as convenient as it Just Working). I'd rather make it Just Work if we can!",I don't quite follow. The `functools.partial()` in my call only happens when the specialization value isn't already in the cache dictionary. So it would only retrace once per unique value for `string_arg`.,"Sorry, I misread your code. (Actually I think it's a bit buggy in that you want to curry the `wrapped` fun on extra level and not get both `fn` and `args`/`kwargs` at the same time.) You're right that you can get cache hits for equal values with this approach, so that part of my comment was mistaken. But now you have separate jit caches (so e.g. we can't get the same eviction logic we would get from having a single cache). In general my only point was just that `with_static(jit, 'string_arg')(fn)` isn't _exactly_ the same as `jit(fn, static_argnames='string_arg')`. Luckily, CC(Checkify: close over all arguments.) will solve the issue by making things Just Work! With that `checkify` will no longer place any constraints on the arguments passed to the `checkify`decorated function. (Internally all the arguments are just closed over; we checked that the caching issues I was concerned about, basically the things CC(checkify: cache jaxpr formation so we don't always retrace) and CC(Fix checkify caching with nested call primitives) were fixing, no longer apply thanks to 6ec9082.)","Awesome! Thanks also for the explanation, that makes sense."
1283,"以下是一个github上的jax下的一个issue, 标题是(Bias initialization based on the input dimension of the layer)， 内容是 ( Description Hey,  I was trying to initialize the biases of my layers according the way it's done in PyTorch's default way of doing so, for linear layers:  but I realized it's almost impossible to do so in a clean way using jax. As in an initializer, I only have access to the shape of the parameter being initialized, which for bias translates to the output dimension of a layer. I imagine this was intended (https://github.com/google/jax/issues/2075issuecomment578465814, https://github.com/google/flax/issues/2749) but I also imagine this could be annoying for a lot of people like me, who would like to initialize their biases based on the input dimension in some way (let's say to numerically test some theoretical analysis). Is there any recommended way of initializing the biases like this? If not, is this supposed to be supported in the future at all? Thanks!  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Bias initialization based on the input dimension of the layer," Description Hey,  I was trying to initialize the biases of my layers according the way it's done in PyTorch's default way of doing so, for linear layers:  but I realized it's almost impossible to do so in a clean way using jax. As in an initializer, I only have access to the shape of the parameter being initialized, which for bias translates to the output dimension of a layer. I imagine this was intended (https://github.com/google/jax/issues/2075issuecomment578465814, https://github.com/google/flax/issues/2749) but I also imagine this could be annoying for a lot of people like me, who would like to initialize their biases based on the input dimension in some way (let's say to numerically test some theoretical analysis). Is there any recommended way of initializing the biases like this? If not, is this supposed to be supported in the future at all? Thanks!  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-09T22:16:45Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15501,"I think this is probably more of a flax issue, than jax. I'm closing this and have opened the same issue in the flax repo: https://github.com/google/flax/issues/3019"
382,"以下是一个github上的jax下的一个issue, 标题是([shard-map] better rep-rule-not-implemented error)， 内容是 (Copy of CC([shardmap] better reprulenotimplemented error) and CC([shardmap] better reprulenotimplemented error), trying to trick copybara into working...)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard-map] better rep-rule-not-implemented error,"Copy of CC([shardmap] better reprulenotimplemented error) and CC([shardmap] better reprulenotimplemented error), trying to trick copybara into working...",2023-04-09T04:29:46Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15495
334,"以下是一个github上的jax下的一个issue, 标题是([shard-map] better rep-rule-not-implemented error)， 内容是 (Copy of CC([shardmap] better reprulenotimplemented error), trying to trick copybara into doing its job...)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard-map] better rep-rule-not-implemented error,"Copy of CC([shardmap] better reprulenotimplemented error), trying to trick copybara into doing its job...",2023-04-09T03:59:35Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15494
811,"以下是一个github上的jax下的一个issue, 标题是(Unexpected behavior when multiplying Jax boolean with jnp.inf)， 内容是 ( Description Hello! I recently was struggling to find a bug in my code, when I realized the problem came from some weird behavior from Jax. Below, note the inconsistency in evaluation when mutliplying `jnp.array(False)` with arrays of length >1:  It seems like there is a low probability that this behavior is intentional, so I decided to open this issue. Thanks in advance for any help!  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.12, using Ubuntu on WSL2  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unexpected behavior when multiplying Jax boolean with jnp.inf," Description Hello! I recently was struggling to find a bug in my code, when I realized the problem came from some weird behavior from Jax. Below, note the inconsistency in evaluation when mutliplying `jnp.array(False)` with arrays of length >1:  It seems like there is a low probability that this behavior is intentional, so I decided to open this issue. Thanks in advance for any help!  What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.12, using Ubuntu on WSL2  NVIDIA GPU info ",2023-04-08T18:02:54Z,bug XLA,closed,0,4,https://github.com/jax-ml/jax/issues/15492,I actually find it super weird that `jnp.array(False) * jnp.inf` gives 0.,This looks like an incorrect optimization in XLA: ,Possible duplicate: CC(Multiplying Nan by False give 0. instead of NaN),Closing as duplicate of CC(Multiplying Nan by False give 0. instead of NaN)
918,"以下是一个github上的jax下的一个issue, 标题是(Is it possible to implement Betainc gradient with respect to a and b?)， 内容是 (I was trying to sample from `pymc` package using `numpyro` backend (which in turns use `jax`) and got the following error  which pointed to this line of code After searching a bit there seems to be a way to calculate this value from this paper in section 3.1 on equation 14, and 29. However the math is a bit over my head   For Generalized Hypergeometric Function 2F1 can be found in `scipy.special.hyp2f1`, and 2F1 & 3F2 implementation can be found in `mpmath` package, and Incomplete Beta Function is  $\psi(x)$ is the digamma function i.e.   [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Is it possible to implement Betainc gradient with respect to a and b?,"I was trying to sample from `pymc` package using `numpyro` backend (which in turns use `jax`) and got the following error  which pointed to this line of code After searching a bit there seems to be a way to calculate this value from this paper in section 3.1 on equation 14, and 29. However the math is a bit over my head   For Generalized Hypergeometric Function 2F1 can be found in `scipy.special.hyp2f1`, and 2F1 & 3F2 implementation can be found in `mpmath` package, and Incomplete Beta Function is  $\psi(x)$ is the digamma function i.e.   [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example.",2023-04-08T14:21:20Z,enhancement,open,3,2,https://github.com/jax-ml/jax/issues/15487,"I think I can work on this. Actually, I love working on implementations of challenging mathematical functions that are optimized for modern hardware like GPU and TPU. Last year, I made some contributions to TensorFlow Probability in this regard:  Partial derivatives of the regularized incomplete beta function with respect to `a` and `b` [link]  Inverse of the regularized incomplete beta function and its partial derivatives [link]  Improved CDF and quantile functions of the Student's Tdistribution, respectively `stdtr` and `stdtrit` (more accurate than the corresponding SciPy implementation) [link] You can use TensorFlow Probability to compute the partial derivatives of `betainc` with respect to all its parameters:  This produces:  : Considering the cost of maintaining this implementation, do you think it's worth writing a specific JAX implementation of the partial derivatives of `betainc`? If so, could you provide some examples of optimized custom gradient implementations that are well documented and tested (and that can help me write idiomatic JAX code)?","Just adding to the usefulness of this: The derivatives of the incomplete Beta are necessary to compute the gradient of the CDF of the Beta, which in turn is the canonical way to differentiate through the Beta , as per the ""Implicit reparametrization gradients"" paper: https://proceedings.neurips.cc/paper_files/paper/2018/hash/92c8c96e4c37100777c7190b76d28233Abstract.html"
652,"以下是一个github上的jax下的一个issue, 标题是(Deprecation warning when parsing nightly packages)， 内容是 ( Description When running `pip` with `f https://storage.googleapis.com/jaxreleases/jaxlib_nightly_releases.html` to install nightlies, I'm getting this deprecation warning:  I'm using pip 22.0.2. Probably best to fix whatever it is angry about soon.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Deprecation warning when parsing nightly packages," Description When running `pip` with `f https://storage.googleapis.com/jaxreleases/jaxlib_nightly_releases.html` to install nightlies, I'm getting this deprecation warning:  I'm using pip 22.0.2. Probably best to fix whatever it is angry about soon.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-07T22:17:02Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15476,"Thanks for the report – this looks like a duplicate of CC(Please provide PEP 503 compliant indices for CUDA versions of packages), but is still something we need to address.",Closing in favor of https://github.com/google/jax/issues/5410
696,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: module 'jax' has no attribute 'Array')， 内容是 ( Description Use this notebook from https://github.com/kingoflolz/meshtransformerjax.git to reproduce the issue.  What jax/jaxlib version are you using? flax0.6.9 jax0.4.8 optax0.1.5.dev0 chex0.1.7  Which accelerator(s) are you using? cloud TPU  Additional system info python 3.8.10  NVIDIA GPU info I have installed the git development version of `flax`, `jax` , `optax` and `chex`, but I am still getting `AttributeError: module 'jax' has no attribute 'Array'` )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,AttributeError: module 'jax' has no attribute 'Array'," Description Use this notebook from https://github.com/kingoflolz/meshtransformerjax.git to reproduce the issue.  What jax/jaxlib version are you using? flax0.6.9 jax0.4.8 optax0.1.5.dev0 chex0.1.7  Which accelerator(s) are you using? cloud TPU  Additional system info python 3.8.10  NVIDIA GPU info I have installed the git development version of `flax`, `jax` , `optax` and `chex`, but I am still getting `AttributeError: module 'jax' has no attribute 'Array'` ",2023-04-07T17:05:08Z,bug,closed,0,5,https://github.com/jax-ml/jax/issues/15466,I think the issue is that that notebook downgrades jax:  That would explain why `jax.Array` does not exist.,"I had retried without the following lines, but still the same error ",Show the output of `pip list` ?,Here you go: ,"I just killed and restarted the jupyter kernel, it seems to work for now given that it now points to some other different error.  "
1262,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Refactor and simplify jax2tf.py)， 内容是 (This is the next step in the refactoring and simplifying jax2tf. An additional goal is to move to jax_export.py as much pureJAX functionalit as possible so that jax_export can start to become a standalone API for native serialization. Previous PRs in this series:    CC([shape_poly] Refactor the computation of the dimension variables in native serialization) and CC([shape_poly] Improved computation of dimension variables for native serialization): refactoring computation of dimension variables    CC([jax2tf] Cleanup of handling of tf.custom_gradient) and CC([jax2tf] Refactor the gradient machinery for native serialization): refactor handling of tf.custom_gradient (moved VJP serialization to jax_export) Here we do the following:   * we move the handling of in/out pytrees to jax_export. This allows the removal of a workaround that introduced a trampoline for the `lower` function for native lowering.   * we move most of the code to set up the input abstract values into jax_export (`poly_spec` and `poly_specs`)   * we separate out )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[jax2tf] Refactor and simplify jax2tf.py,This is the next step in the refactoring and simplifying jax2tf. An additional goal is to move to jax_export.py as much pureJAX functionalit as possible so that jax_export can start to become a standalone API for native serialization. Previous PRs in this series:    CC([shape_poly] Refactor the computation of the dimension variables in native serialization) and CC([shape_poly] Improved computation of dimension variables for native serialization): refactoring computation of dimension variables    CC([jax2tf] Cleanup of handling of tf.custom_gradient) and CC([jax2tf] Refactor the gradient machinery for native serialization): refactor handling of tf.custom_gradient (moved VJP serialization to jax_export) Here we do the following:   * we move the handling of in/out pytrees to jax_export. This allows the removal of a workaround that introduced a trampoline for the `lower` function for native lowering.   * we move most of the code to set up the input abstract values into jax_export (`poly_spec` and `poly_specs`)   * we separate out ,2023-04-07T14:22:18Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/15457,"  PTAL. This is a pretty big refactoring. I will land this when I return from vacation, after April 24th."
603,"以下是一个github上的jax下的一个issue, 标题是(Add missing exports to jax.config and jax.debug)， 内容是 ( Description Add missing exports to jax.config and jax.debug (and any other such files). See CC(Consider defining public API explicitly) for context. Example:  mypy yields  pyright yields   What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.2, macOS 11.7.4  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add missing exports to jax.config and jax.debug," Description Add missing exports to jax.config and jax.debug (and any other such files). See CC(Consider defining public API explicitly) for context. Example:  mypy yields  pyright yields   What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Python 3.11.2, macOS 11.7.4  NVIDIA GPU info _No response_",2023-04-07T03:01:01Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/15453,">  I think this is not meant to be a supported import. This is the supported way to get the same thing:  (yes, it's strange that we rewrite the `config` module with the `config` object: we need to clean that up at some point) >  We should fix these. `breakpoint` is easy (we just need to add `as breakpoint`), but I'm not sure how to tell `mypy` that `callback` and `print` are meant to be exported, because they are renamed on import: https://github.com/google/jax/blob/87a1fea1c71b9b059b286c86bda4942cea766e3a/jax/debug.pyL14L20 Do you know how to tell mypy that the renamed  imports are meant as exported names?","  > I think this is not meant to be a supported import. In that case, should the docs be updated to reflect this? For example, here it currently says: > You can manually set the jax_enable_x64 configuration flag at startup: >  > Do you know how to tell mypy that the renamed imports are meant as exported names? Unfortunately, I do not.","Yeah, that should probably be updated.", I'm also getting an error for `jax.distributed.initialize`:  Should I open a new issue?
1294,"以下是一个github上的jax下的一个issue, 标题是(JEP: Allow finite difference method for non-jax differentiable functions)， 内容是 ( Motivation: Make JAX relevant to many more users JAX provides a powerful generalpurpose tool for automatic differentiation, but it usually requires that users write code that is JAXtraceable endtoend.  Significant numbers of scientific and industrial applications involve large, legacy codebases where the lift to transfer the system to endtoend JAX is prohibitively high. In other cases, users are tied into proprietary software, or the underlying software is not written in python, and also find themselves unable to readily convert the underlying code to JAX. Without JAX, the standard method for performing optimization involves computing derivatives using _finite difference methods_. While these can be integrated into JAX using custom functions, the process is cumbersome, which significantly limits the set of users able to integrate JAX into their work. This JEP proposes a simple method for computing numerical derivatives in JAX. I expect that this change would expand the potential user base of JAX substantially, and could drive)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JEP: Allow finite difference method for non-jax differentiable functions," Motivation: Make JAX relevant to many more users JAX provides a powerful generalpurpose tool for automatic differentiation, but it usually requires that users write code that is JAXtraceable endtoend.  Significant numbers of scientific and industrial applications involve large, legacy codebases where the lift to transfer the system to endtoend JAX is prohibitively high. In other cases, users are tied into proprietary software, or the underlying software is not written in python, and also find themselves unable to readily convert the underlying code to JAX. Without JAX, the standard method for performing optimization involves computing derivatives using _finite difference methods_. While these can be integrated into JAX using custom functions, the process is cumbersome, which significantly limits the set of users able to integrate JAX into their work. This JEP proposes a simple method for computing numerical derivatives in JAX. I expect that this change would expand the potential user base of JAX substantially, and could drive",2023-04-06T00:41:14Z,enhancement,open,2,12,https://github.com/jax-ml/jax/issues/15425,"Thanks for the proposal! We've discussed something along these lines amongst ourselves at various points. It's a good idea! Do you have an idea for the API you'd propose? When this has come up before, our thought was to make it a convenience wrapper around `jax.custom_jvp`. That suggests an implementation using `custom_jvp` as well.","I actually implemented it using fullon custom primitives following this guide. Since it seems like there's interest, I'll make the PR."," any reason not to use a `custom_jvp` or `custom_vjp`? Those are a lot simpler than a custom primitive, and I think it'd be the preferred approach unless I'm missing something.",In particular `custom_jvp` would support both forward and reversemode AD.,"Hello, for the context of API design, I have a small WIP library for finite difference, I implemented a finite difference version of `jax.grad` with couple of options colab ",As far as I understand we can also use `fdx.fgrad` with `custom_jvp` and `pure_callback`  to make nontraceable code work with `jax` transformations. ,"> As far as I understand  So understated! Seems like you've written a package that does most of the work here. :) Given this, is an implementation of finitedifferences in the core JAX package (e.g., under `jax.experimental`) desired? I'm inclined to think that this would be a very valuable addition to the JAX ecosystem, either here or some sister project, because of how useful I'm finding the tool when using third party scientific libraries. FYI, I'm happy to collaborate on a PR  if that's of interest. I'll try to get something up in the next day or so for more commentI've had some busy days in the last week.  I'll look into using `custom_vjp` and `custom_jvp`—I recall having a reason I didn't use those to start with, but it's possible that I just missed something.","To be concrete about the API , here are some guiding principles that I have in mind: 0. A reasonable name. Personally, I think a decorator named `jaxify` is appropriate because it gets at ""what it means for most users"", that is, make their existing code work with JAX. Other names like `jax_finite_difference` are also OK, but they are based on implementation and are a bit more technical. 1. Oneline decorator that ""just works"" for 80% of cases, and provides sensible error messages for the most common issues (e.g., not abiding by the API). 2. More powerful options for ~1519% of the remaining cases. For example:      Step size control      Mode choice (e.g., central, forward, and backward modes)      Allowing different steps sizes or modes per argument. 4. Reasonably efficient, so that, e.g., forward mode uses significantly fewer function evaluations than central mode. 5. Consistent with the existing API. I like the idea that it has the same requirements as `pure_callback` (as  helpfully suggested). 6. It might be nice to warn users when the use of this decorator has kept them from having full hardware acceleration, though that may be best left as a feature for `pure_callback`. (Not sure if this is easy or hard.) 7. Welldocumented, e.g., a notebook with documentation for use.  API Examples  The ""80%"" use case Most use cases should start with a simple decorator:   Power use cases ", – Why bundle together (a) setting up a derivative rule based on FD with (b) setting up a `pure_callback` call?,"There are two reasons I can think of to support FD for the same class of functions that `pure_callback` supports: equivalence and consistency. (Note that I'm suggesting that we use the same API for the functions, nothing more.) **Equivalence:** The class of functions theoretically supportable by a generic FD technique are precisely those that are theoretically supportable by a `pure_callback` mechansim (that is, pure functions that accept and return numpy arrays).  Think about it: almost by definition, the functions we'd want to finitedifference are not supported directly within JAX's JIT, so computing their values during a generic finitedifference routine requires the use of some mechanism like `pure_callback`. If we manage to write an FD routine that supports more functions than `pure_callback`, we could backport the functionality to `pure_callback` using, for example, the `value` part of `jax.value_and_grad`. Conversely, it's pretty easy to see that—at least in principle—we can write a wrapper that will apply finite differences to pure functions that accept and return numpy arrays. That's, of course, the challenge I've set out for myself here. **Consistency:** Given the close link between the sets of functions supportable by `pure_callback` and those supportable by a generic finite difference routine, we should probably just insist that they _are_ the same and test for it. This will allow us to leverage `pure_callback` and keep the overall API complexity roughly constant.  Example documentation  Note: edited for clarity and added an example docstring.","I'd also be very interested in helping with this. I've done a similar thing using `pure_callback` and `custom_jvp` but I haven't been able to get it to support both forward and reverse mode. If I define a `custom_jvp` for the `pure_callback`, forward mode stuff like `jacfwd` works fine. However, if I try to use `jacrev` I usually get an error   Using `custom_vjp` allows reverse mode to work, but since a function can't have both `custom_jvp` and `custom_vjp` defined ( CC(Defining both custom_vjp and custom_jvp)), we're limited to one or the other.","  I added a new functionality, `define_fdjvp` here, which satisfies most of the specs you defined. One note, forward, central and backward are replaced by offsets, so for $`\sum A_i*f(x+a_i)/stepsize`$, you only provide `offsets=jnp.array([ai, ... ])` and $`A_i`$  will be calculated based on the other configs."
577,"以下是一个github上的jax下的一个issue, 标题是([4/n] Introduce Module into FunctionDefLibrary.)， 内容是 ([4/n] Introduce Module into FunctionDefLibrary. Retrieve the serialized StableHLO module from FunctionLibraryRuntime. If XlaCallModuleOp has attribute ""module"", it means the StableHLO module is serialized and embedded in the attribute. Otherwise, it must have attribute ""module_name"", which refers to a StableHLO module in the FunctionLibraryRuntime.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[4/n] Introduce Module into FunctionDefLibrary.,"[4/n] Introduce Module into FunctionDefLibrary. Retrieve the serialized StableHLO module from FunctionLibraryRuntime. If XlaCallModuleOp has attribute ""module"", it means the StableHLO module is serialized and embedded in the attribute. Otherwise, it must have attribute ""module_name"", which refers to a StableHLO module in the FunctionLibraryRuntime.",2023-04-05T23:50:01Z,,closed,0,1,https://github.com/jax-ml/jax/issues/15421,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
715,"以下是一个github上的jax下的一个issue, 标题是([3/n] Introduce Module into FunctionDefLibrary.)， 内容是 ([3/n] Introduce Module into FunctionDefLibrary. Add an option `native_serialization_as_module` to jax2tf to serialize the stablehlo explicitly to `FunctionDefLibrary`. In jax2tf native serialization, the stablehlo is embedded as a string attribute of the `XlaCallModuleOp`. When `native_serialization_as_module` is `True`, the stablehlo is serialized as a `ModuleDef` in `FunctionDefLibrary`, then `XlaCallModuleOp` refers to the `ModuleDef` using the module's name as a string attribute.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[3/n] Introduce Module into FunctionDefLibrary.,"[3/n] Introduce Module into FunctionDefLibrary. Add an option `native_serialization_as_module` to jax2tf to serialize the stablehlo explicitly to `FunctionDefLibrary`. In jax2tf native serialization, the stablehlo is embedded as a string attribute of the `XlaCallModuleOp`. When `native_serialization_as_module` is `True`, the stablehlo is serialized as a `ModuleDef` in `FunctionDefLibrary`, then `XlaCallModuleOp` refers to the `ModuleDef` using the module's name as a string attribute.",2023-04-05T23:43:12Z,,closed,0,1,https://github.com/jax-ml/jax/issues/15420,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
1258,"以下是一个github上的jax下的一个issue, 标题是(Taking gradients through jnp.arctan2 seems to be very unstable)， 内容是 ( Description I'm trying to optimise a model and I'm finding that the gradients are very very unstable  no learning rate lets me reliably move in the right direction to reduce the loss.  I have narrowed down the instability to the line where I call `jnp.arctan2`  to calculate angles. I have included below a minimum working example  which demonstrates the issue.  This may well be a misunderstanding on my behalf of the behaviour of `jnp.arctan2`.  If I run this code as is, then the first 10 iterations descend as expected (see below)  But if I change the return value to include the conversion to angle, then the gradients are very unstable and the output fluctuates wildly  Reducing the learning rate to `1e7` of course minimises the size of the fluctuations, but the directions still vary from iteration to iteration, see below   What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8, Ubuntu 20.04.5 LTS in WSL2  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Taking gradients through jnp.arctan2 seems to be very unstable," Description I'm trying to optimise a model and I'm finding that the gradients are very very unstable  no learning rate lets me reliably move in the right direction to reduce the loss.  I have narrowed down the instability to the line where I call `jnp.arctan2`  to calculate angles. I have included below a minimum working example  which demonstrates the issue.  This may well be a misunderstanding on my behalf of the behaviour of `jnp.arctan2`.  If I run this code as is, then the first 10 iterations descend as expected (see below)  But if I change the return value to include the conversion to angle, then the gradients are very unstable and the output fluctuates wildly  Reducing the learning rate to `1e7` of course minimises the size of the fluctuations, but the directions still vary from iteration to iteration, see below   What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8, Ubuntu 20.04.5 LTS in WSL2  NVIDIA GPU info _No response_",2023-04-05T11:35:55Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15407,"With some further investigation on this minimal example, it seems that there is a sweet spot of learning rate that I had initially missed where the optimisation is stable, so I'll close this issue.  Not sure how to find that sweet spot for the full model but thats the next problem!"
1264,"以下是一个github上的jax下的一个issue, 标题是([shard-map] debugging shmap + prngkey bugs)， 内容是 (fixes CC(jax.random sampling doesn't work inside shard_map) There are a few changes here, since we ran into some things that we wanted to clean up: 1. the `shard_map`related change was that anything lowering from jaxprlevel logical avals their corresponding shardings to HLOlevel (""physical"") types and corresponding HLO OpSharding protos needs to be sure to actually handle the logicaltophysical translation on the shardings, but shmap wasn't doing that, so the changes to `_xla_shard` and `_xla_unshard` handle that; 2. we also didn't have a rep rule for some PRNGrelated primitives, so the `prng.__dict__.values()` change handles that; 3. `PRNGKeyArray` was delegating its `sharding` attribute to the underlying `_base_array.sharding`, but the latter has an incompatible rank (i.e. length of `tile_assignment_dimensions`) because the latter is the sharding for the physical value and doesn't correspond to the logical value, so we instead need to either store the logical sharding (and keep it consistent) or compute it as a function of th)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shard-map] debugging shmap + prngkey bugs,"fixes CC(jax.random sampling doesn't work inside shard_map) There are a few changes here, since we ran into some things that we wanted to clean up: 1. the `shard_map`related change was that anything lowering from jaxprlevel logical avals their corresponding shardings to HLOlevel (""physical"") types and corresponding HLO OpSharding protos needs to be sure to actually handle the logicaltophysical translation on the shardings, but shmap wasn't doing that, so the changes to `_xla_shard` and `_xla_unshard` handle that; 2. we also didn't have a rep rule for some PRNGrelated primitives, so the `prng.__dict__.values()` change handles that; 3. `PRNGKeyArray` was delegating its `sharding` attribute to the underlying `_base_array.sharding`, but the latter has an incompatible rank (i.e. length of `tile_assignment_dimensions`) because the latter is the sharding for the physical value and doesn't correspond to the logical value, so we instead need to either store the logical sharding (and keep it consistent) or compute it as a function of th",2023-04-05T03:26:01Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15399
531,"以下是一个github上的jax下的一个issue, 标题是(jax.random sampling doesn't work inside shard_map)， 内容是 ( Description I'm trying to sample using a PRNGKey locally on each device, inside a `shard_map`. The following code raises an error.     What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU/TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.random sampling doesn't work inside shard_map," Description I'm trying to sample using a PRNGKey locally on each device, inside a `shard_map`. The following code raises an error.     What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU/TPU  Additional system info _No response_  NVIDIA GPU info _No response_",2023-04-05T00:56:43Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/15398,"(Also mentioned in chat) As a workaround, instead of using ""eager"" shmap you can apply a `jax.jit` to avoid the error, e.g.  That runs into a different (trivial) issue which requires passing `check_rep=False` to `shard_map`. I'll fix both in a PR.","Simpler repro: replace `f` with `lambda key: prng.random_wrap(key, impl=random.default_prng_impl())` where we do `from jax._src import prng, random`.",This is great! Thank you  for the ultrafast turnaround
398,"以下是一个github上的jax下的一个issue, 标题是(Checkify: instrumentation context manager)， 内容是 (Trying it out on `gather`. notes:  if you don't discharge an instrumented check, should you get an error? probably, but how? add an effect?  `instrument` > `catch`/`catch_error`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Checkify: instrumentation context manager,"Trying it out on `gather`. notes:  if you don't discharge an instrumented check, should you get an error? probably, but how? add an effect?  `instrument` > `catch`/`catch_error`",2023-04-04T14:32:00Z,,open,0,0,https://github.com/jax-ml/jax/issues/15389
1147,"以下是一个github上的jax下的一个issue, 标题是(Cannot install JAX through a proxy server)， 内容是 ( Description Hi, I'm trying to install JAX on a cluster via a proxy server. The following CLI done is : ` python3 m pip install proxy=MYPROXY upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ` I do get the error :   When I try CPU version for JAX  `python3 m pip install proxy=MYPROXY upgrade ""jax[cpu]""`  I get the same problem.  I have tried installing other packages on the cluster and had no issue at all. I have installed JAX on my local machine and it worked fine. Both `setuptools` are the same on my cluster and local machine   `setuptools==67.6.7` I would imagine that some build dependencies need an internet connection and does not take into account the argument `proxy` in the CLI for the install.  What jax/jaxlib version are you using? jax==0.4.8  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.9.16, Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot install JAX through a proxy server," Description Hi, I'm trying to install JAX on a cluster via a proxy server. The following CLI done is : ` python3 m pip install proxy=MYPROXY upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html ` I do get the error :   When I try CPU version for JAX  `python3 m pip install proxy=MYPROXY upgrade ""jax[cpu]""`  I get the same problem.  I have tried installing other packages on the cluster and had no issue at all. I have installed JAX on my local machine and it worked fine. Both `setuptools` are the same on my cluster and local machine   `setuptools==67.6.7` I would imagine that some build dependencies need an internet connection and does not take into account the argument `proxy` in the CLI for the install.  What jax/jaxlib version are you using? jax==0.4.8  Which accelerator(s) are you using? CPU/GPU  Additional system info Python 3.9.16, Linux  NVIDIA GPU info _No response_",2023-04-04T11:27:11Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15387,"I think you're running into this issue: https://github.com/pypa/pip/issues/6018 It looks like a bug in pip, over which JAX has no control. There are conflicting reports there, but perhaps you could try updating to the latest pip version to see if it fixes the issue?","Thanks for the reply, my current pip is the latest release `pip 23.0.1`. I'll try to investigate further with the issue you mentionned !  "
769,"以下是一个github上的jax下的一个issue, 标题是(jit fails for fft)， 内容是 ( Description As part of an effort of performing fft over a sharded array, I've stumbled upon a basic issue: it seems I'm unable to jit fft(). The following MRE:  will successfully execute the nonjit version, yet will fail when trying to jit.    Error message:   Is there a fundamental issue with jit'ing such a complex function, or an alternative way to perform that? Thank you.  What jax/jaxlib version are you using? jax 0.4.8; jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.12, WSL Ubuntu 22.04  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jit fails for fft," Description As part of an effort of performing fft over a sharded array, I've stumbled upon a basic issue: it seems I'm unable to jit fft(). The following MRE:  will successfully execute the nonjit version, yet will fail when trying to jit.    Error message:   Is there a fundamental issue with jit'ing such a complex function, or an alternative way to perform that? Thank you.  What jax/jaxlib version are you using? jax 0.4.8; jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.12, WSL Ubuntu 22.04  NVIDIA GPU info ",2023-04-04T10:28:16Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15386,"Thanks for the question! The JIT call is failing because the second argument controls the shape of the output, and in JAX output shapes must be static. If you want to JITcompile the FFT operation, one way to do so is to mark the second argument as static:  You can read more about this in JAX Sharp Bits: Dynamic Shapes.",Got it. Thanks for the quick & informative response!
728,"以下是一个github上的jax下的一个issue, 标题是(JAX profiler will crash on NV gpu)， 内容是 ( Description I installed the jax through ""pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html "", my OS is ubuntu 20.04, my cuda version is 12.1, cudnn version is 8.8.0, python version is 3.9,  the profiler will crash even on the simple  case. test case:  log info   What jax/jaxlib version are you using? jaxlib    0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info 3.9, ubuntu, cuda12.1, cudnn 8.8.0  NVIDIA GPU info ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,JAX profiler will crash on NV gpu," Description I installed the jax through ""pip install upgrade ""jax[cuda12_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html "", my OS is ubuntu 20.04, my cuda version is 12.1, cudnn version is 8.8.0, python version is 3.9,  the profiler will crash even on the simple  case. test case:  log info   What jax/jaxlib version are you using? jaxlib    0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info 3.9, ubuntu, cuda12.1, cudnn 8.8.0  NVIDIA GPU info ++  ++",2023-04-04T05:13:44Z,bug NVIDIA GPU,closed,3,7,https://github.com/jax-ml/jax/issues/15384,"Looks like there are at least two things going on here: a) on CUDA 11, we're missing a dependency on `cupti` in our `pip` dependencies, and the rpath necessary to find it b) on CUDA 12, I'm not sure, but I agree it crashes as you say.",b) looks to me like a bug in either cupti or the profiler support in XLA. gdb backtrace: ,"is there any workaround, cuda 11 is not working , it will crash with cudnn intilization failed, seem currently it requires cudnn 8.8",Having the same problem; I am using oficial container nvcr.io/nvdlfwea/jax/jax:23.03py3,"Same problem here, jax0.4.8 or 0.4.7 with the GPU, tried cuda 11 or 12.  I get the ""double free or corruption (!prev)"" before the crash, but I also get the ""TFTRT Warning: Could not find TensorRT"" both times even after trying to install tensorrt (but I may have done that incorrectly). Very frustrating as we're trying to find out why many of our routines are taking so very long with the GPU enabled.","This issue is only about a CUDA 12 bug. The workaround is to downgrade to CUDA 11. If you are having problems on CUDA 11, please share details on how to reproduce.","I'm not sure which change fixed it, but I can't reproduce this with JAX 0.4.20 under CUDA 12. I think we can declare this fixed."
790,"以下是一个github上的jax下的一个issue, 标题是(jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed)， 内容是 ( Description I have a python virtual environment with a clean installation of JAX   When I run my scripts, they work perfectly, but sometimes I get the following error with a success rate of between 2 and 10 successful executions and between 1 and 3 failed executions    What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Ubuntu 20.04  NVIDIA GPU info  CUDNN version (`/usr/local/cuda/include/cudnn_version.h`) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed," Description I have a python virtual environment with a clean installation of JAX   When I run my scripts, they work perfectly, but sometimes I get the following error with a success rate of between 2 and 10 successful executions and between 1 and 3 failed executions    What jax/jaxlib version are you using? jax 0.4.8, jaxlib 0.4.7+cuda12.cudnn88  Which accelerator(s) are you using? GPU  Additional system info Python 3.8.10, Ubuntu 20.04  NVIDIA GPU info  CUDNN version (`/usr/local/cuda/include/cudnn_version.h`) ",2023-04-02T14:16:53Z,bug NVIDIA GPU,closed,0,35,https://github.com/jax-ml/jax/issues/15361,What is your OS? Can you confirm you run the scripts sequentially and so there is nothing that is using the GPU in parallel?,"Hi   The OS is Ubuntu 20.04, as indicated above. Btw, I think the problem may be VS Code. After running the script several times to try to get the error to appear, I see that the error only appears (not always but) when I make a modification to the script and save it. There is also the following log. As you can see (by running the `nvidiasmi` command just before executing the script, and after saving it) there is a GPU consumption. The strange thing is that the consumption comes from the python environment  (`env_gym`) configured in the VS Code bottom right pane and not from the python of the sourced environment where jax is installed (`env_jax`) 🤔 ",Thanks for the results. I think we need a way to give a better error to end users.,"Recently a few error message got a little bit better. Closing as I'm not sure what do to more. But if the issue appear again and the error isn't good enough, poke us again.",I also got this error and it was due to GPU reaching its memory limit, Do you have the full error message you had? I would like to improve the error message in that case.,same error  I think there also the memory reaching the limit. ,Yep I received the same error message as 24 ,Thanks for the extended error message. But can you share the full output without any truncation? There should be information that should help me. Which JAX version did you use?,"I'm having the same problem but for me it's consistent and I'm unable to run simple Jax code. I only have this problem on my newest system with 4x RTX 4090 GPUs. I have a server A100 and a PC with a 3090ti that work smoothly. Ubuntu 22 across all systems. First installed CUDA 11 from condaforge as suggested, same issue. Then switched to loca installation of CUDA and cudnn. Same problem. After a fresh installation of everything when I run `a = jnp.ones((3,))` I get this error: 20230513 09:04:27.790057: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20230513 09:04:27.790140: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 5853872128 bytes free, 25393692672 bytes total. Traceback (most recent call last):   File """", line 1, in    File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/numpy/lax_numpy.py"", line 2122, in ones     return lax.full(shape, 1, _jnp_dtype(dtype))            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/lax/lax.py"", line 1203, in full     return broadcast(fill_value, shape)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/lax/lax.py"", line 768, in broadcast     return broadcast_in_dim(operand, tuple(sizes) + np.shape(operand), dims)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/lax/lax.py"", line 796, in broadcast_in_dim     return broadcast_in_dim_p.bind(            ^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 131, in apply_primitive     compiled_fun = xla_primitive_callable(                    ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/util.py"", line 284, in wrapper     return cached(config._trace_context(), *args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/util.py"", line 277, in cached     return f(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 222, in xla_primitive_callable     compiled = _xla_callable_uncached(                ^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 252, in _xla_callable_uncached     return computation.compile().unsafe_call            ^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2313, in compile     executable = UnloadedMeshExecutable.from_hlo(                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2633, in from_hlo     xla_executable, compile_options = _cached_compilation(                                       ^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 495, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)            ^^^^^^^^^^^^^^^^^^^^^   File ""/home/hoss/anaconda3/envs/jaxtf/lib/python3.11/sitepackages/jax/_src/dispatch.py"", line 463, in backend_compile     return backend.compile(built_c, compile_options=options)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.",I've also tried to swap my PCs 3090ti (which works properly) with one of the 4090s and I got the exact same error. This used to work in the past. I'm pretty sure the GPU hardware is not the problem and they are functional (tested them on Windows machines).,"Did you cut the output? The error tell to look above for more errors. If there is more outputs, give me all what you have. I'll filter what is useful or not.","I'm getting the same kind of error trying to install jax/jaxlib on an EC2 p2.xlarge (with k80s), to provide solidarity! I can provide more details if useful, but basically running some vanilla installation script of Anaconda and trying different variants of `pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` leads Jax to report seeing the GPU when I check `print(xla_bridge.get_backend().platform)` but gives the DNN error above, otherwise.","(I'm also unable to _any_ Jax code, e.g. `a = jnp.ones((3,))`.)"," Please open new issues rather than appending onto closed ones. However, I think the problem in your case is simple: JAX no longer supports Kepler GPUs in the wheels we release. You can probably rebuild jaxlib from source if you need Kepler support, but note NVIDIA has dropped Kepler support from CUDA 12 and CUDNN 8.9, so this may not remain true for long.", no this is all the output. It's several lines of error are you seeing all of it? I managed to resolve this though. I installed CUDA 11 and cudnn 8.6. In my experiments I also installed the latest version of everything but this was the only version combination that worked for me. Now I'm getting other errors but that's a different problem., Point taken and... Thanks for the help! I just switched over to V100s and _voila_!,"I got the same error, maybe it due to the mismatch between your cuda version and the installed jax. I use ubuntu 20.04 with cuda version as below: !image At start, I installed the newest jax as  Then I got the error as reported. So I switched to  Everything works well!","Thanks , for a simple use case I was able to use JAX 0.4.13 and CUDA 11.8 with CUDNN 8.6. I needed to add `/usr/lib/x86_64linuxgnu` to the `LD_LIBRARY_PATH` (installed `libcudnn8` with `aptget`).","I also met this error: The output: 20230731 01:53:45.016563: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:427] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.  XlaRuntimeError                           Traceback (most recent call last) Cell In[4], line 29      26 model = trainer.make_model(nmask)      28 lr_fn, opt = trainer.make_optimizer(steps_per_epoch=len(train_dl)) > 29 state = trainer.create_train_state(jax.random.PRNGKey(0), model, opt)      30 state = checkpoints.restore_checkpoint(ckpt.parent, state) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/random.py:137, in PRNGKey(seed)     134 if np.ndim(seed):     135   raise TypeError(""PRNGKey accepts a scalar seed, but was given an array of""     136                   f""shape {np.shape(seed)} != (). Use jax.vmap for batching"") > 137 key = prng.seed_with_impl(impl, seed)     138 return _return_prng_keys(True, key) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:320, in seed_with_impl(impl, seed)     319 def seed_with_impl(impl: PRNGImpl, seed: Union[int, Array]) > PRNGKeyArrayImpl: > 320   return random_seed(seed, impl=impl) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:734, in random_seed(seeds, impl)     732 else:     733   seeds_arr = jnp.asarray(seeds) > 734 return random_seed_p.bind(seeds_arr, impl=impl) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:380, in Primitive.bind(self, *args, **params)     377 def bind(self, *args, **params):     378   assert (not config.jax_enable_checks or     379           all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args > 380   return self.bind_with_trace(find_top_trace(args), args, params) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:383, in Primitive.bind_with_trace(self, trace, args, params)     382 def bind_with_trace(self, trace, args, params): > 383   out = trace.process_primitive(self, map(trace.full_raise, args), params)     384   return map(full_lower, out) if self.multiple_results else full_lower(out) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/core.py:790, in EvalTrace.process_primitive(self, primitive, tracers, params)     789 def process_primitive(self, primitive, tracers, params): > 790   return primitive.impl(*tracers, **params) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:746, in random_seed_impl(seeds, impl)     744 .def_impl     745 def random_seed_impl(seeds, *, impl): > 746   base_arr = random_seed_impl_base(seeds, impl=impl)     747   return PRNGKeyArrayImpl(impl, base_arr) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:751, in random_seed_impl_base(seeds, impl)     749 def random_seed_impl_base(seeds, *, impl):     750   seed = iterated_vmap_unary(seeds.ndim, impl.seed) > 751   return seed(seeds) File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/prng.py:980, in threefry_seed(seed)     968 def threefry_seed(seed: typing.Array) > typing.Array:     969   """"""Create a single raw threefry PRNG key from an integer seed.     970      971   Args:    (...)     978     first padding out with zeros).     979   """""" > 980   return _threefry_seed(seed)     [... skipping hidden 12 frame] File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/sitepackages/jax/_src/dispatch.py:463, in backend_compile(backend, module, options, host_callbacks)     458   return backend.compile(built_c, compile_options=options,     459                          host_callbacks=host_callbacks)     460  Some backends don't have `host_callbacks` option yet     461  TODO(sharadmv): remove this fallback when all backends allow `compile`     462  to take in `host_callbacks` > 463 return backend.compile(built_c, compile_options=options) XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.  What jax/jaxlib version are you using? Jax0.4.10, jaxlib0.4.10+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Python 3.11.4, Ubuntu 22.04  NVIDIA GPU info ++  ++",'s problem was resolved in https://github.com/google/jax/issues/16901. ,"hi, I have similar issue. please help me! the output: 20230905 14:32:56.559501: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20230905 14:32:56.559528: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 6081413120 bytes free, 25438126080 bytes total. Traceback (most recent call last):   File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 453, in      app.run(main)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/absl/app.py"", line 308, in run     _run_main(main, args)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main     sys.exit(main(argv))   File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 428, in main     predict_structure(   File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 214, in predict_structure     prediction_result = model_runner.predict(processed_feature_dict,   File ""/home/wangyun/pre/alphafoldmultimermain/alphafold/model/model.py"", line 167, in predict     result = self.apply(self.params, jax.random.PRNGKey(random_seed), feat)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/random.py"", line 137, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 320, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 732, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 744, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 749, in random_seed_impl_base     return seed(seeds)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 978, in threefry_seed     return _threefry_seed(seed)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 208, in cache_miss     outs, out_flat, out_tree, args_flat = _python_pjit_helper(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 155, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 2633, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 790, in process_primitive     return primitive.impl(*tracers, **params)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1085, in _pjit_call_impl     compiled = _pjit_lower(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2313, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2633, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 494, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 462, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. What jax/jaxlib version are you using? jax 0.4.9, jaxlib 0.4.9+cuda11.cudnn86 My conda virtual environment: python3.9.0 cudatoolkit               11.8.0              h4ba93d1_12    condaforge cudnn                     8.6.0.163            hed8a83a_0    cudistas But my OS environment:  NVIDIASMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2  ii  cudnnlocalrepoubuntu18048.9.3.28              1.01              amd64        cudnnlocal repository configuration files I try everything what I can , but ……","> What is your OS? Can you confirm you run the scripts sequentially and so there is nothing that is using the GPU in parallel? Hi, I encountered the same problem. When I use A100 to run a single task, it can run normally, but when I submit two tasks at the same time, the above error will be reported. So the reason is that A100 runs two tasks at the same time, will there be a conflict?","> Hi, I encountered the same problem. When I use A100 to run a single task, it can run normally, but when I submit two tasks at the same time, the above error will be reported. So the reason is that A100 runs two tasks at the same time, will there be a conflict? I suppose 2 tasks means 2 process. If not, tell us. By default, JAX will reserve 75% of the GPU memory for the process: https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html So the 2nd process will end up missing GPU memory most of the time. Read that web page to know how to control that 75% memory allocation. If you can lower it to 45% and the first process has enough memory, it will probably work. Otherwise, try a few other values.","> hi, I have similar issue. please help me! >  > the output: 20230905 14:32:56.559501: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:439] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 20230905 14:32:56.559528: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:443] Memory usage: 6081413120 bytes free, 25438126080 bytes total. Traceback (most recent call last): File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 453, in app.run(main) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/absl/app.py"", line 308, in run _run_main(main, args) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/absl/app.py"", line 254, in _run_main sys.exit(main(argv)) File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 428, in main predict_structure( File ""/home/wangyun/pre/alphafoldmultimermain/run_alphafold.py"", line 214, in predict_structure prediction_result = model_runner.predict(processed_feature_dict, File ""/home/wangyun/pre/alphafoldmultimermain/alphafold/model/model.py"", line 167, in predict result = self.apply(self.params, jax.random.PRNGKey(random_seed), feat) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/random.py"", line 137, in PRNGKey key = prng.seed_with_impl(impl, seed) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 320, in seed_with_impl return random_seed(seed, impl=impl) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 732, in random_seed return random_seed_p.bind(seeds_arr, impl=impl) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 744, in random_seed_impl base_arr = random_seed_impl_base(seeds, impl=impl) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 749, in random_seed_impl_base return seed(seeds) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/prng.py"", line 978, in threefry_seed return _threefry_seed(seed) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback return fun(*args, **kwargs) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 208, in cache_miss outs, out_flat, out_tree, args_flat = _python_pjit_helper( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 155, in _python_pjit_helper out_flat = pjit_p.bind(*args_flat, **params) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 2633, in bind return self.bind_with_trace(top_trace, args, params) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace out = trace.process_primitive(self, map(trace.full_raise, args), params) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/core.py"", line 790, in process_primitive return primitive.impl(*tracers, **params) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/pjit.py"", line 1085, in _pjit_call_impl compiled = _pjit_lower( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2313, in compile executable = UnloadedMeshExecutable.from_hlo( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2633, in from_hlo xla_executable, compile_options = _cached_compilation( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/interpreters/pxla.py"", line 2551, in _cached_compilation xla_executable = dispatch.compile_or_get_cached( File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 494, in compile_or_get_cached return backend_compile(backend, computation, compile_options, File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/profiler.py"", line 314, in wrapper return func(*args, **kwargs) File ""/home/wangyun/miniconda3/envs/mutimer3/lib/python3.9/sitepackages/jax/_src/dispatch.py"", line 462, in backend_compile return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. >  > What jax/jaxlib version are you using? jax 0.4.9, jaxlib 0.4.9+cuda11.cudnn86 >  > My conda virtual environment: python3.9.0 cudatoolkit 11.8.0 h4ba93d1_12 condaforge cudnn 8.6.0.163 hed8a83a_0 cudistas >  > But my OS environment: NVIDIASMI 535.54.03 Driver Version: 535.54.03 CUDA Version: 12.2 ii cudnnlocalrepoubuntu18048.9.3.28 1.01 amd64 cudnnlocal repository configuration files >  > I try everything what I can , but …… then, it work. you can look this link (https://blog.csdn.net/2201_75882736/article/details/132812927)"," Hi, I also have the same issue, could anyone please help me? E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:407] There was an error before creating cudnn handle (302): cudaGetErrorName symbol not found. : cudaGetErrorString symbol not found. Traceback (most recent call last):   File ""/bd_byt4090i1/users/state_space_model/DNN/S5/run_train.py"", line 101, in      train(parser.parse_args())   File ""/bd_byt4090i1/users/state_space_model/DNN/S5/s5/train.py"", line 41, in train     key = random.PRNGKey(args.jax_seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/random.py"", line 160, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 707, in random_seed_impl_base     return seed(seeds)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 936, in threefry_seed     return _threefry_seed(seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/traceback_util.py"", line 166, in reraise_with_filtered_traceback     return fun(*args, **kwargs)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 250, in cache_miss     outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 163, in _python_pjit_helper     out_flat = pjit_p.bind(*args_flat, **params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 2677, in bind     return self.bind_with_trace(top_trace, args, params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 815, in process_primitive     return primitive.impl(*tracers, **params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1203, in _pjit_call_impl     return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1187, in call_impl_cache_miss     out_flat, compiled = _pjit_call_impl_python(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/pjit.py"", line 1123, in _pjit_call_impl_python     always_lower=False, lowering_platform=None).compile()   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 2323, in compile     executable = UnloadedMeshExecutable.from_hlo(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 2645, in from_hlo     xla_executable, compile_options = _cached_compilation(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/interpreters/pxla.py"", line 2555, in _cached_compilation     xla_executable = dispatch.compile_or_get_cached(   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 497, in compile_or_get_cached     return backend_compile(backend, computation, compile_options,   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/profiler.py"", line 314, in wrapper     return func(*args, **kwargs)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/dispatch.py"", line 465, in backend_compile     return backend.compile(built_c, compile_options=options) jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. The stack trace below excludes JAXinternal frames. The preceding is the original exception that occurred, unmodified.  The above exception was the direct cause of the following exception: Traceback (most recent call last):   File ""/bd_byt4090i1/users/state_space_model/DNN/S5/run_train.py"", line 101, in      train(parser.parse_args())   File ""/bd_byt4090i1/users/state_space_model/DNN/S5/s5/train.py"", line 41, in train     key = random.PRNGKey(args.jax_seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/random.py"", line 160, in PRNGKey     key = prng.seed_with_impl(impl, seed)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 406, in seed_with_impl     return random_seed(seed, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 690, in random_seed     return random_seed_p.bind(seeds_arr, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 380, in bind     return self.bind_with_trace(find_top_trace(args), args, params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 383, in bind_with_trace     out = trace.process_primitive(self, map(trace.full_raise, args), params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/core.py"", line 815, in process_primitive     return primitive.impl(*tracers, **params)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 702, in random_seed_impl     base_arr = random_seed_impl_base(seeds, impl=impl)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 707, in random_seed_impl_base     return seed(seeds)   File ""/bd_byt4090i1/users/state_space_model/miniconda3/envs/s5/lib/python3.10/sitepackages/jax/_src/prng.py"", line 936, in threefry_seed     return _threefry_seed(seed) jaxlib.xla_extension.XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details. My jax version: jax==0.4.13 jaxlib==0.4.13+cuda11.cudnn86 flax==0.7.4 chex==0.1.8 My gpu information: ++  ++ It should be a RTX 2070. Thanks a lot for the help","You are using an old JAX version (0.4.13) and an old driver  that is for CUDA 11.8. Can you update your NVIDIA driver to at least one that support CUDA 11.8, as this is the min version that is currently supported by JAX? JAX is dropping CUDA 11 in the next releases, so if you can update to CUDA12, that would be better.","> You are using an old JAX version (0.4.13) and an old driver that is for CUDA 11.8. > Can you update your NVIDIA driver to at least one that support CUDA 11.8, as this is the min version that is currently supported by JAX? JAX is dropping CUDA 11 in the next releases, so if you can update to CUDA12, that would be better. Thanks a lot. I tried that, and it worked!","Hi, guys. I'm here on by recommendation.  I'm facing a similar issue: ""Not Enough GPU memory? FAILED_PRECONDITION: DNN library initialization failed.""  I tried almost everything suggested on these page to resolve the GPU memory problem: https://github.com/YoshitakaMo/localcolabfold/issues/210 https://github.com/YoshitakaMo/localcolabfold/issues/224 https://github.com/YoshitakaMo/localcolabfold/issues/228 My current jax, cudnn, and nvidiasmi versions are as follows. (Linux Ubuntu 22.04.2 LTS and RTX 4090)  Here's the problem I'm encountering:  I can't find ""the errors above."" Can someone offer any guesses or solutions? ","JAX releases don't support CUDNN 9 yet. Downgrade to CUDNN 8.9 (or build jaxlib from source with CUDNN 9, that works also)"
741,"以下是一个github上的jax下的一个issue, 标题是(Implementation of scipy.ndimage.gaussian_filter1d and gaussian_filter)， 内容是 (Basic implementation of 1D gaussian filter as defined in scipy.ndimage. Only supports a convolution with zero padding. Extended to ND applying successive convolution on each axis. Minimal testing and performance in this colab notebook. It performs poorly compared to scipy implementation since convolution in JAX does not implement FFT based methods yet, but this may be improved in the future ( CC(FFT Convolution)). This is a start for CC(Implementation of ndimage.filters.gaussian_filter) )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Implementation of scipy.ndimage.gaussian_filter1d and gaussian_filter,"Basic implementation of 1D gaussian filter as defined in scipy.ndimage. Only supports a convolution with zero padding. Extended to ND applying successive convolution on each axis. Minimal testing and performance in this colab notebook. It performs poorly compared to scipy implementation since convolution in JAX does not implement FFT based methods yet, but this may be improved in the future ( CC(FFT Convolution)). This is a start for CC(Implementation of ndimage.filters.gaussian_filter) ",2023-04-02T11:23:37Z,,open,0,1,https://github.com/jax-ml/jax/issues/15359,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
550,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Removed call_tf tests that are not applicable anymore.)， 内容是 ([jax2tf] Removed call_tf tests that are not applicable anymore. A recent change in TensorFlow makes copies of np.ndarray when they are turned into tf.constant. This means that call_tf cannot guarantee anymore nocopy. Removing those tests, and the paragraph in the documentation that describes this property.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Removed call_tf tests that are not applicable anymore.,"[jax2tf] Removed call_tf tests that are not applicable anymore. A recent change in TensorFlow makes copies of np.ndarray when they are turned into tf.constant. This means that call_tf cannot guarantee anymore nocopy. Removing those tests, and the paragraph in the documentation that describes this property.",2023-04-01T09:35:34Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15352
1223,"以下是一个github上的jax下的一个issue, 标题是(Added working example of forward-forward algorithm)， 内容是 (The ForwardForward Algorithm was proposed by Geoffrey Hinton in his recent paper ""The ForwardForward Algorithm: Some Preliminary Investigations"" https://arxiv.org/abs/2212.13345 This algorithm proposes a new way of optimizing neural networks using local gradient updates as an alternative to backpropagation. Each layer is trained greedily by calculating its losses and gradients at each step without passing them to other layers in the network. JAX is suitable for implementing such an algorithm because of its powerful autograd framework. Each layer must maintain its own optimizer state and parameters without worrying about the model graph. This is easy to implement using JAX's functional scope. The example trains and evaluates a simple multilayer perceptron network on the MNIST dataset, and achieves a 1.8% test accuracy error compared to the 1.36% reported in the original paper. Further improvements can be achieved with hyperparameter finetuning and changing the model architecture.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Added working example of forward-forward algorithm,"The ForwardForward Algorithm was proposed by Geoffrey Hinton in his recent paper ""The ForwardForward Algorithm: Some Preliminary Investigations"" https://arxiv.org/abs/2212.13345 This algorithm proposes a new way of optimizing neural networks using local gradient updates as an alternative to backpropagation. Each layer is trained greedily by calculating its losses and gradients at each step without passing them to other layers in the network. JAX is suitable for implementing such an algorithm because of its powerful autograd framework. Each layer must maintain its own optimizer state and parameters without worrying about the model graph. This is easy to implement using JAX's functional scope. The example trains and evaluates a simple multilayer perceptron network on the MNIST dataset, and achieves a 1.8% test accuracy error compared to the 1.36% reported in the original paper. Further improvements can be achieved with hyperparameter finetuning and changing the model architecture.",2023-03-31T21:34:36Z,,open,0,0,https://github.com/jax-ml/jax/issues/15348
1231,"以下是一个github上的jax下的一个issue, 标题是(Added example implementation of forward-forward algorithm)， 内容是 (The ForwardForward Algorithm was proposed by Geoffrey Hinton in his recent paper ""The ForwardForward Algorithm: Some Preliminary Investigations"" https://arxiv.org/abs/2212.13345 This algorithm proposes a new way of optimizing neural networks using local gradient updates as an alternative to backpropagation. Each layer is trained greedily by calculating its losses and gradients at each step without passing them to other layers in the network. JAX is suitable for implementing such an algorithm because of its powerful autograd framework. Each layer must maintain its own optimizer state and parameters without worrying about the model graph. This is easy to implement using JAX's functional scope.  The example trains and evaluates a simple multilayer perceptron network on the MNIST dataset, and achieves a 1.8% test accuracy error compared to the 1.36% reported in the original paper. Further improvements can be achieved with hyperparameter finetuning and changing the model architecture.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Added example implementation of forward-forward algorithm,"The ForwardForward Algorithm was proposed by Geoffrey Hinton in his recent paper ""The ForwardForward Algorithm: Some Preliminary Investigations"" https://arxiv.org/abs/2212.13345 This algorithm proposes a new way of optimizing neural networks using local gradient updates as an alternative to backpropagation. Each layer is trained greedily by calculating its losses and gradients at each step without passing them to other layers in the network. JAX is suitable for implementing such an algorithm because of its powerful autograd framework. Each layer must maintain its own optimizer state and parameters without worrying about the model graph. This is easy to implement using JAX's functional scope.  The example trains and evaluates a simple multilayer perceptron network on the MNIST dataset, and achieves a 1.8% test accuracy error compared to the 1.36% reported in the original paper. Further improvements can be achieved with hyperparameter finetuning and changing the model architecture.",2023-03-31T20:08:32Z,,closed,0,1,https://github.com/jax-ml/jax/issues/15346,"Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). View this failed invocation of the CLA check for more information. For the most up to date status, view the checks section at the bottom of the pull request."
732,"以下是一个github上的jax下的一个issue, 标题是(Sparse `std` throws error)， 内容是 ( Description Hi all, I ran into an issue when trying to compute the standard deviation along an axis using sparse matrices in `BCOO` format. Here is a MWE,   throws a `NotImplementedError`, with the bottom of the stacktrace looking like,  For now I'm not requiring the results to also be sparse so a workaround is  Thanks again for all the support and such a great library.  What jax/jaxlib version are you using? 0.4.8 / 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Sparse `std` throws error," Description Hi all, I ran into an issue when trying to compute the standard deviation along an axis using sparse matrices in `BCOO` format. Here is a MWE,   throws a `NotImplementedError`, with the bottom of the stacktrace looking like,  For now I'm not requiring the results to also be sparse so a workaround is  Thanks again for all the support and such a great library.  What jax/jaxlib version are you using? 0.4.8 / 0.4.7  Which accelerator(s) are you using? CPU  Additional system info Mac  NVIDIA GPU info _No response_",2023-03-31T17:57:19Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/15344,"Thanks for the report – I think this is expected given the current implementation of standard deviation, which effectively computes `(x  x.mean(axis)) ** 2`. Note that if `x` is sparse, then `x  x.mean(axis)` would require densifying the entire array. I think the way you're approaching it (essentially factoring out these operations) is probably the best approach."
1268,"以下是一个github上的jax下的一个issue, 标题是(fix bug from #15335 by checking main_trace tag)， 内容是 (We added the `main_trace` tag field on `core.AxisEnvFrame` so that we could disambiguate which vmap trace instance (""handler"") corresponded to which axis env frame. But in one place, namely `BatchTrace.process_primitive`'s subroutine `BatchTrace.get_frame`, we weren't using it to do that disambiguation: we were only using the (ambiguous, shadowable) name. While CC(add assertions for axis name shadowing bugs) identified the issue and added assertions that we didn't do the _wrong_ thing, this PR fixes the underlying issue and ensures we do the right thing. The assertions are still there; they just don't fire because we now don't get things confused! We should still consider disallowing shadowing:  suggested for example how confusing it might be to write `vmap(hop(fun=lambda x: ...mentions axis name 'i'...), axis_name='i')(x)` where the `hop` implementation calls `vmap(fun, axis_name='i')`. **Now we can enable the shmap test we had to skip!** In addition to enabling that test, I'm going to rename it to be more specific and add tw)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,fix bug from #15335 by checking main_trace tag,"We added the `main_trace` tag field on `core.AxisEnvFrame` so that we could disambiguate which vmap trace instance (""handler"") corresponded to which axis env frame. But in one place, namely `BatchTrace.process_primitive`'s subroutine `BatchTrace.get_frame`, we weren't using it to do that disambiguation: we were only using the (ambiguous, shadowable) name. While CC(add assertions for axis name shadowing bugs) identified the issue and added assertions that we didn't do the _wrong_ thing, this PR fixes the underlying issue and ensures we do the right thing. The assertions are still there; they just don't fire because we now don't get things confused! We should still consider disallowing shadowing:  suggested for example how confusing it might be to write `vmap(hop(fun=lambda x: ...mentions axis name 'i'...), axis_name='i')(x)` where the `hop` implementation calls `vmap(fun, axis_name='i')`. **Now we can enable the shmap test we had to skip!** In addition to enabling that test, I'm going to rename it to be more specific and add tw",2023-03-31T05:14:29Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15337
1265,"以下是一个github上的jax下的一个issue, 标题是(add assertions for axis name shadowing bugs)， 内容是 (Axis name shadowing in `vmap`of`vmap` and `vmap`of`pmap` is buggy, and has been for at least 2 years, and possibly for as long as we've had axis names in vmap. Damn! The basic issue is that we look up the axis frame in the axis environment based on name, e.g. with `core.axis_frame(self.axis_name)` in methods on `BatchTrace`. But if axis names can shadow, that means computing a frame based only on `self.axis_name` could give the wrong one! For example, if we had  then in `BatchTrace.process_primitive` when we're about to call the `pjit` batching rule (since `2 * x` leads to a call to the jitted `jnp.multiply`) we get a `core.axis_frame(self.axis_name).size == 4` when we have `in_vals[0].shape[in_dims[0]] == 2`. That is, when applying `BatchTrace.process_primitive` for the (outer) `vmap`, we accidentally pick up the frame for the (inner) `pmap`. Indeed we always get the frame for the inner guy from `core.axis_frame(self.axis_name)`! The two maps have gotten confused! An analogous thing happens with  except now we're getting two )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,add assertions for axis name shadowing bugs,"Axis name shadowing in `vmap`of`vmap` and `vmap`of`pmap` is buggy, and has been for at least 2 years, and possibly for as long as we've had axis names in vmap. Damn! The basic issue is that we look up the axis frame in the axis environment based on name, e.g. with `core.axis_frame(self.axis_name)` in methods on `BatchTrace`. But if axis names can shadow, that means computing a frame based only on `self.axis_name` could give the wrong one! For example, if we had  then in `BatchTrace.process_primitive` when we're about to call the `pjit` batching rule (since `2 * x` leads to a call to the jitted `jnp.multiply`) we get a `core.axis_frame(self.axis_name).size == 4` when we have `in_vals[0].shape[in_dims[0]] == 2`. That is, when applying `BatchTrace.process_primitive` for the (outer) `vmap`, we accidentally pick up the frame for the (inner) `pmap`. Indeed we always get the frame for the inner guy from `core.axis_frame(self.axis_name)`! The two maps have gotten confused! An analogous thing happens with  except now we're getting two ",2023-03-31T04:24:27Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/15335,"~I think I introduced this bug in  CC(simplify the implementation of vmap collectives) by splitting out the `_main_trace_for_axis_names` function (and leaving behind a `frame = core.axis_frame(self.axis_name)`.~ Nevermind, the bug existed before that PR too! It only checks the top namematching axis frame too."
794,"以下是一个github上的jax下的一个issue, 标题是(Version matched error while ""pip install -r requirements.txt""?)， 内容是 ( Description I've just transport a new machine and found that using pip install through requirements.txt doesn't working. It seems interesting that it works when I use the script in the README.md. Working: `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Not working: `pip install r requirements.txt` Requirements.txt:   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"Version matched error while ""pip install -r requirements.txt""?"," Description I've just transport a new machine and found that using pip install through requirements.txt doesn't working. It seems interesting that it works when I use the script in the README.md. Working: `pip install upgrade ""jax[cuda11_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` Not working: `pip install r requirements.txt` Requirements.txt:   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-03-30T02:51:11Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15306,I think it is just my machine issue. Other package has also encounter the same issue. Sorry;,Glad you got it sorted out!
389,"以下是一个github上的jax下的一个issue, 标题是(Add minimal pyproject.toml specifying build system)， 内容是 (Add minimal pyproject.toml specifying build system Replaces CC(Add minimal pyproject.toml specifying build system), Fixes CC(pyproject.toml based wheel builds))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add minimal pyproject.toml specifying build system,"Add minimal pyproject.toml specifying build system Replaces CC(Add minimal pyproject.toml specifying build system), Fixes CC(pyproject.toml based wheel builds)",2023-03-29T15:55:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15291
1272,"以下是一个github上的jax下的一个issue, 标题是(jnp.save in a distributed mannor through GPUDirect)， 内容是 (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. When we load and save using jax, we typically use `jnp.load`, `jnp.save` or `pickle` to serialize the object. However, when we use distributed array, these operations are gathered the data through the host memory which incurs out of memory and ask too much time spending saving checkpoint. Roughly speaking, saving 13B GPT state (param+optimizer state) requires 13min, which is equivalent to 10~100 update steps. If I save the checkpoint every 1000 steps, save operation increase the training time 10% or more. There is a DMA (Direct Memory Access)like technology inside CUDA which enables direct transfer between GPU memory and disk. https://developer.nvidia.com/blog/gpudirectstorage/ It would be better to use these technique to accelerate the research. If the technology is already applied and just can be used with some jit option, please help me. I've just created a snippet but I don't know how to fil)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",gpt,jnp.save in a distributed mannor through GPUDirect,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. When we load and save using jax, we typically use `jnp.load`, `jnp.save` or `pickle` to serialize the object. However, when we use distributed array, these operations are gathered the data through the host memory which incurs out of memory and ask too much time spending saving checkpoint. Roughly speaking, saving 13B GPT state (param+optimizer state) requires 13min, which is equivalent to 10~100 update steps. If I save the checkpoint every 1000 steps, save operation increase the training time 10% or more. There is a DMA (Direct Memory Access)like technology inside CUDA which enables direct transfer between GPU memory and disk. https://developer.nvidia.com/blog/gpudirectstorage/ It would be better to use these technique to accelerate the research. If the technology is already applied and just can be used with some jit option, please help me. I've just created a snippet but I don't know how to fil",2023-03-29T03:25:59Z,enhancement NVIDIA GPU,open,0,4,https://github.com/jax-ml/jax/issues/15281,"You might look into https://github.com/google/tensorstore , which is what many of my colleagues use for checkpointing their JAX jobs. It is particularly valuable for *distributed* checkpointing since each worker can save pieces of the checkpoint in parallel.",That is a good candidate! I will consider it.,"I reached the gda serialization in the jax.experimental package. Saving LLaMA 65B with float16 (130GB) takes 4 minutes. Is it a decent time to consume or is there any other way to make it better? Zstd compression algorithm (which is the fastest) is used, but the compression ratio is not that great (after saving, 110GB is saved). Do I removed the compression algorithm inside the tensorstore? I think there is a default algorithm (blosc?) when I removed the compression algorithm.",I found out useful information in the following link.. https://google.github.io/tensorstore/driver/zarr/index.htmljsondriver/zarr.metadata.compressor
818,"以下是一个github上的jax下的一个issue, 标题是(Use modern packaging standards for setuptools)， 内容是 (In setuptools 30.3.0 (December 2016), support was added for specifying setup options in setup.cfg as opposed to arguments to the setup function. Options are merged, and setup.py takes priority when disagreements occur. setup.cfg arguments are superior for a couple reasons. They are:  declarative  no python code is executed to evaluate them  easier to read  capable of doing things that you'd need to opencode in python Of particular note, it is possible to load files or statically parse the AST for a simple assignment, via values such as `attr:` or `file:`. This is a cfgexclusive feature.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Use modern packaging standards for setuptools,"In setuptools 30.3.0 (December 2016), support was added for specifying setup options in setup.cfg as opposed to arguments to the setup function. Options are merged, and setup.py takes priority when disagreements occur. setup.cfg arguments are superior for a couple reasons. They are:  declarative  no python code is executed to evaluate them  easier to read  capable of doing things that you'd need to opencode in python Of particular note, it is possible to load files or statically parse the AST for a simple assignment, via values such as `attr:` or `file:`. This is a cfgexclusive feature.",2023-03-29T02:49:42Z,,open,1,5,https://github.com/jax-ml/jax/issues/15280, from https://github.com/google/jax/issues/15256issuecomment1487730643  > but will be somewhat painful. Our current setup process makes use of a lot of the imperative computation flexibility of `setup.py` files Here's an example that shows you can have the best of both worlds. :),"pyproject.toml is the community standard for specifying project metadata declaritively: I would recommend using that over setup.cfg, which will eventually be deprecated and removed. In this case, you would have to add `""optionaldependencies""` to `dynamic`. All other features are supported in pyproject.toml, either in the `project` table or `tool.setuptools`. `url` will become `project.urls.homepage`.","> or `tool.setuptools`. ... which is still in beta stage, unlike something that has worked robustly and stably for half a decade plus. (I'd love to see it marked as stable, but I'm not sure what the issues involved are.) > over setup.cfg, which will eventually be deprecated and removed. This will almost certainly never ever ever happen, because it would cause large chunks of PyPI to become unbuildable without any hope of vanished maintainers ever fixing things. Additionally, I seem to recall the proposed ""deprecation"" method was actually ""internally run a function that converts cfg to toml, and then run another function to load the toml"", which is a bit of indirection that really boils down to ""setup.cfg is still supported"". It's certainly going to end up removed from the documentation, *eventually*.","`find_packages` is now unused in setup.py: I suggest removing the import  > > or tool.setuptools. > > ... which is still in beta stage The docs aren't particularly clear, but it seems specifically automatic discovery, package data, and dynamic dependencies (from file) are in beta. I suppose its fine to wait for it to be marked as stable.  > > over setup.cfg, which will https://github.com/pypa/setuptools/issues/3214 and removed. > > This will almost certainly never ever ever happen, because it would cause large chunks of PyPI to become unbuildable without any hope of vanished maintainers ever fixing things. Thus the deprecation. As a related example, `distutils` is in the final stages of being removed, despite it being used in ~100% of distributed projects 10 or 15 years ago. A GitHub search shows ~64000 instances of using setup.cfg for metadata declaration right now.","> `find_packages` is now unused in setup.py: I suggest removing the import Good point. While I'm at it, distutils.spawn.find_executable() is a truly old way of performing the role of shutil.which... speaking of deprecated things."
273,"以下是一个github上的jax下的一个issue, 标题是(Add minimal pyproject.toml specifying build system)， 内容是 (Fixes CC(pyproject.toml based wheel builds))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add minimal pyproject.toml specifying build system,Fixes CC(pyproject.toml based wheel builds),2023-03-28T23:31:02Z,pull ready,closed,2,0,https://github.com/jax-ml/jax/issues/15274
1403,"以下是一个github上的jax下的一个issue, 标题是(RuntimeError: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' (set JAX_PLATFORMS='' to automatically choose an available backend))， 内容是 ( Description %%capture !pip install upgrade pip  To support manylinux2010 wheels. !pip install upgrade jax jaxlib  CPUonly !pip install flax !pip install jax !pip install optax !pip install transformers !pip install datasets !pip install U jax[cuda11_cudnn82] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install U jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html !git clone https://github.com/huggingface/transformers.git !python run_t5_mlm_flax.py \ 	output_dir=""./norwegiant5base"" \ 	model_type=""t5"" \ 	config_name=""./norwegiant5base"" \ 	tokenizer_name=""./norwegiant5base"" \ 	dataset_name=""oscar"" \ 	dataset_config_name=""unshuffled_deduplicated_no"" \ 	max_seq_length=""512"" \ 	per_device_train_batch_size=""8"" \ 	per_device_eval_batch_size=""8"" \ 	adafactor \ 	learning_rate=""0.005"" \ 	weight_decay=""0.001"" \ 	warmup_steps=""2000"" \ 	overwrite_output_dir=True \ 	logging_steps=""500"" \ 	save_steps=""10000"" \ 	eval_steps=""2500"" \     num_train_epochs=3 \  What jax/jaxlib version are you usi)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,RuntimeError: Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client' (set JAX_PLATFORMS='' to automatically choose an available backend)," Description %%capture !pip install upgrade pip  To support manylinux2010 wheels. !pip install upgrade jax jaxlib  CPUonly !pip install flax !pip install jax !pip install optax !pip install transformers !pip install datasets !pip install U jax[cuda11_cudnn82] f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install U jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html !git clone https://github.com/huggingface/transformers.git !python run_t5_mlm_flax.py \ 	output_dir=""./norwegiant5base"" \ 	model_type=""t5"" \ 	config_name=""./norwegiant5base"" \ 	tokenizer_name=""./norwegiant5base"" \ 	dataset_name=""oscar"" \ 	dataset_config_name=""unshuffled_deduplicated_no"" \ 	max_seq_length=""512"" \ 	per_device_train_batch_size=""8"" \ 	per_device_eval_batch_size=""8"" \ 	adafactor \ 	learning_rate=""0.005"" \ 	weight_decay=""0.001"" \ 	warmup_steps=""2000"" \ 	overwrite_output_dir=True \ 	logging_steps=""500"" \ 	save_steps=""10000"" \ 	eval_steps=""2500"" \     num_train_epochs=3 \  What jax/jaxlib version are you usi",2023-03-28T22:29:58Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/15273,"I note you did this:  If your intent is to use TPU, try omitting the CUDA installation? I suspect you ended up with the CUDA jaxlib which does not support TPU.","Also note ""Google Colab"" Note that JAX has dropped support for Colab (Remote) TPUs in 0.4. If you want a hosted notebook with TPUs, you can either (a) downgrade to 0.3.25, or (b) use Kaggle TPUs, which are fully supported by JAX 0.4. Hope that helps!",Closing. Feel free to comment further if the suggestions above don't help.
472,"以下是一个github上的jax下的一个issue, 标题是(JAX doesn't work with cuda/gpu)， 内容是 ( Description Hi all, For a new project, I am trying to install JAX with cuda/gpu support. I installed cuda/cudnn using conda `cudatoolkit==11.7`. The same conda environment contains working pytorch and tensorflow installs, which seem to work on gpu. $ nvidiasmi )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JAX doesn't work with cuda/gpu," Description Hi all, For a new project, I am trying to install JAX with cuda/gpu support. I installed cuda/cudnn using conda `cudatoolkit==11.7`. The same conda environment contains working pytorch and tensorflow installs, which seem to work on gpu. $ nvidiasmi ",2023-03-28T19:29:18Z,bug NVIDIA GPU,closed,0,16,https://github.com/jax-ml/jax/issues/15268," This is the key message. Note: we found a CuDNN v8.5, but JAX was built against CuDNN v8.6. You need to update CuDNN. Is there an older CuDNN somewhere on your system?","Thanks for the fast response. Instead, I also tried installing JAX built with a lower CuDNN version from https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html using the command:  If I understand correctly, this build should work with `cudnn >= 8.2` (judging from the last error, my system has CuDNN v8.5). I am still getting an error, although it's a slightly different one:  Again, this is in the same conda env where pytorch and tensorflow seem to have no issue with finding cudnn. Thanks a lot!","In JAX 0.4.7, there is a new way to install jax that use pip packages for cuda stuff (except the driver). Can you use pip instead of conda to install it? `pip install upgrade ""jax[cuda12_pip]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html` It works for us, but wasn't widely tested. Do this in a fresh env, as it will install cuda 12 and you don't want both cuda to be installed in your env.",This seems to work. Thanks a lot!,"I don't believe this issue should be closed, as I have confirmed that I encounter the same problem when performing either cuda12_local or cuda11_local installations. So far, I have not been able to find a solution. It appears that the local installations are not functioning properly and may require attention.", can you post a repro with `nvidiasmi` output ... etc?, Using  nvidia/cuda:11.8.0cudnn8develubuntu20.04 docker image installing JAX  would result in the error.  I just verified that installing CuDNN manually solves it. Could this be a linking error?, what version of cudnn is present in that docker image?,ENV NV_CUDNN_VERSION=8.7.0.84 ENV NV_CUDNN_PACKAGE_NAME=libcudnn8 ENV NV_CUDNN_PACKAGE=libcudnn8=8.7.0.841+cuda11.8 ENV NV_CUDNN_PACKAGE_DEV=libcudnn8dev=8.7.0.841+cuda11.8 ," I cannot reproduce. On a cloud VM with an NVIDIA T4 GPU, I did this: ",  This part of the error message:  Seem to indicate there is a driver issues. I did a small PR to improve the error message a little bit: https://github.com/openxla/xla/pull/2335 But are you able to use other software on the GPU? Maybe reinstalling the driver can help you.,"Similar errors here with local install of cuda11.8 and cudnn 8.8 on Ubuntu 20.04 WSL2 and then used: pip install upgrade ""jax[cuda11_local]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html for install. As others have said, Tensorflow and the cudnn samples run without issue. Running any JAX code gives: `E external/xla/xla/stream_executor/cuda/cuda_driver.cc:1207] Failed to get stream capture info: operation not supported E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2432] Execution of replica 0 failed: INVALID_ARGUMENT: stream is uninitialized or in an error state` `++  ++`", WSL is communitysupported. This is not a configuration we test.,https://github.com/openxla/xla/pull/2335 is merged. So the error message should be better in the next release.,">  WSL is communitysupported. This is not a configuration we test. No problem. I will say that I have it running on 3 machines through WSL2 and it has generally ran perfectly. It was just this upgrade with the Titan RTX that seems to have caused issues (upgraded from 11.4 / 8.2 to 12.1 / 8.8 and the latest driver so could just be that combo on that card). Weird issue. 12.1 / 8.8 on the other 2 and they are 2060 and 3050 for GPU, respectively.",I don't think there are any outstanding issues that we have instructions to reproduce in this bug. Please open a new bug with instructions to reproduce if any of these issues still apply!
901,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Refactor the computation of the dimension variables in native serialization)， 内容是 (Currently, JAX native serialization produces a module whose main function takes additional arguments for the values of the dimension variables. These values are then resolved in the XlaCallModule based on a dim_args_spec parameter. We move the code that computes the dimension variables from XlaCallModule to jax_export following pretty much the same technique. This simplifies XlaCallModule and especially its API (the dim_args_spec). So far this is just a refactoring with no semantic changes, but this will allow us to improve the support for dimension variables that occur in linear polynomials, e.g., ""2*b"" rather than just ""b"".)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[shape_poly] Refactor the computation of the dimension variables in native serialization,"Currently, JAX native serialization produces a module whose main function takes additional arguments for the values of the dimension variables. These values are then resolved in the XlaCallModule based on a dim_args_spec parameter. We move the code that computes the dimension variables from XlaCallModule to jax_export following pretty much the same technique. This simplifies XlaCallModule and especially its API (the dim_args_spec). So far this is just a refactoring with no semantic changes, but this will allow us to improve the support for dimension variables that occur in linear polynomials, e.g., ""2*b"" rather than just ""b"".",2023-03-28T09:27:05Z,pull ready,closed,0,2,https://github.com/jax-ml/jax/issues/15258,  please take a look,"> LGTM. >  > Does this impact the Refine Shapes pass at all? I'm not too sure how the setup for that pass to be called works.  It does not, before the Refine Shapes the XlaCallModule will add a wrappper. But since dim_args_spec is empty there will be no wrapper added. Recently I have changed the XlaCallModule to run the shape refinement even if dim_args_spec is empty, expecting the change I am making now."
231,"以下是一个github上的jax下的一个issue, 标题是(Fix typo in betaln.py)， 内容是 (implmentation > implementation)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix typo in betaln.py,implmentation > implementation,2023-03-28T05:44:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15255
941,"以下是一个github上的jax下的一个issue, 标题是(Jax asks for non-existing device numbers)， 内容是 ( Description I am running JAX on a Slurm cluster with `jax.distributed.initialize`. If I ask for a number of process _lower or equal_ to the number of devices available, everything runs fine. When using more process than devices available on a node (trying to have several JAX process use the same GPU concurrently), initializing with `jax.distributed.initialize` leads to it asking for device numbers that do not exist due to this line which asks for a device number equal to the process id.  This fails causing JAX to fall back to `cpu` on some processes and produce this warning:  See also this issue.  What jax/jaxlib version are you using? jax0.4.6 jaxlib0.4.6+cuda11.cudnn86  Which accelerator(s) are you using? GPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax asks for non-existing device numbers," Description I am running JAX on a Slurm cluster with `jax.distributed.initialize`. If I ask for a number of process _lower or equal_ to the number of devices available, everything runs fine. When using more process than devices available on a node (trying to have several JAX process use the same GPU concurrently), initializing with `jax.distributed.initialize` leads to it asking for device numbers that do not exist due to this line which asks for a device number equal to the process id.  This fails causing JAX to fall back to `cpu` on some processes and produce this warning:  See also this issue.  What jax/jaxlib version are you using? jax0.4.6 jaxlib0.4.6+cuda11.cudnn86  Which accelerator(s) are you using? GPU",2023-03-24T23:31:38Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/15214
587,"以下是一个github上的jax下的一个issue, 标题是(Missing version.py file in /jax/jaxlib)， 内容是 ( Description I'm trying to install jaxlib through Poetry using the Github repository :   However, the install fails because `/jax/jaxlib/setup.py` refers to `/jax/jaxlib/version.py` which does not exist. See here.  What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? CPU  Additional system info MacOS Ventura  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Missing version.py file in /jax/jaxlib," Description I'm trying to install jaxlib through Poetry using the Github repository :   However, the install fails because `/jax/jaxlib/setup.py` refers to `/jax/jaxlib/version.py` which does not exist. See here.  What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? CPU  Additional system info MacOS Ventura  NVIDIA GPU info _No response_",2023-03-24T14:38:16Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/15202,"`jaxlib` cannot be built using the `setup.py` file. It must be built like this: https://jax.readthedocs.io/en/latest/developer.html The `setup.py` file is just a stub that gets used to construct the wheel, it does not know how to build. Hope that helps!","Alright, thanks. I'll find a way.","The recommended method fails for another reason:   on Termux.  (While the the base `jax` compiles and installs fine, OTOH:   and while `bazel build` works fine from the `jaxlib` folder.  My ugly fix, pacem Ms MS Copilot: > Title: Installation of jaxlib from source fails due to setup.py issues >  > Body: >  > I encountered several issues when trying to install jaxlib from source on my system. The original error was as follows: >  > FileNotFoundError: [Errno 2] No such file or directory: '/data/data/com.termux/files/home/downloads/jax/1/jax/jaxlib/jaxlib/version.py' >  > To resolve this, I had to make a hardcoded version check in setup.py. However, this led to another error: >  > NameError: name 'find_packages' is not defined >  > I was able to resolve this by importing find_packages from setuptools in setup.py. After making these modifications, I was able to successfully install jaxlib from source. >  More precisely:  +  ~/.../jax/jaxlib $ pip install v . Using pip 24.0 from /data/data/com.termux/files/usr/lib/python3.11/sitepackages/pip (python 3.11) Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com Processing /data/data/com.termux/files/home/downloads/jax/jaxlib   Running command python setup.py egg_info   running egg_info   creating /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo   writing /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/PKGINFO   writing dependency_links to /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/dependency_links.txt   writing requirements to /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/requires.txt   writing toplevel names to /data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/top_level.txt   writing manifest file '/data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/SOURCES.txt'   writing manifest file '/data/data/com.termux/files/usr/tmp/pippipegginfo16uncd2w/jaxlib.egginfo/SOURCES.txt'   Preparing metadata (setup.py) ... done Requirement already satisfied: scipy>=1.9 in /data/data/com.termux/files/usr/lib/python3.11/sitepackages (from jaxlib==0.4.29) (1.13.1) Requirement already satisfied: numpy>=1.22 in /data/data/com.termux/files/usr/lib/python3.11/sitepackages (from jaxlib==0.4.29) (1.26.4) Requirement already satisfied: ml_dtypes>=0.4.0 in /data/data/com.termux/files/usr/lib/python3.11/sitepackages (from jaxlib==0.4.29) (0.4.0) Building wheels for collected packages: jaxlib   Running command python setup.py bdist_wheel   running bdist_wheel   running build   running build_py   creating build   creating build/lib.linuxaarch64cpython311   creating build/lib.linuxaarch64cpython311/triton   copying triton/__init__.py > build/lib.linuxaarch64cpython311/triton   copying triton/dialect.py > build/lib.linuxaarch64cpython311/triton   running build_ext   /data/data/com.termux/files/usr/lib/python3.11/sitepackages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.   !!           ********************************************************************************           Please avoid running ``setup.py`` directly.           Instead, use pypa/build, pypa/installer or other           standardsbased tools.           See https://blog.ganssle.io/articles/2021/10/setuppydeprecated.html for details.           ********************************************************************************   !!     self.initialize_options()   installing to build/bdist.linuxaarch64/wheel   running install   running install_lib   creating build/bdist.linuxaarch64   creating build/bdist.linuxaarch64/wheel   creating build/bdist.linuxaarch64/wheel/triton   copying build/lib.linuxaarch64cpython311/triton/__init__.py > build/bdist.linuxaarch64/wheel/triton   copying build/lib.linuxaarch64cpython311/triton/dialect.py > build/bdist.linuxaarch64/wheel/triton   running install_egg_info   running egg_info   creating jaxlib.egginfo   writing jaxlib.egginfo/PKGINFO   writing dependency_links to jaxlib.egginfo/dependency_links.txt   writing requirements to jaxlib.egginfo/requires.txt   writing toplevel names to jaxlib.egginfo/top_level.txt   writing manifest file 'jaxlib.egginfo/SOURCES.txt'   writing manifest file 'jaxlib.egginfo/SOURCES.txt'   Copying jaxlib.egginfo to build/bdist.linuxaarch64/wheel/jaxlib0.4.29py3.11.egginfo   running install_scripts   creating build/bdist.linuxaarch64/wheel/jaxlib0.4.29.distinfo/WHEEL   creating '/data/data/com.termux/files/usr/tmp/pipwheelhn39ttzz/jaxlib0.4.29cp311cp311linux_aarch64.whl' and adding 'build/bdist.linuxaarch64/wheel' to it   adding 'triton/__init__.py'   adding 'triton/dialect.py'   adding 'jaxlib0.4.29.distinfo/METADATA'   adding 'jaxlib0.4.29.distinfo/WHEEL'   adding 'jaxlib0.4.29.distinfo/top_level.txt'   adding 'jaxlib0.4.29.distinfo/RECORD'   removing build/bdist.linuxaarch64/wheel   Building wheel for jaxlib (setup.py) ... done   Created wheel for jaxlib: filename=jaxlib0.4.29cp311cp311linux_aarch64.whl size=3425 sha256=52935f58e5020104d773ad385b27713ae5616fd53e6567d7af4516796a940f2f   Stored in directory: /data/data/com.termux/files/usr/tmp/pipephemwheelcacheq9nrs06h/wheels/ed/a6/ec/c71a34c5860c8c2d353799fb146d5baac62f79da6080c5b9cb Successfully built jaxlib Installing collected packages: jaxlib Successfully installed jaxlib0.4.29 ```"
1265,"以下是一个github上的jax下的一个issue, 标题是([dynamic-shapes] djax+pjit staging to jaxpr)， 内容是 (This is the first in a sequence of PRs making the dynamic shape machinery work with `pjit`, aka the new ""initialstyle"" `jit`. (Previously it only worked with the old ""finalstyle"" `jit`.) Concretely, we want to make all the tests in `tests/dynamic_api_test.py` work again (better than ever!). We're going to break this down into a sequence of steps: 1. [x] stagingtojaxpr e.g. with `jax.make_jaxpr` (this PR, CC([dynamicshapes] djax+pjit staging to jaxpr)) 3. [ ] paddedandmasked bintbased lowering and execution (e.g. with XLA, using our own paddingandmasking and/or using the HLO DynamicPadder) 4. [ ] dynamic shape compiler lowering and execution (e.g. with IREE) 5. [ ] autodiff e.g. with `jax.jvp`, `jax.linearize`, and `jax.grad` 6. [ ] batching e.g. with `jax.vmap` Once we get these things working again, we'll be able to start landing new features!  This PR tackles staging to dynamic shape jaxprs. The relevant tests, moved to their own `DynamicShapeStagingTest` class and unskipped in this PR, basically involve using `jax.make_jaxp)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[dynamic-shapes] djax+pjit staging to jaxpr,"This is the first in a sequence of PRs making the dynamic shape machinery work with `pjit`, aka the new ""initialstyle"" `jit`. (Previously it only worked with the old ""finalstyle"" `jit`.) Concretely, we want to make all the tests in `tests/dynamic_api_test.py` work again (better than ever!). We're going to break this down into a sequence of steps: 1. [x] stagingtojaxpr e.g. with `jax.make_jaxpr` (this PR, CC([dynamicshapes] djax+pjit staging to jaxpr)) 3. [ ] paddedandmasked bintbased lowering and execution (e.g. with XLA, using our own paddingandmasking and/or using the HLO DynamicPadder) 4. [ ] dynamic shape compiler lowering and execution (e.g. with IREE) 5. [ ] autodiff e.g. with `jax.jvp`, `jax.linearize`, and `jax.grad` 6. [ ] batching e.g. with `jax.vmap` Once we get these things working again, we'll be able to start landing new features!  This PR tackles staging to dynamic shape jaxprs. The relevant tests, moved to their own `DynamicShapeStagingTest` class and unskipped in this PR, basically involve using `jax.make_jaxp",2023-03-24T00:48:56Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15187
276,"以下是一个github上的jax下的一个issue, 标题是(Guard ArrayImpl checks by xla_extension_version.)， 内容是 (Guard ArrayImpl checks by xla_extension_version.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Guard ArrayImpl checks by xla_extension_version.,Guard ArrayImpl checks by xla_extension_version.,2023-03-23T21:09:24Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15181
375,"以下是一个github上的jax下的一个issue, 标题是(improve scan error messages)， 内容是 ( sniped me into this... Consider:  Before (hardwrapped to 80 columns for github):  After this PR (again hardwrapped manually):  Also consider:  Before:  After this PR: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,improve scan error messages, sniped me into this... Consider:  Before (hardwrapped to 80 columns for github):  After this PR (again hardwrapped manually):  Also consider:  Before:  After this PR: ,2023-03-23T20:43:29Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15178
1169,"以下是一个github上的jax下的一个issue, 标题是(Scan on forward fn blows TPU memory when not using unroll)， 内容是 ( Description Hi! I am using a jax.lax.scan over the inference call of a transformer model on a TPU v48. The function I am calling is:  that function is then called inside a jax.pmap over the TPU devices.  The call of this function works when using the unroll argument (with a long compilation time though as expected). However, when not using the unroll argument, it blows the TPU memory    which I think is unexpected as the function should not to take more memory because called inside a scan.   I tried to implement it with a python for loop (which is similar to using the unroll argument if I understand correctly) and it works with also a long compilation time, however, using a jax.lax.fori_loop or jax.lax.while_loop gives the same error.   What jax/jaxlib version are you using? v0.4.6 (get same error on v0.3.25)  Which accelerator(s) are you using? TPU v48  Additional system info Python 3.8  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Scan on forward fn blows TPU memory when not using unroll," Description Hi! I am using a jax.lax.scan over the inference call of a transformer model on a TPU v48. The function I am calling is:  that function is then called inside a jax.pmap over the TPU devices.  The call of this function works when using the unroll argument (with a long compilation time though as expected). However, when not using the unroll argument, it blows the TPU memory    which I think is unexpected as the function should not to take more memory because called inside a scan.   I tried to implement it with a python for loop (which is similar to using the unroll argument if I understand correctly) and it works with also a long compilation time, however, using a jax.lax.fori_loop or jax.lax.while_loop gives the same error.   What jax/jaxlib version are you using? v0.4.6 (get same error on v0.3.25)  Which accelerator(s) are you using? TPU v48  Additional system info Python 3.8  NVIDIA GPU info _No response_",2023-03-23T19:19:26Z,bug,open,2,2,https://github.com/jax-ml/jax/issues/15176,"Thanks for the question! Are you doing any autodiff, or is this pure inference?",Thanks for the quick reply! This is pure inference.
527,"以下是一个github上的jax下的一个issue, 标题是(Jax on Apple Silicon GPU)， 内容是 (Hi, I'm looking to get JAX to work on an Apple M2 GPU. Is this possible? I was able to configure JAX to work on the M2 CPU by configuring a conda environment with the following `.yaml` file. I then installed JAX with `pip install jax jaxlib` after running `conda activate` to activate that conda environment. **YAML file** )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax on Apple Silicon GPU,"Hi, I'm looking to get JAX to work on an Apple M2 GPU. Is this possible? I was able to configure JAX to work on the M2 CPU by configuring a conda environment with the following `.yaml` file. I then installed JAX with `pip install jax jaxlib` after running `conda activate` to activate that conda environment. **YAML file** ",2023-03-23T11:28:32Z,enhancement,closed,1,5,https://github.com/jax-ml/jax/issues/15164,Thanks for the question. See the related discussion in CC(Hardware acceleration on Apple Silicon with Metal plugin)," I just got a pretty powerful M2 Max (w 38 GPU cores), and I really like to try JAX. But it seems the solutions on some of the “Issues” here are CPUs. "," Thanks for the response! I don't quite follow all the discussion in that thread, but am I right that as of now there is no way to do this?",">  I just got a pretty powerful M2 Max (w 38 GPU cores), and I really like to try JAX. But it seems the solutions on some of the “Issues” here are CPUs. Agreed  I haven't been able to find anything about JAX on these GPUs. However, it is possible to get Tensorflow running on M2 GPUs (the `.yaml` environment I created above works), so hopefully this will be coming to JAX in the future..",jaxmetal plugin is the package to accelerate JAX on these GPUs. 
286,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Fix tests broken by upgrade of XlaCallModule)， 内容是 ([jax2tf] Fix tests broken by upgrade of XlaCallModule)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Fix tests broken by upgrade of XlaCallModule,[jax2tf] Fix tests broken by upgrade of XlaCallModule,2023-03-23T09:08:44Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15161
1275,"以下是一个github上的jax下的一个issue, 标题是(Add initial implementation of a `run_state` primitive)， 内容是 ( Background  Side effects and types We often say JAX is inspired by functional programming when discussing the requirement that programs in JAX be *pure* to be transformed. However, taking more inspiration from functional programming, we could lift that requirement. Specifically, JAX could allow sideeffects, but only effects that JAX's type system can track and ones that have welldefined interactions with transformations. What does it mean to track effects in the type system? Consider how the Koka programming language incorporates effects into its type system:  In Koka, the type of a function not only contains its input and output types, but also a list of effect types. For example, the print function has the `console` effect and the `randr` function has the `ndet` effect. Functions can have multiple effects, which often happens when effectful functions are composed. At the Jaxpr level, JAX currently similarly tracks effect types (specifically the effects field in Jaxprs is the set of side effects associated with the Jaxpr). W)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add initial implementation of a `run_state` primitive," Background  Side effects and types We often say JAX is inspired by functional programming when discussing the requirement that programs in JAX be *pure* to be transformed. However, taking more inspiration from functional programming, we could lift that requirement. Specifically, JAX could allow sideeffects, but only effects that JAX's type system can track and ones that have welldefined interactions with transformations. What does it mean to track effects in the type system? Consider how the Koka programming language incorporates effects into its type system:  In Koka, the type of a function not only contains its input and output types, but also a list of effect types. For example, the print function has the `console` effect and the `randr` function has the `ndet` effect. Functions can have multiple effects, which often happens when effectful functions are composed. At the Jaxpr level, JAX currently similarly tracks effect types (specifically the effects field in Jaxprs is the set of side effects associated with the Jaxpr). W",2023-03-22T21:57:54Z,pull ready,closed,5,5,https://github.com/jax-ml/jax/issues/15149,This is it! We've jumped the shark!," Great work and thank you for the detailed writeup! BTW, I think the type of `run_state` should have been ",">  Great work and thank you for the detailed writeup! Thanks! > BTW, I think the type of `run_state` should have been >  >  Fixed, thanks for the catch.", Best PR doc I have ever seen !,The PR description should land in a JEP :) 
1205,"以下是一个github上的jax下的一个issue, 标题是(Running with more process than devices)， 内容是 ( Description I am running JAX on a Slurm cluster with `jax.distributed.initialize`. If I ask for a number of process _lower or equal_ to the number of devices available, everything runs fine. When using more process than devices available on a node (trying to have several JAX process use the same GPU concurently), initializing with `jax.distributed.initialize` leads to it asking for device numbers that do not exist (due to this line which asks for a device number equal to the process id). This fails causing JAX to fall back to `cpu` on some processes. Trying to pass a device number manually via `local_device_ids` causes a (seemingly unrelated) `RET_CHECK failure` down the line:  Can the behavior of `jax.distributed.initialize` be fixed to allow for more processes than devices (or fail in a straightforward way, letting the user know that it is not a usecase)?  What jax/jaxlib version are you using? jax0.4.6 jaxlib0.4.6+cuda11.cudnn86  Which accelerator(s) are you using? GPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Running with more process than devices," Description I am running JAX on a Slurm cluster with `jax.distributed.initialize`. If I ask for a number of process _lower or equal_ to the number of devices available, everything runs fine. When using more process than devices available on a node (trying to have several JAX process use the same GPU concurently), initializing with `jax.distributed.initialize` leads to it asking for device numbers that do not exist (due to this line which asks for a device number equal to the process id). This fails causing JAX to fall back to `cpu` on some processes. Trying to pass a device number manually via `local_device_ids` causes a (seemingly unrelated) `RET_CHECK failure` down the line:  Can the behavior of `jax.distributed.initialize` be fixed to allow for more processes than devices (or fail in a straightforward way, letting the user know that it is not a usecase)?  What jax/jaxlib version are you using? jax0.4.6 jaxlib0.4.6+cuda11.cudnn86  Which accelerator(s) are you using? GPU",2023-03-22T21:07:07Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/15146,"After some trial and error I found the answer! To get it working you need to: * set XLA_PYTHON_CLIENT_MEM_FRACTION to avoid having processes ask for more memory than available, * do not _call_ a Jax function before calling `jax.distributed.initialize`[^0], * pass the `device_id` manually to `jax.distributed.initialize` (`jax.distributed.initialize(local_device_ids=[device_id])`). Note that the behavior when using more processes than devices could still be improved. [^0]: It took me a long time to realize that I needed to get rid of a call to `jax.device_count`...", for viz
1261,"以下是一个github上的jax下的一个issue, 标题是(Use options when creating a PJRT client)， 内容是 (Now the PJRT Client API can receive options, so JAX can pass the following information as an option.  replica_id  partition_id  num_replicas  num_partitions I believe the number of processes and process_id can be derived from the info above. (But I am okay to have them as option too). The information is needed to set up the NCCL library in the IREE runtime through the PJRT Client. These are some source code locations I got from Jieying Luo about the newly added options.  If you are calling the C API PJRT_Client_Create, you can set these two args https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/c/pjrt_c_api.hL222L223.  Note your implementation of PJRT_Client_Create needs to handle how to use the options.  If you are calling GetCApiClient, you can pass in an absl::flat_hash_map https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/pjrt_c_api_client.hL581.  If you are calling from jax, you can pass option in make_c_api_client https://github.com/openxla/xla/blob/)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Use options when creating a PJRT client,"Now the PJRT Client API can receive options, so JAX can pass the following information as an option.  replica_id  partition_id  num_replicas  num_partitions I believe the number of processes and process_id can be derived from the info above. (But I am okay to have them as option too). The information is needed to set up the NCCL library in the IREE runtime through the PJRT Client. These are some source code locations I got from Jieying Luo about the newly added options.  If you are calling the C API PJRT_Client_Create, you can set these two args https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/c/pjrt_c_api.hL222L223.  Note your implementation of PJRT_Client_Create needs to handle how to use the options.  If you are calling GetCApiClient, you can pass in an absl::flat_hash_map https://github.com/openxla/xla/blob/86ed98e42bf7735547ff89d03437b2ad43284c45/xla/pjrt/pjrt_c_api_client.hL581.  If you are calling from jax, you can pass option in make_c_api_client https://github.com/openxla/xla/blob/",2023-03-20T22:39:26Z,enhancement,open,0,0,https://github.com/jax-ml/jax/issues/15106
1332,"以下是一个github上的jax下的一个issue, 标题是(Custom CPU/GPU op with single return value results in ValueError ""Output of translation rule must be iterable"")， 内容是 ( Description I ran into this problem trying to register a custom operation (implemented in C++) that only returns a single output. Mockup code for the jax side:  The above fails with a `ValueError` with message  This is caused by the implementation in hlo_helpers.custom_call , which returns a `OpResult` instance if only a single output is specified in the lowering rule, i.e., the length of `out_types` is one. (If the length is larger than one, this function instead returns `List[OpResult]`.) This then leads to a crash somewhere upstream in the translation stack, i.e., in  jaxpr_subcomp, which expects to _always_ get an iterable and will throw the above exception if this is not the case. Note that the setting of `test_p.multiple_results = False` doesn't factor in here. The behaviour of `custom_call` is only dependent on the length its `out_types` argument, which is in turn determined by the native/C side implementation. As a workaround, I tried to add an additional empty return value (i.e., an array of length 0), but that resul)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Custom CPU/GPU op with single return value results in ValueError ""Output of translation rule must be iterable"""," Description I ran into this problem trying to register a custom operation (implemented in C++) that only returns a single output. Mockup code for the jax side:  The above fails with a `ValueError` with message  This is caused by the implementation in hlo_helpers.custom_call , which returns a `OpResult` instance if only a single output is specified in the lowering rule, i.e., the length of `out_types` is one. (If the length is larger than one, this function instead returns `List[OpResult]`.) This then leads to a crash somewhere upstream in the translation stack, i.e., in  jaxpr_subcomp, which expects to _always_ get an iterable and will throw the above exception if this is not the case. Note that the setting of `test_p.multiple_results = False` doesn't factor in here. The behaviour of `custom_call` is only dependent on the length its `out_types` argument, which is in turn determined by the native/C side implementation. As a workaround, I tried to add an additional empty return value (i.e., an array of length 0), but that resul",2023-03-20T15:36:14Z,bug,closed,1,6,https://github.com/jax-ml/jax/issues/15095,"I think this is actually working as intended. `custom_call` is a private helper, but it is supposed to return a single IR value if there is only one, not a sequence. The fix is to change your lowering rule to return an arity1 tuple, i.e., ",Ha. Sometimes the simplest solutions are the ones you don't find :/ Many thanks! (I still find this behaviour of `custom_call` a bit counterintuitive and documentation on how to use it could be improved.),"Well, it's supposed to be a private helper, which is why it's not documented. We have plans to make it easier to plug in custom kernels without having to write lowering rules, which is I think what you actually wanted here?","I had a related issue, but this fix doesn't seem to work for `jax v0.4.3` and `jaxlib v0.4.3`. Is this expected behavior? The error I get happens on these lines in `core.py`:  Even after updating to jax v0.4.6, jaxlib v0.4.4 I still have the issue. However, I am using `python3.9`, not sure if that has anything to do with it?","Hi   it looks like you're passing a tuple in a location where an array is expected. Can you please open a new issue for your question, and include a minimal reproducible example? Thanks!","> Hi   it looks like you're passing a tuple in a location where an array is expected. Can you please open a new issue for your question, and include a minimal reproducible example? Thanks! I think I realized what my problem was. I was able to track the issue down to my abstract evaluation function:  which caused the issue I believe.  After removing the tuple from the return statement, I got rid of the error. I am wondering however, whether this abstract evaluation is correct if one is returning a pure double value from the corresponding `custom_call`? More details on the minimal example are in this issue."
526,"以下是一个github上的jax下的一个issue, 标题是(How to replay an object_  replay_buffer become types that support jax data)， 内容是 (Please:  I want to pass it as an incoming parameter to the function：  But the update function requires jit acceleration  Error will be reported when I run directly  Thank you very much for your help!Thank you very much for your help!Thank you very much for your help!！！！！！)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,How to replay an object_  replay_buffer become types that support jax data,Please:  I want to pass it as an incoming parameter to the function：  But the update function requires jit acceleration  Error will be reported when I run directly  Thank you very much for your help!Thank you very much for your help!Thank you very much for your help!！！！！！,2023-03-20T13:38:33Z,enhancement,open,0,2,https://github.com/jax-ml/jax/issues/15093,"Hi, thanks for the question! It's hard to tell how to help without more information. Could you edit your question to add a minimal reproducible example? Package imports are important: for example I have no idea what `ReplayBuffer` is or what package it might be coming from. Thanks!","The ReplayBuffer is a class used for experience replay in reinforcement learning, and an object of this class is created as replay_buffer. I want to pass this replay_buffer object as an input parameter to a jitaccelerated update function, but the replay_buffer object does not support the data format of jax. How can I convert the replay_buffer to a data format that supports jax? Thank you for your help. Here is the general content of the ReplayBuffer class. `class ReplayBuffer(Dataset):     def __init__(self,                  observation_space: gym.Space,                  action_space: gym.Space,                  capacity: int,                  alpha:float = 0.6,                  beta_start:float = 0.4,                  beta_frames: float = 1e6,                  next_observation_space: Optional[gym.Space] = None):         if next_observation_space is None:             next_observation_space = observation_space         observation_data = _init_replay_dict(observation_space, capacity)         next_observation_data = _init_replay_dict(next_observation_space,                                                   capacity)         dataset_dict = dict(             observations=observation_data,             next_observations=next_observation_data,             actions=np.empty((capacity, *action_space.shape),                              dtype=action_space.dtype),             rewards=np.empty((capacity, ), dtype=np.float32),             dones=np.empty((capacity, ), dtype=bool),         )         super().__init__(dataset_dict)         self._size = 0         self._capacity = capacity         self._insert_index = 0         self.alpha = alpha         self.beta_start = beta_start         self.beta_frames = beta_frames         self.frame = 1         self.capacity = capacity         self.buffer = []         self.pos = 0         self.priorities = np.zeros((capacity,),dtype = np.float32)     def beta_by_frames(self, frame_idx):         return min(1.0, self.beta_start + frame_idx * (1.0  self.beta_start) / self.beta_frames)     def insert(self, data_dict: DatasetDict):         state=data_dict['observations']         next_state=data_dict['next_observations']         assert state.ndim == next_state.ndim         state = np.expand_dims(state, 0)         next_state = np.expand_dims(next_state, 0)         max_prio = self.priorities.max() if self.buffer else 1.0   gives max priority if buffer is not empty else 1         if len(self.buffer)  cap > new posi = 0     def __len__(self) > int:         return self.buffer     def update_priorities(self, batch_indices, batch_priorities):          for idx, prio in zip(batch_indices, batch_priorities):             self.priorities[idx] = abs(prio)     def sample(self, batch_size)> frozen_dict.FrozenDict:         N = len(self.buffer)         if N == self.capacity:             prios = self.priorities         else:             prios = self.priorities[:self.pos]          calc P = p^a/sum(p^a)         probs = prios ** self.alpha         P = probs / probs.sum()          gets the indices depending on the probability p         indices = np.random.choice(N, batch_size, p=P)         samples = [self.buffer[idx] for idx in indices]         beta = self.beta_by_frames(self.frame)         self.frame += 1          Compute importancesampling weight         weights = (N * P[indices]) ** (beta)          normalize weights         weights /= weights.max()         weights = np.array(weights, dtype=np.float32)         observations, actions, rewards, next_observations, dones = zip(*samples)         batch = {'observations': observations,                          'actions': actions,                             'rewards': rewards,                             'next_observations': next_observations,                             'dones': dones,                             'indices': indices,                             'weights': weights}         print(batch['weights'])         for k,v in batch.items():             batch[k] = np.array(v)         return frozen_dict.freeze(batch)`"
734,"以下是一个github上的jax下的一个issue, 标题是(JAX RNG clashes with PyTorch on GPU)， 内容是 ( Description If you import pytorch code (that uses dataloaders) before trying to create a jax PRNG key, you get an obscure error  If you execute ` key = jax.random.PRNGKey(0)` before importing pytorch, it works fine. (The problem only occurs on GPU, not CPU or TPU.) Code to reproduce this: https://colab.sandbox.google.com/drive/1gGDAGotSA0aJ9Nd1ilV9_xWBy6uFvVkU?usp=sharing   What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,JAX RNG clashes with PyTorch on GPU," Description If you import pytorch code (that uses dataloaders) before trying to create a jax PRNG key, you get an obscure error  If you execute ` key = jax.random.PRNGKey(0)` before importing pytorch, it works fine. (The problem only occurs on GPU, not CPU or TPU.) Code to reproduce this: https://colab.sandbox.google.com/drive/1gGDAGotSA0aJ9Nd1ilV9_xWBy6uFvVkU?usp=sharing   What jax/jaxlib version are you using? 0.4.6  Which accelerator(s) are you using? GPU  Additional system info _No response_  NVIDIA GPU info ",2023-03-19T20:30:56Z,bug NVIDIA GPU,open,1,6,https://github.com/jax-ml/jax/issues/15084,"I suspect this means ""JAX ran out of memory"". See https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html We clearly need to improve the error message at least.","But if I create the JAX PRNG key first, it all works fine. So I don't think it's just a memory issue.",", my understanding of PyTorch memory allocation is not great; , when Pytorch is run/imported, how much does it take of the GPU memory by default?","> when Pytorch is run/imported, how much does it take of the GPU memory by default? No GPU memory will be used by default. The first CUDA operation triggered by PyTorch will create the CUDA context, which will allocate memory. The size of the context depends on the used GPU architecture, the CUDA version, if lazy module loading is enabled etc. PyTorch then uses the internal caching allocator to allocate new device memory when needed and to reuse it when possible. It has thus a growing memory usage. The cache can manually be freed and the current allocations can be checked via `print(torch.cuda.memory_summary())`.", are you able to share the PyTorch allocation when tryin to call `jax.random.PRNGKey()`? ,"Hi   I tried to run the shared colab notebook with latest JAX version 0.4.23, PyTorch 2.1.0+cu121 and T4 GPU. It executed without any error. Kindly find the gist for reference. Thank you."
1033,"以下是一个github上的jax下的一个issue, 标题是(random.* docs are not specific enough about probability density function)， 内容是 ( Description jax.random.normal and other jax.random.* should specify the actual probability density function (see numpy.random.normal for an example).  Otherwise there is the potential for confusion (see e.g. https://math.stackexchange.com/questions/1013575/continuousprobabilityaveragepowerofagaussianrandomvariablemathcaln) For the doc fix, one possibility would be to put the PDF equation inline in the docs, as numpy.random.normal does.  Another possibility would be to state that the PDF is the same as some numpy routine.   For jax.random.normal, that would be numpy.random.normal(loc=0.0, scale=1.0).  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,random.* docs are not specific enough about probability density function," Description jax.random.normal and other jax.random.* should specify the actual probability density function (see numpy.random.normal for an example).  Otherwise there is the potential for confusion (see e.g. https://math.stackexchange.com/questions/1013575/continuousprobabilityaveragepowerofagaussianrandomvariablemathcaln) For the doc fix, one possibility would be to put the PDF equation inline in the docs, as numpy.random.normal does.  Another possibility would be to state that the PDF is the same as some numpy routine.   For jax.random.normal, that would be numpy.random.normal(loc=0.0, scale=1.0).  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-03-19T19:22:41Z,documentation,closed,0,1,https://github.com/jax-ml/jax/issues/15083,"Thanks, this is a great suggestion."
1286,"以下是一个github上的jax下的一个issue, 标题是(make mlir arg and result names work with static_argnums/argnames)， 内容是 (This is the first step in a revision to how we handle the debug info pertaining to staged functions' parameter names and result pytree paths. To limit complexity, this first step adds machinery required to make our MLIR lowerings' parameter and result names work, but it does *not* yet unify it with existing argname machinery used at tracing time (in partial_eval.py, e.g. `partial_eval.DebugInfo` etc). That unification will come in a follow up commits. (I wrote the unified version first, then broke it down into this sequence of commits.) Another thing that will arrive in followup commits is `pmap` support (handling `static_broadcasted_argnums`). This PR doesn't include support for `pmap` because `pmap`'s final style implementation requires slightly different machinery than jit/pjit's initial style implementation. Indeed this PR removes the previous support for `pmap` arg/result info, and skips the corresponding tests, because the previous support didn't handle `pmap`'s `static_broadcasted_argnums` (and I think it could even lea)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,make mlir arg and result names work with static_argnums/argnames,"This is the first step in a revision to how we handle the debug info pertaining to staged functions' parameter names and result pytree paths. To limit complexity, this first step adds machinery required to make our MLIR lowerings' parameter and result names work, but it does *not* yet unify it with existing argname machinery used at tracing time (in partial_eval.py, e.g. `partial_eval.DebugInfo` etc). That unification will come in a follow up commits. (I wrote the unified version first, then broke it down into this sequence of commits.) Another thing that will arrive in followup commits is `pmap` support (handling `static_broadcasted_argnums`). This PR doesn't include support for `pmap` because `pmap`'s final style implementation requires slightly different machinery than jit/pjit's initial style implementation. Indeed this PR removes the previous support for `pmap` arg/result info, and skips the corresponding tests, because the previous support didn't handle `pmap`'s `static_broadcasted_argnums` (and I think it could even lea",2023-03-18T22:19:45Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15080
1104,"以下是一个github上的jax下的一个issue, 标题是(separate register_pytree_node and register_pytree_with_keys tests)， 内容是 (In CC(Add JAX pytree key path API to tree_utils.) we added tests for `register_pytree_with_keys` etc, but because those replaced some `register_pytree_node` tests (e.g. replacing `register_pytree_node(AnObject, ...)` with `register_pytree_with_keys(AnObject, ...)`), it meant we had less test coverage of the preexisting `register_pytree_node` path. The aim of this PR is to restore the `register_pytree_node` tests that existed before CC(Add JAX pytree key path API to tree_utils.), but also to keep all the new test coverage as well. The basic approach is just to: 1. put back `register_pytree_node(AnObject, ...)` as it was before CC(Add JAX pytree key path API to tree_utils.); 2. introduce the `AnObject2` trivial subclass and to apply `register_pytree_with_keys(AnObject2, ...)` to it; 3. include `AnObject2` in both pathful and nonpathful pytree tests.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,separate register_pytree_node and register_pytree_with_keys tests,"In CC(Add JAX pytree key path API to tree_utils.) we added tests for `register_pytree_with_keys` etc, but because those replaced some `register_pytree_node` tests (e.g. replacing `register_pytree_node(AnObject, ...)` with `register_pytree_with_keys(AnObject, ...)`), it meant we had less test coverage of the preexisting `register_pytree_node` path. The aim of this PR is to restore the `register_pytree_node` tests that existed before CC(Add JAX pytree key path API to tree_utils.), but also to keep all the new test coverage as well. The basic approach is just to: 1. put back `register_pytree_node(AnObject, ...)` as it was before CC(Add JAX pytree key path API to tree_utils.); 2. introduce the `AnObject2` trivial subclass and to apply `register_pytree_with_keys(AnObject2, ...)` to it; 3. include `AnObject2` in both pathful and nonpathful pytree tests.",2023-03-18T21:38:51Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15079
770,"以下是一个github上的jax下的一个issue, 标题是([pytrees] fix function underlying tree-flattening with keys)， 内容是 (There were two bugs in the `_generate_key_paths` function underlying `tree_flatten_with_path`, leading to disagreement between `len(tree_flatten(x)[0])` and `len(tree_flatten_with_path(x)[0])` for some `x` 1. pytree nodes that weren't registered as pytreenodeswithkeys were treated as leaves 2. namedtuples that were registered as pytree nodes were being flattened as generic namedtuples rather than using the explicitly registered flattener It'd be nice to have some kind of propertybased test to ensure that for all `x` we have: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[pytrees] fix function underlying tree-flattening with keys,"There were two bugs in the `_generate_key_paths` function underlying `tree_flatten_with_path`, leading to disagreement between `len(tree_flatten(x)[0])` and `len(tree_flatten_with_path(x)[0])` for some `x` 1. pytree nodes that weren't registered as pytreenodeswithkeys were treated as leaves 2. namedtuples that were registered as pytree nodes were being flattened as generic namedtuples rather than using the explicitly registered flattener It'd be nice to have some kind of propertybased test to ensure that for all `x` we have: ",2023-03-18T02:13:04Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/15071
1082,"以下是一个github上的jax下的一个issue, 标题是(Why does `lax.reshape_p` carry `dimensions`(info. about transposition)?)， 内容是 (Currently, the `lax.reshape_p` primitive carries two pieces of information:  `new_sizes`: the shape of the output  `dimensions`: a permutation representing the transposition to the input before actually applying the reshape. Historical, XLA's `xla.ReshapeOp` takes the `dimensions`, but now `stablehlo.ReshapeOp` only takes `new_sizes`. As consequence, currently, logic for `transpose_p` is inlined in `_reshape_{batch...}_rule` just to handle the case where `dimensions is not None`. I think we can do better by simply the entrypoint for `lax.reshape_p` which is `lax.reshape`, and in turn simplify all `lax.reshape_p`'s rules:  As a data point, `_reshape_typecheck_rule` already generate the decomposed form in Jaxpr. My question is: is there a good reason why we want to carry the transposition information with `lax.reshape_p`?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Why does `lax.reshape_p` carry `dimensions`(info. about transposition)?,"Currently, the `lax.reshape_p` primitive carries two pieces of information:  `new_sizes`: the shape of the output  `dimensions`: a permutation representing the transposition to the input before actually applying the reshape. Historical, XLA's `xla.ReshapeOp` takes the `dimensions`, but now `stablehlo.ReshapeOp` only takes `new_sizes`. As consequence, currently, logic for `transpose_p` is inlined in `_reshape_{batch...}_rule` just to handle the case where `dimensions is not None`. I think we can do better by simply the entrypoint for `lax.reshape_p` which is `lax.reshape`, and in turn simplify all `lax.reshape_p`'s rules:  As a data point, `_reshape_typecheck_rule` already generate the decomposed form in Jaxpr. My question is: is there a good reason why we want to carry the transposition information with `lax.reshape_p`?",2023-03-17T11:18:50Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/15053,"I tagged Peter because he might have more context on this... I suspect the reason this parameter still exists is because it used to be a requirement, and now removing it would require fixes to a number of downstream users. Peter, should we think about deprecating this in favor of explicit transposition?"
539,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Refactor the backwards compatibility tests.)， 内容是 ([jax2tf] Refactor the backwards compatibility tests. Move the test data into a separate directory, otherwise the test file will get too big. Keep only `dict` in the test data. Also found and fixed a bug where the xla_call_module was running on CPU even if there was a TPU available. Fix with tf.device(...).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Refactor the backwards compatibility tests.,"[jax2tf] Refactor the backwards compatibility tests. Move the test data into a separate directory, otherwise the test file will get too big. Keep only `dict` in the test data. Also found and fixed a bug where the xla_call_module was running on CPU even if there was a TPU available. Fix with tf.device(...).",2023-03-17T08:26:27Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15050
1250,"以下是一个github上的jax下的一个issue, 标题是(Regression in JAX while_loop)， 内容是 ( Description The following gist leads to an infinite loop with jaxlib==0.4.6 even though it is guaranteed to terminate and does exactly that with jaxlib==0.4.4: https://gist.github.com/Edenhofer/ece9a2e3e8c67721dbdd706b3966f04c . The code enters the infinite loop on the following CPUs AMD Ryzen 7 4800H, AMD Ryzen 5 PRO 3400G, AMD Opteron Processor 6376 . On an Intel Xeon Gold 6150 CPU the example runs fine. However, a more elaborate version of the example also leads to an infinite loop on Intel hardware. The gist reimplements the subproblem solver for a trust region Newton conjugate gradient scheme. It is conceptually similar to the scipy implementation. The error only arises for the eggholder potential and is not reproducible for any other of the simple cost function shown in the repro. Apologies for not being able to trim down the reproducing example even further. The error seems to be very brittle and slight modifications make it work again although oftentimes yielding wrong results.  What jax/jaxlib version are you using? )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Regression in JAX while_loop," Description The following gist leads to an infinite loop with jaxlib==0.4.6 even though it is guaranteed to terminate and does exactly that with jaxlib==0.4.4: https://gist.github.com/Edenhofer/ece9a2e3e8c67721dbdd706b3966f04c . The code enters the infinite loop on the following CPUs AMD Ryzen 7 4800H, AMD Ryzen 5 PRO 3400G, AMD Opteron Processor 6376 . On an Intel Xeon Gold 6150 CPU the example runs fine. However, a more elaborate version of the example also leads to an infinite loop on Intel hardware. The gist reimplements the subproblem solver for a trust region Newton conjugate gradient scheme. It is conceptually similar to the scipy implementation. The error only arises for the eggholder potential and is not reproducible for any other of the simple cost function shown in the repro. Apologies for not being able to trim down the reproducing example even further. The error seems to be very brittle and slight modifications make it work again although oftentimes yielding wrong results.  What jax/jaxlib version are you using? ",2023-03-16T18:49:32Z,bug,closed,1,1,https://github.com/jax-ml/jax/issues/15035,Fixed in jax (and jaxlib) version 0.4.28 or earlier.
406,"以下是一个github上的jax下的一个issue, 标题是(Remove the check for `if not isinstance(old_token, array.ArrayImpl)` since py_executable always return jax.Arrays)， 内容是 (Remove the check for `if not isinstance(old_token, array.ArrayImpl)` since py_executable always return jax.Arrays)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Remove the check for `if not isinstance(old_token, array.ArrayImpl)` since py_executable always return jax.Arrays","Remove the check for `if not isinstance(old_token, array.ArrayImpl)` since py_executable always return jax.Arrays",2023-03-16T00:07:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/15022
434,"以下是一个github上的jax下的一个issue, 标题是([sparse] add BCOO lowering for div)， 内容是 (We had avoiding this previously because dividing by zero is a densifying operation, but we already support mul which has similar issues if the operand contains infinities. Fixes CC(Sparse rule for div is not implemented))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] add BCOO lowering for div,"We had avoiding this previously because dividing by zero is a densifying operation, but we already support mul which has similar issues if the operand contains infinities. Fixes CC(Sparse rule for div is not implemented)",2023-03-14T18:59:23Z,pull ready,closed,1,0,https://github.com/jax-ml/jax/issues/14987
485,"以下是一个github上的jax下的一个issue, 标题是([dynamic-shapes] don't require buf objects have dtype attribute)， 内容是 (Fixes ireeorg/ireejax CC(v0.2 tasks) An alternative fix would've been just to add the dtype attribute to IreeBuffer. But it seems better not to make demands on the underlying runtime objects when we don't need to. I had to run the test with: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[dynamic-shapes] don't require buf objects have dtype attribute,Fixes ireeorg/ireejax CC(v0.2 tasks) An alternative fix would've been just to add the dtype attribute to IreeBuffer. But it seems better not to make demands on the underlying runtime objects when we don't need to. I had to run the test with: ,2023-03-14T18:19:19Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14986
1278,"以下是一个github上的jax下的一个issue, 标题是([FR] support `jax.distributed` for a subset of TPU cores)， 内容是 (Hello Jax team, I have noticed that `jax.distributed` currently only supports a subset of GPU cores, and I was wondering if it would be possible to extend this support to a subset of TPU cores as well. I have tried running a code snippet with jax.pmap on TPUs using the suggested snippet from  in https://github.com/google/jax/issues/8809issuecomment988495501, but it only recognizes the local TPU device and not other devices, resulting in a failed psum operation. For comparison, I have also included the output of the same code snippet when run on GPUs for reference.  On GPUs, when running with   produces  on TPUs, when running with the snippet suggested by  at https://github.com/google/jax/issues/8809issuecomment988495501  we get the following.  Note that the TPU example does not recognize other devices, thus making the `psum` fails.   Use cases We are using `jax.distributed` this way in our new distributed RL platform Cleanba, which supports different ways to leverage accelerators. Specifically, the users can only use 1 acceler)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[FR] support `jax.distributed` for a subset of TPU cores,"Hello Jax team, I have noticed that `jax.distributed` currently only supports a subset of GPU cores, and I was wondering if it would be possible to extend this support to a subset of TPU cores as well. I have tried running a code snippet with jax.pmap on TPUs using the suggested snippet from  in https://github.com/google/jax/issues/8809issuecomment988495501, but it only recognizes the local TPU device and not other devices, resulting in a failed psum operation. For comparison, I have also included the output of the same code snippet when run on GPUs for reference.  On GPUs, when running with   produces  on TPUs, when running with the snippet suggested by  at https://github.com/google/jax/issues/8809issuecomment988495501  we get the following.  Note that the TPU example does not recognize other devices, thus making the `psum` fails.   Use cases We are using `jax.distributed` this way in our new distributed RL platform Cleanba, which supports different ways to leverage accelerators. Specifically, the users can only use 1 acceler",2023-03-14T14:49:35Z,enhancement,closed,0,2,https://github.com/jax-ml/jax/issues/14977,"Hi, sorry for the delay on this! I'm not 100% clear on the exact configuration you're trying to run. I updated https://gist.github.com/skye/f82ba45d2445bb19d53545538754f9a3 with a few more examples of communicating processes. For example, to run 2 communicating processes each with 2 chips:   yields:  Note that for communicating processes, you must have an equal number of chips in each process. Please let me know if this answers your question, or if you're trying to do something else!","That you so much ! This is precisely what I needed! My experiments show that in my particular setting, 8 TPUv4 cores perform similarly to 8 A100 cores. Closing this issue now. "
299,"以下是一个github上的jax下的一个issue, 标题是([Rollforward] Move PyBuffer methods used by PyArray to c++.)， 内容是 ([Rollforward] Move PyBuffer methods used by PyArray to c++. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Rollforward] Move PyBuffer methods used by PyArray to c++.,[Rollforward] Move PyBuffer methods used by PyArray to c++. ,2023-03-13T23:30:11Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14963
553,"以下是一个github上的jax下的一个issue, 标题是(Command is longer than CreateProcessW's limit error)， 内容是 ( Description When building attempting to build JAX on Windows with CUDA support, I run into the following error.    What jax/jaxlib version are you using? jaxlib v0.4.6, jax 0.4.6  Which accelerator(s) are you using? GPU  Additional system info Windows 10, Python 3.9, Cuda 11.7, Cudnn 8.4.0  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Command is longer than CreateProcessW's limit error," Description When building attempting to build JAX on Windows with CUDA support, I run into the following error.    What jax/jaxlib version are you using? jaxlib v0.4.6, jax 0.4.6  Which accelerator(s) are you using? GPU  Additional system info Windows 10, Python 3.9, Cuda 11.7, Cudnn 8.4.0  NVIDIA GPU info _No response_",2023-03-13T16:09:15Z,bug build contributions welcome NVIDIA GPU Windows,closed,0,2,https://github.com/jax-ml/jax/issues/14950,This is still an issue in jaxlib v0.4.7,I think its related to this.... https://github.com/bazelbuild/bazel/issues/5163
605,"以下是一个github上的jax下的一个issue, 标题是(Improve pytype inference for Sharding type.)， 内容是 (Improve pytype inference for Sharding type. * Define use_cpp_class and use_cpp_method decorators as noops for type checking. * Remove the use of abc.ABC when defining the Sharding type. This triggers a pytype bug: the easiest fix seems to be to skip the use of the ABC. * Write use_cpp_class decorator differently on ArrayImpl to work around pytype bug. * Fix a few new type errors.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improve pytype inference for Sharding type.,Improve pytype inference for Sharding type. * Define use_cpp_class and use_cpp_method decorators as noops for type checking. * Remove the use of abc.ABC when defining the Sharding type. This triggers a pytype bug: the easiest fix seems to be to skip the use of the ABC. * Write use_cpp_class decorator differently on ArrayImpl to work around pytype bug. * Fix a few new type errors.,2023-03-13T15:55:41Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14948
293,"以下是一个github上的jax下的一个issue, 标题是([Rollback] Move PyBuffer methods used by PyArray to c++.)， 内容是 ([Rollback] Move PyBuffer methods used by PyArray to c++. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Rollback] Move PyBuffer methods used by PyArray to c++.,[Rollback] Move PyBuffer methods used by PyArray to c++. ,2023-03-11T17:40:10Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14932
575,"以下是一个github上的jax下的一个issue, 标题是(Profiling JAX with Nsight compute not working)， 内容是 ( Description Hi, I am trying to profile JAX code with Nsight compute. However, I am unable to do it. Here's a simple example:  ` ncu targetprocesses all python test.py` shows:    What jax/jaxlib version are you using? jax v0.4.6, jaxlib v0.4.6  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.13. OS Linux  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Profiling JAX with Nsight compute not working," Description Hi, I am trying to profile JAX code with Nsight compute. However, I am unable to do it. Here's a simple example:  ` ncu targetprocesses all python test.py` shows:    What jax/jaxlib version are you using? jax v0.4.6, jaxlib v0.4.6  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.13. OS Linux  NVIDIA GPU info ",2023-03-11T03:45:00Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/14930
329,"以下是一个github上的jax下的一个issue, 标题是([typing] better annotations for jnp.ndarray.at)， 内容是 (Fixes CC(Type hint  `at` property in `basearray.pyi`); part of CC(Tracking Issue: JAX Type Annotations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[typing] better annotations for jnp.ndarray.at,Fixes CC(Type hint  `at` property in `basearray.pyi`); part of CC(Tracking Issue: JAX Type Annotations),2023-03-10T22:59:26Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14927
749,"以下是一个github上的jax下的一个issue, 标题是(Type hint  `at` property in `basearray.pyi`)， 内容是 (Type hint `jax.Array.at` to aid IDEs. Didn't submit a PR directly because 1. Not sure if there is any circular import issue 2. `_IndexUpdateHelper` is in internal, do we change the name or `__name__` to make it look better? General question for JAX developers: is there a 'canonical' name for the `at`properties? `jax.Array.at` is not convenient for verbal communication. Maybe, functional array index accessor? I really want a name that is searchable, and I could refer to, say when answering questions in Github discussions.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Type hint  `at` property in `basearray.pyi`,"Type hint `jax.Array.at` to aid IDEs. Didn't submit a PR directly because 1. Not sure if there is any circular import issue 2. `_IndexUpdateHelper` is in internal, do we change the name or `__name__` to make it look better? General question for JAX developers: is there a 'canonical' name for the `at`properties? `jax.Array.at` is not convenient for verbal communication. Maybe, functional array index accessor? I really want a name that is searchable, and I could refer to, say when answering questions in Github discussions.",2023-03-10T17:58:19Z,enhancement,closed,0,4,https://github.com/jax-ml/jax/issues/14909,"Thanks for raising this! I think I left this out because circular imports make it tricky, but we should be able to figure it out. The name we use now is ""index update helper"", but that's a bit of a mouthful. If you search for ""jnp.ndarray.at"" you find this page: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html We'd be open to suggestions for making that more discoverable.","> The name we use now is ""index update helper"" I don't think it's a bad name. As for discoverability, I'd suggest inserting the exact phrase in ndarray.at document, Array class, jax.ops document, and maybe also in the shape bits. Something like  in CS paper title fashion. I was trying to sell the arrayapi people this api, now I can tell them the name of the product.","Unfortunately, the header on the `ndarray.at` doc page is generated by sphinx autodoc, so it's not easy to change it. We can certainly add more words in the dosctring though.","> We can certainly add more words in the dosctring though. Yes, this is what I meant."
630,"以下是一个github上的jax下的一个issue, 标题是([JAX] Split _src/xla_bridge.py into a separate Bazel target.)， 内容是 ([JAX] Split _src/xla_bridge.py into a separate Bazel target. Include _src/distributed.py and _src/clusters/*.py in the same target because they are in a stronglyconnected component. [XLA:Python] Set type of ArrayImpl to Any, since the JAX change now allows pytype to see that some values are ArrayImpls but ArrayImpls are not instances of jax.Array to Pytype. Fix type of buffer_from_pyval.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[JAX] Split _src/xla_bridge.py into a separate Bazel target.,"[JAX] Split _src/xla_bridge.py into a separate Bazel target. Include _src/distributed.py and _src/clusters/*.py in the same target because they are in a stronglyconnected component. [XLA:Python] Set type of ArrayImpl to Any, since the JAX change now allows pytype to see that some values are ArrayImpls but ArrayImpls are not instances of jax.Array to Pytype. Fix type of buffer_from_pyval.",2023-03-10T13:49:00Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14903
271,"以下是一个github上的jax下的一个issue, 标题是(Move PyBuffer methods used by PyArray to c++.)， 内容是 (Move PyBuffer methods used by PyArray to c++. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Move PyBuffer methods used by PyArray to c++.,Move PyBuffer methods used by PyArray to c++. ,2023-03-10T05:08:59Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14895
741,"以下是一个github上的jax下的一个issue, 标题是([Colab] jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure)， 内容是 ( Description Running a Colab notebook that was working on Colab GPU last month, getting `INTERNAL: RET_CHECK failure` all of sudden. Tried: https://github.com/google/jax/issues/13504, which is `!export XLA_PYTHON_CLIENT_MEM_FRACTION=0.7` Did not work. GPU: Tesla T4 on Colab Not sure if this is a Jax issue, any help?   What jax/jaxlib version are you using? jax 0.3.25  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16  NVIDIA GPU info GPU: Tesla T4 on Colab)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[Colab] jaxlib.xla_extension.XlaRuntimeError: INTERNAL: RET_CHECK failure," Description Running a Colab notebook that was working on Colab GPU last month, getting `INTERNAL: RET_CHECK failure` all of sudden. Tried: https://github.com/google/jax/issues/13504, which is `!export XLA_PYTHON_CLIENT_MEM_FRACTION=0.7` Did not work. GPU: Tesla T4 on Colab Not sure if this is a Jax issue, any help?   What jax/jaxlib version are you using? jax 0.3.25  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.16  NVIDIA GPU info GPU: Tesla T4 on Colab",2023-03-10T00:56:53Z,bug NVIDIA GPU,open,0,5,https://github.com/jax-ml/jax/issues/14893,"This typically means you've installed the `jaxlib` GPU wheel for the wrong CUDA version. Colab comes with a compatible JAX version preinstalled, so if you reset your runtime JAX should work outofthebox. That said, if you want to update you should be sure to use the `cuda11_cudnn82` version of jaxlib (see https://github.com/google/jaxpipinstallationgpucuda for installation instructions).","I am also having the exact same issue with any jax version newer than 0.3.13, using a 187Gb GPU, and having tried all CUDA compatible subversions of JAX and JAXLIB with my CUDA version. Would help if there is any solution to this issue. (note I have this error whenever I try to create a jax object, jax numpy array even of dimension 1x1)"," do you still experience this issue with the latest jax on Colab ?  If yes, can you please provide some repro code or link to your failing colab ? Thanks!",how solve this problem finally？,"Hey, any solution to this?"
772,"以下是一个github上的jax下的一个issue, 标题是(xmap with SerialLoop and FLAX (same for equinox))， 内容是 ( Description When trying to xmap an axis and to use a SerialLoop (in order to reduce memory usage) on a function that evaluates a neural network the following error is raised:   This does not happen if Mesh is used, or no resource mapping is used at all. Same happens for Equinox, but not for very simple functions like jnp.sin...  How to reproduce with FLAX:   What jax/jaxlib version are you using? jax v0.4.5, jaxlib 0.4.4+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.9, Linux  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,xmap with SerialLoop and FLAX (same for equinox)," Description When trying to xmap an axis and to use a SerialLoop (in order to reduce memory usage) on a function that evaluates a neural network the following error is raised:   This does not happen if Mesh is used, or no resource mapping is used at all. Same happens for Equinox, but not for very simple functions like jnp.sin...  How to reproduce with FLAX:   What jax/jaxlib version are you using? jax v0.4.5, jaxlib 0.4.4+cuda11.cudnn82  Which accelerator(s) are you using? GPU  Additional system info Python 3.10.9, Linux  NVIDIA GPU info ",2023-03-09T17:00:23Z,bug NVIDIA GPU,open,0,1,https://github.com/jax-ml/jax/issues/14877,Hi   It appears that this issue has been resolved in later versions of JAX. I tested the issue on colab with GPU Tesla T4 and on WSL2 with GPUs RTX A5000 and GeForce RTX 2060 and it work fine without any failures.  Output:  Attaching the colab gist for reference. Also please find the below screenshot on WSL2 with GPU GeForce RTX 2060. !image !image Note: `xmap` is deprecated in JAX 0.4.26 and will be removed in future release. It recommended to use `jax.experimental.shard_map` or `jax.vmap` with `spmd_axes_name` argument to express SPMD deviceparallel computations. Thank you.
1265,"以下是一个github上的jax下的一个issue, 标题是(Support linear extrapolation in jnp.interp?)， 内容是 (`numpy.interp` does not support extrapolation, instead leaving this functionality to the much more complex scipy.interpolate.interp1d, which uses a class based interface. So I propose to interpret the string values `left='extrapolate'` and `right='extrapolate'` as indicating that extrapolation should be done instead of only using a constant value:  This value would be consistent with the keyword argument used by the `fill_value` argument to scipy.interpolate.interp1d.  There is no potential confusion about strings indicating interpolation modes versus values because `left` and `right` currently only support numeric arguments. This would be useful and extremely easy to implement in JAX. Basically all that needs to be done to obtain linear extrapolation is to conditionally remove these calls to `where` at the end of `jnp.interp` https://github.com/google/jax/blob/6634600c46c99e3f0861e9258aace3fdfe250baf/jax/_src/numpy/lax_numpy.pyL1042L1043 Any thoughts? I can see a case for asking to do this in NumPy first, but I'm not sure the)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Support linear extrapolation in jnp.interp?,"`numpy.interp` does not support extrapolation, instead leaving this functionality to the much more complex scipy.interpolate.interp1d, which uses a class based interface. So I propose to interpret the string values `left='extrapolate'` and `right='extrapolate'` as indicating that extrapolation should be done instead of only using a constant value:  This value would be consistent with the keyword argument used by the `fill_value` argument to scipy.interpolate.interp1d.  There is no potential confusion about strings indicating interpolation modes versus values because `left` and `right` currently only support numeric arguments. This would be useful and extremely easy to implement in JAX. Basically all that needs to be done to obtain linear extrapolation is to conditionally remove these calls to `where` at the end of `jnp.interp` https://github.com/google/jax/blob/6634600c46c99e3f0861e9258aace3fdfe250baf/jax/_src/numpy/lax_numpy.pyL1042L1043 Any thoughts? I can see a case for asking to do this in NumPy first, but I'm not sure the",2023-03-08T20:26:42Z,enhancement,closed,4,1,https://github.com/jax-ml/jax/issues/14858,Deviating from the NumPy API in ways  suggests SGTM!
398,"以下是一个github上的jax下的一个issue, 标题是(bad error on VJP of functions returning typed key arrays)， 内容是 (Found by  in CC(expose `random_wrap` and `random_unwrap` (plus discussion of early typed key surprises))!  Error is:  (RNGs: key types and custom implementations))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,bad error on VJP of functions returning typed key arrays,Found by  in CC(expose `random_wrap` and `random_unwrap` (plus discussion of early typed key surprises))!  Error is:  (RNGs: key types and custom implementations),2023-03-08T17:41:21Z,bug,closed,1,0,https://github.com/jax-ml/jax/issues/14856
311,"以下是一个github上的jax下的一个issue, 标题是(Stop using version 1 of XlaCallModuleOp)， 内容是 (Stop using version 1 of XlaCallModuleOp Also remove configuration flag jax2tf_use_stablehlo.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Stop using version 1 of XlaCallModuleOp,Stop using version 1 of XlaCallModuleOp Also remove configuration flag jax2tf_use_stablehlo.,2023-03-08T08:13:03Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14847
1304,"以下是一个github上的jax下的一个issue, 标题是([shape_poly, call_tf] Some improvements for call_tf in a shape polymorphic program)， 内容是 ([shape_poly, call_tf] Some improvements for call_tf in a shape polymorphic program This is another attempt to land a rolledback change https://github.com/google/jax/pull/14734 (cl/514070997). See b/272154366 for more details. The use case for call_tf with shape polymorphism is when we have a JAX program that calls into TF function, and we want to serialize the JAX program with some shapes unknown. Previously this use case did not work, except in the special case when the output shape of the called TF function returns statically known shapes. The idea is that we allow the user of call_tf to specify the output shape. This can be done even in presence of shape polymorphism, by writing the output shape as an expression in terms of the input shapes. This is what other JAX primitives do, e.g., concat, so we are simply enabling call_tf to get the same behavior. This change should be enough for oldstyle jax2tf, but will require more work for native serialization. We also removed some old code that was trying to workaround some limitat)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[shape_poly, call_tf] Some improvements for call_tf in a shape polymorphic program","[shape_poly, call_tf] Some improvements for call_tf in a shape polymorphic program This is another attempt to land a rolledback change https://github.com/google/jax/pull/14734 (cl/514070997). See b/272154366 for more details. The use case for call_tf with shape polymorphism is when we have a JAX program that calls into TF function, and we want to serialize the JAX program with some shapes unknown. Previously this use case did not work, except in the special case when the output shape of the called TF function returns statically known shapes. The idea is that we allow the user of call_tf to specify the output shape. This can be done even in presence of shape polymorphism, by writing the output shape as an expression in terms of the input shapes. This is what other JAX primitives do, e.g., concat, so we are simply enabling call_tf to get the same behavior. This change should be enough for oldstyle jax2tf, but will require more work for native serialization. We also removed some old code that was trying to workaround some limitat",2023-03-08T07:48:15Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14846
299,"以下是一个github上的jax下的一个issue, 标题是(Set ArrayImpl.__name__ to ArrayImpl)， 内容是 (Set ArrayImpl.__name__ to ArrayImpl Fixes https://github.com/google/jax/issues/14768)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Set ArrayImpl.__name__ to ArrayImpl,Set ArrayImpl.__name__ to ArrayImpl Fixes https://github.com/google/jax/issues/14768,2023-03-07T21:10:20Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14831
1217,"以下是一个github上的jax下的一个issue, 标题是(Cannot build from source on Ubuntu 22.04.1 LTS (Google Cloud))， 内容是 ( Description I am trying to build JAX from source, and I'm getting stuck on this `//:python_include: Genrules without outputs don't make sense` error.  It is talking about this genrule, which seems fishy (locally for me it is in /home/silvasean/.cache/bazel/_bazel_silvasean/2be0f603d22f00664ced77380d946e12/external/local_config_python/BUILD)  My bazelfu is not strong enough to really debug this. It seems to be possibly coming from tensorflow/third_party/py/python_configure.bzl but I can't figure out reproduce the build step that generates the bogus genrule to debug the issue. Any help would be appreciated. Reproduction steps (following https://jax.readthedocs.io/en/latest/developer.html exactly as far as I can tell): https://gist.github.com/silvasean/5ea1c43be9e2dc1d2317d985eb696a76  What jax/jaxlib version are you using? 88d5a4110b9f902dc57ac35878c04c1eb3bf54c1  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22.04.1 LTS  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Cannot build from source on Ubuntu 22.04.1 LTS (Google Cloud)," Description I am trying to build JAX from source, and I'm getting stuck on this `//:python_include: Genrules without outputs don't make sense` error.  It is talking about this genrule, which seems fishy (locally for me it is in /home/silvasean/.cache/bazel/_bazel_silvasean/2be0f603d22f00664ced77380d946e12/external/local_config_python/BUILD)  My bazelfu is not strong enough to really debug this. It seems to be possibly coming from tensorflow/third_party/py/python_configure.bzl but I can't figure out reproduce the build step that generates the bogus genrule to debug the issue. Any help would be appreciated. Reproduction steps (following https://jax.readthedocs.io/en/latest/developer.html exactly as far as I can tell): https://gist.github.com/silvasean/5ea1c43be9e2dc1d2317d985eb696a76  What jax/jaxlib version are you using? 88d5a4110b9f902dc57ac35878c04c1eb3bf54c1  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22.04.1 LTS  NVIDIA GPU info ",2023-03-07T15:19:48Z,bug needs info,closed,0,5,https://github.com/jax-ml/jax/issues/14822,Did you also try https://github.com/openxla/openxlapjrtpluginbuildingjaxfromsource ?,"> Did you also try https://github.com/openxla/openxlapjrtpluginbuildingjaxfromsource ? Yes, that's actually where I started. I then found that the vanilla jax build doesn't work.",I think it's coming from here: https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl I can help debug further later today if you need it,"I can't reproduce this. I took a fresh Ubuntu 22.04.2 VM, ran:  and everything built fine. You'll have to share more detailed reproduction instructions.","Thanks , that narrowed this down. The issue is related to having `python3dev` installed, and my system got into a ""bad state"" where bazel had the ""not python3dev installed"" state cached. It appears that cache invalidation retains its position in the pantheon of most difficult problems in computer science :rofl:  To explain how I got here: 1. I followed the steps in openxlapjrtplugin for building JAX from source: link 2. The steps there don't include installing python3dev 3. I did the bazel build, and it cached the ""not having python3dev installed"" state, which created the bogus genrule 4. Later when I installed python3dev (per the JAX docs), it was not seen and the issue persisted. The solution was ensuring python3dev was installed and removing `~/.cache/bazel`. Also, an interesting side note: In your instructions you install `apt install python3pip` which installs `python3dev` incidentally. In my flow I got pip via `apt install python3venv` which installs the `python3pipwhl` Ubuntu package as a byproduct. But `python3pipwhl` does not install `python3dev`, unlike `python3pip`. Phew"
622,"以下是一个github上的jax下的一个issue, 标题是(Excessive memory allocation with `jax.vmap`)， 内容是 ( Description The below reproducing example defines a function `refine` that if `vmap`ed allocates memory excessively and eventually leads to an OOM even though the naivenonbatched pass is well behaved and consumes little memory.   What jax/jaxlib version are you using? jaxlib==0.4.4, jax==0.4.4  Which accelerator(s) are you using? GPU  Additional system info python 3.9.12, Linux  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Excessive memory allocation with `jax.vmap`," Description The below reproducing example defines a function `refine` that if `vmap`ed allocates memory excessively and eventually leads to an OOM even though the naivenonbatched pass is well behaved and consumes little memory.   What jax/jaxlib version are you using? jaxlib==0.4.4, jax==0.4.4  Which accelerator(s) are you using? GPU  Additional system info python 3.9.12, Linux  NVIDIA GPU info ",2023-03-07T00:22:47Z,bug NVIDIA GPU,open,0,0,https://github.com/jax-ml/jax/issues/14808
521,"以下是一个github上的jax下的一个issue, 标题是(Jaxlib build fails. Tensorflow link 404 error)， 内容是 ( Description Building jaxlib (cuda support) fails. Looks like the storage.googleapis.com/mirror.tensorflow.org links are invalid (404).   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Jaxlib build fails. Tensorflow link 404 error, Description Building jaxlib (cuda support) fails. Looks like the storage.googleapis.com/mirror.tensorflow.org links are invalid (404).   What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? GPU  Additional system info Ubuntu 22  NVIDIA GPU info _No response_,2023-03-06T15:34:06Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/14797,"The 404 errors are benign: they simply mean one of the possible mirrors doesn't exist. The real problem is you don't have `python` in your path, which is currently required. (Your `python` might be an alias to `python3`, for example.)"
754,"以下是一个github上的jax下的一个issue, 标题是(Test failures on aarch64-darwin)， 内容是 ( Description I'm seeing the following test failures when testing on aarch64darwin:  In particular, I'm running on an Apple M1 Max 2021 chip. Complete test logs can be seen here. All the test failures appear to be numerical precision issues. It looks like tolerances just need to be made slightly more generous. I'm running with the following pytest flags:   What jax/jaxlib version are you using? jax v0.4.5, jaxlib v0.4.4  Which accelerator(s) are you using? n/a  Additional system info Python python33.10.10 from Nixpkgs  NVIDIA GPU info n/a)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Test failures on aarch64-darwin," Description I'm seeing the following test failures when testing on aarch64darwin:  In particular, I'm running on an Apple M1 Max 2021 chip. Complete test logs can be seen here. All the test failures appear to be numerical precision issues. It looks like tolerances just need to be made slightly more generous. I'm running with the following pytest flags:   What jax/jaxlib version are you using? jax v0.4.5, jaxlib v0.4.4  Which accelerator(s) are you using? n/a  Additional system info Python python33.10.10 from Nixpkgs  NVIDIA GPU info n/a",2023-03-06T00:37:16Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/14793
862,"以下是一个github上的jax下的一个issue, 标题是(`jax.ensure_compile_time_eval` puts the trace stack in a bad state)， 内容是 (Found this one with , who was trying to test out whether we always maintain a correct trace stack state. It turns out that we can violate our invariants by at least one route: the current implementation of `jax.ensure_compile_time_eval`. This overrides the trace stack's dynamic and base traces with a fresh main level0 `EvalTrace`, without regard for what's already in the trace stack. We can elicit an incorrect error with a slight adaptation of `RematTest.test_remat_grad_python_control_flow_static_argnums`:   Maybe we should also `assert` some trace stack invariants in `core.new_main` and `core.new_base_main`?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.ensure_compile_time_eval` puts the trace stack in a bad state,"Found this one with , who was trying to test out whether we always maintain a correct trace stack state. It turns out that we can violate our invariants by at least one route: the current implementation of `jax.ensure_compile_time_eval`. This overrides the trace stack's dynamic and base traces with a fresh main level0 `EvalTrace`, without regard for what's already in the trace stack. We can elicit an incorrect error with a slight adaptation of `RematTest.test_remat_grad_python_control_flow_static_argnums`:   Maybe we should also `assert` some trace stack invariants in `core.new_main` and `core.new_base_main`?",2023-03-04T03:50:46Z,bug,open,1,1,https://github.com/jax-ml/jax/issues/14776,Thanks for putting this together!
298,"以下是一个github上的jax下的一个issue, 标题是(Make jaxlib type check with pytype.)， 内容是 (Make jaxlib type check with pytype. Add a missing type signature to xla_client.pyi.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Make jaxlib type check with pytype.,Make jaxlib type check with pytype. Add a missing type signature to xla_client.pyi.,2023-03-03T21:43:57Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14771
1298,"以下是一个github上的jax下的一个issue, 标题是([shape_poly, call_tf] Add support for call_tf in a shape polymorphic program)， 内容是 (The use case for call_tf with shape polymorphism is when we have a JAX program that calls into TF function, and we want to serialize the JAX program with some shapes unknown. Today this use case does not work, except in the special case when the output shape of the called TF function returns statically known shapes. The idea is that we allow the user of call_tf to specify the output shape. This can be done even in presence of shape polymorphism, by writing the output shape as an expression in terms of the input shapes. This is what other JAX primitives do, e.g., concat, so we are simply enabling call_tf to get the same behavior. This change should be enough for oldstyle jax2tf, but will require more work for native serialization. We also removed some old code that was trying to workaround some limitations in shape inference in TF. I think that those workarounds are ugly, and I am prepared to give error messages rather than keep that code. So far no tests fail. Still to do:  * ~~more testing~~  * ~~check that the TF function re)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[shape_poly, call_tf] Add support for call_tf in a shape polymorphic program","The use case for call_tf with shape polymorphism is when we have a JAX program that calls into TF function, and we want to serialize the JAX program with some shapes unknown. Today this use case does not work, except in the special case when the output shape of the called TF function returns statically known shapes. The idea is that we allow the user of call_tf to specify the output shape. This can be done even in presence of shape polymorphism, by writing the output shape as an expression in terms of the input shapes. This is what other JAX primitives do, e.g., concat, so we are simply enabling call_tf to get the same behavior. This change should be enough for oldstyle jax2tf, but will require more work for native serialization. We also removed some old code that was trying to workaround some limitations in shape inference in TF. I think that those workarounds are ugly, and I am prepared to give error messages rather than keep that code. So far no tests fail. Still to do:  * ~~more testing~~  * ~~check that the TF function re",2023-03-01T14:31:20Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14734,  
1296,"以下是一个github上的jax下的一个issue, 标题是([custom_vjp] bwd function should not be WrappedFun, may run multiple times)， 内容是 (The `custom_jvp_p` primitive is kind of like the `xla_call_p` primitive underlying finalstyle (old) `jit`. With `xla_call_p`, we have one Python callable (wrapped in an `lu.WrappedFun`) which is called exactly once. With `custom_jvp_p` we have two functions (each wrapped in an `lu.WrappedFun`), representing either the undifferentiated primal function or its JVP rule, and exactly one is called exactly once (determined not at runtime but rather by whether an AD pass hits at tracing time). The calledexactlyonce property is useful for reasoning about values plumbed through mutable cells. Indeed one of the main reasons `lu.WrappedFun` exists is to handle populating `Store`s for such plumbing. While for `xla_call_p` and `custom_jvp_p` we have one and two functions, respectively, wrapped as `lu.WrappedFun`s corresponding to how they're called exactly once, the `custom_vjp_p` primitive is parameterized by _three_ callables. The three callables represent the primal function, the forward pass under autodiff, and the backward pass under )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"[custom_vjp] bwd function should not be WrappedFun, may run multiple times","The `custom_jvp_p` primitive is kind of like the `xla_call_p` primitive underlying finalstyle (old) `jit`. With `xla_call_p`, we have one Python callable (wrapped in an `lu.WrappedFun`) which is called exactly once. With `custom_jvp_p` we have two functions (each wrapped in an `lu.WrappedFun`), representing either the undifferentiated primal function or its JVP rule, and exactly one is called exactly once (determined not at runtime but rather by whether an AD pass hits at tracing time). The calledexactlyonce property is useful for reasoning about values plumbed through mutable cells. Indeed one of the main reasons `lu.WrappedFun` exists is to handle populating `Store`s for such plumbing. While for `xla_call_p` and `custom_jvp_p` we have one and two functions, respectively, wrapped as `lu.WrappedFun`s corresponding to how they're called exactly once, the `custom_vjp_p` primitive is parameterized by _three_ callables. The three callables represent the primal function, the forward pass under autodiff, and the backward pass under ",2023-03-01T01:19:25Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14728
998,"以下是一个github上的jax下的一个issue, 标题是(Improved documentation on `jax.random.gamma`)， 内容是 (Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I think the documentation for the `jax.random.gamma` random variable (sampler) could possibly be clearer. In particular, letting the user know that the standard form is implemented and how to incorporate the rate (or alternatively scale) parameter in their samples. Maybe an addendum along the lines of  *""we implement the standard gamma density. Dividing the sample by the rate is equivalent to sampling from* $\text{Gamma}(\alpha, \text{rate})$*. And multiplying the sample by the scale is equivalent to sampling from* $\text{Gamma}(\alpha, \text{scale})$."" The discussion for this started here and I'm mentioning  for visibility. Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Improved documentation on `jax.random.gamma`,"Please:  [x] Check for duplicate requests.  [x] Describe your goal, and if possible provide a code snippet with a motivating example. I think the documentation for the `jax.random.gamma` random variable (sampler) could possibly be clearer. In particular, letting the user know that the standard form is implemented and how to incorporate the rate (or alternatively scale) parameter in their samples. Maybe an addendum along the lines of  *""we implement the standard gamma density. Dividing the sample by the rate is equivalent to sampling from* $\text{Gamma}(\alpha, \text{rate})$*. And multiplying the sample by the scale is equivalent to sampling from* $\text{Gamma}(\alpha, \text{scale})$."" The discussion for this started here and I'm mentioning  for visibility. Thanks!",2023-02-26T18:51:40Z,enhancement,closed,0,0,https://github.com/jax-ml/jax/issues/14691
702,"以下是一个github上的jax下的一个issue, 标题是(np.linalg.norm should use numerically stable p-norms.)， 内容是 (I think that it would make to implement https://timvieira.github.io/blog/post/2014/11/10/numericallystablepnorms/ similar to to how it's implemented for the logsumexp here. This would allow for larger norms to work.  While  while  maybe this should be a special argument as it increases the memory read and most people should be fine w/o it?  [x ] Check for duplicate requests.  [x ] Describe your goal, and if possible provide a code snippet with a motivating example.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,np.linalg.norm should use numerically stable p-norms.,"I think that it would make to implement https://timvieira.github.io/blog/post/2014/11/10/numericallystablepnorms/ similar to to how it's implemented for the logsumexp here. This would allow for larger norms to work.  While  while  maybe this should be a special argument as it increases the memory read and most people should be fine w/o it?  [x ] Check for duplicate requests.  [x ] Describe your goal, and if possible provide a code snippet with a motivating example.",2023-02-25T04:32:16Z,enhancement,open,2,0,https://github.com/jax-ml/jax/issues/14679
555,"以下是一个github上的jax下的一个issue, 标题是(Add PyArrayResultHandler which behaves like)， 内容是 (Add PyArrayResultHandler which behaves like functools.partial(jax.arrays.ArrayImpl) with the added benefit that the new PyExecuteResults type can explode directly into ArrayImpls if passed to explode_with_handlers(). Note that this also helps with deprecating PyBuffer as the fastpath does not need to call the PyBuffer constructor.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add PyArrayResultHandler which behaves like,Add PyArrayResultHandler which behaves like functools.partial(jax.arrays.ArrayImpl) with the added benefit that the new PyExecuteResults type can explode directly into ArrayImpls if passed to explode_with_handlers(). Note that this also helps with deprecating PyBuffer as the fastpath does not need to call the PyBuffer constructor.,2023-02-25T02:56:53Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14678
1324,"以下是一个github上的jax下的一个issue, 标题是(Very slow JIT compilation due to constant-folding of very large BCOO sparse matrix nonzero entry array)， 内容是 ( Description I've got a use case where I'd like to store the nonzero entries of a very large sparse matrix, and then access them later during a machine learning training loop. Unfortunately, using JIT compilation results in constantfolding of this array, making it extremely slow on large problems. Here's an MWE that runs on my laptop and captures the typical behavior:  Calling the function without JIT executes in about a tenth of a second, but calling it with JIT takes almost a minute. On larger problems in the codebase which prompted this MWE, I have had it crash due to running out of memory after about an hour. This produces warnings similar to the following:  The problem seems to be that the stored array, `nonzeroes` has shape `(n,2)`, which in this case is very large, yet the JIT compiler tries to constantfold it. This seems like a bug, unless there are good reasons why arrays with millions of elements should be constantfolded, in which case it would be very helpful to have some way of telling the compiler not to do so in )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Very slow JIT compilation due to constant-folding of very large BCOO sparse matrix nonzero entry array," Description I've got a use case where I'd like to store the nonzero entries of a very large sparse matrix, and then access them later during a machine learning training loop. Unfortunately, using JIT compilation results in constantfolding of this array, making it extremely slow on large problems. Here's an MWE that runs on my laptop and captures the typical behavior:  Calling the function without JIT executes in about a tenth of a second, but calling it with JIT takes almost a minute. On larger problems in the codebase which prompted this MWE, I have had it crash due to running out of memory after about an hour. This produces warnings similar to the following:  The problem seems to be that the stored array, `nonzeroes` has shape `(n,2)`, which in this case is very large, yet the JIT compiler tries to constantfold it. This seems like a bug, unless there are good reasons why arrays with millions of elements should be constantfolded, in which case it would be very helpful to have some way of telling the compiler not to do so in ",2023-02-24T01:42:29Z,bug,open,0,10,https://github.com/jax-ml/jax/issues/14655,"It seems like the compiler isn't making a great choice here with respect to the runtime/compiletime tradeoffs involved in constant folding. If you change your code slightly though, the problematic array will be computed at runtime: ","> It seems like the compiler isn't making a great choice here with respect to the runtime/compiletime tradeoffs involved in constant folding. If you change your code slightly though, the problematic array will be computed at runtime: >  >  Thanks! Unfortunately, while this would avoid the problem here, I can't fix things upstream in that way  the array `nonzeroes` in my codebase is computed using what is effectively a blackbox, CPUonly algorithm outside of JAX. Perhaps a better way to write the MWE would have been to use `scipy.sparse.eye` or similar instead. Do you know if there are any workarounds I can implement to prevent the compiler from constantfolding the array?","I don't know... it's really an XLA bug, and I'm not sure of a way to change what XLA does here. Maybe you could rewrite your code so that `nonzeros` is explicitly passed to the jitcompiled outer function? I know that's probably not the answer you're looking for, but I think it would work...","Thanks! Unfortunately, passing around `nonzeroes` won't work, since this is something computed by the package which should not be exposed to the user, and the functions it would need to be passed into are called by the user. Should I file a bug report upstream? Would the TensorFlow repository be the right place?","It looks like one possible workaround for now is to use an optimization barrier:  This is still somewhat experimental, so unfortunately there is no public API for this.","That said, it's probably worth filing an XLA bug for this. It should be something that the compiler handles automatically: https://github.com/openxla/xla","Thanks, that works! Two comments: 1. The function `_optimization_barrier` **must** be called inside `product`, and not outside of it, so for instance `nonzeroes = _optimization_barrier(sparse.eye(n).indices)` will not work. 2. This XLA bug might be specific to integer arrays: in my upstream codebase, I have other arrays which are also precomputed, but wrapping BCOO nonzero index arrays in `_optimization_barrier` is sufficient to get JIT to not freeze. Very much appreciate your help with this!",", even though it seems like an XLA bug, it would be helpful to mention which hardware you are using (compilers have different backends, so code paths are different).","> , even though it seems like an XLA bug, it would be helpful to mention which hardware you are using (compilers have different backends, so code paths are different). Sure! Have reproduced this issue on both Nvidia GPU and Apple M1.","I know this thread is about a year old, but I thought I would note that I've run into similar issues with JITing large sparse arrays (in my case, for moderate sized finite element simulations  ~2M elements). Beyond slow constant folding, I've also run into the limit where XLA seg faults if the array is too large (generally somewhere around > 400,000,000 nonzero elements in the array). In this case, _optimization_barrier has no effect for me and the only solution is to provide the sparse array indices as an input to my function. I've run into this issue on both Linux and Windows. Due to the size of the arrays, I've only been able to try this on the CPU (as my GPUs don't have enough memory for problems this large)."
789,"以下是一个github上的jax下的一个issue, 标题是(Add `random.chisquare` and `random.f` )， 内容是 (For my works related to statistics part, chisquare or F distribution are often used, and there're `random.chisquare` and `random.f` in `numpy`, I'd implement these for my convenience in `JAX`, I'd like to know, would you like to add `chisquare` or `f` in future? Actually I'veimplement these for my convenience in `JAX` and pass the tests Here's my code  I use gamma to produce chisquare, then chisquare to F, but when  comes to small `dfden` and `dfnum` situation, I tried to change prng key to pass the tests. As above, I wonder if you guys like to add theses features?)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Add `random.chisquare` and `random.f` ,"For my works related to statistics part, chisquare or F distribution are often used, and there're `random.chisquare` and `random.f` in `numpy`, I'd implement these for my convenience in `JAX`, I'd like to know, would you like to add `chisquare` or `f` in future? Actually I'veimplement these for my convenience in `JAX` and pass the tests Here's my code  I use gamma to produce chisquare, then chisquare to F, but when  comes to small `dfden` and `dfnum` situation, I tried to change prng key to pass the tests. As above, I wonder if you guys like to add theses features?",2023-02-23T14:37:45Z,enhancement,closed,0,3,https://github.com/jax-ml/jax/issues/14640,Hi  thanks for bringing this up! I think a welltested `chisquare` and `f` functionality would be a welcome contribution to `jax.random`. Would you like to prepare a pull request?,Sure，I think I’ll open a PR soon  ,"Since random.chi2 and random.f has been merged, the issue is closed"
924,"以下是一个github上的jax下的一个issue, 标题是(Optimize canonicalize_shape)， 内容是 (Optimize canonicalize_shape I was looking at some profiles and noticed canonicalize_shape showing up as a noticeable overhead in certain cases. Which makes sense, given that we carefully check all possible cases before trying to consider integers as plausible elements (which are the most popular _by far_). And this function is pretty hot, because it gets called any time we create a new `ShapedArray`. I wrote a small benchmark that repeatedly calls canonicalize_shape on a 4sized tuple of integers. Before: 7.62µs ± 8% After: 1.42µs ± 2% So a pretty easy 5x improvement overall. And in more real cases, when resharding an array onto 8 TPUs, 50% of the time was spent on creating shapes for avals of device buffers.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Optimize canonicalize_shape,"Optimize canonicalize_shape I was looking at some profiles and noticed canonicalize_shape showing up as a noticeable overhead in certain cases. Which makes sense, given that we carefully check all possible cases before trying to consider integers as plausible elements (which are the most popular _by far_). And this function is pretty hot, because it gets called any time we create a new `ShapedArray`. I wrote a small benchmark that repeatedly calls canonicalize_shape on a 4sized tuple of integers. Before: 7.62µs ± 8% After: 1.42µs ± 2% So a pretty easy 5x improvement overall. And in more real cases, when resharding an array onto 8 TPUs, 50% of the time was spent on creating shapes for avals of device buffers.",2023-02-23T14:19:46Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14639
643,"以下是一个github上的jax下的一个issue, 标题是([sparse] remove handling of padded indices from COO/CSR)， 内容是 (While trying to fix failures that arose in CC([sparse] modify impl rules to always use lowering path), it became clear that the `COO` and `CSR` representation is fundamentally flawed in the case that `nse` is set to larger than the true nse. Fixing this would take a lot of effort that would be better spent on the longterm plans with BCOO and BCSR. Eventually we should deprecate and remove `COO` and `CSR`.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] remove handling of padded indices from COO/CSR,"While trying to fix failures that arose in CC([sparse] modify impl rules to always use lowering path), it became clear that the `COO` and `CSR` representation is fundamentally flawed in the case that `nse` is set to larger than the true nse. Fixing this would take a lot of effort that would be better spent on the longterm plans with BCOO and BCSR. Eventually we should deprecate and remove `COO` and `CSR`.",2023-02-22T23:01:13Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14631
570,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Use CUDA and ROCM instead of GPU for XlaCallModuleOp platforms)， 内容是 ([jax2tf] Use CUDA and ROCM instead of GPU for XlaCallModuleOp platforms JAX is moving to using ROCM and CUDA instead of the generic GPU platform type and it is already supporting separate lowerings for ROCM and CUDA. To keep up with this functionality, we move the XlaCallModuleOp to supporting ROCM and CUDA platforms.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Use CUDA and ROCM instead of GPU for XlaCallModuleOp platforms,"[jax2tf] Use CUDA and ROCM instead of GPU for XlaCallModuleOp platforms JAX is moving to using ROCM and CUDA instead of the generic GPU platform type and it is already supporting separate lowerings for ROCM and CUDA. To keep up with this functionality, we move the XlaCallModuleOp to supporting ROCM and CUDA platforms.",2023-02-22T09:03:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14621
1286,"以下是一个github上的jax下的一个issue, 标题是(Deal with NaN errors in jax.experimental.ode.optimal_step_size())， 内容是 ( Description The following is related to my discussion about diffrax with kidger here: https://github.com/patrickkidger/diffrax/issues/223  It can happen when solving certain ODE systems that the error estimate for adaptive stepping will become NaN (e.g., if a trial timestep is too large). Currently this means jax.experimental.ode.odeint will fail. If we use jax.debug.print to print the values of the state variables, derivatives, and other intermediary quantities inside the integrator function while odeint is running, we would see NaN's in all timesteps except perhaps the first few (after an initial NaN appears, odeint cannot recover). The fix is simple  change this line in jax.experimental.ode.optimal_step_size() from   to something like  I don't have a minimal working example because the code I was running into this problem with is a bit complicated (it involves a nonautonomous ODE system). However, you can plug in mean_error_ratio = jnp.nan in the above two examples to see what I mean. I wrote my own adaptive RK23 (BogackiS)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Deal with NaN errors in jax.experimental.ode.optimal_step_size()," Description The following is related to my discussion about diffrax with kidger here: https://github.com/patrickkidger/diffrax/issues/223  It can happen when solving certain ODE systems that the error estimate for adaptive stepping will become NaN (e.g., if a trial timestep is too large). Currently this means jax.experimental.ode.odeint will fail. If we use jax.debug.print to print the values of the state variables, derivatives, and other intermediary quantities inside the integrator function while odeint is running, we would see NaN's in all timesteps except perhaps the first few (after an initial NaN appears, odeint cannot recover). The fix is simple  change this line in jax.experimental.ode.optimal_step_size() from   to something like  I don't have a minimal working example because the code I was running into this problem with is a bit complicated (it involves a nonautonomous ODE system). However, you can plug in mean_error_ratio = jnp.nan in the above two examples to see what I mean. I wrote my own adaptive RK23 (BogackiS",2023-02-21T23:22:58Z,bug contributions welcome,open,0,0,https://github.com/jax-ml/jax/issues/14612
350,"以下是一个github上的jax下的一个issue, 标题是(Add support for StableHLO Serialized Portable Artifacts in XlaCallModuleOp and JAX2TF)， 内容是 (Add support for StableHLO Serialized Portable Artifacts in XlaCallModuleOp and JAX2TF)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,Add support for StableHLO Serialized Portable Artifacts in XlaCallModuleOp and JAX2TF,Add support for StableHLO Serialized Portable Artifacts in XlaCallModuleOp and JAX2TF,2023-02-21T22:05:19Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14609
442,"以下是一个github上的jax下的一个issue, 标题是(Kaggle TPU notebook wastes around a minute during initialization)， 内容是 ( With jax/jaxlib 0.4.4 installed:  Each timeout takes several seconds. Everything appears to work fine after the 6th attempt and failure, but I'm wondering why the error message is happening at all.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,Kaggle TPU notebook wastes around a minute during initialization," With jax/jaxlib 0.4.4 installed:  Each timeout takes several seconds. Everything appears to work fine after the 6th attempt and failure, but I'm wondering why the error message is happening at all.",2023-02-21T15:16:56Z,TPU,closed,0,2,https://github.com/jax-ml/jax/issues/14600,"The latest jax 0.4.6 release fixes this. Leaving this issue open until Kaggle updates their default jax version, but you can manually upgrade in the meantime as a workaround: `!pip install U jax[tpu] f https://storage.googleapis.com/jaxreleases/libtpu_releases.html`","The default jax version on Kaggle notebooks is now 0.4.6, woohoo! Create a new notebook to try it out; old notebooks will still have the old image. I verified that running `import jax` + `jax.devices()` work smoothly."
528,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Enable strict platform checking for native serialized modules.)， 内容是 (This takes advantage of recent changes to XlaCallModule that allow us to use a `platforms` attribute to specify what are the allowable platforms for a serialized module. Also add a `experimental_native_lowering_strict_checks` parameter to `jax2tf.convert` to disable the check.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Enable strict platform checking for native serialized modules.,This takes advantage of recent changes to XlaCallModule that allow us to use a `platforms` attribute to specify what are the allowable platforms for a serialized module. Also add a `experimental_native_lowering_strict_checks` parameter to `jax2tf.convert` to disable the check.,2023-02-21T08:23:26Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14594, PTAL
526,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Add support for cross-platform lowering in native serialization)， 内容是 (Allow the user of native serialization to specify the platform for which the serialization to be done. This relies on newly added support for platform checking in XlaCallModule op (version 3). The implementation here is temporary, pending having a proper crosslowering API. )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",llm,[jax2tf] Add support for cross-platform lowering in native serialization,"Allow the user of native serialization to specify the platform for which the serialization to be done. This relies on newly added support for platform checking in XlaCallModule op (version 3). The implementation here is temporary, pending having a proper crosslowering API. ",2023-02-20T11:32:15Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14587
987,"以下是一个github上的jax下的一个issue, 标题是(Integer convolution with int8 inputs and int32 output)， 内容是 (I'm trying to do a convolution with an int8 input image and kernel tensors, then get back an int32 output tensor, but I don't see any way to do that:  gives back an int8 result. But there is an option to give preferred_element_type to get the output from a preferred type. But it doesn't support int32 or int16 types. It only supports float32 and int8 types. `out = jax.lax.conv(lhs=jax.numpy.transpose(img,[0,3,1,2]), rhs=jax.numpy.transpose(kernel,[3,2,0,1]), window_strides=(1, 1), padding='SAME', preferred_element_type ='jax.numpy.float32')` I could cast the output tensor to jax.numpy.int32 after the convolution, but I want to use a hardwareaccelerated int8 convolution, if available. Are there any available solutions in order to do that? Thanks!)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Integer convolution with int8 inputs and int32 output,"I'm trying to do a convolution with an int8 input image and kernel tensors, then get back an int32 output tensor, but I don't see any way to do that:  gives back an int8 result. But there is an option to give preferred_element_type to get the output from a preferred type. But it doesn't support int32 or int16 types. It only supports float32 and int8 types. `out = jax.lax.conv(lhs=jax.numpy.transpose(img,[0,3,1,2]), rhs=jax.numpy.transpose(kernel,[3,2,0,1]), window_strides=(1, 1), padding='SAME', preferred_element_type ='jax.numpy.float32')` I could cast the output tensor to jax.numpy.int32 after the convolution, but I want to use a hardwareaccelerated int8 convolution, if available. Are there any available solutions in order to do that? Thanks!",2023-02-20T09:04:08Z,enhancement needs info NVIDIA GPU,open,0,2,https://github.com/jax-ml/jax/issues/14585,Which platform are you using? GPU/TPU?,I'm using Nvidia RTX 6000 GPU. My platform details are also here. Ubuntu 20.04 Cuda 12.0 Cudnn 8.5 Jax 0.4.4 Jaxlib 0.4.4
665,"以下是一个github上的jax下的一个issue, 标题是(Document jax.Array methods & attributes)， 内容是 (Currently there is no documentation for jax Array attributes and methods. CC(DOC: improve documentation for jax.Array methods) was an attempt at this, but it caused issues and had to be rolledback in CC(BUG: avoid passing functions directly to abstractmethod) We can't document `ArrayImpl` directly, because its import path does not match its `__module__` and `__name__`, and so sphinx treats it as an alias. I'm not sure the best way to proceed.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Document jax.Array methods & attributes,"Currently there is no documentation for jax Array attributes and methods. CC(DOC: improve documentation for jax.Array methods) was an attempt at this, but it caused issues and had to be rolledback in CC(BUG: avoid passing functions directly to abstractmethod) We can't document `ArrayImpl` directly, because its import path does not match its `__module__` and `__name__`, and so sphinx treats it as an alias. I'm not sure the best way to proceed.",2023-02-17T18:37:02Z,documentation,closed,1,2,https://github.com/jax-ml/jax/issues/14563,"Once https://github.com/google/jax/commit/44082be10376805c2f80cda0dfbb917f1e916853 is part of the `minimum_jaxlib`, we can add `ArrayImpl` to the HTML docs and fix this issue.","Chatting with  and  offline, I think we want to add true abstract method definitions to `jax.Array` and document the methods that way. This will require some reworking of how tracer methods are dispatched, but this shouldn't be too problematic."
856,"以下是一个github上的jax下的一个issue, 标题是(XlaRuntimeError: UNIMPLEMENTED: Only 1 computation per replica supported, 8 requested.   In call to configurable 'train' (<function train at 0x7f364f9c11f0>))， 内容是 ( Description I meet this error . Anyone can slove it ? Thank you very much ........  What jax/jaxlib version are you using? jax                           0.4.3 jaxlib                        0.4.2  Which accelerator(s) are you using? TPU on google colab pro plus.TPU v2  Additional system info I am trying to training t5x(https://github.com/googleresearch/t5x) on TPU .  This is my code  https://drive.google.com/file/d/1yo3eHCc9T4uTSpRyzW4vNQkB76kEwLlB/view?usp=sharing  NVIDIA GPU info I don't use GPU . Just using TPU)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"XlaRuntimeError: UNIMPLEMENTED: Only 1 computation per replica supported, 8 requested.   In call to configurable 'train' (<function train at 0x7f364f9c11f0>)", Description I meet this error . Anyone can slove it ? Thank you very much ........  What jax/jaxlib version are you using? jax                           0.4.3 jaxlib                        0.4.2  Which accelerator(s) are you using? TPU on google colab pro plus.TPU v2  Additional system info I am trying to training t5x(https://github.com/googleresearch/t5x) on TPU .  This is my code  https://drive.google.com/file/d/1yo3eHCc9T4uTSpRyzW4vNQkB76kEwLlB/view?usp=sharing  NVIDIA GPU info I don't use GPU . Just using TPU,2023-02-17T09:55:48Z,bug needs info,open,0,4,https://github.com/jax-ml/jax/issues/14552,is it possible to get a repro?,"If you could answer the questions in place of *No response*, it would be quite helpful for diagnosing your problem – thanks!",I just update more information   ,"Thanks  this is a version compatibility issue. For Colab TPU, please use jax & jaxlib version 0.3.25"
1255,"以下是一个github上的jax下的一个issue, 标题是(Colab INTERNAL: RET_CHECK failure)， 内容是 ( Description Dear All, I am facing a weird error message when running my code on Colab Nvidia GPU. It is very strange because the same code worked previously (2 weeks ago), and it also works seamlessly when run on Colab CPU. Here is a minimal working example.  Here, I install JAX & associated libraries. `'''(1) Import JAX ''' USE_CPU = False USE_GPU = True if USE_CPU:  !pip install upgrade q pip jaxlib jax if USE_GPU:   !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install upgrade q git+https://github.com/deepmind/dmhaiku.git !pip install upgrade q git+https://github.com/deepmind/optax.git   import jax` Outputing: `Looking in indexes: https://pypi.org/simple, https://uspython.pkg.dev/colabwheels/public/simple/ Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Requirement already satisfied: jax[cuda] in /usr/local/lib/python3.8/distpackages (0.4.3) Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/distpackages (from)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Colab INTERNAL: RET_CHECK failure," Description Dear All, I am facing a weird error message when running my code on Colab Nvidia GPU. It is very strange because the same code worked previously (2 weeks ago), and it also works seamlessly when run on Colab CPU. Here is a minimal working example.  Here, I install JAX & associated libraries. `'''(1) Import JAX ''' USE_CPU = False USE_GPU = True if USE_CPU:  !pip install upgrade q pip jaxlib jax if USE_GPU:   !pip install upgrade ""jax[cuda]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html !pip install upgrade q git+https://github.com/deepmind/dmhaiku.git !pip install upgrade q git+https://github.com/deepmind/optax.git   import jax` Outputing: `Looking in indexes: https://pypi.org/simple, https://uspython.pkg.dev/colabwheels/public/simple/ Looking in links: https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html Requirement already satisfied: jax[cuda] in /usr/local/lib/python3.8/distpackages (0.4.3) Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/distpackages (from",2023-02-16T16:27:10Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/14521,You're using the wrong version of jaxlib: it requires a newer cudnn than colab has installed. See https://github.com/google/jax/issues/14464 Hope that helps!,Thank you!
831,"以下是一个github上的jax下的一个issue, 标题是(Raise a user-friendly error message if in/outfeed-based host_callback…)， 内容是 (… stuff is used with PJRT C API. Prior to this change, it would crash horribly instead. I manually tested by running the following on a Cloud TPU v48:  And verifying that all errors were the new error message. The new error message is: `host_callback functionality isn't supported with the new Cloud TPU runtime. See https://jax.readthedocs.io/en/latest/debugging/index.html and https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html for alternatives. Please file a feature request at https://github.com/google/jax/issues if none of the alternatives are sufficent.`)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Raise a user-friendly error message if in/outfeed-based host_callback…,"… stuff is used with PJRT C API. Prior to this change, it would crash horribly instead. I manually tested by running the following on a Cloud TPU v48:  And verifying that all errors were the new error message. The new error message is: `host_callback functionality isn't supported with the new Cloud TPU runtime. See https://jax.readthedocs.io/en/latest/debugging/index.html and https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html for alternatives. Please file a feature request at https://github.com/google/jax/issues if none of the alternatives are sufficent.`",2023-02-16T00:02:27Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14504
645,"以下是一个github上的jax下的一个issue, 标题是(bitcast_convert_type does not support cross-size casts)， 内容是 ( Description The current definition of `jax.lax.bitcast_convert_type` requires that the source and destination types have the same size. The underlying XLA op does not have this requirement and has welldefined semantics for differentlysized types.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,bitcast_convert_type does not support cross-size casts, Description The current definition of `jax.lax.bitcast_convert_type` requires that the source and destination types have the same size. The underlying XLA op does not have this requirement and has welldefined semantics for differentlysized types.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? TPU  Additional system info _No response_  NVIDIA GPU info _No response_,2023-02-15T21:41:17Z,bug P1 (soon),closed,0,0,https://github.com/jax-ml/jax/issues/14499
406,"以下是一个github上的jax下的一个issue, 标题是(Modify JaxArrayTest.test_defragment to work on any numbers of devices)， 内容是 (Modify JaxArrayTest.test_defragment to work on any numbers of devices Also skip it when the PJRT C API is enabled, since the C API only supports auto defrag.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Modify JaxArrayTest.test_defragment to work on any numbers of devices,"Modify JaxArrayTest.test_defragment to work on any numbers of devices Also skip it when the PJRT C API is enabled, since the C API only supports auto defrag.",2023-02-15T18:45:58Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14492
1252,"以下是一个github上的jax下的一个issue, 标题是(Jnp.concatenate does work right)， 内容是 ( Description Hi, I'm writing a deep neural network training code with Jax, Flax, and Optax. I observed a wired thing when I use jnp. concatenate() to aggregate two matrices into a certain dimension. The operation does not yield the right results as seen in jax.debug.print(). I conducted the operation inside the loss function, which is contained in another training function decorated with jax.pmap(). Part of the code is shown below:  During running, part of the `scale4fr1adj:0` loss and `scale4fr1adj:1` loss are:   When conducting `reproj_losses = jnp.concatenate(reproj_losses, axis=3)`, we got  It is not a normal operation from jnp.concatenate(), but more like a reshape() operation of the loss maps. The output shapes are shown in the source code. Can you help to provide some hints on what caused this? Thank you very much!  What jax/jaxlib version are you using? jax 0.3.24; jaxlib 0.3.24+cuda11.cudnn82; flax0.4.2; optax 0.1.4  Which accelerator(s) are you using? GPU  Additional system info Linux, python 3.8.5  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jnp.concatenate does work right," Description Hi, I'm writing a deep neural network training code with Jax, Flax, and Optax. I observed a wired thing when I use jnp. concatenate() to aggregate two matrices into a certain dimension. The operation does not yield the right results as seen in jax.debug.print(). I conducted the operation inside the loss function, which is contained in another training function decorated with jax.pmap(). Part of the code is shown below:  During running, part of the `scale4fr1adj:0` loss and `scale4fr1adj:1` loss are:   When conducting `reproj_losses = jnp.concatenate(reproj_losses, axis=3)`, we got  It is not a normal operation from jnp.concatenate(), but more like a reshape() operation of the loss maps. The output shapes are shown in the source code. Can you help to provide some hints on what caused this? Thank you very much!  What jax/jaxlib version are you using? jax 0.3.24; jaxlib 0.3.24+cuda11.cudnn82; flax0.4.2; optax 0.1.4  Which accelerator(s) are you using? GPU  Additional system info Linux, python 3.8.5  NVIDIA GPU info ",2023-02-15T17:44:58Z,needs info,closed,0,2,https://github.com/jax-ml/jax/issues/14490,Hi  thanks for the report. I'm having trouble understanding what your expected output is. Can you provide a minimal reproducible example showing the inputs and output of `jnp.concatenate` and how it differs from what you expect?,Closing for lack of information. Please let us know if you're still having this issue!
445,"以下是一个github上的jax下的一个issue, 标题是([PJRT:C] Add PJRT_Client_Defragment)， 内容是 ([PJRT:C] Add PJRT_Client_Defragment Also adds some helper functions: * PJRT_RETURN_STATUS_IF_ERROR (macro) * AwaitEvent * DefragmentClient * GetHostBuffer [JAX] Modify JaxArrayTest.test_defragment to work on any numbers of devices)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[PJRT:C] Add PJRT_Client_Defragment,[PJRT:C] Add PJRT_Client_Defragment Also adds some helper functions: * PJRT_RETURN_STATUS_IF_ERROR (macro) * AwaitEvent * DefragmentClient * GetHostBuffer [JAX] Modify JaxArrayTest.test_defragment to work on any numbers of devices,2023-02-15T01:16:01Z,,closed,0,1,https://github.com/jax-ml/jax/issues/14478,Closing Copybara created PR due to inactivity
1097,"以下是一个github上的jax下的一个issue, 标题是(arange XlaRuntimeError v0.4.3 colab gpu)， 内容是 ( Description jnp.arange causes XlaRuntimeError.      XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in  > 1 x = jnp.arange(33) 16 frames /usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options, host_callbacks)    1024    TODO(sharadmv): remove this fallback when all backends allow `compile`    1025    to take in `host_callbacks` > 1026   return backend.compile(built_c, compile_options=options)    1027     1028 _ir_dump_counter = itertools.count() XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr  What jax/jaxlib version are you using? v0.4.3  Which accelerator(s) are you using? GPU  Additional system info Google Colab  NVIDIA GPU info Tue Feb 14 16:45:09 2023        ++  ++)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,arange XlaRuntimeError v0.4.3 colab gpu," Description jnp.arange causes XlaRuntimeError.      XlaRuntimeError                           Traceback (most recent call last) [](https://localhost:8080/) in  > 1 x = jnp.arange(33) 16 frames /usr/local/lib/python3.8/distpackages/jax/_src/dispatch.py in backend_compile(backend, built_c, options, host_callbacks)    1024    TODO(sharadmv): remove this fallback when all backends allow `compile`    1025    to take in `host_callbacks` > 1026   return backend.compile(built_c, compile_options=options)    1027     1028 _ir_dump_counter = itertools.count() XlaRuntimeError: INTERNAL: RET_CHECK failure (external/org_tensorflow/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:626) dnn != nullptr  What jax/jaxlib version are you using? v0.4.3  Which accelerator(s) are you using? GPU  Additional system info Google Colab  NVIDIA GPU info Tue Feb 14 16:45:09 2023        ++  ++",2023-02-14T16:51:21Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/14470,Thanks for the question! This looks like an issue of mismatched CUDA versions; see https://github.com/google/jax/issues/14464 for a similar answer.,Closing as a duplicate of CC(XLA Error when running `rng = jax.random.PRNGKey(0)` in google colab) ; the fix is to install the correct version of jaxlib.,"Hi, I'm seeing the exact same error, on WSL with cuda 11.8. I tried both jaxlib versions. I also tried updating my cuda to 12.0, but see the same error. Attempting with fresh install of https://github.com/NTT123/a0jax if it helps to repro ",I don't get this error if i install 0.4.2 of jax[cuda] with: ,> Thanks for the question! This looks like an issue of mismatched CUDA versions; see CC(XLA Error when running `rng = jax.random.PRNGKey(0)` in google colab) for a similar answer. Thank you!, I suspect you don't have the correct version of CuDNN installed.
1261,"以下是一个github上的jax下的一个issue, 标题是(Linking errors when building on windows)， 内容是 ( Description  Follow the fix in https://github.com/google/jax/issues/14369issuecomment1429839230 There will still be linking error as in https://github.com/google/jax/issues/14369issuecomment1428121867 > If all those fixed, you will get another link error as follows: >  >   Cause of multiply defined `tsl::profiler::internal::g_annotation_enabled` `TF_COMPILE_LIBRARY` is not properly defined https://github.com/tensorflow/tensorflow/blob/959d1b144dc03fbda586f8a60dd4c117025e6c18/tensorflow/tsl/platform/macros.hL61L69 which if further caused by  parameter `is_external` of macro `get_win_copts` is not properly propagated in, in https://github.com/tensorflow/tensorflow/blob/959d1b144dc03fbda586f8a60dd4c117025e6c18/tensorflow/tensorflow.bzl  Fortunately, we can `copts=/DTF_COMPILE_LIBRARY` to workaround this problem.  Cause of already defined `xla::runtime::ffi::GetXlaFfiStream` `xla::runtime::ffi::GetXlaFfiStream` is first defined as weak symbol in `ffi.cc` then defined as normal symbol in `executable.cc` and msvc does not support w)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Linking errors when building on windows," Description  Follow the fix in https://github.com/google/jax/issues/14369issuecomment1429839230 There will still be linking error as in https://github.com/google/jax/issues/14369issuecomment1428121867 > If all those fixed, you will get another link error as follows: >  >   Cause of multiply defined `tsl::profiler::internal::g_annotation_enabled` `TF_COMPILE_LIBRARY` is not properly defined https://github.com/tensorflow/tensorflow/blob/959d1b144dc03fbda586f8a60dd4c117025e6c18/tensorflow/tsl/platform/macros.hL61L69 which if further caused by  parameter `is_external` of macro `get_win_copts` is not properly propagated in, in https://github.com/tensorflow/tensorflow/blob/959d1b144dc03fbda586f8a60dd4c117025e6c18/tensorflow/tensorflow.bzl  Fortunately, we can `copts=/DTF_COMPILE_LIBRARY` to workaround this problem.  Cause of already defined `xla::runtime::ffi::GetXlaFfiStream` `xla::runtime::ffi::GetXlaFfiStream` is first defined as weak symbol in `ffi.cc` then defined as normal symbol in `executable.cc` and msvc does not support w",2023-02-14T14:54:06Z,bug Windows,closed,0,7,https://github.com/jax-ml/jax/issues/14466,Seems like Triton has been changed such that the advice above is no longer valid (build will still fail for similar reasons).,"Are both of these issues still current, or just the `GetXlaFfiStream` issue?", do you have suggestions on how we might avoid the weak symbol here? MSVC apparently does not support weak symbols.,Still current. But the first one is mainly configuration issue. So only the second one need to be address ATM.,The weak symbol problem has been resolved on openxla/xla latest main.,"Is this issue still a problem? (I know it isn't an CPU, because the new Windows CPU CI build is mostly happy: https://github.com/google/jax/actions/workflows/windows_ci.yml )","> The weak symbol problem has been resolved on openxla/xla latest main. That is, the second is fixed in https://github.com/openxla/xla/commit/e634d4ab1067445c0f89f4b2bbe0bafaf0400051 The first one is still there, but as it can be workaround from outside, feel free to close this issue."
650,"以下是一个github上的jax下的一个issue, 标题是(XLA Error when running `rng = jax.random.PRNGKey(0)` in google colab)， 内容是 ( Description I am getting an XLA weird error when running   in Google colab. You can check the error here. The full code is  and returns  Also reported here https://github.com/google/jax/discussions/14463discussion4856887  What jax/jaxlib version are you using? jax == 0.4.3; jaxlib == 0.4.3+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Google colab  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,XLA Error when running `rng = jax.random.PRNGKey(0)` in google colab, Description I am getting an XLA weird error when running   in Google colab. You can check the error here. The full code is  and returns  Also reported here https://github.com/google/jax/discussions/14463discussion4856887  What jax/jaxlib version are you using? jax == 0.4.3; jaxlib == 0.4.3+cuda11.cudnn86  Which accelerator(s) are you using? GPU  Additional system info Google colab  NVIDIA GPU info ,2023-02-14T11:22:45Z,bug,closed,0,1,https://github.com/jax-ml/jax/issues/14464,"That's admittedly a bad error message, but it's happening because of a version incompatibility. Colab has CuDNN 8.4, but the version of jaxlib you installed requires CuDNN 8.6 or newer. Try:  See: https://github.com/google/jaxpipinstallationgpucuda"
1260,"以下是一个github上的jax下的一个issue, 标题是(Track back from jaxlib version to source code control)， 内容是 (I'm trying to set up a CI flow that involves installing jax/jaxlib nightly and then also being able to checkout a corresponding commit in the source control system (i.e. to run more tests or build against the same XLA backend). As an example, we do the following with IREE:  This lets us track back (even arbitrary nightlies without tags, etc) to the source control system, and it is used by various integrators to anchor their flows. I couldn't find a corresponding linkage in jaxlib's version.py. I'm not familiar with how Jax assembles its release artifacts. If you have pointers, I could try to add something. In IREE, our setup.py files probe for a special nonchecked in version_info.json (https://github.com/ireeorg/iree/blob/main/runtime/setup.pyL110) and use this to generate the version.py. When running from a git repo, the revisions are populated on the fly, but source tarballs can just include it all in the version_info.json. There are a lot of ways to do it. Just mentioning that by reference in case if helpful.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Track back from jaxlib version to source code control,"I'm trying to set up a CI flow that involves installing jax/jaxlib nightly and then also being able to checkout a corresponding commit in the source control system (i.e. to run more tests or build against the same XLA backend). As an example, we do the following with IREE:  This lets us track back (even arbitrary nightlies without tags, etc) to the source control system, and it is used by various integrators to anchor their flows. I couldn't find a corresponding linkage in jaxlib's version.py. I'm not familiar with how Jax assembles its release artifacts. If you have pointers, I could try to add something. In IREE, our setup.py files probe for a special nonchecked in version_info.json (https://github.com/ireeorg/iree/blob/main/runtime/setup.pyL110) and use this to generate the version.py. When running from a git repo, the revisions are populated on the fly, but source tarballs can just include it all in the version_info.json. There are a lot of ways to do it. Just mentioning that by reference in case if helpful.",2023-02-14T04:59:06Z,enhancement,open,0,1,https://github.com/jax-ml/jax/issues/14461,"To give a little bit of background about the flow I am using, my project has a `sync.py` that you can use to sync to different development binaries in order to limit how much needs to be built. If you run `sync.py nightly`, it installs the ireecompiler nightly and then uses the embedded revision to clone/check out the matching revision of the runtime code, which it has a source dependency on (there is no source dependency on the compiler). I wanted to do the same thing with jaxlib nightlies: Install the latest jaxlib nightly, get the jax commit and derive the corresponding tensorflow (or xla) commit to clone/checkout those repos as well. Then everything is set up to take additional source deps suitable for building a runtime, run tests, etc. Doing it this way reduces what needs to be built to construct derived projects to ~hundreds of files, reducing development iteration time and CI burden. It works pretty well for daisy chaining against IREE in this way and I figured could also help for the jax/xla side of the deps."
761,"以下是一个github上的jax下的一个issue, 标题是(possible memory leak)， 内容是 ( Description I' m experiencing  very large memory usage in forward mode . I'm studying AD and wrote a small script to compare memory usage in forward mode vs reverse mode. I was expecting less usage in forward mode, but I found the opposite and in forward mode my script reach very soon memory exaustion. Here is the script https://gist.github.com/frhack/2436e2daf6fbc9d30bbcac62ca35ee9a thanks  What jax/jaxlib version are you using? 0.4.3/0.4.3  Which accelerator(s) are you using? CPU  Additional system info debian 11/ core i7  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,possible memory leak," Description I' m experiencing  very large memory usage in forward mode . I'm studying AD and wrote a small script to compare memory usage in forward mode vs reverse mode. I was expecting less usage in forward mode, but I found the opposite and in forward mode my script reach very soon memory exaustion. Here is the script https://gist.github.com/frhack/2436e2daf6fbc9d30bbcac62ca35ee9a thanks  What jax/jaxlib version are you using? 0.4.3/0.4.3  Which accelerator(s) are you using? CPU  Additional system info debian 11/ core i7  NVIDIA GPU info _No response_",2023-02-13T19:56:45Z,question,open,0,1,https://github.com/jax-ml/jax/issues/14447,"Thanks for the question! The script compares `jacfwd` with `grad`, but that comparison is a bit applestooranges: `jacfwd` has to push forward an entire batch of vectors, whereas grad only pulls back one. Based on your description here, I'm guessing you want to compare `jax.jvp` with `jax.grad`. For more on `jvp` vs `jacfwd`, see the autodiff cookbook. The memory difference is most pronounced with deeper computations than the one in your script, as `grad` requires memory which scales with the computation depth while `jvp` does not."
785,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Allow functions with unused dimension variables in native lowering)， 内容是 (In the jax2tf shape polymorphism with native lowering we raise an error if we try to convert a function whose arguments have dimension variables if those arguments are not used in the computation. This is because the unused arguments are dropped during lowering, and then there are no arguments from which to derive the value of the dimension variables at invocation time. These errors are especially annoying for gradient functions that actually do not use the dimension variables. We should not give an error in that case.  )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Allow functions with unused dimension variables in native lowering,"In the jax2tf shape polymorphism with native lowering we raise an error if we try to convert a function whose arguments have dimension variables if those arguments are not used in the computation. This is because the unused arguments are dropped during lowering, and then there are no arguments from which to derive the value of the dimension variables at invocation time. These errors are especially annoying for gradient functions that actually do not use the dimension variables. We should not give an error in that case.  ",2023-02-13T09:36:33Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/14437,Fixed in CC([shape_poly] Fixed bug with dimension variables in unused args) 
786,"以下是一个github上的jax下的一个issue, 标题是(A few developer workflow enhancements for working with jaxlib.)， 内容是 (It seems to me that jaxlib development must be mostly happening on CI, because some basics are pretty essential. Here are a few things I've been typing/carrying for a while in my flow: * Add .bazelrc.user to .gitignore so it doesn't accidentally get checked in. * Add configs for 'debug_symbols' and 'debug' that make some things minimally workable under a debugger (or to get backtraces, etc). * Add `forcereinstall` to the copy/paste command to update a built jaxlib wheel (without this, if you are iterating, it fairly quietly does nothing).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,A few developer workflow enhancements for working with jaxlib.,"It seems to me that jaxlib development must be mostly happening on CI, because some basics are pretty essential. Here are a few things I've been typing/carrying for a while in my flow: * Add .bazelrc.user to .gitignore so it doesn't accidentally get checked in. * Add configs for 'debug_symbols' and 'debug' that make some things minimally workable under a debugger (or to get backtraces, etc). * Add `forcereinstall` to the copy/paste command to update a built jaxlib wheel (without this, if you are iterating, it fairly quietly does nothing).",2023-02-11T05:07:01Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14424
621,"以下是一个github上的jax下的一个issue, 标题是(Jax at head infinite loops printing an array)， 内容是 ( Description Tensorflow checked out commit: d06d16ef29767129aeaaefd007481aeb759e5400 Jax: dc6bf9b725fcdd96f8152d6d3a1ce3f0c14d9ced This simple test program:  Fails with:  I can reproduce this on all backends, including CPU.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Jax at head infinite loops printing an array," Description Tensorflow checked out commit: d06d16ef29767129aeaaefd007481aeb759e5400 Jax: dc6bf9b725fcdd96f8152d6d3a1ce3f0c14d9ced This simple test program:  Fails with:  I can reproduce this on all backends, including CPU.  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? _No response_  Additional system info _No response_  NVIDIA GPU info _No response_",2023-02-11T04:24:00Z,bug,open,0,15,https://github.com/jax-ml/jax/issues/14422,"Hey  ! It works on my machine. That is, both at https://github.com/google/jax/commit/dc6bf9b725fcdd96f8152d6d3a1ce3f0c14d9ced and at current github HEAD 1bdcd5e13, using pypi's latest jaxlib 0.4.3 (but not a local build), that script doesn't fail. Did you build jaxlib yourself, or install it? Can you try it with pypi's latest? (I think: `pip install upgrade jaxlib`.) I wonder if it's a jax/jaxlib version incompatibility issue...","> I wonder if it's a jax/jaxlib version incompatibility issue... I suspect it is. I'm building with a jaxlib built from head. Working on some features that need the latest, and thought it was something I did, so I reproduced it with a clean build of at head jax and tensorflow repos and a custom build jaxlib. There isn't a problem with what is on pypi and I haven't fully bisected but this just started recently.", found that cl/508789203 is a rollback in progress!,I think this PR rollback should fix it: https://github.com/google/jax/pull/14421,"Not so sure: Just rebased to head (61da7811742d69b3d0e33c211418d1d131cf475b) and it appears to be working. But if I cherrypick that rollback, the issue returns. So something fixed it in the last few hours but it looks like the rollback might rebreak it :) In any case, going to let it be for the weekend and hope it stabilizes...",I think it is a combination of jax at head and TF at head (jaxlib). We need both changes to be reverted to get the previous behavior. We want to rollback that change anyways because of other failures in google. Let's wait for the rollback to get in and then try again? (Or i can by building jaxlib from scratch),(I'm not blocked now and assume that you all can get it past the oscillation... the repro is pretty straightforward),Rollback is submitted and is OSS'd JAX commit: https://github.com/google/jax/commit/9316188b3af802c136e78bc1908a46992633e1f0 TF commit: https://github.com/tensorflow/tensorflow/commit/1be8ca3a774653e69831fe82cf4b140062dcfcf6 I am building and reproing now,The rollback fixes it Command I used to build jaxlib: `python3 build/build.py bazel_options=override_repository=org_tensorflow=/home/yashkatariya/tensorflow`. The tensorflow directory was cloned from master branch of TF. Same for JAX (main branch of JAX) , feel free to try it out at HEAD of both jax and TF and see if it is resolved in your env too :),"Thanks! I'll check tomorrow. FWIW, here is the environment I am verifying: https://github.com/openxla/openxlapjrtplugin That should work tomorrow (after a nightly dep roll) with just a local patch to work around: https://github.com/openxla/xla/issues/1237 (shameless plug to get some attention on that too :) ) I'll need to rebase my patch train, but I expect what you've done has it working.","Hi Stella, can you expound more on what is going on here? Does this mean that you're using the newest python but an old build of the jaxlib extention? Are you using a pjrt version of iree?","Sure. I checked out the jax repo from head then synced the backing tensorflow to nearhead and built a jaxlib per https://jax.readthedocs.io/en/latest/developer.htmlbuildingjaxlibfromsourcewithamodifiedtensorflowrepository. This was because I am working with some folks who have landed changes to jaxlib/tensorflow that I need in order to make progress. Due to this, I'm often developing with a nearhead jaxlib. I am doing this to support the PJRT plugin, which is where I noticed the issue. However, this bug report is just the result of running the script I included against the default XLA CPU backend, so it is not related to anything I am doing.","Right, but we have a tensorflow/compiler/xla/python/xla_client.py with a _version number which allows us to synchronize changes between jax and jaxlib. If that file gets out of sync with the compiled blob, then you could get problems like this.","Is there a written procedure for upgrading jaxlib? When developing nearhead on the OSS side, it seems like step 1 might be to do that locally and then work further. All I can see is this: https://github.com/google/jax/blob/c49af18b9b6a560c1d6cbc516e5c800ad4fb2ca1/WORKSPACEL3 Which implies that we always have our version numbers and such in a state so that just bumping the tf hash should be sufficient (modulo bugs and other stuff, I assume). Let's say that instead of encountering this issue while trying to accomplish my own objective, my objective was to upgrade the tensorflow hash. I would have updated the pin as at the top of the file and then encountered this error, which presumably would have needed to be fixed prior to actually bumping the pin."
437,"以下是一个github上的jax下的一个issue, 标题是([XLA:Python] Fix overly pessimistic handling of singleton dimensions in dlpack code.)， 内容是 ([XLA:Python] Fix overly pessimistic handling of singleton dimensions in dlpack code. Requires an accompanying jaxlib change. Fixes https://github.com/google/jax/issues/14399)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[XLA:Python] Fix overly pessimistic handling of singleton dimensions in dlpack code.,[XLA:Python] Fix overly pessimistic handling of singleton dimensions in dlpack code. Requires an accompanying jaxlib change. Fixes https://github.com/google/jax/issues/14399,2023-02-10T22:06:23Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14414
1063,"以下是一个github上的jax下的一个issue, 标题是(Google Cloud ""Run a calculation on a Cloud TPU VM by using JAX"" issue)， 内容是 ( Description I am executing the commands outline in Run a calculation on a Cloud TPU VM by using JAX in the Google Cloud Docs. I'm following it verbatim, only difference is I have to run.   in order to get   to work. **The issue**: When I execute the flax example training script I get the following warnings along with an error message.  My guess is that jax TPU does not use CUDA, but I want to know what is throwing the error. Does tensorflow think I'm trying to use GPU?  Note that the training does proceed as expected just much slower when compared to the timestamps in the documentation  ~1 second vs. ~45 seconds per epoch Any help would be greatly appreciated!  What jax/jaxlib version are you using? 0.4.3  Which accelerator(s) are you using? TPU  Additional system info v3  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Google Cloud ""Run a calculation on a Cloud TPU VM by using JAX"" issue"," Description I am executing the commands outline in Run a calculation on a Cloud TPU VM by using JAX in the Google Cloud Docs. I'm following it verbatim, only difference is I have to run.   in order to get   to work. **The issue**: When I execute the flax example training script I get the following warnings along with an error message.  My guess is that jax TPU does not use CUDA, but I want to know what is throwing the error. Does tensorflow think I'm trying to use GPU?  Note that the training does proceed as expected just much slower when compared to the timestamps in the documentation  ~1 second vs. ~45 seconds per epoch Any help would be greatly appreciated!  What jax/jaxlib version are you using? 0.4.3  Which accelerator(s) are you using? TPU  Additional system info v3  NVIDIA GPU info _No response_",2023-02-10T17:24:35Z,bug,open,0,0,https://github.com/jax-ml/jax/issues/14407
339,"以下是一个github上的jax下的一个issue, 标题是(Improve error message when passing a mismatched Sharding to device_put.)， 内容是 (Improve error message when passing a mismatched Sharding to device_put. Before:  After: )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Improve error message when passing a mismatched Sharding to device_put.,Improve error message when passing a mismatched Sharding to device_put. Before:  After: ,2023-02-08T17:08:13Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14354
481,"以下是一个github上的jax下的一个issue, 标题是(Correctly hash auto_spmd fields in compilation cache key.)， 内容是 (Correctly hash auto_spmd fields in compilation cache key. I'm in the process of adding test coverage for this (https://github.com/google/jax/pull/14314), which is how I found this! I manually verified with the new test coverage that it's fixed.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Correctly hash auto_spmd fields in compilation cache key.,"Correctly hash auto_spmd fields in compilation cache key. I'm in the process of adding test coverage for this (https://github.com/google/jax/pull/14314), which is how I found this! I manually verified with the new test coverage that it's fixed.",2023-02-07T00:50:27Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14320
476,"以下是一个github上的jax下的一个issue, 标题是(Add option to run tests with persistent compilation cache enabled.)， 内容是 (Add option to run tests with persistent compilation cache enabled. This can help us get a lot more coverage of the compilation cache, since all compiles will trigger it, instead of having to write explicit compilation cache tests.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Add option to run tests with persistent compilation cache enabled.,"Add option to run tests with persistent compilation cache enabled. This can help us get a lot more coverage of the compilation cache, since all compiles will trigger it, instead of having to write explicit compilation cache tests.",2023-02-06T19:01:06Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14314
1284,"以下是一个github上的jax下的一个issue, 标题是(Checkpoint and fft don't play well togethet after jax > 0.3.20)， 内容是 ( Description Hello Jax Team, I have noticed that after upgrading to Jax greater than 0.3.20, the output of my acoustic simulator (based on Jax) had some edge cases where the gradient calculations were incorrect. After some investigation, I believe the issue is related to the combination of the `fftn` functions and `jax.checkpoint` and it happened somewhere between the release of `jax 0.3.21` and `jax 0.3.24`. I have provided a piece of code that reproduces the issue below. I apologize for its length, but I wasn't able to make it shorter.  I have also added comments in the code that begin with ` ERROR SOURCE` to highlight some possible changes that make the issue disappear.  I have run the code with two different Jax versions, keeping all the rest the same. Following are the relevant outputs of pip list for each environment.  When look at the variable `gradient`, by taking the slice `gradient[29,:,:,0]`, there are two very different results for the two Jax versions. I have included images of the results for reference. !image So)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Checkpoint and fft don't play well togethet after jax > 0.3.20," Description Hello Jax Team, I have noticed that after upgrading to Jax greater than 0.3.20, the output of my acoustic simulator (based on Jax) had some edge cases where the gradient calculations were incorrect. After some investigation, I believe the issue is related to the combination of the `fftn` functions and `jax.checkpoint` and it happened somewhere between the release of `jax 0.3.21` and `jax 0.3.24`. I have provided a piece of code that reproduces the issue below. I apologize for its length, but I wasn't able to make it shorter.  I have also added comments in the code that begin with ` ERROR SOURCE` to highlight some possible changes that make the issue disappear.  I have run the code with two different Jax versions, keeping all the rest the same. Following are the relevant outputs of pip list for each environment.  When look at the variable `gradient`, by taking the slice `gradient[29,:,:,0]`, there are two very different results for the two Jax versions. I have included images of the results for reference. !image So",2023-02-05T19:11:45Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/14302,"It looks like this is not an issue anymore with jax 0.4.11, I guess this can be closed.",Closing!
788,"以下是一个github上的jax下的一个issue, 标题是('Failed to fetch URL on try 1 out of 6: Timeout was reached' on Kaggle TPU)， 内容是 ( Description When updating to new Jax version (0.4.1/0.4.2) on Kaggle TPU VM, jax.local_devices() take a few minutes to run. Full error as below:  Here's the public kaggle notebook for reproduction https://www.kaggle.com/code/liminchen1/updatejaxtpubug Is there any setting I can set? Or point me towards the code responsive for it? Or is it an XLA issue?  What jax/jaxlib version are you using? JAX 0.4.1  Which accelerator(s) are you using? TPU  Additional system info On Kaggle VM, newest environment  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",agent,'Failed to fetch URL on try 1 out of 6: Timeout was reached' on Kaggle TPU," Description When updating to new Jax version (0.4.1/0.4.2) on Kaggle TPU VM, jax.local_devices() take a few minutes to run. Full error as below:  Here's the public kaggle notebook for reproduction https://www.kaggle.com/code/liminchen1/updatejaxtpubug Is there any setting I can set? Or point me towards the code responsive for it? Or is it an XLA issue?  What jax/jaxlib version are you using? JAX 0.4.1  Which accelerator(s) are you using? TPU  Additional system info On Kaggle VM, newest environment  NVIDIA GPU info _No response_",2023-02-05T10:34:03Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/14300,"I just tested the issue today and realize the error changed. Instead of taking around 470 seconds, it now takes 90 seconds. It now ends after one set of 6 retries. ",Hi Cakes  It looks like this issue appears to be resolved. The Kaggle notebook you provided (https://www.kaggle.com/code/liminchen1/updatejaxtpubug) executed successfully and printed local_devices in approximately 7.1287e05 seconds. Could you please confirm this resolution and close the issue if everything is working as expected?,"Yes, it seems fixed in newer version. Been ages and I forgot about this issue. "
493,"以下是一个github上的jax下的一个issue, 标题是(improve error message when jit tracer is passed into a shape)， 内容是 (Currently we print something like:  But we have better error messages if the user had typed something like `int(...)`, e.g. explaining why the value is a Tracer in the first place (what operations produced it, or what nonstatic arguments it depends on).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,improve error message when jit tracer is passed into a shape,"Currently we print something like:  But we have better error messages if the user had typed something like `int(...)`, e.g. explaining why the value is a Tracer in the first place (what operations produced it, or what nonstatic arguments it depends on).",2023-02-03T18:11:36Z,better_errors,closed,0,0,https://github.com/jax-ml/jax/issues/14279
1239,"以下是一个github上的jax下的一个issue, 标题是(shard_map (shmap) prototype and JEP)， 内容是 (Add the `shard_map` / `shmap` JEP design doc, and prototype implementation in `jax.experimental.shard_map`. `shmap` is a simple multidevice parallelism API which lets us write perdevice code with explicit collectives, where logical shapes match perdevice physical buffer shapes and collectives correspond exactly to crossdevice communication. It composes well with `pjit` (which is now just `jit`), both in terms of efficiently passing a `shmap`'s outputs to a `pjit`'s inputs, and in terms of nesting a `shmap` inside a `jit`/`pjit`. In particular, `shmap` can be used inside a `jit`/`pjit` to drop temporarily into a ""manual collectives"" mode, letting us write explicit perdevice code and collective communication (rather than relying on the compiler to automatically partition computation and insert collectives).  See the JEP for more! TODO: * [x] split out some notstrictlyrelated changes into separate PRs ( CC(minor tweaks to type annotations, specialize code on those types)) * [x] add JEP doc * [x] write pr message)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,shard_map (shmap) prototype and JEP,"Add the `shard_map` / `shmap` JEP design doc, and prototype implementation in `jax.experimental.shard_map`. `shmap` is a simple multidevice parallelism API which lets us write perdevice code with explicit collectives, where logical shapes match perdevice physical buffer shapes and collectives correspond exactly to crossdevice communication. It composes well with `pjit` (which is now just `jit`), both in terms of efficiently passing a `shmap`'s outputs to a `pjit`'s inputs, and in terms of nesting a `shmap` inside a `jit`/`pjit`. In particular, `shmap` can be used inside a `jit`/`pjit` to drop temporarily into a ""manual collectives"" mode, letting us write explicit perdevice code and collective communication (rather than relying on the compiler to automatically partition computation and insert collectives).  See the JEP for more! TODO: * [x] split out some notstrictlyrelated changes into separate PRs ( CC(minor tweaks to type annotations, specialize code on those types)) * [x] add JEP doc * [x] write pr message",2023-02-02T23:25:32Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14273,You'll need to update tests/BUILD.
399,"以下是一个github上的jax下的一个issue, 标题是(Remove placeholder functions for unimplemented NumPy functions.)， 内容是 (These don't seem necessary now JAX has fairly complete coverage of the NumPy API. Also removes the accidental export of _NOT_IMPLEMENTED in several modules.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Remove placeholder functions for unimplemented NumPy functions.,These don't seem necessary now JAX has fairly complete coverage of the NumPy API. Also removes the accidental export of _NOT_IMPLEMENTED in several modules.,2023-02-02T17:31:57Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14269
307,"以下是一个github上的jax下的一个issue, 标题是([sparse] implement __len__ on sparse objects)， 内容是 (...also improve some tests. Came across this when playing with sparse convolutions.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[sparse] implement __len__ on sparse objects,...also improve some tests. Came across this when playing with sparse convolutions.,2023-02-01T19:46:22Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14251
1148,"以下是一个github上的jax下的一个issue, 标题是(Running vmap with jit disabled on a function with python control flow gives errors)， 内容是 ( Description I have a function that takes in one value, does some nonlinear transformations to it based on some python control flow and outputs a single value.  Let's call it   Now, I can call  to get the derivative wrt some parameters. Im trying to use  to run this on 10,000 samples. I know my function cannot be jitted (in it's current state) so I run:   It gives the following error:  I know this bug was raised before but I went through the other posts and couldn't find an answer to fix this. Is there a way to get the gradient for the 10000 values  with vmap or other, without having the underlying function be jitable?  What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? CPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Running vmap with jit disabled on a function with python control flow gives errors," Description I have a function that takes in one value, does some nonlinear transformations to it based on some python control flow and outputs a single value.  Let's call it   Now, I can call  to get the derivative wrt some parameters. Im trying to use  to run this on 10,000 samples. I know my function cannot be jitted (in it's current state) so I run:   It gives the following error:  I know this bug was raised before but I went through the other posts and couldn't find an answer to fix this. Is there a way to get the gradient for the 10000 values  with vmap or other, without having the underlying function be jitable?  What jax/jaxlib version are you using? pip install ""jax[cuda11_cudnn82]"" f https://storage.googleapis.com/jaxreleases/jax_cuda_releases.html  Which accelerator(s) are you using? CPU  Additional system info Python V 3.10.6. WSL.  NVIDIA GPU info _No response_",2023-02-01T16:02:42Z,question,closed,0,4,https://github.com/jax-ml/jax/issues/14245,"Thanks for raising this! Actually, it's not a bug; this is expected behavior with `vmap`. The issue is that different elements of the batch might need to take different paths in the control flow (e.g. different branches of an `if` when the boolean predicate to the `if` is batched). But we can't take both sides of Python control flow, so we can't trace such a function. The error message is saying that we're tracing (i.e. specializing) on an abstract value representing the set of possible values in the batch (e.g. a value representing the set `{True, False}` for a boolean value), rather than specializing on a particular value (e.g. `True` or `False`), but some Python control flow in the function is demanding a concrete value, which we don't have. Both `vmap` and `jit` behave similarly in that they trace on abstracted values rather than concrete ones: `vmap` because it needs to represent all elements in the batch, and `jit` because it needs to represent all possible values we might apply a function to later (since we're staging the computation out of Python to be compiled). > Is there a way to get the gradient for the 10000 values with vmap or other, without having the underlying function be jitable? Technically yes, in some cases, but it sounds like no in your case. We don't require the whole function to be jittable; we just need any values used in Python control flow to be available / specialized on during trace time. So if we used `vmap` without batching over e.g. the boolean predicate values of any `if`s, we'd be fine. One way to get batchable (and jittable) control flow is to use structured control flow primitives like `jax.lax.cond`, `jax.lax.switch`, or the loop primitives. These are more restrictive than Python control flow, and more awkward to write, but the upside is that they're JAXtransformable, including with `vmap` (though `vmap`of`cond` may produce the equivalent of a `jnp.where`). WDYT?","If you think all elements in your batch may take only one side of the branch, and therefore the whole batch _could_ take just one side of the Python control flow, then you could tweak your program to use a collective, like this:  But I'm guessing that's not what you want to do, and using structured control flow primitives is better.",", Thanks a lot for your answer. That makes a lot of sense. Regarding ""We don't require the whole function to be jittable; we just need any values used in Python control flow to be available / specialized on during trace time."" Can you elaborate a bit more on that (i.e. what does available/specialized mean) or share some examples? Thanks",Here are some references: * video (starting at about 32min in) * How to think in JAX tutorial * Autodidax: JAX core from scratch I may be forgetting some others!
331,"以下是一个github上的jax下的一个issue, 标题是(jax.numpy: remove private members from interface)， 内容是 (In particular, this removes `jnp._add_newdoc_ufunc`, `jnp._pyinstaller_hooks_dir`, and possibly others.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jax.numpy: remove private members from interface,"In particular, this removes `jnp._add_newdoc_ufunc`, `jnp._pyinstaller_hooks_dir`, and possibly others.",2023-02-01T00:48:50Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14237
1274,"以下是一个github上的jax下的一个issue, 标题是(`lax.cond` excessive caching behavior, how to avoid?)， 内容是 ( Description I noticed that when using `lax.cond`, the conditional functions are converted to jaxprs and are cached.   This causes a memoryleak like behavior when the conditionals are changing across compilation runs. Here's a MWE:  In this example, I expect `f1.clear_cache` to discard anything related to compiling `g`. However, I'm observing that memory keeps growing across iterations. The memory footprint only reaches a plateau after a few thousand iterations of the loop. Here's a plot of the memory growth of the above snippet: !mem Notice the linear memory growth from 1 ~ 80 seconds.  My use case is that I have an `g` with varying shapes across the outer loop.  Therefore, recompilation is required. Moreover, `clear_cache` is required, to avoid excessively caching `g1` that are used only once. My understanding is: Each new reference produced by `jax.grad(f)` confuses `lax.cond`, causing it to cache all the jaxprs of `jax.grad(f)`.  This problem is really unfortunate for me, since my `f` is quite complicated, and a few iterat)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"`lax.cond` excessive caching behavior, how to avoid?"," Description I noticed that when using `lax.cond`, the conditional functions are converted to jaxprs and are cached.   This causes a memoryleak like behavior when the conditionals are changing across compilation runs. Here's a MWE:  In this example, I expect `f1.clear_cache` to discard anything related to compiling `g`. However, I'm observing that memory keeps growing across iterations. The memory footprint only reaches a plateau after a few thousand iterations of the loop. Here's a plot of the memory growth of the above snippet: !mem Notice the linear memory growth from 1 ~ 80 seconds.  My use case is that I have an `g` with varying shapes across the outer loop.  Therefore, recompilation is required. Moreover, `clear_cache` is required, to avoid excessively caching `g1` that are used only once. My understanding is: Each new reference produced by `jax.grad(f)` confuses `lax.cond`, causing it to cache all the jaxprs of `jax.grad(f)`.  This problem is really unfortunate for me, since my `f` is quite complicated, and a few iterat",2023-01-31T08:56:08Z,bug,closed,0,11,https://github.com/jax-ml/jax/issues/14220,"I don't think this has anything to do with the jit cache; we can directly query the size of the jit cache this way, and see that it is not growing:  I suspect the memory growth you're seeing has to do with other cacheing that JAX is using internally; for example JAX uses `functools.lru_cache` in many places to tradeoff between memory and runtime cost. An example is here: https://github.com/google/jax/blob/574c0e704722ca6a336458521d4bd7597c268cd6/jax/_src/util.pyL243L259 Unfortunately, there's currently no way to globally clear all those caches during the course of running a JAX program. A related request that you might follow is here: https://github.com/google/jax/issues/10828","Thanks for the prompt response! Yes I agree with you that it has less to do with jit cache. The line of `lax.cond` that creates jaxpr is: https://github.com/google/jax/blob/main/jax/_src/lax/control_flow/conditionals.pyL238 But then, this gets cached by https://github.com/google/jax/blob/b374080e855deb2af6d7c5f9decfefd547c94e95/jax/_src/lax/control_flow/common.pyL65 This cache uses a default size of 4096 entries. My problem is that each entry (a jaxpr) in my case takes a few to maybe 100 megabytes of RAM. This quickly become out of control and get me OOMs, since I'm running on a cluster with memory constraints. Further, `jax.clear_backends` doesn't seem to clear this cache (I'm still seeing OOMs)","Thanks for raising this. Here are a few things we could do to improve this (and we should probably do all of them): 1. provide an easy way to inspect cache sizes and clear them 2. adapt our caches to have size limits rather than numberofentries limits For 2, we may or may not need a C++ implementation for performance. I'm guessing for this application in particular we can make do with a Python implementation, so the main challenge may be just computing the total size of the entries (keys and values) in the cache. For 1, we started brainstorming in CC(brainstorming clearing all caches), but we need to actually land it (and any other pieces we need)..."," Thanks! These options looks quite promising! One curious question, though, is why `lax.while_loop` does not have this problem?  Could we use the same caching mechanism for `while_loop` and `cond`? For now, I will workaround the issue using `select`. ","I think the reason `cond` and `while_loop` behave differently is because `cond` traces with this function, which has a strong reference cache: https://github.com/google/jax/blob/518bb56c6eb1b94b82c457607fa4217fbf769d8d/jax/_src/lax/control_flow/common.pyL65L67 And `while_loop` traces with this function, which has a weak reference cache: https://github.com/google/jax/blob/518bb56c6eb1b94b82c457607fa4217fbf769d8d/jax/_src/lax/control_flow/common.pyL57L58 I'm not sure what the reason is for the different cacheing mechanisms between these two functions."," good find. I suspect we just haven't gotten to adding `weakref_lru_cache` everywhere it should be, since it was a later development. One simple issue here is that `weakref_lru_cache` creates a weakref to its first argument, and cond/switch uses `_initial_style_jaxprs_with_common_consts` which has a sequence of funs as its first argument. Builtin sequence types don't work with weakrefs, and I vaguely recall some complexity with making our own weakrefable sequence type... like, if we drop a reference to one element in the sequence, we need to drop the whole cache entry. Yeah now that I think about this, I recall talking about this with . Parker, if you remember anything else / different, let us know! Otherwise the todo item may be to work out a scheme for applying something like `weakref_lru_cache` to functions which have a `Sequence[Callable]` first argument.",We could change the call signature of the utility to accept `*args` if the sequence argument is the only problem.,Giving it a try in CC(Use weakref_lru_cache for initial_style_jaxprs_with_common_consts),"Looking at this closer, I'm not sure that cache is the issue. For example:   The cache for this function is only hit once in your sample code. I'm having trouble replicating the memory growth you're reporting. Is it possible that you're seeing memory growth ondevice? Each of your loops creates an ondevice buffer which is then immediately dereferenced, but it won't actually be deleted from device until python garbage collection cleans up the reference and triggers an asynchronous buffer deletion on your device."," Try this code:  I got the following output for the two seconds:  In your snippet, `jit` cache is catching all the recompilations. But, in my use case, `jit` cannot cache since the shapes are varying. "," CC(fix memory leak in cond jaxpr tracing) fixed this! To verify, I ran this variant of the repro code:  Woohoo!"
322,"以下是一个github上的jax下的一个issue, 标题是(Fix test failures under SciPy 1.10.0.)， 内容是 (Using `jax.numpy.power` inside a callback means that the callback yields JAX arrays, which confuse SciPy.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Fix test failures under SciPy 1.10.0.,"Using `jax.numpy.power` inside a callback means that the callback yields JAX arrays, which confuse SciPy.",2023-01-30T20:34:59Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14207
1258,"以下是一个github上的jax下的一个issue, 标题是(pure_callback for eigendecomposition)， 内容是 ( Description Hi, eigendecomposition with host_callback was successful with jit and pmap. But while I was trying to do vmap, host_callback was not the option so tried pure_callback option.     import numpy as np     import jax.numpy as jnp     X = jnp.arange(1000**2).reshape((1000,1000))     def eig(X):         type_complex=jnp.complex64         _eig = lambda x: np.linalg.eig(x)           eigenvalues_shape = jax.ShapeDtypeStruct(X.shape[:1], type_complex)         eigenvectors_shape = jax.ShapeDtypeStruct(X.shape, type_complex)         result_shape_dtype = jax.ShapeDtypeStruct(         shape=(eigenvalues_shape.shape, eigenvectors_shape.shape),         dtype=type_complex         )         return jax.pure_callback(_eig, result_shape_dtype, X)     print(eig(X)) > TypeError: Shapes must be 1D sequences of concrete values of integer type, got ((1000,), (1000, 1000)). It seems that pure_callback doesn't accept multiple return values with different size (like eigendecomposition). Is it by design? Thanks!  What jax/jaxlib version are yo)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,pure_callback for eigendecomposition," Description Hi, eigendecomposition with host_callback was successful with jit and pmap. But while I was trying to do vmap, host_callback was not the option so tried pure_callback option.     import numpy as np     import jax.numpy as jnp     X = jnp.arange(1000**2).reshape((1000,1000))     def eig(X):         type_complex=jnp.complex64         _eig = lambda x: np.linalg.eig(x)           eigenvalues_shape = jax.ShapeDtypeStruct(X.shape[:1], type_complex)         eigenvectors_shape = jax.ShapeDtypeStruct(X.shape, type_complex)         result_shape_dtype = jax.ShapeDtypeStruct(         shape=(eigenvalues_shape.shape, eigenvectors_shape.shape),         dtype=type_complex         )         return jax.pure_callback(_eig, result_shape_dtype, X)     print(eig(X)) > TypeError: Shapes must be 1D sequences of concrete values of integer type, got ((1000,), (1000, 1000)). It seems that pure_callback doesn't accept multiple return values with different size (like eigendecomposition). Is it by design? Thanks!  What jax/jaxlib version are yo",2023-01-29T11:20:53Z,question,closed,0,10,https://github.com/jax-ml/jax/issues/14199,"A couple issues with your first implementation:  dtypes must match. numpy routines return `complex128`, so we must specify that  your funciton returns two values, so you must specify this with two `ShapeDtypeStruct` objects rather than one With those changes, the code works: ","Aha, No need to bundle in one ShapeDtypeStruct. Yes, I was switching a lot between 64 and 128 so wasn't careful with that. Thanks a lot!","Why do you call `np.linalg.eig(x) ` instead of `jnp.linalg.eig(x)`? What if one wants to take gradients through `jnp.eig` (or its equivalent, instead of `jnp.eigh`) on a GPU? Including  as we have the same question","If you want `pure_callback` to be compatible with autodiff, you have to define the autodiff rules explicitly. There's an example here: https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.htmlexamplepurecallbackwithcustomjvp","Thank you, Jake!",", Hope this can help. This enables backprop of eigendecomposition and enforces running on CPU while the whole process running on GPU. Mathematics was adopted from other papers. https://github.com/kcml2/meent/blob/main/meent/on_jax/primitives.py  ",Above code has bug and below is fixed. ,Thank you so much!," Hi, I found another bug in the code. Below is the fixed. FYI, the previous code showed ~3E5 difference between backproped gradient and numerical gradient.  > /Users/yongha/miniforge3_m1/envs/rcwa/bin/python /Users/yongha/project/rcwa/meent/QA/backprop.py  > JAX grad_ad: >  [[[0.31936418 0.21070505 0.08193991 0.56743577 0.22805356] >   [ 0.09779809 0.05806508  0.1413275   0.26680362 0.04509193]] >  [[0.07998023 0.06814561 0.0119665  0.16095988 0.10340981] >   [0.02519205 0.07270359  0.01071986 0.00646068 0.02300493]] >  [[0.01214202 0.06787044 0.0075716  0.02270837 0.08079771] >   [0.003161   0.09658575 0.02822204 0.0118468  0.03896227]]] > > JAX grad_numeric: >  [[[0.31936418 0.21070505 0.08193991 0.56743577 0.22805356] >   [ 0.09779809 0.05806508  0.1413275   0.26680362 0.04509193]] >  [[0.07998023 0.06814561 0.0119665  0.16095988 0.10340981] >   [0.02519205 0.07270359  0.01071986 0.00646068 0.02300493]] >  [[0.01214202 0.06787044 0.0075716  0.02270837 0.08079771] >   [0.003161   0.09658575 0.02822204 0.0118468  0.03896227]]] > JAX norm:  5.965338263715071e09 >  > Torch grad_ad: >  tensor([[[0.3194, 0.2107, 0.0819, 0.5674, 0.2281], >          [ 0.0978, 0.0581,  0.1413,  0.2668, 0.0451]], >         [[0.0800, 0.0681, 0.0120, 0.1610, 0.1034], >          [0.0252, 0.0727,  0.0107, 0.0065, 0.0230]], >         [[0.0121, 0.0679, 0.0076, 0.0227, 0.0808], >          [0.0032, 0.0966, 0.0282, 0.0118, 0.0390]]], dtype=torch.float64) > > Torch grad_numeric: >  tensor([[[0.3194, 0.2107, 0.0819, 0.5674, 0.2281], >          [ 0.0978, 0.0581,  0.1413,  0.2668, 0.0451]],  >         [[0.0800, 0.0681, 0.0120, 0.1610, 0.1034], >          [0.0252, 0.0727,  0.0107, 0.0065, 0.0230]], >         [[0.0121, 0.0679, 0.0076, 0.0227, 0.0808], >          [0.0032, 0.0966, 0.0282, 0.0118, 0.0390]]]) > > torch.norm:  tensor(3.3405e08, dtype=torch.float64) >  > Process finished with exit code 0 >",Basically same but unnecessary `.conj()` were removed. 
1303,"以下是一个github上的jax下的一个issue, 标题是(Build Fail with ""fatal error LNK1169: one or more multiply defined symbols found"")， 内容是 ( Description Using the following build command `python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cuda_compute_capabilities=""7.5"" cuda_version=""11.7"" cudnn_version=""8.4.0"" noenable_rocm noenable_tpu` Build fails at the **linking** stage with the following error message. `b'LINK : warning LNK4044: unrecognized option \'/lm\'; ignored\r\nffi.lib(ffi.obj) : error LNK2005: ""struct XLA_FFI_Stream * __cdecl xla::runtime::ffi::GetXlaFfiStream(class xla::runtime::PtrMapByType const *,class xla::runtime::DiagnosticEngine const *)"" (?GetXlaFfiStream@@?$PtrMapByType@@$0BA@@) already defined in executable.lib(executable.obj)\r\n   Creating library bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.lib and object bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.exp\r\nLINK : warning LNK4286: symbol \'?g_trace_lev)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,"Build Fail with ""fatal error LNK1169: one or more multiply defined symbols found"""," Description Using the following build command `python .\build\build.py enable_cuda cuda_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cudnn_path=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7"" cuda_compute_capabilities=""7.5"" cuda_version=""11.7"" cudnn_version=""8.4.0"" noenable_rocm noenable_tpu` Build fails at the **linking** stage with the following error message. `b'LINK : warning LNK4044: unrecognized option \'/lm\'; ignored\r\nffi.lib(ffi.obj) : error LNK2005: ""struct XLA_FFI_Stream * __cdecl xla::runtime::ffi::GetXlaFfiStream(class xla::runtime::PtrMapByType const *,class xla::runtime::DiagnosticEngine const *)"" (?GetXlaFfiStream@@?$PtrMapByType@@$0BA@@) already defined in executable.lib(executable.obj)\r\n   Creating library bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.lib and object bazelout/x64_windowsopt/bin/external/org_tensorflow/tensorflow/compiler/xla/python/xla_extension.so.if.exp\r\nLINK : warning LNK4286: symbol \'?g_trace_lev",2023-01-26T08:21:38Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/14165," could this be due to the new XLA runtime for CPU or GPU. i see ""xla::runtime::ffi::GetXlaFfiStream"" symbol. Is there some c++ language feature we used that broke Windows build?",hartshorne could you make a more readable error report? something with proper newlines?,,"We intentionally define this symbol twice, the default implementation uses weak linking here: https://github.com/openxla/xla/blob/f95fde0a6bfe3e7f92f26294586be978434f3e79/xla/runtime/ffi.ccL123L128. The ""real"" implementation in `executable.cc` is supposed to overwrite it at link time. But looks like MSVC doesn't like it? `ABSL_ATTRIBUTE_WEAK` is incorrectly defined?",Skimming through https://stackoverflow.com/questions/2290587/gccstyleweaklinkinginvisualstudio/11529277 CC(未找到相关数据)77 it looks like GCC/LLVM style weak linking doesn't work in MSVC. If there are no flags/attributes to make it work with MSVC I'll take a look at changing the registration mechanism to something that does not rely on weak symbols.,It seems quite a few build issues come from the fact that MSVC always has its own (often off standard) quirks and the various teams involved with JAX / TF / XLA never use MSVC. Is there any reason not to just move to a situation where the Windows build of JAX is by default set to compile with LLVM (as it is available for Windows)?,"I'm guessing this issue is stale, given we have a Windows CPU CI build that is not showing this problem at head. And yes, I think we would like to build our Windows wheels with clang, because it would be one fewer difference between platforms. We will probably wait for TensorFlow to make that switch, though, first (I gather it is in progress), simply to share the work."
1291,"以下是一个github上的jax下的一个issue, 标题是(Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.21.7)， 内容是 (Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.21.7 What's Changed  Disable GoogleDocstring._lookup_annotation by @​hoodmane in toxdev/sphinxautodoctypehints CC(What should JAX do about topK?) Fix napoleon handling of numpy docstrings with no explicitly provided return type by @​hoodmane in toxdev/sphinxautodoctypehints CC(dev timeline for more batched ops)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.21.6...1.21.7    Changelog Sourced from sphinxautodoctypehints's changelog.  1.21.7   Fixed a bug where if a class has an attribute and a constructor argument with the same name, the constructor argument type would be rendered incorrectly (issue 308)   Fixed napoleon handling of numpy docstrings with no specified return type.   1.21.6  Fix a Field list ends without a blank line warning (issue 305).  1.21.5  More robust determination of rtype location / fix issue 302  1.21.4  Improvements to the locati)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.21.7,"Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.21.7 What's Changed  Disable GoogleDocstring._lookup_annotation by @​hoodmane in toxdev/sphinxautodoctypehints CC(What should JAX do about topK?) Fix napoleon handling of numpy docstrings with no explicitly provided return type by @​hoodmane in toxdev/sphinxautodoctypehints CC(dev timeline for more batched ops)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.21.6...1.21.7    Changelog Sourced from sphinxautodoctypehints's changelog.  1.21.7   Fixed a bug where if a class has an attribute and a constructor argument with the same name, the constructor argument type would be rendered incorrectly (issue 308)   Fixed napoleon handling of numpy docstrings with no specified return type.   1.21.6  Fix a Field list ends without a blank line warning (issue 305).  1.21.5  More robust determination of rtype location / fix issue 302  1.21.4  Improvements to the locati",2023-01-23T17:03:01Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/14119,Still waiting on sphinxbooktheme 0.4.0,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
1289,"以下是一个github上的jax下的一个issue, 标题是([shape_poly] Generalize binary operations with symbolic dimensions.)， 内容是 (Previously binary operations involving symbolic dimensions would work only when the other operand is convertible to a symbolic dimension, e.g., an integer. This resulted in errors when trying ""x.shape[0] * 3.5"" and the recourse was to ask the user to add an explicit ""jnp.array(x.shape[0])"". Now we allow binary operations with any operand and the ""jnp.array"" is added automatically if the other operand is not an integer or a symbolic dimension. This means that instead of an error they may be an error downstream if one tries to use the result as a dimension. There is one known case where JAX works with static shapes and with the previous behavior, but will fail now. When you operate on `np.ndarray` and symbolic dimension, previously this was kept as a `np.ndarray` but not it is turned into a JAX array. The following program will now fail if `x.shape[0]` is a symbolic dimension.: `jnp.ones(np.arange(5) * x.shape[0])` Instead you should write `jnp.ones([i * x.shape[0] for i in range(5)])` We also took the opportunity to make some i)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,[shape_poly] Generalize binary operations with symbolic dimensions.,"Previously binary operations involving symbolic dimensions would work only when the other operand is convertible to a symbolic dimension, e.g., an integer. This resulted in errors when trying ""x.shape[0] * 3.5"" and the recourse was to ask the user to add an explicit ""jnp.array(x.shape[0])"". Now we allow binary operations with any operand and the ""jnp.array"" is added automatically if the other operand is not an integer or a symbolic dimension. This means that instead of an error they may be an error downstream if one tries to use the result as a dimension. There is one known case where JAX works with static shapes and with the previous behavior, but will fail now. When you operate on `np.ndarray` and symbolic dimension, previously this was kept as a `np.ndarray` but not it is turned into a JAX array. The following program will now fail if `x.shape[0]` is a symbolic dimension.: `jnp.ones(np.arange(5) * x.shape[0])` Instead you should write `jnp.ones([i * x.shape[0] for i in range(5)])` We also took the opportunity to make some i",2023-01-21T12:15:01Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14106
681,"以下是一个github上的jax下的一个issue, 标题是([sparse] refactor tests to improve runtime)， 内容是 (This effectively combines the `_CheckAgainstDense` and `_CompileAndCheckSparse` test utilities. Previously each test would do two jit and two nonjit calls of each tested function; with this change it's down to one jit and one nonjit call. We lose some coverage in the area of recompilation checks (this is why `_CompileAndCheckSparse` would call the jitcompiled function twice) but given that test timeouts have become an issue, I think that's an OK tradeoff.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[sparse] refactor tests to improve runtime,"This effectively combines the `_CheckAgainstDense` and `_CompileAndCheckSparse` test utilities. Previously each test would do two jit and two nonjit calls of each tested function; with this change it's down to one jit and one nonjit call. We lose some coverage in the area of recompilation checks (this is why `_CompileAndCheckSparse` would call the jitcompiled function twice) but given that test timeouts have become an issue, I think that's an OK tradeoff.",2023-01-20T19:18:58Z,kokoro:force-run pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14096,"So, it turns out that this doesn't have an appreciable affect on the runtime of `sparse_test.py`. Perhaps the slow tests are dominated by JIT compile time, so that doing repeated operations doesn't make them appreciably slower? Still, I like this change because it makes the test utilities cleaner, so I think we should merge it."
689,"以下是一个github上的jax下的一个issue, 标题是(jnp.linalg.inv gave incorrect results)， 内容是 ( Description I was trying to use  as a replacement for  but found that  behaved differently from  and gave incorrect results for matrix inversion. See below for a reproducible example:  Multiplying the original matrix with its jnp inverse matrix does not return an identity matrix:  but  gave the correct results:   What jax/jaxlib version are you using? jax0.4.1, jaxlib0.4.1  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.5  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,jnp.linalg.inv gave incorrect results," Description I was trying to use  as a replacement for  but found that  behaved differently from  and gave incorrect results for matrix inversion. See below for a reproducible example:  Multiplying the original matrix with its jnp inverse matrix does not return an identity matrix:  but  gave the correct results:   What jax/jaxlib version are you using? jax0.4.1, jaxlib0.4.1  Which accelerator(s) are you using? GPU  Additional system info Python 3.9.5  NVIDIA GPU info ",2023-01-20T01:46:09Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/14086,"Thanks for the question – this comes from the fact that JAX defaults to 32bit computations, which are more performant on accelerators. If you enable X64 mode before running your computation, you should see that the JAX accuracy is consistent with that of NumPy. For example, add this to the top of your script:  Then **restart your runtime** and reexecute, and the output should look like this: ",Thanks! That worked.
486,"以下是一个github上的jax下的一个issue, 标题是(CI: adjust permissions for upstream-nightly build)， 内容是 (The report job failed with a permissions error: https://github.com/google/jax/actions/runs/3958254462/jobs/6779789502 This should fix the issue. For what it's worth, the root cause is a failure in numpy nightly, and is tracked here: numpy/numpy/issues/23033)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,CI: adjust permissions for upstream-nightly build,"The report job failed with a permissions error: https://github.com/google/jax/actions/runs/3958254462/jobs/6779789502 This should fix the issue. For what it's worth, the root cause is a failure in numpy nightly, and is tracked here: numpy/numpy/issues/23033",2023-01-20T00:35:02Z,pull ready,closed,0,1,https://github.com/jax-ml/jax/issues/14085,Successfully created issues (see links above) I need to revert some of the temporary changes.
770,"以下是一个github上的jax下的一个issue, 标题是(jit compilation with large inlined arrays crashes JAX.)， 内容是 ( Description The following code snippet would result in kernel crashing on Colab (due to OOM).  Not inlining the `dataset` array works. This is a simplified usage from the code here https://github.com/deepmind/acme/blob/master/acme/datasets/tfds.pyL199L200. My temporary workaround is to fork and change the code to not inline the jax array.  What jax/jaxlib version are you using? jax v0.4.1, jax 0.3.5  Which accelerator(s) are you using? CPU/GPU  Additional system info Colab or Local machine with 3080  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",dspy,jit compilation with large inlined arrays crashes JAX.," Description The following code snippet would result in kernel crashing on Colab (due to OOM).  Not inlining the `dataset` array works. This is a simplified usage from the code here https://github.com/deepmind/acme/blob/master/acme/datasets/tfds.pyL199L200. My temporary workaround is to fork and change the code to not inline the jax array.  What jax/jaxlib version are you using? jax v0.4.1, jax 0.3.5  Which accelerator(s) are you using? CPU/GPU  Additional system info Colab or Local machine with 3080  NVIDIA GPU info _No response_",2023-01-19T21:51:31Z,bug needs info,closed,0,11,https://github.com/jax-ml/jax/issues/14080,What is happening is that it is recompiling it every time and baking the dataset into the graph. This is causing the oom because dataset is large enough that we run out of memory before the caching logic kicks in and starts evicting programs. Seems like a bug in the acme repository.,I am not sure if I understand what you mean by recompiling in this case though. Wouldn't the JIT happen only once in this case?,"My mistake, I thought that this was getting called in a loop. Regardless, by capturing dataset you are 'baking' that constant into the XLA graph itself which will copy it to the CPU for the compilation process (which might be what causes the OOM) and generally isn't a good idea. Additionally, I cannot reproduce the oom.",I can reproduce this when running on colab.. not sure what's different... It's still quite weird to me that I either get a hang locally or run OOM on Colab. The constant array is at most a few hundred Mb large and I don't expect that to cause any issue for XLA.,Can you post some of the details of the oom?,"No problem, here is the colab that I use. https://colab.research.google.com/drive/1mSaw6wxlSCGjiCmXWRo2_1RtbAye1s9?usp=sharing",Looks like the constant is getting copied many times (At least when I run this locally). There is possibly something slightly different in colab (I'm seeing an 8GB alloc in the oom logs). This leads to a significant blowup. I also noticed in your logs the following message which implies that you're not even using GPU:  This problem might not be present in the GPU/TPU backends which is why the it isn't a problem for other users of this deepmind code.,"For GPU, I do not have OOM but instead the process hangs and after some time I get the slow JIT compile error. On Tue, 24 Jan 2023 at 19:21, Parker Schuh ***@***.***> wrote: > Looks like the constant is getting copied many times (At least when I run > this locally). There is possibly something slightly different in colab (I'm > seeing an 8GB alloc in the oom logs). This leads to a significant blowup. > > I also noticed in your logs the following message which implies that > you're not even using GPU: > > WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) > > This problem might not be present in the GPU/TPU backends which is why the > it isn't a problem for other users of this deepmind code. > > — > Reply to this email directly, view it on GitHub > , or > unsubscribe >  > . > You are receiving this because you authored the thread.Message ID: > ***@***.***> >","I have not encountered this issue previously working with other datasets. I only found that this becomes a problem (when I use GPU, it hangs) when my dataset consists of images (so they are significantly larger than other datasets that I used). Still, it is a little surprising for me since the dataset is at most a few hundred megabytes large which should still fit nicely in memory.","For me this is a 300MB constant copied ~9 times for some reason which roughly works out to be multiple GB. These allocations happen inside the XLA compiler, so we would need to make a bug against XLA. This is somewhat surprising to me, but the use case is not very convincing.  Maybe the constant is getting converted to a text constant in the colab backend which is causing additional blowups?","Agree that the use case is a bit artificial, as I also don't see particular reasons that I would want to inline the array. I will file an issue on the acme side suggesting not to inline the array as well."
228,"以下是一个github上的jax下的一个issue, 标题是([sparse] update primitive coverage in module doc)， 内容是 ()请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[sparse] update primitive coverage in module doc,,2023-01-19T17:07:30Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/14077
1260,"以下是一个github上的jax下的一个issue, 标题是(Sharding unable to utilize all RAM/HBM)， 内容是 ( Description When using sharding to distribute a large array over several devices, each shard is unable to utilize the entire device's RAM/HBM. In fact, it seems to be limited to about half that size. I've recreated this both on Cloud TPU (v28, having 8 devices of 8GB each) and on the host of the TPU with 8 CPU devices. _I'm using the nifty jax_smi to monitor live memory utilization_  Single device allocation  Output:  Memory usage:  Here I've allocated almost the entire HBM for a single TPU v2 device.  Sharding over 8 devices  success When sharding, I'm able to allocate half of the above size (combined over all shards). The following will succeed, with the array size being 1_000_000_000, exactly half of before:  Output:  Memory usage:   Sharding over 8 devices  fail When slightly increasing the array size (to 1_100_000_000), we fail with RESOURCE_EXHAUSTED:  Output:  (_I've truncated the entire error message, having a long stack_)  CPU devices To verify this is not specific to TPU, I've recreated this on several CPUs on the T)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Sharding unable to utilize all RAM/HBM," Description When using sharding to distribute a large array over several devices, each shard is unable to utilize the entire device's RAM/HBM. In fact, it seems to be limited to about half that size. I've recreated this both on Cloud TPU (v28, having 8 devices of 8GB each) and on the host of the TPU with 8 CPU devices. _I'm using the nifty jax_smi to monitor live memory utilization_  Single device allocation  Output:  Memory usage:  Here I've allocated almost the entire HBM for a single TPU v2 device.  Sharding over 8 devices  success When sharding, I'm able to allocate half of the above size (combined over all shards). The following will succeed, with the array size being 1_000_000_000, exactly half of before:  Output:  Memory usage:   Sharding over 8 devices  fail When slightly increasing the array size (to 1_100_000_000), we fail with RESOURCE_EXHAUSTED:  Output:  (_I've truncated the entire error message, having a long stack_)  CPU devices To verify this is not specific to TPU, I've recreated this on several CPUs on the T",2023-01-19T15:27:18Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/14075,"Your question contains the answer. You need to avoid the intermediate allocation somehow. If you actually are trying to create jnp.zeros(), then consider running jnp.zeros() as part of an initialization pjit or pmap instead.","I was thinking that might be the reason, yet the numbers do not add up: In the 3rd example (""fail""), the full intermediate allocation + the shard would add up to about 5.1GB, whereas the limit is 7.5GB. Is there an additional (somewhat large) overhead in that process? (in any case, I would make use of pjit & pmap)","This is the XLA computation that the slice lowers to:  Looks like it doubles the memory, with the original array + all 8 slices on the same devices (limiting you to 1/16th of the total memory available). Normally this isn't a problem because each shard happens independently and the duplicated memory is temporary.",Got it. Thank you.
474,"以下是一个github上的jax下的一个issue, 标题是(Reflow three boxes on docs main page for mobile)， 内容是 (On mobile the boxes for key jax features don't reflow. Trying to figure out how to do so, though this admittedly is not something I'm familiar with. Creating PR for discussion and collaboration. ://sphinxdesign.readthedocs.io/en/latest/grids.html )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Reflow three boxes on docs main page for mobile,"On mobile the boxes for key jax features don't reflow. Trying to figure out how to do so, though this admittedly is not something I'm familiar with. Creating PR for discussion and collaboration. ://sphinxdesign.readthedocs.io/en/latest/grids.html ",2023-01-18T23:41:15Z,,closed,0,2,https://github.com/jax-ml/jax/issues/14070,I did roughly the same thing here: https://github.com/google/jax/pull/14049 Just waiting for a review (cc. ),Ah way ahead of me. Closing
1046,"以下是一个github上的jax下的一个issue, 标题是(Fix test failures under NumPy 1.24.)， 内容是 (Fix test failures under NumPy 1.24. NumPy 1.24 release notes: https://numpy.org/devdocs/release/1.24.0notes.html The fixes vary, but there are three particularly common changes: * NumPy 1.24 removes a number of deprecated NumPy type aliases references (np.bool, np.int, np.float, np.complex, np.object, np.str, np.unicode, np.long). This change replaces them with their recommended replacements (bool, int, float, complex, object, str, str, int). * Under NumPy 1.24 no longer automatically infers dtype=object when ragged sequences are passed to np.array(). See https://numpy.org/neps/nep0034inferdtypeisobject.html . In most cases the fix is to pass dtype=object explicitly, but in some cases where the raggedness seems accidental other fixes were used. * NumPy 1.24 is pickier about the dtype= option passed to comparison ufuncs.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Fix test failures under NumPy 1.24.,"Fix test failures under NumPy 1.24. NumPy 1.24 release notes: https://numpy.org/devdocs/release/1.24.0notes.html The fixes vary, but there are three particularly common changes: * NumPy 1.24 removes a number of deprecated NumPy type aliases references (np.bool, np.int, np.float, np.complex, np.object, np.str, np.unicode, np.long). This change replaces them with their recommended replacements (bool, int, float, complex, object, str, str, int). * Under NumPy 1.24 no longer automatically infers dtype=object when ragged sequences are passed to np.array(). See https://numpy.org/neps/nep0034inferdtypeisobject.html . In most cases the fix is to pass dtype=object explicitly, but in some cases where the raggedness seems accidental other fixes were used. * NumPy 1.24 is pickier about the dtype= option passed to comparison ufuncs.",2023-01-18T21:17:01Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14067
376,"以下是一个github上的jax下的一个issue, 标题是(Don't run FP8 dtype test on TPU.)， 内容是 (Don't run FP8 dtype test on TPU. This change makes dtypes_test.py pass even when not using Bazel (e.g. with pytest). It also improves TPU coverage when using Bazel.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,Don't run FP8 dtype test on TPU.,Don't run FP8 dtype test on TPU. This change makes dtypes_test.py pass even when not using Bazel (e.g. with pytest). It also improves TPU coverage when using Bazel.,2023-01-18T18:47:59Z,,closed,0,0,https://github.com/jax-ml/jax/issues/14060
470,"以下是一个github上的jax下的一个issue, 标题是(lax.cond leads to excessive compilation in JAX 0.4.1)， 内容是 (Reported in https://github.com/google/jax/discussions/14032discussioncomment4718316. Minimal reproduction:  Output in JAX v0.3.25:  Output in JAX v0.4.1:  Assigning to  because it's possible this is somehow related to the jax.Array change)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,lax.cond leads to excessive compilation in JAX 0.4.1,Reported in https://github.com/google/jax/discussions/14032discussioncomment4718316. Minimal reproduction:  Output in JAX v0.3.25:  Output in JAX v0.4.1:  Assigning to  because it's possible this is somehow related to the jax.Array change,2023-01-18T16:56:33Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/14058,"Actually, I just checked and `config.jax_array` has no effect on this.",Maybe try setting `jax_experimental_subjaxpr_lowering_cache` config to True and see what happens?,`jax_experimental_subjaxpr_lowering_cache` doesn't seem to affect it either way.,"The root cause seems to be CC(checkify: error types), which added these lines https://github.com/google/jax/blob/a37121e19512ea5ee0ad523eda39fa5bbd8c5442/jax/_src/lax/control_flow/conditionals.pyL258L261 Commenting them out fixes the issue."
1004,"以下是一个github上的jax下的一个issue, 标题是(Increased memory requirement without nn.scan)， 内容是 ( Description When using NVIDIA GPUs, we are noticing significant memory overheads when we turn we set nn.scan = False.  With nn.scan = True: As we increase the number of transformer layers, the number of assigned buffers remains constant. The total memory allocation increases mainly due to the increased parameter allocation. This seems like the expected behavior. With nn.scan = False: As we increase the number of transformer layers, the number of assigned buffers increases, and the preallocated temp allocation increases signficantly. In this case, the increased preallocated temp allocation becomes the biggest contributor to the total memory allocation. It seems like temporary buffers are not being reused when nn.scan = False. I have attached some of our results:  ++ ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",transformer,Increased memory requirement without nn.scan," Description When using NVIDIA GPUs, we are noticing significant memory overheads when we turn we set nn.scan = False.  With nn.scan = True: As we increase the number of transformer layers, the number of assigned buffers remains constant. The total memory allocation increases mainly due to the increased parameter allocation. This seems like the expected behavior. With nn.scan = False: As we increase the number of transformer layers, the number of assigned buffers increases, and the preallocated temp allocation increases signficantly. In this case, the increased preallocated temp allocation becomes the biggest contributor to the total memory allocation. It seems like temporary buffers are not being reused when nn.scan = False. I have attached some of our results:  ++ ```",2023-01-18T14:24:03Z,performance XLA NVIDIA GPU GPU,open,0,5,https://github.com/jax-ml/jax/issues/14056,Can you also open a bug on openxla to have the Google XLA team see it? Link the new bug here.," I am unable to open an issue on openxla. It says: ""An owner of this repository has limited the ability to open an issue to users that have contributed to this repository in the past.""", Can you try again? When I try to open an issue here: https://github.com/openxla/xla/issues/new I seem to be able to do it.,"Hi  , is this an issue in XLA or JAX or both? Is this tracked internally to Google already?",I've created a Google internal bug to track this
1309,"以下是一个github上的jax下的一个issue, 标题是(expose `random_wrap` and `random_unwrap` (plus discussion of early typed key surprises))， 内容是 (I've been working on producing a MWE for this issue for the last month or so.  Part of this is setting everything up identically right before the freeze. What I want to do is run the program that fails and print out the key array, and then produce a MWE where I reconstruct that key array.  The problem is that I'm using the new custom PRNG and these don't seem to have any interface for construction from an array. I don't want to step on any toes, but it seems to me that the design complicates this kind of use pattern.  There are some excellent aspects to the current design.  Jax does an excellent job of having small interfaces, and the new PRNG is no exception.  However, it's just one function too small for my needs. Trying to shoehorn this feature request to fit the pimpl idiom looks like it's going to be awkward.   It would end up being something like `PRNGKeyArray(impl, key_array)`, which means exposing the pimpl objects (`threefry_prng_impl`, etc.), and `PRNGKeyArray`, and providing access to the pimpl objects—all things th)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,expose `random_wrap` and `random_unwrap` (plus discussion of early typed key surprises),"I've been working on producing a MWE for this issue for the last month or so.  Part of this is setting everything up identically right before the freeze. What I want to do is run the program that fails and print out the key array, and then produce a MWE where I reconstruct that key array.  The problem is that I'm using the new custom PRNG and these don't seem to have any interface for construction from an array. I don't want to step on any toes, but it seems to me that the design complicates this kind of use pattern.  There are some excellent aspects to the current design.  Jax does an excellent job of having small interfaces, and the new PRNG is no exception.  However, it's just one function too small for my needs. Trying to shoehorn this feature request to fit the pimpl idiom looks like it's going to be awkward.   It would end up being something like `PRNGKeyArray(impl, key_array)`, which means exposing the pimpl objects (`threefry_prng_impl`, etc.), and `PRNGKeyArray`, and providing access to the pimpl objects—all things th",2023-01-17T23:52:19Z,enhancement,closed,0,19,https://github.com/jax-ml/jax/issues/14046,Current workaround: ,"If you're importing `_src.prng` for workarounds, a better workaround may be:  In particular, this works under `jit`. I'll try to write back soon about the broader questions up top. Just wanted to get this to you in the meantime. As always, thanks for putting thought into these things and for the discussion/feedback, Neil!","Coming back to this, the ""workaround"" might actually be an answer to part of the question.  – regardless of the idiom by which we set up PRNG implementations, my original thought was to expose `random_wrap` and `random_unwrap` (perhaps by a different name) as public API functions. Would that suffice for your needs as more than a workaround? I'm also happy to separately review improvements to the internal idiom we use for PRNG implementations (e.g. a switch to inheritance), assuming it's a separate concern, and especially if it cleans things up or makes them easier! The reason that I propose to have `random_{,un}wrap` be the way only in/out of key arrays follows my previous comment: these functions bind corresponding arraycasting primitives, and in particular calling them is invariant under `jit` (and staging more generally). Constructing or unwrapping a Python `PRNGKeyArray` (or derivative thereof) is merely the corresponding impl rule for these primitives. Does this seem right?","Sorry for the delay.  I wanted to invest some time in understanding `random_wrap`.  I'm afraid I don't understand what JIT invariance means.  My understanding of Jax internals still has a long way to go.  I don't want to use up your valuable time, but why is it that simple construction doesn't work with the JIT?  In other words, why do you need to do `random_wrap(array, impl)` instead of `PRNGKeyArray(impl, array)`? > Would that suffice for your needs as more than a workaround? That means you'll be exposing the `PRNGImpl` classes like `threefry_prng_impl`?  Yes, that definitely works. > I'm also happy to separately review improvements to the internal idiom we use for PRNG implementations (e.g. a switch to inheritance), assuming it's a separate concern, and especially if it cleans things up or makes them easier! Yes, it's a separate concern!  And I love your commitment to finding the best design.","This seems to be related:  Would it be possible to make newstyle `KeyArray`s pytrees?  And then arm `PRNGKey` with a custom derivative that sends a zero cotangent back? Also, how do I produce a zero cotangent for a new style key array?","> In other words, why do you need to do `random_wrap(array, impl)` instead of `PRNGKeyArray(impl, array)`? Calling `random_wrap(array, impl)` binds the `random_wrap_p` primitive. When we're staging, we can capture it as such, and so we can later lower it, as well as lower operations on its output (such as slicing; see the `gather` below):  When evaluating eagerly, the primitive is indeed implemented as the expression `PRNGKeyArray(impl, array)`. > Would it be possible to make newstyle `KeyArray`s pytrees? Not quite, because they aren't equivalent to (some tuple of) basic arrays. It's maybe easiest to consider a few examples of what would go wrong if you made them pytrees (naturally, ones that flatten to the underlying `uint32` array). One example is that we can vmap a function over them down to the last dimension, then operate on the key bits—something we want to disallow. Another example is that if we `tree_map` over them, we'd accidentally ""unwrap"" them back down to `uint32` arrays. > Also, how do I produce a zero cotangent for a new style key array? What do you do for current `uint32`style raw key arrays?",> This seems to be related: [...] That's a bug! Thanks for catching it. Filed CC(bad error on VJP of functions returning typed key arrays),We should tag the tracking issue too: CC(RNGs: key types and custom implementations) This is a great thread Neil! Thanks for kicking the tires and asking useful questions.,"> It's maybe easiest to consider a few examples of what would go wrong if you made them pytrees (naturally, ones that flatten to the underlying `uint32` array). One example is that we can vmap a function over them down to the last dimension, then operate on the key bits—something we want to disallow. Another example is that if we `tree_map` over them, we'd accidentally ""unwrap"" them back down to `uint32` arrays. Just a thought, but does Jax support structured arrays?  If so, you could make all of the key information a single record.  Then, it would be essentially atomic in the eyes of vmap and treemap. A related, but less elegant solution would be to register new dtypes corresponding to the key types of your PRNGs. > What do you do for current `uint32`style raw key arrays? I don't do anything yet.  I'm investigating storing the key array that I used on various components of my model in the inference output.  In order for that to work, key arrays need to be pytrees, and I need to be able to produce zero cotangents for them. It does seem a bit odd that I can JIT a function that accepts a seed dynamically, but I can't jit a function that accepts a key array dynamically.","> Just a thought, but does Jax support structured arrays? If so, you could make all of the key information a single record. It doesn't, but we have a semiinternal notion of ""opaque dtypes"" on which the key arrays (and one or two other such things) are built. See CC(prototype unfettered element types in jaxpr arrays), renamed in CC(internal rename: swap mentions of ""custom eltypes"" for ""opaque dtypes""). The two are slightly different. Structured arrays allow for a standard numpy array of structs, whereas opaque dtypes allow for the data an array to be more arbitrary in structure, and allows for the array to customize what operations it supports. With opaque dtypes, our underlying data doesn't have to be an arrayofrecords, slicing doesn't have to be slicing into such an arrayofrecords, various array operations like might not be allowed, etc. > Then, it would be essentially atomic in the eyes of vmap and treemap. Yeah, there might be several ways to achieve this type of ""atomicity in the eyes of vmap."" The opaque dtypes mechanism is sufficient but probably not necessary. Another approach we've kicked around is a generalization of pytrees that involves instantiating typeclasses for different transformations (e.g. vmappable means roughly that you have a pair of rules `from_elt` and `to_elt`). This isn't a complete exploration though, especially not for all transformations. > It does seem a bit odd that I can JIT a function that accepts a seed dynamically, but I can't jit a function that accepts a key array dynamically. How do you mean? This sounds like another potential bug. I can do this: ","Thanks for the very informative explanation! > How do you mean? This sounds like another potential bug. I can do this: I guess I misunderstood the consequences of key arrays not being pytrees.  If they're not a pytree, how are they dynamically passed to a jitted function? And an unrelated question, after CC(bad error on VJP of functions returning typed key arrays) is fixed, how should I produce a zero cotangent for a key array?  For a normal pytree, you would do `tree_map(lambda x: jnp.zeros_like(x), some_tree)`.","> If they're not a pytree, how are they dynamically passed to a jitted function? They're indeed not a pytree, but they do correspond to a ""JAX type"" (that error you were getting with the VJP bug is spurious). This is part of what the internal ""opaque dtype"" mechanism sets up. In other words: we made it work! > And an unrelated question, after CC(bad error on VJP of functions returning typed key arrays) is fixed, how should I produce a zero cotangent for a key array? I'm not sure that it makes sense to make one? Key arrays correspond to uintdtyped arrays under the hood, but either way neither of those forms a vector space. If a function returns a key array, what does it means to differentiate it? Int dtypes present a special case that we (perhaps questionably) decided to handle, and so `zeros_like` works there. Pragmatically: if you're taking VJP of a function whose output has some key arrays and some numeric ones, you could wrap it in one that only outputs numeric ones (and drops the keys) and take VJP of that? There's also that `has_aux` option to `jax.vjp`. Does that get you what you need? I do appreciate your point that the interaction between key arrays and our AD API changes as we upgrade to typed key arrays, and some userside code might need rewriting. We'll have to think on whether we can make the upgrade easier...","> I do appreciate your point that the interaction between key arrays and our AD API changes as we upgrade to typed key arrays, and some userside code might need rewriting. We'll have to think on whether we can make the upgrade easier... Right.  I may be able to work around this, but it's going to be complicated. One pattern that's pretty common in my code is to have classes like this:  When inferring a trajectory using scan, I use the `zeros` method to initialize the state, and the `contangent` method to produce a custom cotangent to pushed into a VJP to train these model components. I understand your idea about not adding key arrays to model component outputs, but instead to create auxilliary outputs. This will complicate my design.  I'll have to have a pair of outputs—one for regular things, and one for auxilliary things.  (I guess while I'm at it, I can put all of the nondiferrentiated things in the auxxilliary output, which is a benefit.) Alternatively, we could consider something like `jax.custom_derivatives.zero_cotangent`:  Is that what `interpreters.ad.instantiate_zeros` does? > They're indeed not a pytree, but they do correspond to a ""JAX type"" Maybe I don't understand the definition of ""pytree"".  I thought anything that supports `tree_util.tree_flatten` is a pytree?  Or does pytree imply an aggregate type? So `Array` is not a pytree for that reason?","> Maybe I don't understand the definition of ""pytree"". [...] I rather meant more narrowly that the type is not registered with `tree_util.register_pytree_node[_class]`. By your more basic definition, key arrays are indeed pytrees (specifically, leaves, just like other arrays are). > Right. I may be able to work around this, but it's going to be complicated. Yeah, I can understand that this is more complex to express, at least relative to what you could write earlier. We'll have to think about this, but perhaps slightly orthogonally. For example, maybe we should allow `None` or a `SymbolicZero(shape, dtype)` as a generalized convenient shorthand for (co)tangents that indicate ""this is not involved in differentiation."" The shorthand here is really just to avoid the `has_aux` and closure rewriting, but it would have essentially equivalent meaning. And it could be nice for more than only key arrays, too. > Alternatively, we could consider something like `jax.custom_derivatives.zero_cotangent` This sounds like it'd be along the lines of the above, in that it could be how you'd obtain such a hypothetical `SymbolicZero(shape, dtype)`. We might also want it for tangents. Maybe we could call it `symbolic_zero_perturbation_like` or something that's less verbose but along the same lines. I'm thinking out loud here. > Is that what `interpreters.ad.instantiate_zeros` does? Kind of, but not exactly in that it doesn't know about primaltangent correspondences. Anyway, that's an internal helper, which will be rendered only available via `jax._src` at the next opportunity. I wouldn't depend on it.","> This sounds like it'd be along the lines of the above, in that it could be how you'd obtain such a hypothetical `SymbolicZero(shape, dtype)`. We might also want it for tangents. Maybe we could call it `symbolic_zero_perturbation_like` or something that's less verbose but along the same lines. >  > I'm thinking out loud here. Awesome. I think there's a lot of beauty to the way that you can return `None` _out of a custom VJP_ to represent a zero cotangents for a nondifferentiated primal _input_.  What I'm looking for is a way to pass something _into a custom VJP_ to represent a zero cotangent of nondifferentiated primal _output_. So, I really like your idea of `SymbolicZero`.  Like you say, it would be nice for it to work for more than only key arrays—ideally,  all ""Jax types""? > Anyway, that's an internal helper, which will be rendered only available via `jax._src` at the next opportunity. I wouldn't depend on it. I'm definitely not using that!  :smile:   I only found it because I guessed that you might be producing zero cotangents somewhere in Jax code.  If you already have a zero cotangent function inside Jax, I figured it would strengthen the argument for exposing such a function.","I brought up both `None` and `SymbolicZero` because both come up in custom JVPs as of CC(custom_jvp symbolic zeros support). Symbolic zeros enter the rule, but the rule can return `None`s if preferred. Plain `None`s are fine if information like the shape and dtype aren't needed or can be inferred. I haven't entirely thought through whether a `None` is enough when invoking a VJP. > If you already have a zero cotangent function inside Jax, I figured it would strengthen the argument for exposing such a function. To be sure, I think we've determined that this isn't quite the function you're looking for anyway – is that right? That is, your request is not ""let me obtain a proper zero cotangent for this arbitrary primal array."" Instead it is ""let me indicate, by passing a sentinel value in place of a cotangent, an intent not to perturb at this output position."" (In fact, there isn't a cotangent space for your primal array, and in particular no zero contangent.)","> > Is that what interpreters.ad.instantiate_zeros does? > Kind of, but not exactly in that it doesn't know about primaltangent correspondences. Anyway, that's an internal helper, which will be rendered only available via jax._src at the next opportunity. I wouldn't depend on it. I do depend on this... :D Please keep in available somewhere! > Plain `None`s are fine if information like the shape and dtype aren't needed or can be inferred. I haven't entirely thought through whether a `None` is enough when invoking a VJP. I'm pretty sure plain `None`s are totally fine. That's the API offered by `equinox.filter_vjp` and it works smoothly. (After all you can always just reconstruct the symbolic zero with a `Zero(core.get_aval(primal).at_least_vspace())`.)","> That is, your request is not ""let me obtain a proper zero cotangent for this arbitrary primal array."" Instead it is ""let me indicate, by passing a sentinel value in place of a cotangent, an intent not to perturb at this output position."" Yes, exactly! Ideally, the system should work on components:  Where `cotangents_in` is a tuple of pytrees of cotangent inputs, some of which may _contain_ symbolic zero _components_.  For example, `cotangents_in[3]` may be equal to `(jnp.ones(3), {'foo': SymbolicZero(PRNGKey(2))})`. This goes beyond what's supported by `None` in a custom VJP backwards pass (which can only send `None` for an entire cotangent output—not for a component.  My ideal interface is therefore: `SymbolicZero(any_pytree)` where `any_pytree` could be an array, a key array, or any aggregate pytree type. > I'm pretty sure plain `None`s are totally fine. That's the API offered by `equinox.filter_vjp` and it works smoothly So you support `None` even in place subelements of cotangents (as in my example above)?","Closing because at this point we have `jax.random.{key_data,key_impl}` for unwrapping and `jax.random.wrap_key_data` for wrapping."
1291,"以下是一个github上的jax下的一个issue, 标题是(Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.21.0)， 内容是 (Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.21.0 What's Changed  Put Literal args in code blocks by @​hoodmane in toxdev/sphinxautodoctypehints CC(Added batching rules for convolutions + pooling.) Handle collections.abc.Callable as well as typing.Callable by @​hoodmane in toxdev/sphinxautodoctypehints CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.) Remove redundant return type for attributes by @​hoodmane in toxdev/sphinxautodoctypehints CC(Fix some TODOs that were previously blocked on a Jaxlib release.) Put rtype before examples or usage section by @​hoodmane in toxdev/sphinxautodoctypehints CC(Update XLA version to include fix for XLA reducewindow on CPU) If module is _io, use io instead by @​hoodmane in toxdev/sphinxautodoctypehints CC(fixed TypeError caused by body_fun of foreach loop) Handle types from types module by @​hoodmane in toxdev/sphinxautodoctypehints CC(vmap into)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.21.0,"Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.21.0 What's Changed  Put Literal args in code blocks by @​hoodmane in toxdev/sphinxautodoctypehints CC(Added batching rules for convolutions + pooling.) Handle collections.abc.Callable as well as typing.Callable by @​hoodmane in toxdev/sphinxautodoctypehints CC(Make translation rule for select_and_gather_add work even when jax_enable_x64 is disabled.) Remove redundant return type for attributes by @​hoodmane in toxdev/sphinxautodoctypehints CC(Fix some TODOs that were previously blocked on a Jaxlib release.) Put rtype before examples or usage section by @​hoodmane in toxdev/sphinxautodoctypehints CC(Update XLA version to include fix for XLA reducewindow on CPU) If module is _io, use io instead by @​hoodmane in toxdev/sphinxautodoctypehints CC(fixed TypeError caused by body_fun of foreach loop) Handle types from types module by @​hoodmane in toxdev/sphinxautodoctypehints CC(vmap into",2023-01-16T17:03:15Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/14024,We can’t update this yet,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it."
498,"以下是一个github上的jax下的一个issue, 标题是(`jax.random.uniform` returns `nan` if `minval` or `maxval` has `inf` or an extreme value)， 内容是 ( Description  The same happens also if I set `minval=jnp.iinfo(jnp.float32).min`.  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax.random.uniform` returns `nan` if `minval` or `maxval` has `inf` or an extreme value, Description  The same happens also if I set `minval=jnp.iinfo(jnp.float32).min`.  What jax/jaxlib version are you using? 0.3.25  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_,2023-01-13T16:00:59Z,bug,closed,0,4,https://github.com/jax-ml/jax/issues/14003,"Thanks for the report – it's not obvious to me that a uniform distribution with infinite range is welldefined. Cosider that the uniform distribution is defined such that the probability density function satisfies $$ \int_{x_{min}}^{x_{max}} u(x) dx = 1 $$ and when $x_{min}$ or $x_{max}$ approach infinity, the pdf is everywhere zero, meaning you have zero probability of choosing any finite range of values. What do you think?","Thanks for the quick answer , I very much agree! I raised the issue because I was expecting `inf` to translate to the max representable `jnp.float32`, with the range being finite.","Thanks  that makes sense. I think though the current behavior is more consistent than clipping infinite values to the float range. For example, numpy's uniform distribution also treats infinite bounds as invalid:  JAX cannot raise an error in this case for the same reasons that it cannot error on outofbound indices (exceptions/warnings depending on runtime values is not supported), so JAX returns NaN.","Got it  I'll close the issue as there seems to be no action here in jax, but thanks very much for the exchange!"
572,"以下是一个github上的jax下的一个issue, 标题是(`jax[cuda]=0.4.1` version is slower on A100 GPU)， 内容是 ( Description I first encountered this issue in a larger code I was trying to run. Then, I was able to create a bare minimum version as follows. I think the `jnp.linalg.cholesky` is making it slower because matrix multiplication alone did not show a performance degradation.  Outputs on different devices and JAX versions were the following:  ```)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,`jax[cuda]=0.4.1` version is slower on A100 GPU," Description I first encountered this issue in a larger code I was trying to run. Then, I was able to create a bare minimum version as follows. I think the `jnp.linalg.cholesky` is making it slower because matrix multiplication alone did not show a performance degradation.  Outputs on different devices and JAX versions were the following:  ```",2023-01-13T14:05:13Z,bug NVIDIA GPU,closed,0,6,https://github.com/jax-ml/jax/issues/14002,"I am also seeing this.  Here are my timing measurements on a v100: !image and here they are on a a100: !image I've also measured this on a p100, and the timings are similar to the p100. Interestingly, when I measured jax.linalg.cholesky on a v100 back in September 2023, I got a curve that looked very similar to the a100 curve I get now.  So my theory is that the cholesky implementation improved sometime between now and then, but only for v100 and p100's.","Hi   I have tested the provided repro on GCP VM instances having GPUs A100, V100 and P100, individually, with JAX version 0.5.0 and the time taken by JAX is as follows:    0.000226063552 Could you please verify in your system and check if the issue is resolved? Also, please let us if the issue still persists. Thank you.",Verified!,Hi   Could you please close the issue if the resolved? Thank you.,"I didn't create the issue, so GitHub is telling me I don't have permissions to close it.",Great to know that this issue is resolved. Thanks to the people who worked on it and verified the solution. I am closing the issue now.
433,"以下是一个github上的jax下的一个issue, 标题是(Zero-length indices in `jax.numpy.mgrid`)， 内容是 ( Description   What jax/jaxlib version are you using? jax commit f729da4a368e9, jaxlib 0.4.1  Which accelerator(s) are you using? CPU/GPU  Additional system info python 3.10.9, Linux (Arch Linux)  NVIDIA GPU info )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Zero-length indices in `jax.numpy.mgrid`," Description   What jax/jaxlib version are you using? jax commit f729da4a368e9, jaxlib 0.4.1  Which accelerator(s) are you using? CPU/GPU  Additional system info python 3.10.9, Linux (Arch Linux)  NVIDIA GPU info ",2023-01-12T12:04:14Z,bug,closed,0,0,https://github.com/jax-ml/jax/issues/13981
814,"以下是一个github上的jax下的一个issue, 标题是(Unexpected result when solving a spring-mass system)， 内容是 ( Description Would like to report a strange result when trying to implement a springmass solver using `jax.experimental.ode.odeint`.  !0a5d05772a724f26829b7c7be5afbb95 The result above shows the displacement going to a large positive number which I thought would have been controlled by both the spring constant k and damper c. Here is some working code using `scipy.integrate.odeint`:   !0de086c804ce44cf90ba9038027deb04  What jax/jaxlib version are you using? jax v0.4.1  Which accelerator(s) are you using? CPU  Additional system info Apple silicone  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Unexpected result when solving a spring-mass system, Description Would like to report a strange result when trying to implement a springmass solver using `jax.experimental.ode.odeint`.  !0a5d05772a724f26829b7c7be5afbb95 The result above shows the displacement going to a large positive number which I thought would have been controlled by both the spring constant k and damper c. Here is some working code using `scipy.integrate.odeint`:   !0de086c804ce44cf90ba9038027deb04  What jax/jaxlib version are you using? jax v0.4.1  Which accelerator(s) are you using? CPU  Additional system info Apple silicone  NVIDIA GPU info _No response_,2023-01-11T23:38:02Z,bug,closed,0,8,https://github.com/jax-ml/jax/issues/13975,"It looks like `F` is always applied as a positive force  shouldn't it multiply the velocity in order to be a damping term? In other words, if I'm reading it correctly, at every step the velocity is increased by `F/m`, which seems like it would cause the position to run away toward positive infinity if the spring forces can't overcome that.","The reason why I wrote it that way is because I thought the above would evaluate as the following : !image The intention would be that the force balance is evaluated as $\ddot{x} = \frac{k}{m}x  \frac{c}{m}\dot{x} + \frac{F}{m} $ At some point, $kx$ would grow large enough that it counteracts the constant $F$. Is there some nuance in my implementation that is forcing $\frac{F}{m}$ to be additive at each time step?","Oh, thanks for the details. It's easier to see when it's written out like that","Is it a problem that your input has shape `(1, 2)` and you output has shape `(2, 1)`?","Yeah that's my bad. Have now corrected this so that inputs and outputs are (2,1). However the result appears to be the same.  !c3e9ff888c6745abb7430a486b341b55","I see the issue: the call signature for the function passed to odeint is `func(y, t, *args)`: https://github.com/google/jax/blob/f7c915e6a239b87b1e4f35710adb35014cdc940f/jax/experimental/ode.pyL153L154 Your function signature is `func(y, F, *args)`. So you are effectively integrating a function where the external force is increasing monotonically with time, which is enough to overcome the force of the spring trying to pull it back. If you define your function like this instead, I think it should work as expected: ","I see my mistake now, that's worked perfectly. I should read the docstrings in more detail, thanks so much for looking into this.  !6a934b51ee654207bbe3108789c1f5cb","Great, glad it's working!"
1265,"以下是一个github上的jax下的一个issue, 标题是(Can't initialize cloud TPU on GKE TPU Nodes)， 内容是 ( Description We are trying to run JAX on GKE Nodes. https://cloud.google.com/tpu/docs/systemarchitecturetpuvmtpu_nodes_4 The docker image works fine on the TPU VM instances but the same one never works when we try to run it on the Google Kubernetes Engine TPU Nodes. Following is a part of the error log from GKE. _ > Traceback (most recent call last):\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 335, in backends\n    backend = _init_backend(platform)\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 387, in _init_backend\n    backend = factory()\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 191, in tpu_client_timer_callback\n    client = xla_client.make_tpu_client()\n  File \""/usr/local/lib/python3.9/distpackages/jaxlib/xla_client.py\"", line 126, in make_tpu_client\n    return _xla.get_tpu_client(\njaxlib.xla_extension.XlaRuntimeError: NOT_FOUND: No ba16c7433 device found."" _ As we were able to run the TensorFlow base ima)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Can't initialize cloud TPU on GKE TPU Nodes," Description We are trying to run JAX on GKE Nodes. https://cloud.google.com/tpu/docs/systemarchitecturetpuvmtpu_nodes_4 The docker image works fine on the TPU VM instances but the same one never works when we try to run it on the Google Kubernetes Engine TPU Nodes. Following is a part of the error log from GKE. _ > Traceback (most recent call last):\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 335, in backends\n    backend = _init_backend(platform)\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 387, in _init_backend\n    backend = factory()\n  File \""/usr/local/lib/python3.9/distpackages/jax/_src/lib/xla_bridge.py\"", line 191, in tpu_client_timer_callback\n    client = xla_client.make_tpu_client()\n  File \""/usr/local/lib/python3.9/distpackages/jaxlib/xla_client.py\"", line 126, in make_tpu_client\n    return _xla.get_tpu_client(\njaxlib.xla_extension.XlaRuntimeError: NOT_FOUND: No ba16c7433 device found."" _ As we were able to run the TensorFlow base ima",2023-01-11T19:09:14Z,bug,open,0,2,https://github.com/jax-ml/jax/issues/13969,https://github.com/google/jax/issues/12917,"GKE is still on the legacy TPU Node architecture, which are different from TPU VMs (see this blog post for a short summary of the difference).  I think you're getting that particular error because you're trying to use a TPU VM setup on a TPU Node. The `jax[tpu]` install doesn't work on TPU Nodes, only TPU VMs. I think a regular `pip install jax jaxlib` would get around that error. However, as discussed in https://github.com/google/jax/issues/12917, JAX doesn't work very well on TPU Nodes, and at this point has only besteffort support. (Thanks  for digging that thread up!) See that thread for options on how to proceed. Feel free to ask about any of them here."
1277,"以下是一个github上的jax下的一个issue, 标题是([jax2tf] Behavior of dimension polynomials as hashables)， 内容是 (jax2tf supports shape polymorphism by using dimension polynomials that duck type as integers inside shapes. See documentation. The design of dimension polynomials prioritised soundness vs. completeness. In particular, the comparison `b == 2` raises `InconclusiveDimensionOperation` because neither `True` nor `False` matches all the possible outcomes of the operation should `b` replaced with an arbitrary nonnegative integer. This choice gives us visibility into cases where the user program or the JAX internals would take different paths based on the shapes. However, it creates quite a bit of drag. E.g., we introduced `core.symbolic_equal_dim` to return `False` in those cases where `==` would return `InconclusiveDimensionOperation`. There are plenty of uses of `core.symbolic_equal_dim` in JAX core, but we hope that users would never need to use it. Recently, I learned about another cost of this implementation: dimension polynomials are not wellbehaved hashables. They do implement `__hash__` and `__eq__` but in some rare cases whe)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,[jax2tf] Behavior of dimension polynomials as hashables,"jax2tf supports shape polymorphism by using dimension polynomials that duck type as integers inside shapes. See documentation. The design of dimension polynomials prioritised soundness vs. completeness. In particular, the comparison `b == 2` raises `InconclusiveDimensionOperation` because neither `True` nor `False` matches all the possible outcomes of the operation should `b` replaced with an arbitrary nonnegative integer. This choice gives us visibility into cases where the user program or the JAX internals would take different paths based on the shapes. However, it creates quite a bit of drag. E.g., we introduced `core.symbolic_equal_dim` to return `False` in those cases where `==` would return `InconclusiveDimensionOperation`. There are plenty of uses of `core.symbolic_equal_dim` in JAX core, but we hope that users would never need to use it. Recently, I learned about another cost of this implementation: dimension polynomials are not wellbehaved hashables. They do implement `__hash__` and `__eq__` but in some rare cases whe",2023-01-11T12:57:38Z,enhancement,closed,0,1,https://github.com/jax-ml/jax/issues/13964,"The hashability became a real problem, with many large tests starting to fail randomly. The first attempt was CC([shape_poly] Fix the hashing and equality of symbolic dimensions) (having a `DimExprHashable` to be used internally and `DimExpr` to be used outside JAX core). This turned out to be hard to land. Not surprisingly, some advanced users needed the same powers that we need internally.  In the end I chose to land CC([shape_poly] Fix the hashing and equality of symbolic dimensions), just make equality return `False` if the symbolic expressions are not evidently equal. This introduces some unsoundness, but turned out to be the simpler solution and hopefully a good compromise. "
830,"以下是一个github上的jax下的一个issue, 标题是(enforcing eigendecomposition on CPU in TPU process)， 内容是 ( Description Hi, I'm trying to do eigendecomposition of nonhermitian matrix on TPU in Colab. As we know, it's available only on CPU so I enforced eig run on CPU with jit option. I tried 2 options. 1. w/o host_callback 2. w/  host_callback  Result  In the first option, initial run was successful but the second outputs Error about casting. Second option returned 'add_outfeed' error. are they bug or misused?  What jax/jaxlib version are you using? jax 0.3.25,  jaxlib 0.3.25+cuda11.cudnn805  Which accelerator(s) are you using? TPU  Additional system info google colab  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,enforcing eigendecomposition on CPU in TPU process," Description Hi, I'm trying to do eigendecomposition of nonhermitian matrix on TPU in Colab. As we know, it's available only on CPU so I enforced eig run on CPU with jit option. I tried 2 options. 1. w/o host_callback 2. w/  host_callback  Result  In the first option, initial run was successful but the second outputs Error about casting. Second option returned 'add_outfeed' error. are they bug or misused?  What jax/jaxlib version are you using? jax 0.3.25,  jaxlib 0.3.25+cuda11.cudnn805  Which accelerator(s) are you using? TPU  Additional system info google colab  NVIDIA GPU info _No response_",2023-01-11T07:14:10Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/13959,"I suspect this an artifact of TPU colab, which is less well supported than TPU VMs.  ?","Hi , It appears the issue has been resolved on Colab TPU v28. I tested the provided repro by modifying the code according to the updated JAX code base. Replaced the `host_calback.call` with `io_callback`, since `host_callback` is deprecated in JAX version 0.4.26 and removed in JAX 0.4.35. Also changed the `type_complex` argument of `eig2` to `jnp.complex64` instead of `jnp.complex128`, since TPU does not support double precision. The modified code successfully runs on Colab TPU without any exceptions. Please find the Colab gist for reference. Thank you.",This is exciting news. Thanks for the update! 
440,"以下是一个github上的jax下的一个issue, 标题是(DOC: add FAQ entry on converting a tracer to an array)， 内容是 (I've answered versions of this question half a dozen times in recent months. Rendered preview of the new section: https://jax13946.org.readthedocs.build/en/13946/faq.htmlhowcaniconvertajaxtracertoanumpyarray)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,DOC: add FAQ entry on converting a tracer to an array,I've answered versions of this question half a dozen times in recent months. Rendered preview of the new section: https://jax13946.org.readthedocs.build/en/13946/faq.htmlhowcaniconvertajaxtracertoanumpyarray,2023-01-10T19:23:56Z,documentation pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13946
335,"以下是一个github上的jax下的一个issue, 标题是(compute jit number of replicas using least-common-multiple, not max)， 内容是 (fix CC(pmap inside jit causes assert not ragged assertion) (see discussion thread there))请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,"compute jit number of replicas using least-common-multiple, not max",fix CC(pmap inside jit causes assert not ragged assertion) (see discussion thread there),2023-01-10T05:30:08Z,kokoro:force-run pull ready,open,0,0,https://github.com/jax-ml/jax/issues/13938
1272,"以下是一个github上的jax下的一个issue, 标题是(pmap inside jit causes assert not ragged assertion)， 内容是 ( Description I am aware that it may not be a good idea to use `pmap` inside `jit` https://github.com/google/jax/issues/2926 but I want to use `pmap` inside the cost function passed to `jax.numpy.linalg.cg()` and which jit compiles it for me, so I have no choice... Running the following minimal example on 16 Tesla V100 GPUs demonstrates the problem (here I jit manually):  The third value in the for loop triggers a `assert not ragged` assertion   very similar to the one reported here for `scan` https://github.com/google/jax/issues/2018 . Other batch shapes also trigger the error. Some work, some don't. I wasn't able to find any pattern. The same error is triggered when spoofing 16 devices in a CPU only environment. Because of the advice here https://github.com/google/jax/issues/13858 I have also tried v0.3.10 but got the same result as on v0.4.1.  What jax/jaxlib version are you using? jax 0.4.1, jaxlib 0.4.1+cuda11.cudnn86  Which accelerator(s) are you using? CPU, GPU  Additional system info Python 3.8.12, Linux  NVIDIA GPU inf)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,pmap inside jit causes assert not ragged assertion," Description I am aware that it may not be a good idea to use `pmap` inside `jit` https://github.com/google/jax/issues/2926 but I want to use `pmap` inside the cost function passed to `jax.numpy.linalg.cg()` and which jit compiles it for me, so I have no choice... Running the following minimal example on 16 Tesla V100 GPUs demonstrates the problem (here I jit manually):  The third value in the for loop triggers a `assert not ragged` assertion   very similar to the one reported here for `scan` https://github.com/google/jax/issues/2018 . Other batch shapes also trigger the error. Some work, some don't. I wasn't able to find any pattern. The same error is triggered when spoofing 16 devices in a CPU only environment. Because of the advice here https://github.com/google/jax/issues/13858 I have also tried v0.3.10 but got the same result as on v0.4.1.  What jax/jaxlib version are you using? jax 0.4.1, jaxlib 0.4.1+cuda11.cudnn86  Which accelerator(s) are you using? CPU, GPU  Additional system info Python 3.8.12, Linux  NVIDIA GPU inf",2023-01-09T19:09:30Z,bug,open,1,5,https://github.com/jax-ml/jax/issues/13931,"Thanks for raising this, with the clear reproducer! The issue is that we compute the number of replicas needed by the `jit`ted computation with a `max` over all the numbers of replicas needed by each pmap inside it, but actually we need a least common multiple. Here's another reproducer: ","Actually, we don't really need a leastcommonmultiple here. We just need that for nested pmaps; having a max across separate pmaps is right. So the CC(compute jit number of replicas using leastcommonmultiple, not max) fix isn't ideal yet: it'll pass these tests, but require too many devices. For example, for the code in the OP to run would require lcm(3, 13) = 39 devices. But the 'axis_groups' calculations we perform while lowering to HLO _do_ currently require this leastcommonmultiple logic. So to require fewer devices, we'll need to adapt that logic to create unevenly sized axis groups. (That's basically what the assertion is checking will never happen.)","> but I want to use pmap inside the cost function passed to jax.numpy.linalg.cg() and which jit compiles it for me, so I have no choice... You meant `jax.scipy.sparse.linalg.cg` right?","Awesome! Thanks for looking into this so quickly! Yes I mean `jax.scipy.sparse.linalg.cg`. I was under the impression that the number of devices must match the 0th axis of `batch`, so for the case `[np.ones((13, 1)), np.ones((3, 2))]` the frst `pmap` in the list comprehension would run on 13 devices and the second on 3. Is that no longer true after the function has been jit compiled?  In the actual use case I have a function that needs to compute a number of things that are not a multiple of the number of devices with `pmap` inside the cost function passed to `jax.scipy.sparse.linalg.cg`. I want to do that by calling `pmap` twice. Once with a batch whose 0th axis is equal to the number of devices and then a second call to spread the computation of the remained over a subset of devices.","Is there any chance of getting a full fix for this, which also solves the problem of requiring an excessive amount of devices? Or maybe a workaround how `pmap` could be called from a `jit` compiled function that avoids the problem in the first place? Thanks very much for your time and effort!"
1291,"以下是一个github上的jax下的一个issue, 标题是(Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.20.1)， 内容是 (Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.20.1 What's Changed  Fixed default options not displaying for parameters without type hints. by @​BhavyeMathur in toxdev/sphinxautodoctypehints CC(added support for np.newaxis to jax.numpy)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.20.0...1.20.1    Changelog Sourced from sphinxautodoctypehints's changelog.  1.20.1  Fixed default options not displaying for parameters without type hints.  1.20  Use hatchling instead of setuptools Add support for typing.ParamSpec Allow star prefixes for parameter names in docstring  1.19.2  Fix incorrect domain used for collections.abc.Callable.  1.19.1  Fix bug for recursive type alias.  1.19.0  Support for CPython 3.11, no longer adds Optional when the argument is default per recommendation from PEP484.  1.18.3  Support and require nptyping&gt;=2.1.2  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Update sphinx-autodoc-typehints requirement from ~=1.18.0 to ~=1.20.1,"Updates the requirements on sphinxautodoctypehints to permit the latest version.  Release notes Sourced from sphinxautodoctypehints's releases.  1.20.1 What's Changed  Fixed default options not displaying for parameters without type hints. by @​BhavyeMathur in toxdev/sphinxautodoctypehints CC(added support for np.newaxis to jax.numpy)  Full Changelog: https://github.com/toxdev/sphinxautodoctypehints/compare/1.20.0...1.20.1    Changelog Sourced from sphinxautodoctypehints's changelog.  1.20.1  Fixed default options not displaying for parameters without type hints.  1.20  Use hatchling instead of setuptools Add support for typing.ParamSpec Allow star prefixes for parameter names in docstring  1.19.2  Fix incorrect domain used for collections.abc.Callable.  1.19.1  Fix bug for recursive type alias.  1.19.0  Support for CPython 3.11, no longer adds Optional when the argument is default per recommendation from PEP484.  1.18.3  Support and require nptyping&gt;=2.1.2  1.18.2  Support and require nptyping&gt;=2.1.1  1.18.1  Fix mocked",2023-01-09T17:02:35Z,dependencies python,closed,0,2,https://github.com/jax-ml/jax/issues/13928,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting ` ignore this major version` or ` ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an `ignore` condition with the desired `update_types` to your config file. If you change your mind, just reopen this PR and I'll resolve any conflicts on it.",Not possible for the reason given in the comment above the line in question.
343,"以下是一个github上的jax下的一个issue, 标题是(Enable jax2tf strided_slice test requiring dynamism calculation)， 内容是 (Enable jax2tf strided_slice test requiring dynamism calculation The underlying issue has been fixed.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Enable jax2tf strided_slice test requiring dynamism calculation,Enable jax2tf strided_slice test requiring dynamism calculation The underlying issue has been fixed.,2023-01-09T06:48:43Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13920
1310,"以下是一个github上的jax下的一个issue, 标题是(When lowering with repeated inputs the compiled function fails only after a second call.)， 内容是 ( Description I was incorrectly setting my training loop and compiling ahead of time, I used by mistake a reference to the same pytree twice and got errors in an unexpected way when calling the compiled function. I managed to reduce to the following:  I was originally getting a similar error message but saying `buffers` instead of `arguments`.  I solved my problem before detecting the repeated reference by using `keep_unused=True`, but nothing was telling me that the function was removing an argument, and the function failed only at the second call, so, it was particularly hard to catch. Here is an example in Colab: https://colab.research.google.com/gist/jorgeecardona/6c2c32a26e0176317b4ec204aa7bb07f/bugexample.ipynbscrollTo=bC0Ceqwetddh I would expect either a clear message of which variables are removed when lowering, and to fail on the first call to the compiled function (unless I am missing an extra step here, I don't see a clear reason why the function fails only at the second call)  What jax/jaxlib version are you using? )请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,When lowering with repeated inputs the compiled function fails only after a second call.," Description I was incorrectly setting my training loop and compiling ahead of time, I used by mistake a reference to the same pytree twice and got errors in an unexpected way when calling the compiled function. I managed to reduce to the following:  I was originally getting a similar error message but saying `buffers` instead of `arguments`.  I solved my problem before detecting the repeated reference by using `keep_unused=True`, but nothing was telling me that the function was removing an argument, and the function failed only at the second call, so, it was particularly hard to catch. Here is an example in Colab: https://colab.research.google.com/gist/jorgeecardona/6c2c32a26e0176317b4ec204aa7bb07f/bugexample.ipynbscrollTo=bC0Ceqwetddh I would expect either a clear message of which variables are removed when lowering, and to fail on the first call to the compiled function (unless I am missing an extra step here, I don't see a clear reason why the function fails only at the second call)  What jax/jaxlib version are you using? ",2023-01-09T06:15:08Z,bug,closed,0,2,https://github.com/jax-ml/jax/issues/13918,Hi   It looks like this issue has been resolved in the latest JAX versions. I executed the mentioned code in JAX version 0.4.24 without any errors:  This Gist also demonstrates successful execution of the code in Colab using JAX version 0.4.23.,Thanks for following up   it looks like this issue can be closed!
587,"以下是一个github上的jax下的一个issue, 标题是(Can't import stax from jax)， 内容是 ( Description I am bit new in jax and I was trying to import the **stax** library from **Jax** library. But, it keeps giving me this error:  Please, help me to solve this error :) **Systems requirements:** Jax latest version  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Windows  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Can't import stax from jax," Description I am bit new in jax and I was trying to import the **stax** library from **Jax** library. But, it keeps giving me this error:  Please, help me to solve this error :) **Systems requirements:** Jax latest version  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Windows  NVIDIA GPU info _No response_",2023-01-06T13:10:10Z,bug,closed,1,2,https://github.com/jax-ml/jax/issues/13896,"`jax.experimental.stax` was moved to `jax.example_libraries.stax` in in JAX v0.2.25 and the deprecated alias was removed in JAX v0.3.16. In current JAX versions, you can import it like this: ","That answer my question, thank!"
1274,"以下是一个github上的jax下的一个issue, 标题是(Hand Vectorisation Gives Unexpected Poor Performance)， 内容是 ( Description Hi there The JAX Team, thanks for your patience and support! I am finding inconsistent performance when I hand vectorize code. Here is a simple example writing alternatives to `np.hypot` for a very specific input shape of coordinates.   On my device (:sigh: not those keywords) I get the following times  !image What's more, I can't seem to wrap my head around how the timing relates to the `jaxpr` representation. I was not expecting to be able to predict times perfectly, but I was expecting more complex `jaxpr`, with more visited instructions of similar size, to take longer, but this does not seem to be the case. Is there any merit in trying to ""optimise"" the `jaxpr` i.e. reduce the number of instructions. Obviously some instructions take more time than others, is there any way of knowing which instructions to avoid? Regards Jordan  What jax/jaxlib version are you using? jax 0.4.1 and jaxlib 0.4.1  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8 Ubuntu 22.04  NVIDIA GPU info _No respons)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Hand Vectorisation Gives Unexpected Poor Performance," Description Hi there The JAX Team, thanks for your patience and support! I am finding inconsistent performance when I hand vectorize code. Here is a simple example writing alternatives to `np.hypot` for a very specific input shape of coordinates.   On my device (:sigh: not those keywords) I get the following times  !image What's more, I can't seem to wrap my head around how the timing relates to the `jaxpr` representation. I was not expecting to be able to predict times perfectly, but I was expecting more complex `jaxpr`, with more visited instructions of similar size, to take longer, but this does not seem to be the case. Is there any merit in trying to ""optimise"" the `jaxpr` i.e. reduce the number of instructions. Obviously some instructions take more time than others, is there any way of knowing which instructions to avoid? Regards Jordan  What jax/jaxlib version are you using? jax 0.4.1 and jaxlib 0.4.1  Which accelerator(s) are you using? CPU  Additional system info Python 3.10.8 Ubuntu 22.04  NVIDIA GPU info _No respons",2023-01-05T23:36:08Z,bug,closed,0,6,https://github.com/jax-ml/jax/issues/13891,"Sorry, the times I gave were for `1024` not `100`.","In general I wouldn't expect the length of a jaxpr to always be directly correlated with the runtime, for a couple reasons:  individual primitives require widely varying numbers of flops. As an extreme example, `lax.linalg.svd` and `lax.neg` both lower to a single primitive, but will have vastly different runtimes as the size of the input increases.  even if you're looking at operations with comparable runtimes, the jaxpr doesn't directly map onto the eventual operations done in the compiled code. In general the compiler is able to fuse, elide, and otherwise optimize sequences of operations, and this is not reflected in the jaxpr. That said, it's hard to guess by looking at code (or jaxprs) exactly how fast it will execute, because it has to do with the details of the compiler and the execution engine. For example, when I run your benchmark on a GPU, I find the three approaches to be essentially indistinguishable in performance  This is likely due to the fact that the XLA:GPU compiler has had seen a lot more development effort that the XLA:CPU at this point, and so can better target the hardware for each particular sequence of operations. So overall, my advice would be to write code in a way that is as clear as possible, don't worry about the jaxpr, and let the compiler worry about the optimization.","Thanks for the feedback on the `jaxpr`. I thought it translated a little more closely to the machine, but that matches the actual results I have been getting. I'll chalk this up as a machine problem and avoid vectorising code that I am only going to run locally.  Thanks again, Jordan","I should have added one thing: if you want to get a better idea of what the compiler is doing with your code, there are APIs for that. for example:   Raw HLO is definitely not the easiest thing to read, but this can help you get a better idea of the low level execution of your code.","Awesome thanks,  I nearly asked how to view the assembly but this is probably better. Is HLO separate from `mlir` or is this `mlir` HLO (possibly a dumb question)? Regards Jordan","I believe that in the current version of JAX, this is MHLO, which is also used by MLIR, but  would know the details of that."
370,"以下是一个github上的jax下的一个issue, 标题是(Make Shard.device and Shard.data read-only properties.)， 内容是 (A user was confused when modifying `x.addressable_shards.data`, since it doesn't modify the underlying data buffers (and it shouldn't!).)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,Make Shard.device and Shard.data read-only properties.,"A user was confused when modifying `x.addressable_shards.data`, since it doesn't modify the underlying data buffers (and it shouldn't!).",2023-01-05T14:42:02Z,pull ready,closed,0,0,https://github.com/jax-ml/jax/issues/13881
545,"以下是一个github上的jax下的一个issue, 标题是(GPU tests: disable cusparse_lowering to avoid segfaults.)， 内容是 (We've started seeing failures in the GPU CI jobs: https://source.cloud.google.com/results/invocations/8a57fcfe439c4fea9434ae71d017625e/targets/jax%2Ftesting%2Fcpu%2Fpresubmit_github/log I suspect this is due to increased test coverage, and similar to the other cusparserelated GPU segfaults we've been seeing.)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,GPU tests: disable cusparse_lowering to avoid segfaults.,"We've started seeing failures in the GPU CI jobs: https://source.cloud.google.com/results/invocations/8a57fcfe439c4fea9434ae71d017625e/targets/jax%2Ftesting%2Fcpu%2Fpresubmit_github/log I suspect this is due to increased test coverage, and similar to the other cusparserelated GPU segfaults we've been seeing.",2023-01-04T19:10:57Z,,closed,0,1,https://github.com/jax-ml/jax/issues/13863,"Hmm, looks like this wasn't the issue. Now that I look more closely, it's the CPU test that's failing, not the GPU test. I suspect it's probably a timeout rather than a cusparse issue."
592,"以下是一个github上的jax下的一个issue, 标题是(AttributeError: _src when importing jax)， 内容是 ( Description I'm trying to use `jax` in a brand new virtual environment. I have `jax 0.4.1` and `jaxlib 0.4.1` . When I try to import jax.   If I comment this line in the `__init__` file then everything works fine.   What jax/jaxlib version are you using? 0.4.1  Which accelerator(s) are you using? _No response_  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",yi,AttributeError: _src when importing jax, Description I'm trying to use `jax` in a brand new virtual environment. I have `jax 0.4.1` and `jaxlib 0.4.1` . When I try to import jax.   If I comment this line in the `__init__` file then everything works fine.   What jax/jaxlib version are you using? 0.4.1  Which accelerator(s) are you using? _No response_  Additional system info Linux  NVIDIA GPU info _No response_,2023-01-04T11:04:15Z,bug,closed,0,7,https://github.com/jax-ml/jax/issues/13857,"Thanks for the report – this is a very strange error and I'm not sure what might be causing it. Can you say more about what exactly you are executing, and how you created your environment?","I faced this issue too, when I tried to install the package from within IPython console.. but went off, when restarted the pykernel.","Thanks for the info. With that as a clue, I've been able to reproduce the issue this way:  also like this:  I suspect you were somehow reloading / reimporting JAX, which led to this error. We should probably avoid this by checking `hasattr(jax, '_src')` before deleting it, although I suspect reloading the library in an active runtime could probably lead to other issues as well (for example, objects that are assumed to be singletons may no longer evaluate as equal using an `is` check, pytrees may be doubly registered, etc.)","Yeah, maybe it would be a good idea to handle that situation by raising a more informative error message? It might be safer to pass on this given that it's not a standard use case and would require some careful thinking from us",I would propose for now we do something like this: ,Looks very reasonable to me!,Sounds great  thanks! I'll send you a PR to review.
458,"以下是一个github上的jax下的一个issue, 标题是(removing repeated average function implementation and docs rendering)， 内容是 (Closes https://github.com/google/jax/issues/13853 The function `jax.numpy.average` is repeated many times on the documentation page jax.numpy.average This PR removes duplicated code that caused the issue Thanks)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,removing repeated average function implementation and docs rendering,Closes https://github.com/google/jax/issues/13853 The function `jax.numpy.average` is repeated many times on the documentation page jax.numpy.average This PR removes duplicated code that caused the issue Thanks,2023-01-04T04:28:21Z,,closed,0,4,https://github.com/jax-ml/jax/issues/13855,"Thanks  I don't think this is the right fix. These typing overloads do serve a purpose: they ensure that when the function is used in typechecked code, the static type checker can infer the correct number of outputs. If your goal is fixing the docs, I'd suggest modifying the docs rather than the source code. This documentation is generated via sphinx autodoc: perhaps that extension has settings that control how overloaded function definitions are displayed?",Yeah sure it is the issue with sphinx you're right there might be support for this as if it is many overloads it may seem very weird, CC([typing] improve sphinx rendering of type aliases) improves the situation somewhat by preserving the `ArrayLike` alias rather than printing out the full union it represents.,I think as these function overloading are needed we can close this PR
456,"以下是一个github上的jax下的一个issue, 标题是(remove repeated average function implementation and docs rendering)， 内容是 (Closes https://github.com/google/jax/issues/13853 The function `jax.numpy.average` is repeated many times on the documentation page jax.numpy.average This PR removes duplicated code that caused the issue Thanks)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,remove repeated average function implementation and docs rendering,Closes https://github.com/google/jax/issues/13853 The function `jax.numpy.average` is repeated many times on the documentation page jax.numpy.average This PR removes duplicated code that caused the issue Thanks,2023-01-04T04:11:41Z,,closed,0,0,https://github.com/jax-ml/jax/issues/13854
590,"以下是一个github上的jax下的一个issue, 标题是(The `jax.numpy.average` function is rendering repeatedly in Docs)， 内容是 ( Description This is being caused by repetition of the same function at the implementation side  reduction functions I am going to open a pull request that can fix the mentioned issue !image  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_)请根据以上内容标注这个issue,                用一句简短的话描述这个issue类型是bug报告还是其它类型（例如用户提出需求，请教问题等），设计的模型或者主要对象是什么，由于什么问题出现了什么bug，需要什么帮助。你的回答只需要包含这个句子，不需要包含其他内容",rag,The `jax.numpy.average` function is rendering repeatedly in Docs, Description This is being caused by repetition of the same function at the implementation side  reduction functions I am going to open a pull request that can fix the mentioned issue !image  What jax/jaxlib version are you using? _No response_  Which accelerator(s) are you using? CPU  Additional system info Linux  NVIDIA GPU info _No response_,2023-01-04T04:08:07Z,bug,closed,0,3,https://github.com/jax-ml/jax/issues/13853,"Hi  thanks for the report. I think this is intended: the `jnp.average` function has multiple type overloads, and this is how sphinx represents this in the documentation. The key difference in each is the annotation of `returned`, and how it affects the return value. Still, I agree that it makes the docs a bit busy. I wonder if there's a way to tell sphinx to avoid repeating the definition?",Hi   The `jax.numpy.average` function now renders only once in the documentation as follows:   Please have a look at the documentation here: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.average.html Thank you.,"Yes, it is fixed. Thanks for the update, I am closing this as completed."
